Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
1.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512226,Introductory article,March 2000,Riccardo Viale,,,Male,Unknown,Unknown,Male,,2
1.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512227,Bounded rationality in social science: Today and tomorrow,March 2000,Herbert A. Simon,,,Male,Unknown,Unknown,Male,,258
1.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512228,"Reasons, cognition and society",March 2000,Raymond Boudon,Riccardo Viale,,Male,Male,Unknown,Male,,11
1.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512229,How to build and use agent-based models in social science,March 2000,Nigel Gilbert,Pietro Terna,,Male,Male,Unknown,Male,,235
1.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512230,Reasoning and pragmatics,March 2000,Guy Politzer,Laura Macchi,,Male,Female,Unknown,Mix,,
1.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512231,Deduction and induction: Reasoning through mental models,March 2000,Bruno G. Bara,Monica Bucciarelli,,Male,Female,Unknown,Mix,,
1.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512232,Through the agents' minds: Cognitive mediators of social action,March 2000,Cristiano Castelfranchi,,,Male,Unknown,Unknown,Male,,15
1.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512311,Individualism and holism: New controversies in the philosophy of social science,September 2000,Pierre Demeulenaere,,,Male,Unknown,Unknown,Male,,10
1.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512312,"The limits of probability modelling: A serendipitous tale of goldfish, transfinite numbers, and pieces of string",September 2000,Ranald R. Macdonald,,,Male,Unknown,Unknown,Male,,6
1.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512313,Collective and joint intention,September 2000,Raimo Tuomela,,,Male,Unknown,Unknown,Male,,15
1.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512314,"Dualism, monism, physicalism",September 2000,Tim Crane,,,Male,Unknown,Unknown,Male,,4
1.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512315,Interaction without reduction: The relationship between personal and sub-personal levels of description,September 2000,Martin Davies,,,Male,Unknown,Unknown,Male,,13
1.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512316,Do actions occur inside the body?,September 2000,Helen Steward,,,Female,Unknown,Unknown,Female,,7
2.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512073,On the functional orgins of essentialism,March 2001,H. Clark Barrett,,,Unknown,Unknown,Unknown,Unknown,,
2.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512074,The role of pragmatic rules in the conjunction fallacy,March 2001,Giuseppe Mosconi,Laura Macchi,,Male,Female,Unknown,Mix,,
2.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512075,Is self-knowledge compatible with externalism?,March 2001,Pierre Jacob,,,Male,Unknown,Unknown,Male,,
2.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512076,A mathematical theory of evidence for G.L.S. Shackle,March 2001,Guido Fioretti,,,Male,Unknown,Unknown,Male,,10
2.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512077,How do norms emerge? An outline of a theory,March 2001,Karl-Dieter Opp,,,Unknown,Unknown,Unknown,Unknown,,
2.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512078,The norms of thought: Are they social?,March 2001,Pascal Engel,,,Male,Unknown,Unknown,Male,,5
2.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512079,F.A. Hayek’s theory of mind and theory of cultural evolution revisited: Toward and integrated perspective,March 2001,Evelyn Gick,Wolfgang Gick,,Female,Male,Unknown,Mix,,
2.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512355,Enterprise risk management: Applications of economic modeling and information technology,September 2001,Christine P. Ries,,,Female,Unknown,Unknown,Female,,
2.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512356,A Re-examination of illusory inferences based on factual conditional sentences,September 2001,Paolo Cherubini,Alberto Mazzocco,Aurore Russo,Male,Male,Female,Mix,,
2.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512357,Preface,September 2001,Lorenzo Magnani,Nancy J. Nersessian,,Male,Female,Unknown,Mix,,
2.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512358,Models in science and mental models in scientists and nonscientists,September 2001,William F. Brewer,,,Male,Unknown,Unknown,Male,,6
2.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512359,Developmental issues in model-based reasoning during childhood,September 2001,Patricia H. Miller,,,Female,Unknown,Unknown,Female,,3
2.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512360,Conceptual mapping through keyword coupled clustering,September 2001,Zvika Marx,Ido Dagan,,Male,Male,Unknown,Male,,2
2.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512361,Reasoning based on categorisation for interpreting and acting: a first approach,September 2001,Elisabetta Zibetti,Vicenç Quera,Francesc Salvador Beltran,Female,Unknown,Male,Mix,,
2.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02512362,Moral reasoning without rules,September 2001,Alan H. Goldman,,,Male,Unknown,Unknown,Male,,
3.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02511861,Daniel Kahneman: the Nobel Prize for Economics awarded for Decision-making psychology,March 2002,Rino Rumiati,Nicolao Bonini,,Male,Unknown,Unknown,Male,,
3.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02511862,Preface,March 2002,Lorenzo Magnani,Nancy J. Nersessian,,Male,Female,Unknown,Mix,,
3.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02511863,Conjectures and manipulations: External representations in scientific reasoning,March 2002,Lorenzo Magnani,,,Male,Unknown,Unknown,Male,,3
3.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02511864,The history of models. Does it matter?,March 2002,Christian Haak,,,Male,Unknown,Unknown,Male,,
3.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02511865,Cheating neuropsychologists: A study of cognitive processes involved in scientific anomalies resolution,March 2002,Luca Pezzullo,,,Male,Unknown,Unknown,Male,,
3.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02511866,Physical models and fundamental laws: Using one piece of the world to tell about another,March 2002,Susan G. Sterrett,,,Female,Unknown,Unknown,Female,,16
3.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02511867,Toward a history-based model for scientific invention: Problem-solving practices in the invention of the transistor and the development of the theory of superconductivity,March 2002,Lillian Hoddeson,,,Female,Unknown,Unknown,Female,,1
3.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02511868,The nature of model-based understanding in condensed matter physics,March 2002,Sang Wook Yi,,,,Unknown,Unknown,Mix,,
3.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02511869,Discovering relativity beliefs: Towards a socio-cognitive model for Einstein's relativity theory formation,March 2002,Andrea Cerroni,,,Female,Unknown,Unknown,Female,,1
3.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02511870,Multiagent system based scientific discovery within information society,March 2002,Francesco Amigoni,Viola Schiaffonati,Marco Somalvico,Male,Female,Male,Mix,,
3.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02511871,Abstraction via generic modeling in concept formation in science,March 2002,Nancy J. Nersessian,,,Female,Unknown,Unknown,Female,,18
3.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02513145,A naturalistic perspective on intentionality. Interview with Daniel Dennett,September 2002,Marco Mirolli,Daniel Dennett,,Male,Male,Unknown,Male,,1
3.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02513146,"From knowledge to individual action. Confidence, the hidden face of uncertainty. A rereading of the works of Knight and Keynes",September 2002,Samira Guennif,,,Female,Unknown,Unknown,Female,,4
3.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02513147,"Thinking about biology. Modular constraints on categorization and reasoning in the everyday life of Americans, Maya, and scientists",September 2002,Scott Atran,Douglas I. Medin,Norbert Ross,Male,Male,Male,Male,,9
3.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02513148,Framing and the theory-simulation controversy. Predicting people's decisions,September 2002,Josef Perner,Anton Kühberger,,Male,Male,Unknown,Male,,2
3.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/BF02513149,"Starvation, serotonin, and symbolism. A psychobiocultural perspective on stigmata",September 2002,Daniel M. T. Fessler,,,Male,Unknown,Unknown,Male,,3
4.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/s11299-005-0009-4,Editorial,June 2005,Riccardo Viale,,,Male,Unknown,Unknown,Male,,
4.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/s11299-005-0001-z,Scoring and keying multiple choice tests: A case study in irrationality,June 2005,Maya Bar-Hillel,David Budescu,Yigal Attali,Female,Male,Male,Mix,,
4.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/s11299-005-0004-9,‘‘Just forget it.’’ Memory distortions as bounded rationality,June 2005,Bruno S. Frey,,,Male,Unknown,Unknown,Male,,3
4.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/s11299-005-0005-8,Flying bicycles: How the Wright brothers invented the airplane,June 2005,Philip N. Johnson-Laird,,,Male,Unknown,Unknown,Male,,17
4.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/s11299-005-0008-5,Cooperation and trust in group context,June 2005,Raimo Tuomela,Maj Tuomela,,Male,Female,Unknown,Mix,,
4.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/s11299-005-0006-7,A note on concave utility functions,June 2005,Martin M. Monti,Simon Grant,Daniel N. Osherson,Male,Male,Male,Male,,2
4.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/s11299-005-0003-x,The social and communicative function of conditional statements,June 2005,Jonathan St. B. T. Evans,,,Male,Unknown,Unknown,Male,,15
4.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/s11299-005-0007-6,On the import and rationale of value attribution,June 2005,Nicholas Rescher,,,Male,Unknown,Unknown,Male,,1
4.0,1.0,Mind & Society,,https://link.springer.com/article/10.1007/s11299-005-0002-y,"A matter of trust: The search for accountability in Italian politics, 1990–2000",June 2005,Cristina Bicchieri,Ram Mudambi,Pietro Navarra,Female,Male,Male,Mix,,
4.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/s11299-005-0012-9,"Neuropsychological data, intuitions, and semantic theories",December 2005,Diego Marconi,,,Male,Unknown,Unknown,Male,,
4.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/s11299-005-0013-8,"Explanations by mechanisms in the social sciences. Problems, advantages and alternatives",December 2005,Karl-Dieter Opp,,,Unknown,Unknown,Unknown,Unknown,,
4.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/s11299-005-0010-y,Spontaneous coordination and evolutionary learning processes in an agent-based model,December 2005,Pierre Barbaroux,Gilles Enée,,Male,Male,Unknown,Male,,
4.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/s11299-005-0011-x,Cognitive perspectives in economics,December 2005,Ludovic Dibiaggio,,,Male,Unknown,Unknown,Male,,
4.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/s11299-005-0014-7,Interactions in economic models: Statistical mechanics and networks,December 2005,Müge Ozman,,,Female,Unknown,Unknown,Female,,
4.0,2.0,Mind & Society,,https://link.springer.com/article/10.1007/s11299-005-0015-6,Leaving the ``gothic cathedral'' of economics,December 2005,Massimiliano Ugolini,,,Male,Unknown,Unknown,Male,,
5.0,1.0,Mind & Society,16 May 2006,https://link.springer.com/article/10.1007/s11299-006-0007-1,Is the mind Bayesian? The case for agnosticism,June 2006,Jean Baratgin,Guy Politzer,,Male,Male,Unknown,Male,"In the imposing literature devoted to the psychological study of probability judgment, the use of a theoretical model as a referential norm for “rational” behavior is a usual methodology.
Footnote 1 Since the pioneering study by Rouanet (1961), and following the famous article by Edwards et al. (1963), the Bayesian model of probability has been the most frequently used as a normative reference or as a possible descriptive model. The implicit question that these studies attempt to answer is whether human beings perform probability judgment in a “Bayesian” manner. These interrogations go far beyond the realm of psychology: they also apply to the various domains that use the Bayesian model, such as economics (Davis and Holt 1993), law (Callen 1982), medicine (Casscells et al. 1978), artificial intelligence (Cohen 1985) and philosophy (Stich 1990). The present paper is not directly aimed towards these important debates concerning probabilistic functioning in humans, but is rather a methodological examination of the conditions under which the Bayesian model may be used as a normative theory. In other words, it aims to find an answer to the following question: does the psychological literature take into consideration the various implications and constraints imposed by the usage of the Bayesian model as a normative reference? It will be argued that the answer is negative, and the obstacles which the experimental methodology should overcome if the model were to be used will be reviewed. This paper will be organized as follows. In the next section, the descriptive and normative uses of the Bayesian model made in the past will be reminded, and the main criticisms against using the Bayesian model reviewed. Then, the status of Bayesian model in the psychological literature and in probability theory will be examined in turn (Sections 3 and 4); these preliminary considerations are necessary to arrive at a definition of Bayesianism: such a definition is clearly a prerequisite in order to answer the main question that the paper addresses. The answer to this question will be presented in Section 5, in which the requirements of the Bayesian model, specially pragmatic, will be spelled out, and the misunderstandings of Bayesian model in experimental studies exposed. Finally, in Section 6, several questions related to the scope of Bayesian model will be considered and some of its limitations will be pointed out.",24
5.0,1.0,Mind & Society,18 May 2006,https://link.springer.com/article/10.1007/s11299-006-0008-0,"Cognitive/affective processes, social interaction, and social structure as representational re-descriptions: their contrastive bandwidths and spatio-temporal foci",June 2006,Aaron V. Cicourel,,,Male,Unknown,Unknown,Male,"The brain develops through interaction with itself and coping with physical and cultural environments (Elman et al. 1996). Environments at issue both constrain and facilitate emergent brain-based cognitive/affective mechanisms and changing cultural belief systems and practices. A key assumption here is that continuity exists between the biological evolution of nonhuman and human primates vis-à-vis the emergence of brain and cognitive/affective mechanisms, and cultural or social practices consistent with subsequent more complex human primate development. Despite informed speculation, we lack clear evidence about the evolution of human brain structures and how this evolution can be linked to cognitive/affective mechanisms and processes, and cultural systems associated with abstract systems of thought, language use, forms of social organization, and everyday cultural beliefs and practices. The basic triadic structures and processes of brain, cognitive/affective, and cultural organization and practices vary across primates, but we will assume that within each species, their emergence and development were closely connected. In other words, survival was contingent on individual, collective and interdependent development and a necessary interaction or feedback among the triadic structures and processes. The accelerated (“recent”) historical evolution of human in contrast to nonhuman primates (despite a prior common biological evolution, see below) of these triadic phenomena created different, but interdependent yet not reducible levels of biological, cognitive/affective and cultural systems that I will gloss by such terms as “levels of analysis” and “levels of explanation”. Humans have endowed each level of abstraction or analysis with its own complexity and the idea of a culturally ascribed uniqueness that has promoted labor-intensive conditions for the study of (“bounded” or tractable) folk and scientific notions or concepts. In science and art, boundaries are becoming increasingly blurred, but in the pages that follow, I will focus primarily on the ways in which different levels of analysis continue to be the goal of contemporary practices. Humans have developed practical, clever accounting practices to summarize and seek explanations for the emergence and uniqueness of their species, and their general and specific characteristics. Demographic and historical studies are good examples of abstract levels of analysis. For example, the study of such notions as differential fertility and migration patterns, as well as the use of terms that depict different kinds of socially organized entities (prides, broods, hives, mating systems, housing, food production, nesting, animal colonies, roosts, flocks, families, clans, tribes, communities, villages, and towns), and activities (religion, healthcare practices, economic transactions, and marital relations). Humans have created other kinds of explanations about their origins and evolution by the study of sub-atomic and bio-chemical phenomena, skeletal structure, bone density, archeological research on former human living sites and the likely origin, and composition of artifacts. Surviving artifacts, such as early clay pots, writing tablets, calendars, and wooden or metal or paper products that suggest a system of trade or barter, artwork and music all become resources for describing and re-describing a given level of abstractness or analysis. The study of such clever sources of biological, physical, and cultural evolutionary activities, therefore, always presupposes explicit forms of literacy and or representational formalisms. It is often difficult, however, to examine such illusive or “silent” brain, cognitive, and cultural mechanisms that enable us to produce emotion, reasoning, gestures, symbolic representation, prosody and language use, and cultural belief systems and practices. Yet these “silent” mechanisms, and intangible belief systems, and more perceivable communicative practices have made it possible to create new ways of creating and depicting frames of reference for understanding human historical evolution and development. A given level of abstraction, therefore, often treats as self-evident the cognitive/affective processes, prosody and language use, and daily life cultural beliefs and practices that make possible inferences and re-descriptive claims about phenomena examined. This ability to create levels of analytic independence is itself an extraordinary human achievement and implies that we often rely on cognitive/affective mechanisms and cultural beliefs and practices as resources rather than as topics of inquiry without having to identify how these resources make possible a given level of abstraction or analysis. I propose that the cognitive and cultural resources presupposed in the study and explanation of different levels of analysis have their roots in a process called “representational re-description” (Karmiloff-Smith 1992). The notion of re-description refers to the human ability to synthesize and compress or summarize our thinking; first by producing speech narratives, gestures, bodily and facial movements, and artistic or graphical exemplars, and second by the development and use of calendars, written displays, tools, physical, chemical, and electronic mediums. These re-descriptive devices seek to capture experiences and thoughts that go beyond the limitations of our sensory capacities. Ways of understanding, therefore, include the necessity of producing synthesized or compressed or compact informational resources. The production and use of such resources assume that producers and recipients of such resources are capable of using different cognitive/affective mechanisms, cultural beliefs, forms of language, paralinguistic elements, gestures, bodily and facial movements, and different forms of written texts or artistic or graphical displays. Infants, children, and adults, therefore, can be said to update and acquire new and existing knowledge by “representational re-description” or the capacity to make flexible use of stored knowledge “by iteratively re-representing in different representational formats what its internal representations represent” (Karmiloff-Smith 1992, p. 15). The re-descriptive process requires the cultural representation of overlapping internal and external stimuli (knowledge sources). The paper suggests that the notion of re-description is a general mechanism at the core of how we create, differentiate, and integrate different levels of explanation and analysis. Its observable manifestations will be illustrated throughout the paper. Every level of analysis necessarily creates or imposes a particular temporal bandwidth (milliseconds, seconds, minutes, hours, days, months, years, decades, and centuries) explicitly or implicitly. The idea of different or independent levels of analysis, however, necessarily presupposes individual and collective systems. All levels of analysis depend on identifiable human faculties. For example, flexible intelligence, recursion, and language acquisition (Premack 2004). Each level of analysis, therefore, has developed its own (sometimes overlapping) theoretical perspectives and methods to justify claims about the independent realities each seeks to control, explain, describe, and predict. The insistence on autonomy is strongly pursued despite each level gradually absorbing and relying on elements from neighboring levels. The notion of re-description, therefore, serves as an integrative function, and allows one to understand the relation of one level to another and how to move from one level to another.",15
5.0,1.0,Mind & Society,11 May 2006,https://link.springer.com/article/10.1007/s11299-006-0009-z,"Juliet: If they do see thee, they will murder thee. A satisficing algorithm for pragmatic conditionals",June 2006,Alejandro López-Rousseau,Timothy Ketelaar,,Male,Male,Unknown,Male,"According to Evans (2005), speakers use conditional statements of the form “if P, then Q” to influence the actions and beliefs of listeners. And they do so in a context, by having them imagine the actual possibility of P and the practical consequence of Q, before making a decision on how to act or what to believe. Take his example of an editor telling an author: “if you submit your paper to our journal, we will publish it”. For Evans, this is an instance of a promise, because it strongly encourages the act of submission by the listener, as the reward of publication is controlled by the speaker. More broadly, it is a statement of the speaker meant to induce an action of the listener. Figure 1 shows our taxonomic representation of conditional statements analysed by Evans (2005), including their defining terms and features according to him. It is not an exhaustive taxonomy, as it does not include conditionals not discussed by Evans, such as permissions and obligations, requests and orders, and so on.
 The taxonomic representation of social and communicative conditionals We basically agree with Evans’ (2005) conditionals, except for his distinction of inducement and advice in terms of influence strength. According to him, an inducement is stronger than an advice, because in an inducement the speaker controls the consequent event, whereas in an advice the speaker does not. Take his above example of a promise and compare it to his other example of a colleague telling an author: “If you submit your paper to their journal, they will publish it”. For him, this is an instance of a tip, because it weakly encourages the act of submission by the listener, as the reward of publication is not controlled by the speaker, but by others. We do believe that the speaker’s control of the consequences is the discriminating feature between an inducement and an advice, but we do not believe that this feature makes an inducement necessarily stronger than an advice. Take the example of a modern Juliet telling her Romeo: “if my brothers see you, they will kill you”. According to Evans (2005) and ourselves, this is a warning, because it seeks to deter the act by Romeo, and the punishment is not controlled by Juliet, but by her brothers. However, this warning is stronger, not weaker than the threat of Juliet telling Romeo: “if my brothers see you, I will kill you”. Although now the punishment is controlled by Juliet, not her brothers. Or take a pharmacist’s advice: “if you take this pill, it will calm you”. And compare it to a pharmacist’s promise: “if you take this pill, I will calm you”. Evidently, an inducement is not necessarily stronger than an advice. In fact, it is context that determines the strength of a conditional. For example, a medical warning is stronger when made by an expert than a novice doctor.",9
5.0,1.0,Mind & Society,10 May 2006,https://link.springer.com/article/10.1007/s11299-006-0010-6,Against causal descriptivism,June 2006,Panu Raatikainen,,,Male,Unknown,Unknown,Male,"The new theory of reference (due to Kripke, Donnellan, Putnam and others) maintains, against the more traditional descriptivist views on meaning, that the meaning of an expression is not in general determined by a description (or a cluster of descriptions) speakers associate with the expression. More generally, it argues that meaning is not determined only by what is internal to the speakers’ mind, but that the social and physical environment may also play some role in it. It is assumed here that meaning, whatever else it may be, determines the reference, or the extension, of an expression. In its classical form, descriptivism now has apparently few adherents. There is, however, a new form of descriptivism, which hopes to restore the old order. Its ingenious idea is, so to say, to mimic the new causal-historical theory of reference in its descriptions; hence its name “causal descriptivism”. Such a form of descriptivism has been suggested by David Lewis, Frederick Kroon, and Frank Jackson (see Lewis 1984; Kroon 1987; Jackson 1998), for example, and seems to enjoy some popularity. In its simplest form, causal descriptivism submits that speakers associate with, say, “Einstein”, a description along the lines of The entity that caused my current use of “Einstein”. More generally, one may express the basic idea of causal descriptivism schematically as follows: speakers associate with a name “N” a description of the form The entity standing in relation R to my current use of the name “N”, and this description determines the reference of “N”. The relation R here is drawn from the rival non-descriptivist (e.g., causal-historical) theory of reference. There is also another somewhat related recent variant of descriptivism, favored for example by Searle (1983), Bach (1987) and Katz (1990, 1994), called nominal descriptivism or metalinguistic descriptivism.Footnote 1 According to it, a description, which suits the purpose, for a proper name “N”, is of the form The thing to which “N” refers. At first sight, causal descriptivism (or its relative nominal descriptivism) may appear appealing, but I shall argue that upon closer scrutiny, it turns out to be quite problematic (see also Devitt and Sterelny 1999; Soames 2001).",5
5.0,1.0,Mind & Society,03 May 2006,https://link.springer.com/article/10.1007/s11299-006-0011-5,Emotional consensus in group decision making,June 2006,Paul Thagard,Fred W. Kroon,,Male,Male,Unknown,Male,"How do you and your friends decide what movies to attend together? Do you create a chart of the movies available, each rate them numerically, and then sum the ratings to make the group decision? Or do you discuss the available movies until everyone has a good feeling about the best movie to go to? Similarly, if you are in a North American or other university department that does its own faculty hiring, how does your department proceed? Does it use either of the following procedures? Procedure A: jointly produce a rating of candidates on a scale of 1–10 for their potential contributions to research, teaching, and administration; jointly agree on a weighting of the importance of research, teaching, and administration; multiply the ratings time the weightings to produce a score for each candidate, and make an offer to the first one. Procedure B: argue heatedly about the strengths and weaknesses of different candidates and the value of different kinds of research and teaching; gradually reach some consensus, or at least take a vote on which candidate should be made an offer; try to mollify those who did not get the candidate they wanted. Procedure A sounds rational, but we have never heard of it being conducted, whereas something like procedure B is very common. Movie going and academic hiring are just two examples of the pervasive phenomenon of group decision making, which abounds in organizations as diverse as families, businesses, juries, and political parties. Unless there is an autocrat empowered to make decisions for the whole group, a joint decision will often require extensive discussion and negotiation before a consensus or majority opinion is reached. In ideal cases, the discussion produces sufficient communication that all members of the group come to share the same view of what to do, for example all agreeing about which job candidate is the best. Group decisions are often highly emotional. In academic hiring, different members of a department are usually enthusiastic about some candidates and scornful about others. These emotional attitudes partly reflect the records of the candidates, but also reflect the values of the professors doing the hiring, who often have strong opinions about the value of particular aspects of academic work, for example teaching versus research, or research on some topics rather than others. It is not coincidental that professors tend to most admire the kinds of work that they know best, so conflict arises in a department because everyone wants to hire replicates of themselves. Ideally, conflict is resolved by acquiring and exchanging sufficient information that the members of the department arrive at similar emotional reactions toward the different candidates. This paper presents a theory and computational model of group decision making understood as emotional consensus. It proposes that individual decision making is inherently emotional, and that group decision making requires communication of emotional as well as factual information. In the ideal case, sufficient communication of both facts and emotions will lead to cognitive/emotional consensus about what to do, a consensus that consists of at least partial convergence of both beliefs and emotional values. After reviewing the emotional character of individual decision making, we describe a set of mechanisms for transmitting emotional values between people. Some of these mechanisms are implemented in a computational model that simulates how emotional communication can lead to consensus in group decisions.",19
5.0,1.0,Mind & Society,12 May 2006,https://link.springer.com/article/10.1007/s11299-006-0012-4,Subjective measures of unconscious knowledge of concepts,June 2006,Eleni Ziori,Zoltán Dienes,,Female,Male,Unknown,Mix,,
5.0,2.0,Mind & Society,08 August 2006,https://link.springer.com/article/10.1007/s11299-006-0013-3,Caring about framing effects,November 2006,Amber N. Bloomfield,Josh A. Sager,Douglas L. Medin,Female,Male,Male,Mix,,
5.0,2.0,Mind & Society,06 September 2006,https://link.springer.com/article/10.1007/s11299-006-0016-0,Internalism about truth,November 2006,Wolfram Hinzen,,,Male,Unknown,Unknown,Male,"In philosophical “naturalism” as emanating from W. v. Quine, “naturalizing” semantics has meant to account for paradigmatically semantic features of expressions in terms of some or other of their non-semantic features. This presupposes some prior metaphysical distinction between what has to be ruled in and out of an approach qualifying as “naturalistic”. Simultaneously with this naturalization project, the biolinguistic program (Lenneberg 1967; Chomsky 1995; Uriagereka 1998; Jenkins 2000) evolved, equally a self-proclaimed “naturalistic” approach to the human faculty of language. Here, though, the naturalism at work is methodological only, rather than metaphysical (see Chomsky 2000). As a consequence of this, a form of “mentalism” has survived virtually unchallenged in nearly all work in the latter tradition, while mostly rated as inconsistent with naturalism in the former. This mentalism is not principled—there is no commitment, in particular, to the mental’s being “non-physical”. Still, there is not a commitment to its being “physical” here either. Historically, the first program continued the behaviourist tradition of B. F. Skinner, which aimed to deprive mental categories from playing an explanatory role in human behaviour (see Hinzen 2006c, for qualifications). The biolinguistic and Chomskyan program, by contrast, was not only a sharp departure from behaviourism, but also from the externalism that defined it. Assuming that the internal cognitive structures that support our language competence are not a function of what happens to us in ontogeny, it adopts an internalist methodology, which sticks to the bounds of the organism prior to tackling the wider task of understanding the organism’s embedding in a physical and social environment. Insofar as human meaning patterns with form—or insofar as generative procedures jointly with a mental lexicon determine interpretively relevant properties of linguistic structures—we are dealing with an internalist approach to human linguistic meaning here as well (see Chomsky 2000; Hinzen 2006a, and some examples given below). In a nutshell, whatever meaning is, it does not supervene on relations of reference; rather, how we use language for purposes such as reference depends on what structures our language faculty provides us with. If the naturalization of meaning proceeds on an internalist path, could the naturalization of truth so proceed, too? In principle, it seems that although we are of course embedded in an environment, function in it and talk about it, acts of ascribing truth to a sentence need no more than its meaning be explainable by appeal to these facts: these may follow, rather, from us being creatures endowed with a notion of truth. And even though there is the familiar intuition that what somebody said or thought is true if and only if it agrees with reality, this needs to tell us nothing about the origin of the concept of truth that figures in this statement, or explain our having of it. This is the hypothesis that this paper explores. In general, an externalist strategy in biological explanation would explain a feature of some organism by appeal to its relations to an environment and the forces the latter exerts on it (see, e.g., Godfrey-Smith 1996). Consider concretely the naturalization of meaning as pursued in the first program mentioned above. There, in essence, it is carried out via the notion of reference, which we may regard as a naturalization of meaning, if (i) the latter is exhausted or relevantly determined by reference, and (ii) reference is naturalistically relatively unproblematic, two assumptions widely held in philosophy (though, again, unambiguously rejected in the biolinguistic program, see again Chomsky 2000; Hinzen 2006a). The naturalization of meaning in that case has an externalist character, since mind-world (or language-world) relations (of a causal or functional sort) are thought to determine meaning. Proceeding on this externalist course, how would one bring the truth-theoretic (rather than reference-theoretic) properties of a linguistic expression into the scope of a naturalistic paradigm of explanation thus understood? At first, this may not seem particularly problematic. One natural strategy would be to try reducing truth to reference (or generalizing the latter notion to some notion of “satisfaction”). After all, reference is the prime relational notion, and truth is usually supposed to be paradigmatically relational as well, involving the world essentially. Reference of course is intuitively defined only for names (or nominals); nonetheless, the suggestion would be that not only the meaning of nominals reduces to reference, but the meaning of sentences/propositions does, too (though the referents would be different, presumably). However, I will suggest not only that this does not seem to be a promising strategy, but also that there does not appear to be a good candidate for another relational notion (such as representation, correspondence, or functioning isomorphism), which can take the place of reference. In fact, I have argued (Hinzen 2006a; 2006b) that reference-based paradigms of naturalization are not the most promising strategy in the case of nominals either, which today seems as problematic as ever. Maybe then we should abandon a relational concept of truth, and pursue the understanding of human truth on an internalist path. Note throughout that “internalism” will here be understood as an alternative paradigm and strategy of biological explanation, and other philosophical associations connected with this term should be kept strictly away.Footnote 1
 An explanation of some aspect of an organism is internalist, if it does not make reference to the embedding of the organism in an external environment. Rather, the possession of the trait—the organism’s internal structure—will explain (or contribute to an explanation of) how the organism will, given its structure, be embedded in its environment and function within it. Internalist explanations are generally non-functionalist ones (in the relevant teleonomic/teleosemantic sense of function), in that, on this approach, functions are taken to be things to be explained by internal structures, rather than things that do the explaining. Biological internalism has been vigorously defended both in the Nineteenth century and in our own time, though it got into eclipse in the course of the formation of the Neo-Darwinian Synthesis (see Gould 2002 for an extensive discussion and references, and see Sect. 7). Humans are unique in the living world in distinguishing truth from falsehood: we, and any one of us, pending severe cognitive disabilities, are creatures pondering, judging, and valuing the truth, and we may regard this as one of our biological traits. It might be said that, from Aristotle to Barwise, truth has been the prime relational notion: truth is something external to which human thought is answerable. Still, and again, it could be our possession of a (or rather the) concept of truth, as a species-specific element figuring in our cognitive economy, that explains how we act in making judgements involving it: having that concept, we are enabled to think of certain of the things we cognitively represent as true or false. In philosophical discussions, concepts are often regarded not only as learned or externally induced, but also as something essentially “linguistic” and socially shared. Thus, e.g., concepts might be said to only arise from linguistic interaction; from forming beliefs; from relating to the world; or they are held to be nothing that is only or exclusively in the head. On such grounds, the option just canvassed might be ruled out from the start. But it should not be, since empirical studies in developmental psychology, as well as theoretical arguments (Fodor 1998, 2004), suggest the alternative possibility that humans have knowledge of concepts whose possession is a presupposition for language learning rather than a result of it (Bloom 2000; Gleitman and Gleitman 1997; Carey 2006; Hauser and Spelke 2004). I will, on the basis of this line of research, take it that it is certainly possible that much like our notions of object, number, or cause, our notion of truth may be a shared species-property as well, being part of our intrinsic mental organization. A defender of a relational conception of truth would by contrast argue that truth has essentially to do with how language or the mind relate to the world, and is also explained by such relations. Suppose for concreteness that that relation is representation. Then the idea (some would say: the truism) would be that a sentence or proposition represents the world as being a certain way, and if (and only if) the world is that way, the sentence or proposition is true. Truth is intrinsically relational in this way; a concept of truth, if there is one, is this very relation, or is abstracted from it; and the concept does not need to be appealed to in the understanding of it. Perhaps it is thought: something that of its nature crosses the organism-boundary cannot be due to our mental organization. Unsurprisingly, even friends of internalism concede a defeat right here: truth, say Piattelli-Palmarini and Cecchetto (1997, p. 452), “cannot be easily integrated into the internalist framework” in linguistic theory (in which they think nominal reference can be more easily so integrated). But the putative argument just sketched is a fallacy. Even if truth as such was intrinsically relational, it simply would not follow that our human notion or concept of truth is intrinsically relational, or has an externalist explanation: the truth-relation might come about as a consequence rather than be a cause of the truth-concept. This would be in the same sense in which infant studies suggest that our concept of a cause is nothing we find out about empirically, by observing “constant conjunctions” in a Humean fashion, or that supervenes on external relationships, but is a conceptual primitive that is independent of observation and a presupposition for experiment and scientific inquiry (see Carey 2006; and see Lowe 2006). While I appreciate that the existence of protons may not depend on the concept of a proton, I see less reason to believe that there was truth in the world prior to there being the concept of truth (though maybe there were facts).Footnote 2 This concept did not arise from our relating to the world somehow, if, in particular, the human concept of truth depends on the existence of propositions, these being what is true or false. For, propositions are thought of as being what is expressed by sentences; and sentences, as Carstairs-McCarthy (1999) has made plausible (and see Hinzen 2004, 2006b, c), were no evolutionary necessity. In evolution, a language with a syntax of the human style need not have arisen; instead a variant might have evolved that did not contain any sentences (but only nominals, say, or perhaps no categories at all), while still serving well enough to communicate. I see no reason to assert that in that case, where there would have been no sentences, there would have been propositions; and hence that there would have been truth. On this line of reasoning, truth cannot predate the human sentence. No sentences, no propositions; no propositions, no truth; and sentences are possibly an evolutionary accident. The point, then, when we want to explain truth, is to get the human sentence; and from relating to the world somehow (causally, teleologically, etc.), or merely from some increase in complexity, the structural specifics of human language will not simply spring into existence. So much for an opening shot against the standard relational picture: we simply do not expect truth to be intrinsically relational, as it depends on structural specifics that more likely than not have themselves no externalist rationale (see further Sect. 6). It is thus good to find that truth does indeed not seem to have that feature, which is what I now undertake to show.",2
5.0,2.0,Mind & Society,25 July 2006,https://link.springer.com/article/10.1007/s11299-006-0014-2,Symposium on “Cognition and Rationality: Part I”,November 2006,Massimiliano Carrara,Paolo Cherubini,Pierdaniele Giaretta,Male,Male,Unknown,Male,,1
5.0,2.0,Mind & Society,06 September 2006,https://link.springer.com/article/10.1007/s11299-006-0015-1,"Symposium on ‘‘Cognition and Rationality: Part I’’ Relationships between rational decisions, human motives, and emotions",November 2006,Cristiano Castelfranchi,Francesca Giardini,Francesca Marzo,Male,Female,Female,Mix,,
5.0,2.0,Mind & Society,09 August 2006,https://link.springer.com/article/10.1007/s11299-006-0017-z,Symposium on “Cognition and Rationality: Part I” Minimal rationality,November 2006,Isaac Levi,,,Male,Unknown,Unknown,Male,,
5.0,2.0,Mind & Society,08 August 2006,https://link.springer.com/article/10.1007/s11299-006-0018-y,Symposium on “Cognition and Rationality: Part I” The rationality of scientific discovery: abductive reasoning and epistemic mediators,November 2006,Lorenzo Magnani,,,Male,Unknown,Unknown,Male,"Creativity is certainly an important aspect of our definition of “intelligence” but the literature associates many different notions to creativity. This ambiguity has brought to a lack of consensus in the research community. The common views associate to creativity unusual and mysterious qualities that drive the concept of creativity to a confused verbosity. Statements like “to break the rules”, “to think different”, “to destroy one Gestalt in favour of a better one”, and “to arrange old elements into a new form”, present in the field of psychological research on creativity since 1950s, certainly do not clarify the topic, and seem to lead to the Freudian conclusion that creativity cannot be understood. This conclusion has also been supported by many philosophers who studied conceptual change in science during the second half of the last century. They distinguished between a logic of discovery and a logic of justification (i.e. between the psychological side of creation and the logic argument of proving new discovered ideas by facts). The consequent conclusion was that a logic of discovery (and a rational model of discovery) could not exist: scientific conceptual change is cataclysmic and irrational, dramatic, incomprehensible and discontinuous. Many other studies already argued that creativity can be understood (Boden 1991; Sternberg 1999; Sternberg et al. 2002), but paid attention mainly to the psychological and experimental aspects, disregarding the philosophical, logical, and computation ones. In AI research, however, since Simon, two characteristics seem to be associated to creativity: the novelty of the product and the unconventionality of the process that leads to the new product. Hence, in a strictly pragmatic sense, when we can clarify what behaviour we are looking for, we could implement it in a machine: a methodological criterion enables us to define and consider just those practical effects we conceive to be associated with novelty and unconventionality (Buchanan 2001). I maintain we can overcome many of the difficulties of creativity studies developing a theory of abduction, in the light of Charles Sanders Peirce’s first insights. On the other hand, philosophers of science in the twentieth century, following the revolutionary theory developed by Kuhn (1962),Footnote 1 have traditionally distinguished between the logic of discovery and the logic of justification. Most have concluded that no logic of discovery exists and, moreover, that a “rational” model of discovery is impossible. In short, scientific creative reasoning should be non rational or irrational and there is no reasoning to hypotheses. In all these descriptions, the problem is that the definition of concepts like “creativity” and “discovery” is a priori. Following Peirce, the definitions of concepts of this sort have not usually rested upon any observed facts, at least not in any great degree; even if sometimes these beliefs are in harmony with natural causes. They have been chiefly adopted because their fundamental propositions seemed “agreeable to reason”. That is, we find ourselves inclined to believe them. Usually this frame leads to a proliferating verbosity, in which theories are often incomprehensible and bring to some foresight just by intuition. But a theory which needs intuition to determine what it predicts has poor explanatory power. It just “makes of inquiry something similar to the development of taste” (Peirce 1986, p. 254). A suggestion that can help to solve the enigma of discovery and creativity comes from the “computational turn” developed in the last years. Recent computational philosophy research in the field of cognitive science make use of tools able to give up those puzzling speculative problems, or, at least, to redefine them in a strict pragmatic sense. In fact, taking advantage of modern tools of logic, artificial intelligence, and other cognitive sciences, computational philosophy permits to construct actual models of the studied processes. It is an interesting constructive rational alternative that, disregarding the most abstract level of philosophical analysis can offer clear and testable architectures of creative processes.",3
5.0,2.0,Mind & Society,30 August 2006,https://link.springer.com/article/10.1007/s11299-006-0019-x,Symposium on “Cognition and Rationality: Part I” Relevance effects in reasoning,November 2006,Jean-Baptiste Van der Henst,,,Unknown,Unknown,Unknown,Unknown,,
6.0,1.0,Mind & Society,19 January 2007,https://link.springer.com/article/10.1007/s11299-006-0020-4,Bounded awareness: what you fail to see can hurt you,June 2007,Dolly Chugh,Max H. Bazerman,,Female,Male,Unknown,Mix,,
6.0,1.0,Mind & Society,06 February 2007,https://link.springer.com/article/10.1007/s11299-006-0021-3,The connectionist self in action,June 2007,David DeMoss,,,Male,Unknown,Unknown,Male,"You are a self, a self-reflective agent who acts freely for reasons. You are a brain, an interconnected maze of neurons navigating its body in a complex environment. Your self is your brain; that is, a standard human brain properly wired to a system of flesh and blood is a self, or at least it makes itself a self given time and experience. The brain is a connectionist system, a parallel distributed processor. It exchanges outputs for inputs via networks of neurons that function as vector transformers converting one neuronal activation pattern into another. What this brain’s connectionist system is able to do is very impressive. It can act freely and self-reflectively for reasons. Here is my thesis: the connectionist brain with its abilities to categorise, to develop goal-directed dispositions, to problem-solve what it should do, and to second-order reflect has the capacity to become a free, rational, moral, agent—that is, the capacity to become a self; it becomes a self by engaging second-order reflection in the hermeneutical task of constructing narratives that rationalise action. The first half of this thesis relies for its defense on other essays of mine (see DeMoss 1999, 2001) that explain some of the abilities a brain would have if treated as a connectionist system; in Sect. 2 below, I provide a brief summary of this background. The second half, the claim that the connectionist brain constitutes a self by constructing rational narratives, is the focus of the present essay. In Sect. 3, I elaborate and argue for this thesis. And then in Sect. 4, I use John Searle’s recent writings on rationality in action as a foil to clarify and defend my thesis, arguing against his claim that an explanation of behavior citing the agent’s reasons “requires us to postulate an irreducible self” (Searle 2000, p. 8), and replying to the objection that the connectionist self cannot be free. I will be responding to Searle (2000) and Searle et al. (2001). Both essays offer material also discussed in Searle (2001).",
6.0,1.0,Mind & Society,23 January 2007,https://link.springer.com/article/10.1007/s11299-006-0023-1,Symposium on “Cognition and Rationality: Part II”,June 2007,Massimiliano Carrara,Paolo Cherubini,Pierdaniele Giaretta,Male,Male,Unknown,Male,,
6.0,1.0,Mind & Society,15 February 2007,https://link.springer.com/article/10.1007/s11299-006-0024-0,The “vanishing” of the disjunction effect by sensible procrastination,June 2007,Maria Bagassi,Laura Macchi,,Female,Female,Unknown,Female,"Tversky and Shafir (1992) studied the decision processes in choice under uncertainty. When thinking under uncertainty, people often do not consider appropriately each of the relevant branches of a decision tree, as required by consequentialism. In a more formal way, we could say that decision makers prefer option A (versus B) when they are in a certain condition and they also prefer A when they are not in that condition, but they refuse A (or prefer B) when they don’t know in which condition they are. This pattern of preference clearly violates Savage’s (1954) sure-thing principle, and Tversky and Shafir called it the disjunction effect. According to sure-thing principle: if x is preferred to y knowing that event A occurred, and if x is preferred to y knowing that A did not occurr, then x should be preferred to y even when it is not known whether A occurred. The sure-thing principle (Expected Utility Theory) is not able to explain this phenomenon, given that it treats the choices as dependent on their anticipated consequences. According to the theory, each alternative is associated with an expected value and chosen according to the principle of value maximization. An alternative explanation to formal modeling is the reason-based analysis of Tversky and Shafir. The authors attribute this pattern of preference to a lack of clear reasons for accepting an option (A) when the subjects are in the disjunctive condition. The authors speak about a sort of “loss of acuity induced by the presence of the uncertainty. Once the outcome of the exam is known the student has good reasons for taking the trip: if the student has passed the exam, the vacation is presumably seen as a reward; if the student has failed the exam, the vacation becomes a consolation” (Tversky and Shafir 1992, p. 306). The phenomenon of the disjunction effect was studied using several scenario problems (Bastardi and Shafir 1998, 2000; Croson 1999; Kühberger et al. 2001; Shafir and Tversky 1992; Tversky and Shafir 1992; Shafir et al. 1993; Shafir 1994). In the present research we consider the well-known Hawaii problem (while the gamble problem was treated in Bagassi and Macchi 2006a). Imagine that you have just taken a tough qualifying examination. It is the end of the fall quarter, you feel tired and run-down, and you are not sure that you passed the exam. In case you failed you have to take the exam again in a couple of months—after the Christmas holidays. You now have an opportunity to buy a very attractive 5-day Christmas vacation package to HawaiiFootnote 1 at an exceptionally low price. The special offer expires tomorrow, while the exam grade will not be available until the following day. Would you: buy the vacation package; not buy the vacation package; pay a $5 non-refundable fee in order to retain the rights to buy the vacation package at the same exceptional price the day after tomorrow—after you find out whether or not you passed the exam. Imagine that you have just taken a tough qualifying examination. It is the end of the fall quarter, you feel tired and run-down, and you find out that you passed the exam (failed the exam. You will have to take it again in a couple of month—after the Christmas holidays). You now have an opportunity to buy a very attractive 5-day Christmas vacation package to Hawaii at an exceptionally low price. The special offer expires tomorrow. Would you: buy the vacation package; not buy the vacation package; pay a $5 non-refundable fee in order to retain the rights to buy the vacation package at the same exceptional price the day after tomorrow. In the Hawaii problem, more than half of the subjects who know the outcome of the exam (54% in the passed condition and 57% in the fail condition) choose option x—buy the vacation package, but only 32% do it in the uncertain condition of not knowing the outcome of the exam. This is a crucial demonstration that Tversky and Shafir produced to show the disjunction effect. Here, as well as in the gamble problem, decision makers prefer option x (to buy the vacation package) when they are in a certain condition (passed exam) and they also prefer x when they are not in that condition (failed exam), but they refuse x (or prefer z) when they don’t know which condition they are in (they don’t know the outcome of the exam).",5
6.0,1.0,Mind & Society,19 January 2007,https://link.springer.com/article/10.1007/s11299-006-0025-z,"The psychology of dynamic probability judgment: order effect, normative theories, and experimental methodology",June 2007,Jean Baratgin,Guy Politzer,,Male,Male,Unknown,Male,"One criterion to classify the literature devoted to the psychological study of probabilistic judgement and decision-making is the use of a theoretical model establishing a referential norm for “rational” behaviour. One type of studies (call them “type A”) compares participants’ responses to an expected result prescribed by a normative model. Typical normative models are Subjective Expected Utility Theory in decision theory, or the Bayesian model in the context of probabilistic judgement. The main object of the studies of type A is to describe and explain the processes at play in human probabilistic judgement. This line of research examines people’s rationality through their judgements and decisions as departure from a norm. A second type of studies (call them type B) also attempts to analyse the processes by which individuals make probabilistic judgements. However, instead of taking into account a prescriptive model, these studies rely exclusively on descriptive models. Studies of type B strive to maximize the degree of fit of the models used to describe probabilistic judgement. Typically, they attempt to answer the following questions: did bankers’ forecast of monetary policy turn out to be true, does it rain when the meteorologist predicts rain? This distinction is classic, and can be found in many publications (see for example, Funder 1987; Hammond et al. 1980; Kruglanski and Ajzen 1983). Whereas dynamic probability judgment, that is, revision of degrees of belief, has been the object of a majority of studies of type B, surprisingly the dynamic coherence of degrees of belief has hardly been addressed by studies of type A. In this paper, our goal is to examine how it is possible to study the revision of degrees of belief in the context of type A research work. We will first of all recall the notion of coherence applied to the revision of degrees of belief in the typical framework used in the literature of type A, that is to say, the “Bayesian model”. We will assess the theoretical limits and consequences of this model to study the revision of degrees of belief. The importance of considering other normative rules than Bayes’ rule in the situations where new messages provoke a complete revision of the knowledge base will be underlined. Secondly, we will analyse the main results of the experimental literature of type A following the methodological framework discussed in Baratgin and Politzer (2006) to study the revision of degrees of belief; this framework is based on elementary qualitative properties of the normative rules used in studies of type A. In particular, the apparent order effect phenomenon will be confronted with the commutative property of Bayes’ rule. Besides, we will define a new effect in relation with the revision of degrees of belief that we call the apparent redundancy effect, and confront it with the idempotence property of Bayes’ rule. Finally, we will reinterpret results obtained in this kind of experimental situations in the light of pragmatic analysis.",18
6.0,1.0,Mind & Society,09 February 2007,https://link.springer.com/article/10.1007/s11299-006-0026-y,How the construction of mental models improves learning,June 2007,Monica Bucciarelli,,,Female,Unknown,Unknown,Female,"Learning can be concerned with the acquisition of new knowledge, thus with declarative knowledge, or with being able to improve our own performances, thus with procedural knowledge (Bara 1995). Declarative knowledge is a theory of the world, conceived of as a set of all the conceptual entities describing, in the form of propositions, classes of objects (e.g., theater), relationships (e.g., people are in the theater), processes (e.g., the play), behavioural norms (e.g., one does not smoke in the theater), etc. It does correspond to knowing that, and comprises what may be expressed in words about the object or about the state of affairs at issue. Declarative knowledge represents what people believe themselves to know about any given entity in the world, and it is, in principle, conscious; it can be expressed verbally, and it may be reflected upon voluntarily. It is transparent, in the sense that it can be read by the rest of the system’s procedures, and if activated intentionally it may become conscious. For example, people familiar with the definition of “mammal” are also aware that they know the definition if they focus their attention on it. All our educational establishments and a large part of professional training are centered on declarative knowledge, even if this has greater weight in the training of, for example, a lawyer than in that of a pianist. Procedural knowledge refers to the knowledge that a system possesses and that enables it to interact effectively with the world, even though such information is not represented in an explicit fashion so as to be directly readable by other parts of the system. It does correspond to knowing how to act. An intentional change requires the existence of an agent executing a clearly defined sequence of operations in order to generate the desired effect. An example of this type of change is giving an invited lecture, where the preconditions consist in there being a conference, the agent is an invited guest, the operations to be executed are those that permit the guest to go to the conference, and the effect is that among the invited speakers at the conference there be the agent himself. This part of procedural knowledge is transparent because the system has free access to such knowledge. A large part of procedural knowledge is, instead, constituted by opaque procedures—methods of acting that are triggered automatically, without requiring control or attention. Typical examples are riding a bicycle or solving a problem—all things that an expert knows well and can do perfectly, but which can only be translated into verbal terms with gross approximations. Opaque procedures operate unconsciously and their reconstruction can take place only a posteriori, in an incomplete and arbitrary manner. Summing up, procedural knowledge corresponds to knowing how to act in a given situation, living the situation from within, and not standing outside of it to speak about it or to reflect on it. In practice, the distinction between declarative and procedural knowledge is less clear cut than in the definition. And indeed, their relationship is complex and hard to define, corresponding broadly to the relationship between experience and symbol. To infer the explicit knowledge equivalent of a piece of tacit knowledge corresponds to building a theory of a phenomenon, with the proviso that the phenomenon is introspective instead of pertaining to the external world. Just as a propositional theory may be constructed starting from tacit knowledge, so may procedural knowledge be built from explicit knowledge. For example, I can read a book on strategies in reasoning and then do my utmost to apply those principles to the way I solve reasoning problems, trying to keep the procedures thus generated under strict control until they are efficiently stored and coordinated. In this paper, I will focus on two specific sorts of learning: learning from a discourse (declarative knowledge) and learning to reason (procedural knowledge). In my proposal, deep learning involves the construction and manipulation of models, which are mental processes postulated by mental model theory (MMT; Johnson-Laird 1983, Johnson-Laird and Byrne 1991). MMT claims that individuals construct models when they have to draw inferences. In particular, it is assumed that people construct a model for each sentence, integrate such models, and consider what if anything follows. Eventually, when they behave rationally, people look for a model that falsifies the conclusion initially drawn in order to draw a new necessary conclusion, valid in all the integrated models of the premises. Consider, for example, the sentence “The circle is on the left of the triangle”. An analogical representation could be the following one:   where there are tokens for the elements in the sentence, and they are in the same relation as the elements in the sentence. Now consider the description “the star is on the right of the triangle” and the respective model:  The integration of the two representations through the overlapping of the common elements produces the following integrated mental model:   from which it is possible to extract information that was no explicit in the sentences, and it is possible to infer that “the circle is on the left of the star”, or “the star is on the right of the circle”. As there are no alternative integrated models in which the conclusion is falsified, the conclusion itself is valid. When it is possible to construct alternative integrated models, the conclusion has to be supported by all of them. If no such conclusion exists, then “no valid conclusion” follows from the sentences. The example concerns the domain of relational reasoning, but the mental model theory explains and predicts, on the basis of the same assumptions, the performance of individuals of different ages with different inferential tasks (see, for a review, García-Madruga et al. 2000; Johnson-Laird 2001). The construction of models is effortful with respect to the encoding of the same information through propositional representations (i.e., simple verbal comprehension), but it recompenses in terms of possibility to draw inferences. As it should be clear by the example above, reasoning means “going beyond” what is explicitly asserted by some premises. This occurs within the constraints posited by necessity in deductive reasoning (i.e., the conclusion must hold in all the models of the premises) and within the constraints posited by plausibility in everyday reasoning (i.e., the conclusion must hold in at least one model of the premises). In particular, in discourse comprehension, through the construction of models the listener can infer implicit information that is possibly, but not necessarily, true. The listener establishes the relationships and the details that are missing in the set of propositional representations that constitute the discourse, whose explicit content is usually a scheme of facts. Thus, for example, given the sentences “It’s raining” and “Claudia has no umbrella”, we might derive a series of plausible inferences, such as “She buys an umbrella”, or “She decides to stay home”. I will argue that deep learning involves drawing inferences by means of the construction and manipulation of models; the learner has to give sense to a series of information within a coherent system of meanings and hence construct models from which inferences can be drawn. Biggs (2001) claims that students’ approaches to learning depends upon their perceptions of the context, their own goals, feelings of self-efficacy, and so on. In this sense, it is inappropriate to categorize students as surface or deep learners. However, it is possible to classify in these terms each specific approach to learning. Thus, Biggs proposes that some approaches involve low cognitive level activities (e.g., memorize, identify), some approaches involve intermediate level activities (e.g., comprehend main ideas, relate), and others high level activities (e.g., apply to far domains, reflect). Within the theoretical framework, I propose that high-level activities consist of the construction and manipulation of mental models.",18
6.0,1.0,Mind & Society,15 February 2007,https://link.springer.com/article/10.1007/s11299-006-0022-2,"Verisimilitude, cross classification and prediction logic. Approaching the statistical truth by falsified qualitative theories",June 2007,Roberto Festa,,,Male,Unknown,Unknown,Male,"The methodological problem of the evaluation of falsified theories has been systematically analysed in philosophy of science, starting at least from Lakatos‘ methodology of scientific research programs.
Footnote 1 After 1974, it has been investigated also within the post-Popperian approaches to verisimilitude. Although such approaches have dealt extensively with qualitative theories, it seems to us that the problem of the verisimilitude of theories stated in first order languages with two or more families of predicates—henceforth, Q-theories—deserves further attention. In fact, the role of Q-theories for social sciences, and some natural sciences, seems to be much more important than philosophers of science, including those concerned with verisimilitude, are ready to concede. In this paper we are mainly concerned with the circumstance—suggested by some results obtained within the statistical analysis of cross classification—that Q-theories can be used to describe the statistical structure of cross classified populations, and that their performance in this task can be precisely defined and evaluated. We will argue that the notion of verisimilitude may provide an appropriate tool for measuring the statistical adequacy of Q-theories. First of all, we will give a short outline of the post-Popperian approaches to verisimilitude and of the related verisimilitudinarian non-falsificationist methodologies—henceforth, VNF-methodologies (Sect. 2). Secondly, we will explicate the notion of Q-theory and we will define suitable measures of the qualitative verisimilitude of Q-theories (Sect. 3). Afterwards, in Sect. 4, we will introduce appropriate measures for the statistical verisimilitude of Q-theories. This will allow us to give a clear formulation of the intuitive idea that the statistical truth about cross classified populations can be approached by falsified Q-theories. Finally, in Sect. 5, we will argue that some basic intuitions underlying VNF-methodologies are shared by the so-called prediction logic, developed by the statisticians and social scientists David K. Hildebrand, James D. Laing and Howard Rosenthal in a series of papers published between 1974 and 1977, which culminated in their not yet adequately appreciated book Prediction Analysis of Cross Classification (1976b).
Footnote 2 Moreover, we will show that our measures of the statistical verisimilitude of Q-theories include, as a particular case, their notion of predictive success.",6
6.0,2.0,Mind & Society,22 February 2007,https://link.springer.com/article/10.1007/s11299-007-0027-5,"Social institution, cognition, and survival: a cognitive–social simulation",November 2007,Ron Sun,Isaac Naveh,,Male,Male,Unknown,Male,"Computational social simulation has been seen as constituting a new way of exploring social processes. It has undergone significant growth in recent years. Many theoretical arguments were made in support of its role in social theorizing; see, for example, Gilbert (1995), Moss (1999), Sun (2001), Castelfranchi (2001), and Sun (2006). A particularly pertinent factor is that social simulation can provide support for functionalist explanations of social phenomena. For example, functionalists argued that some specific forms of social structures were functional for society. However, functionalist explanations were often treated as dubious because of the difficulty in verifying such explanations (Gilbert 1995). A particularly important problem with functionalism is that it involves explaining a cause by its effect. It is customary to explain an effect by its cause, and it seems post hoc to explain a cause by its effect. A related problem is that, while focusing on a specific moment in history, it tends to ignore the historical processes leading up to a specific social phenomenon. Social simulation, however, can help to substantiate functionalist explanations, by remedying both of these above two problems. First of all, social simulation focuses on processes, and thus it may help to provide some historical perspectives. For example, Cecconi and Parisi (1998) focused on the evolution of survival strategies in tribal societies. Similarly, Doran et al. (1994) provided explanations for the increasing complexity of tribal societies in the Upper Paleolithic period. Second, the effect of a cause, which is central to functionalist explanations, can be verified through experimentation using computational social simulation. Consequently, with social simulation, this style of explanation can be better verified or validated, and thus may become more convincing. However, a significant shortcoming of current computational social simulation is that most of the work assumes very rudimentary cognition on the part of agents. Although agents are often characterized as being “cognitive”, there have been relatively few attempts to carefully emulate human cognition. Models of agents have frequently been custom-tailored to the task at hand, often amounting to little more than a restricted set of highly domain-specific rules. Although such an approach may be adequate for achieving some limited objectives of some simulations, it is overall unsatisfactory (see the arguments by Sun and Naveh 2004). It not only limits the realism, and hence the applicability, of social simulations, but also precludes the possibility of tackling the question of the micro–macro link in terms of cognitive–social interaction (cf. Sawyer 2003; Alexander et al. 1987). Computational models of cognitive agents incorporating a wide range of cognitive functionalities (such as various types of memory, various modes of learning, and various sensory motor capabilities) have been developed in cognitive science (e.g., Newell 1994; Sun 2002). In cognitive science, they are often known as cognitive architectures. Recent developments in computational modeling of cognitive architectures provide new avenues for precisely specifying complex cognitive processes in tangible ways (e.g., Sun 2002). We have argued elsewhere why such models of cognition can greatly enhance social simulation in general (see, in particular, Sun 2006), which we will not repeat here. To make the same point in a different way, let us look into an existing social simulation as an example. Cecconi and Parisi (1998) created simulated social groups. In these groups, to survive and reproduce, an agent must possess certain resources. A group in which each agent uses only its own resources is said to adopt an individual survival strategy (ISS). However, in some other groups, resources may be transferred from one individual to another. A group in which there is transfer of resources among agents is said to adopt a social survival strategy (SSS). For instance, the “central store” (CS) is a mechanism to which the individuals in a group transfer (part of) their resources. The resources collected by the CS can be redistributed to the members of the group (Cecconi and Parisi 1998). In Cecconi and Parisi (1998), a number of simulations were conducted comparing ISS groups with SSS groups (adopting CS strategies). They used neural networks to model individuals and a genetic algorithm to model evolution. Networks (representing individuals) survive and reproduce differentially based on the quantity of food they are able to consume. Cecconi and Parisi studied how their abilities evolved and also the evolutionary changes in group size. In particular, they explored what conditions determined group survival or extinction. This work is interesting, because it provides a fertile ground for exploring a range of issues, ranging from social institutions to individual behaviors, and from evolution to individual learning, and so on. However, as is, in this work there is very little in the way of cognition by individual agents in their struggle to survive. We believe that investigation, modeling, and simulation of social phenomena need cognitive science (Sun 2001), because such endeavors need a better understanding, and better models, of individual cognition, only on the basis of which better models of aggregate processes can be developed. Cognitive models may provide better grounding for understanding multi-agent phenomena, by incorporating realistic constraints, capabilities, and tendencies of individual agents in terms of their cognitive processes (which were not present in, e.g., Cecconi and Parisi 1998). This point was argued in Sun (2001). This point has also been made, for example, in the context of cognitive realism of game theory (Kahan and Rapaport 1984; Camerer 1997), or in the context of deeper models for addressing human–computer interaction (Gray and Altmann 2001). In Axelrod (1984), it was shown that even adding a cognitive factor as simple as memory of past several events into an agent model can completely alter the dynamics of social interaction (in the iterated prisoner’s dilemma in particular). In this work, we aim to redress the neglect of cognitive science in social simulation, by incorporating more detailed and more realistic models of cognitive processes into social simulation. The specific model that we adopt is the CLARION cognitive architecture (Sun et al. 2001, 2005; Sun 2002, 2003). CLARION has been successful in simulating a variety of psychological tasks.
Footnote 1 Therefore, we are in a good position to extend the effort on CLARION to the capturing of a wide range of social phenomena through integrating cognitive modeling and social simulation. Some preliminary work has already been done (see, e.g., Sun and Naveh 2004). In this work, we revamp the simple (not cognitively realistic) simulation of Cecconi and Parisi (1998). The general setup is similar to that of Cecconi and Parisi (1998). The world is of a limited physical dimension, made up of a two-dimensional grid. Within its physical dimensions, food items and agents are randomly distributed among the tiles. The food crops grow by seasons: Every once in a while, food is replenished among the tiles. There are the harsh, medium, and benign conditions, which are distinguished by the agent-to-food ratios. (Social structures and agent behaviors are expected to be different in these different conditions; more on this later.) Agents are of a limited life span, which varies from individual to individual, depending on the energy level of an agent, the maximum lifespan, and other factors. Agents look for and consume food in an effort to prolong their life spans. As in Cecconi and Parisi (1998), there is a “central store” in some cases. Agents may be required to contribute to the central store in these cases. However, different from Cecconi and Parisi (1998), we introduced some further social institutions. In the case of mandatory contribution to the central store, we introduce penalty for not contributing to the central store. Penalty for violating the norm may be attributed to two possible sources: (1) social sources, such as a tribal enforcement mechanism that extracts penalty from violators, (2) internal sources, for example, from an internal feeling of guilt. Most notably, in our work, different from that of Cecconi and Parisi (1998), agents are more cognitively realistic. They are constructed out of a cognitive architecture, which captures a variety of cognitive processes in a psychologically realistic way in detail (see Sun 2002, 2003). Therefore, we expect our simulations of social survival strategies will shed more light on the role of cognition in determining survival strategies and its interaction with social structures (social institutions) and processes. The reason that we introduce these additional factors is to investigate the interaction between social structures/processes and individual cognition (i.e., the micro–macro link). Through experimentation and rigorous data analysis, we hope to arrive at a more precise and more detailed understanding than the previous simulations. This task is appropriate to our goal of understanding cognitive–social interaction. In a way, social processes may be viewed, at least in part, as processes for distributing and re-distributing power and wealth. This survival task captures such processes in a microcosm. In the remainder of this paper, first, a more realistic cognitive architecture, CLARION, will be described, which, among other things, captures the distinction between explicit and implicit learning. This architecture will then be applied to the survival task simulation. The idea here is mainly to substitute more cognitively sophisticated agents, based on CLARION, for the simpler agents used in Cecconi and Parisi (1998). Analysis of results and discussions will be presented. Some brief concluding remarks will then complete this paper.",19
6.0,2.0,Mind & Society,28 February 2007,https://link.springer.com/article/10.1007/s11299-007-0028-4,A dual process model for cultural differences in thought,November 2007,Hiroshi Yama,Miwa Nishioka,Junichi Taniguchi,Male,Female,Male,Mix,,
6.0,2.0,Mind & Society,26 April 2007,https://link.springer.com/article/10.1007/s11299-007-0031-9,Towards the emergence of meaning processes in computers from Peircean semiotics,November 2007,Antônio Gomes,Ricardo Gudwin,João Queiroz,Unknown,Male,,Mix,,
6.0,2.0,Mind & Society,14 March 2007,https://link.springer.com/article/10.1007/s11299-007-0033-7,Semiosis in cognitive systems: a neural approach to the problem of meaning,November 2007,Eliano Pessa,Graziano Terenzi,,Male,Male,Unknown,Male,"Inquiries into the nature of meaning and reference have been one of the main topics in the philosophy of mind and language since the time of Frege. The fundamental problems to be addressed are invariably the following: what is meaning? What is reference? Are they distinct? And, if it is so, how do they differ? And, how can it happen that some external patterns (like the expression ‘it is raining’) come to signify other external patterns of stimulation (like the fact that ‘rain is the case’) in such a way as to become functional substitutes of the latter in relevant situations? Such issues are of great significance for Cognitive Science, as it appears when we deal with problems such as the symbol grounding problem (Christiansen and Chater 1992; Harnad 1990). In this context, a view that has tacitly gained widespread acceptance is the one according to which the notion of meaning is, under many respects, strictly tied to the notion of content. It would be the case, in fact, that content should be considered as what semantically distinguishes mental representations in general, whereas meaning should be considered as content restricted to linguistic representations: as a consequence of this fact, the notion of meaning would be an instance of the notion of content. Clearly, problems with the notion of meaning automatically stem from problems with the notion of content. In fact: what is for a mental representation to have a content? What is the content of a semantic representation? Is content somehow articulated or is it not? What distinctions, relevant to the notion of content, are to be made? If we take the concept of representation to be a relation involving a set of things which stand for or refer to something else, then we could say that content is a property of representations. Indeed, it is the characteristic semantic property, defining and distinguishing semantic objects properly: anything having some content is a particular semantic object (i.e. representation) different from other semantic objects. According to standard taxonomies (Eliasmith 2000) theories of content (and meaning) fall under one of three groups of proposals: the causal accounts (Dretske 1981; Fodor 1987), the functional role accounts (Harman 1982), and the two-factor accounts (Block 1986; Field 1977). Causal theories claim that content is determined by causal relations that hold between representations (or linguistic expressions) and the external environment. Compatibly with this view, the notion of content, which defines on its own right the representation relation, is conceived as a causal relation, i.e. an external relation holding between the representing items and the represented ones (i.e. referents). According to conceptual role accounts, instead, the content of a representation is determined by its role in the internal conceptual “reticulum” in which it occurs. More precisely, the content of a representation is determined by concepts it causes and is caused by in a cognitive system. The basic motivation is that even when causes are the same, meanings can be different. The problem with such an approach is that internal conceptual roles alone cannot account for truth-conditions (i.e. the connection between representations and their referents in the world) and can be charged with relativism (Fodor and Lepore 1992). The last group of theories, namely the two-factor accounts, comprises proposals that try to overcome the difficulties common to both the aforementioned approaches by suggesting their possible integration. Even though such proposals are successful, at least conceptually, in overcoming many of the aforementioned difficulties, nevertheless they still do not specify how the two factors effectively get stuck together—this is what Eliasmith (2000) calls the alignment problem (Fodor and Lepore 1992)—and do not provide any directions for identifying a possible way out of relativism. According to standard formulations the causal and conceptual factors are two independent aspects of meaning. On the other hand, the main focus of this paper is the problem of understanding semiosis and meaning in cognitive systems. To this aim we argue that a unified two-factor account can be considered as a viable approach to the problem of meaning only if it is framed in terms of a non-standard formulation, according to which both external and internal information are considered as two non-independent aspects of meaning, thus contributing as a whole in determining its nature. In order to overcome the difficulties linked to standard two-factor accounts, we put forward a theoretical scheme that is based on the introduction of suitable categorization and representation spaces endowed with a set of transformations, and we try to show how it can be implemented, when dealing with a single agent, by a neural network architecture. Such a choice is due to the fact that, being architectures of this kind constituted by units which process in a non-linear way the incoming information, a learning procedure based on examples can, in principle, give rise within them to inner representations of the external input patterns in which both the structure of these patterns and the (functional) role they played in the learning process itself are tied together in a non-separable way (Chalmers 1990, 1993). From a mathematical point of view, this circumstance is nothing but a consequence of well known theorems on the approximation abilities of neural networks, such as the one of Hornik et al. (1989). Adopting such a framework, we designed a particular neural architecture for the implementation of categorization and representation spaces, later described within the paper in a more detailed way. The simulations of a learning process in the context of a particular semantic task performed by resorting to this architecture show a strong dependence of performance both on the structure of the representation space and on the functional couplings between the process of symbol categorization and the process of internal symbol transformation. Moreover, a comparative analysis carried out on different instances of this same architecture shows that, notwithstanding their differences, similar representations are developed as a consequence of the fact that the different instances are facing a similar semantic task. Despite the importance of this result, we underline that it can be considered only as a first step towards a full understanding of semiosis and meaning in cognitive systems. Namely the next step requires the introduction of suitable models of social and environmental interaction, so as to account for the emergence of cognitive architectures themselves deputed to perform symbol categorization and transformation. However, we suggest that this step can be implemented by resorting to models of interacting agents, each one being endowed with a neural network architecture similar to the one introduced in this paper. The specific form of the latter, within this enlarged context, would not be introduced only on the basis of computational convenience (as in the case of a single agent), but should emerge as a by-product of a social interaction between cognitive agents. According to this view, the proposal made within this paper appears as very useful for a theory of social construction of meaning, as it provides the latter with its basic building blocks.",2
6.0,2.0,Mind & Society,13 March 2007,https://link.springer.com/article/10.1007/s11299-007-0030-x,Can tacit knowledge fit into a computer model of scientific cognitive processes? The case of biotechnology,November 2007,Andrea Pozzali,,,Female,Unknown,Unknown,Female,"Modern information and communication technologies have surely had a great influence on philosophy. This influence is twofold: on one side, these technologies have changed the way in which philosophers work, introducing them to the use of tools such as computers, e-mail, virtual libraries and, last but not least, the Internet and its huge amount of data. On the other side, information and communication technologies have provided philosophers with a whole new set of problems and issues to reflect upon and have favoured the development of new streams of philosophical research. As a matter of fact, these technologies have contributed to the rising of a whole new field of philosophy: computer ethics, which deals with such ethical problems such as issues of privacy protection in a digital era and of freedom of speech on the Internet, among many others (Stichler and Hauptman 1997). All this considered, it is possible to say that in our days we are probably starting to witness the fulfilment of this old prophecy by Sloman: “I am prepared to go so far as to state that within a few years, if there remain any philosophers who are not familiar with some of the main developments in artificial intelligence, it will be fair to accuse them of professional incompetence, and that to teach courses in philosophy of mind, epistemology, aesthetics, philosophy of science, philosophy of language, ethics, metaphysics, and the other main areas of philosophy, without discussing the relevant aspects of artificial intelligence will be as irresponsible as giving a degree course in physics which includes no quantum theory” (Sloman 1978, p. 5). The growing enthusiasm surrounding this “computational turn in philosophy” should not lead us to think that the diffusion of information and communication technologies brings only positive effects on the activities of philosophers, anyway, as this can in fact have also negative effects, for instance by breeding a certain degree of conformity and by hampering human creativity. The aim of this paper is not to explore the consequences of the diffusion of computer on human creativity, indeed. Rather, I will try to express a critical point of view on the computational turn in philosophy by taking into consideration a very specific field of study: philosophy of science. The paper starts by briefly discussing the main contributions that information and communication technologies have given to the rising of computational philosophy of science, and in particular to the cognitive modelling approach. Then I will try to analyse to what extent this kind of computational models really succeed in giving us a realistic account of scientific thinking. In particular, by focusing on the presence of tacit knowledge in science, I will try to give just a few suggestions for further research. How can we try to represent the process of scientific knowledge development in a computational model, if this process is influenced by tacit knowledge, that cannot by definition be easily codified or formalized? Would it be possible to develop new ways of handling this specific type of knowledge, in order to incorporate it in computational models of scientific thinking? Or should tacit knowledge lead us to other approaches in using computer sciences to model scientific cognition?",2
7.0,1.0,Mind & Society,24 March 2007,https://link.springer.com/article/10.1007/s11299-007-0029-3,The role of cognitive and socio-cognitive conflict in learning to reason,June 2008,Katiuscia Sacco,Monica Bucciarelli,,Female,Female,Unknown,Female,"A long and consolidated tradition, both philosophical and psychological, identifies the heart of human rationality in the ability to draw deductively valid inferences. The surprising finding is that people often fail in reasoning tasks, while in everyday life, they prove to be able to reason efficiently. This contradiction was emphasized by Evans and Over (1996), who opened an ongoing debate about human rationality and by Stanovich and West (2000). The questions underlying the debate are: if humans are indeed rational, why might they fail in inferring logically correct conclusions in reasoning tasks? On one hand, non-logic approaches suggest that people are not rational and that they are systematically biased by what catches their attention most in a given situation, neglecting the ‘logically salient’ data (for a review, see Evans 1982). On the other hand, there are scientists who strongly believe in human rationality. Pragmatic rule theorists, for example, believe that people act according to previous experience: this provides them with a set of domain-dependent rules applicable to the current situation by analogy (Griggs 1983) or with specific schemas for sets of situations (Cheng and Holyoak 1985). However, pragmatic rules only seem to give a description or, at the best, a post hoc explanation of human cognition. The processes which lead people to succeed or fail in reasoning tasks appear to be out of their scope. Other theorists propose that individuals use abstract rules of inference similar to logic calculus (see Braine 1998; Rips 1994). These logical systems, when correctly applied to the premises of an argument, lead to the normatively correct conclusion. However, for these approaches it is not easy to explain the errors people make in reasoning tasks. The mental model theory provides a plausible answer to the question of rationality (Johnson-Laird 1983; Johnson-Laird and Byrne 1991). According to this theory, people can mentally construct models of a state of affairs and manipulate such models in order to make inferences. According to this view, people are rational in theory, in that they are able to falsify their own putative conclusions, however they tend to fail in practice; they possess the basic competence to be rational, but they fail in the executive phase when rationality must be put into action. Given a sufficiently long time, strong motivation and an efficient working memory, they would manage to reach valid conclusions. The theory is able to predict and explain people’s performances in all kinds of deduction, and in our opinion it is currently the best explanation of human reasoning. However, we think our comprehension of reasoning by models would benefit from a deeper investigation into the functioning of the falsification process. Bucciarelli (2007) claims that cognitive conflicts (CCs) and socio-cognitive conflicts (SCCs) favor learning to reason, as they make the subject falsify. She grounds her assumption on the following findings. First, Bruner et al. (1966) find evidence in favor of the claim that the main mechanism of development is the conflict between different modalities of representation; when two systems of representation do not correspond, the child performs a clear-cut revision of his way of solving problems. Therefore, in giving an account of the development of children’s ability to reason, they introduce the term ‘conflict’. For example, a CC in the physical domain is a discrepancy between a child’s expectations and an empirical outcome that contradicts those expectations; the confrontation of a subject’s point of view with contradictory visual feedback would constitute an example of this category (see, e.g., Levin et al. 1990). As regards the SCC, from a developmental point of view, different theorists underline the importance of social interactions in cognitive development, by claiming that the first stages of thinking development are deeply social, since individual abilities become structured thanks to interaction with others (see, e.g., Vygotskij 1978). Doise and Mugny (1984) empirically found that the experience of the SCC assists children to reach the correct solutions to certain problems. In particular, when working alone, children at a certain stage in their development are unable to perform certain tasks because they cannot decenter, i.e., they focus on certain aspects of problems and exclude other, more important aspects; however, when exposed to differing points of view with regard to such problems, the same children find it easier to decenter. It follows that social interactions should provide an obvious forum for the possibility of exchanging perspectives. Studies on the SCC follow two main approaches (Druyan 2001): they either focus on interactions between peers with different perspectives (see, e.g., Ames and Murray 1982; Druyan and Levin 1996; Perret-Clermont 1980), or on interactions between individuals of different status, such as an adult (e.g., a teacher) and a child, or an expert and a layman (see, e.g., Saljo and Wyndhamm 1990). In our study, we deal with SCCs of the first type, where a subject is pressed to compare his/her perspective with that of another individual having a comparable social status. ‘This need to verify one’s own perspective in coordination with other perspectives structures the process of interpersonal negotiations in ways that can promote cognitive growth’ (Bearison 1986, p 136). On the other hand, in interactions of the second type, knowledge is imparted in a linear way, as one partner is the authority and the source of correct knowledge, which must be conveyed to the partner. In the literature, SCCs have mainly been used to induce children to solve conservation tasks. Doise and Mugny (1984), for example, added social interaction to a liquid conservation task by saying that two glasses of juice were a prize for two equally good children, thus introducing the social norm that the two children have the right to receive the same prize. The experimental subjects, who at first did not realize that the two glasses contained the same quantity of liquid, showed an understanding of the rules of conservation thanks to the social context. Siegler (1995) also obtained improved performance when asking the child to explain the point of view of the individual giving the correct answer to a conservation task. Consistent with the above-mentioned literature, and in line with Bucciarelli (2007), we argue that humans’ ability to assume different points of view is central to reasoning (also see Bucciarelli and Johnson-Laird 2001), and we aim to study possible different effects of CCs and SCCs on the ability to reason. Generally speaking, we assume that there is a strict relationship between mindreading and the falsification phase postulated by the model theory. Indeed, mindreading presupposes the assumption of different points of view, an ability which can be triggered by the experience of a conflict between one’s own and other persons’ beliefs. Such an ability plays an important role in the falsification phase of a reasoning process, during which temporarily abandoning the conclusion that has been reached, in favor of a different one, is crucial. Some evidence of the relationship between mindreading, falsification and SCC is presented in the next paragraph.",8
7.0,1.0,Mind & Society,13 March 2007,https://link.springer.com/article/10.1007/s11299-007-0032-8,Conditionals and conditional thinking,June 2008,Andrea Manfrinati,Pierdaniele Giaretta,Paolo Cherubini,Female,Unknown,Male,Mix,,
7.0,1.0,Mind & Society,26 April 2008,https://link.springer.com/article/10.1007/s11299-008-0046-x,Symposium on “A multi-methodological approach to language evolution”,June 2008,Angelo Cangelosi,,,Male,Unknown,Unknown,Male,,
7.0,1.0,Mind & Society,13 July 2007,https://link.springer.com/article/10.1007/s11299-007-0041-7,From grasping to complex imitation: mirror systems on the path to language,June 2008,Michael A. Arbib,James Bonaiuto,,Male,Male,Unknown,Male,"We will work within an existing framework (the Mirror System Hypothesis MSH) which advances the claim that the evolution of the language-ready brain rested upon the evolution in hominids of a capacity for complex imitation, focusing particularly upon the changes in the brain that preceded, and made possible, a brain which supports communication using protosign. The evolutionary framework that we adopt here comes from comparative neurobiology—we compare the mirror neurons for grasping of the macaque monkey brain to the mirror systems of the human brain, and use this comparison to ground the MSH on the evolution of the language-ready brain. First, the monkey. Both premotor area F5 and parietal area PF of the macaque brain contain mirror neurons each of which fires vigorously both when the monkey executes a certain limited set of actions and when the monkey observes some other perform a similar action. By contrast, canonical neurons in F5 fire vigorously when the monkey executes certain actions but not when it observes the actions of others. Turning to the human, we must rely on brain imaging rather than single-neuron recording. Imaging data show that the human brain contains mirror
systems in both frontal and parietal lobes, namely regions that show high activation both when a human performs a manual action and when the human observes a manual action, but not when the human simply observes an object. It is widely assumed that such mirror regions contain mirror neurons, based on similarities between the human and macaque brain. Strikingly, the frontal mirror system for grasping in the human is associated with Broca’s area, hitherto thought of as an area for speech production but now better understood for its involvement in the production of language as a multi-modal performance engaging face, voice and hands. The MSH (Rizzolatti and Arbib 1998; Arbib 2005, 2006) asserts that the parity requirement for language in humans—that what counts for the speaker must count approximately the same for the hearer—is met because Broca’s area (often associated with speech production) evolved atop the mirror system for grasping with its capacity to generate and recognize a set of actions. This is a hypothesis on the evolution of the language-ready brain, rather than of the structure of language itself. The crucial point is that humans have capacities denied to monkeys. Mirror regions in a human can be activated when the subject imitates an action, or even just imagines it, but there is a consensus that monkeys cannot imitate save in the most rudimentary sense. By contrast, chimpanzees exhibit “simple imitation”, the ability to approximate an action after observing and attempting its repetition many times; while humans alone among the primates have the capacity for “complex imitation”, being able to recognize another’s performance immediately as a combination of more-or-less familiar actions and to use this recognition to approximate the action, with increasing practice yielding increasing skill. This ability provides a crucial substrate for the child learning language, but it also provides—through its perceptual component—brain mechanisms that could be adapted for the ability to recognize the words of a novel sentence and how they fit together in a hierarchical structure to convey meaning. Arbib (2005) modified and developed MSH by hypothesizing seven stages in the evolution of language. The first three stages are pre-hominid: 
S1
: Grasping. 
S2: A mirror system for grasping, shared with the common ancestor of human and monkey. 
S3: A system for simple imitation of grasping shared with the common ancestor of human and chimpanzee. The next three stages distinguish the hominid line from that of the great apes: 
S4
: A complex imitation system for grasping. 
S5:
Protosign, a manual-based communication system that involves the breakthrough from employing manual actions for praxis to using them for pantomime (not just of manual actions), and then going beyond pantomime to add conventionalized gestures that can disambiguate pantomimes. 
S6:
Protospeech, resulting from linking the mechanisms for mediating the semantics of protosign to a vocal apparatus of increasing flexibility. Arbib argues that protosign and protospeech evolved together in an expanding spiral and that brain mechanisms supporting Stages S1 through S6 suffice to support Stage S7: 
S7
: 
Language: the change from action-object frames to verb-argument structures to syntax and semantics. The commentaries published in Arbib (2005) provide arguments and counter-arguments for these various claims. The present article does not revisit the evolutionary argument, but instead addresses the challenge of developing models of the brain mechanisms which support this evolutionary succession. We focus here on models for the earlier, rather than the later, stages in this progression, introducing a new model of action recognition learning by macaque mirror neurons which addresses data on auditory input, a model for opportunistic planning of sequential behavior, and studies of how to embed a macaque-like mirror system in a larger ape-like or human-like circuit to support “simple imitation” and then “complex imitation”. Other articles (e.g., those collected in Arbib 2006) carry the story forward to protolanguage and language. The closing discussion then returns to the relevance of these mechanisms for language and its evolution.",9
7.0,1.0,Mind & Society,17 July 2007,https://link.springer.com/article/10.1007/s11299-007-0040-8,"Language learning, power laws, and sexual selection",June 2008,Ted Briscoe,,,Male,Unknown,Unknown,Male,"A diagnostic of a power law distribution is that a log–log plot of frequency against rank yields a (nearly) straight line. For instance, Zipf (1935) plotted word token counts in a variety of texts against the inverse rank of each distinct word type and showed that typically such plots approximate a straight line. The characteristic ‘Zipf curve’ of word frequency against rank deviates from this line because the relative frequency of very common word types, such as the English determiners the and a, tend to be more similar than the power law predicts, as also does the relative frequency of very rare words in the tail of the distribution. Zipf’s ‘law’ is often expressed as:
 where B > 1, the exponent, defines the slope of the plot, frequency c(w) is the token count of word type w in text, and rank r(w) is the position of word type w in the list of word types sorted in descending order of frequency, c(w). Guiraud’s (1954) related law states that the number of word types V in a text is proportional to the length of that text N:
 Although the models of power law distributions of which I am aware have a dynamical component, they have received little attention from evolutionary linguists. I know of only one argument, due to an evolutionary psychologist (Miller 2000), which utilises Zipf’s observation about word frequencies in attempting to explain large, redundant vocabularies in terms of sexual selection. I argue against this explanation in Sect. 4, but, before doing so, I discuss the ubiquity of power law distributions in Sect. 2, some relevant models of them in Sect. 3, return to Miller’s argument in Sect. 4, and then discuss some issues power law distributions raise for evolutionary models of iterated language learning in Sect. 5.",4
7.0,1.0,Mind & Society,20 November 2007,https://link.springer.com/article/10.1007/s11299-007-0044-4,Embodiment versus memetics,June 2008,Joanna J. Bryson,,,Female,Unknown,Unknown,Female,"There is no doubt that embodiment is a key part of human and animal intelligence. Many of the behaviours attributed to intelligence are in fact a simple physical consequence of an animal’s skeletal and muscular constraints (Raibert 1986; Port and van Gelder 1995; Paul 2004). Taking a learning or planning perspective, the body can be considered as bias, constraint or (in Bayesian terms) a prior for both perception and action which facilitates an animal’s search for appropriate behaviour. Since this search for expressed behaviour is intelligence, there can be no question that the body is a part of animal intelligence. In other words, in nature at least, autonomous behaviour and bodies have co-evolved. The influence of the body continues, arguably through all stages of reasoning (Lakoff and Johnson 1999; Chrisley and Ziemke 2002; Steels and Belpaeme 2005), but certainly at least sometimes to the level of semantics. For example, Glenberg and Kaschak (2002) have demonstrated the action-sentence compatibility effect. That is, subjects take longer using a gesture to signal comprehension of a sentence about motion if the signalling gesture must be in the opposite direction as the motion indicated in the sentence. For example, given a joystick to signal an understanding of ‘open the drawer’, it is easier to signal comprehension by pulling the joystick towards you than by pushing it away. Boroditsky and Ramscar (2002) have similarly shown that comprehension of ambiguous temporal events is strongly influenced by the hearer’s physical situation with respect to current or imagined tasks and journeys. These sorts of effects have lead some to suggest that the reason for the to-date rather unimpressive state of natural language comprehension and production in Artificially Intelligent (AI) systems is a consequence of their lack of embodiment (Harnad 1990; Brooks and Stein 1994; Roy and Reiter 2005). The suggestion is that, in order to be meaningful, concepts must be grounded in the elements of intelligence that produce action. The pursuit of embodied AI has lead us to understand resource-bounded reasoning which explains apparently suboptimal or inconsistent decision-making in humans (Chapman 1987). It has also helped us to understand the extent to which agents can rely on the external world as a resource for cognition—that perception can replace or at least supplement long-term memory, reasoning and model building (Brooks 1991; Clark 1997; Ballard et al. 1997; Clark and Chalmers 1998). However, despite impressive advances in the state of artificial embodiment (e.g. Chernova and Veloso 2004; Schaal et al. 2003; Kortenkamp et al. 1998), there have been no clear examples of artificial natural language systems improved by embodiment. No speech recognition, text generation or interactive tutoring system has utilised embodiment to improve its semantic performance—indeed, this idea still seems absurd. If embodiment is the key to semantics, why is it ignored by even high-end language research and entertainment systems? I believe this is because embodiment, while certainly playing a part in both the evolution of human semantics and its development in individuals, is not in itself sufficient to explain all semantics in either context. We have seen neat examples of the embodied acquisition of limited semantic systems (e.g. Steels and Vogt 1997; Steels and Kaplan 1999; Roy 1999; Billard and Dautenhahn 2000; Sidnera et al. 2005; Hawes et al. 2007). These systems demonstrate not only that a semantics can be established between embodied agents, but also the relation between the developed lexicon and the agents’ physical plants and perception. However, such examples give us little idea of how words like infinity, social or represent might be represented. Further, they do not show the necessity of physical embodiment for a human-like level of comprehension of current natural language semantics. On the other hand, if some abstract semantic system underlies our representation of words such as justice, it is possible that that the semantic processes for that system may also be sufficient for understanding terms like kick and mother which originally evolved in reference to categories learned through embodied experience. If so, this might explain why (for example) congenitally blind people use visual metaphors as naturally as those who actually see. This article does not contest the importance of understanding embodiment to understanding human intelligence as a whole. This article does contest one of the prominent claims of the embodied intelligence movement—that embodiment is the only means of grounding semantics (Brooks and Stein 1994). Roy and Reiter (2005) in fact define the term grounded as ‘embodied’, which might be fine (compare with Harnad 1990) if grounded had not also come to be synonymous with meaningful. The central claim of this article is that while embodiment may have been the origin of most semantic meaning, it is no longer the only source for accessing a great deal of it. Further, some words (including their meanings) may have evolved more or less independently of grounded experience, possibly via memetic processes. In this article, I propose a model consisting of an interaction between a disembodied, memetic semantics and embodied knowledge. The disembodied semantics is not the traditional, logic-based symbol system that the embodiment theory originally arose in opposition to. Rather it is another cognitively minimalist representational system similar to other well-established forms of perceptual learning. I claim that humans are capable of recognising and relating patterns in sequential streams of information and can thus derive a form of semantics from their cultural environment of speech streams. This article begins by redefining some linguistic terms—not because they generally need redefining, but solely for local use in this article for the purpose of elucidating the model. I then review the current literature on the automatic, perception-like acquisition of semantic relationships. Next I present my model of how such a semantic system fits into human intelligence and language use. Finally I examine the implications of the model for the evolution of language.",11
7.0,1.0,Mind & Society,13 July 2007,https://link.springer.com/article/10.1007/s11299-007-0042-6,"How can “cheap talk” yield coordination, given a conflict?",June 2008,Mark Jeffreys,,,Male,Unknown,Unknown,Male,"Komarova and Nowak (2003) call for applications of game-theoretic studies of nonkin cooperation to research on the evolution of language, claiming that, “we speak because we cooperate, we cooperate because we speak” (p. 336). Responding to this challenge, the experiments reported here adapt the methods of behavioral economics, the better to measure the utility of the modern human language capacity, per se, and in hopes of loosening the “chicken-or-egg” knot that entangles the evolution of the human capacity for nonkin cooperation with that of our capacity for speech.",2
7.0,1.0,Mind & Society,05 July 2007,https://link.springer.com/article/10.1007/s11299-007-0039-1,Language co-evolved with the rule of law,June 2008,Chris Knight,,,,Unknown,Unknown,Mix,,
7.0,1.0,Mind & Society,04 July 2007,https://link.springer.com/article/10.1007/s11299-007-0038-2,Primate social knowledge and the origins of language,June 2008,Robert M. Seyfarth,Dorothy L. Cheney,,Male,Female,Unknown,Mix,,
7.0,2.0,Mind & Society,21 March 2007,https://link.springer.com/article/10.1007/s11299-007-0036-4,Ontology for information systems: artefacts as a case study,November 2008,Massimiliano Carrara,Marzia Soavi,,Male,Female,Unknown,Mix,,
7.0,2.0,Mind & Society,13 March 2007,https://link.springer.com/article/10.1007/s11299-007-0035-5,Empirical modeling and information semantics,November 2008,Gordana Dodig-Crnkovic,,,Female,Unknown,Unknown,Female,"A model is a simplified representation of a complex system or a process developed for its understanding, control and prediction; it resembles the target system in some respects while it differs in other respects that are not considered essential. It follows that a model, which is valid for one objective, may not be valid for another. Models are abstracted or constructed on the grounds that they potentially satisfy important constraints of the target domain. Model-based reasoning is essential for all sciences, particularly for the empirical. It supports conceptual change and facilitates novel insights as demonstrated in Magnani et al. (1999). When discussing models, two concepts are fundamental: verification and validation; where model verification is the confirmation that the model is constructed as a model specification based on a problem formulation and model validation is the demonstration that the model, within its domain of applicability, is consistent with its objectives. Consequently, the term “valid” refers to a model that adequately represents a target system in its domain of applicability. Determining whether or not a model is an appropriate representation of reality, for a well-specified goal, is the essence of model validation. The relationship between the model and the physical reality is established by conducting empirical tests. Determining whether or not a model is an appropriate representation of reality, for a well specified goal, is the essence of model validation, but there are other significant factors to be considered such as the relevance of the goal itself, Dodig-Crnkovic 2003
 Simulation as a special case of modeling implies time-dependent goal-directed experimentation with a dynamic model. Simulation can be used in analysis, control, and design, Wildberger (2000). It is a tool, which facilitates the gaining of insight, the testing of theories, experimentation with strategies of control, and prediction of performance. In the concept of simulation as a model-based computational activity, the emphasis is on the generation of model behaviour. Simulation can be interpreted as model-based experimental knowledge generation, Ören (2001), and can be combined with different types of knowledge generation techniques such as optimization, statistical inference, reasoning and hypothesis processing. Questions of interest are to what degree the results of modeling and simulation can be trusted and can they be said to generate reliable information? The former may be answered in a pragmatic way, by asking what would be the alternative to using model-based reasoning, learning and prediction techniques. In the case of weather forecasting, for example, we know that the reliability of the prediction is not extremely high, but it is improving, and it should be compared to a pure guess, which obviously is a less successful prediction method. The output of a model for producing weather forecasts may be seen as information that is probable but not certain (true), yet necessary and useful.",2
7.0,2.0,Mind & Society,23 January 2008,https://link.springer.com/article/10.1007/s11299-007-0045-3,Is domain-general thinking a domain-specific adaptation?,November 2008,Vittorio Girotto,Katya Tentori,,Male,Female,Unknown,Mix,,
7.0,2.0,Mind & Society,23 October 2008,https://link.springer.com/article/10.1007/s11299-008-0051-0,Moral justification and feelings of adjustment to military law-enforcement situation: the case of Israeli soldiers serving at army roadblocks,November 2008,Shaul Kimhi,Shifra Sagy,,Male,Female,Unknown,Mix,,
7.0,2.0,Mind & Society,06 November 2007,https://link.springer.com/article/10.1007/s11299-007-0043-5,Refusing to budge: a confirmatory bias in decision making?,November 2008,Lea-Rachel D. Kosnik,,,Unknown,Unknown,Unknown,Unknown,,
7.0,2.0,Mind & Society,08 May 2007,https://link.springer.com/article/10.1007/s11299-007-0037-3,"Taboo or tragic: effect of tradeoff type on moral choice, conflict, and confidence",November 2008,David R. Mandel,Oshin Vartanian,,Male,Unknown,Unknown,Male,"The ability to take moral considerations into account when making decisions has long been regarded as a distinguishing feature of human consciousness. Buttressing this view, early cognitivist theories of moral choice placed a strong emphasis on the role of analytic processes in deliberating about moral problems (see Shweder and Haidt 1993). These analytic processes were hypothesized to increase in complexity and sophistication over a series of developmental stages, and the higher stages involved reasoning about moral issues such as justice in accordance with abstract principles that require analytic thinking (Kohlberg 1984; Piaget 1965). More recently, however, some theorists have proposed that analytic thinking takes a secondary role to intuitive processes. For instance, according to Haidt’s (2001, 2004) influential social-intuitionist account, people tend to reach moral judgments quickly on the basis of emotionally driven intuitions, and they flesh out decision-bolstering reasons using analytic thinking only if held accountable by others. In the present article, we refer to intuitive and analytic processes as the low and high endpoints of a continuum of subjective mental effort, respectively. Our thesis is that moral judgment and choice can be more or less effortful—namely, more analytic or more intuitive—and that much of the variance in the degree of subjective mental effort applied to a moral problem will depend on the characteristics of the problem itself. This thesis is in line with recent trends in the literature that focus on the interplay between problem and agent characteristics in understanding decision making in the moral domain (see Krebs and Denton 2005). In this respect, what is sorely needed is a clearer understanding of the features that make a moral problem subjectively easy or hard to resolve. Our focus in this paper is primarily on one feature that we hypothesize to be important in this regard; namely, whether or not the problem permits one to arrive at a choice without violating a moral prohibition, edict, or norm. Specifically, we hypothesize that moral problems allowing for such an escape route will be perceived as relatively easier to resolve than those that do not allow the decision maker to avoid violating a moral norm. This problem cannot be addressed in a typical study of moral decision making, in which the focus is exclusively on people’s choices because choices per se do not directly reveal how much conflict the decision maker may have actually endured in reaching the decision. Rather, the subjective ease or difficulty of choice (or of the problem for which a choice must be made) is likely to be revealed by decision makers’ degrees of perceived conflict in reaching their decision and in their confidence in that choice. In support of this view, Simmons and Nelson (2006), have shown that participants who chose an intuitively appealing option over a less intuitively appealing one that was nevertheless of equal expected value tended to feel more confident in their choices than participants who chose the less-intuitive option. For instance, in some of their studies, participants were asked to choose between two sports teams given a point spread, which in fact made the two alternatives equally probable. In line with intuition, the majority of bets were placed on the favorite team, and those who bet on this intuitive option felt more confident than those who betrayed their intuition and bet on the underdog (given the point spread). In short, Simmons and Nelson (2006), found that the “easy decision” of picking the intuitive option led people to feel more confident in their choices than those who chose the counterintuitive option (despite its equal likelihood of yielding a winning bet). Given the importance of examining indirect measures of subjective mental effort in the context of moral, we conducted an experiment that directly examined the effect of problem type on the levels of perceived moral conflict and confidence in choice. Subjective assessments of mental effort and problem difficulty may be viewed as opposing sides of the same coin. In general, a problem will be perceived as difficult if its resolution feels effortful. The substantive difference is one of emphasis: the term mental effort emphasizes the cognitive processes involved in resolving a problem, whereas the term problem difficulty emphasizes features of task structure. Thus, to say that problem difficulty is a determinant of mental effort is more of a tautological statement than an explanatory one, and we wish to acknowledge this fact at the outset of our discussion. Determinants of both, we would argue, may be located in features of problems as well as social–cognitive constraints on decision makers. Our primary interest here is in the nexus of these features—that is, problem features that elucidate constraints on human minds faced with compelling social problems of a moral nature. Recent research by Greene et al. (2004), has revealed intriguing findings regarding this issue. These authors compared the effect of what they termed easy and hard moral problems on decision makers’ response times and patterns of cortical activation. Greene et al.’s (2004) hard moral problems had a specific structure: namely, they pitted the negative social-emotional response associated with violating a moral norm against a more abstract, cognitive response that maximized the welfare of the aggregate. For example, one of their hard problems, called “crying baby,” involved a situation in which a mother in a war-torn region is faced with the following choice: smother her crying child, which would save her own life as well as the lives of other hiding villagers, or do not smother her child, in which case enemy soldiers would kill everyone. According to Greene et al. (2004), this is a hard problem because in order to maximize the aggregate good (i.e., to save more lives) one must opt for a mother to kill her own child, which would in turn result in the negative social-emotional state associated with violating a moral norm (i.e., do not kill children). In contrast, according to Greene et al. (2004), an example of an easy problem was one involving a teenage mother who had to decide whether or not to kill her unwanted newborn infant. Greene et al. (2004), treat this as an easy problem because, in their view, maximizing the aggregate good does not require the violation of a moral norm. That is, the decision not to kill the infant is in line with the moral norm prohibiting murder and is also presumably in line with maximizing the aggregate good (i.e., the baby lives). Greene et al. (2004) found that participants took longer to make a decision for hard problems than for easy problems. They also found that compared to easy problems, hard problems activated cortical areas known to mediate the effortful processing of conflict, abstract reasoning, and cognitive control. According to Greene et al. (2004), choosing to maximize the aggregate good is difficult in hard problems because by doing so one must violate a moral norm, which in turn elicits a strong negative social–emotional response. This has the effect of inhibiting the consideration of norm-violating acts in order to avoid the negative emotional response that accompanies norm violation. More generally, we propose that the subjective difficulty of moral problems is determined largely by whether at least one of the available options allows the decision maker to escape from having to violate what otherwise ought to be an inviolable moral principle. In the absence of such an escape route, the decision maker is likely to face a conflict situation that may be difficult to resolve, especially if the stakes are high. Indeed, if every option leads to a fundamental moral breach then the conflict may very well appear intractable. Thus, we would argue that the crying-baby problem is hard precisely because no matter what the mother chooses, she is forced to violate a moral rule with dire consequences. To summarize, then, we propose that assessments of problem difficulty (on the task side) and mental effort (on the cognitive processes side) will be direct reflections of the level of moral conflict experienced by the decision maker. Our analysis is closely related to Tetlock et al.’s (2000) distinction between tragic and taboo tradeoffs. A key feature of tragic tradeoffs is that they force decision makers to choose the lesser of multiple evils, thus making them hard to reason through or “dilemma-ish.” For example, having to decide, which of two children in need of a liver transplant will get the only available liver would represent a tragic tradeoff. Much like Greene et al.’s (2004) hard problems, the difficulty of tragic tradeoffs is due to their structure: no matter what choice is made, the decision maker is forced to violate an absolute prohibition, such as killing someone or letting someone die when one could have prevented it. By contrast, taboo tradeoffs offer decision makers a moral “way out,” provided they accept the constitutive incommensurability of the tradeoff; that is, provided they abide by social norms proscribing the monetization of certain values, such as the protection of human life, or otherwise treat such values in a non-compensatory manner. For instance, having to decide between conducting a costly liver transplant for a sick child or allocating the same sum of money toward renovations in the hospital would represent a taboo tradeoff. Note that the taboo–tragic distinction does not necessarily map on to the easy–hard distinction perfectly in this example. It may very well be the case that allocating the money to the renovation yields the greatest aggregate good, which would make this a hard problem according to Greene et al.’s (2004) criteria. In our view, the life versus money tradeoff is in an important sense not a hard problem precisely because the moral proscription against monetizing life gives the decision maker a moral way out. Thus, our parsing of the moral problem space comes somewhat closer to Tetlock et al.’s taboo-tragic distinction than to Greene et al.’s easy–hard distinction. We draw on Tetlock et al.’s (2000) terminology in this paper precisely because the notion of what makes some tradeoffs tragic as opposed to taboo is indicative, we believe, of an important proximal determinant of mental effort. However, the present research also goes beyond past work by directly examining the effect of tradeoff type on decision makers’ assessments of the moral conflict they experienced in reaching their choices. Specifically, we hypothesized that, compared to taboo tradeoffs, tragic tradeoffs will prompt higher levels of moral conflict, which we define in the present study as the extent to which one regards the relevant problem as a moral dilemma. A second, inter-related objective of the present research was to introduce the measurement of confidence into studies of moral decision making. As noted earlier, we believe that the literature on moral decision making would benefit by doing so because such studies can shed light on the experience of mental effort required to resolve moral problems. For example, consider the following moral dilemma known as the trolley problem (Foot 1967; Thompson 1976): a runaway trolley is about to kill five workmen, and you are the only one who can intervene. If you do nothing, the five workmen will die. Alternatively, if you flip a switch that turns the trolley onto another track, the five workmen would be saved, but one workman on the other track would die. What would you do: let the five workmen die, or save them but kill another man? If you were like most people, you would choose the latter option (Greene et al. 2001). However, now consider a version of the trolley problem in which the lone workman was replaced by a priceless statue. What would you do then? We predicted that in the statue version most participants would similarly opt to save the five workmen (and let the statue be destroyed). However, we also predicted that participants would be less confident in their choices involving the tragic tradeoff (i.e., the workman version) than in their choices involving the taboo tradeoff (i.e., the statue version). Thus, we anticipated that even though this manipulation of tradeoff type would have a negligible effect on choice, it would have a significant effect on confidence. The predicted effect of tradeoff on confidence, we further hypothesized, would be mediated by participants’ level of moral conflict. That is, we anticipated that the tragic tradeoff would generate more conflict than the taboo tradeoff, and that variation in moral conflict would, in turn mediate the effect of tradeoff on confidence such that conflict would be inversely related to confidence. Our inclusion of confidence and conflict measures also allowed us to test Simmons and Nelson’s (2006) intuitive betrayal hypothesis, which as discussed earlier involves the tendency for people who betray their intuitions to feel less confident in their choices than people who choose in line with their intuitions. In the present study, as in some of Simmons and Nelson’s studies, we define the intuitive option as the one selected by the majority of participants. Thus, the intuitive betrayal hypothesis predicts that participants making the majority choice would feel more confident than those making the minority choice. We extended this test as well to our measure of moral conflict. Another objective of our research was to investigate whether norm-violating options involving an act of commission would be less likely to be chosen than those involving an act of omission. In the present context, omission bias refers to a preference for harm caused by omissions over equal or lesser harm caused by actions (Baron and Ritov 2004). The omission bias seems to be motivated by unwillingness to cause direct harm. In a particularly relevant study, Baron (1992) asked participants to imagine that they were one of four innocent prisoners held captive in the Middle East. The captors will surely kill two of the other prisoners unless the participant kills one of the three prisoners. The participants are asked to assume that their fellow captives will not know about their decision and, moreover, that they believe in the sincerity of their captors’ offer. When asked, “Do you think that you would kill the one to save the two?” (1992, p. 322), approximately 88% indicated that they would reject the offer. This finding provides some evidence in support of the omission bias because the majority of participants chose not to act even though inaction would presumably result in more deaths. Nevertheless, factors unrelated to the omission bias may explain this result. For instance, the assumption that the captors would keep their word is highly suspect. Given the captors’ control of the situation, there is little reason why participants should trust the integrity of the offer. Indeed, the scenario brings to mind one in which the captors’ intent may be to double cross the participant. There is also a sense in which it may be more heroic not to play the captors’ game. Accordingly, it would be of value to test for the omission bias in cases in which these other possible explanations are excluded. Cushman, Young, and Hauser (2006) tested the omission bias using alternative versions of the trolley problem that were unlikely to evoke the kinds of inferences that might bias people toward inaction. For instance, in the omission version of one problem they can save five people if they do not pull a lever (thus killing one in the process), whereas in the commission version they can save the same five people by pulling the lever (also resulting in one dying). Thus, the only difference is whether the response that saves more lives requires an act of omission or an act of commission. Cushman et al.’s (2006) study involved two stages. In the first stage, participants were presented with dilemmas and asked to rate the moral permissibility of a protagonist’s omission or commission. In the second stage, they were presented with pairs of dilemmas that were identical but for a critical feature—for our purpose, the omission–commission distinction—and then asked to justify differences in their own ratings. A key finding of their study was that participants viewed harm-inflicting acts of omission as more permissible than harm inflicting acts of commission. While revealing, a shortcoming of their study was that it did not examine the effect of action on choice directly, nor did it examine whether the inaction effect survives a between-subjects design in which attention is not so obviously drawn to the action–inaction difference. Thus, to address this issue in a more rigorous manner, we conducted a direct test of the omission bias by manipulating in a between-subjects design whether the harm-inflicting “save-five” option in the trolley problem was associated with either hitting or not hitting the switch. In the standard (workman) version of the trolley problem, the description of the lives that might be lost is identical but for their number: five people will die by default and one person will die if action is taken. The comparability of these generic descriptions of potential losses of the same type may lead to a consequentialist mindset in which decision makers reason that preserving five lives is better than preserving one. This is in line with Nichols and Mallon’s (2006) view that moral decisions are influenced by multiple factors, including cost–benefit analysis. The final objective of the present research, then, was to examine the effect of the comparability of losses in a tragic tradeoff. To investigate this issue, we included an additional condition in our design that forced decision makers to choose between saving the five workmen and letting one child die or preserving the child’s life but allowing the five workmen to die. We predicted that a smaller proportion of participants in this condition would opt to save the five workmen than in the standard version because the diminished comparability of losses would likely undermine a simple “five beats one” analysis.",16
7.0,2.0,Mind & Society,28 February 2007,https://link.springer.com/article/10.1007/s11299-007-0034-6,"Tacit knowledge, implicit learning and scientific reasoning",November 2008,Andrea Pozzali,,,Female,Unknown,Unknown,Female,"The belief in the fact that any attempt to model scientific reasoning should take into consideration the role of tacit knowledge is spreading throughout the community of research. As a matter of fact, tacit knowledge research is currently witnessing a tremendous interest by many scholars belonging to different disciplines such as philosophy, cognitive sciences, neurosciences, social sciences, economics and so on. The number of books and scientific papers that deal with tacit knowledge shows a kind of exponential growth, and the same holds for the number of new authors approaching this field.Footnote 1 Yet, despite all this current excitement and the significant advances the field has made in the last few years, we still are very far from developing a clear definition and categorization of tacit knowledge itself. One of the reasons for this can be due to the fact that the literature has been concerned more with tacit knowledge as a product than with tacit knowing as a process. Tacit knowledge research was thus constrained to operate within the framework provided by the standard theory of knowledge and its classical categorization of knowledge in “competence”, “acquaintance” and “justified true beliefs” (Lehrer 1990). In this categorization, tacit knowledge was considered almost exclusively as “competence” and was thus identified with skills, know-how and physical abilities. The possibility that tacit knowledge could also assume other forms was somehow ruled out and this did not help both empirical and theoretical research. Focusing on tacit knowing as a process not only seems to be more in line with Polanyi’s original work, but it could help to develop a better understanding of tacit knowledge itself, as I will try to argue. As we are concerned here mainly with modeling scientific reasoning, I will try to illustrate my thesis by focusing only on a very specific field of analysis that is the role of tacit knowledge in science. Anyway, the general conclusions I draw from this analysis can be applied to tacit knowledge research as whole. As already mentioned, the concept of “tacit knowledge” was first introduced in the epistemological debate by Michael Polanyi (1958), who used it to refer to all those kinds of scientific knowledge that cannot be expressed in explicit form (spoken words, formulae, maps, graphs, mathematical theory and so on). Even if a great part of overall scientific knowledge is stored in laws, theories, formulae and so on, another part of it consists of practices, abilities, personal insight and expertise that cannot be codified and still represent the working toolbox of every skilled researcher. The concept of tacit knowledge has been further on developed in philosophy and sociology of science in order to incorporate it within a more comprehensive theory of practices and their role in social reality (Lynch 1993; Schatzi et al. 2001): “Practices are held to be a condition of understanding that cannot itself be understood in fully explicit terms. ‘We know more that we can say’ as Polanyi’s slogan puts it (...). In the case of an activity, such as producing a particular kind of scientific observation, the notion of practice (...) describes what it is that practitioners of a technique possess that enables them to perform, but which is not and perhaps cannot be formulated in a cookbook description of the technique” (Turner 1999, p. 149). Research on tacit knowledge in science has not been limited to philosophical analysis. It has also tried to perform an empirical analysis of the actual role of tacit knowledge in the processes of scientific and technological development. Many field studies have been conduced on this topic, ranging from laser building (Collins 1992) to nuclear weapons invention (MacKenzie and Spinardi 1995), from biological procedures (Cambrosio and Keating 1988) to veterinary surgery (Pinch et al. 1996). Other interesting contributions come from history of technology, in particular for what concerns engineering developments (Vincenti 1990; Ferguson 1992). In all these cases, tacit knowledge has been shown to be highly influential in the processes that led to scientific invention and to technological development. Unfortunately, this literature seems to be affected by a sort of “bias”, as it has mainly identified “tacit knowledge” with implicit skills and kinaesthetic abilities (often defined also as know-how); other possible forms of tacit knowledge have been neglected. A few examples will suffice to show how this reduction of tacit knowledge to skills actually goes on. Collins (1992, p. 56), talks of tacit knowledge as of “... the name given by Michael Polanyi (1958) to our ability to perform skills without being able to articulate how we do them. The standard example is the skill involved in riding a bicycle. (...) Tacit knowledge usually finds its application in practical settings such as bike riding or other ‘skilled’ occupations”. Even if he recognizes that tacit knowledge can also be applied to mental activity, Collins equates these last activities with a sort of “social skill” (the whole of social knowledge that enables a given subject to be part of the cultural and social life of the community he belongs to and that differentiates him from strangers, that do not have this knowledge, and from newborns, that still have to acquire it). Also MacKenzie and Spinardi (1995, p. 45) refers to tacit knowledge mainly as a sort of physical skill, and once again bike riding is used to provide a standard example: “Motor skills supply a set of paradigmatic examples of tacit knowledge in everyday life. Most of us, for example, know perfectly well how to ride a bicycle yet would find it impossible to put into words how we do so. There are (to our knowledge) no textbooks of bicycle riding, and when children are taught to ride, they are not given long lists of written or verbal instructions. Instead, someone demonstrates what to do and encourages them in the inevitably slow and error-ridden process of learning for themselves”. In a more recent empirical work on tacit knowledge, Collins tried to develop a new categorisation, that was specifically “... intended not to deepen our understanding at a philosophical level but to explicate the idea clearly and draw out its implications for scientific practice” (Collins 2001, p. 71). This categorisation identified five different kinds of tacit knowledge (concealed knowledge, mismatched salience, ostensive knowledge, unrecognised knowledge and uncognized/uncognizable knowledge) and this in turn seemed to provide the possibility to consider tacit knowledge as something more than purely physical skills. Unfortunately, the results of the empirical fieldwork performed by Collins were not strong enough to provide a clear support for this categorisation. Much of the tacit knowledge Collins identified fall in fact in the category of “ostensive knowledge”, that seems to represent just an alternative definition for skill-like knowledge. Moreover, at the end of his paper Collins once again reverts to the identification of tacit knowledge as a whole with skill, by using also the well-known analogy with bike-riding: “Knowing how difficult a skill is, is another important part of learning to master it. If one believed that bike-riding could be mastered in one minute, a few minutes of falling off would lead one to distrust claims that bikes could be ridden at all, and one would never learn to ride—still more so with, say, playing a musical instrument” (Collins 2001, p. 82).",14
8.0,1.0,Mind & Society,23 April 2009,https://link.springer.com/article/10.1007/s11299-009-0055-4,Special issue on “cognition and emotion in economic decision making”,June 2009,Nicolao Bonini,Rob Ranyard,Luigi Mittone,Unknown,Male,Male,Male,"The papers of this special issue of Mind and Society are a selection of papers presented at an international research workshop jointly sponsored by the European Association of Decision Making (EADM), and the International Association of Research in Economic Psychology (IAREP). The workshop, on Cognition and Emotion in Economic Decision Making, was organized by Nicolao Bonini, Luigi Mittone and Rob Ranyard, and was held in Rovereto, Italy, on January 26–27, 2007.Footnote 1
 Economic decision making requires choosing appropriate courses of action to promote the economic well being of oneself or others in a complex, dynamic and uncertain world. The economic perspective is important, not least because it can provide an understanding of the complex environment at the level of the global and national economies as well as the level of the small group or organisation. In contrast, the psychological perspective has mainly focussed on understanding the cognitive and emotion-based mechanisms that underlie economic decisions. Complementing these two, the neuroscience perspective aims to understand the neural structures and processes that underpin cognitive and emotional processes related to economic behaviour. The entry of neuroscience into decision research is relatively recent, and some psychologists and economists have been slow to acknowledge its potential. This may be partly because it can be seen as a reductionist enterprise, attempting to explain human experience and behaviour solely in terms of physiological and chemical processes within the brain and body. However, relating mental experience and decision behaviour to neural correlates can be seen in terms of more comprehensive theoretical goals. It can be argued from a critical realist perspective, for example, that a full understanding of economic, social and psychological phenomena requires stratified explanations (Archer et al. 1998). That is, explanations that integrate mechanisms operating at different levels, ranging from macro social and economic structures to the psychological, and ultimately to the neural level. Consider the following passage from Antonio Damasio’s Descartes’ Error (1994; 2006, p. 78): The solution to the problem of social violence will not come from addressing only social factors and ignoring neurochemical ones. Nor will it come from blaming one neurochemical correlate alone. Consideration of both social and neurochemical factors is required, in appropriate measure. This sentiment is also relevant to questions of economic decision making, and was one of the motivations behind the workshop on Cognition and Emotion in Economic Decision Making from which the papers in this special issue are derived. One of the challenges of the workshop was to contribute to the stratified explanation of economic decision making, identifying and ultimately integrating explanatory mechanisms operating at the neural, psychological and socioeconomic levels of analysis. The keynote speakers at the workshop represented the above-mentioned three perspectives of psychology, economics and neuroscience. The discipline of psychology is said to have undergone a ‘cognitive revolution’ signalled by Miller et al. (1960). In decision research this was developed by Simon (1957) in his seminal works on bounded rationality. Our first keynote speaker, Paul Slovic, was at the forefront of applying the concepts of the cognitive revolution to decision research, for example identifying possible cognitive mechanisms underlying preference reversals and related phenomena (Lichtenstein and Slovic 1971; Slovic 1995). Several papers in this special issue deal with important cognitive aspects of economic decision behaviour, including risk perception, and expectations brought about by previous experience. If there was a cognitive revolution in the psychology of decision making in the 1960s, then it can be said without exaggeration that there was an ‘emotional revolution’ in the field in the 1990s, signalled by Damasio’s (1994) important text referred to earlier. Once again, Paul Slovic was at the centre of this development, beginning with his important work on the affect heuristic (Slovic et al. 2002). Our second keynote speaker at the workshop, economist George Loewenstein, has also made important contributions to understanding the role of emotions underlying decisions, including his work on the ‘risk as feelings’ perspective (Loewenstein et al. 2001). Most of the papers in this special issue deal with aspects of the role of emotion in economic decision making. Finally, our third keynote speaker at the workshop, Alan Sanfey, has taken different approaches to the study of economic decision making, including the neuroscientific. He presented a comprehensive overview of his innovative research programme identifying some neural correlates of economic decisions (Harlé and Sanfey 2007; Sanfey et al. 2006). Although this perspective is not directly represented in this special issue, many of the papers have been indirectly influenced by it, including Singh and Kahn’s investigation of risk taking in Damasio’s Iowa Gambling Task and papers on the role of emotions in the Ultimatum Game (see also Sanfey et al. 2003).",
8.0,1.0,Mind & Society,11 November 2008,https://link.springer.com/article/10.1007/s11299-008-0052-z,Dispositional anger and risk decision-making,June 2009,Elisa Gambetti,Fiorella Giusberti,,Female,Female,Unknown,Female,"Risky decision-making involves making choices with the potential for either positive or negative outcomes. For example, choosing to take a new experimental drug for headaches could result in recovery or in other illnesses because of unknown side effects. Nevertheless, there is no unequivocal definition of risk. As Fischhoff (1985) observes: “People disagree more about what risk is, than about how large it is”. Risky decision-making is a general concept and may be viewed as comprising various elements. A common dictionary definition of risk is “the possibility of loss”. But an outcome that is a loss for someone might well be a gain for another. For example, moving to another city for one’s work may be a loss for one individual and a gain for another. Moreover, if two individuals both consider the same outcome to be a loss, there is still room for differences in the significance of that loss. However, according to Yates and Stone (1992), the significance attributed to potential loss is a critical element of risk. These individual differences can be very important in risky decision-making. For example, a substantial body of research has shown that different personality characteristics can influence and explain risky decision-making (e.g., Dahlbäck 1990; Lauriola et al. 2005). Recently, Maner et al. (2007) provided evidence for a link between dispositional anxiety and risk-avoidant decisions. According to the authors, risk-avoidant decision-making can be viewed as the output of a motivational process, initiated by the experience of anxiety, that leads individuals to avoid threats associated with potentially risky courses of action. On the other hand, research suggested that risky decision-making can be profoundly influenced by emotional experiences (e.g., Slovic et al. 2004). For example, emotions such as fear, disgust and anger have been shown to guide judgments and risky choices (Lerner and Keltner 2001; Fessler et al. 2004). Thus, there are reasons to affirm that risky decision-making may be linked to individual personality differences and to particular emotional states. The current study was designed to evaluate the extent to which the tendency to experience anger might be associated with individual differences in risky decision-making. Anger is an emotional reaction which varies in intensity from mild irritation to extreme fury in response to environmental circumstances perceived as frustrating, annoying or irritating. Trait anger is defined as a tendency to experience angry feelings in a variety of situations and it may be considered as a temperament and a reaction (Spielberger 1999). The former refers to a general temperament of low threshold reactivity in which anger is experienced in response to several relatively innocuous triggers; the latter is defined as a narrower pattern of reactivity to specific classes of stimuli, such as competition, rejection or perceived unfairness. Therefore, the difference between trait anger temperament and trait anger reaction is in the variety of stimuli that evoke anger. In general, trait anger may be viewed as a personality characteristic because of the tendency to react in a specific manner across time and situations. According to Lerner and Tiedens (2006), anger merits a special attention in decision-making for several reasons. First, anger is one of the most frequently experienced emotions. According to Averill (1982), most people report becoming mildly to moderately angry anywhere from several times a day to several times a week. Second, anger has an unusually strong ability to capture attention. Hansen and Hansen (1988) have demonstrated what they call “The Anger Superiority Effect”: a tendency for people to pay particular attention to angry faces. Third, anger has infusive influences on decisions and judgments. Several studies found that anger makes people indiscriminately optimistic about their own chances of success and it activates optimistic beliefs about experiencing future life events (e.g., Lerner et al. 2003; Fischhoff et al. 2005). According to Lerner and Keltner (2000), the appraisal-tendency approach can explain these data. The theoretical background of this approach may be found in cognitive models of emotions asserting that emotions are supported and regulated by a variety of cognitive processes (e.g., Beck 1999). In particular, Lazarus (1991) defended the notion that no emotion can arise without a cognitive appraisal attributing an emotional meaning to a situation. On the basis of the appraisal-tendency approach, emotions involve differences in the way that individuals appraise their environment. In this sense, the experience of emotion activates a cognitive predisposition to appraise the environment along several cognitive dimensions, for example the dimension of certainty (i.e., the degree to which future events seem more or less predictable and comprehensible), the dimension of control (i.e., the degree to which events seem to be brought about by individual agency or situational agency) and the dimension of responsibility (i.e., the degree to which someone, or something, other than oneself or oneself seems to be responsible) (Smith and Ellsworth 1985). Although such cognitive appraisals were traditionally conceptualized as causes of the experience of emotions (e.g., Lazarus 1991; Beck 1999), it is important to point out that emotions may arise in any number of ways, including non-cognitive routes, such as bodily feedback or unconscious priming (e.g., Parkinson 1996). In these cases, appraisals do not play a causal role in creating the emotions, but nonetheless the corresponding appraisals will ultimately be experienced (Keltner et al. 1993). Emotions and cognitive appraisals have a recursive relationship, each making the other more likely. For example, the more anger one feels, the more one perceives others as responsible for a negative event; and the more one perceives others as responsible for a negative event, the more anger one feels (Quigley and Tedeschi 1996). Emotions predispose individuals towards acting in specific ways to meet environmental problems. For instance, anger increases the tendency to perceive new situations as predictable, comprehensible and under individual control. As a consequence, it increases optimism and feelings of invulnerability and it is related to the perception of low risk across new situations (e.g., Lowenstein and Lerner 2003). Once activated, anger can influence people’s perceptions, form their decisions and guide their behavior, regardless of whether the decisions at hand have anything to do with the source of one’s anger. In this case, anger manipulations have been defined as incidental because they are irrelevant to present judgments and choices (Lerner and Keltner 2000). The appraisal-tendency approach can explain the effects of both momentary and dispositional emotions. Whereas momentary emotions refer to immediate affective reactions to a particular target, dispositional emotions refer to the tendency to react with specific emotions across time and situations (e.g., Lazarus 1994). There is evidence suggesting that dispositional emotion resembles momentary emotion in important ways, and thus, should yield similar effects on judgments (e.g., Gross et al. 1998). Consequently, individuals prone to feel anger across different situations are likely to perceive their environment in a particular manner and, for this reason, they can make particular decisions. For example, trait anger can be linked to the perception of the environment, that in their turn can be related to risky decisions. In this sense, the perception of familiarity and salience can mediate the relationship between trait anger and decision-making under risk. According to previous studies (e.g., Smith and Ellsworth 1985), incidental anger increases the perception of certainty and control over situations and, for these reasons, it should increase the perception of familiarity. Furthermore, Hockey et al. (2000) found that familiarity reduces uncertainty. In such terms, making a decision in a situation perceived as usual and well-known should increase the preference for risky choices. Trait anger may also be linked to the perception of the salience of situations, defined as the importance for an individual to obtain a favorable outcome. According to Hemenover and Zhang (2004), incidental anger activates a defensive optimism where the importance and impact of negative events are de-emphasized. Therefore, trait anger can be linked to the perception of low importance in several situation and, in turn, it could give rise to risky decisions. Although it is clear that incidental anger promotes decision-making biases that increase one’s tolerance for risk, few studies have systematically examined both the relationship between trait anger and risky decision-making and some possible mediators of this relationship, such as the perception of the familiarity and the salience of situations. The impact of these mediators on risky decision-making is important. There is a growing body of research suggesting that cognitive assessments of the environment, activated by specific emotions, have an important influence on risky decision-making. The appraisal-tendency approach is an example. However, few studies have examined the impact of dispositional emotions on the perception of the situations and consequent behavior, with most of the work to date focused on risk assessment or form of cognition, such as attribution, certainty or control. The first aim of the present study was to examine the relationship between trait anger and risky decision-making in hypothetical everyday decision-making scenarios. In addition, we wanted to evaluate how anger might interact with contextual characteristics (i.e., familiarity and salience) to affect risky decision-making. In fact, these factors may be mediators of the relationship between dispositional anger and risky decision-making. In this sense, anger should be linked to risky decision-making both directly and via its association with the perception of the context. The evaluation of the mechanisms with which dispositional anger is linked to risky decision-making, also by means of mediating factors, may be a worthy addition to the literature. In particular, we try to build a bridge between two lines of research: research on emotions in judgments and decisions (e.g., Lerner and Keltner 2000) and studies that did not mention emotion but that concerned contextual characteristics (e.g., Hockey et al. 2000). In contrast to much previous research, this study examined risky decision-making in everyday scenarios representative of a wide range of situations (i.e., financial, social, health), characterized by uncertainty and ambiguity, frequently faced by individuals in their normal lives. In particular, we adopt an approach based on Kogan and Wallach’s (1964) use of real life scenarios, although emphasizing situations rather than major life events. Pietromonaco and Rook’s (1987) and Hockey et al. (2000) studies are the only two of this kind that we have encountered in the current literature.",9
8.0,1.0,Mind & Society,10 July 2008,https://link.springer.com/article/10.1007/s11299-008-0048-8,Emotions and perceived risks after the 2006 Israel–Lebanon war,June 2009,Uri Benzion,Shosh Shahrabani,Tal Shavit,Male,Unknown,,Mix,,
8.0,1.0,Mind & Society,16 August 2008,https://link.springer.com/article/10.1007/s11299-008-0050-1,Heterogeneity in choices on Iowa Gambling Task: preference for infrequent–high magnitude punishment,June 2009,Varsha Singh,Azizuddin Khan,,Female,Unknown,Unknown,Female,"The Somatic Marker Hypothesis (SMH) (Damasio 1994) states that emotion-based visceral response called somatic markers guide our choices. This hypothesis has been quite popular because of its specificity at the neural and behavioral level (for a critical review, see Dunn et al. 2006). A method that is commonly used to test SMH is that of analyzing the choices made while performing Iowa Gambling Task (IGT). This task involves making 100 successive choices between one of four decks of cards, labeled as decks A′, B′, C′ and D′ that are presented on a computer screen. The participant is unaware of the number of choices to be made while performing the task. The participant is also unaware of the schedule of reward–punishment. The schedule is pre-set in such a way that card selection from deck A′ and B′ results in a ‘win’ of $100 per card, but five in ten cards selected from deck A′ result in a loss of $35–$150 and one in ten cards drawn from deck B′ results in a loss of $1,250. This produces a net loss of $250 over 10 trials if 10 cards that are selected are from decks A′ and B′. As a result, these two decks are considered disadvantageous in the long term. Selection from decks C′ and D′ results in a ‘win’ of $50 per card. However, five in ten cards drawn from deck C′ result in a loss of $25–$75 and one in ten cards from deck D′ results in a loss of $250. This brings the net profit per 10 cards from decks C′ and D′ to $250. Hence, these two decks are considered advantageous in the long term. Higher gains from decks A′ and B′ are associated with higher losses (net loss of $250 per 10 trials) and these decks produce short term or immediate rewards. However, they are bad choices in the long term. However, lower gains of decks C′ and D′ are associated with smaller losses (net gain of $250 per 10 trials) and though these decks are not highly rewarding in the short term, they bring delayed rewards or rewards in the long term. Most studies (e.g. Franken and Muris 2005; Suhr and Tsanadis 2007) analyze IGT performances based on net score method, i.e. the number of advantageous or safe choices (deck C′ and D′) minus the number of disadvantageous or risky choices (deck A′ and B′). The four decks also differ on punishment schedule, such that decks B′ and D′ dispense infrequent–higher magnitude punishments than decks A′ and C′, which have frequent–lower magnitude punishments (Bechara et al. 1994). In recent times, non-traditional ways of analyzing IGT choices i.e. based on punishment attribute (frequency of punishment) are being explored because a number of studies have reported a preference in IGT choice that is not based on traditionally analyzed reward attribute. Preferences based on attributes of punishment (loss) in IGT are not well understood, because of the disproportionate emphasis placed on traditionally focused reward attributes of IGT choices. As shown in Table 1, IGT choices differ on reward (decks A′ and B′ versus decks C′ and D′) and punishment attributes (decks B′ and D′ versus decks A′ and C′). Even though it is largely believed that there is no advantage in selecting cards based on punishment attribute, i.e. from infrequent punishment decks B′ and D′ or frequent punishment decks A′ and C′ (Dunn et al. 2006), a selective preference for deck B′ has been observed (Lin et al. 2007; van den Bos et al. 2006). Studies have shown a selective preference or avoidance for decks B′ and D′ (especially deck B′) among clinical patients and in some normal control groups (Bark et al. 2005; Fishbein et al. 2005; Ritter et al. 2004; Shurman et al. 2005; Toplak et al. 2005; Wilder et al. 1998). Choice of decks B′ and D′ is explained by a preference for low frequency punishment and high frequency reward (ratio of ‘wins’ to ‘losses’), but not on the basis of magnitude of reward–punishment (Shurman et al. 2005). On the other hand, magnitude of reward–punishment is found to guide IGT choices rather than long-term or short-term nature of reward, as is commonly believed (Tomb et al. 2002). Between frequency and magnitude, an ecological perspective suggests that frequency would be a more crucial determinant because it is also known to be salient (Wilder et al. 1998). On the other hand, magnitude could be easily remembered and have a much long lasting effect. Thus, a thorough investigation of preferences based on magnitude and frequency of reward and punishment is further warranted. Selective preference based on non-traditionally analyzed IGT choice attributes has opened up another area of investigation i.e., individual differences in IGT choices. There is a growing need to employ personality measures to understand IGT choice preferences (Davis et al. 2007), especially those based on less explored IGT choice attributes. Even though healthy participants have shown heterogeneity in their preferences for different decks, the contribution of individual differences to this heterogeneity has so far been largely ignored (Franken and Muris 2005; Suhr and Tsanadis 2007). Only recently have there been attempts to employ personality measures to explain heterogeneity and performance deficit on IGT (Davis et al. 2007). One such measure of interest is impulsivity. However, it was found to be unrelated to IGT performance when assessed through a measure of impulsivity and a decision making style questionnaire (Franken and Muris 2005). It produced counter-intuitive IGT results warranting further exploration (Davis et al. 2007). Its independence from reward–sensitivity also remains an unresolved issue (Franken and Muris 2005). A selective sensitivity to reward or punishment, as assessed through Behavioral Inhibition System (BIS) and Behavioral Activation System Scale (BAS) (Gray 1987), is the most commonly employed measure of disposition that is believed to influence IGT choices (Franken and Muris 2005; Peters and Slovic 2000; van Honk et al. 2002). However, the BIS-BAS measure has produced mixed findings in IGT studies. For instance, high BAS scores are associated with both good (Franken and Muris 2005) and bad IGT performance (van Honk et al. 2002). Another factor commonly explored within IGT is cognitive or trait-based disinhibition (Bechara et al. 2000). It was related to disadvantageous performance on a task similar to IGT, but was found to overlap with reward sensitivity instead of insensitivity to future outcomes that is primarily supposed to determine IGT performance (Crone et al. 2003). Over all, studies using self-reported measures to explain IGT choices so far have produced either counter-intuitive or inconsistent results. Therefore, there is a need to identify measures of individual differences that can account for IGT choice being made, based on various choice attributes embedded within IGT. This study is an attempt to employ measures of individual differences that account for either the reward attribute based on which IGT choices are normally analyzed to support SMH theory, or the punishment attribute, which is rarely analyzed in IGT studies. In this section, we elaborate on the rationale for selecting specific measures of disposition that can influence choice behavior in IGT. According to SMH, decision making in IGT is based on our ability to categorize our choices broadly as either good (profitable) or bad (risky) (Damasio 1994). This process of ‘labeling’ is said to narrow down the choice-set, enabling further efficient processing of a limited number of options. Thus, SMH is an account of a quick categorization performed at the neuro-anatomical level that can be a requisite for satisfying behavior (Simon 1955). Consistent reliance on such quick grading performed by somatic markers could be associated with a tendency to satisfied and make quick decisions under uncertain conditions. In the absence of such useful labels, there can be a prolonged search for an elusive ‘ultimate’ choice, which is characteristic of people with a tendency to maximize (Schwartz 2000). According to Schwartz et al. (2002), choices are made based on categorization of good (acceptable) or bad (unacceptable), and not with an intention to maximize or search for the ‘best’ option. These authors show that, contrary to the rational choice theory, people who maximize (maximizers) are more likely to experience regret as their choice-set expands. Given that the tendency to maximize versus satisfying and SMH theory depend on a heuristic categorization of options into ‘good’ or ‘bad’, we employ this as one of the measures to account for IGT choices, and explore whether the IGT choices are made based on either reward or punishment attributes. A dispositional measure of information processing based on the dual process theory is employed in this study. The dual process account suggests that people differ in two fundamental ways in the way they utilize and rely on information. One such account is the Cognitive Experiential Self Theory. This theory terms these two ways as experiential and rational information processing styles (Epstein 1983). Experiential thinking is defined as emotion-based, holistic, and rapid information processing; whereas rational thinking is characterized as logic-based, analytical, and slow processing. Somatic markers or emotion-based bodily responses could be thus more readily interpreted and used by those who display more of experiential processing style. Accordingly, experiential thinking is associated with greater reliance on heuristics (Shiloh et al. 2002) and preconscious processing of information (Epstein 2003). Conversely, rational processing is associated with an impaired capacity to interpret somatic information (Norris et al. Epstein 2003). Accordingly, rational information processing style shows a specific pattern of card selection on a modified version of IGT (Peters and Slovic 2000). However, whether there is an advantage in the subconscious pre-conscious processing of experiential processing or a disadvantage of rational processing is yet to be understood (Shiloh and Shenhav-Sheffer 2004). By employing a measure of experiential and rational information processing, we hope to explore if the two styles can account for IGT choices, based either on traditionally analyzed reward attributes or in terms of punishment attributes within IGT choices. Another measure of disposition that can influence IGT choices is risk, which is known as a cognitive and emotion-based evaluation of choice (Loewenstein et al. 2001). Performance on IGT is known to display deficits in risk estimation (Bechara et al. 1994). Moreover, individual differences in risk perception were found to be correlated to neural circuitry involved in IGT performance (Fukui et al. 2005). Hence, we employ a measure of risk attitude where perceived risk, perceived benefit, and likelihood of engaging in risky behavior are assessed to produce a single measure of risk attitude (for a detailed review, see Weber et al. 2002). Thus, if IGT choices are made based on reward attributes, a higher propensity to take risk as indicated by a higher risk attitude will be associated with more number of choices that produce immediate rewards rather than long-term rewards. This exploratory study, to our knowledge, is the first to utilize these three measures of dispositions to explore heterogeneity based on either reward or punishment attributes in IGT choices. It is also worth noting that the measures employed, namely, maximization-regret scale, rational-experiential (information processing) inventory (REI), and risk attitude scale, as discussed in the earlier section, are employed because they are more likely to be associated with IGT choices based on the commonly analyzed reward attribute, rather than punishment attribute of IGT choices. Thus, the aim of this study is: (1) to analyze IGT choices based on a comparison between the traditionally analyzed reward attribute and the less explored punishment attribute of IGT choices, and (2) to explore whether the measures of individual differences employed can account for a selective preferences in IGT choices based on either traditionally analyzed reward (short-term vs. long term reward) or non-traditionally analyzed punishment (frequent-low magnitude versus infrequent high magnitude punishment) choice attribute. A brief description of each measure of individual differences is provided in the next section.",10
8.0,1.0,Mind & Society,12 February 2009,https://link.springer.com/article/10.1007/s11299-009-0054-5,Consumer decision in the context of a food hazard: the effect of commitment,June 2009,Michele Graffeo,Lucia Savadori,Rino Rumiati,Female,Female,Male,Mix,,
8.0,1.0,Mind & Society,10 July 2008,https://link.springer.com/article/10.1007/s11299-008-0047-9,When happiness pays in negotiation,June 2009,Davide Pietroni,Gerben A. Van Kleef,Rino Rumiati,Male,Male,Male,Male,"In day-to-day interaction, in business, and in international relations, negotiation provides an important means to escape social conflict. Negotiation can be defined as “the process whereby two or more parties attempt to settle what each shall give and take, or perform and receive, in a transaction between them” (Rubin and Brown 1975, p. 2). It can also be described as a process of joint decision making between interdependent individuals with apparent divergent interests (Pruitt and Carnevale 1993). One weakness that these definitions have in common is that they do not reflect that negotiation is an emotional process. In many cases, the divergent interests that lie at the heart of social conflict and negotiation give rise to intense emotions, which may strongly influence the course of the negotiation (Barry and Oliver 1996; Van Kleef et al. 2004a). How do emotions affect the bargaining process and outcomes? Who achieves a more advantageous agreement, the negotiator who expresses anger or the negotiator who expresses positive emotions during the bargaining process? Surprisingly, the role of emotions—which seems so central to situations of conflict—has only received limited attention in the bargaining literature, and these scarce investigations arrived to inconsistent conclusions about the direction of the interpersonal effects of emotions on the negotiators’ outcomes. In the current article, we address the interpersonal effects of emotions in bargaining on both material and relational outcomes, and in particular we focus on the effects of expressed anger and happiness during negotiation.",12
8.0,1.0,Mind & Society,10 February 2009,https://link.springer.com/article/10.1007/s11299-009-0053-6,Expectations and social decision-making: biasing effects of prior knowledge on Ultimatum responses,June 2009,Alan G. Sanfey,,,Male,Unknown,Unknown,Male,"An important recent development to emerge within the field of judgment and decision-making has been the integration of methods and results from several different disciplines, each of which has historically examined the decision-making process at different levels and with different methodologies. This interdisciplinary field, popularly known as Neuroeconomics (e.g. Sanfey et al. 2006), has begun to redress this lack of integration by seeking to better understand decision-making by taking into account the cognitive and neural constraints on this process, as investigated by Psychology and Neuroscience, while in addition utilizing the tasks and mathematical decision models that have emerged from Economics. By using these complementary strengths this approach offers a promising avenue to examine decision-making at different levels of analysis and, eventually, to perhaps arrive at a comprehensive account of how this process operates. One productive direction this emerging field has taken has been to investigate the processes underlying social decision-making. Typically, the experimental study of decision-making has examined choices in which the decision-maker selects between options that have direct consequences for only themselves. The canonical type of decision task investigated involves choices between monetary gambles—for example, participants might be asked whether they prefer a 50% chance of $100, or $50 for sure. Though the outcomes and likelihoods of these problems can be quite complex and uncertain, it is important to note that these decisions are typically made in ‘social isolation’, that is, the outcome rarely affects anyone other than ourselves. The study of these choices has illuminated many important aspects of decision-making. For example, they have demonstrated that much of our decision behavior deviates from the predictions of standard economic theories such as Utility Theory, and have led in turn to revised versions of these models that better capture actual decision-making behavior. However, it is still unclear to what extent these processes may also be harnessed for decisions in a social context. Given that humans live in highly complex social environments, many of our most important decisions are often made in the context of direct or indirect social interactions. Indeed, our everyday choices are often dependent on the simultaneous decisions of others, for example when we are deciding to ask someone on a date, selecting a job candidate, or entering a business negotiation. The nature of decision-making may change fundamentally when the outcome of a decision is dependent on the decisions of others. For example, the standard expected utility computation that underlies many of the existing theories and models of decision-making is complicated by the fact that we must also attempt to infer the values and probabilities of our partner or opponent in attempting to reach the optimal decision. These social situations (e.g. Loewenstein et al. 1989; Blount 1995) have been comparatively understudied in the decision-making literature, at least as compared to individual decisions, and so the integration of psychological methods allied to the tasks developed in Experimental Economics may allow for more detailed examination of this important process. One aspect of the Neuroeconomic approach, relevant to the present study, has been to utilize some of the tasks developed in Experimental Economics to address important questions regarding social decision-making. The set of methods used by Experimental Economics are easy to understand for players and straightforward to utilize in the laboratory, and require that players employ sophisticated reasoning about the motivations of their partners in the game. An additional advantage of using these tasks is that behavior can be mathematically modeled by Game Theory (von Neumann and Morgenstern 1947). Game Theory is a collection of rigorous models attempting to understand and explain situations in which decision-makers must interact with one another, with these models applicable to such diverse scenarios as bidding in auctions, salary negotiations, and jury decisions. From an experimental standpoint, the mathematical framework of Game Theory provides a formal common language in which findings from different research approaches can be compared, and deviations from model predictions quantified, as opposed to the more ad-hoc models used by Psychology. One specific focus of these games has been bargaining behavior, with the Ultimatum Game often used to examine both how negotiations are struck and how people respond to conditions of both equality and inequality. In the Ultimatum Game (Guth et al. 1982) two players are given the opportunity to split a sum of money provided by the experimenter. One player is deemed the proposer and the other the responder. The proposer makes an offer as to how this money should be split between the two. This player is free to propose any split they want, from abjectly unfair (“I will keep all the money”), to fair (“We will split the money evenly”). The second player (the responder) then must make a decision, to either accept or reject this offer. If the offer is accepted the money is split as proposed, but if the responder rejects the offer then neither player receives anything. In either event, the game is over. Importantly, both players are fully aware of the rules of the game at all times. The standard solution to the Ultimatum Game, assuming that people are motivated purely by financial self-interest, is for the proposer to offer the smallest sum of money possible to the responder. The responder, in turn, accepts this offer on the reasonable grounds that any monetary amount is preferable to none. However, hundreds of studies indicate that, at least in industrialized cultures, modal offers are typically around 50% of the total amount, demonstrating that proposers do not behave in accordance with standard models, and instead are willing to be more generous than predicted. Responders are also quite consistent. Low offers (of 20% of the total or less) have about a 50% chance of being rejected. This very robust finding is intriguing, demonstrating that circumstances exist in which people are motivated to actively turn down monetary reward. In general, the probability of rejection increases substantially as offers decrease in magnitude. Thus, people’s choices in the Ultimatum Game do not conform to a model in which decisions are driven purely by financial gain. Since the Ultimatum Game was introduced, there have been many examinations of both proposer and responder behavior (for a useful summary of the principal findings, see Camerer 2003), often undertaken with the goal of finding the circumstances under which players would act as the standard model predicts. However, the more obvious manipulations, such as higher stakes and anonymity of partners, have shown only modest effects on behavior. For example, List and Cherry (2000) examined Ultimatum behavior in games with both $20 and $400 stakes, and found little difference in either proposer or responder behavior across the different amounts. As with stake amounts, there is a weak effect of anonymity on Ultimatum behavior, namely that offers to anonymous (as compared to non-anonymous) partners are slightly lower, as are rejection rates (Bolton and Zwick 1995). Demographic variables have also been studied, though not as extensively, with again relatively weak effects found across many variables. For example, gender appears to have an effect on rejections, with males and females offering similar amounts but females rejecting less often (Eckel and Grossman 2001). The robust patterns of behavior in the Ultimatum Game have also led to revised formulations of Game theoretic models, with social preferences included as an important determinant of behavior. However, the primary question of interest to decision-making researchers is why people reject offers, that is, what motivations underlie the decision to actively reject money. The game is so simple that it is improbable that these rejections are due either to a failure to understand the rules of the game or an inability to conceptualize a single-shot interaction with a partner. Based on participant reports, it appears that low offers are often rejected following an angry reaction to an offer perceived as unfair (Pillutla and Murnighan 1996; Xiao and Houser 2005). Objecting to unfairness has been proposed as a fundamental adaptive mechanism by which we assert and maintain a social reputation (Nowak et al. 2000), and the negative emotions provoked by unfair treatment in the Ultimatum Game can lead people to sacrifice sometimes considerable financial gain in order to punish their partner for the perceived slight. Neuroimaging studies have provided further evidence for emotion-based rejection of unfair offers. A functional magnetic resonance imaging (fMRI) study (Sanfey et al. 2003) examined unfair behavior in the Ultimatum Game, and found a brain area, namely the anterior insula, that exhibited greater activation as the unfairness (i.e. inequity) of the offer increased. Further, this area was more active when playing with another human than with a computer partner, and, importantly, the activation of this area reliably predicted the player’s decision to either accept or reject the offer, with rejections of unfair offers being associated with significantly higher activation than acceptances of these offers. The presence of anterior insula activations in this study is notable, as this brain region is also responsive to physically painful and disgusting stimuli (Derbyshire et al. 1997; Calder et al. 2001). In addition, it is involved in mapping physiological states of the body, including touch and visceral sensations of autonomic arousal (Critchley et al. 2000), as well as in aversive conditioning. These results suggest that anterior insula and associated emotion processing areas may play a role in marking a social interaction as aversive. Further, in two recent studies, both patients with lesions to the ventromedial prefrontal cortex, a brain area associated with emotional processing (Koenigs and Tranel 2007), and normal players primed with a negative emotional state (Harle and Sanfey 2007) reject unfair offers more frequently than controls, further suggesting that dysregulation of affective processing has consequences for this type of social decision-making. Therefore, initial investigations of responder behavior in Ultimatum Game have proposed an account whereby an affective-based neural system is activated by conditions deemed unfair by the player, with the resultant decision often being to reject the unfair offer, even at a financial cost to oneself. However, one question that has been unanswered is to what extent expectations of fairness may mediate this emotional reaction to inequality. That is, does expecting to be treated unfairly by a partner in the Ultimatum Game reduce this negative emotional response, and thereby increase the probability that these offers will be accepted? Or rather is our definition of, and reaction to, unfairness so ingrained that expecting good or bad outcomes will have no difference to our subjective feeling upon receiving an unfair offer? The present study sought to address this question. Expectation effects within Psychology have been detailed in many different forms and contexts over the years. For example, the well-known placebo effect demonstrates that inert pharmaceutical compounds such as sugar pills are shown to be as efficacious as actual medication, at least for a sizable minority (usually around 30%) of the population. This is one example of the impressive degree to which our expectations shape our interpretations of the world and our response to stimuli (Amanzio and Benedetti 1999). On a more prosaic level, expectations of quality or enjoyment can affect the judgment of many consumer products. For example, in a set of studies (Klaaren et al. 1994) participants’ expectations about upcoming vacations were shown to influence their post-vacation evaluations, and expectations about a movie’s quality influenced the enjoyment of the film. Additionally, several studies have examined the effect of expectations of a beer’s brand identity (Allison and Uhl 1964) or constituent ingredients (Lee et al. 2006) on judgments of taste, both demonstrating the strong effect of expectations on these judgments. In a similar vein, expectations primed via pricing can play a powerful role in judgments. One study (Shiv et al. 2005) demonstrated that consumers who paid a discounted price for an energy drink appeared to yield a lesser benefit from this product than users who paid full price, a result the authors claim is mediated by the expectancies of efficacy that price provides. More recently, functional neuroimaging has been used to examine the neural correlates of expectation in relation to judgment, with these studies corroborating the behavioral examinations of expectations. The retrieval of brand information in a cola-tasting study was associated with both increased judgments of quality and specific neural activations (McClure et al. 2004). Recently, an imaging study of wine-tasting (Plassman et al. 2008) reported that increasing the sales price of a specific wine increased behavioral reports of quality, as well as activating areas of the prefrontal cortex. Social decision-making contexts such as negotiation have also been used to examine the effect of expectations, with emotional manipulations often employed to assess how expectations can alter decision-making. Van Kleef et al. (2004) reported that participants in a negotiation task were more willing to concede ground to an opponent when the opponent demonstrated anger, with this anger used by the participant as a signal to the opponent’s limits. Of more immediate relevance, Kopelman et al. (2006) found that negotiators who strategically displayed negative emotion prior to making a UG offer were more likely to elicit rejections than those displaying either neutral or positive emotions, independent of offer amount. In the domain of social decision-making, one study has used Game Theory and functional neuroimaging to examine a related question, showing that declarative information previously learned about a partner can greatly modulate decision behavior when paired with that person. In this study (Delgado et al. 2005), participants saw general personality information about partners prior to playing a Trust Game with these people. This consisted of vignettes regarding the moral character of the partner, with each described as either a morally positive or a morally negative person. This prior knowledge led to biases in participants’ trust behavior, with a consequent reduced activity in reward-related brain areas in response to partners’ game behavior. The authors’ interpretation of these data is that responses to the direct actions of another can be reduced when we have been led to expect a certain pattern of behavior. This suggests that prior social knowledge about a particular partner can reduce the degree to which we directly learn from actual behavior, and demonstrates what has been called a “top-down” influence on social decision-making. Though this study is suggestive, to date there have been no studies that have explicitly manipulated the general expectations of players in Game Theoretic tasks (that is, what most other people will do, as opposed to what a particular partner might do), and then directly observed the consequences on their behavior. Additionally, several studies within Experimental Economics have addressed related questions to the one of interest here. For example, a number of experiments have examined the effect of comparing the to-be-decided-on offer to one from a different proposer (e.g. Knez and Camerer 1995; Cason and Mui 1998). These studies report rather modest effects of this so-called ‘outside option’; with rejection rates increasing slightly when information is provided that other responders are receiving more. This suggests that additional information about the proposer’s offer is sometimes used by responders in deciding what they are willing to accept. In a more recent study, Bohnet and Zeckhauser (2004) provided responders with the average amount offered by the group of proposers who participated in the experiment, prior to the responder decision to accept or reject. The presence of this information indeed had an effect on rejection rates, with higher rejection rates observed when participants knew the average amount on offer. Though these results are intriguing, questions remain. In this study, expectations were not directly manipulated, and the average amount revealed to responders was reasonably high (approximately 35% of the pot). It is therefore still an open question whether ‘low’ expectations, that is, information that the average offer is quite unfair, would still have an effect on behavior. Additionally in this study, proposers were told, before submitting their own offers, whether or not the average amount would be revealed to responders. This had the effect of actually increasing offers in the expectation condition, making direct comparisons between the ‘expectation’ and ‘non-expectation’ group more challenging. Nonetheless, these results do suggest that providing responders with information concerning typical proposer behavior could have important effects on decision behavior in an Ultimatum Game. Taken in total, the above results suggest that expectations of fairness and unfairness may well have a significant impact on social decision-making, although to date this question has not been directly addressed by explicitly manipulating the fairness expectations of participants. To answer this question, the present study provided different groups of players with information concerning ‘typical’ patterns of proposer play in the Ultimatum Game, informing participants that proposers are usually quite fair or unfair in the offers they make (though to avoid potential demand characteristics, the words fair and unfair were not used in these instructions). Then, participants played the Ultimatum Game as responder, and we observed the effects of these expectations on decision behavior in the game. If, as expected, these expectations had noticeable effects on the patterns of responses, for example if higher expectations of fairness led to higher rejection rates, this would have important implications for models of fairness and equality, and would further contribute to the literature on how expectations and norms can drive behavior in a novel context.",43
8.0,1.0,Mind & Society,26 September 2008,https://link.springer.com/article/10.1007/s11299-008-0049-7,Giving or taking: the role of dispositional power motivation and positive affect in profit maximization,June 2009,Markus Quirin,Martin Beckenkamp,Julius Kuhl,Male,Male,Male,Male,"The nature of selfish versus prosocial behaviour in the context of limited resources challenges many disciplines like philosophy, biology, sociology, economics, and psychology. Despite high interest in this issue, the factors that influence socio-economic decisions into the direction of selfishness or generosity have not been fully identified yet. And indeed, the situation, the actor’s personality and affective states, and not least, interactions among these factors may influence socio-economic decisions. The present research investigates how the power motive, specifically the need for dominance, and the level of positive affectFootnote 1 influence independent socio-economic decisions, i.e. decisions taken without being influenced or receiving feedback by parties who are concerned in terms of being at advantage or at disadvantage (e.g., decisions about donations). Important aspects of such independent decisions have been investigated in experimental economics via the so-called dictator game. In this game, one player makes a proposal of how to share sums of money to an anonymous other, whereas the other player does not have any influence on the decision. What factors may influence decisions in such a situation? Rational choice theory (RCT; cf. Heath 1976) assumes that people base decisions exclusively on economic, instrumental considerations, which is nicely expressed in the tag of man as homo oeconomicus (Rubinstein 1998, p. 8 f.). Thus, RCT would predict that individuals always take decisions that maximize their profit. Within this classical conception of man as homo oeconomicus, selfishness and rationality go hand in hand and are synonymous. However, numerous experiments demonstrated that it is not the case that persons in a situation of independent decisions and anonymity are purely selfish and decide not to share (cf. Camerer 2003). Likewise, it could be demonstrated in numerous other experimental situations involving socio-economic decisions that people deviate systematically from the basic assumptions of classic RCT, suggesting that either influential constraints are not considered or the models are basically imperfect (e.g., Kahneman and Tversky 1996). Reviewing the extant literature on trust games, McCabe et al. (2003, p. 268) concluded that “there is ample evidence suggesting that a considerable proportion of play in two-person trust games deviates from that predicted by standard non-cooperative game theory—“a significant percentage of anonymously paired subjects arrive at cooperative outcomes.” Consequently, people often are much less selfish than RCT would expect. Such results stimulated theoretical work on reciprocity and inequity aversion (cf. List 2007; Henrich et al. 2005). But what kind of factors may moderate the balance between selfish and prosocial behaviour in rational decisions? As argued here, it is plausible to assume that individuals differ in the degree to which they behave individualistically or cooperatively. As outlined in the following section, this behavioural dichotomy may be associated with a personality trait commonly referred to as the power motive, and this is why we were interested in investigating how this variable predicts socio-economic decisions. Additionally, we argue that many results referring to the influence of affects on socio-economic decisions may be explained by a dynamic motivation approach. We think that such an approach may enrich current research about socio-economic decisions and mental states and may have the potential to integrate inconsistent results from this area of research.",7
8.0,2.0,Mind & Society,15 July 2009,https://link.springer.com/article/10.1007/s11299-009-0067-0,Special issue on: Social simulation,December 2009,Rosaria Conte,,,Female,Unknown,Unknown,Female,"Previous considerations do not necessarily suggest rosy expectations. Movements at the frontiers between adjacent fields of science, as well as present directions and objectives of funding programmes, suggest caution. First, the prestige of mathematical modelling not only was poorly affected by the generative paradigm launched by Epstein (2006), but also leads mainstream communities to a defensive and antagonist attitude towards simulation. In economics, for example, experimental (natural or artificial) findings not supported by mathematical demonstrations are not published in the top ten economic journals (I take this from private conversations with economists of well-established international reputation). This is not the forum for a remake of the epistemological discussion about simulation, which goes on from the dawn of the field. The question is rather how turn major divisions into occasions of scientific mutual interchange and development. Some scientific trends represent both challenges and opportunities for further development of simulation modelling. Specific developments of disciplines currently point into diverging directions, which might affect simulation in complex, not easily predictable ways. Consider neuroscience. Looking at one very well-known aspect of it, i.e. brain imaging, we are probably entitled to pessimism. Currently available techniques of brain imaging result in a challenge for behavioural, including simulation, modelling: rather than explaining how behaviour unfolds, they are aimed at localizing specific areas of the brain responsible for it. On the other hand, simulation is likely to be progressively applied in subfields like hybrid agent architectures (e.g., Gray 2007), in which neural and sensory motor aspects should be integrated with representational aspects of action. In turn, these will probably find application in robotics, a strategic field strongly emphasized in the 7th European FW and likely to maintain a strong leadership in the next few years. At the same time, the role of simulation is likely to increase in subfields of evolutionary studies, ethology and primatology, where there is a strong need for testing operational models. These fields might receive impulse from simulation based comparison among the competitive advantages of different mental or behavioural features in different species or in the same species at different levels of evolution. Analogous considerations apply to evolving social systems and institutions, where simulation modelling might help apply the evolutionary algorithm not only to social groups, like in group-selection theory, but also to social and cultural artefacts. Why then should not we expect that future ESSA conferences have satellite workshops or special sections on such subfields as Simulated Ethology, Artificial History and Artificial Law? Let us work for that.",
8.0,2.0,Mind & Society,14 July 2009,https://link.springer.com/article/10.1007/s11299-009-0066-1,Special issue on: Social simulation,December 2009,Flaminio Squazzoni,,,Male,Unknown,Unknown,Male,"This special issue consists of a collection of selected papers that were presented at the fifth conference of the European Social Simulation Association held in Brescia 1st to 5th September 2008. Although not intentionally aimed at providing a close representation from the wide variety of topics presented at the conference, it does exemplify the multi-faceted research of this community that revolves around social simulation. Presently chaired by Rosaria Conte (Scott Moss, Nigel Gilbert and Wander Jager as past-presidents), The European Social Simulation Association (ESSA) was originally established in 2003 to bring together scientists from various disciplines interested in promoting social simulation research, education and application in Europe. It also cooperates with other similar scientific associations established in the US (NAACSOS) and in Japan (PAAA). Although relatively recent, the association now has about four hundred members, a particularly successful newsletter (now managed by Nick Gotts with past contributions from David Hales, Frédéric Amblard, Shah Jamal Alam and Bogdan Werth). It has five special interest groups on topics such as market dynamics, policy, social conflict, reputation and societal transitions and actively promotes conferences, workshops, seminars and education initiatives, particularly addressed to post-graduate and PhD students. ESSA scientists originate from various disciplines, including social sciences, cognitive sciences, political sciences, economics, computer sciences, ecology, artificial intelligence, philosophy and physics, all share a common objective of explaining social phenomena by using computer simulation models. This brief introductory paper aims to put these papers into context and to highlight the main challenges facing social simulation.",
8.0,2.0,Mind & Society,17 May 2009,https://link.springer.com/article/10.1007/s11299-009-0058-1,‘Binge’ drinking in the UK: a social network phenomenon,December 2009,Paul Ormerod,Greg Wiltshire,,Male,Male,Unknown,Male,"In this paper, we analyse the recent growth of ‘binge’ drinking in the UK. By this, we mean the rapid consumption of large amounts of alcohol, especially by young people, leading to anti-social behaviour in urban centres. British soccer fans have often exhibited this kind of behaviour abroad, but it has become widespread amongst young people within Britain itself. Vomiting, collapsing in the street, shouting and chanting loudly, intimidating passers-by and fighting are now regular night-time features of many British towns and cities. A particularly disturbing aspect is the huge rise in drunken and anti-social behaviour amongst young females. The phenomenon is of serious concern to the British government, not merely for the anti-social behaviour related to it, but because of the longer term health implications for young people of massive intakes of alcohol in very short periods of time. Martinic and Measham suggested that the experience of the UK is part of a more general pattern of heavy, rapid drinking which is emerging in a number of countries of the world, and uses the phrase ‘extreme drinking’ rather than that of ‘binge’ drinking to characterise it (Martinic and Measham 2008). There is a growing literature which demonstrates the importance of social networks for consumer choice in what might be termed ‘regular’ consumer markets (Gladwell 2000). The concept of the ‘tipping point’ is used to explain on why some books, films and music emerge out of obscurity with small marketing budgets to become popular hits when many a priori indistinguishable efforts fail to rise above the noise. A much more formal analysis of the importance of social networks in determining success or failure in the film industry was published by Vany and de Walls (1996). In many social and economic contexts, individuals are faced with a choice between two alternative actions, and their decision depends, at least in part, on the actions of other individuals. Schelling (1973) describes this class of problem as one of ‘binary decisions with externalities’. An important feature of such systems is that they are ‘robust yet fragile’ (Watts 2002; Ormerod and Colbaugh 2006). In other words, behaviour may remain stable for long periods of time and then suddenly exhibit a cascade in which behaviour changes on a large scale across the individuals within the system. Two recent American studies (Christakis and Fowler 2007, 2008) using the Framingham Heart Study data baseFootnote 1 have demonstrated the importance of social networks in determining the behaviour of individuals on matters of public health, specifically obesity and smoking. The Framingham data base contains detailed information on over 12,000 individuals, monitored over more than three decades since 1971. The social networks of individuals on this data base have been important determinants of both the spread of obesity and the reduction in smoking over this period. In terms of obesity, for example, the chance of any individual being obese increased by 57% if he or she had a friend who became obese. When a spouse stopped smoking, the other was 67% less likely to smoke. The aim of this paper is to examine the extent to which the sudden emergence of the binge drinking problem in the UK can be explained as a social network phenomenon. In other words, we are investigating whether imitation on social networks is a sufficient condition to account for the recent rapid rise in binge drinking. Models based on completely different assumptions may also be able to replicate key features of the data. Here, our purpose is to establish whether imitation on social networks is sufficient to explain the rise. We use the methodology developed in earlier research (Ormerod 2007) where we consider a small amount of straightforward and readily accessible information. An agent based model is set up in which agents face the binary decision on whether or not to binge drink. Transmission of binge drinking behaviour across agents connected on a social network is determined according to a threshold rule. The theoretical model is calibrated against empirical evidence. We deduce from this, using an agent-based model, the type of social network across which information flows and agents influence each other’s behaviour in this context. Specifically, we show that information appears to flow across a small world network. The approach described here can be used more generally in areas where policy makers are interested in regulating and altering agent behaviour. An important aspect of the methodology is that it is feasible both to construct empirically grounded agent based models and to draw useful implications from them, whilst at the same time requiring only small amounts of data. Section 2 describes the basic data, Sect. 3 the initial evidence for the existence of imitative behaviour on social networks, Sect. 4 the theoretical model and results. Section 5 examines the robustness of the results, and Sect. 6 gives a brief discussion.",27
8.0,2.0,Mind & Society,04 July 2009,https://link.springer.com/article/10.1007/s11299-009-0063-4,Normal = Normative? The role of intelligent agents in norm innovation,December 2009,Marco Campenní,Giulia Andrighetto,Rosaria Conte,Male,Female,Female,Mix,,
8.0,2.0,Mind & Society,26 May 2009,https://link.springer.com/article/10.1007/s11299-009-0056-3,Modelling the emergence and dynamics of social and workplace segregation,December 2009,Mohamed Abdou,Nigel Gilbert,,Male,Male,Unknown,Male,"According to the Contact Hypothesis (Amir 1969; Allport 1954), contact between different social (ethnic, racial and/or religious) groups promotes trust, tolerance and social integration. When individuals’ social networks become more segregated (that is, social relationships are formed with others of similar gender, race, ethnicity, and/or religion) the chance of their contacts crossing their group’s boundary is reduced, leading to increasing levels of stereotyping and intolerance. Similarly, workplace segregation may also reduce the chance of intergroup contact. In addition to these negative social effects, workplace segregation may have negative economic consequences. For example, it may introduce high levels of income and employment inequality among social groups (Carrington and Troske 1998; Glass 1990; Granovetter 1995; Tassier and Menczer 2008). Moreover, workplace segregation may also affect the economic system as a whole through its effects on the efficiency of allocating workers to jobs (Becker 1971). Although both act as barriers against intergroup contact, workplace segregation and social segregation (segregation of individuals’ social networks) have been studied as two separate areas of research. Moreover, research (especially on workplace segregation) has focused mainly on the consequences (for example income and employment inequality) rather than the determinants and dynamics of segregation. In research on social segregation, the homophilous attitudes of individuals have been considered the main determinant of segregation (Amir 1969; Allport 1954). The homophily principle implies that people are attracted by nature to others similar to themselves, and are more likely to create social ties with them as described by the proverb “birds of a feather flock together” (McPherson et al. 2001). This similarity among individuals may be evaluated on the basis of gender, race, religion and ethnicity or other factors. On the other hand, in research on workplace segregation, referral hiring (hiring through social or familial contacts) has been considered the main source of segregation. Using social and familial contacts to search for jobs is a widespread practice. Granovetter (1973, 1995) found that more than half of workers in the U.S. knew about their jobs through informal channels (friends, relatives, and other social contacts). In a recent study conducted by the authors about workplaces and social networks in Egypt, 65% of workers used these informal methods of seeking employment. Elliott (2001) explains how referral hiring could promote workplace segregation: [Referral hiring] creates a built-in bias toward incumbents: members of a particular ethnic group concentrate in particular jobs and when new employment opportunities become available at their workplace, they pass this information along to [their] social contacts, often of the same race and ethnic background”(p. 401) According to Elliot’s analysis, the relationship between social segregation and workplace segregation has been assumed to be a one-way causal relationship. In this relationship, the independent variable is the level of social segregation and the dependent variable is the level of workplace segregation, with level of referral hiring as a moderating variable (see for example Elliott 2001; Tassier 2005; Tassier & Menczer 2008). When using referral hiring for employment, the effect of segregated individual social networks would be expected to increase workplace segregation. In the current work, we argue that the relationship between social and workplaces segregation can also go in the other direction; that is, workplace segregation can affect (as well as be affected by) social segregation. Empirical literature confirms that high percentage of our social relations are embedded in organizations (Grossetti 2005). Organizations contribute to the construction of the pool of candidates whom people might create social relations with. Thus, when these pools become segregated social segregation is promoted and vice versa. In this paper, we introduce a general framework to study the dynamic and reciprocal relationships between social segregation and workplace segregation, homophily levels, and referral hiring. An agent-based simulation model for the labour market has been developed based on this framework. The model creates an artificial society where agents (people) use their social networks to search for jobs. As simulated time passes, agents change their social networks by creating new social links (ties) with each other while other links dissolve. Also, the composition of workplaces (the proportion of workers from different societal groups) may change through the turnover of workers. The model describes this process of continuous change in the composition of workplaces and social networks of agents, and how it affects levels of workplace segregation and the segregation of the social networks of agents. We argue here that agent-based modelling (ABM) is an appropriate methodology that fits well the purpose of the current work. Our main objective is to understand the co-emergence and dynamics of two complex phenomena; that is, social and workplace segregation. Both involve non-linear interactions among heterogeneous agents on different levels (for example, workers and firms), which makes computational models more flexible than mathematical and statistical models (Gilbert and Troitzsch 2005). Besides, agents provide a good analogy to individuals; therefore, ABM is more appropriate than other computational techniques when dealing with interactions involving people. For these reasons ABM has been successfully used in segregation research since the famous Schelling (1971) model of residential segregation. This paper is organized as follows. A brief review of related research is presented in Sect. 2. In Sect. 3, we introduce the proposed framework for the relationship between social segregation and workplace segregation. Then, in Sect. 4, the agent-based simulation model is presented. The results of the simulation model are discussed in Sect. 5. In Sect. 6, we discuss how the model was validated against different sources of empirical data. Finally, the conclusion and summary of the main findings are presented in Sect. 7.",8
8.0,2.0,Mind & Society,26 May 2009,https://link.springer.com/article/10.1007/s11299-009-0060-7,The social transmission of choice: a simulation with applications to hegemonic discourse,December 2009,Edmund Chattoe-Brown,,,Male,Unknown,Unknown,Male,"Despite criticism of Rational Choice Theory (RCT), there is a formulation of it that seems very hard to refute. If actors know a set of actions and their payoffs at a particular moment and fail to make use of that information in choosing, they would appear to be irrational not just in the technical but also in the everyday sense. One might call them forgetful, weak willed or wishful (Elster 1989, pp. 13–21). There is already extensive discussion of RCT which questions our ability to know payoffs a priori or to apply the utility maximising principle in cases of uncertainty (Tintner 1941a, b). However, in this paper, I will consider a different question with important implications for both RCT and sociology. How does a choice come to be characterised as it does even if, once so characterised, it can still be made using pure “economic” rationality? Recently RCT has developed models where agents differ in their decision procedures for the same choice (Axelrod 1980a, b; Nowak and Sigmund 1992, 1993) or use information from other agents to improve their performance, when they cannot evaluate payoffs to actions a priori for example. These learning models of various kinds (Arifovic 1994; Banerjee 1992; Bikhchandani et al. 1998; Becker 1991; Dawid 1999; Ellison and Fudenberg 1995; Macy 1991; Peyton Young 1993; Riechmann 2001; Scharfstein and Stein 1990) lead to both encouraging findings—rational outcomes can be achieved from limited rationality and imitation (Epstein 1997) and perverse ones—initial mistaken decisions can be “locked in” at the collective level by imitation (Becker 1991). However, to my knowledge, none of this wide family of interesting models rejects the assumption that all agents share a perception of the choice set. The only exceptions are a few papers in game theory which recognise this problem (christening it framing) but do not explain it (Bacharach 1993; Bacharach and Bernasconi 1997; Bacharach and Stahl 2000; Sugden 1995). In these models, it is recognised that players may be in different awareness states. For example, suppose that two agents need to meet and they cannot communicate. Each has a map of a particular area showing a number of houses represented as coloured boxes. Both have to choose a house and if they choose the same, that is the good outcome. All other outcomes are bad because they do not meet. If there is nothing to distinguish the houses then agent one faces the problem of trying to assess what agent two will choose, knowing that agent two faces the same difficulty. This is a classic coordination game: It does not matter which choice you make as long as you make the same as the other player. The framing issue arises if, owing to a printing fault, there is a very minor colour difference in one of the marked houses on the map. If it is sufficiently distinct that both players spot it, it solves their coordination problem since that house ‘stands out’ as the one to choose. On the other hand, if the colour difference is relatively minor, each player has to assess how likely the other is to be in a particular ‘noticing state’ before deciding whether co-ordination on the oddity is an improvement over randomisation. Here the two ways that the agents may frame the choice are “choose randomly between ten identical houses” or “choose randomly between nine identical houses or choose the distinct house”. Although this example shows how people face different choice sets in an everyday way, the framing literature makes no attempt to explain why individuals see choices as they do. This is partly because, in the example, framing is the result of a perceptual process (noticing). In this paper, however, I shall explain frames as the outcome of a sociological process (communication) with a psychological component (innovation). To recap then, this paper considers a situation in which not only the payoffs and probabilities of chosen actions may be uncertain or unknown (already well explored in RCT), but also actions may be unknown (apparently neglected to date). This means that, supposing there are five possible actions in some situation (A–E), agents may be aware of the same subset (A, B and C), different but overlapping subsets (A, B and B, C) and different and non-overlapping subsets (A, B and C, D) as well as the full set. The next section describes the assumptions and design of a simulation to explore the phenomenon of choice sets with varying sizes. Section 3 describes and analyses the results with particular reference to the possibility of “hegemonic choices” created by a numerically dominant group. The final section concludes and discusses further possibilities for research.",
8.0,2.0,Mind & Society,17 May 2009,https://link.springer.com/article/10.1007/s11299-009-0059-0,Investigating the force multiplier effect of citizen event reporting by social simulation,December 2009,Mark A. Kramer,Roger Costello,John Griffith,Male,Male,Male,Male,"Citizen event reporting (CER), in which concerned citizens are recruited to volunteer information, can be helpful to organizations seeking to bolster information gathering. This approach has most commonly been applied to crime-fighting efforts. By employing the local citizenry, many crimes that would otherwise go undetected and unpunished can be prosecuted. Examples of traditional CER include: 
Neighborhood Watch (http://www.usaonwatch.org). Founded by the US National Sheriff’s Association, USAonWatch.org provides resources to communities interested in starting or improving Neighborhood Watches. 
Crime Stoppers International (http://www.c-s-i.org). This organization takes tips and gives rewards to assist criminal investigators. The organization claims 700,000 arrests and seizure of $9 billion in stolen property and illicit drugs worldwide through the end of 2008. 
DropaDime.net (http://www.dropadime.net) is a web-based, central location for reporting any type of crime or fraud. 
WeTip (https://www.wetip.com) is a US nonprofit providing anonymous phone answering and tip collection service concerning school safety, corporate ethics, illegal drugs, and more, claiming 15,000 cases solved and 8,000 criminals convicted since 1972. While CER itself is not new, widespread wireless communication has facilitated a host of new reporting possibilities, such as on-the-spot pictures or video, geo-location data, mobile applications, map-based visualizations, and real-time peer communication. Social media such as Twitter produced timely and detailed information in the recent Mumbai terror attacks (Beaumont 2008), which could help guide first responders. Some of the emerging application areas for CER include environmental monitoring (http://www.cybertracker.org), disaster response (http://www.instedd.org; Schellong 2006), election monitoring (Kinkade and Verclas 2008), and disease monitoring (Georgia Division of Public Health 2008). Another promising application of CER emerges in the context of asymmetric warfare. Unlike traditional force-on-force warfare, in asymmetric warfare the enemy is blended into the population at large, emerging only momentarily to mount attacks. This strategy presents severe challenges for traditional information gathering systems, which rely on sophisticated sensors to detect troops, facilities, and equipment. While effective against fixed facilities, large mobile equipment, and traditional forces, these techniques are less likely to detect hostile individuals and loosely-coupled groups. The conflicts in Iraq and Afghanistan have highlighted the need to gather information known primarily by the local populace. Of particular concern is detection of improvised explosive devices (IEDs), bomb-making facilities, location of hostages, and identification of militant insurgents and terrorists. Soldiers on patrol are highly motivated to locate bomb-makers and IEDs; however, civilians far outnumber troops in urban battlefields. Civilian reports on suspicious activities and devices represent a potentially significant force multiplier.Footnote 1
 There are many challenges to making CER work in hostile situations, where foes constitute a significant subgroup in an otherwise friendly or malleable population. The first challenge concerns citizen participation, which may be discouraged through social pressure, threats and intimidation. Traditional CER systems rely both on explicit incentives (monetary rewards) and social/indirect incentives (e.g., reduction in neighborhood crime) to encourage participation. Many CER systems also assure safe, anonymous reporting to reduce fear of reprisal, having to speak directly to police, and potential legal entanglements. While traditional incentives should be helpful in hostile situations, they may not necessarily be sufficient to cause an occupied population to cooperate with an occupying force. While important, issues of participation, incentives, and rewards are not discussed further in the current paper. The second challenge concerns the trustworthiness of information gathered from the population. In situations involving insurgency and tribal rivalry, one cannot assume that citizens who report events are well-meaning or reports are truthful. Enemies may attempt to “game” the system by reporting false events designed to elicit certain responses. There are several reasons a foe might attempt to manipulate a CER system: 
To harm first responders. An insurgent wishing to harm first responders or degrade the emergency response system might use the CER system to lure troops into ambushes, or attack them with roadside bombs (Stojanovic 2004). Misinformation designed to evoke the desired response from first responders might take the form of a single report or a coordinated series of staged reports from colluding individuals. 
To harm a third party. Clans in Iraq and Afghanistan have attempted to exploit US troops by falsely reporting that another clan was an insurgent group, provoking a military reaction against the rival clan (Associated Press 2008). 
To create a decoy. This occurs when a false report is created to draw attention and resources away from an area where a hostile act is planned. Throughout this paper, we assume that citizen reports are called into an operations center by phone (via voice, text, or mobile application) or computer. At the operations center, a decision maker (DM) decides whether to dispatch resources. We assume each decision has a measurable outcome in terms of costs and benefits. For example, a response might require manpower, equipment, and supplies, and temporarily make these resources unavailable for other tasks, entail risk (endangerment of the first responders), and offer benefits in terms of protecting the peace, apprehending a suspect, or providing aid. We capture the essence of the problem by assuming the DM seeks to maximize the net utility (the total benefit less the total cost) of all responses. For the DM, a fundamental issue is the trustworthiness of reports. In a friendly environment, false alarms (generated by well-meaning citizens, juvenile troublemakers, faulty building detection systems, etc.) represent a nuisance, but in a hostile environment such as a war zone, the risks are higher. Thus it is important to identify false and misleading reports, or at the very least, to choose a strategy that appropriately balances risks. With anonymous reporting, the DM cannot discriminate true and false reports on the basis of reporter identity, greatly limiting the decision rules that can be implemented. If caller identity is known, however, then reputations can be learned and factored into decision making, leading to more sophisticated, reputation-based decision rules. The concept of operations is shown in Fig. 1. Concept of operations for citizen event reporting as an aid to information gathering In this paper, we explore some of the significant questions involved with deploying CER systems in hostile environments: What percentage of foes in a population will undermine the CER approach through collection of too many false or misleading reports? What decision strategies minimize the impact of untrustworthy reports? Is it necessary to require reporters to reveal their identity, or is anonymous reporting be sufficient? Our approach was to create a social simulation that includes citizens (both friends and foes), soldiers, and DMs. In creating our model, we drew on several previous studies. Epstein et al. (2001) modeled civil violence in which a central authority tries to suppress a rebellion, or quell violence between two subgroups. Epstein’s citizens harbor a “grievance” which is a combination of hardship and the citizen’s perception of the illegitimacy of the central authority. Citizen actions are also governed by a personal risk tolerance. Goh et al. (2006) took this a step further by allowing citizens and cops to evolve new behavior by random mutation and selection, allowing successful strategies to spread via learning. Doran (2006) identified positive feedback between insurgent numbers, military success, and recruitment. Other related studies have used agent-based models to explore crime prevention (Groff and Birks 2008; Vasconcelos and Furtado 2005). Recent news reports have suggested that increased security and reduced threat of retaliation has led a sudden increase in the number Iraqi citizens coming forward with tips in Shiite areas of Baghdad (Gamel 2008). This suggests there may be a “tipping point” characterized by a sudden transition from a state where there are few citizen reports, to one where there are many citizen reports. The feedback mechanism could be that citizens will report when they observe other citizens collecting incentive payments while remaining free of retaliation.",2
8.0,2.0,Mind & Society,26 May 2009,https://link.springer.com/article/10.1007/s11299-009-0057-2,Self-emerging coordination mechanisms for knowledge integration processes,December 2009,Edoardo Mollona,Andrea Marcozzi,,Male,Female,Unknown,Mix,,
8.0,2.0,Mind & Society,16 June 2009,https://link.springer.com/article/10.1007/s11299-009-0061-6,Hybrid models for achieving and maintaining cooperative symbiotic groups,December 2009,İlker Yıldırım,Pınar Yolum,,Male,Female,Unknown,Mix,,
9.0,1.0,Mind & Society,14 May 2010,https://link.springer.com/article/10.1007/s11299-010-0072-3,In memoriam of Mary Douglas (1921–2007),June 2010,Aaron V. Cicourel,,,Male,Unknown,Unknown,Male,,
9.0,1.0,Mind & Society,08 July 2009,https://link.springer.com/article/10.1007/s11299-009-0064-3,Predicting blame assignment in a case of negligent harm,June 2010,David R. Mandel,,,Male,Unknown,Unknown,Male,"In many if not most societies, one of the first questions posed when harm takes place is “Who’s to blame?” How social observers go about answering that question is a reflection of their moral cognition and their prosecutorial mindset (Tetlock et al. 2007). Blame assignment can have important ramifications for stakeholders, especially the actors directly involved in a relevant case. Those absolved of blame tend to be seen as victims of harm, while those who incur most of the blame tend to be seen as perpetrators of harm, even if they too sustained harm in the process. The present article examines how people assign blame in an accident case involving asymmetric negligence on the part of the two actors involved. Alongside the question of who is to blame in cases of harm, several other questions tend to arise, such as: How was the harm caused? Who had control over the situation? And, as already noted, to what extent might the outcome of various acts have been foreseeable? The aim of the present study was to examine the extent to which these types of attributions or judgments were predictive of the blame assigned by social observers to each of the relevant actors. In many instances, harm is brought about without the intent to cause it or without an actor’s identification with it (e.g., see Frankfurt 1988; Woolfolk et al. 2006), and in those cases it is often called “accidental.” Of course, not all accidents are equally accidental; some are more foreseeable than others, and thus may seem less accidental, particularly in hindsight. Moreover, the degree to which harm was, or ought to have been, foreseeable will often vary across the actors involved in the relevant case. Accidents resulting from negligence offer a case in point. Imagine an accident in which a negligent driver injures an innocent driver. Both were involved in “the accident,” but observers will likely judge that the negligent driver, unlike his innocent counterpart, ought to have foreseen the consequences of his negligent acts. In some sense, we may judge the accident as less accidental for the negligent driver because the risk of harm was implied by the negligent acts, even if the necessity of harm, intention to harm, and identification with the act of harming were utterly lacking. Previous literature indicates that each of the types of attributions noted earlier—causality, foreseeability, and controllability—may influence blame assignment. Most accounts of blame assignment (e.g., Fincham and Schultz 1981; Shaver 1985; Shultz and Schleifer 1983) highlight the importance of causal judgment as a determinant of blame. In effect, for an actor to be blamed for harm, he or she must have had a causal role. However, most theoretical accounts tracing back to Heider (1958) also posit that having a causal role does not necessitate blame assignment. Other factors such as control, foreseeability, and intent are also important to consider. For instance, an actor who might have played a causal role in an accident but nevertheless had little or no control over the relevant behavior would seem an unlikely candidate to be blamed (e.g., Alicke 2000; Mandel and Lehman 1996). Indeed, in some cases where criminal activity is causally linked to a biological basis outside the actor’s control, blame and legal culpability is not assigned. For instance, Mobbs et al. (2007) describe a case in which a surgeon carved his name in a patient’s stomach after surgery. It was discovered that the surgeon had Pick disease (a form of dementia presumably resulting from degeneration of frontal and angular cortices) and was not held responsible for his actions by the jury, indeed not even by the victim. The extent to which harm was (or should have been) foreseeable has also been posited to affect the assignment blame or responsibility (e.g., Brewer 1977; Schlenker et al. 1994; Shaver 1985; Shaw 1968). Even if an actor played a role in causing harm and had control over his or her behavior, if the actor could not have reasonably foreseen the harm caused by his or her behavior, then the actor may be unlikely to be assigned a significant proportion of blame. Note that foreseeability has two distinct meanings. It may refer to an act whose consequences were intended (Hart and Honoré 1959), or it may refer to an act whose consequences, although unintended, ought to have been predicted as likely (or at least not unlikely) to occur. In Shaver’s (1985) theory, blame assignment follows from ascriptions of intent. However, in Shultz and Schleifer’s (1983) theory, blame assignment follows from ascriptions of moral responsibility, which does not require intent. Negligence implies moral responsibility since it involves reasonably foreseeable risk brought about by a lack of due care in behavior (D’Arcy 1963; Hart 1968; Mackie 1977; Prosser 1955). Actors who commit intentional harm tend to be seen as more morally responsible for their behaviors than those who commit unintentional negligent harm, and the latter in turn tend to be seen as more responsible than those who commit accidental harm (Karlovac and Darley 1989; Shultz and Wright 1985; Shultz et al. 1986). Consistent with these findings, Pizarro et al. (2003) found that participants assign less blame to an actor who impulsively brings about harm than to one who had done so intentionally. These findings suggest that the degree to which an actor’s behaviors are judged to be predictive of harm (and hence negligent) would affect the degree of blame assigned to that actor. Indeed, actors are particularly likely to engage in counterfactual excuse making (e.g., “If only I had known that x was the case, I would have acted differently”) when they can point to the unforeseeability of the consequences of their actions and they are also under accountability pressure (Markman and Tetlock 2000). In spite of the indicative nature of previous research on the determinants of blame assignment, the question of the relative weight and configuration of these factors as predictors of blame remains unanswered and in need of further empirical examination. Fincham and Jaspars (1983), for instance, found that foreseeability—namely, the perceived probability of a particular outcome given an actors’ behaviour—was significantly predictive of blame in some studies but not in others. In spite of the interactive effects predicted by descriptive models of blame assignment, no study was found in the literature that specifically pitted alternative hypotheses about the interactive effect of people’s subjective assessments of causality, controllability, and foreseeability on their assignment of blame. Given that descriptive theories of blame posit an interactive role of these subjective factors, such empirical tests ought to be an evident requirement. The present study was designed to provide an initial test of alternative, configural determinants of blame assignment using participants’ own assessments of causality, controllability, predictability of outcome, and blame. The present study examined this issue in the context of a vehicular accident case in which one driver was criminally negligent (namely, he caused serious bodily harm to another driver while under the influence of alcohol) and the other driver was innocent, at least in a legal sense (he had the right of way at the time of the accident and there was no evidence of, or even reason to suspect, impairment). Previous research using this case (Mandel and Lehman 1996; see also Kahneman and Tversky 1982) found that most participants assigned causality primarily to the culpable driver, even though most mentally undid the innocent driver’s misfortune of being in the accident by changing aspects of his own controllable behavior (e.g., the unusual route he took to drive home from work or his decision to stop quickly for a yellow light at the critical intersection). Mandel and Lehman (1996), however, did not ask participants to assign blame to the two drivers in the case. Nor did they investigate how observers judge causality in the case since participants were asked to imagine themselves as either the innocent or culpable driver. In the present study, participants took the role of third-party observers and rated the extent to which each driver was (a) causally responsible, (b) in control of his behavior, (c) engaged in behavior that was predictive of an accident outcome, and (d) blameworthy. Using these subjective assessments, several predictive models of blame assignment were tested. The first three steps of the model explored main effects: As a starting point (step 1), the blame assigned to one driver was predicted on the basis of the blame assigned to the other. If blame is assigned to the two actors like complementary slices of a pie, then the blame assigned to one actor should be predicted on the basis of blame assigned to the other. The second step of the model included the causality, controllability, and predictability judgments of the driver whose blameworthiness was being assessed and the third step included these judgments of the other driver. The final two steps of the model explored the predictive utility of higher-order interaction effects among the subjective estimates: The fourth step included causality × controllability and the causality × predictability interactions for the driver whose blameworthiness was being assessed. The fifth and final step of the model included the three-way (causality × controllability × predictability) interaction term for the same driver. Some or all of these interaction terms may be expected to contribute independent predictive value if blame assigned was determined on the basis of a configuration of judgment attributes, such as a focus on controllable and reasonably foreseeable causes. Finally, because these predictive models were tested on both the blame ratings of the innocent driver and of the negligent driver, differences in predictors of blame assignment as a function of the actor’s status (i.e., innocent vs. negligent) could also be ascertained in the present study. If blame assignment is attuned to assessments of negligence, as previous accounts have proposed, then one might hypothesize that blame assigned to the innocent driver would also be influenced by an observer’s assessments of the negligent driver, whereas blame assigned to the negligent driver would not be influenced by assessments of the innocent driver. This asymmetric discounting hypothesis predicts that blame assigned to the innocent driver would be discounted in light of the causal impact of the negligent driver, whereas the blame assigned to the negligent driver would not be discounted in light of the innocent driver’s causal impact. Thus, it was predicted that in step 3 of the model for the innocent driver the causality ratings of both that driver and the negligent one would significantly predict blame. In contrast, it was predicted that the causality rating of the negligent driver but not the causality rating of the innocent driver would predict blame assigned to the negligent driver.",5
9.0,1.0,Mind & Society,17 June 2009,https://link.springer.com/article/10.1007/s11299-009-0062-5,"Trust, secrecy and accuracy in voting systems: the case for transparency",June 2010,Roberto Casati,,,Male,Unknown,Unknown,Male,,3
9.0,1.0,Mind & Society,08 July 2009,https://link.springer.com/article/10.1007/s11299-009-0065-2,Moral disengagement and tolerance for health care inequality in Texas,June 2010,Alfred L. McAlister,,,Male,Unknown,Unknown,Male,"There is copious evidence that socioeconomic stratification and unequal distribution of social advantage has a powerful, perhaps predominant, influence on a population’s health (Tarlov and St Peter 2000). Some societies appear to tolerate these disparities more than others. Differences between societies in their tolerance for the suffering caused by economic and health care inequality may be ascribed to cultural differences in political ideology (Tarlov and St Peter 2000), but their specific elements have not been elucidated. The concept of moral disengagement (Bandura 1999) was developed to help explain how people excuse themselves for inflicting suffering upon others. According to this concept, people use self-deceptive psychological maneuvers that make harmful actions (or inaction) acceptable by disengaging the social norms and personal standards that restrain aggression through civility. Mechanisms of moral disengagement include justificatory beliefs that make harmful actions appear to be necessary or even beneficial, attributing blame to victims and minimizing the degree to which harm and suffering are perceived and acknowledged. In addition to making it easier to inflict suffering overtly, these psychological mechanisms may increase tolerance for the suffering caused by disparities in access to health care. Beliefs supporting arguments against state-subsidized health care or other government services might be expected to employ these mechanisms by providing moral justification with the assertion that government assistance does more harm than good, attributing blame for inequalities to perceived characteristics of disadvantaged groups and minimizing the acknowledged consequences of reductions in government services by claiming that adequate help is available from the private sector. The pilot research reported here was designed to study how these beliefs relate to political affiliation and ideology, and to support for policies to reduce disparity. The study was conducted in Texas, where state policy is highly influenced by conservative political ideology (Micklethwait and Wooldridge 2004) and a very high proportion of children lack health insurance (Mitka 2008).",2
9.0,1.0,Mind & Society,21 October 2009,https://link.springer.com/article/10.1007/s11299-009-0068-z,How pretence can really be metarepresentational,June 2010,Cristina Meini,Alberto Voltolini,,Female,Male,Unknown,Mix,,
9.0,1.0,Mind & Society,11 March 2010,https://link.springer.com/article/10.1007/s11299-010-0069-y,Neuro-cybernetics of socio-scientific systems,June 2010,Masudul Alam Choudhury,Mohammad Shahadat Hossain,,Unknown,Male,Unknown,Male,"The principal objective of this paper is to undertake an epistemological inquiry for the development of a generalized theory of systems and cybernetics that applies to all ‘learning’ systems. Information technology and socio-scientific systems are particular areas of such inquiry. In the epistemological sense of learning, the generalized theory of systems and cybernetics when used to expand the understanding of learning-type socio-scientific systems, presents a study in unity of knowledge. The epistemology of unity of knowledge means here the organic symbiosis between diverse systems on the basis of a law of pervasive complementarities between the symbiotic entities of the examined systems. The epistemology of unity of knowledge and its worldview are unique and universal across all socio-scientific areas. In this sense of a unique and universal theory of learning systems investigated by a general theory of systems and cybernetics the epistemological inquiry becomes what we call, a neuro-cybernetic study. The generalized theory of systems and cybernetics in terms of the epistemological foundation of unity of knowledge, which answers the scientific meaning of a paradigm of ‘everything’, is the underlying essence of uniqueness and universality. The technical meaning of the concepts of ‘universality’ and ‘uniqueness’ in terms of a phenomenology to uniquely decipher scientific truth and falsehood in socio-scientific domains is explained further in the “Technical Appendix”. In this paper we undertake studies in the light of a comprehensive understanding of management, organizational institutional and socio-scientific systems with ethical connotations that can be treated by a compendium of system and cybernetic theories. Our study is trans-cultural in content. Subsequently, we derive the generalized theory of neuro-cybernetics pertaining to the mentioned areas. An application of the general theory is given for computer modeling for flood control in Bangladesh.",4
9.0,1.0,Mind & Society,14 April 2010,https://link.springer.com/article/10.1007/s11299-010-0071-4,Re-assessing ecology of tool transparency in epistemic practices,June 2010,Bernardo Pino,,,Male,Unknown,Unknown,Male,"In this paper, a revised understanding of tool transparency will be attempted. As we proceed, I shall primarily address a critical assessment of a strongly reactive account of epistemic tool use. Specifically, it will be claimed that a fully ecological perspective of epistemic tool transparency is still unsatisfactory in important respects. By contrast, the current proposal is expected to constitute a new evaluative approximation to certain overlooked reflective aspects of tool transparency in cognitive externalization. From the perspective of the Hypothesis of Extended Mind (HEM hereafter), much of what is of interest to a scientific account of human cognition can be explained in terms of an intimate causal interplay between the brain, the body and the world (see Clark 1997). At the same time, these causal loops would be deeply regulated by certain types of organizing principles such as Nontrivial Causal Spread (NCS), or the Principle of Ecological Assembly (PEA).
Footnote 1 Whereas the former supposedly occurs whenever something is achieved by factors and forces that go well beyond the boundaries of a clearly demarcated system, the latter makes reference to a cognitive agent’s tendency to recruit, on the spur of the moment, “whatever mix of problem-solving resources will yield an acceptable result with a minimum of effort.” (Clark 2008, p. 13). These kinds of principles, in turn, are clearly subordinated to what Clark and Chalmers (1998) first presented as the Parity Principle. According to this principle, any part of the world, in a given task, can be considered as constituting part of a cognitive process, provided that (for that time) this part of the world happens to function as a process that, were it to occur in the head, we would not hesitate to accept as cognitive in its own right. What this view of cognition yields is a picture of human agents whose basic intuitions, not only about their true physical limits, but also about what ultimately causes their intelligent (goal-oriented) behaviour, are fundamentally wrong. A case in point is the experimental account of epistemic tool use as based on a fully online and ecologically-locked perspective.Footnote 2 In this paper, I will attempt to examine some significant weaknesses of such an account, without disregarding the role of inner processing as the locus of reflective thinking. In this respect, I will try to re-assess certain aspects of the radical view presented by advocates of the HEM. As there is no available definite explanation of how human advanced cognition can directly develop from what we know as online cognition, I will present a provisional characterization of tool transparency which intends to defend the idea of a process of derived or mediated cognitive delegation. The involved reification of this cognitive delegation will be mainly based on (a) Peirce’s known ideas about beliefs as habits of action, (b) Magnani’s proposals on the disembodiment of the mind effect, and (c) a possible social derivation (reification) from (a) and (b) of some relevant peripheral, sense-act template routines. Human cognitive agents’ tendency to engage in social practice as a learning-enhancing process completes the proposal. This approach should provide a less “reactive” view of so-called transparent tool use in general, and epistemic tool use in particular. Whereas transparent tool use has been normally associated with embodied/habituated practices, some aspects of epistemic tool use transparency have been either neglected or partially accounted for. I intend to propose a more balanced view on the latter, on the understanding that embodiment and transparent tool ecology do not support all the relevant cognitive processes involved in epistemic tool use transparency. In this view, epistemic actions can be understood as both the result of an interplay between decoupled representation-based inner processing and online action routine templates, in the service of goal-oriented and problem-solving behaviour. A particularly tempting implication of this approach will be to entertain the possibility that epistemic actions are the result of continuous delegation of stabilized multi-purpose chunks of formerly simple pragmatic actions. I will defend the thesis that a brain-centred decoupled mediated interplay between so-called internal and external representations involves continuously resorting to a rich source of previously contracted habits of action. I will characterize this process as a kind of derived cognitive delegation. Thus, the derived (usually fleeting) cognitive status of extra-mental resources will be said to depend on the intentional agent’s internal, already fixated, processes or states. To further ground some critical observations regarding a fully ecological characterization of epistemic tool use, I will connect Peirce’s ideas on the fixation of habits as rules of action and Wenger’s work on Communities of Practice-based learning. In this view, the fixation of multi-purpose rules of action (M-P RA) will correlate agents’ regular participation and interaction within the context of communities of practice (CoP) of special sorts. What these sorts of CoP afford community members is a suitable social environment for the development of epistemic tool transparency.",3
9.0,1.0,Mind & Society,07 April 2010,https://link.springer.com/article/10.1007/s11299-010-0070-5,L. Magnani: Abductive cognition: the epistemological and eco-cognitive dimensions of hypothetical reasoning,June 2010,Paul Thagard,,,Male,Unknown,Unknown,Male,,2
9.0,2.0,Mind & Society,03 November 2010,https://link.springer.com/article/10.1007/s11299-010-0082-1,Special issue on “experimental economics and the social embedding of economic behaviour and cognition”,December 2010,Christophe Heintz,Nicholas Bardsley,,Male,Male,Unknown,Male,,1
9.0,2.0,Mind & Society,26 September 2010,https://link.springer.com/article/10.1007/s11299-010-0075-0,Sociality and external validity in experimental economics,December 2010,Nicholas Bardsley,,,Male,Unknown,Unknown,Male,"Experimental economists have sometimes argued that if their design implements a theoretical model, they do not have to worry about about the extent to which their results generalise beyond the lab to situations of interest in the world, or ‘external validity.’ This stance, known as the ‘blame the theory’ defence of experiments, has recently been critiqued by Bardsley et al. (2010), Chaps. 2 and 5. According to this critique the defence holds more promise for game- or decision-theoretic experiments than applied economics ones.Footnote 1 We did not offer an agreed view on how externally valid laboratory economics experiments are, though, concentrating instead on what the lines of debate over this are likely to be. Here I offer subsequent reflections on external validity. The focus is particularly on model-implementing, applied economics designs, for which blaming the theory is problematic. Such designs, which purport to study a particular market or other real world situation of interest, actually result in decision- or game-theoretic experiments. I will argue that many of the intended inferences that this class of experimental designs intend to support are therefore not warranted. How misleading such exercises are likely to be depends partly on how uniform is behaviour across contexts and types of agent. Such ‘uniformity’ would hold to the extent that behaviour were invariant to certain factors which are not explicitly theorised to make a difference, a notion that will be clarified by means of examples. I contend that a very relevant source of variability is ‘relationality’ and give examples showing that this is not just a hypothetical concern. Relationality, which also goes by the name of ‘the social construction of social reality,’ also sets some epistemic limits for (non-covert, non-deceptive) designs which do not implement models. However, no general scepticism towards lab experiments is implied. Experimental methods, appropriately deployed and communicated, remain a powerful social scientific tool. Section 2 considers why the blame the theory defence does not work for all types of experiment. Section 3 considers a typical example of an experiment that is not covered by the defence. Section 4 introduces a general reason why such designs in particular may lack external validity, and Sects. 5 and 6 consider some relevant evidence.",4
9.0,2.0,Mind & Society,05 June 2010,https://link.springer.com/article/10.1007/s11299-010-0073-2,Social norms or social preferences?,December 2010,Ken Binmore,,,Male,Unknown,Unknown,Male,"Every so often, an attempt is made by economists to muscle in on neighboring disciplines like psychology and sociology. This paper argues against a new imperial venture that originates from within the discipline of behavioral economics. Behavioral economics seems not to fit the standard mold at first sight because it eschews the language of human or social capital. It traces its origins instead to the experiments of the psychologists, Kahneman and Tversky, whose work is now generally accepted to have blown away some of the cobwebs of tradition that economic theorists once treated as axiomatic. But now there are those who blow so hard that they seem determined to blow away everything in social science that does not fit within their newly created orthodoxy. As an economist who joined the experimental pioneers at an early stage, I continue to welcome the breath of fresh air that behavioral economics has brought to my discipline. Studies of the psychology of fairness, reciprocity and reputation are particularly important for my own interest in the evolution of social norms. However, the group of behavioral economists criticized in this paper seem to me to have betrayed the scientific ethos of the founders of their subject by letting themselves fall prey to a new economic dogma—that social behavior is always best explained by assuming that people simply maximize social (other-regarding) utility functions. Among much else, it is argued that people are nicer than the selfishness axiom of neoclassical economics admits, and that their discoveries are sufficiently robust that they can be used to inform public policy (Fehr and Gintis 2007; Gintis 2009; Henrich et al. 2004; Thaler and Sunstein 2008). They certainly sing a beguiling song, but it would be a mistake to take their claims at face value. For example:  It is not true that it is axiomatic in neoclassical economics that people are selfish. On the contrary, students are traditionally taught the motto de gustibus non est disputandum, which means that we must take whatever the preferences of economic agents happen to be as given. Neoclassical economics is therefore in no danger of being refuted if people sometimes prefer other things to money. Nor do experiments show that all traditional economics is hopelessly wrong. On the contrary, in the market contexts of most interest to economists, laboratory experiments show that traditional theory works rather well (Smith 1991).
Footnote 1 It is true that traditional economic theory sometimes fails miserably in other contexts, notably in respect to human behavior in risky situations (Kahneman and Tversky 1979), but that is no reason to condemn the theory in contexts where it works pretty well. Nor is it true that all or any of the theories propounded by behavioral economists to explain what they call anomalous data are particularly successful. For example, the traditional idea that people will act to maximize expected utility predicts very badly, but Prospect Theory—which is Kahneman and Tversky’s alternative behavioral theory—predicts no better when its extra parameters are taken into account (Camerer and Harless 1994; Hey and Orme 1994). The claim that theories of other-regarding preferences are robust seems to be without any serious foundation at all. It is not even true that behavioral economics can always be regarded as a new alternative to neoclassical economics. On the contrary, theories of other-regarding utility functions might better be classified as retroclassical economics, as they revert to the dogma that people actually do have utility functions in their heads that they seek to optimize when interacting with others—an idea that was popular in Victorian times, but which was abandoned by neoclasssical economists many years ago. I think it fair to say that almost all behavioral economists endorse the idea that social norms can be adequately described using social or other-regarding utility functions, but only a vocal minority insist that the evidence is so strong that the hypotheses favored by other disciplines deserve no better fate than the waste basket. My own guess is that their insistence on blurring the distinction between a social norm and a social preference will turn out to be a bad mistake. In explaining why, I occasionally need to refer to the wider criticisms outlined above, but it will be necessary to look elsewhere for a full justification of these complaints (Binmore 2007; Binmore and Shaked 2010). It will also be necessary to consult the appendices to this paper when the claims being made require technical support.",57
9.0,2.0,Mind & Society,26 September 2010,https://link.springer.com/article/10.1007/s11299-010-0077-y,Cooperation in and out of the lab: a comment on Binmore’s paper,December 2010,Francesco Guala,,,Male,Unknown,Unknown,Male,"Thanks to game theorists and experimental economists, the social sciences have become a source of inspiration for philosophers again. An important step forward has been the transformation of economics into a research programme that closely resembles the best sub-fields of natural science, with their fertile mix of theoretical models, empirical data, and simulations. Another welcome development is the increasing integration of economic theory with evolutionary biology and neuroscience, in a dialectic of borrowing and lending that has enriched all sides and reduced the isolation of the dismal science from its powerful neighbours. Ken Binmore has made important contributions to these developments, and all philosophers interested in the foundations of social science will pay attention to his work in the years to come. In this brief comment I will focus on the role he has played (and continues to play) in the central debate animating behavioural and experimental economics in the new century. My task is made easier by the fact that I share Binmore’s doubts about the claims of “behavioural economists”, and like him I believe that the fairness and cooperation observed in the economic lab is to a large extent an artefact of the specific settings we create in the economic lab.Footnote 1
 Although I agree with Binmore’s conclusions, however, I have qualms about the way he supports them. Some of his arguments are invalid, in my view, while others are based on weak empirical premises. This is unfortunate and might have two negative consequences. On the one hand, Binmore might fail to persuade other social scientists and philosophers who are rightly impressed by the body of evidence generated by behavioural economists. (Indeed, the quick diffusion of behavioural economists’ results in the literature suggests that this is already happening.) On the other, he may persuade behavioural economists that they are facing a fundamentally ideological opposition, and that the critiques of Binmore and others have no genuine scientific value. I will proceed as follows: in the next section, I will re-define slightly the main coordinates of the controversy—which in my view revolves around the concept of reciprocity—to highlight where the core disagreement between Binmore and the behaviouralists lies. In Sect. 3 I will review critically Binmore’s main arguments against the Strong Reciprocity interpretation of Public Goods and Ultimatum games. Section 4 is devoted to showing briefly that the real weakness of Strong Reciprocity theory is its complete lack of support from field data. On the contrary, there is a considerable amount of evidence showing that outside the laboratory cooperation is sustained by Weak Reciprocity mechanisms such as those that feature prominently in Binmore’s interpretation of experimental data. Section 5 concludes with a few general comments.",
9.0,2.0,Mind & Society,01 October 2010,https://link.springer.com/article/10.1007/s11299-010-0079-9,Has punishment played a role in the evolution of cooperation? A critical review,December 2010,Nicolas Baumard,,,Male,Unknown,Unknown,Male,"In 2002, Fehr and Gächter (2002) published a study on altruistic punishment in humans. They showed that, in an experimental game, participants punish defectors in a cooperative activity although the punishment is costly for them and yields no material gain. They also showed that cooperation flourishes if punishment is possible, and breaks down if it is ruled out. This was not the first time that such a pattern was observed (see Yamagishi 1986 for similar results). However, for the first time, the existence of punishment was linked to evolutionary theories of cooperation. Fehr and Gächter explained that their experimental results were good evidence in favour of group selection (Gintis et al. 2003; Sober and Wilson 1998). First, they demonstrated that people are willing to punish others altruistically—that is, at a cost to themselves and for no benefits. This was taken as evidence that humans are able to sacrifice their individual welfare to increase the welfare of the group, and that cooperation has evolved not for the benefit of individuals but for the benefit of the group. Second, they demonstrated that punishment is a powerful tool to sustain cooperation. At the time when Fehr and Gächter published their results, group selection was considered unlikely since it was easy for defectors to exploit the contribution of altruistic individuals. However, if defectors are punished by altruistic individuals, they can no longer benefit from their defection, and sacrificing oneself for the group becomes evolutionary advantageous.Footnote 1 This was just what Fehr and Gächter’s experiment suggested. Since then, punishment has become a central theme both among theoreticians (Boyd et al. 2003, 2010; Kurzban and DeScioli 2009; Sripada 2005) and experimentalists (Barclay 2006; de Quervain et al. 2004; Henrich et al. 2006, 2010; Price 2005). However, despite this surge of interest in punishment, the empirical evidence remains weak. Supporters of group selection and students of punishment in behavioural games have assumed that evidence of punishment in the laboratory means evidence of punishment in real life. But is this really the case? In this article, I contend that there is no empirical evidence that altruistic punishment has played a role in the evolution of human cooperation. It is actually a well-established fact in anthropology of law that in small scale societies wrongdoers are not punished by altruistic individuals willing to defend the interests of the group. When someone harms or steals someone else, most of the time nothing is done to punish the culprit. If the wrongdoing is very serious and threatens the safety of the victim, she may retaliate in order to preserve her reputation or deter future aggression. In most cases, however, people simply stop wasting their time interacting with immoral individuals. Thus, rather than supporting the above-cited experimental results, ethnographic data seem to contradict them and to invalidate the theory of the evolution of cooperation by altruistic punishment. I will thus suggest that ethnographic data are better explained by individual selection, and in particular by partner choice theory (Bshary and Noë 2003; Bull and Rice 1991; Noë et al. 1991; Roberts 1998). Individual selection is often reduced to partner fidelity, as typified in the iterated Prisoner’s Dilemma game (Axelrod 1984; Axelrod and Hamilton 1981; Trivers 1971). Individuals who at any time fail to cooperate with their partner can be penalised by those same partners in subsequent trials. These models have been shown to be limited in their ability to explain cooperation outside dyadic relationships (Boyd and Richerson 1988). In the last two decades, a new class of individual selection models, based on partner choice, has emerged. In partner-fidelity models, guarding against cheating is the central issue. By contrast, in partner choice models, the emphasis is less on guarding against cheating and more on competition between potential cooperative partners. Partner control and partner choice models imply different evolutionary dynamics of cooperation. In partner control models, an individual sanctions an uncooperative partner by aborting the relationship, thereby losing the advantages of cooperation in the process. In partner choice models, an individual reacts to an uncooperative partner by starting a new cooperative relationship with another partner. Thus, there may be no cost in sanctioning uncooperative partners. Indeed, switching to a better partner is in the interest of individuals. It is worth emphasising that partner choice models do not require any kind of altruistic punishment. First, switching from one individual to another, presumably more cooperative individual is not altruistic but clearly beneficial to the agent. Second, leaving one’s partner because he is cheating is not a punishment since it does not aim to inflict a cost on him. The cost inflicted on the cheater (through the loss of his partner) is just a by-product of the fact that his previous partner prefers to interact with someone else. To sum up, partner choice works solely on the basis of individual interests. In this theory, cooperative dispositions have evolved not to help the group, but rather to benefit the individuals themselves. The starting point of these theories is that, in the ancestral environment, individuals were in competition to be recruited in cooperative ventures. They had an interest in finding the best cooperative partners and, conversely, they needed to be chosen by the best cooperative partners. This competition is likely to lead to selection for cooperative dispositions. Indeed, individuals who harm and steal from others, cheat and take more than their fair share of the benefit, will be avoided in the future and will progressively loose the opportunity to enjoy the benefits of cooperation. We shall see that this is what we observe in hunter-gatherer societies. As Kaplan and Gurven (2001) put it, cooperation may emerge from the fact that people in hunter-gatherer societies “vote with [their] feet.” (on this point, see Aktipis 2004). In this article, I shall study the ethnographic literature on punishment among hunter-gatherers. I conclude that it is unlikely that punishment has played a role in the evolution of human cooperation. On the contrary, the stability of cooperative behaviours seems to rely on the fact that individuals can switch from one partner to another. In the discussion, I shall argue that there is further evidence that cooperation in groups can be stabilised without punishment. I shall also discuss two apparent problems for partner choice theories: large-scale cooperation and punishments in economic games. I will suggest that rather than casting doubt on individual selection and supporting group selection, these two phenomena provide evidence in favour of individual selection. We shall see that (1) people produce large-scale cooperation through institutions in which punishment is not altruistic but rewarded on an individual basis and that (2) punishment in experimental games can be explained without altruism and is indeed often better explained by individual considerations. To sum up, in an explanation of the evolution of cooperation, the use of punishment seems both uncertain—there is no conclusive evidence in favour of its existence—and unnecessary—cooperation seems to flourish very well in its absence.",47
9.0,2.0,Mind & Society,10 October 2010,https://link.springer.com/article/10.1007/s11299-010-0078-x,Network formation in repeated interactions: experimental evidence on dynamic behaviour,December 2010,Michele Bernasconi,Matteo Galizzi,,Female,Male,Unknown,Mix,,
9.0,2.0,Mind & Society,19 September 2010,https://link.springer.com/article/10.1007/s11299-010-0076-z,Is neuroeconomics doomed by the reverse inference fallacy?,December 2010,Sacha Bourgeois-Gironde,,,,Unknown,Unknown,Mix,,
10.0,1.0,Mind & Society,09 October 2010,https://link.springer.com/article/10.1007/s11299-010-0080-3,Punishment is not a group adaptation,June 2011,Nicolas Baumard,,,Male,Unknown,Unknown,Male,"For a long time, evolutionary theories of human cooperation were dominated by ‘mutualistic’ theories such as reciprocal altruism (Trivers 1971) or indirect reciprocity (Alexander 1987) in which cooperation arises because it is mutually beneficial to individuals (West et al. 2007). The observation of punitive behaviours in behavioural games (Fehr and Gächter 2002) has demonstrated the limits of standard mutualistic theories since in these experiments, participants are ready to punish cheaters at a cost to themselves and for no direct benefits (Gintis et al. 2003). By contrast, the existence of punitive behaviours seems to favour ‘altruistic’ theories in which individuals sacrifice for the group (Gintis et al. 2003; Sober and Wilson 1998). Indeed, it is exactly the kind of altruistic behaviour predicted by group selection. Furthermore, it suggests that cooperation has evolved through the sacrifice of altruistic individuals who were ready to incur some costs to prevent cheating. In response to these altruistic approaches to punishment, supporters of mutualistic theories have proposed to explain the evolution of punishment in terms of the individual’s interest in curbing the benefits of cheating (Price et al. 2002), in collectively controlling powerful individuals (Boehm 2000), or in managing moral reputation (Barclay 2006). Here, I propose a radical alternative to these responses, namely that punishment is a not an adaptation and that there was no specific selective pressure to inflict costs on wrongdoers in the ancestral environment. This alternative is based on a contractualist approach to cooperation (Baumard 2008, 2010b; see also Gauthier 1986; Rawls 1971). In this theory, cooperation evolved through partner choice for mutual advantage and consists in respecting others’ interests exactly as if individuals had really agreed on a contract. In the ancestral environment, individuals were in competition to be recruited in the most fruitful ventures, and it was vital to share the benefits of cooperation in a mutually advantageous manner. If individuals took a bigger share of the benefits, their partners would leave them for more interesting partners. If they took a smaller share, they would be exploited by their partners who would receive more than what they had contributed to produce. This context is thus likely to have led to selection for a ‘sense of fairness’, a cognitive adaptation promoting and enabling respect for others’ interests. This theory predicts the kind of morality proposed by contractualist philosophers. Individuals should behave with each other as if they had agreed on a mutually advantageous contract. In this contractualist approach, cooperation (fairness) evolves by partner choice, and does not require any use of punishment. Thus, there is no instinct for punishment. Punitive behaviours are only a way to restore fairness by compensating the victim or penalizing the wrongdoers. In the following sections, I detail further this theory and draw on behavioural economics, legal anthropology, and cognitive psychology to show that if offers a better theory of punitive behaviours. More precisely, I show that cooperation does not need punishment to evolve, and that when people inflict a cost on others, they do not aim to sustain group cooperation. But first, we first need to define more precisely what ‘punishment’ is and what it is not.",18
10.0,1.0,Mind & Society,06 November 2010,https://link.springer.com/article/10.1007/s11299-010-0081-2,Extrinsic and intrinsic motivations to innovate: tracing the motivation of ‘grassroot’ innovators in India,June 2011,Saradindu Bhaduri,Hemant Kumar,,Unknown,Male,Unknown,Male,"There is little disagreement over the fact that technological changes and innovations are crucial for economic change. Not much agreement, however, prevails on what drives technological change and innovation. In modern economies, intellectual property rights and fiscal incentives are often regarded as two major tools of innovation policy. Conventionally, intellectual property rights (patents in particular) were believed to add fuel of incentives to the fire of individual genius. However, widespread use of strategic patenting to stall, rather than promoting, innovation has dented the credibility of patent as a prime driver of innovation (see, for instance, Mansfield 1986a; Gallini 2002; Noble 1977: Chap. 6; Suarez-Villa 2009). Inventors, especially, small firms and individuals, seem to rely more on trade secrets, building complementary services, lead time and goodwill to earn value for their innovations than patents (Mansfield 1986a; Cohen et al. 2000; Bessen and Meurer 2008). Indeed, Bessen and Meurer (2008: 174) show that small innovative entities earn substantially less value for patents than their larger counterparts. Over time, it has also been realised that patent has, by and large, failed to induce innovative activities where commercial prospects are low (e.g. innovating orphan drugs, in the areas of neglected diseases).Footnote 1 Fiscal incentives also have been criticised for their inability to induce innovation to any considerable extent because of its limitation to alter risk perception, and reach out to small scale R&D by small firms or individuals. This is especially true when difference between social and private (economic) return to R&D is substantial (see Mansfield 1986b; Hall and Reenen 2000). Besides such empirical criticisms of the viability of extrinsic incentives (like IPR, trade secrets, lead time and fiscal incentives), the effectance motivation theory points out that externally provided incentives only have limited success in motivating human behaviour in the longer run. Precisely, this literature argues that extrinsic incentives are only useful in the short run, but they might prove to be counterproductive in the long run by crowding out intrinsic motivation of individuals (Kohn 1993; Frey 1997; Sansone and Harackiewicz 2000). This conceptual framework has been used to theorise, as well as to empirically verify, various economic acts, ranging from backward bending blood supply curve (Osterloh and Frey 2000), to willingness to host nuclear waste in one’s backyard (Osterloh and Frey 2000) to R&D activities (Kreps 1997).Footnote 2
 Schumpeter (1934) had also argued that the motive to accumulate private property can only explain part of innovative activities (p. 94), especially in early capitalist societies. The wheel of economic development in such societies is kept moving by “personality” and “will” of individuals (p. 89–94). In his view, “the joy of creating, of getting things done” associated with the behavioural traits that “seek out difficulties…and takes delight in ventures” stand out as the most independent factor of behaviour in explaining the process of economic development (pp. 93–94). In more matured phases of capitalism, however, innovation might become a routine activity of large firms and such psychological factors might become less important for innovation (Schumpeter 1943, p. 132). Noble (1977) also gives detailed account of how scientists lose their individual freedom in such a set up, which redefined their roles merely as ‘R&D inputs’. This paper intends to identify the various kinds of motivation behind “grassroot innovation” in India. The term grassroot refers to individual innovators, who often undertake innovative efforts to solve localised problems, and generally work outside the realm of formal organisations like business firms or research institutes. Although recent literature on organisational science and evolutionary economics have emphasised that intrinsic motivation is a key driver of economic activities, studies relating intrinsic motivation to innovative behaviour are few and far between. Our paper attempts to contribute to this growing body of research by analysing the motivation of grassroot innovators in India. The primary reason behind inadequate focus on this area seems to be the unavailability of any operational indicator of intrinsic motivation in the field of innovation. An objective of this paper, therefore, is also to develop such operational indicators of intrinsic motivations to innovate. In the next section we discuss the various characteristics of grassroot innovation. Section 3 provides a review of the various strands of motivation theories and explores their link with innovation studies. We discuss our data and methodology in Sect. 4. Section 5 analyses the data and develop the indicators of motivation. Finally, Sect. 6 synthesises and draws broad policy implications.",59
10.0,1.0,Mind & Society,02 July 2010,https://link.springer.com/article/10.1007/s11299-010-0074-1,Affective problem solving: emotion in research practice,June 2011,Lisa M. Osbeck,Nancy J. Nersessian,,Female,Female,Unknown,Female,"In recent decades, analyses of both the cognitive basis of emotion and the emotional basis of cognition have led to new questions concerning the clarity with which these processes can be distinguished meaningfully. Theories emphasizing the basis of emotion in (cognitive) appraisal of events and situations began in earnest in mid-century and have been influential for decades (Arnold 1960; Lazarus et al. 1970; Roseman 1991). More recently, the profound impact of emotion on cognitive tasks and processes has been argued compellingly, prompting the appearance of the rather intriguing term “emotional cognition” (Moore and Oaksford 2002; Thagard 2008). Because even cognitive theories of emotion acknowledge a somatic or bodily feeling component to emotional experience, the literature on emotional cognition is more broadly situated within the extensive recent literature on embodied mind and embodied cognition (Johnson 2008; Lakoff and Johnson 1999; Varela et al. 1991). Interest in emotion and its role in cognitive processes has prompted appeal to an “affective revolution” comparable in some respects to the “cognitive revolution” of the twentieth century (Haidt 2007: 998). But against the mounting evidence pointing to intimate connections between emotion and cognition, there is comparatively little analysis of the role of emotion in scientific practice. One notable historical example is Polanyi’s Personal Knowledge, a chapter of which concerns “Intellectual Passions.” Polanyi argues compellingly for the “manifest emotional force” of scientific process most obvious in the discovery phase, including the delight attending acts of ingenuity and the joy of contemplating natural mysteries, the intellectual beauty of mathematics, or the appreciation of elegance (Polanyi 1958/1973: 133). Maslow’s Psychology of Science (1966) draws from Polanyi’s insights in similarly arguing for the importance of the scientists’ passionate engagement with ideas and problems, in contrast to the stereotype of the detached and coldly rational observer. The aesthetic experience of scientific theories is likewise acknowledged by a wide range of theorists, historic and contemporary, within the general parameters of scientific values (Kuhn 1977; McAllister 1996). Similarly, Mitroff (1974) and Mahoney (1976) include emotion in their analyses of the contribution of subjectivity to scientific process. In contemporary science studies, however, emotion principally emerges as an important consideration in biographical studies of particular scientists, as implicated in studies of scientific creativity, or analysis of the personality traits of scientists in the collective (Feist 2006; Simonton 2004). One of the obstacles to analyzing emotion in science lies in the great variety of terms used in reference to this class of experiences and functions. Feelings, desires, emotions, moods, passions, pathos, and being “affected” by objects can be conceptually and historically distinguished; emotions are used in reference to both fleeting states and dispositions. Moreover the conceptual relations between emotion and other psychological categories—notably motivation and personality—vary broadly over decade and academic perspective. The rather convoluted route by which psychologists have related the categories of emotion and motivation over the decades is documented in many textbooks on motivation, whereby motivation has been distinguished from emotion and theorized in terms of drive (e.g. Franken 2006). Similarly, motivation is sometimes linked to personality, notably by Maslow who relates the former to the dispositional tendencies and potential capacities of any person (1954/1987). Personality, in turn, links back to emotion, through long-term and characteristic patterns of emotional responding. In contemporary cognitive science, Paul Thagard’s chapters on scientific cognition in his recent Hot Thought (2008) provide one of the few extensions of the recent emotional cognition literature to the domain of science. Thagard uses historical case data: Watson and Crick’s discovery of DNA structure and Dr. Patrick Lee’s discovery that the retrovirus has the potential to kill cancer cells, to analyze the role of emotion in all phases of inquiry. His analysis, detailed most clearly in relation to Watson, offers “a rich set of examples of possible emotional concomitants of scientific thinking” (2008: 173). Thagard began by identifying “emotion words” across the 143 pages of The Double Helix, Watson’s “personal account” of his famous discovery with Crick (Watson 1969). Thagard found a total of 235 emotion words: 125 attributed to Watson’s own experience, 35 to Crick, and 13 in reference Watson and Crick in combination, and 60 in relation to other researchers. Thagard also coded the emotion words identified as having positive or negative valence, and he found more than half to have positive valence. He then coded in terms of “basic” emotions assumed to be universal: happiness, sadness, anger, fear, disgust, and surprise, but identified additional words that had emotional tone but did not fit these basic categories: interest, hope, and beauty. Finally, Thagard evaluated the place of each emotion word in the context of different phases of inquiry. He began with Reichenbach’s (1938) basic distinction between the contexts of discovery and justification, and then drew a further division between investigation and discovery to account for the extensive work that frequently precedes discovery. Thagard analyzed most of Watson’s emotion words as taking place in the context of investigation, but identified emotion in all three phases, including justification. In this paper we extend the analysis of emotion as expressed in historical records to expressions in interview data we collected in engineering research laboratories, where lab members were asked to discuss their on-going research.",11
10.0,1.0,Mind & Society,03 February 2011,https://link.springer.com/article/10.1007/s11299-011-0083-8,From individual to social counterintuitiveness: how layers of innovation weave together to form multilayered tapestries of human cultures,June 2011,M. Afzal Upal,,,Unknown,Unknown,Unknown,Unknown,,
10.0,2.0,Mind & Society,28 September 2011,https://link.springer.com/article/10.1007/s11299-011-0091-8,The future of behavioral game theory,December 2011,Herbert Gintis,,,Male,Unknown,Unknown,Male,,5
10.0,2.0,Mind & Society,26 February 2011,https://link.springer.com/article/10.1007/s11299-011-0084-7,Educational models of knowledge prototypes development,December 2011,Flavia Santoianni,,,Female,Unknown,Unknown,Female,"Historically, the possible relationships between the implicit and explicit have been viewed in different ways. The dominating paradigm in the second half of the last century was the computational metaphor whereby the mind interprets knowledge as being explicit, aware, rational, symbolic, sequential and linear upward. The computational approach compares the mind to a calculating machine and has found expression in the theories on information processingFootnote 1 which state that mental activity uses cognitive structures as dynamic functional components. As in the organization of microstructures in a calculating machine, in the mind’s processing procedures the problem-solving operations to be carried out are subject to pre-determined rules whose acquisition takes place prior to the implementation of a task. Pre-determining rules is one of the processing criteria to manage mental responses and representations (Gagné 1973). The concepts of rule acquisition and mental representationFootnote 2 lies at the basis of the cognitivist interpretation of the mind’s functioning as something working at the “level of representation” (Gardner 1988). These concepts are coupled with the idea that representation and communication processes use systems of symbols, systems of meanings containing information units (Gardner 1987). The meaning-processing learning models (Santoianni 2003) follow sequential procedures of relatively simple operations (Lieberman 1990) and entail different levels of analysis—from elementary to higher, more complex ones—as shown in one of the earliest flow charts, designed by Broadbent in 1954, which showed the presence of stages and sequences in cognitive processing.Footnote 3
 It was the idea whereby a sequential system can activate at the same time only a definite and limited number of data that generated critical approaches towards the computational paradigm. Unlike the linear sequentiality characterising the computational mind, slowed down by von Neumann’s so-called ‘bottleneck’ (Rumelhart and McClelland 1991), artificial intelligent systems—created by connectionism in the mid Eighties starting from mid-20th century cybernetics—reflect the self-organizing dynamics of adaptive behaviors in living systems. In other words, they are parallel systems, whose properties emerge from the collective behavior of several simple units interacting in parallel with other units based on principles of interaction (Parisi 1991). The unified structure of connectionist systems makes it possible to consider the processing management modes as being interrelated with the mechanisms the cognitive system adopts to store/retrieve data and their sub-symbolic representations. As early as the late Sixties, as an alternative to the computational, aware and symbolic metaphor of the mind, the idea was put forward whereby learning could use unaware processing modes instead (Reber 1967). In the Seventies a debate arose on the possible relationship between the two types of learning, i.e. explicit and implicit. Are they two different types of learning—distinct, autonomous and parallel, or two different aspects co-existing in the same process—inter-related, interdependent and one (the implicit) prior to the other (the explicit)? A widely accepted idea seems to be that knowledge as a whole cannot be considered explicit, since it is not plausible to state that everything that is learnt is intentional and available to awareness (Le Doux 1998). However, as long as the classical metaphor of the computational mind received the greatest support (Sternberg 1990), implicit learning could only be outside the aware, rational mind as something foreign, according to what has become known as the “shadow theory” of implicit learning (Cleeremans 1997). The separation between the two systems—implicit and explicit, in more or less different ways, has had an impact on education: if they are separated, both processes can be activated autonomously, and the pedagogical value will lie in making the implicit dimension explicit, by channeling its aspects of potential disorder into the management of learning. With regard to this, in the educational field the last decades of last century saw research on implicit learning being combined with studies on meta-reflection. A tendency towards ‘dissolving’ the implicit into the explicit appeared as a result, that is interpreting the implicit through metacognitive and metaemotional instruments (Hacker et al. 1998). Research on meta-reflection has been interpreted in a variety of ways, however, and when grafted onto research areas concerning the development of cognitive processes, it has produced a prism of points of view whereby transforming the implicit into the explicit is not a necessary precondition for education. Implicit theories can indeed be validated, modified more or less radically and only possibly abrogated (Wellman and Gelman 1997). With respect to this, the implicit development of ideas, theories and concepts does not entail in itself a negative quality in educational terms; on the contrary, its value of primary configuration in knowledge processes is taken into account—processes which are later defined through explicit review but still play a significant role in interpretation. This change in perspective can be attributed to the gradual changing of the interpretative paradigm of the mind. Around the Eighties, the return to a connectionist view of learning types (Parisi 1989a) entailed a gradual giving up of an idea of the mind as being exclusively aware, abstract, rational and symbolic. Connectionist models saw relations of identity between the biological principles existing in nature and the processing mechanisms of intelligent machines, hypothesizing sub-symbolic, approximate and probabilistic interpretative models of reality, open to case-related variability and focusing on the evolutive role of time in the genesis of simulated networks. Self-organizing models, sensitive to the role of feedback as a regulatory adjusting function (Tabossi 1988) and introduced into adaptive, evolutive and dynamic contexts related to the biophysical matrix (Parisi 1989b; Frauenfelder and Santoianni 1997). The view of a situated, embodied mind (Frauenfelder and Santoianni 2003) encompassing the concepts of non-linearity and non-sequentiality can consequently also contemplate the presence of a non-traditional type of learning, such as implicit learning (Santoianni and Sabatano 2007). Implicit learning is now an integral part of the situated, embodied mind: while having its own peculiar specificity, it is no longer foreign to the mind but it is inside the mind. Hence the problem becomes to justify the presence of implicit processes in relation to explicit processing—an interpretative paradigm bringing together both dimensions. Rather than being one next to the other, the two dimensions seem to be able to be activated one after the other, or one together with the other, within a single process. Therefore, although they are distinct processes with different characteristics, they would not be activated only in a separate, parallel, autonomous way, independently of one another, but also in an inter-related, interdependent and presumably collaborative way. Implicit processes may be activated before explicit processing and may interact with it: they change but they are not always transformed by explicit reflection. The literature acknowledges the influence of prior knowledge on the complexification of the cognitive system (Dochy et al. 1999) both in the explicit processing of thought development processes—in more or less gradual ways (Carey 1991)—and in implicit processing. Implicit processes, however, play a default role (Reber 1993) in every phase of the interpretation of reality; the activation of the implicit component together with explicit processing could occur on demand, that is when required based on specific learning situations. The hypothesis of implicit processing as a cognitive antecedent of explicit processing—in any phase of cognitive development and without the need to become explicit—could be justified by the adaptive nature of the implicit dimension itself. The differences between the explicit and implicit could originate from phylogenesis and maintain some peculiar features in ontogenesis, such as the degree of resistance of the implicit to cognitive dysfunctions (Crawford et al. 2000). There could also be the presence of less significant variations in implicit processing than in explicit processing, related to the age difference between individuals (Reber 1992) and to individual differences in cognitive processing (Reber et al. 1991). In this view, implicit processing would appear as the oldest form of learning, prior to the acquisition of the awareness typical of explicit communication by the human race. Implicit learning would therefore be a process ‘at the root’ of the behavioral adaptive repertoire of every complex organism—a generalisable process being constantly present in cognitive processing. According to an evolutive model of implicit learning this process would be ‘prior’ to the explicit, in phylogenetic terms, and thus it would be characterized by specific functioning patterns (Kelso 1995) enabling it to maintain its prototypical qualities in ontogenesis too where, however, the presence of the implicit seems to slow down as the individual’s life unfolds (Don et al. 2003). The role played by implicit learning in individual life seems to be in inverse proportion to development. The greater the age, the more explicit learning takes over implicit learning, which still remains at the basic level. Both spatial and linguistic thought can be affected by implicit and explicit levels of knowledge. If spatial thought constitutes a cognitive antecedent at the phylogenetic level, in ontogenesis, however, the formation of ideas, theories and concepts expressed through language can also have an implicit origin. Rather than distinguishing between them, this matrix brings together individual approaches to knowledge (Stanovich 2009) and can be identified in the minor variations among the learning responses of several individuals (Reber et al. 1991). As age increases, individual approaches to knowledge grow more specialized and complex, though only in part. Some approaches, especially if they are functional from an adaptive perspective, can remain unchanged over time and thus manifest relatively constant, univocal features across individuals. Acknowledging implicit learning as holding a permanent prototypical nature, justified by its phylogenetic origin of primary adaptive function, makes it possible to better understand why it can play a role of non-linguistic cognitive antecedent in ontogenesis, compared to explicit processing, as a prototypical function inter-related to the shaping of explicit learning. According to this hypothesis, the role of implicit processing would be to act as the primary interpretative criterion adopted by the cognitive system in approaching understanding or knowledge-constructing processes. Knowledge prototypes would represent cognitive antecedents not necessarily liable to continuous transformations towards the explicit, with a permanent prototypical character. If implicit processing is taken as the default level being constantly available to explicit processing and, with it, contributing to the interpretation of reality, an interesting question from an educational perspective would be how such collaborative reaction can take place and, from a representational point of view, what format it may take on. Structural functions should therefore be identified which may be used in the mutual cognitive “exchange” between implicit and explicit levels of knowledge, possible traits d’union, the points linking the supposed cognitive continuity between implicit and explicit. This implies finding a ‘transitional’ representational format which may somehow constitute the link between the unaware shaping of an idea, represented to oneself as an implicit “knowledge object” (Entwistle 1995; Entwistle and Smith 2002) and its aware expression. This format could be defined by means of dynamic activation patterns of internal representations of thought meant as objects of knowledge able to affect cognitive performances directly, even without going through an explicit review and still collaborating with it. According to the Thinking Prototypes Theory (Santoianni 2009a, 2009b), if cognitive collaboration between implicit and explicit is continuous—in the child as well as in the adult, in inverse proportion to development, yet constantly present—modalities can be identified linking implicit and explicit knowledge and definable through dynamic patterns. In this hypothesis, implicit processing may consist of a prototype, i.e. an illustrative model acting as cognitive antecedent in the primary evolutive stages of cognitive development and in any learning situation, but knowledge prototypes would be not entirely implicit, due to their continuous interacting with the explicit. Knowledge prototypes may be considered possible linking
patterns in the presupposed intersection between implicit and explicit processing, activating as ‘linking nodes’ of knowledge approaches and shareable as generalisable cognitive organizing criteria. These dynamic
linking
patterns, which link implicit and explicit, play the role of prototypes of the possible models of explicit knowledge construction; they are competences shared by the formation of both linguistic and mathematical thinking and can be systematized into 3 classes (union, separation and correlation) made up of 2 logical functions each (add integration, chain sequentiality; each identification, compare comparison; focus inference, link correlation). This systematization into classes wishes to be an experimental hypothesis, dynamic and revisable, which is based—as will be shown later—on an inductive analysis of Italian language and is liable to change depending on the various linguistic matrixes. The core hypothesis is that in cognitive development it is possible to study the shaping and recognition of shared dynamic patterns, with an implicit matrix regarding the emerging characters of univocality, invariance and prototypicality, representing logical functions as linking modalities with the explicit levels of knowledge. The indication of prototypicality as permanent seems to characterize the functions of implicit learning as an aspect which is constantly present in cognitive development, integrating explicit processing. In an educational perspective, it would no longer be necessary for the implicit to be transformed into explicit; what would be needed, instead, is acknowledging the collaborative and integrative role implicit learning plays with regard to explicit knowledge, thus becoming a form of implicit knowledge. The prototypical, ‘elementary’ nature of the implicit component could be qualified as its own peculiar characteristic, distinguishing it from explicit processing without entailing a transfer from one to the other, but rather specific, situated convergences, collaborations and overlaps mediated by the activation of linking patterns. Overcoming a dichotomous vision of the explicit-implicit relationship does not mean that the implicit system could not be activated independent of the explicit system. What must be acknowledged is the continuing dimension of cognitive behavior, which leads us out of the “polarity fallacy” (Reber 1993). The concept of continuity takes the place of those of separation and dichotomy. In this hypothesis the linking patterns could refer to spatial representations, which are more likely to be affected by the implicit dimension of knowing (Chun and Jiang 1998). Indeed, spatial thought can entail the activation of both explicit and implicit processing. Both spatial and linguistic thinking can be generated and affected by explicit as well as implicit learning, although spatial thinking does not require verbalization and thus can also be expressed at lower levels of awareness. In the hypothesis that spatial and linguistic thinking are regulated by the implicit, in a collaborative relationship with the explicit, the question to be asked is how the linking patterns can express their belonging to both fields, and to the implicit in particular viewed as a real form of knowledge. As a matter of fact implicit learning can produce basic knowledge, regardless of the aware, cognitive learning effort—an abstract knowledge representing the structure of the environment, which can be used adaptively to solve problems and make decisions concerning new stimulating situations (Reber 1989). The computational field originally recognized the link of diagram-like spatial representations with explicit processing. Spatial representations were distinguished from textual representations (sentences) because the latter could contain implicit information about the logical relations existing among the various parts of a text, while the arrangement of textual information in space entailed the idea that these relations were made explicit (Larkin and Simon 1987). Spatial representations are defined as visual organizers which reflect thought patterns on the structure of knowledge content (Clarke 1991) and play a major role in the management of learning. The effectiveness of graphic organizers lies in the possibility given to students to delve into the logical relations among the various parts of a text. It is necessary, however, to determine whether a graphic representation should be interpreted or constructed. The difference may lie in identifying the logical relations among the parts of a diagram implicitly, by means of internal mental images, and having to represent them graphically through external mental images. The internal visualization of the representations can also occur in an implicit way only, whereas its external representation requires the activation of explicit reflexive processes. Thus if a student shifts from a text to a spatial representation it is not essential for the logical relations between parts of a text to be made visible, and the same goes for those between the parts of the diagram. In other words, the shift from the text to the spatial representation takes place in a cognitive collaboration which enables the activation of implicit processing while not excluding the explicit dimension. The present research aims at developing and defining the Thinking Prototypes Theory (Santoianni 2009a, 2009b) as a possible interpretative paradigm for a wide application in education. At the basis of complex thinking this theory identifies prototypical forms of knowledge, which are viewed as prototypical logical processes remaining in place throughout cognitive development and being activated in the possible collaboration opportunities between explicit and implicit thinking. Thinking prototypical processes are knowledge prototypes which express the logical relations at the basis of concept structuring in language, mathematics and possibly other fields of knowledge, theoretical and practical alike. In this hypothesis they would be abstract structural categories which are functional to adaptation and are affected by the implicit and explicit. Regarding the explicit component, the abstract structures that make it up are not independent of the type of knowledge they can regulate. The learnability of abstract structures, like the formation of situated knowledge structures (Hirschfeld and Gelman 1994), can vary depending on the type of domain. On the other hand, the abstract patterns of reasoning can affect learning as well; it should not be neglected that the formation of knowledge structures may vary depending on both domain specificity and domain general abstract models (Waldmann et al. 1995). This remark may also be true for both explicit processing, where the issue is debated (Wattenmaker 1995), and implicit processing, where there is greater agreement on the domain-general nature of implicit processes (Stanovich 2009). Knowledge prototypes and their related logical processes can thus be domain specific, that is related to the type of content they represent, and probably context-dependent, that is liable to be affected by the educational practice. At the same time, because they refer to an implicit primary and adaptive modeling they can also be domain general, while they are individual-dependent insofar as the formation of knowledge structures is always general and specific at the same time (Case 1996).",11
10.0,2.0,Mind & Society,19 March 2011,https://link.springer.com/article/10.1007/s11299-011-0085-6,What good is moral reasoning?,December 2011,Hugo Mercier,,,Male,Unknown,Unknown,Male,"In the classical cognitivist framework, moral reasoning was seen as a set of skills specialized for moral judgments and decisions. However, with the recent ‘intuitive turn’ in moral psychology, it has become apparent that it is more likely to be a set of intuitions that underpin morality. While there is debate over what intuitions morality consists of (Baumard in press; DeScioli and Kurzban 2009; Haidt 2007), many agree that it is these intuitions that make us moral. Reasoning still plays a role in our moral judgments and decisions by allowing us to ponder upon our intuitions or confront them with one another, but it is not clear that this ability is specific to morality (e.g., Haidt 2008; Harman et al. in press; Pizarro et al. 2003). On the contrary, it seems as if moral reasoning was nothing more (or less) than reasoning applied to moral intuitions. As a consequence of this shift in moral psychology, it becomes possible to look at the psychology of reasoning more generally in order to better understand moral reasoning. Indeed, several theories of moral reasoning are framed in, or at least compatible with, the more general dual process framework (Greene et al. 2004; Haidt 2001). Dual process theories of the mind divide up cognitive mechanisms in two categories (Evans 2007; Kahneman 2003; Sloman 1996; Stanovich 2004). The bulk of these mechanisms belong to ‘system 1’. System 1 mechanisms have been characterized as being automatic, relying on associations and heuristics. They tend to work quickly and to require little effort, as they do not usually rely on working memory (Evans 2003). By contrast, system 2 mechanisms are said to be controlled and based on rules. They are slow and effortful, taxing working memory. They are also usually thought to be conscious. Haidt’s characterization of moral reasoning as “a conscious process [which] means that the process is intentional, effortful, and controllable and that the reasoner is aware that it is going on” (Haidt 2001, p. 818) for instance, puts moral reasoning squarely within the ‘system 2’ category. Still, these definitions are vague, mostly reflecting a cluster of associated traits that have been observed to characterize different cognitive mechanisms. In what follows, I will rely on the more specific definition of reasoning put forward in Mercier and Sperber (in press-b), as outlined presently. While there is a quasi-consensus that system 1 is made of many different mechanisms, it is less obvious that system 2 can also be carved up into distinct cognitive mechanisms, or how the division should be made. Mercier and Sperber (in press-b) have suggested that reasoning is a specialized ‘system 2’ mechanism that bears on reasons: reasoning allows us to find and evaluate reasons. A reason is a part of an argument: it is used to support a given conclusion. When you look for a reason, you try to find a proposition that would be inconsistent with the rejection of the conclusion. For instance, if Mary wants to convince Peter that the movie “There Will Be Blood” is a good movie, she can tell him that it stars Daniel Day Lewis, who only plays in good movies. Unless Peter finds a counterargument—disputing the second premise for instance—it would be inconsistent for him to accept the argument while refusing to yield to the conclusion. From the listener’s or the reader’s perspective, evaluating a reason implies trying to gauge the degree of inconsistency between the acceptance of the reason and the rejection of the conclusion. In the example above, Peter may find that indeed once he has accepted the premises it would be inconsistent to reject the conclusion, since he cannot find any way to refute the premises or to reconcile the two. Paxton and Greene (2010, p. 6) put forward a similar definition of moral reasoning as the “conscious mental activity through which one evaluates a moral judgment for its (in)consistency with other moral commitments, where these commitments are to one or more moral principles and (in some cases) particular moral judgments”. Mercier and Sperber’s characterization is a more general formulation of the same principle that applies to reasoning beyond the moral realm. In this article, ‘reasoning’ will be used in the restricted sense that follows from this definition (see Mercier and Sperber in press-b, and Paxton and Greene 2010, for a defense of this definition). This allows focusing on a very specific type of mental mechanism and excluding other processes that may at least in part belong to ‘system 2’—and that are likely to also play a role in our moral lives—such as planning, hypothetical thinking or perspective taking (see Mercier and Sperber in press-a).",32
10.0,2.0,Mind & Society,30 June 2011,https://link.springer.com/article/10.1007/s11299-011-0086-5,On the economics of sleeping,December 2011,Tinna Laufey Asgeirsdottir,Gylfi Zoega,,Female,Male,Unknown,Mix,,
10.0,2.0,Mind & Society,28 September 2011,https://link.springer.com/article/10.1007/s11299-011-0092-7,Symposium on “Collective representations of quality”,December 2011,Camille Roth,Dario Taraborelli,Nigel Gilbert,,Male,Male,Mix,,
10.0,2.0,Mind & Society,13 August 2011,https://link.springer.com/article/10.1007/s11299-011-0089-2,Disaggregating quality judgements,December 2011,Bruce Edmonds,,,Male,Unknown,Unknown,Male,"Publishing is now simple, cheap and reliable.Footnote 1 Using websites, preprint repositories, blogs etc. makes it almost trivial to make any text or other electronic resource available to anyone with effective access to the internet. With print journals there was a considerable cost to typesetting, printing, and distributing texts so it was necessary to select what (non-vanity) items were published, so as to recoup the costs involved. To ensure this, what was published was selected, on the basis of what the readers would wish to read and hence (directly or indirectly) pay for. For the academic literature this involves judgements on such criteria as relevance, soundness, novelty, importance, and clarity. Only the papers that were judged to be worth publishing were made available for readers. Journals have to pay for their costs. However, journals have another reason for their existence than merely making knowledge available—publishing it—they also help select what is good so the reader does not have to trawl through lots of “sub-standard” papers. In other words it reduces the search costs for the reader—the time and effort the readers spend finding the articles that they want to read. With the growth of published material following the advent of internet publishingFootnote 2 this search cost has drastically increased and academics are faced with an exploding literature—growing far faster than they can cope with. Furthermore, the literature is fragmenting—in many fields (and especially interdisciplinary fields such as complexity science) it is no longer the case that one can keep up with the important developments by only reading a few core journals. Papers are distributed across many sources and fields. Internet services such as Google, del.icio.us, twine etc. aim to help a user discover the information they wish to access. In particular, Google Scholar allows one to search the online literature according to its internet citation strength—roughly how many times it is cited by other academic papers on the internet. Thus one can quickly find the texts that other academics have thought worth to link to or cite. This allows readers to browse the results of their search for papers on a particular topic in order of internet citation strength. Since internet citation strength is a strong indicator of importance, quality etc. this makes it more likely that a reader will find the more significant texts on any particular topic. However, all these systems use a rough proxy for the importance of a paper—number of citations, what other people are interested in etc. they do not provide direct and reliable quality information but they do it cheaply (i.e., without the need for employing lots of people to select papers). Any particular measure will have its advantages and biases—there is no perfect measure that will suit everybody (Van Noorden 2010).",
10.0,2.0,Mind & Society,19 August 2011,https://link.springer.com/article/10.1007/s11299-011-0087-4,Quality versus mere popularity: a conceptual map for understanding human behavior,December 2011,R. Alexander Bentley,Michael J. O’Brien,Paul Ormerod,Unknown,Male,Male,Male,"When faced with making decisions that involve multiple options, agents can do one of two things: They can learn individually, meaning they attempt to think everything through by themselves, or they can cut corners and learn socially by using other agents as sources of information. Social learning characterizes diffusion models (Bass 1969; Rogers 1962), informational cascades (Bikhchandani et al. 1992; O’Brien and Bentley 2011; Ormerod and Colbaugh 2006; Schiffer 2005; Watts 2002), information scrounging (Mesoudi 2008; Rogers 1988), and even maladaptive behaviors that spread by “hitchhiking” on the prestige of the people demonstrating them (Henrich and Gil-White 2001). Indeed, evolutionary anthropologists and psychologists (e.g., Dunbar and Shultz 2007; Herrmann et al. 2007) have argued persuasively that the anomalously large brain (neocortex) size in humans evolved primarily for social-learning purposes. In view of the different processes and scales involved in decision-making, especially decisions about the quality of a behavior or product, how do we determine which one predominates in a given situation?",9
10.0,2.0,Mind & Society,09 September 2011,https://link.springer.com/article/10.1007/s11299-011-0088-3,"Lovely weather, isn’t it? On the social dynamics of quality judgment",December 2011,Andrzej Nowak,Katarzyna Samson,Michal Ziembowicz,Male,Female,Male,Mix,,
10.0,2.0,Mind & Society,14 September 2011,https://link.springer.com/article/10.1007/s11299-011-0090-9,Food quality as a public good: cooperation dynamics and economic development in a rural community,December 2011,Riccardo Boero,,,Male,Unknown,Unknown,Male,"The recent evolution of agriculture has seen at work the interplay of dynamics of further spreading of industrialization in food production and dynamics of lengthening of transportation distances due to the involvement of peripheral areas and the reduction of transportation costs. Such a trend implies an increase in the complexity (Murdoch 2000: 410ff) and in the length of the whole food chain, from the production to the distribution to final customers. But peripheral areas that are nowadays starting to be involved in the industrialization of the food sector are often areas characterized by fragile and sensitive environments which are threatened by the change implied by such a kind of development process. Similarly, such areas are generally characterized by traditional small producers: the evolution of technology (from biotechnology to the standards required by distribution chains) and the diffusion of industrialization promote the diffusion of larger producers capable of gaining stronger economies of scale and they, thus, negatively impact the social sustainability of the development process. Thus it is not surprising that much attention has been posed on alternative initiatives for rural development, and in fact the just mentioned criticalities of the main development paradigm represent a chance for different models of rural development as the one studied in this paper. Looking at the several examples of different approaches to rural development it can be noted that they all share some characteristics, the most important of which is that they are based upon a non price-based competition that enroots its competitive advantage in innovating the exploitation of local resources (Murdoch 2000: 415). The innovation here referred to is of a particular kind which does not aim at labor saving but at transforming local resources, knowledge and social relationships in economic advantages. The means by which the local resource gives raise to the development process is quality, meaning quality of the processes of exploitation and transformation and thus of the resulting product. Products in fact are, literally, the outcome of a production process which transforms natural and human inputs in an output the quality of which is not only dependent on the quality of the inputs that have been used but also on the quality of the rules and procedures which drive the production process. In other words, the crucial role played by transformation processes in quality creation is confirmed by the alimentary sector, in line with other experiences in quality control and certification in other economic industries where the quality of the outcome can be assessed and promoted only through the quality of the procedures governing production. The phenomenon studied in this work is a rural development initiative, promoted by the Slowfood movement and named “Presidia”, aiming at preserving traditional foods at risk of extinction by creating production systems and markets for such products. Presidia are based upon the cooperative behavior of several producers and quality is the key element guaranteeing the systemic welfare. In the following of the paper a dilemma concerning the level of quality in one single example of Presidia will be studied through a field analysis and an agent-based simulation (Gilbert 2008; Gilbert and Troitzsch 2005), thus the paper is organized as follows: in the following section Presidia will be presented along the specific case that has been studied. Then, in the third section the research question will be introduced, and in the fourth one the model will be described. Finally model simulations results will be presented and some conclusions discussed.",7
11.0,1.0,Mind & Society,27 April 2012,https://link.springer.com/article/10.1007/s11299-012-0105-1,Special issue on: Dual process theories of human thought: the debate,June 2012,Laura Macchi,David Over,Riccardo Viale,Female,Male,Male,Mix,,
11.0,1.0,Mind & Society,04 January 2012,https://link.springer.com/article/10.1007/s11299-011-0093-6,Defining features versus incidental correlates of Type 1 and Type 2 processing,June 2012,Keith E. Stanovich,Maggie E. Toplak,,Male,Female,Unknown,Mix,,
11.0,1.0,Mind & Society,08 January 2012,https://link.springer.com/article/10.1007/s11299-011-0096-3,"Dual processes, probabilities, and cognitive architecture",June 2012,Mike Oaksford,Nick Chater,,Male,Male,Unknown,Male,"Dual process approaches (Evans 2003, 2007; Stanovich and West 2000; Wason et al. 1975) invoke two separate cognitive systems to explain performance on a variety of cognitive tasks. These are labelled System 1 and System 2. System 1 processes are rapid, parallel and automatic and only their final product is posted in consciousness. They are most often described as involving associative learning processes as captured by neural networks and are described as not requiring the resources of working memory (Evans 2003, 2007). In contrast, System 2 thinking is slow and sequential and makes use of the central working memory system. In particular, System 2 “permits abstract hypothetical thinking that cannot be achieved by System 1” (Evans 2003, p. 454). In the area of human reasoning, Evans (2003, 2007) contrasts the dual processes approach with single system/process theories, in particular “single-level probabilistic treatments” (Evans 2007, p. 98) like those he argues are provided by Oaksford and Chater’s (2007, 2009a, b) probabilistic approach to human reasoning. In this paper, we will summarise the arguments that we have put forward against this characterisation of the probabilistic approach arguing that the comparison between dual process theories and our probabilistic approach conflates computational levels of explanation (Oaksford and Chater 2011). However, our argument in this paper is more general and not solely directed at Evans characterisation of the probabilistic approach. Indeed, as we shall see, under at least one interpretation, we agree with Evans. In particular, we agree that System 2 processes at least as far as they involve conditional, if … then, reasoning are probabilistic (Evans and Over 2004). However, there are other theorists supportive of a dual process approach (Heit and Rotello 2010; Klauer et al. 2010; Rips 2001, 2002; Stanovich 2000, 2011) who argue for a System 2 based on standard truth functional binary logic. Our arguments particularly target this group of dual process theorists. We also explore the consequences of single function dual process implementations of the probabilistic approach in a standard cognitive architecture that invokes a distinction between working memory and long-term memory. Finally we consider the argument that a single function view cannot explain non-modal responses or the correlations between IQ and normative responses (Stanovich 2010, 2011). We propose that single function view can be reconciled with these findings.",22
11.0,1.0,Mind & Society,03 March 2012,https://link.springer.com/article/10.1007/s11299-012-0102-4,"Probabilities, beliefs, and dual processing: the paradigm shift in the psychology of reasoning",June 2012,Shira Elqayam,David Over,,Female,Male,Unknown,Mix,,
11.0,1.0,Mind & Society,24 January 2012,https://link.springer.com/article/10.1007/s11299-011-0094-5,Dual systems and dual attitudes,June 2012,Keith Frankish,,,Male,Unknown,Unknown,Male,"According to dual-system theory, humans possess two reasoning systems, usually referred to as System 1 and System 2 (henceforth S1 and S2). Typically, S1 is characterized as a collection of autonomous subsystems, many of them evolutionarily ancient, which are shaped by biology and personal experience, and whose operations are fast, automatic, non-conscious, parallel, and independent of working memory. S2, by contrast, is held to be a uniquely human system, which is shaped by culture and formal tuition, and whose processes are slow, controlled, conscious, serial, and demanding of working memory. Numerous converging lines of argument from different disciplines support this broad picture (for surveys, see Frankish and Evans 2009; Frankish 2010; Evans 2011). However, many issues remain unsettled, in particular about how the two systems are realized in the brain. And there are pressing questions about S2. Why did a new reasoning system evolve alongside the older one, especially as there appear to be a number of specifically human adaptations within S1, including systems for language and theory of mind? And how could S2 processes be shaped by culture and formal tuition? How could one learn to reprogram one’s reasoning system? (Carruthers 2009). There is a way of thinking about S2 that offers answers to these questions. The idea is to think of S2 processes as intentional actions, involving the manipulation of sensory images, in particular linguistic ones. On this action-based view, S2 is a ‘virtual’ system, which does not have a separate neural basis, but is partially realized in S1 processes. Peter Carruthers has described a detailed action-based architecture for S2. This paper examines this architecture and explores a question about its interpretation. Specifically, it asks whether it is compatible with the view that S2 has its own suite of propositional attitudes, distinct from those associated with S1. (By ‘propositional attitudes’, or ‘attitudes’ for short, I mean any thoughts with propositional content—beliefs, desires, intentions, hopes, etc.) In other words, does the action-based version of dual-system theory support a dual-attitude theory as well? Carruthers argues that it does not, and that (with limited exceptions) talk of conscious attitudes should be eliminated (Carruthers 2011, Chap. 12). However, I shall argue that this is too hasty. There are good reasons to believe that we have distinct S2 attitudes, and, developed in the right way, the action-based view allows for this. The rest of the paper is structured as follows. Section 2 sketches the action-based view and its advantages. Section 3 introduces dual-attitude theory and explains Carruthers’s case for its incompatibility with the action-based view. Section 4 replies, arguing for a view of S2 attitudes as virtual ones, which are partially realized in S1 attitudes. The final section considers some objections to this view and responds to them.",11
11.0,1.0,Mind & Society,27 March 2012,https://link.springer.com/article/10.1007/s11299-012-0103-3,Intuitive and analytical processes in insight problem solving: a psycho-rhetorical approach to the study of reasoning,June 2012,Laura Macchi,Maria Bagassi,,Female,Female,Unknown,Female,"The general label of dual-process theories (Evans and Over 1996; Hammond 1996; Sloman 1996; Stanovich and West 2000) covers a number of models, which share an ancient dichotomy between logical and common reasoning, and has always characterized psychological theories on reasoning. A representative example could be the syllogistic reasoning studies that highlighted the conflict between logic and naive, intuitive reasoning (see, for instance, the conflict hypothesis by Politzer 1986, Mosconi 1998). Although the traditional dual-process models differ among themselves, they all distinguish two reasoning systems: an associative system (System 1), which is fast, automatic and effortless, and a rule-based system (System 2), which is slow, analytic and effortful. The associative system reflects “similarity and temporal contiguity” and is generally unconscious, while the rule-based system is characterized by productivity, systematicity, sequential operations and catches the “logical” structure of the environment (Sloman 1996). In particular, the role of language and context in cognition processes associated with language, belief-based and pragmatic reasoning, belong to System 1, whereas processes independent of language such as abstract, logical reasoning, belong to System 2. Processing both the intention and the context of a task, which is crucial to communicative interaction, has usually been considered an obstacle to correct reasoning, while decontextualization of the task would favour a correct form of abstract reasoning. The primacy of conversational features in System 1 leads to “… the fundamental computational bias in human cognition—the tendency toward automatic contextualization of problems. On the other hand, the more controlled processes of System 2 serve to decontextualize and depersonalize problems. This system […] is not dominated by the goal of attributing intentionality nor by the search for conversational relevance” (Stanovich and West 2000, p. 659). Recently, some theoretical models have suggested moving away from the S1 and S2 dichotomy by reconsidering the role of intuition (Betsch 2008), and the relationship between thought and language (Evans and Frankish 2009; Stanovich 2011). It is interesting to note that the main revisions of the dual-process approach share a modular theory of thinking, which implies interaction between the module of thought and the module of language, in its communicative function (linguistic and pragmatic modules, and the module of Theory of Mind in the social intelligence domain). Both Frankish (2009, 2010) and Carruthers (2006, 2011) postulate a virtual system, which controls processes at the unconscious level. Both authors consider the conscious mind to be a language-dependent virtual machine. In particular, manipulating utterances of inner speech is crucial for producing the modular level effects that are appropriate to the thoughts that utterances express. Mercier and Sperber (2009; see also Sperber and Wilson 2002) also propose a “massive modularity” approach to thinking and language interaction, where pragmatics are a sub-module of the Theory of Mind. Evans (2009, 2010) considers the accountability of the modular view of mind (old vs. new) to be plausible and argues that the new mind evolved in humans through the development of modules, especially the language module, “which could support abstract and metarepresentational thought”. “….The contents of our consciousness include visual and other perceptual representations of the world, extracted meanings of linguistic discourse, episodic memories, and retrieved beliefs of relevance to the current context.” (2009, p. 37, 40). Evans (2010) considered that, as much of the contents which pass through our working memory (WM) is semantically rich, it would be highly problematic to describe Type 1 processes as contextualized and Type 2 processes as abstract and decontextualized. His new conception then overcomes the traditional dichotomous view, endorsing a clear evolutionary argument for a two minds theory, incorporating the distinction between Type 1 and 2 processes. According to our hypothesis, insight problem solving is a crucial issue at this stage of the debate. Insight problems have traditionally been considered tests of giftedness (see for instance, Sternberg and Davidson 1986). More recently, Frederick (2005) used insight problems for the Cognitive Reflection Test (CRT). Our claim is that the reflective, conscious, serial process does not guarantee a solution to insight problems, since it is reached only when the subject suddenly restructures and has an in-sight. The insight problem solution is achieved by productive, creative thinking, resulting mainly from a covert and unconscious thinking process, and an overall spreading activation of knowledge, which includes a relevant, analytic, goal oriented search, which goes beyond associative or reproductive thinking. The conscious mind suddenly becomes aware of the solution when it is discovered in its wholeness, not step-by-step after a period of incubation. We have already considered the insight problem solution for the traditional dichotomy of System 1 and System 2 to be problematic (Macchi and Bagassi 2006). Which System could guarantee the solution to an insight-problem? The characteristics of the processes attributed to the traditionally conceived System 2 (slow, serial, self-aware) were typical of a kind of reasoning by which problems are solved with a step-wise search enforced by the WM limits,Footnote 1 traditionally studied by information processing theorists. However, this approach neglected other kinds of reasoning, such as the solution to insight problems. It involves a restructuring process that can be fast, but also slow, parallel, both intuitive and analytic, preceded by a period of incubation, which is not ascribable to consciousness only. Even if the traditional dualistic view has been generally revised, it is still worthwhile to reflect on this special type of problem solution. In our perspective, this way of thinking is very close to the process involved in the understanding of an utterance: people drop the default interpretation (fixation in insight problem solving) in order to “restructure”, to grasp the meaning relevant to the context and to the speaker’s intention. In doing so, reasoning also involves implicit, parallel processes. Restructuring cannot be entirely supported by WM, which is occupied by fixation and a default interpretation or Einstellung. Like other new models, we therefore suggest a view of thinking that considers context processing, which is always relevance-oriented, as cutting across both intuitive and analytical, reflexive thinking. Context, “situations” as Barsalou (2009) would say, appear to enhance cognitive processing by increasing the “breadth and specificity of inference”. Rather than searching through every piece of knowledge stored in long-term memory, all the system has to do is focus on the knowledge and skills relevant to the current situation. Reasoning is the ability to produce specialized representations (and their relationships), which support goal pursuit in the current setting. In the study of classical reasoning tasks, communicative heuristics, which process context, the intention of the speaker and the aim of the task, have usually been considered as an obstacle to the solution. According to our view however, this interpretative processing is instead the core of thinking, a way of functioning shared by both language and thought (Bagassi and Macchi 2006; Bagassi et al. 2009; Macchi 2000; Macchi and Bagassi 2006). We consider thinking and speaking, the logos, as two sides of the same cognitive process, which is close to the concept of effective communication of ancient rhetorics (see the pioneering Psycho-rhetorical approach by Mosconi 1978). Linguistic mechanisms are deeply interpenetrated by interactive thinking, and, vice versa, communicative heuristics deeply inhabit our minds (Levinson 1995). The presupposition of this hypothesis is that a context is always “in action”. Default-heuristic responses may also be given when rules are applied to abstract tasks (Stanovich 2009). Consider, for instance, Wertheimer’s well-known problem of the square and parallelogram: the privileged altitude of the parallelogram is that inside the figure and relative to the long side (Fig. 1). The “square and parallelogram” problem. Given that AB = a and AG = b, find the sum of the areas of square ABCD and parallelogram EBGD
 The default representation (a preferred organization of stimulus and a generalised interpretation of the text of the problem) does not, however, allow participants to pursue the solution. Geometric shape perception is addressed by analytical default, that results in a fixation. In our view, overcoming a fixation requires elaboration and comprehension of the context that is more relevant to the aim, and not abstraction. The interrelations between the parts of the default interpretation have to be loosened in order to perceive the new, in other words to restructure. The impasse, the difficulty of reaching a solution, should activate an increasingly focussed and relevant search and a reconsideration of the available data in the light of the goal. It is an analytical research “that recognizes the primacy of the whole, and its dynamic nature, and its decisive role in articulating its parts” (Wertheimer 1985, pp. 24, 25). According to Wertheimer, a “solution becomes possible only when the central features of the problem are clearly recognized, and paths to a possible approach emerge. Irrelevant features must be stripped away, core features must become salient, and some representation must be developed that veridically reflects how various parts of the problem fit together; relevant relations among parts, and between parts and whole must be understood, must make sense”. (ibidem, p. 23). This process is analogous to that involved in communicative interaction, for instance, grasping a witty comment and understanding rhetorical figures of speech such as irony and metaphors. This search activity goes beyond the boundaries of WM. The process that leads to the discovery of the solution through restructuring is therefore mainly unconscious and can only be described a posteriori.Footnote 2 It is characterized by a period of incubation, which may vary in length, and is necessary to loosen the interpretation default constraints on WM and allow for a new exploration guided by the relevance of the knowledge of the individual. The traditional distinction between fast-intuitive and slow-reflexive adds little to our understanding of the different types of thought processes. We are not referring to different types of thought, conceived as either an architecture of the mind (two systems) or as cognitive styles (intuitive vs. reflexive), but to different types of process, depending on the characteristics of the problem the subject is called to resolve. We have explored this issue by examining the potential interference in insight processes that may result from concurrent verbalization, faced with a classical interpretation that shows the benign effect of the verbalization on non-insight problems. With non-insight problems (e.g., “Criptoarithmetic”, “Missionaries”, “Tower of Hanoi” problems), the solution process cannot be reached all at once. It is serial and stepwise, with the solution being processed step by step, with a gradual simplification of the problem, by a rule-based and conscious reasoning and so reached at the end of a path,. With tasks of this kind, “on-line” serial verbalization is typically benign, and think-aloud yields valid data (Ericsson and Simon 1984) on stepwise thinking. These data render the study of thinking in humans possible by simulating the “typical protocols” of problem solving in well-known computer programs. Restructuration in insight problem solving, on the contrary, requires a goal-oriented search into the individual’s knowledge. While employed in this search, thinking is capable of handling a huge amount of information and widespread solution research by parallel processing (Wertheimer 1985). A similar in fieri search may be silent or accompanied by a spontaneous and fragmentary form of verbalization. Only in retrospect, after the problem has been solved, is it possible to find verbalization that expresses the solution that has been reached through restructuring. In insight problem solving, as the solution has yet to be discovered the on-line
serial verbal report of a solution path and its steps promotes a reliance on the default interpretation, which is not functional to the solution. In this study we investigated the interrelationship between language and thought in insight problem solving, in both its positive (Experiments 1 and 3) and negative (Experiment 2) effects. First, we hypothesized that fixation is the result of a thought process that privileges default interpretation of the perceptive stimulus, as well as the text of the problem. 
Restructuring in insight problems, therefore, is a form of overcoming the default interpretation, and productively re-interpreting the relationship between data and the aim of the task, guided by the principle of relevance. Hence, overcoming fixation is not achieved by abstracting from the context, but contextualizing further and in greater depth. If this is true, then a relevant reformulation of the problem should produce an increase in restructuring and in the solutions (Experiment 1). In Experiment 2, we examined the disruptive effect of serial concurrent verbalization on the solution of insight problems, by comparing “verbalization” versus “no-verbalization” conditions. Subjects were instructed to think aloud while solving the problems. We examined two classical problems: the “Square and parallelogram” and the “Pigs in a pen”. We predicted that serial verbalization would decrease insight solutions. In our final experiment, we further investigated the interpretative activity of thinking, by studying the “Bat and ball” problem, which is part of the CRT. Correct performance is usually considered to be evidence of reflective cognitive ability (correlated with high IQ scores), versus intuitive, erroneous answers to the problem (Frederick 2005). According to our hypothesis, independently of the different cognitive styles, erroneous responses could be the effect of the rhetorical structure of the text, where the question is not adequate to the aim of the task. Consequently, we predicted that if the question were to be reformulated to become more relevant, the subjects would find it easier to grasp the correct response. In the light of our perspective, the cognitive abilities involved in the correct response were also reinterpreted.",24
11.0,1.0,Mind & Society,03 January 2012,https://link.springer.com/article/10.1007/s11299-011-0097-2,Rational decision making: balancing RUN and JUMP modes of analysis,June 2012,Tilmann Betsch,Carsten Held,,Male,Male,Unknown,Male,"Beginning with the rise of cognitivism in the 1950’s, psychologists began to adopt fundamental principles of rational choice theory. Ward Edwards (1954) advised psychologists to use utility theory—the subjective version of the rational choice model—as a descriptive model of decision making. Accordingly, humans were assumed to use a linear rule for integrating subjectively perceived probabilities with the utilities assigned to the outcomes of decision alternatives. The same linear rule for information integration was also adapted in other areas of psychology such as research on the attitude-behavior relation (Fishbein and Ajzen 1974), motivation (Atkinson 1957) and judgment (Anderson 1971). The usefulness of rational choice theory as a descriptive model of decision making has been challenged (Simon 1955). Most prominently, the heuristics-and-biases research program in cognitive psychology (e.g., Kahneman et al. 1982; Nisbett and Ross 1980) has accumulated striking evidence that individuals systematically violate basic assumptions of rational choice theory and other normative models (rules of probability, logic etc.). These findings indicate that individuals are not able to make rational decisions but, rather, use simple heuristics. The application of heuristics was said to be responsible for systematic violations of normative standards. These results suggest that we should dismiss the variants of rational choice theory as descriptive models. On the other hand, such theories are so beautifully simple and rigorously formulated that psychologists were reluctant to dismiss them.",5
11.0,1.0,Mind & Society,05 January 2012,https://link.springer.com/article/10.1007/s11299-011-0095-4,The social functions of explicit coherence evaluation,June 2012,Hugo Mercier,,,Male,Unknown,Unknown,Male,"Communication is an incredibly powerful tool but one fraught with the dangers of manipulation. Evolutionary biologists have shown that for a communication mechanism to be stable, it has to be beneficial both to the senders and to the receivers (Dawkins and Krebs 1978; Krebs and Dawkins 1984). If deception is too costly for receivers, they stop receiving, extinguishing communication. In the case of ostensive communication, humans rely on a set of mechanisms to evaluate communicated information. These mechanisms of epistemic vigilance (Sperber et al. 2010) allow us to avoid most harmful communication. Trust calibration is crucial for epistemic vigilance: we do not place equal weight on the words of family members and strangers, doctors and hobos. Yet trust calibration is also insufficient: we shouldn’t believe everything family members and doctors tell us, or discard all of what strangers and hobos say. To supplement trust, we must rely on an analysis of the content of communicated information, enabling a fine-grained discrimination between statements from the same speaker. Content analysis for epistemic vigilance can take two related forms. First, the content of communicated information can be pitted against our previous thoughts—beliefs, desires, plans, etc. While our thoughts are the products of mental mechanisms that work for our better good, communicated information can be deceptive. Incoherence between the two thus provides some grounds for rejection of the communicated information. The second type of content analysis pits communicated information against our previous beliefs about the speaker—or, more specifically, against thoughts the speaker has given us reason to attribute her. The aim here is to detect internal incoherence in the speaker. Internal incoherence is a useful cue to evaluate communicated information for several reasons. When the speaker makes incoherent factual claims about the world something is amiss, since the world is coherent. She’s either lying or mistaken: in either case, we should not accept what she says. Even when the speaker talks of other people’s mental states—which can be incoherent—she should remain coherent. If she isn’t, it means either that she has introduced the incoherence herself, which is problematic, or that she has failed to spot the incoherence in the other person’s thoughts, which is also problematic. Finally, when the speaker talks about her mental states or behaviors, she should also remain coherent but for different reasons, presently explained. At a diner party, Simon tells his friend Margo that he’s taking a holiday in Hawaii the following month. Next month, Margo sees Simon in the Bahamas. She feels justified in asking him what made him change his plans, even though he didn’t make any explicit commitment—he didn’t swear he would go to Hawaii. And Simon would have to provide a reason for his behavior if he did not want to be perceived as fickle, or weird, or worse—could he have lied to Margo? The reason why Simon’s behavior can be poorly perceived is that people are implicitly committed to everything they say (see Walton and Krabbe 1995). Once someone says something, it can be used by others to guide their own behavior. They should be able to rely on it, even if it doesn’t seem to affect them directly. For instance, even if Margo does not plan on travelling with Simon, she could use what he said to infer that Hawaii is a good vacation spot at this time of the year. As a result, it is safer for speakers to assume that listeners take them to be committed to what they have said. Speakers who are not consistent can expect some questions. Interestingly, the scrutiny by listeners should not be restricted to statements. In the example above, it is Simon’s behavior that is incoherent with his previous statement. There is no reason to think that Margo’s reaction would have been much different if he had told her instead, a week after their first discussion, that he was going to the Bahamas. Here as well, he would have had to provide some kind of justification for his change of mind, indicating that such turnarounds make people tick. Finally, in many cases people do not have to explicitly agree with a statement for coherence evaluation to be activated. The same mechanisms extend to thoughts we have reasons to think others may attribute us. For instance, if I go out of my way to order a vegetarian dinner at a French restaurant (not a mean feat), I would be justified in thinking that other people would think I’m a vegetarian. If that belief is in any way important in the current situation, failing to correct people’s mistaken attribution may be perceived as slightly misleading. Deniability can be maintained—as opposed to the case of an explicit statement—but people can blame me nonetheless. How does this mechanism of coherence evaluation work? Coherence itself is determined by regulatory, system 1 processes. We do not need new mechanisms to tell us which belief is coherent with which. All that is necessary is a mechanism that represents the degree of coherence between different mental states and draws inferences from it. However, the vast quantity of beliefs any human has would make the general application of coherence evaluation computationally intractable. So one of the main questions is: what mental states should be taken into account in this coherence evaluation? We can distinguish between bottom-up and top-down factors. Bottom-up factors are dictated by inference mechanisms that do not have an evaluative function—mostly language comprehension. When we try to make sense of an utterance, many beliefs—or other mental states—are made relevant (Sperber and Wilson 1986). If one of these beliefs is incoherent with what we are being told, not only are we likely to reject the utterance, but the incoherence is likely to be represented and used to explain the rejection. Top-down factors, by contrast, are part of the evaluation process. The range of beliefs taken into consideration may be proportional to the distrust we have towards the speaker: less trusted speakers can make us extend the search for incoherence.Footnote 3
 Several threads of evidence confirm the existence of a coherence evaluation mechanism used for epistemic vigilance. The most direct evidence comes from an examination of what happens when people are confronted with communicated information that contradicts prior beliefs. Young children (3-year-olds) are able to reject communicated information that conflicts with the product of their own perception (Clément et al. 2004). It is still possible however that at this age children only reject such communicated information without representing the incoherence and drawing inference from it—for instance inferences about the trust they should grant the speaker in the future. We know however that 6-year-olds can represent incoherent information since they are able to point to the passage of stories that conflict with their general knowledge (Vosniadou et al. 1988). In adults, the degree of incoherence with background beliefs is a primary determinant of the way communication is treated: the less coherent communicated information is with our beliefs, the less likely it is to be accepted (Petty and Cacioppo 1979; Edwards and Smith 1996; Yaniv 2004). People are also sensitive to incoherence within communicated information. The internal coherence of statements is one of the most important cues used to tell truths from lies (see, for review, DePaulo et al. 2003). When people read, they are sensitive to incoherent statements—they take longer to read them and memorize them better (e.g., Albrecht and O’Brien 1993; Myers et al. 1994). There seems to be less direct evidence that behaviors that are incoherent with previous statements are detected and lead to inquiries or negative judgments. People tend to dislike those who fail to keep their promises, and there is a need for a mechanism of coherence evaluation in order to detect when someone has failed to keep their promises. Yet one could argue that such a mechanism should be restricted to strong, explicit forms of commitment. The evidence regarding implicit commitments seems to be scant. One indication is the strength of the ad hominem tu quoque, which relies entirely on our distaste for people who say or do incoherent things. In a tu quoque, one of the debaters attacks the other debater’s position by declaring that she had either said or done things that are incoherent with her position. For instance, rebuttals against someone advocating recycling could be: “but you don’t recycle” or “but you said last week that it was not worth it.” Most treatises on argumentation consider the tu quoque to be a fallacy, and I will not try to dispute this judgment here. It is enough to notice that it is usually considered to be a very effective argument, showing that people mistrust those who fail to remain consistent.",15
11.0,1.0,Mind & Society,12 February 2012,https://link.springer.com/article/10.1007/s11299-012-0100-6,Analytic thinking: do you feel like it?,June 2012,Valerie Thompson,Kinga Morsanyi,,Female,Female,Unknown,Female,"Under this proposal, Type 1 processes generate two distinct outputs: the content of the initial answer and an accompanying sense of the correctness of that answer (Simmons and Nelson 2006; Thompson 2009). An item from Frederick’s (2005) Cognitive Reflection Test (CRT) illustrates the point: If it takes 5 machines 5 min to make 5 widgets, how long would it take 100 machines to make 100 widgets? ____ minutes. The answer “100” is accompanied by a strong sense that it is the correct one, and is given in error by about two-thirds of participants. This sense of correctness is a metacognitive judgment, one of several that are routinely used to assess the workings of our cognitive processes, and in particular, the degree to which such processes have functioned or will function correctly (see Dunlosky and Bjork 2008 for a review). As illustrated by the widget example, however, metacognitive processes function imperfectly. This is because the origins of the sense of confidence are derived from experiences associated with producing the answer, rather than the content of the answer itself (e.g., Benjamin et al. 1998; Jacoby et al. 1989; Koriat 2007; Schwartz et al. 1997). Thus, it is possible for people to express high degrees of confidence in completely false or inaccurate memories (Roediger and McDermott 1995; Sporer et al. 1995). We propose that the answer “100” feels right for similar reasons. As is the case in other cognitive domains, we propose that judgments of confidence on reasoning tasks are, in fact, inferences based on experiences associated with generating an answer, such as the fluency with which the answer comes to mind (see Koriat 2007 for a review). For example, the fact that an item is remembered quickly and easily is sufficient to create a strong, and sometimes misleading, sense that it has been or will be correctly recalled (e.g., Benjamin et al. 1998; Costermans et al. 1992; Jacoby et al. 1989; Kelley and Lindsay, 1993; Robinson et al. 1997; Whittlesea and Leboe 2003). Similarly, the FOR that accompanies the answer “100” is strong because the answer was generated fluently. Given that the fluency heuristic can be misleading, why do we rely on it? The reason is that when people retrieve facts from memory, fluency is very often a valid cue to difficulty (Koriat 2007). For example, repeated encounters with an item increases speed of recognition, so that the names, concepts, and words that are encountered most often are recognized most quickly. Thus, objects that come to mind quickly are those that have been encountered frequently (Hertwig et al. 2008), so that relying on the fluency of retrieval will often give accurate feedback about the contents of memory. Consequently, despite knowing very little about tennis, one might be relatively confident that Roger Federer is a top player, based on the fact that one can quickly recognize his name (Hertwig et al. 2008).",46
11.0,1.0,Mind & Society,15 February 2012,https://link.springer.com/article/10.1007/s11299-012-0101-5,"Implicit cognition, emotion, and meta-cognitive control",June 2012,Ron Sun,Robert C. Mathews,,Male,Male,Unknown,Male,"The long-standing goal of our research has been to understand the interaction of implicit and explicit processes of cognition. Mathews et al. (1989) brought up the issue of the co-existence of two separate processes, implicit and explicit, in cognition, and the synergy resulting from their interaction, on the basis of experimental data on implicit learning tasks. Sun (1994, 1995) developed a computational cognitive model that contained distinct implicit and explicit processes (for modeling everyday commonsense reasoning). Sun et al. (2001, 2005) applied a variant of the model to the understanding of implicit learning tasks capturing the synergy resulting from the interaction of implicit and explicit processes. On that basis, our present work focuses on the role of emotional processes in individuals’ choices and how meta-cognitive control may alter them. Even though people are sometimes unaware that their emotional reactions are influencing their choices, such an effect can be demonstrated. Moreover, there are individual differences in their natural tendencies in this regard. In this research, we examine meta-cognitive control of emotional biases and distractions, and variables that may alter an individual’s natural tendencies in this regard. The specific questions addressed are whether and how emotional and meta-cognitive processes can also be separated into implicit and explicit components, and how such a separation can be utilized to improve self-regulation of emotional distractions (such as emotional “addiction” or phobia). We hypothesize that emotional distractions can be explained through motivational blending and competition, and may be overcome through meta-cognitive training/learning. This work involves a combination of human and computational experiments. The human experiments are to tease apart implicit and explicit processes involved in emotion and meta-cognition. The focus is on self-regulation, in the sense of controlling one’s own cognitive and emotional processes and also in the sense of allocating tasks to explicit or implicit processes. In computational modeling, we aim to capture and explain human data through the use of two types of computation for capturing implicit and explicit processes respectively as well as through motivational modeling. The result will be a broad computational theory of the emotion/meta-cognition interaction with a novel perspective centered on the implicit/explicit distinction.",16
11.0,1.0,Mind & Society,27 March 2012,https://link.springer.com/article/10.1007/s11299-012-0104-2,Spot the difference: distinguishing between two kinds of processing,June 2012,Jonathan St. B. T. Evans,,,Male,Unknown,Unknown,Male,"The distinction between two kinds of thinking, which we might term reflection and intuition, has a long history in psychological and philosophical writing (Frankish and Evans 2009). Many authors have suggested that intuitive thinking is immediate, linked with emotion and other feelings and lacking in any form of conscious analysis. Reflective thought, on the other hand, appears to be a more prolonged process with conscious awareness of the reasoning that underlies our actions. In post-war psychology, a wide variety of dual process theories have appeared in the psychology of learning, reasoning, decision making and social cognition which appear to formalise this traditional distinction (for some leading examples and reviews see Epstein 1994; Evans 2007a, 2008, 2010a; Evans and Over 1996; Reber 1993; Kahneman 2011; Kahneman and Frederick 2002; Lieberman 2007; Sloman 1996; Smith and DeCoster 2000; Smith and Collins 2009; Stanovich 1999, 2011). As such theories have grown in popularity, however, a number of critiques have also been published (e.g., Keren and Schul 2009; Kruglanski and Gigerenzer 2011; Osman 2004). This makes it all the more important for theorists to define clearly the nature of the differences between the two kinds of processing and the kind of evidence that can support their claims. As reading of the papers in this special issue will confirm, dual-processing approaches can be widely diverse in their nature as well as their topic of application. Hence, I will focus this epilogue mostly on a question that is either explicit or implicit in these contributions: just what is the difference between these two kinds of processing? First, I must deal with some terminological issues. Several of the authors refer to System 1 and System 2, terms originally introduced by Stanovich (1999). Perhaps these terms are harmless if treated in a metaphorical manner, as a short-hand for parts of our brain-mind that deliver intuitive and reflective thinking, respectively (Kahneman 2011). I will assume that is the intention of the authors using the terms in this issue, and use them in this sense myself at times when discussing their papers. However, both Keith Stanovich and I have abandoned this terminology in recent writings (Evans 2010a; Stanovich 2011; Stanovich and Toplak this issue) for fear that readers may think we are referring literally to two specific cognitive or neurological systems. It has been clear for some time that System 1 refers to a diverse set of fast and autonomous processes (Evans 2006a; Stanovich 2004). We both prefer to call this category of processes by the older label Type 1 (Wason and Evans 1975). Less obviously, there are also dangers of thinking of System 2 as a singular system. The features typically attributed to System 2 thinking are that it is slow, conscious, reflective, capable of solving abstract, difficult and novel problems and correlated in its efficiency with IQ and working memory capacity. All of which might lead one to think it is a single system, perhaps to be equated with conceptions of working memory as an executive control system (Baddeley 2007; Engle 2002; Norman and Shallice 1986). Measures of working memory capacity are, after all, correlated with many forms of explicit information processing and problem solving of the kind attributed to System 2 (Barrett et al. 2004). They are also highly correlated with IQ and its surrogates (Colom et al. 2004) which themselves are often taken as diagnostic of System 2 reasoning (Stanovich 1999; Stanovich and West 2000). However, it seems to me that to say that System 2 or working memory is the system that does reasoning, planning, reading, learning, hypothetical thinking etc. is vacuous. We might as well attribute all these functions to the conscious mind. This is why I have defined Type 2 thinking in recent writing as requiring working memory among other resources and suggested that there could be multiple Type 2 systems drawing upon the same singular resource of working memory (Evans 2010a, b). Systems 1 and 2 have been attributed, by various authors, with lengthy lists of features (see Evans 2008) many of which can be simply be described as typical of Type 1 and 2 processing. However, these feature lists have created problems in that authors both friendly and hostile have tended to assume that they all of necessity go together. I recently showed that there is a “received” version of dual process theory, not attributable to any single author, which incorporates a number of fallacies (Evans 2011). For example, while historically, Type 1 intuitive processes have been associated with cognitive biases and Type 2 reflective thinking with normatively correct solutions, it is a serious fallacy to take the correctness of the response as being diagnostic of the type of processing. The reason for this is clearly explicated by Stanovich and Toplak (this issue) who very helpfully distinguish between what they call defining features as opposed to incidental correlates of the two kinds of processing. I entirely endorse this distinction although I would prefer to say typical or common correlates, as “incidental” appears to downplay their importance. Stanovich and Toplak suggest that the defining feature of Type 1 processes is their autonomy and that of Type 2 processes is that of cognitive decoupling. My own definition, that Type 2 processing necessarily engages working memory resources, is not that different since they also emphasise that cognitive decoupling loads working memory and gives an advantage in Type 2 processing to those of higher intelligence. Decoupling is also essential for supposition and all forms of hypothetical thinking which I have strongly linked with Type 2 processing (Evans 2007a; Evans and Over 1996). As Stanovich and Toplak point out, correlates of Type 1 and 2 processing are not necessary features and their absence under some circumstances does not falsify the theories as some critics have claimed (Keren and Schul 2009; Kruglanski and Gigerenzer 2011). This is also what I was getting at with my list of fallacies (Evans 2011). For example, normative correctness cannot be a defining feature of Type 2 processing because it is an externally imposed evaluation and not intrinsic to definitions based upon explicit processing through working memory. It has become increasingly clear in recent research that Type 1 processing can be effective and that Type 2 processing can often be the cause of cognitive biases (Evans 2007a; Reyna 2004; Stanovich 2011). But it is also not a coincidence that correct solutions to laboratory tasks in reasoning and decision making are more often found in those of higher cognitive ability, and less often under conditions which suppress Type 2 thinking, such as speeded tasks or concurrent working memory loads. This is because these tasks, with their inherent novelty, complexity and abstractness often require Type 2 thinking for their solution. Similarly, while Type 2 thinking is typically slower than Type 1, this not a defining characteristic either. Type 2 thinking can be quick when analysis is shallow or when tried and trusted heuristics are applied with little reflection (Evans 2010b).",20
11.0,2.0,Mind & Society,17 February 2012,https://link.springer.com/article/10.1007/s11299-012-0098-9,Gigerenzer’s ‘external validity argument’ against the heuristics and biases program: an assessment,December 2012,Andrea Polonioli,,,Female,Unknown,Unknown,Female,"In the last four decades, psychologists have reported several puzzling phenomena concerning the way people reason and make decisions. Specifically, they have suggested that in many contexts the process of decision-making seems to violate the normative rules of reasoning. However, the interpretation of these findings has been controversial, resulting in important debates among psychologists, economists and philosophers (Cohen 1981; Gigerenzer 1996; Kahneman and Tversky 1996; Stein 1996; Binmore 1999). Two main perspectives can be identified, namely the ‘pessimistic’ interpretation (Samuels et al. 2002, 236) advocated by Daniel Kahneman, Amos Tversky and other supporters of the influential heuristics and biases project (HB), which in the 1970s revolutionized research on human judgment, and the more ‘optimistic’ perspective provided by the Centre for Adaptive Behaviour and Cognition (ABC) led by Gerd Gigerenzer, which attempts to offer an alternative to the dominant paradigm in decision making, behavioural economics and cognitive science. Throughout this paper the acronyms HB and ABC will be used. Whereas the pessimistic view claims that people tend to systematically commit reasoning errors, the optimistic perspective offered by ABC tries to downplay the significance of the psychological findings supposedly revealing irrational behaviour by suggesting an ‘adaptive thinking approach’ to decision making that focuses on the goals behaviour can attain, rather than on the rationality of beliefs. In order to support this alternative view, ABC scholars have presented evolutionary arguments against the normative theories of decision and choice, suggesting that ‘the bias lies not in the behaviour but in the normative criteria used’, as ‘organisms did not evolve to follow a mathematically tractable set of principles—rather, natural selection favoured decision strategies that resulted in greater survival and reproduction’ (Stevens 2010, p. 110). As a result, they argue that ‘looking at the relation of heuristics to environments is often normatively more useful than evaluating reasoning and decision-making according to the standard norms of probability or decision-theory’ (Gigerenzer and Sturm in press). In addition, supporters of ABC have tried to test the robustness of the experimental findings provided by HB and to identify their proper range of validity. In particular, the ABC researchers take issue with the validity of the generalizations drawn from the HB studies by exploiting an ‘external validity argument’: they point out that the experimental designs used by Kahneman et al. are unrepresentative of the target toward which they want to generalize. This essay is organized in six parts. Section 2 will present the HB approach to decision making. Section 3 will then introduce the ABC approach. Section 4 will carefully consider Gigerenzer’s ‘external validity argument’ against HB and its structure, while Sects. 5, 6 and 7 will question the validity of this argument. The purpose of this paper is to argue that Gigerenzer’s external validity argument is untenable.",6
11.0,2.0,Mind & Society,09 February 2012,https://link.springer.com/article/10.1007/s11299-012-0099-8,The problem of future knowledge,December 2012,Nicholas Rescher,,,Male,Unknown,Unknown,Male,"Philosophers since Aristotle have stressed that knowledge about the future poses drastic problems.Footnote 1 And the issue of knowing the future of knowledge itself is particularly challenging. No-one can possibly predict the details of tomorrow’s discoveries today. To be sure, there is no inherent problem about predicting that a certain discovery will be made. But its nature is bound to be unfathomable. After all, if we could solve tomorrow’s problems today they simply would not be tomorrow’s problems. We may assume—or suppose—that man will continue to exist and to do so in a form that will enable him to pursue the prospect of inquiry into the nature of things. With this supposed, we do actually know some important facts about the body of knowledge that will be available to the knowers of the future. One of these relates to retrospective knowledge: knowledge of particular facts regarding the past. That Caesar crossed the Rubicon, that Napoleon lost at Waterloo, that Hitler led Germany to invade Poland in 1939, that there were 48 states in the continental US in the year 2000—these and their like are parts of our currently available body of knowledge that will continue in place in the future. And another of these preserved kinds of knowledge relates to various trans-temporal facts: the speed of sound, the specific gravity of lead, the molecular structure of water, the evolutionary history of man. Such facts will continue in place—at least in a rough and approximate formulation. However, that aspect of the future which is most evidently unknowable is the future of invention, of discovery, of innovation—and particularly in the case of science itself. As Immanuel Kant insisted long ago that every new discovery opens the way to others, every question that is answered gives rise to yet further questions to be investigated.Footnote 2 And the fruits of future science are not yet ripe for present picking. The landscape of natural science is ever-changing: innovation is the very name of the game. Not only do the theses and themes of science change but so do the very questions. Scientific inquiry is a creative process of theoretical and conceptual innovation; it is not a matter of pinpointing the most attractive alternative within the presently specifiable range, but one of enhancing and enlarging the range of envisageable alternatives. Such issues pose genuinely open-ended questions of original research: they do not call for the resolution of problems within a preexisting framework but for a rebuilding and enhancement of the framework itself. Most of the questions with which present-day science grapples could not even have been raised in the state-of-the-art that prevailed a generation ago. It is in principle infeasible for us to tell now not only how future science will answer present questions but even what questions will figure on the question agenda of the future, let alone what answers they will engender. In this regard, as in others, it lies in the inevitable realities of our condition that the details of our ignorance are—for us at least—hidden away in an impenetrable fog of obscurity. The contrast between present knowledge and future knowledge is clearly one that we cannot characterize in detail. It would be utterly unreasonable to expect prognostications of the particular content of scientific discoveries. It may be possible in some cases to speculate that science will solve a certain problem, but how it will do so lies beyond the ken of those who antedate the discovery itself. If we could predict discoveries in detail in advance, then we could make them in advance.Footnote 3 In matters of scientific importance, then, we must be prepared for surprises. Commenting shortly after the publication of Frederick Soddy’s speculations about atomic bombs in his 1950 book Science and Life, Robert A. Millikan, a Nobel laureate in physics, wrote that “the new evidence born of further scientific study is to the effect that it is highly improbable that there is any appreciable amount of available subatomic energy to tap.”Footnote 4 In science forecasting, the record of even the most qualified practitioners is poor. For people may well not even be able to conceive the explanatory mechanisms of which future science will make routine use. In inquiry as in other areas of human affairs, major upheavals can come about in a manner that is sudden, unanticipated, and often unwelcome. Major scientific breakthroughs often result from research projects that have very different ends in view. Louis Pasteur’s discovery of the protective efficacy of inoculation with weakened disease strains affords a striking example. While studying chicken cholera, Pasteur accidentally inoculated a group of chickens with a weak culture. The chickens became ill, but, instead of dying, recovered. Pasteur later reinoculated these chickens with fresh culture—one strong enough to kill an ordinary chicken. To Pasteur’s surprise, the chickens remained healthy. Pasteur then shifted his attention to this interesting phenomenon, and a productive new line of investigation opened up. In empirical inquiry, we generally cannot tell in advance what further questions will be engendered by our endeavors to answer those on hand. New scientific questions arise from answers we give to previous ones, and thus the issues of future science simply lie beyond our present horizons. It is a key fact of life that ongoing progress in scientific inquiry is a process of conceptual innovation that always places certain developments outside the cognitive horizons of earlier workers because the very concepts operative in their characterization become available only in the course of scientific discovery itself. (Short of learning our science from the ground up, Aristotle could have made nothing of modern genetics, nor Newton of quantum physics.) The major discoveries of later stages are ones which the workers of a substantially earlier period (however clever) not only have failed to make but which they could not even have understood, because the requisite concepts were simply not available to them. Thus, it is effectively impossible to predict not only the answers but even the questions that lie on the agenda of future science. Detailed prediction is outside the realm of reasonable aspiration in those domains where innovation is preeminently conceptual.",13
11.0,2.0,Mind & Society,03 June 2012,https://link.springer.com/article/10.1007/s11299-012-0108-y,Perceptions of collective narratives and identity strategies: the case of Palestinian Muslims and Christians in Israel,December 2012,Adi Mana,Shifra Sagy,Serene Mjally-Knani,Male,Female,Unknown,Mix,,
11.0,2.0,Mind & Society,11 May 2012,https://link.springer.com/article/10.1007/s11299-012-0106-0,Mirror neurons as a conceptual mechanism?,December 2012,Cristina Meini,Alfredo Paternoster,,Female,Male,Unknown,Mix,,
11.0,2.0,Mind & Society,10 July 2012,https://link.springer.com/article/10.1007/s11299-012-0109-x,How do common investors behave? Information search and portfolio choice among bank customers and university students,December 2012,Marco Monti,Riccardo Boero,Laura Martignon,Male,Male,Female,Mix,,
11.0,2.0,Mind & Society,05 September 2012,https://link.springer.com/article/10.1007/s11299-012-0110-4,The endogenous nature of the measurement of social preferences,December 2012,John Smith,,,Male,Unknown,Unknown,Male,"It is commonly assumed that subjects have stable preferences over outcomes. It is also commonly assumed that standard techniques to measure these preferences are reliable and can be performed in a nonintrusive manner.
Footnote 1 If these two assumptions hold then the order in which we perform measurement of the preferences should not affect the subsequent observations. However, we present evidence which challenges these assumptions. In particular, we find evidence that measures of social preferences can affect subsequent behavior. It is significant if a systematic violation of these assumptions is found. Measures of preferences are of interest primarily because they are helpful in making predictions regarding behavior. However, if the outcome of a measurement can affect future behavior, either because preferences are not stable or because the measure is not reliable, then the value of the measure is diminished. In order to investigate whether the order of the measurement can affect the outcome of the measures, we offer an extremely simple experimental setup: we offer subjects two standard measures but vary the order of their presentation. One might be tempted investigate these order issues with a measure of social preferences and play in a strategic game (for instance, the prisoner’s dilemma). However, if the experimenter observed that the relationship between the measure and behavior in the game is affected by the order in which the items are given, this difference is not exclusively attributable to the order of the measures. This is because behavior in a strategic game is not exclusively a function of preferences but also, for instance, expectations regarding the behavior of others. Therefore, rather than directing subjects to play a strategic game, we offer two similar, commonly-used measures of social preferences, and vary the order in which they are presented to the subjects. By doing this, we are confident that the effects which we find are not due to the more complicated features involved in the play of a strategic game. It has been known for some time that many subjects do not simply maximize their own material payoffs.
Footnote 2 Specifically, it is often observed that some subjects will sacrifice their own material payoffs so that other subjects will receive a better material outcome. Researchers often attempt to infer the nature of these social preferences by posing a series of allocation decisions, often referred to as dictator games.
Footnote 3 These decisions entail a choice of an allocation of hypothetical or material outcomes distributed between the subject and another subject. One measurement technique is to simply pose a dictator game to a subject. Another measurement technique, which involves a specific sequence of dictator games, is the social value orientation (SVO). In our experiment, we vary the order of the SVO and a standard, lager stakes dictator game. While we find that SVO outcomes are significantly related to outcomes in the dictator game, we also find that the mappings between these outcomes are related to the order in which they are given. Specifically, we find that the subjects, for whom the SVO indicates prosocial preferences, act even more prosocially in the larger stakes dictator game when the SVO is administered first. By contrast, we find that the subjects for whom SVO indicates selfish preferences are unaffected by the order. We also find that subjects with a perfectly consistent SVO measure are more generous in the dictator game when they are first given the SVO measure. To better understand these results we run an identical experiment, with the exception that the dictator game exhibits a relative price of each allocation of 1-to-3, rather than the standard 1-to-1. In other words, each \(\hbox{\$}0.50\) kept by the subject reduces the recipient’s payoffs by \(\hbox{\$}1.50\). In this case, we find no significant difference between the prosocials who complete the SVO before the dictator game and the prosocials who complete the SVO after the dictator game. However, across all subjects we find that those who first complete the SVO are more generous in the dictator game than subjects who complete the SVO last. Further, we find that this effect is stronger when we restrict attention to those with a perfectly consistent SVO measure. Given the results of our experiment, we are unable to distinguish between the explanation that the measurement affects the social preferences of the subject or that the measure affects the subsequent performance of another measure. Although we cannot distinguish between these two explanations, we can conclude that, given the assumptions commonly applied to experiments, we should not observe the behavior found in this experiment. The results of our experiment suggest that standard techniques of measuring social preferences can influence subsequent behavior. Further, as we have uncovered a systematic relationship between the treatment, the action of the subjects, and the measure, we therefore describe our results as endogenous rather than unstable. We use SVO because it is relatively easy to administer and interpret. The specific technique which we use, adapted from Van Lange et al. (1997), consists of 9 items with three possible choices involving material payoffs accruing to the subject and another subject.Footnote 4 Each of the nine items has an individualistic response, a prosocial response and a competitive response. The individualistic response is the one in which the material payoffs accruing to oneself are the largest. In other words, selecting the individualistic choice suggests that the subject neither positively nor negatively values the material payoffs accruing to the other subject. The prosocial response is the one in which the sum of the material payoffs accruing to both the subject and the other subject is the largest. In other words, selecting the prosocial response suggests that the subject positively values the material payoffs accruing to the other subject. The competitive response is the one in which the difference between the material payoffs accruing to the subject and the other subject is the largest. In other words, selecting the competitive choice suggests that the subject negatively values the material payoffs accruing to the other subject. Further, there is much written on the stability of SVO. For instance, Bogaert et al. (2008) suggest that over the 40 years since its introduction by Messick and McClintock (1968), SVO has been widely regarded as providing a stable measure of a personality trait. However, recent work has suggested instances where SVO can be affected by the setting and is thereby a less than perfectly stable measure. Iedema and Poppe (1994) show that the measurement of SVO can be affected by self-presentation effects. Smeesters et al. (2003) show that priming certain types of behavior can lead to a different mapping from SVO to behavior.Footnote 5 While SVO is considered relatively stable, to our knowledge there is no work suggesting that outcomes of SVO can affect subsequent behavior. It is obviously problematic that the order of the measurement of preferences might affect the relationship between the measure and behavior related to the measure. A measure is primarily useful to the extent that it can form a basis for making predictions about behavior.Footnote 6 When behavior and the measure of preferences are functions not exclusively of preferences then the usefulness of the measure is somewhat degraded. Social value orientation also appears in the economics literature.Footnote 7 However each of these papers use the ring measure (Griesinger and Livingston 1973), which is slightly different than the technique which we employ. The ring measure consists of 24 pair-wise items, rather than 9 items with 3 responses.Footnote 8 However, similar to the technique which we employ, the ultimate objective is to classify subjects on the basis of their social preferences. Relatively little is known about the relationship between the ring measure and the measure which we employ (Bogaert et al. 2008). However, we opt for the latter as it requires fewer responses and, in our opinion, is more transparent. As a result, we conjecture that the effects which we find would only be strengthened by the use of the ring measure. Finally, measuring social preferences via dictator games, like SVO, has the advantage that it only considers a situation where strategic issues are absent. Although all decisions would be made in the absence of the feedback of the actions of other dictators, it still remains possible that the subject would anticipate some implicit reciprocal arrangement. Therefore, similar to Carpenter (2005), we employ a triadic design whereby each dictator decides an allocation involving self and another dictator. This other dictator does not decide on an allocation involving the original dictator but rather on a third dictator. Another commonly used social preference measurement technique was developed by Andreoni and Miller (2002). SVO is similar to this technique in that both pose a series of dictator games however there remain important differences. In Andreoni and Miller, choice is much less restricted than in SVO. Each SVO item has only three possible responses, whereas in Andreoni and Miller each item seeks an allocation of tokens ranging from 40 to 100. As a result, Andreoni and Miller yields less coarse data than does SVO. However, the choice in Andreoni and Miller is less transparent than SVO, as the latter explicitly lists the material allocation of each choice. We are not aware of a study which compares the relative merits of SVO and that proposed by Andreoni and Miller. Charness and Rabin (2002) pose a series of simple games to learn the specific form of social preferences related to relative wealth and reciprocity.Footnote 9 As the nature of the social preferences might depend on whether other’s payoffs are higher than or lower than the subject’s own payoffs, Charness and Rabin vary this aspect of their items. By contrast, in SVO the subject decides among choices where monetary payoffs accruing to oneself are never less than that accruing to the other subject. Also, in contrast to the technique employed by Charness and Rabin, SVO is not equipped to evaluate preferences for reciprocity. Consider the relationship between our paper and research on endogenous social preferences.Footnote 10 For instance, Carpenter (2005) and Canegallo et al. (2008) investigate how the strategic environment can affect preferences.Footnote 11 Also, Güth et al. (2008) find that subjects who contribute more in a public goods game are significantly more trusting in a subsequent investment game. By contrast, we study whether the decision in a commonly used measure of social preferences can affect subsequent behavior.Footnote 12
 Borgloh et al. (2010) is perhaps closest to our paper. The authors describe an experiment where subjects are given an unfamiliar measure of altruism and a familiar measure, where the authors vary the order of the measures. The authors find evidence that the order affects the behavior in the unfamiliar task but not in the familiar task.Footnote 13 Likewise, we vary the order of tasks and examine the differences in behavior. In our view, our results are best described as endogenous. First, we find evidence that social preferences are not merely unstable, but that their measure can systematically affect subsequent behavior. Second, we note that in the research mentioned above, the environment affects either preferences or the measure of preferences. We also note that within this literature, a unique word has not emerged as a description of this behavior. However, the word endogenous does appear prominently in many descriptions,Footnote 14 and we therefore argue that it is the most appropriate description of our observations. Finally, we note that we are careful to qualify our use of endogenous with the word measurement. This is because can cannot distinguish between a change in social preferences and a change in the efficacy of the measures of social preferences. The present paper shares some features with the framing effects literature. For instance, it has been found that the there can be systematic differences in the responses to questions based on how the questions are framed (Tversky and Kahneman 1981).Footnote 15 Like the framing literature, the present paper appears to provide evidence against the assumptions that preferences are stable and can be measured in a reliable, nonintrusive manner. However, unlike the framing literature, the effects which we find persist after the initial ""frame"" and this persistence seems to be based, at least in part, upon the actions of the subject. Specifically, in Study 1 we find that prosocial subjects are affected by the order and across both studies we find that the consistent subjects are affected by the order. Our paper also relates to the order effects literature, which finds evidence that the order of the questions can affect the answers. Research has found that the question order can affect the answers to self-reported health questions (Bowling and Windsor 2008), identification with a racial or ethnic group (Mallett et al. 2011), self-reported interest in politics and religion (McFarland 1981), satisfaction with public services (Van de Walle and Van Ryzin 2011), and even preferences for soft drinks (Welch and Swift 1992).Footnote 16
 Although these papers suggest that the order is likely to affect the responses in our setting, they are not helpful in suggesting hypotheses regarding the direction of the effect. Specifically, the literature discusses reasons for the order effects which they identify (fatigue, improved familiarity, priming, saliency, etc.) however none would seem to uniquely suggest a direction of the effect. First, the experiment is relatively brief and simple, and we would therefore not expect fatigue or familiarity to affect the results. Second, we are essentially asking the same question twice: measurement of social preferences via SVO and via the dictator game. As a result, based on the reasons provided in the literature, it is not clear to us in which direction we should expect the change. Does the SVO prime subjects to be selfish or generous? Do selfish actions in the SVO prime subjects to be less selfish or more selfish in the dictator game? Does the consistency of the responses in the SVO affect the dictator game choice? Our experiments suggest that the answer to these questions can depend on the form of the dictator game and the nature of the actions selected by the subject.",3
11.0,2.0,Mind & Society,22 June 2012,https://link.springer.com/article/10.1007/s11299-012-0107-z,Emanuele Bardone: Seeking chances: from biased rationality to distributed cognition,December 2012,Merja Bauters,,,Female,Unknown,Unknown,Female,,
12.0,1.0,Mind & Society,08 May 2013,https://link.springer.com/article/10.1007/s11299-013-0127-3,Special issue on “Cultural and cognitive dimensions of innovation” edited by Petra Ahrweiler and Riccardo Viale,June 2013,Riccardo Viale,,,Male,Unknown,Unknown,Male,,
12.0,1.0,Mind & Society,08 May 2013,https://link.springer.com/article/10.1007/s11299-013-0128-2,Special issue on “Cultural and cognitive dimensions of innovation” edited by Petra Ahrweiler and Riccardo Viale,June 2013,Petra Ahrweiler,,,Female,Unknown,Unknown,Female,"Early German Sociology of Knowledge started the twenties of the twentieth century with a provoking statement: reality formation is relative to the mental contexts of human beings (Mannheim 1969: 54f). The mental contexts are determined by the different and ever-changing social contexts, which work as media separating people from any direct contact to the world. The social contexts mediate all experience and generate, produce, and prepare the world we know. This is why we need to consider knowledge always as dependant on the social context of the person who knows. Knowledge is inherently social. There is no exempt position outside social contexts for any human observer, which would guarantee a universal claim to knowledge. Many positions in Philosophy of Science rejected the idea that social or historical factors have any relevance for achieving knowledge (Blalock 1969 summarising reasons). In realistic approaches, knowledge is justified true belief. Truth is a relation between elements of language statements, which is secured by a connection between language symbols and objects represented by those symbols. Objects in reality have their counterparts in language symbols, “thus, a system of symbols, linked to the world in this way, can be said to be a representation of reality—a mirror of nature” (Lakoff 1987: 162). Any influence of empirical factors on the relation between the representing symbols and the represented outer world has to be rejected from this position: “according to the objectivist paradigm, true knowledge of the external world can only be achieved if the system of symbols we use in thinking can accurately represent the external world. The objectivist conception of mind must therefore rule out anything that can get in the way of that: perception, which can fool us; the body, which has its frailties; society, which has its pressures and special interests” (Lakoff 1987: 183). Neither does it count to co-observe processes of knowledge generation and the processing system, nor is it interesting who or what is carrying out these processes. Knowledge is not produced, but discovered or achieved. “Knowledge is knowledge, regardless of how it is organized, processed, or remembered” (Lakoff 1987: 166). At best, it is confirmed that social factors influence scientific knowledge in cases of errors, mistakes etc. (cf. Lakatos 1970; Laudan 1990). The wrong and the false can be accounted to social factors—they are responsible for any “noise”, or the muddying of representation processes; true and justified knowledge is generated by (correct) representations of objects. The “Impartiality or Causality Tenet” of Sociology of Knowledge (Bloor 1976), where not only errors and mistakes, but everything must be accounted to social factors, is not an option for these positions. Interdisciplinary Science and Technology Studies (STS) inherited the difficulties of pointing out the importance of social dimensions for knowledge production, and is still divided into two camps. The first one just continues work in the logical vacuum “at pains to emphasize that they embrace relativism wholeheartedly” (Nicholas 1984: 265; also Knorr-Cetina 1989: 93). The second one committed in the eyes of their colleagues “a betrayal of their disciplinary standpoint” (Bloor 1976: 1), because they decided to revise the statements of Sociology of Knowledge. Looking especially at knowledge production in science, they concentrate on organisational issues of science as a social institution. The task has slightly changed: abstracting from cognitive claims and contents of knowledge produced, analyses focus on how for example social conditions such as group memberships and education paths influence the organisation of academic disciplines etc. Many of these approaches deal empirically with the question of influencing factors for scientific knowledge. For example, constructivist laboratory studies (Knorr-Cetina, Latour and Woolgar) show how the data stream to be investigated by the lab-using sciences is generated with the help of the lab and how it is—again with the help of the lab—evaluated and interpreted. Knowledge production and scientific theory formation are communicated as lab-internal constitution processes of meaning concerning self-produced data. In these approaches, knowledge creates the object, not vice versa. Following this perspective, it is futile to ask for influencing factors of knowledge: “realizing these goals renders the social/cognitive dichotomy obsolete. Distinctions between the cognitive and the social […] are constantly blurred and redrawn in the laboratory” (Knorr Cetina 
1981: 23). Furthermore, also the normative dimension, the question concerning the claims and validity of the knowledge produced, is finally dismissed: it has to be transferred to micro-sociological analyses concerning the consensus practice of small social communities, here the laboratory community of practice (cf. Pickering 1992). The social organisational form of the conceptual compatibility or incompatibility of knowledge claims is originally that of rational, interdisciplinary discourse. The contemporary forms of the production of knowledge and technology even find their place in a more heterogeneous cultural environment: “the communicative intercourse of multiple agents constitutes the parameter toward which the production of knowledge must orient itself” (Weingart 1997: 20). Michael Gibbons, Camille Limoges, Helga Nowotny, Simon Schwartzman, Peter Scott and Martin Trow (Gibbons et al. 1994) have diagnosed a new, sophisticated mode of knowledge production for modern societies that appears to demonstrate the following characteristics: The so-called “Mode 2” of knowledge production transcends disciplinary boundaries; knowledge is developed close to users; and the organisation of this production is non-hierarchically and weakly institutionalised. Participants from different areas will participate from the beginning and will define novel criteria for the quality control of the produced knowledge. The need for reflection by all participants will persist throughout the process of knowledge production; knowledge production will be socially evaluated and publically reviewable. Other authors speak in regard to the same phenomenon of “post-academic science” (Ziman 1996) or “post-normal science” (Functowicz and Ravetz 1993, Elzinga 1997). Gibbons et al. take this change in the production of knowledge to be fundamental. The provocative novelty of Mode 2 affects more than just the sciences: “the emergence of Mode 2, we believe, is profound and calls into question the adequacy of familiar knowledge-producing institutions, whether universities, government research establishments, or corporate laboratories” (Gibbons et al. 1994: 1). The collaborative nature of today’s knowledge production processes is mirrored by the network arrangements we observe for innovation, the creation of new, technologically feasible, commercially realizable products, processes and organizational structures (Schumpeter 1912; Fagerberg et al. 2006). Here, heterogeneous organizations generate and exchange knowledge, financial capital, and other resources in networks of relationships that are embedded in institutional frameworks on the local, regional, national and international level (Ahrweiler 2010). The new forms of knowledge and technology production trade since long under the nickname of “Innovation Networks”. They were seen as “quasi-social technology”, answering to the opacity of the market and to the uncertainties in relation to the material and financial feasibility of a new product (Kowol and Krohn 1995: 90). As Kowol and Krohn describe it, the central evolutionary “mechanism is procedurally self-organizing and structurally network-forming: the common achievement of formation and processes of learning will be organized through networkesque relationships between the technology generating, using, and regulating social systems. In comparison to formally bounded organization these innovation networks possess a higher degree of openness, permeability and space for ambiguity and authorize systemic as well as inter-organisational processes of agreement—something between science, economics and politics. In the language of self-organization theory: Networks make possible recursive closure through informational openness” (Kowol and Krohn 1995: 78). Sociological examination of the functions and institutions of comprehensive negotiation networks shows that the once distinct dimensions of knowledge production, dissemination and use are being fused via quasi socio-technological means. In this process the possibility of and readiness for participants‘continual “side-changing adoption of perspectives” plays a decisive role. The achievement of integration, calculated specifically for each problem and innovation, seems to require that “each sphere takes on some of the role of the other” (Etzkowitz et al. 1998). According to Alfred Schütz, each actor reconstructs and adopts the situation specific perspective of another relevant actor (in tune with classifications of the situation and the actors), with which a sort of “exchange” is carried out (cf. Schütz 1974: 137ff). The side-alternating adoption of perspectives and roles by the actors of modern knowledge production manifests itself in their changed self-understanding as underlined by new terms such as “entrepreneurial university” or “open-source products”. The objective of this Special Issue is to analyse the cognitive and social dimensions, which shape the economic, technological and political innovation agenda today. It contains theoretical and empirical contributions. The theoretical part starts with a paper authored by Marian Adolf, Jason L. Mast, and Nico Stehr, which analyses innovation as the outcome of a process of cognitive displacement for which a bundle of social and cognitive competencies is required. Jason L. Mast continues this theoretical trait by investigating the spaces for invention and innovation in Cultural Theory. It is in line with this approach of cognitive displacement, when Nancy Nersessian and Miles MacLeod later state that what is driving innovation is a determination to construct new cognitive niches. Lorenzo Magnani even reconstructs scientific innovation as “eco-epistemic warfare” taking advantage of recent results coming from the area of distributed and abductive cognition. Andrea Lavazza and Riccardo Manzotti further emphasise that creativity is more than just a symbolic reshuffling or a moment of semantic extension, but that it has an internal and an external aspect. Petra Ahrweiler and Mark Keane, finally, suggest a framework for modelling the component interactions between cognitive and social aspects of scientific creativity and technological innovation. Most of the empirical articles in this Special Issue aim at focusing on the cultural sources of economic value creation coming from science, arts and the creative industries with a special focus on new emerging digital technologies. For example, Anne Beaulieu, Matt Ratto, and Andrea Scharnhorst analyse the processes and reflections about interdisciplinary work around simulation-building, and propose a tool to facilitate discussions. Scott Dexter and Aaron Kozbelt discuss how Free and Open-Source Software can act as a model domain for answering “Big Questions” about creativity and innovation. Emanuele Bardone and Ilya Shmorgun show that Multitouch Smartphones cannot be separated from their cognitive ecology, but that it is precisely the way in which they become permeable to the context that allows us to potentially come up with new uses and in so doing improve our ability to solve problems. A continuous increase in products or services with a particularly symbolic or aesthetic value can be observed in economic value creation accompanied by a strong growth in employment in comparison to other economic sectors. Creative industries are defined as combining the classical “cultural industries” such as art, architecture, design, music, film and literature with “new” creative industries such as advertisements and software/games/multimedia. Papers addressing these areas are the one authored by Alkim Almila Akdag Salah and Albert Ali Salah on creating and sharing arts in an online social network site, the one by Nona Schulte-Römer on arts and culture festivals as sites for technical innovation, and by Christian Barrère for the case of Taste Industries.",1
12.0,1.0,Mind & Society,21 February 2013,https://link.springer.com/article/10.1007/s11299-013-0112-x,The foundations of innovation in modern societies: the displacement of concepts and knowledgeability,June 2013,Marian Adolf,Jason L. Mast,Nico Stehr,Male,Male,Male,Male,"Indeed innovation has become one of those ‘big words,’ as Clifford Geertz may have put it, and joined the likes of grand concepts such as democracy and knowledge. It is a word that is frequently invoked but difficult to define. It carries strong normative connotations, and routinely meets with approval and partiality when it is used. To put it more formally, the term innovation typically performs the speech-act of commending what it tries to describe (cf. Sartori 1968; Broman 2002: 5). While it is difficult to separate normative from analytical elements in the case of the concept of innovation, the most common conception refers to the successful implementation of a novel idea. Separating the genesis of a new idea (invention) from its practical realization (innovation), however, can be a complicated endeavor, we suggest (cf. Beckenbach and Daskalakis 2010). It is poignant that the idea of innovation plays such a central role in much of our contemporary political discussion about the economy, the wealth of nations, and the competitive advantages of societies while we appear to be unable to arrest and fence in the notion of innovation itself. The notion of innovation in the sense of novelty is also contained in such concepts as social change, development, evolution, mutation, creation, growth, imitation, invention, modernization, revolution, progress, discovery, and so on. In other words, there cannot possibly be a general theory of innovation since this would amount to a theory of life itself (Moldaschl 2010: 9). The concept of innovation refers to processes, namely change or novelty that is as least as universal as its opposite, namely routine or habitual conduct. In fact, most social contexts and the vast majority of social action are characterized by nothing more than routine attributes and habitual conduct. Using a term the French sociologist and anthropologist Gabriel Tarde introduced, most human action is based on imitation. As an aside, habitual action has of course the constructive function of stabilizing human conduct, enhancing the predictability of social action and opening up avenues free from the pressures and constraints to act. Whether or not habitual conduct or imitation is always a carbon copy of previous social conduct (in that limited sense all conduct is a modification of previous conduct) is not at issue, what is at issue is the overwhelming constraint in social life to repeat and therefore get on with life.Footnote 4 Nonetheless, within a historical perspective ranging across the centuries, the volume and the speed with which modifications of social conduct occur has of course accelerated with the dawn of the industrial society.Footnote 5
 If one desires to talk sensibly about innovation, one must proceed with a relational understanding.Footnote 6 For example, if one wants to account for technical innovations, one does not need a theory of technology since technology only evolves in the context of society and not by itself. However, what would be required is a socio-economic theory of technical innovation. Such a theory would refer to a combination of factors such as the creativity of social action, economic incentives, and institutional conditions (or, on a smaller scale, social or action networks) that enhance technical innovativeness (cf. Moldaschl 2010: 14). In the case of Joseph Schumpeter’s theory of social change within firms, the yeast that propels change within this set of complex factors is the creative entrepreneur. Our relational concept of innovation concentrates on those features of the subject or the collectivity that enable innovation. As Woolgar (1998: 442) has emphasized, whether or not “ideas counts as new, necessarily depends on the social networks involved.” Subjects of course are embedded within a specific social context that either validates a novel idea as new or resists such a declaration about its own social network. A novel idea is not self-validating but has to be recognized as such by other social actors. As an increasing number of innovation studies have shown,Footnote 7 the realization of knowledge, or its translation into technical artifacts, is an extremely complex intellectual and organizational process that relies on sources of knowledge and on “action networks” both “internal” and “external” (for example, on “public science,” see Gibbons and Johnston 1974) to firms or organizations.Footnote 8 Thus, innovation is comprised of both a process of displacing concepts or creating novel ideas, to which we now turn, as well as social contexts of reception. Having commented on the difficulty of defining innovation, we nonetheless offer a broad definition as an orienting tool: conceptual innovation refers to the epistemological realm between a paradigm shift on the one hand, and explaining new experiences and phenomena with pre-existing theories or understandings, on the other. Schön ([1963] 1967) work on conceptual displacement, we suggest, offers a bridge between the novel and the routine by arguing that it is through metaphorical extension, the application of existing metaphors to new conditions and problems, that allow for the birth of innovative ideas. Building on Schön ([1963] 1967: 53) investigations, conceptual innovations can be described as a process of the displacement of concepts, that is, as a “shift of old concepts to new situations,” puzzling experiences, or phenomena. The old concept becomes “a symbol or metaphor for the new situation.” The new concept then evolves as a result of the work that goes into “the making, elaboration, and correction of the metaphor.” The metaphor changes, and the new experience is shaped into something more familiar yet still distinct, while its peculiar characteristics are made intelligible and somehow more mundane. Cognitive displacement refers to the entire working or spelling out process of a new metaphor. As Schön ([1963] 1967: 57) points out, the displacement of concepts always occurs in specific contexts from which, as he put it, the source of energy comes. The displacement of concept may be speculative or playful, for example, “as when a child is amused at the idea of a boiling tea-kettle as a baby crying, or a biologist is intrigued with the notion that heredity is the transmission of coded information.”",4
12.0,1.0,Mind & Society,22 February 2013,https://link.springer.com/article/10.1007/s11299-013-0120-x,Cultural theory and its spaces for invention and innovation,June 2013,Jason L. Mast,,,Male,Unknown,Unknown,Male,"In this article I examine the spaces for invention and innovation offered in structuralist, immaterial theories of culture. It is an unusual place to start investigating processes of inspired material change, as cultural structuralism appears to have written pronouncements into its core about the fabric of culture that render creativity and innovation if not theoretically impossible then exceedingly rare, and in these latter cases, as resulting from accident or from outside disruption. But inconvenience is not a sufficient reason for avoiding the most compelling theories of meaning and interpretation to enliven the social sciences in recent times. If we accept that humans are meaning oriented actors, then theories of meaning, regardless of how many challenges they pose to the very existence of our empirical phenomenon, must be confronted. This article represents a step toward addressing these challenges. First I will outline Ferdinand de Saussure’s theory of the sign and John Austin’s theory of the performative and offer a brief definitions of invention and innovation in semiotic and performative terms. Second I will offer a broad overview of structural theories of culture that have extended and revised Saussure’s theory and identify how this literature appears to challenge at a fundamental level the potential for invention and innovation to occur. Third, in order to give an example of how structural theory seems to foreclose invention and innovation I reconstruct Marshall Sahlins’s analysis of modern industrial food production. Sahlins’s work demonstrates the structural impediments to innovation. However, a more contemporary example I give illustrates that efforts to invent and innovate in the food production and consumption arenas are occurring while they nonetheless face profound signifying challenges in the current era. Fourth I will show how Judith Butler’s work on the performativity of gender enlivens the definitions of invention and innovation offered here. Next I turn to Jeffrey Alexander’s cultural analysis of how the computer was folded into moral discourses as it was introduced to the American public and moved from representing a distanced sacred object to the ubiquitous totem of personal expression that we experience it as in the early twenty-first century. After discussing these strong structural formulations, I will briefly discuss two more pragmatic approaches to meaning and invention and innovation, those offered by Ann Swidler’s toolkit theory of culture in action and innovation scholar Donald Schön’s theory of the displacement of metaphors.",3
12.0,1.0,Mind & Society,21 February 2013,https://link.springer.com/article/10.1007/s11299-013-0119-3,The creative industry of integrative systems biology,June 2013,Miles MacLeod,Nancy J. Nersessian,,Male,Female,Unknown,Mix,,
12.0,1.0,Mind & Society,22 February 2013,https://link.springer.com/article/10.1007/s11299-013-0118-4,Scientific innovation as eco-epistemic warfare: the creative role of on-line manipulative abduction,June 2013,Lorenzo Magnani,,,Male,Unknown,Unknown,Male,"As defined by Oshawa and McBurney (2003), a chance is a new event or situation conveying both an opportunity and a risk in the future. Recently, a number of contributions have acknowledged the abductive dimension of seeking chances with relation to science (Magnani 2005, 2010; Magnani and Bardone 2008; Abe 2009). As maintained by Magnani and Bardone (2008) and Abe (2009), the process of chance detection (and creation) is resulting from an inferential process—mainly abductive—in which the agent exploits latent clues and signs signaling or informing the presence of an action opportunity (Magnani and Bardone 2008). Accordingly, an inference is a form of sign activity in which the word sign encompasses several types of sign, for instance, symbol, feeling, image, conception, and other representation (Peirce 1931–1958, 5.283). Moreover, the process of inferring—and so the activity of chance seeking and extracting—is carried out in a distributed and hybrid way (Magnani 2010). This approach considers cognitive systems in terms of their environmental situatedness: instead of being used to build a comprehensive inner model of its surroundings, the agent’s perceptual capacities are seen as simply used to obtain “what-ever” specific pieces of information are necessary for its behavior in the world: not only the agent represents the external world but also modify it delegating representations to the environment to promote possible manipulations of them. The agent constantly “adjusts” its vantage point, updating and refining its procedures, in order to uncover a piece of information. This resorts to the need of specifying how to efficiently examine and explore and to the need of “interpreting” an object of a certain type. It is a process of attentive and controlled perceptual exploration through which the agent is able to collect the necessary information: a purposefully moving through what is being examined, actively picking up information rather than passively transducing (Thomas 1999). In this sense, humans like other creatures are ecological engineers, because they do not simply live their environment, but they actively shape and change it looking for suitable chances, epistemic for example, like in the case of scientific abductive cognition. Generally speaking, the activity of chance-seeking as a plastic behavior is administered at the eco-cognitive level through the construction and maintenance of the so-called cognitive niches (Magnani 2009). The various cognitive niches humans live in are responsible for delivering those clues and signs informing about an (environmental) chance. So, the mediating activity of inferring as sign activity takes place (and is enhanced) for the presence of the so-called eco-cognitive inheritance system (Odling-Smee et al. 2003). That is, humans can benefit from the various eco-cognitive innovations as forms of environmental modifications brought about and preserved by the previous generations. Indeed, many chance-seeking capacities are not wired by evolution, but enter one’s behavioral repertoire because they are secured not at the genetic level, but at the eco-cognitive one—in the cognitive niches. The second important point to mention is that humans as chance extractors act like eco-cognitive engineers (Magnani and Bardone 2008; Bardone 2011). Accordingly, they take part in the process of extracting chances by performing smart manipulation of the environment in order to turn an external constraint into a part of their extended cognitive system. Humans as eco-cognitive engineers are also chances-maintainers. Humans act so as to preserve those chances (and “chances of chances”) that have been proved as successful and worth pursuing. That is what allows us to have an eco-cognitive inheritance that, in turn, enriches or even creates the behavioral chances a community or a group of people has for surviving and prospering. In summary, chances are provided by the continuous eco-cognitive activity of humans as chance extractors. But what exactly happens when agents are involved in cognitive processes related to scientific innovation? In the following sections I will delineate some basic aspects of conceptual innovation in science, taking advantage of an eco-epistemic perspective, suitably intertwined with recent results coming from the area of distributed and abductive cognition.",2
12.0,1.0,Mind & Society,07 March 2013,https://link.springer.com/article/10.1007/s11299-013-0124-6,An externalist approach to creativity: discovery versus recombination,June 2013,Andrea Lavazza,Riccardo Manzotti,,Female,Male,Unknown,Mix,,
12.0,1.0,Mind & Society,01 March 2013,https://link.springer.com/article/10.1007/s11299-013-0123-7,Innovation networks,June 2013,Petra Ahrweiler,Mark T. Keane,,Female,Male,Unknown,Mix,,
12.0,1.0,Mind & Society,21 February 2013,https://link.springer.com/article/10.1007/s11299-013-0117-5,Learning in a landscape: simulation-building as reflexive intervention,June 2013,Anne Beaulieu,Matt Ratto,Andrea Scharnhorst,Female,Male,Female,Mix,,
12.0,1.0,Mind & Society,08 March 2013,https://link.springer.com/article/10.1007/s11299-013-0125-5,Free and open source software (FOSS) as a model domain for answering big questions about creativity,June 2013,Scott Dexter,Aaron Kozbelt,,Male,Male,Unknown,Male,"The increasing prevalence of free and open source software has been a major development in software engineering over the last 15 years (DiBona et al. 1999, 2005; Weber 2004; Feller et al. 2005; Chopra and Dexter 2007). Users worldwide have eagerly adopted software developed using FOSS, such as the GNU/Linux operating system, which undergirds many sensitive computing-intensive corporate and scientific computing installations; the Apache server, one of the most widely deployed Web servers; and Mozilla Firefox, a popular web browser. Moreover, FOSS is an increasingly common component of commercial software development (Capra et al. 2011), a development that impacts the cultures of both corporate participants and the FOSS community (Lin 2006). In this paper, we argue that FOSS is a novel and useful domain for the empirical study of creativity. Not only has FOSS been closely aligned with recent changes in cultural and technical production (Benkler 2006; von Hippel 2006), but its inherent openness provides a remarkable trove of data which may yield new evidence bearing on the nature of creativity. Below, we briefly summarize the important characteristics of the free and open source software domain; in subsequent sections we point to some gaps in the current state of our understanding of creativity, then discuss the potential of the study of FOSS to close these gaps. Unlike proprietary software, which tends to be generated in corporate environments and whose code is a closely guarded trade secret, FOSS projects are designed to invite voluntary contributions: programmers can modify an existing public codebase and share the modified code with others, without restriction (Free Software Foundation 2012). However, FOSS is more than a unique software development ethos: its characteristics and mores continue to shape many broader aspects of society and culture. Scientific practice is increasingly dependent on software, and the transparency of this software is understood more and more as a crucial element of modern scientific peer review (Barnes 2012). FOSS is increasingly prominent in discussions about openness and transparency in e-government, as in the recently adopted European Interoperability Framework (European Commission 2010) for European public services. And of course FOSS is closely tied to various aspects of the “free culture” movement, including Creative Commons licensing (Lessig 2004; Doctorow 2008). FOSS further possesses a number of characteristics that make it uniquely suitable as a domain for the study of creativity, as we shall see below. Because of its self-organizing dynamic, FOSS permits and demonstrates a variety of modes of collaboration in the service of collectively developing the code. Perhaps more importantly, because software development (and FOSS in particular) relies heavily on the existence of a full archive of both the code and developers’ discussions of it, it is possible to characterize, very precisely, the state of the code at any point in its history. That is, FOSS offers a very rich ‘paper trail’ of the evolution of the code itself, including ‘meta’ discussion about changes on mailing lists, on code submissions, and/or in comments embedded in the code. Thus, FOSS projects, individually and collectively, present a rich source of longitudinal data against which hypotheses about the collaborative and creative processes of software development may be tested.",4
12.0,1.0,Mind & Society,12 February 2013,https://link.springer.com/article/10.1007/s11299-013-0121-9,Ecologies of creativity: smartphones as a case in point,June 2013,Emanuele Bardone,Ilya Shmorgun,,Male,Male,Unknown,Male,"Creativity is an important aspect of what makes us human. Even though we have developed highly intelligent computers, they still cannot reach our ability of engagement in everyday creative skills (Sawyer 2012). Applying creativity to using a stock of knowledge to facilitate novel problem solving may lead to innovation. Innovation can take many forms, for example product innovation, however design and incremental process innovation are more common (Yusuf 2009). Technology can in turn be seen as a means of supporting the realisation of one’s creative potential and expressing creativity by providing convenient access to a wide range of information and computational tools (Bonnardel and Zenasni 2010). The smartphone can be considered a great example of technology providing us with information at our fingertips anytime, anywhere (Satyanarayanan 2010). Yet the field of mobile computing has recently experienced a paradigm shift, its significance in enhancing human creativity still has not been entirely clarified. A paradigm shift occurs when the old way of explaining reality no longer works and we are forced to look for new means to make sense of things (Kuhn 1962; Rosado 1997). This has now occurred in the field of mobile computing with the advent of the smartphone. Our old way of looking at the smartphone as merely a communications device is no longer valid. In this paper we will try to draw a conceptual map, which could help us orient during this moment of friction between the old paradigm and the emergent one. The current state of things may be considered a critical point. On the one hand, it is apparent that mobile devices have lots of unexplored potential and we are only beginning to scratch the surface of the things these devices will enable us to do. On the other hand, we have been operating mostly in the dark without a clear understanding of what our mobile devices have to offer and how people use them in the real world to support creativity. In this paper, by creativity we refer both to the process and the result of creative re-use (or re-purposive appropriation) as part of a problem-solving activity. The main objective of this paper is to reach a preliminary understanding of how to support the creative re-use of smartphones as an integral part of everyday problem-solving. To this end we claim that a crucial and yet unexplored pathway is to shed light on the role played by the context of one’s activity. As we will argue in this paper, the context of one’s activity is not just a neutral setting where creativity may emerge. There is an ongoing paradigm shift, which concerns all the participants—companies, designers, and users. However, at the present stage of our research, we are not yet able to provide guidelines for designers or describe the breadth that such a paradigm shift may have on creativity in other domains, such as the arts and science. This paper is structured as follows. In the first section we briefly illustrate the main elements of ubiquitous computing as the technological milieu for the emergence of smartphones. The most interesting aspect of this new trend is related to the fact that we can no longer regard a piece of technology in isolation, but as something becoming part of one’s context. In the second section we demonstrate how users do not just use technological objects, but how they are also more active than ever in trying to creatively exploit latent possibilities—a process we refer to as creative re-use or re-purposive appropriation. In the third section we introduce the notion of affordance, which is serving the purpose of articulating our notion of a cognitive ecology along with the idea that smartphones are inherently polysemous. We then introduce mobile computing as context-permeable computing. This allows us, in the final part of the paper, to point out several intriguing challenges for design that lie ahead.",2
12.0,1.0,Mind & Society,22 February 2013,https://link.springer.com/article/10.1007/s11299-013-0113-9,Flow of innovation in deviantArt: following artists on an online social network site,June 2013,Alkim Almila Akdag Salah,Albert Ali Salah,,Unknown,Male,Unknown,Male,"Rogers’s (1995) theory of diffusion of innovations started an avalanche of studies about how recognizable patterns in social systems travel. Especially, and most strikingly, the processes of diffusion for novel ideas, for new products, and the results of medical research have been scrutinized in early 1960s–1970s with different techniques (Rogers 1976; Rogers and Agarwala-Rogers 1976). The concept of diffusion became even more important when it was applied to organizational management problems, i.e. how knowledge should be spread out properly in an organization to keep it dynamic, and to advertising campaigns, i.e. how to persuade users to buy new products. In this paper, we apply some of the basic ideas of the theory of diffusion of innovation to a new setting, i.e. how innovation travels among a community of artists. In this setting, at least two major aspects differ from previous studies of innovation. First, what is being diffused in the artistic context almost never ends up replacing all its alternatives on a grand scale; even the most successful diffusion remains bounded in its effects, and the adoption of innovation is almost never black and white. The second major difference is that in the spread of innovative products and ideas, the population (broadly speaking) may show skepticism and resistance, whereas artistic communities tend to embrace innovation and novel ideas and to share information on old and new techniques, styles and genres more freely. If we take a look at the history of Art Academies and how before them art education was handed down from one generation to the next from masters to students, we see that the spirit of sharing has not changed at all. What has changed since the introduction of the Internet is the speed and means of sharing information. Thanks to new tools and theories available to us today, we can follow the process of how artistic content travels, for instance in online social network sites, which build communities around ideas and form some of the most important information sharing platforms of our time (Boyd and Ellison 2007; Boyd 2007; Lewis et al. 2008; Mayer and Puller 2008). Our aim in this paper is to discuss the conceptual and technical tools that are needed to define innovative artistic content in such a social network site (SNS), and to seek quantitative measures for the specification of this content. This paper is structured as follows: In the next section, we describe our setting, namely deviantArt, our choice of SNS for this study, followed by our research questions and methodology. We exemplify how image, text and network analysis illuminate our case study, and conclude with a brief discussion.",8
12.0,1.0,Mind & Society,22 February 2013,https://link.springer.com/article/10.1007/s11299-013-0114-8,Fair framings: arts and culture festivals as sites for technical innovation,June 2013,Nona Schulte-Römer,,,Female,Unknown,Unknown,Female,"The new must be brought into the familiar world and enter into exchange with prior experiences. It must be given meaning and evaluated. The new must be different, but to be recognizable as the new, it requires observers to make a concentrated effort. (Nowotny 2008: 2). Before groundbreaking ideas, novel artefacts or unfamiliar processes can be considered as innovations they have to pass a cognitive test. Only if they are recognised and valorised as ‘new’ and ‘better’, in comparison to what already exists, will they successfully enter the world (Johannessen et al. 2001; Braun-Thürmann 2005; Marz 2010; Canzler and Marz 2011). Studies on science, technology and society (STS) have highlighted how meaning-making results in the emergence of networks of actors and artefacts that allow the social integration and stabilisation of the new. Here, socio-material settings are crucial as the material composition and social fabric of the situations in which the new is conceived influence how it finds spokespeople, audiences and users (Akrich et al. 2002a, b). As networks emerge around the new, the latter links and makes sense to more and more people and things. The transformation of an invention into successful innovations can thus be conceptualised as a process of network formation based on the successful “translation” (Callon 1986) of irritation into meaningful information that makes a cognitive impression, engages new audiences and stabilises the new (Hutter 2011). Yet, the shaping of such information is not trivial. It requires “investments in form”  (Thévenot 1986, 2007) and “framings” (Bijker 1987, 1992; Callon 1998). As I will show in the following, festivals consist of and provide a variety of such investments. Sharing historic roots with fun or trade fairs, they offer hybrid settings in which the presentation and consumption of novelty represent a key characteristic. My argument is twofold: After mapping the theoretical field, I outline how the recognition and positive evaluation of an innovative use of lighting technology was facilitated by the specific socio-material settings of two different festival performances. I show that festivals provide fair spaces—in the multiple sense of the word—as they are not only fun places but also offer different social groups open access and a variety of opportunities to share experiences and encounter the new. My second argument is that festivals promote very specific formats and framings for the production and positive recognition of the new without providing a robust management of overflows.",7
12.0,1.0,Mind & Society,17 February 2013,https://link.springer.com/article/10.1007/s11299-013-0122-8,Heritage as a basis for creativity in creative industries: the case of taste industries,June 2013,Christian Barrère,,,Male,Unknown,Unknown,Male,"In contemporary societies, science and technology are increasingly significant, and lead to a great deal of innovation. Nevertheless, alongside the well-known trends of scientification, technologization and intellectualization of modern capitalism, artistic and cultural fields are evolving. Some areas—such as design, ready-to-wear and video games—combine the artistic and industrial dimensions. Some economists have proposed new categories to describe these new fields: cultural industries, creative industries and creative economy being the main ones. In this paper we consider industries that have connected the artistic and industrial dimensions yet which are often underestimated: fashion, perfume, gastronomy, tourism, wines and a few others. We call them taste industries (TI), because demand for their goods derives from the logic of tastes rather than that of needs and necessity. Then tastes refer to pleasures and hedonism but their distance from necessity also allows individuals to implement ethic values and social preferences. We can observe a remarkable expansion of the field of taste: taste for pleasures (from food to sport) that used to be vile is more and more legitimated. In all developed countries, consumption surveys show that the purchase of taste goods is taking on more and more importance in the family budget. Spending on tourism represents 9 % of gross world product and 250 million jobs in the world, while spending on luxury industries amounts to more than €200 billion. Hirschman and Holbrook (Hirschman and Holbrook 1982) introduced the idea of hedonistic consumption by distinguishing two main types of utility: a direct, material, instrumental or functional utility, deriving from necessity and needs, and a hedonist utility, related to pleasure and desire. Other sociological observations (Lipovetsky 1987, 2006; Le Breton 2002) on the changing practices of distinction, and on the search for identityFootnote 1 have subsequently been incorporated into the theory (Cova and Cova 2001) as new forms of an ‘explosion of subjectivity’ (Addis and Holbrook 2001) that is characteristic of the contemporary period. On the other hand a lot of observations testify that individuals mix hedonic and moral considerations when they choose taste goods and services. They include their search for pleasure within meta-preferences or values (Sen 1977): they prefer tourism forms respecting the local populations, they choose green products, and they demand local produce and fair trade goods. The aim of this paper is to focus on the specificities of creative processes; in Taste industries creativity works on the basis of heritages, i.e. past and accumulated creativity, which plays a key role.",7
12.0,2.0,Mind & Society,09 December 2012,https://link.springer.com/article/10.1007/s11299-012-0111-3,"Hegel’s “Objective Spirit”, extended mind, and the institutional nature of economic action",November 2013,Ivan A. Boldyrev,Carsten Herrmann-Pillath,,Male,Male,Unknown,Male,"The foundations of the theory of institutions are today subject to constant rethinking. Philosophers, social, political and legal theorists, and economists are engaged in discussions on various basic questions about the nature and structure of institutional reality. The concept of institutions has enjoyed a particularly remarkable renaissance in recent interaction between philosophy and economics (e.g. Searle 2005), thus reviving the analytical stance of early institutionalist thinkers. These discussions are methodologically important because the answers to those basic questions guide research strategies in various fields of social sciences, including economics. An essential philosophical concern regarding these efforts seems to be to grasp the dimension of creativity of human institutions, in the sense of the human capability to create social systems that are not directly determined by any kind of biological or other physical causality, and yet reach beyond the scope of human intentionality (in Hayek’s famous phrase, being a product of human action, but not of human design). This is reflected in the emergence of institutions: there is no direct physical cause that transforms paper slips into ‘money’. Human social life is artificial to a large, if not essential degree. Yet, many contributions to this literature, including Searle (1995, 2010), adopt a naturalistic attitude to institutions. At first sight, this would imply treating institutions just as epiphenomena of ‘natural phenomena’, such as genetically embodied dispositions to social actions. But this would fail to recognize the dual meaning of naturalism, which also can imply that institutional facts possess their own nature, in the sense of being independent causes in the physical world (Bhaskar 1989; for a related Hegelian view on ‘normative essentialism’, see Ikäheimo and Laitinen 2011). In Searle’s parlance, observer-relative (or, in his more recent terminology, intentionality-relative) facts are facts after all, and thus are part and parcel of ‘social ontology’, which is in turn understood as part and parcel of general ontology. In this paper, we show that naturalism in this sense has been anticipated to a significant degree by Hegel in his concept of ‘Objective Spirit’. In fact, in his Philosophy of Right, Hegel famously characterized the institutional reality of objective spirit as a “second nature”: The basis [Boden] of right is the realm of spirit in general and its precise location and point of departure is the will; the will is free, so that freedom constitutes its substance and destiny [Bestimmung] and the system of right is the realm of actualized freedom, the world of spirit produced from within itself as a second nature. (par. 4, Hegel 1991, 34; compare par. 151, p. 195) Hegel’s notion of ‘spirit’ includes both the individual minds and their products in what is essentially the inter-subjective domain, and thus becomes a part of ‘nature’ as element of the external world. This transformation has been the object of recent theorizing on social ontology; yet the connection with Hegelian thinking is largely unexplored (Ikäheimo and Laitinen 2011, 7). Our paper is a resolute step in this direction and reaches towards implications for economics. While there is little doubt that contemporary thought cannot be “Hegelian” in the orthodox sense of his system, there is also widespread agreement that Hegel’s philosophy and social theory can be a source of inspiration for various disciplines, including the social sciences. We argue that the bridge between Hegel and economics is the concept of ‘extended mind’ in modern cognitive sciences. One of the most fundamental achievements of Hegel was an attempt to reconcile the fact of first-person experience with the fact of the external world (for a modern approach to this issue, see Strawson 2009). Although he is one of the towering intellectual figures of German idealism, Hegel himself was clearly aiming at a philosophical system that fully integrates the insights of the natural sciences of his days (Westphal 2008; Ferrini 2009). In adopting a naturalistic approach to mind, and in contrast to many modern versions of naturalism, Hegel already took the fundamental difficulty of naturalism head on and proposed a solution: namely, that naturalism might not be able to account for the fact of human freedom and creativity, which rests upon the autonomy of the first-person experience. Hegel’s solution to this problem is still unique: He asserted that it is precisely the externalized processes that ultimately establish this freedom in full extent. We pose the question: Can the modern approaches to the ‘extended mind’, which focus on these externalized processes, be fruitfully combined with Hegelian philosophy? In a truly Hegelian spirit, these approaches focus on the products of the mental processes and their feedback on the capacities of human cognition. Thus far, they mainly concentrate on technological artifacts in the broadest sense, as in the classic paper by Clark and Chalmers (1998). However, right from the beginning, researchers on the extension of human cognition into the outer world also included patterns of social interaction into their focus (Hutchins 1995). These patterns can be seen as pertaining to ‘institutions’ in our context. We argue that these recent developments are of utmost significance for economics, and that the conceptual framework for integrating them into economics can be built on Hegelian groundwork. With regard to economics, we approach a core theoretical notion in its edifice and demonstrate the institutional nature of preferences by expanding on earlier attempts at establishing an externalist framework for economics (Herrmann-Pillath 2012a). This is because in a Hegelian approach we can synthesize two seemingly conflicting foundational positions in current economics, namely the radical subjectivism of the theory of value and the behaviorism of the revealed preference approach to explaining choice. Currently, this methodological tension in contemporary economics erupts at the interface between neuroeconomics and utility theory, and affects basic assumptions such as the notion of consumer sovereignty. It is widely recognized that the modern theory of preferences does not make any statements about processes internal to human subjects, but only about observed actions, i.e. revealed preferences (Gul and Pesendorfer 2008; Bernheim 2009). In this regard, economics is strictly externalist (Ross 2005, 2011). However, this externalism meets with the emphasis on internalist subjectivism in the theory of value, which pertains to the normative dimension of economics, such as welfare economics, in the sense of choice and individual freedom. We argue that this tension can only be resolved by taking a radical step, namely to treat preferences as expressions of human institutions transforming fundamental human drives and needs (compare Witt 2000), and we show that this view can be systematically grounded in a Hegelian approach to the individual and ‘spirit’. In a nutshell, this is because Hegel has already offered a solution to the contradiction that haunts the philosophy of mind to this day, which is the tension between the fact of subjectivity, like first-person experience, and the possibility to explain human action in terms of causal interactions with the external world. We argue that this tension also inheres in the incompatibility between the externalist revealed preference concept of utility and the internalist emphasis on subjective value and individual autonomy of preferences. Hegel’s approach to the dynamic relationship between ‘subjective’ and ‘objective spirit’ overcomes this tension. Hegel’s philosophy of mind is externalist in a radical sense, namely identifying the mind (Geist) with the trajectory of a developmental system dialectically moving from ‘subjective spirit’ to ‘objective spirit’, the latter denoting those structures of the external world that emerge from human action (Quante 2008). This is a fundamental transformative process that is analyzed in great detail in the ‘Phenomenology of Spirit’ and ‘Encyclopedia’. We think that these contributions are highly relevant for the modern Philosophy of Mind, especially in relation to economics, if we take the notion of institution as a bridging concept. The Self, then, is by no means an internal phenomenon inaccessible to other minds, but it is established via those external structures, precisely if we consider autonomy as a defining feature of the Self. The autonomy of the self emerges from subjective spirit, but can only develop through its institutionalized expressions in interactions with other Selves. One of Hegel’s core ideas is that individual autonomy and freedom are not a presupposition of analysis, but actually an outcome of the transformations that lead from ‘Subjective’ to ‘Objective Spirit’. This is not to mean that the subjective factor and critical attitude towards existing institutions are downplayed or reduced to impersonal social structures, but that the individual agency and autonomy itself cannot be void from its social, institutional contexts, these structures teleologically ‘define’ it on the higher level of freedom. Otherwise subjective spirit remains abstract and turns into what Hegel in the Philosophy of Right calls ‘arbitrary will’ (Willkür, see Hegel 1991, p. 49).Footnote 1
 If we refer this idea to the economic concept of preferences, we would envisage that preferences would turn out to be facts that are external to the individual in the same way, namely structures of ‘Objective Spirit’ that transform underlying drives and needs into particular wants that are expressed in social context (consider a need for caloric intake that is transformed into a culturally shaped food preference). The straightforward way to reach this conclusion is to take the concept of revealed preferences seriously in the Hegelian sense, namely that preferences would be seen as facts in terms of being actions taken by people. In a Hegelian view, there are no preferences ‘behind’ the actions that take place in the external world (one cannot fully reduce the specific appetite for sweets to the need for caloric intake). The Hegelian view also transcends the standard approach to preferences in a very substantial way, namely showing that preferences, though being actualized in individual actions, can only be possible as collective level phenomena. This does not amount to a simple theory of social determination of preferences (in the sense of opposing homo sociologicus to homo economicus), but builds on the idea that preferences are performative in the same sense as institutions are performative. This notion of performativity relates to one essential aspect of Hegel’s philosophy, specifically the expressivity of mental phenomena (Taylor 1985). Performative actions imply collective intentionality in an essential way. If we ask how collective intentionality is established in the first place, we have to turn to recognition, another central Hegelian term. We will show how all these different aspects can be conceptually unified in a distinction between the individual and the person, and the pivotal role of the notion of identity that both separates and intermediates between the two. This paper proceeds as follows. In section two we present a brief introduction into the basic elements of Hegelian philosophy, building on the recent revival of international Hegelian studies that establishes close connections with the Anglo-Saxon tradition of analytical philosophy. For us, this new development gives the opportunity to merge recent developments in cognitive sciences and the philosophy of mind with Hegelian positions. The central Hegelian insight is that mental phenomena, in terms of the ‘spirit’, are fundamentally mediated via institutions. In section three we further scrutinize this insight in highlighting three underlying elements of a Hegelian approach: (1) The continuity thesis, which establishes the unity of internal and external mental phenomena; (2) The performativity thesis, which claims that mental phenomena and institutions coalesce into one process of expressive actions; and, (3) The recognition thesis, which posits that performative action is only possible when based on the collective intentionality that emerges from mutual recognition. In section four we show how these propositions can be applied to the economic theory of preferences. Section five concludes this paper.",8
12.0,2.0,Mind & Society,24 February 2013,https://link.springer.com/article/10.1007/s11299-013-0115-7,Perturbation theory in cognitive socio-scientific research: towards sociological economic analysis,November 2013,Masudul Alam Choudhury,Mohammad Saleh Ahmed,,Unknown,Male,Unknown,Male,"The mainstream socio-scientific research profession has been very active in using optimization methods for solving problems, predicting results, and forecasting futures. How well have they done in this front? Is there a need for a better way of conducting an objective study of events that defy the optimal and efficiency outlook of the physical sciences and social sciences? A cursory look at perennially uncertain and fuzzy events that fill up our lives points out that optimal and efficient methods of analysis, prediction and forecasting have run into deep methodological problems for explaining reality. Along the lines of the questions raised to tackle socio-scientific problems in the light of their perturbed nature in which everything remains embedded in complexity, there is this question: How can we select the appropriate method whereby the unstable nature of embedded variables in complex systemic interaction can be studied, both from the positive side of the existing state of the problem under study as well as from the perspectives of normative reformation? The imminent methodology in this direction is found to be epistemological and phenomenological in nature. The latter field by its very nature of analysis invokes the study of relational unity of knowledge as the episteme of evolutionary learning processes that involve relations among functional variables. The formal methodology, by the same epistemological approach of unity of knowledge, invokes the normative possibility of changing the inter-variables and inter-systemic organic relations between the functional variables. This conceptual part of the epistemological formalism examines the normative design of the investigated issue as it ought to be under the framework of systemic unity of knowledge and its induction of the cognitive issues at hand. Such ideas are also to be found in the functional ontological approach given by Gruber (1993), McCulloch (1970), Maxwell (1962), Edel (1970), and Hammond et al. (1991).",1
12.0,2.0,Mind & Society,21 February 2013,https://link.springer.com/article/10.1007/s11299-013-0116-6,Equilibria analysis in social dilemma games with Skinnerian agents,November 2013,Ugo Merlone,Daren R. Sandbank,Ferenc Szidarovszky,Male,Male,Male,Male,"Social dilemmas are situations in which each individual has a clear and unambiguous incentive to make a choice that provides a poorer outcome for all when it is made by all individuals than the outcome they would have received when none of them had made the choice (Dawes and Messick 2000, p. 111). Social dilemmas arise from collective actions in any part of or the entire society and have been discussed in several disciplines: economics (Ledyard 1995; Ostrom 1998), psychology (Dawes 1980), sociology (Kollock 1998). Furthermore, particular applications can be developed in homeland security (Brams and Kilgour 1988), public goods (Tabarrok 1998), international political economy (Conybeare 1984) among others. According to Kollock (1998, p. 183), “social dilemmas are situations in which individual rationality leads to collective irrationality”. Among other social dilemmas, N-person prisoner’s dilemma game has come to be viewed as one of the most common representations of collective action problems (Ostrom 2000). According to Santos et al. (2008, p. 213), the “N-person prisoners dilemma constitutes the most used metaphor to study public goods games”, yet according to the literature the two games are not immediately equivalent. For example, Conybeare (1984) analyzes the differences between the prisoners’ dilemma and public good games and a classification depending on the existence of provision point in the game is provided in Ledyard (1995). Nevertheless Hauert and Szabo (2003) generalize prisoner’s dilemma to an arbitrary number of players and by a simple transformation link the resulting game to a public good game. The prisoner’s dilemma and more generally the N-person prisoner’s dilemma can be considered a binary game with externalities (Schelling 1973). In the recent literature, this kind of interactions has been analyzed by several contributions. Bischi and Merlone (2009) provides a discrete-time dynamical system to model a class of binary choice games with externalities as those described by Schelling (1973). The dynamical properties of this game is studied in Bischi et al. (2009a, b); Gardini et al. (2011); Dal Forno et al. (2012) extendes the analysis to ternary choice games. Another intriguing social dilemma, the Braess paradox, is studied in Dal Forno and Merlone (2013). Finally, when considering binary games with interactions limited to neighborhoods, Merlone et al. (2007) proves that the number of different types of games is finite and then tight upper bounds for the number of game types were derived in the general case and the different game types were identified. In the study of social dilemma learning has an important role as it may be a possible explanation for the emergence of norms (Ostrom 2000), the dynamics of cooperation, but also as a way to make agents to base their predictions on experiential induction rather than logical deduction (Macy and Flache 2002). Other authors concentrated on the N-person prisoner’s dilemma considering both behavior patterns to model boundedly rational agents and learning. In Szilagyi (2003) some interesting results show strong dependence on the choice of model parameter values. Nevertheless, in this contribution there are some strong assumptions about learning. In fact, Szilagyi (2003) considers only the case in which cooperators and defectors have the same learning factors and provides an algebraic equation to characterize equilibria. Assuming that cooperators and defectors have the same learning factors is a serious limitation for several real life applications. For example, consider developing a homeland security model where a number of countries unite to fight terrorism activities from a specific group or threat. Each country agrees to contribute to the effort at a different level based on its size, resources, susceptibility to the threat and other factors. In this model, the payoff for each country is a level of security less its contribution to the effort. This is an N-person prisoners dilemma game where the learning curves for a cooperating and defecting country would likely be different because a defector would have to consider the negative political and foreign relation consequences in additional to the payoff when considering its next position. The purpose of this study is to relax this assumption and to develop an analytical solution to the N-person prisoner’s dilemma game when agents have different learning factors. Our equation is a straightforward generalization of the equilibrium equation with equal learning factors. As it will be explained later, both equations hold only under certain conditions. We will also develop a modified dynamic model where these restrictions are eliminated. However, in this more realistic case no analytical solution is found. Therefore agent based simulation is used to examine the dynamic properties of the system. In an agent based simulation model of binary games individual agents may cooperate with each other for the collective best interests or defect to pursue their own self interest. Each agent receives a reward or punishment based on its individual decision and the collective decisions of the other agents. The study of social dilemma problems is very important since it helps us understand problems that society is facing today. The prisoner’s dilemma is an important example of a social dilemma which is frequently examined in the literature. Agent based social simulation started in the 1990s, (Gilbert and Terna 2000) and this research area is growing significantly. The main advantage of agent based simulation is that it is a bottoms-up approach where the agents’ attributes may include personalities, characteristics and learning capabilities. Agent based simulation can be used to study artificial societies for emergence of groups with common attributes or social segregation (Epstein and Axtell 1996). This methodology is used in this paper for two purposes. First the analytical solution is verified when its conditions are satisfied, and second, in the more realistic model when no analytical solution is available, it is the most appropriate tool to analyze the dynamic properties of the model, because, as mentioned in Young (1998), simulation can be used to establish constructive sufficiency. The structure of the paper is the following. In Sect. 2 we summarize the prisoner’s dilemma and its N-person version which is the object of our analysis. In Sect. 3 we derive the analytic solution for Skinnerian agents. The cases of other types of boundedly rational agents will be discussed briefly. In Sect. 4 we describe the simulation analysis and compare the simulation and theoretical results. Finally, the last section is devoted to conclusions and further research directions.",5
12.0,2.0,Mind & Society,17 May 2013,https://link.springer.com/article/10.1007/s11299-013-0130-8,On the role of emotion in rational choice,November 2013,Junichi Minagawa,,,Male,Unknown,Unknown,Male,"People behave both rationally and emotionally in their decision making. This view of human beings is common in daily life. However, not much attention has been paid to emotional aspects in the economics literature, as there has been a long tradition of assuming rationality in economic decisions. Notable exceptions to this are, for example, Smith (1759), Hawtrey (1926), Scitovsky (1976), Hirshleifer (1987), and Frank (1988), who explore multiple motivations in human behavior (for a survey in this area and for more recent studies, see Elster (1998) and references cited therein). A version of the above view of human beings is also expressed by Knight (1960, p. 71):
 The concept of the economic man is valid and useful; it is fundamentally true that men behave economically, that is, as economic men, to an important degree. But also to a large extent they do not; their motivation is mixed; they behave in many other ways, even in part at the same time. […] People often behave romantically, in one or another of many senses, which is in principle the opposite of behaving with economic rationality; it covers the poetic, humorous, or disgusting, the feelings side of life. Therefore, there is a need to go beyond the traditional approach by seeking to explain seemingly irrational human behavior. One such attempt has been made in the literature on non-exponential discounting such as hyperbolic or quasi-hyperbolic discounting. Examples are Strotz (1956), Ainslie (1985), Loewenstein and Prelec (1992), and Laibson (1997), who study dynamic inconsistency in intertemporal choice. An everyday example of this is: we set the alarm clock to rise early but shut it off the next morning and go back to sleep (Loewenstein and Thaler 1989). To overcome such weakness of will (Ainslie 2001), we put the alarm clock across the room (Schelling’s 1978 self-management). This paper tries to add an emotional aspect to the concept of rational economic man; in particular, it analyzes the role of emotion in terms of a static rational choice model in which consumption takes time in addition to money. The key concept here is “time illusion”. Subjectively, time is variable. Casual observation and some evidence indicate that the perception of the time spent on an activity is often different from the actual time spent, and depends on how enjoyable the activity is; the more enjoyment people feel, the shorter the perception of the time spent (see, e.g., Henley et al. 1981; Hornik 1984, 1992; Hornik and Zakay 1996; Droit-Volet and Gil 2009). Consequently, and this is the novelty of this paper, if time illusion is present it is likely to affect people’s evaluation of the opportunity (or time) cost of the activity, i.e., the shorter the perception of time, the lower the evaluation of opportunity cost. The model proposed in this paper provides a basis for explaining the consumption bias in favor of a particular activity by a difference in opportunity rather than by a difference in preference. The main implication is that an activity that is perceived as less time intensive is demanded more as the price of time increases. This result may be seen as a modified version of the theory of Becker (1965) and Linder (1970) when the price of time is equal to the wage rate and there is no time illusion, and may also be compatible with the paradoxical fact, pointed out by Scitovsky (1974), that with rising incomes, a prevalence of time-consuming do-it-yourself activities is observed. Most importantly, we have a testable prediction, which is obtained for a special case in which the money and time spent on one activity are respectively equal to those of another activity but the degrees of time illusion are different: an activity that one perceives as taking less time is demanded more as the price of time increases. That is, an increase in the price of time leads individuals to spend more time in a more enjoyable activity relative to other similar activities (e.g., watching a favorite genre of movies, as compared to other genres). It is also shown that this implication distinguishes our approach from others. The organization of the paper is as follows. In the next section, the concept of time illusion is presented. This concept is incorporated into a static rational choice model and its implications are discussed in Sect. 3. Then, an application is shown in Sect. 4, before concluding in Sect. 5.",
12.0,2.0,Mind & Society,09 August 2013,https://link.springer.com/article/10.1007/s11299-013-0132-6,Reflecting on Gigerenzer’s critique of optimisation,November 2013,Andrea Polonioli,,,Female,Unknown,Unknown,Female,"The claim that normative standards need to take into account the resources of real cognizers is accepted by several recent accounts of rationality (e.g. Cherniak 1984; Bishop and Trout 2005). For instance, Cherniak suggests that ‘the quick but dirty heuristics uncovered by psychological research may be not irrational sloppiness, but instead the ultimate speed-reliability trade-off to evade intractability’ (1984, 740). However, among the attempts to ground rationality in actual cognition one that deserves special attention has been proposed by Gerd Gigerenzer and other researchers in the Centre for Adaptive Behaviour and Cognition (ABC). Their project deserves a great deal of attention, especially because it is one of the few that has the courage to portray an entire new framework on rationality. In a series of recent publications, these scholars have attempted to substantiate their claim that ‘new norms of rationality can be derived from empirical research’ (Gigerenzer and Sturm 2012, 244). Since ABC scholars have tried to question classic paradigms in cognitive science and behavioural economics, it should be briefly recalled here what their empirical project is about. In the past four decades, psychologists of decision making have presented many disturbing results concerning the way people make decisions, suggesting that under several conditions human decision-making departs from the normative rules of reasoning (see Hastie and Dawes 2001 for a review on the topic). Still, in a series of perceptive and influential papers published in the past 15 years, Gigerenzer and his collaborators have started to question the pessimistic conclusions drawn by many researchers in the field, not only appealing to methodological arguments intending to show that the findings were due to unrepresentative contexts (e.g. Hertwig and Gigerenzer 1999), but also presenting new and interesting empirical results (e.g. Gigerenzer and Goldstein 1996). More precisely, these scholars have tried to show that the heuristics people are endowed with perform well in the real world, in spite of their simplicity. The claim that quite simple rules might be reliable had already been defended in the literature (e.g. Grove and Meehl 1996), yet Gigerenzer et al. have presented imposing evidence that people do naturally employ simple but accurate heuristics as default strategies, achieving adaptive behaviour in a number of important domains. On the background of their descriptive studies, Gigerenzer et al. have recently tried to draw out important normative conclusions in their works. According to them, these findings speak against the framework of optimisation, where an optimising model is one that ties rationality to the maximum of some function, and expected utility theory and Bayesian models of theoretical inference are of course the best known examples of optimising models. ABC scholars are eager to point out that, whereas ‘the idea that sound decisions are reached by optimisation, with or without constraints, has been criticised as descriptively inadequate, but maintained as the only normative standard, the research program of studying the adaptive toolbox goes one step further and analyses how sound decisions can actually be made without omniscience, optimisation, and general logical calculus’ (Gigerenzer 2001a, 19). Specifically, according to ABC scholars, There are several motives for abandoning the ideal of optimisation. First, in many real-world situations, no optimising strategy is known. […] Second, even when an optimising strategy exists, it may demand unrealistic amounts of knowledge about alternatives and consequences, particularly when the problem is novel and time is scarce. […] Third, strategies that do not involve optimisation can outperform strategies that attempt to optimise. (Gigerenzer 2001b, 3305) The challenge Gigerenzer et al. attempt to deliver is a fierce attack to the orthodox view of rationality (e.g. Gigerenzer and Selten 1999). Given the nature of the claims made, it should come as no surprise that the ABC’s challenge has resulted in turn in scepticism by advocates of the standard view of rationality. The debate that originated is an interesting one, and there are several questions worthy of discussion that arise when considering the challenge mounted by the ABC project. For instance, a group of physiologists and cognitive neuroscientists looked towards standard optimising models of economic theory as a tool to test and develop algorithmic models of the neural hardware for choice, challenging the claim that optimising strategies are really so far from what real people can aspire to (e.g. Glimcher 2009). In this paper I want to focus on a broader and more fundamental issue. Specifically, if we consider the usual presentation of the challenge, it looks as if Gigerenzer’s and the standard approach based on the ideal of optimisation are simply incompatible ways of reflecting on rational behaviour, but a question that arises is whether it is really the case that these approaches should be treated like chalk and cheese. What I venture to claim here is that there is a way to look at these approaches from which the disagreement seems to be less radical than ABC scholars usually declare. The structure of the paper is as follows. In Sect. 2 I present Gigerenzer’s challenge to optimisation. In Sect. 3 I introduce my argument, and the relevant distinction between two sorts of reflection upon norms of rationality. In Sect. 4 I apply this distinction to Gigerenzer’s challenge. I conclude in Sect. 5.",
12.0,2.0,Mind & Society,31 March 2013,https://link.springer.com/article/10.1007/s11299-013-0126-4,Notes on Lorenzo Magnani Understanding Violence,November 2013,Massimo Durante,,,Male,Unknown,Unknown,Male,,1
12.0,2.0,Mind & Society,31 May 2013,https://link.springer.com/article/10.1007/s11299-013-0131-7,Re-assessing the Heuristics debate,November 2013,Andrea Polonioli,,,Female,Unknown,Unknown,Female,"Defenders of Kahneman and Tversky’s heuristics-and-biases approach (HBA) and the fast-and-frugal heuristics approach (FHA) of Gigerenzer and his colleagues are notoriously divided over fundamentals concerning human rationality, up to the point that they have been often presented as two diametrically opposed approaches to human rationality. Primarily, HBA suggests that people are prone to widespread and systematic reasoning errors, whereas FHA maintains that a more optimistic view of human rationality is preferable. In The Heuristics Debate (HD), Kelman attempts to achieve two main goals. First, HD tries to deliver an updated and rigorous reconstruction of the controversy between HBA and FHA. Second, HD seeks to explore the implications of the debate for jurisprudence and policymaking. There are several reasons why the book seems to add to the already extensive literature on the so-called “rationality wars.” An important contribution is HD’s attempt to draw out the practical implications of the debate. The idea that claims about human rationality affect policy-making has already been explored in detail in the literature. Economists who are sympathetic to psychological research within HBA have often accepted that cognitive errors are ubiquitous and damaging, and that governments should paternalistically intervene to “de-bias” individuals (e.g. Thaler and Sunstein 2008). On the other hand, while accepting that “de-biasing” is needed, supporters of FHA disagree with HBA on the extent to which “de-biasing” is required (e.g. Berg and Gigerenzer 2007, 2010
). HD seeks to unearth and discuss several important and unexplored practical implications of the dispute. Specifically, Kelman attempts to relate the dispute to important issues such as how to design an optimal system for criminal punishment. While these are certainly important aspects of the controversy, discussing them seems to presuppose the correct reconstruction of the dispute. In this review essay the focus is on HD’s analysis of the debate between HBA and FHA. It seems that in addition to several merits, HD also has some problems. Let us discuss the merits first. It is especially laudable that HD delivers a careful reconstruction of the dialectical situation, trying to take into account the arguments of both the approaches. Consider that advocates of both the schools tend to be self-referential, ignoring the points made by their opponents. Commentators also usually focus only on some specific arguments and aspects of the dispute (e.g. Vranas 2000; Samuels et al. 2002; Bishop 2006; Gruene-Yanoff 2007; Rysiew 2008). A case in point is the discussion of individual differences in rational thinking. While this is explicitly presented by advocates of HBA as an important arrow in their quiver, critics and commentators rarely discuss such evidence. It is notable that HD discusses this recent development of HBA. It is also remarkable that HD avoids some distortions that the reader might find in the literature. HD rightly shows that the dispute has to be resolved, and not dissolved, in contrast to what is argued by some commentators, who have claimed that the debate is based on nothing but minor disagreements and rhetorical flourishes (e.g. Samuels et al. 2002; Gruene Yanoff 2007). Another distortion is that according to FHA “rational” behavior is that which maximizes fitness (e.g. Stanovich and West 2003). As it should emerge quite clearly from this review essay, Kelman’s account does not make this error. However, not all is well in HD’s reconstruction of the dispute. While HD certainly provides the reader with a detailed reconstruction, the fundamental question of what constitutes a “rational” judgment, which represents the fundamental bone of contention between HBA and FHA, has not been discussed adequately. This is quite unfortunate given that, by failing to appreciate the scope and nature of the normative disagreement, Kelman also fails to acknowledge the interplay between normative and methodological considerations. With regard to this aspect, HD fits into a tradition of inaccurate presentations of the dispute. The present study takes the opportunity to rectify these errors, discussing the fundamental bones of contention between the two schools of thought.",3
12.0,2.0,Mind & Society,15 May 2013,https://link.springer.com/article/10.1007/s11299-013-0129-1,Lorenzo Magnani & Ping Li (Eds.) Philosophy and cognitive science: Western & Eastern studies,November 2013,Ryan D. Tweney,,,,Unknown,Unknown,Mix,,
13.0,1.0,Mind & Society,03 May 2014,https://link.springer.com/article/10.1007/s11299-014-0145-9,Special issue on “Bounded Rationality updated”,June 2014,Marco Novarese,Riccardo Viale,,Male,Male,Unknown,Male,,
13.0,1.0,Mind & Society,09 April 2014,https://link.springer.com/article/10.1007/s11299-014-0142-z,In memory of Herbert A. Simon,June 2014,Katherine Simon Frank,,,Female,Unknown,Unknown,Female,,
13.0,1.0,Mind & Society,24 May 2014,https://link.springer.com/article/10.1007/s11299-014-0146-8,The economics of wishful thinking and the adventures of rationality,June 2014,Massimo Egidi,,,Male,Unknown,Unknown,Male,"In occasion of her visit to the London School of Economics in November 2008, Queen Elizabeth II asked why so few economists had warned about the emerging financial crisis, perhaps the most severe crisis after that of 1929 (“if these things were so large, how come everyone missed them?”); and one might well ask, then, why so few of them had remained unheard. Two answers followed, one from the academic world, the other one from the business world, both showing quite a bit of embarrassment in admitting that even financial experts can run into distortions of assessment, and that might be unable to rationally master risk and uncertainty. The first letter, signed by a group of eminent economists of the British Academy, including Tim Besley, a member of the Bank of England’s Monetary Policy Committee, and historian Peter Hennessy, was sent on 22 July 2009. The letter stated that: … against those who warned, most were convinced that banks knew what they were doing. They believed that the financial wizards had found new and clever ways of managing risks. Indeed, some claimed to have so dispersed them through an array of novel financial instruments that they had virtually removed them. It is difficult to recall a greater example of wishful thinking combined with hubris. There was a firm belief, too, that financial markets had changed. And politicians of all types were charmed by the market. These views were abetted by financial and economic models that were good at predicting the short-term and small risks, but few were equipped to say what would happen when things went wrong as they have. …….. So in summary, Your Majesty, the failure to foresee the timing, extent and severity of thecrisis and to head it off, while it had many causes, was principally a failure of the collective imagination of many bright people, both in this country and internationally, to understand the risks to the system as a whole.” (Besley and Hennessy 2009). Few months before, in Spring 2008, a significant number of US economic players had become overexposed to financial risk, and realized they were unable to pay their own debts: it was just the harsh beginning of a massive financial crisis, the effects of which we are still suffering. The reaction from most economists and financial experts was surprise; the most pragmatics amongst them looked back over post-keynesian economist Hyman Minsky—that was largely considered out-of-date at that time—who 30 years earlier had developed a theory concerning the dynamic forces driving financial crises. Minsky identifies three different classes of “economic units”, which he labeled as hedge, speculative, and Ponzi finance, characterized by distinct income-debt relations. Hedge financing units are those which can fulfill all of their contractual payment obligations by their cash flows: the greater the weight of equity financing in the liability structure, the greater the likelihood that the unit is a hedge financing unit. Speculative finance units are units that can meet their payment commitments on “income account” on their liabilities, even as they cannot repay the principle out of income cash flows. Such units need to “roll over” their liabilities: (e.g. issue new debt to meet commitments on maturing debt). Governments with floating debts, corporations with floating issues of commercial paper, and banks are typically hedge units. For Ponzi units, the cash flows from operations are not sufficient to fulfill either the repayment of principle or the interest due on outstanding debts by their cash flows from operations. Such units can sell assets or borrow. Borrowing to pay interest or selling assets to pay interest (and even dividends) on common stock lowers the equity of a unit, even as it increases liabilities and the prior commitment of future incomes. A unit that Ponzi finances lowers the margin of safety that it offers the holders of its debts.Footnote 1
 Minsky asserts that if hedge financing dominates, then the economy may well be an equilibrium-seeking and containing system: conversely, the greater the weight of speculative and Ponzi finance, the greater the likelihood that economy is a “deviation-amplifying” system. Moreover, he suggests that over periods of growth, market economies tend to move from a financial structure dominated by hedge finance to a structure increasingly emphasizing speculative and Ponzi finance. To control the natural instability of a dynamic market economy, he emphasized the importance for Governments to create adequate constraining institutions to stabilize the economy and control the rise of speculative bubbles. After the Seventies, however, the trend went exactly in the opposite direction, with the political authorities progressively removing market barriers, and failing to give regulatory agencies the authority to regulate investment banks activities. These policies were abetted by the wide success of new theoretical pillars that to some extent justified the reduction of regulatory activities: the theory of rational expectation, and the theory of efficient markets. Whatever the opinion about the realism and theoretical robustness of Minsky approach, it is important to note that his model does not rely on exogenous shocks to explain the generation of business cycles and crises; in his view, the fundamental background of the dynamic process leading to a crisis, is the existence of three stable classes of economic agents with dissonant beliefs and expectations that, as we have seen, he classified as hedge, speculative and Ponzi. The idea that financial markets are populated by heterogeneous agents, i.e. traders that are different with respect to their expectations about future prices and dividends, has been resurrected and is growing in relevance in the recent years, but at that time (around 1970), Minsky’s assumption of heterogeneity of expectations was rapidly marginalized by the emergence of the new, successful paradigm based on rational expectations hypothesis. This approach is based on assumptions opposite to Minsky’s: following Muth, in fact, agents are supposed to be able to make prediction about the relevant economic variables, without systematically failing; in consequence “expectations, since they are informed predictions of future events, are essentially the same as the predictions of the relevant economic theory”.Footnote 2 Economic agents are supposed to formulate expectations coherent with the “model” of the economy, and for this reason neither then they can fail systematically their predictions nor they can formulate systematically heterogeneous predictions. Commenting the impact of rational expectations hypotesis, and the subsequent emergence of the limits of this approach, Robert Shiller notes: The efficient markets theory reached its height of dominance in academic circles around the 1970s. At that time, the rational expectations revolution in economic theory was in its first blush of enthusiasm, a fresh new idea that occupied the center of attention. The idea that speculative asset prices such as stock prices always incorporate the best information about fundamental values and that prices change only because of good, sensible information meshed very well with theoretical trends of the time. Prominent finance models of the 1970s related speculative asset prices to economic fundamentals, using rational expectations to tie together finance and the entire economy in one elegant theory. He continues: One could easily wish that these models were true descriptions of the world around us, for it would then be a wonderful advance for our profession. We would have powerful tools to study and to quantify the financial world around us. Unfortunately, elegance is hardly synonym of realism. And: Wishful thinking can dominate much of the work of a profession for a decade, but not indefinitely. The 1970s already saw the beginnings of some disquiet over these models, and a tendency to push them somewhat aside in favor of a more eclectic way of thinking about financial markets and the economy. Browsing today again through finance journals from the 1970s, one sees some beginnings of reports of anomalies that didn’t seem likely to square with the efficient markets theory, even if they were not presented as significant evidence against the theory. For example, Eugene Fama’s 1970 article, “Efficient Capital Markets: A Review of Empirical Work,” while highly enthusiastic in its conclusions for market efficiency, did report some anomalies like slight serial dependencies in stock market returns, though with the tone of pointing out how small the anomalies were.Footnote 3
",2
13.0,1.0,Mind & Society,01 February 2014,https://link.springer.com/article/10.1007/s11299-014-0136-x,Is it rational to have rational expectations?,June 2014,Alan Kirman,,,Male,Unknown,Unknown,Male,"In a world where the future is uncertain, and individuals take decisions both for the present and the future we cannot achieve an understanding of the evolution of an economy without taking into account the expectations that individuals hold about the future. As the term « rational expectations » suggests, there has been a widespread belief in economics that people’s anticipations about the future should, in some sense, be consistent with the way in which that future unfolds. Yet, this use of the term « rational » is recent and does not correspond to the use of the term in the earlier economic literature. There, rationality was originally meant to convey the idea that individuals acted in their own best interest, however that was defined. Why then, has this topic returned to centre stage? This is principally because of a recent reaction against the theoretical macroeconomic models which have ruled the roost for the last thirty years. At the beginning of that period, with the adoption of a rigorous form of methodological individualism as a basis for macroeconomic models there was a need to complete the specification of the individual with a description of what his expectations were. The so-called « rational expectations Revolution », involved the incorporation of that concept as a central feature of modern macroeconomic models. The concept was not only supposed to be a neat way of eliminating the difficulty posed by the fact that the decisions of individuals today are, at least in part, based on what they anticipate for the future. It went further, and the assumption came to be that the people in the economist’s model understood and shared that model and based their expectations and decisions on that model. Individuals in the more advanced models were taken to have a comprehensive and accurate view of how the economy evolves. But, since the onset of the crisis this approach has come in for heavy criticism, particularly from policy-makers. There is a strong sentiment that we are attributing much too much reasoning capacity and knowledge to individuals. Herb Simon’s ideas on bounded rationality inevitably came back into the picture. In this paper I will outline the history of the concept of rational expectations, and its place in economic theory. I will then describe the sources of the dissatisfaction with the idea and will suggest that we should simply remove the rational expectations approach as one of the pillars of modern macroeconomic models.",15
13.0,1.0,Mind & Society,08 March 2014,https://link.springer.com/article/10.1007/s11299-014-0137-9,Endogenous preference formation on macroeconomic issues: the role of individuality and social conformity,June 2014,Guido Baldi,,,Male,Unknown,Unknown,Male,"Decisions on macroeconomic issues are often characterized by a high level of abstractness, uniqueness and uncertainty. For instance, if a sovereign debt crisis occurs, should the government default on its debt, or implement spending cuts and tax increases? Or, should a central bank adopt an inflation target? As I will argue in this paper, individuals cannot be expected to have so-called “deep” preferences on such issues. I propose a simple way to model the formation of preferences as the result of an interaction between what I call private motivating intuitions (or initial convictions) and social opinions. Preference formation is rational in the sense that individuals reflect on their preferences and do not act thoughtlessly. This is modeled as a process in which the salience of preferences is influenced by the degree of individualism and conformity. In this way, the preference-formation process provides reasons for preferences and preference modifications. As I refer to a minimal definition of rationality, full consistency or transversality are not necessarily attributes of rational behavior. While the macroeconomic debate on rationality mainly centers on the formation of expectations, I focus on the role of rationality in the formation of preferences. Using a simple model, I illustrate how the salience of preferences changes with different degrees of individuality and conformity. Individuality is associated with the persistence of private motivating intuitions or convictions and conformity is related to the perceived dissonance between private intuitions and social opinions. The results of simple simulation exercises where experts or policymakers influence social opinions stress the importance of individuality in determining the salience of preferences. A high degree of individuality or a low degree of conformity prevents individuals from overreacting. A low degree of individuality and a high degree of conformity, however, lead to overreactions in cases where social opinion makers (for example experts or policy-makers) err for a short period of time. But high individuality and low conformity may also have drawbacks: if the degree of conformity is low or the degree of individuality is high, delayed adjustments may be observed. As mentioned above, I assume that an individual’s action-inducing preferences over possible worlds, states or outcomes depend on two factors: the intuitions that motivate the individual and social opinions that implicitly attach different weights to the motivating intuitions. Social opinions are influenced by “opinion-makers” such as experts, policy-makers or journalists. In the approach taken in this paper, social opinions are not chosen strategically and cannot be attributed to persons. An active interaction between individuals and social opinions is therefore not modeled in this paper. Both private intuitions and social opinions are essential for individual behavior. Changes in social opinions can cause changes in private habits by giving rise to new perceptions and dispositions within individuals. In this way, changes in social opinions produce downward effects. When deriving preferences, individuals partly interiorize social opinions. These arguments are related to the concept of procedural rationality proposed by Simon (1976), according to which the choice at a particular point in time is context-specific. This involves a rejection of the idea that human nature, or at least the motives, goals and preferences of individuals are entirely psychologically given. However, as it is shown in this paper, this does not involve a rejection of the importance of individuality. In psychological terms, one could argue that private motivating intuitions are produced by the “fast system”, while the action-inducing preferences are determined by the “slow system”. In this way, the motivating intuitions together with the social opinions determine the salience of preferences. It has been shown that the past influences the current salience of preferences. This brings us to the notion of habits. Habits are propensities for responding in a specific way to a particular class of situations (see, for instance, Rutherford 1994). Repetition is an important feature of habits. Habits, however, are much more complex than mere repetitions and involve the imitation of others, which makes habits closely related to social institutions. According to Hodgson (2004), “…the reason why we have evolved the capacity to form habits is to deal with the uncertainty, complexity and variability of circumstances…”. This stresses the importance of studying habits in the context of macroeconomics and, in particular, rare macroeconomic events that are often associated with uncertainty, complexity and uniqueness. The understanding of the notion “habit” in this paper as being related to preferences differs from the understanding in parts of the macroeconomic literature, where habits are formed over economic variables such as consumption and may yield correlated behavior or correlated choice over time.Footnote 1
 By acknowledging the existence of habits, I argue that macroeconomics should take into account that preferences of individuals can be endogenous, in particular over the medium- and long-term. In this paper, preferences of individuals are the result of an interaction between private motivating intuitions (influenced by private habits) and social habits or, as I call them, social opinions. Social opinions act as a weighting function that individuals use to attach different weights to their intuitions. By emphasizing the importance of private intuitions and social opinions, the model of this paper does not strictly adhere to methodological individualism. But, at the same time, individuals are not exclusively seen as purely social beings. Private motivating intuitions are influenced by private habits and are associated with individuality. Conformity is characterized by collective behavior. Conformity implies that the social status is sufficiently important for an individual. Individuals depart from their heterogenous intuitions or convictions to partly conform to a single, more homogeneous way of behavior. The arguments made in this paper are inspired by various sources. The importance of habits and endogenous preferences was stressed long ago by the proponents of the early institutionalist movement in the 1920s and 1930s (see Rutherford 1994 or Hodgson 2004 for useful overviews). In addition, this paper draws on the literature on endogenous preferences in microeconomics (see, for example, Rabin 1994, 2013; Bowles 1998, 2004). Conformity has been analyzed in an economic model, for instance, by Bernheim (1994). Interpreting social habits or opinions as a weighting function for private motivating intuitions has certain relations to the arguments in Dietrich and List (2013), and the discussion in the philosophy and economics literature on reasons for preferences. Recently, Hoff and Stiglitz (2010) introduced the notion of “social constructs”, which is very similar to the notion of social habit used by others and the notion of social opinions in this paper. They argue that social constructs provide the cognitive frames that individuals use to shape their perception. Similar to my arguments, Rodrik (2014) uses the notion of “ideas” to describe how policy decisions are derived. My paper is also loosely related to the literature on social capital (see, for instance, Knack and Keefer 1997), although I do not interpret social opinions as being capital stock. As noted by Rabin (2013) and Harstad and Selten (2013), trying to improve the psychological realism of economics does not necessarily imply that one has to entirely reject the concept of optimization. Instead, incorporating the impact of habits or the desire for conformity can be done within the existing modeling framework. The rest of this paper is organized as follows. Section 2 illustrates a simple modelling approach of endogenous preference formation that takes into account the private and social dimension of preference formation. Section 3 presents the results of simple simulation exercises. Finally, Sect. 4 contains the conclusion.",1
13.0,1.0,Mind & Society,06 April 2014,https://link.springer.com/article/10.1007/s11299-014-0141-0,"A three-pronged simonesque approach to modeling and simulation in deviant “bi-pay” auctions, and beyond",June 2014,Joe Johnson,Naveen Sundar Govindarajulu,Selmer Bringsjord,Male,,Unknown,Mix,,
13.0,1.0,Mind & Society,23 April 2014,https://link.springer.com/article/10.1007/s11299-014-0143-y,Bounded rationality in problem solving: Guiding search with domain-independent heuristics,June 2014,Pat Langley,Chris Pearce,Miranda Emery,,,Female,Mix,,
13.0,1.0,Mind & Society,04 April 2014,https://link.springer.com/article/10.1007/s11299-014-0139-7,The interpretative heuristic in insight problem solving,June 2014,Laura Macchi,Maria Bagassi,,Female,Female,Unknown,Female,"According to Simon, the problem solver’s search for the solution is “an odyssey” through problem space from one state of knowledge to another, until his/her knowledge state includes the solution. But when our Ulysses is struggling with an insight problem, he can dare to create an alternative space and venture into the unknown, with his interpretative compass pointing to the perceptive and verbal information, in other words, to the context. The problem solver will only be able to let go the usual representation, the representation that has been successful over time, by a spreading exploration of his/her available knowledge, relevantly processing information unconstrained by consciousness. Our view, therefore is that the Interpretative Heuristic is a characteristic inherent to all insight problem solving processes, and in general, is an adaptive characteristic of the human cognitive system. It guarantees cognitive economy when meanings and relations are familiar, permitting their recognition. This same process becomes much more arduous when meanings and relations are unusual, allowing to face the novel. When this happens, s/he has to come to terms with the fact that the usual interpretation will not work, which is a necessary condition to exploring other ways of interpreting the problem. This in itself is not mysterious or inexplicable, it is the result of a constant search for possible relations between the parts and the whole until everything falls into place and nothing is left unexplained, using an interpretative heuristic type process. This is similar to the process adopted when a generalized meaning does not contribute to our understanding, and it is necessary to select a more appropriate meaning. The original representation of the data changes when a new relation is discovered, giving rise to a gestalt, a different vision of the whole, something which has a new sense. The same data, seen in a different light, provide a view that is different to the default, the original starting point. As we have seen, new relations are found by exploring different interpretations, and not by exhaustive searches or abstractions. What is important to underline here is the impact of Simon’s extended theory for the study of creative thought, i.e., the representational change, which reminds us of the Gestaltists’ restructuring, and which is still unexplained by the dichotomic theories. This search activity goes beyond the boundaries of WM. The process that leads to the discovery of the solution through restructuring is therefore mainly unconscious and can only be described a posteriori. It is characterized by a period of incubation which may vary in length, necessary to loosen the interpretation default constraints on WM and allow new explorations of the problem, guided by the relevance of the individual’s knowledge. The traditional distinction between fast-intuitive and slow-reflexive, conceived as either an architecture of the mind (two systems) or as cognitive styles (intuitive vs. reflexive), adds little to our understanding of the different types of thought processes. How better to close than with the inspiring words written by Herbert Simon: There is much beauty in the superficial complexity of nature. But there is a deeper beauty in the simplicity of underlying process that accounts for the external complexity. There is a beauty in the intricacy of human thinking when an intelligent person is confronted with a difficult problem. But there is a deeper beauty in the basic information processes and their organization into simple schemes of heuristic search that make that intricate human thinking possible. It is a sense of this latter beauty-the beauty of simplicity-that we tried to convey to you (Simon and Newell 1971, p. 159).",6
13.0,1.0,Mind & Society,24 December 2013,https://link.springer.com/article/10.1007/s11299-013-0134-4,"Roles of implicit processes: instinct, intuition, and personality",June 2014,Ron Sun,Nick Wilson,,Male,Male,Unknown,Male,"Our long-standing research goal has been to understand the interaction of implicit and explicit psychological processes. Sun (1995, 1994) developed a computational cognitive model that contained distinct implicit and explicit processes for modeling human everyday commonsense reasoning. Sun et al. (2001, 2005) applied a variant of the model to the understanding of human skill acquisition, capturing the synergy resulting from the interaction of implicit and explicit processes. On that basis, the present work explores implicit and explicit processes in shaping an individual’s characteristic behavioral patterns, that is, personality. The questions are how psychological processes can be separated into implicit and explicit components, and how such a separation figures into personality, which may have significant implications. In particular, we want to explore the role of instinct and intuition in determining personality (characteristic behavioral patterns). We argue that personality may be fundamentally based on instinct (resulting from basic human motivation), along with related processes (including intuition), within a comprehensive cognitive architecture. This is implemented within the computational cognitive architecture CLARION. Note that instinct and intuition are both folk psychological notions. As such, they are not precisely defined; in fact they are quite elusive concepts. They have been used in a variety of different ways. We hope that, by using computational modeling, we may be able to clarify these concepts, in a mechanistic, process-based way, with the precision and concreteness that computational theories often offer. For instance, “instinct”, according to Merriam-Webster Dictionary, is “a largely inheritable and unalterable tendency of an organism to make a complex and specific response to environmental stimuli without involving reason.” According to Oxford English Dictionary, it is “an innate, typically fixed pattern of behaviour in animals in response to certain stimuli.” The question is how instinct may be carried out mechanistically (i.e., computationally), and moreover, how “fixed” or “unalterable” it is (i.e., whether it is “tunable” to some extent). Some initial work has been done to elucidate this notion. For instance, Sun (2009) discussed how an adequate, necessary, and appropriate representation of basic human motivation and related metacognitive, action decision-making, and other processes, within a generic, comprehensive computational model, captures some forms of instinct. Such representations and processes also capture the interaction of internally felt needs and external environmental factors in determining goals and actions by individuals, and therefore much of characteristic behavioral patterns (i.e., personality; Sun and Wilson 2011). Likewise, the notion of “intuition” needs clarification also. Although intuition has often been defined as “the immediate apprehension of an object by the mind without the intervention of any reasoning process” (Oxford English Dictionary), or “immediate apprehension or cognition” (Merriam-Webster Dictionary), we may instead view intuition as a form of reasoning (Sun 1994; Sun and Zhang 2006). In our view, reasoning encompasses both explicit processes (especially explicit rules and logics) on the one hand, and implicit processes (including intuition) on the other (Sun 1994). In fact, intuition and insight resulting from intuition are arguably important elements of human reasoning (Helie and Sun 2010). They supplement and guide explicit reasoning involving, for example, rules and logics. Related to this, previous psychological or AI theories of human problem solving (which relies on reasoning) have largely focused on explicit processes that gradually bring one closer to a solution, step by step, in an explicit, deliberative way. This approach to problem solving and reasoning is often ineffective when the problem is complex, ill-understood, or ambiguous. In such a case, an alternative approach relying more on intuition might be more appropriate. To address this issue, Helie and Sun (2010) proposed a CLARION theory of creative problem solving that centers on the interaction of implicit and explicit processing, relying on intuition and insight (resulting from cumulating intuition), which has been demonstrated through computational simulation of empirical data. Based on the prior work on both intuition and instinct, it may be argued that personality, as studied in social–personality psychology, may be computationally captured and consequently explained based on instinct and intuition, along with other processes. The present summary article will address how instinct and intuition determine personality (i.e., characteristic behavioral patterns). Sociocultural influences are also extremely important to understanding personality. Such influences may manifest in an individual through “tuning” instinct and intuition when the individual interacts with the world, sociocultural or physical, in addition to other possibilities. Therefore, personality may be influenced by sociocultural factors (Forgas et al. 2003). It is necessary to provide a framework capable of accounting for such influences. In particular, it is necessary to account for the influences in a mechanistic, process-based way. An outline of a general framework related to personality may be summarized as follows on the basis of the CLARION cognitive architecture: Within the cognitive architecture, there is the constant interaction among various subsystems. Within the motivational subsystem, there is a set of basic motives, termed drives, which are universal across individuals. Individual differences may be explained (in a large part) by the differences across this set of drives in terms of drive strengths in different situations by different individuals (Winter et al. 1998). These drives, with their different drive strengths, lead to setting of different goals as well as major cognitive parameters by the metacognitive subsystem. Individual differences in terms of drive strengths are consequently reflected in the resulting goals and major cognitive parameters. On the basis of the goals set and the major cognitive parameters chosen, an individual makes action decisions, within the action-centered subsystem. The action decisions may be examined and reasoned, within the non-action-centered subsystem, before the action decisions are finalized. As a result, actions reflect fundamental individual differences as well as situational factors. It is our contention that such a framework (as implemented in the CLARION cognitive architecture, to be detailed later) may capture the relative invariance (such as the Big Five personality dimensions; Clark and Watson 1999; Digman 1990; John and Srivastava 1999) within a specific individual in terms of behavioral propensities and inclinations at different times and with regard to different situations (social or physical), as well as behavioral variability. The (relative) invariance of personality has been extensively argued for in social–personality psychology (e.g., Caprara and Cervone 2000; Epstein 1982; Murray 1938), and with the use of a generic computational cognitive architecture (namely, CLARION), it can be captured computationally. In the remainder of the paper, first, a theory of personality is reviewed. Then, the CLARION cognitive architecture that implements this theory is sketched. Some simulations using CLARION are briefly described. Possibilities for tuning of various processes constituting personality is then discussed. Some concluding remarks end the paper. Note that the present article is a brief summary report of a large research project. Therefore, details will necessarily be sketchy and not all aspects can be covered. In particular, many technical details have to be omitted (although citations of previous publications partially remedy this problem).",9
13.0,1.0,Mind & Society,20 March 2014,https://link.springer.com/article/10.1007/s11299-014-0138-8,Numerals as triggers of System 1 and System 2 in the ‘bat and ball’ problem,June 2014,Antonio Mastrogiorgio,Enrico Petracca,,Male,Male,Unknown,Male,"The dual-system theory of reasoning is a major current field of research into human rationality. This theory distinguishes between automatic/intuitive/fast and reflective/deliberate/slow processes of reasoning (e.g. Pacini and Epstein 1999; for overviews see Osman 2004; Evans 2008; for the debate on this topic see Macchi et al. 2012), often labeled respectively ‘System 1’ and ‘System 2’ (Kahneman 2003, 2011). Individuals sometimes express uncontrolled, impulsive judgments—which may be the source of errors—while other times they adopt a more deliberate and controlled process of reasoning, which is supposed to prevent from committing such errors. The Cognitive Reflection Test (CRT), introduced by Shane Frederick (2005), has been employed to assess individuals’ ability to suppress impulsive (and then potentially wrong) judgments in favor of reflective and deliberate ones. Lots of research has flourished on this topic, whose common denominator lies in the emphasis on the individual side of the phenomenon: it aspires to identify relevant individual variables (IQ, numeracy, risk preferences, etc.)—and then relative differences among individuals—capable of discriminating performances in the CRT. This approach is related to the so-called individual differences approach to cognition and rationality (see Stanovich and West 2000). The emphasis on individual differences, however, may risk to overshadow a fundamental and complementary part of the story. The well-known metaphor of scissors, introduced by Newell and Simon (1972, p. 55), claims that a theory of reasoning must focus not only on the cognition of individuals but also, as a necessary condition, on the task environment (see also Callebaut 2007). As Gigerenzer and Gaissmaier claim, “if one looks only at one blade, cognition, one cannot understand why and when it succeeds or fails” (2011, p. 457). On the basis of such an argument, research on System 1 and System 2 could be re-addressed accordingly, so as to identify also environmental variables that are responsible for selective dual-system dynamics. This paper is precisely devoted to such an inquiry. Our aim is to study the way in which numerals, part of the arithmetical arrangement of mathematical tasks, are related to the dual-system dynamics of activation. In what follows, we will inquire such an argument in the context of the ‘bat and ball’ problem, part of the CRT. To such a purpose we will run an experiment, whose literature connections, hypothesis, methodology and results will be respectively made explicit and discussed in the next sessions. In the last section we will make some broader remarks on the theoretical framework suitable to support our results.",12
13.0,1.0,Mind & Society,27 March 2014,https://link.springer.com/article/10.1007/s11299-014-0140-1,"Slow and fast thinking, historical-cultural psychology and major trends of modern epistemology: unveiling a fundamental convergence",June 2014,Nathalie Bulle,,,Female,Unknown,Unknown,Female,"According to dual-process theories in psychology, human thought processes develop along two distinct and complementary types of processes, one (type 1) fast, effortless, automatic, contextualized and non-conscious, the other (type 2) slow, effortful, controlled, decontextualized and conscious or reflective. These two types of processes are imputed to two different evolutionary lines. The first, which is shared with animals, is evolutionary old, directed towards action (pragmatic) and undemanding of working memory, whereas the second is distinctively human, evolutionary recent, logical and demanding of working memory (Evans 2009). These distinctions do not necessarily support the hypothesis of the existence of two systems of thought. On the contrary, the two types of processes tend to operate jointly. We may, as Frankish (2009) suggests, assume that they refer to two levels of thought provided, we add, that we consider type 2 processes potential impact on type 1 processes, in human development. These processes are linked to two kinds of learning, the first implicit and associative, the other one explicit and rule-based. Finally, the second type of processes is described as supporting human rationality, by controlling cognitive simulations or “mental models” and, more generally, hypothetical reasoning. The hypothesis of the existence of two differentiated paths of development, at the origin of two types of thought processes, lies at the heart of the Russian psychologist, Lev Vygotsky’s theory. According to Vygotsky, higher mental functions are not derived from the distant evolution of elementary processes. They come from a recent evolution of the human psyche. They are the result of an internalization of auxiliary means of thought—i.e. cognitive tools, such as concepts, symbols and numerical system—that are social constructs from the outset. This is Vygotsky’s assumption when he suggests that every function in the child’s cultural development appears twice: first on the social level, and later on the individual level; first, between people (interpsychological), and then inside the child (intrapsychological): higher functions originate as actual relations between human individuals that have been internalized and therefore serve the faculties of reflection (Vygotsky [1930–1933] 1978: 57). For instance, the child begins counting in his head, using his “logical memory” by calling on internal signs; thought proper springs from the “internalization” of language, which is characterized by the transition from the child’s egocentric language to internal dialogue. The individual’s acquisition of socially elaborated tools of mind interacts with his primary cognitive functions while breaking with their development: psychological activity is reconstructed on the basis of sign operations. “Although children’s use of tools during their preverbal period is comparable to that of apes, as soon as speech and the use of signs are incorporated into any action, the action becomes transformed and organized along entirely new lines” (Vygotsky [1930–1933] 1978: 24). In this respect it is impossible to assimilate the role of the work tool, which helps man subject natural forces to his will, with that of the sign, which he uses to act upon himself. The tool is externally oriented whereas the sign is internally oriented. Attempts to equate the sign with the external tool, as it is the case in John Dewey’s works, lose the specificity of each type of activity, artificially reducing them into one (Vygotsky [1930–1933] 1978: 53). Human evolution has therefore led to the emergence of a radically new type of intellectual activity, which is not destined for direct action on the outside world, but is inwardly oriented towards the mind itself. The fact that higher thought processes develop first as external forms of behavior and are mediated by external signs is, as Vygotsky ([1934] 1999: 53) specifies, determined by the specific nature of the higher function which “does not arise as a continuation of elementary processes but is a social method of behavior applied to one’s self.”Footnote 1 In the mind, internalized cultural forms play the role of symbolic stimuli upon which individuals can act by subjecting their own powers to their will: “for higher functions, the central feature is self-generated stimulation, that is, the creation and use of artificial stimuli which become the immediate causes of behavior”—or else, of thought processes. The internalization of external cognitive tools correlatively underpins the development of thought to a higher level of control that corresponds to voluntary action. This internalization—which is, in Vygotsky’s works, at the basis of slow thought processes—is “the distinguishing feature of human psychology, the basis of the qualitative leap from animal to human psychology”. This new evolutionary line, which is not a product of biological evolutionary processes but of mankind’s historical and cultural dimension, is the line that drives human development: “In the process of development, the child not only matures, but also becomes rearmed. Precisely this ‘rearmament’ causes the greatest development and change that we observe in the child as he transforms into a cultural adult” (Vygotsky and Luria [1930] 1993: 168). The internalization of abstract cognitive tools is closely related to the recursive character of human thought. Recursion refers to the possibility of using thought to fit elements into one another in a hierarchical way. In a recent work, the New Zealand psychologist, Michael Corballis (2011), defends that recursion—we only find elementary recursion in some animals—is what distinguishes human psychology from animal psychology. Thanks to the recursive capacities of the mind, the internalization of cognitive tools—with, particularly, the psychological construction of conceptual structures—permits human intellectual capacities to increase tenfold. According to Michael Corballis, recursion has developed from two capacities: mental time travel, which implies being able to insert events into the present consciousness, and theory of mind, which implies being able to put oneself in the place of another person in order to understand him/her. These capacities are only present at an elementary level in some animals. Corballis notes that monkeys respond to transitive acts (linked to objects), but not to intransitive acts (when a movement is mimed). The internalization of intransitive acts on the basis of the sharing of episodic information through mimes could have paved the way toward the understanding of acts that are symbolic rather than linked to objects—the way, therefore, toward applying symbolic acts to oneself. Hominoids were bipeds and this meant their hands were free for communication by gestures. Language could have evolved on the basis of the sharing of episodic information through mimes, which would have become more abstract, more linked to the face, the mouth and to voice control to produce in the end an infinite combination of articulated sounds. The recursive capacities of the mind would have been developed adaptively by natural selection over the last two million years because they underpin first and foremost mental time travel and theory of mind. These capacities would have depended on the development of the working memory’s potential and the potential for hierarchical organization. Note that, for Jean Piaget, the development of general structures of thought is conceived as a continuous process from elementary functions to the extended capacities of deductive thought. Everything takes place as if Piaget was only interested in the general properties of recursion in human thought. These properties are supposed to be based on a natural progress of logical-mathematical skills developed by the subject’s interaction with his/her environment. Piaget gave the name of reflective abstraction to a reconstruction process that allows the integration of an operating structure from a step or previous level into a richer, higher level structure. This sui generis process (following a biological model of development) represented by reflective abstraction would be motivated internally by a movement of equilibrium—that refers to an economic or physical metaphor—which would require the subject to qualitatively raise the level of understanding of his/her instruments of knowledge (Piaget [1967] 1971: 292). This dynamic has been contradicted by the fact that the stages it defines may or may not be respected by children or adults in specific areas according to whether they have acquired adequate conceptual structures or not. Since every concept is in some way linked to others, the total body of concepts acquired during a lifetime influences the acquisition and use of other concepts, and this also explains why most children are unable to engage in general abstract reasoning before the age of eleven or twelve (Novak1977: 122). The development of Man’s recursive faculties is therefore not the product of a child’s natural intellectual maturation based on his/her interaction with the environment but is rather the product of a distinctive, historical and cultural line of development that is linked to the possibility of using auxiliary means of thinking and is dependent on these means. According to Vygotsky, one of the elements proving the existence of a development path that is specific to man is that animals, even the most intelligent, are not capable of developing their intellectual capacities through imitation or learning (Vygotsky [1934] 1986: 188)—and in this respect we have seen that animals cannot internalize cognitive tools that are not linked to objects, as is the case for children. Animals are only capable of learning through training dressage. Children, on the contrary, develop through collaboration and imitation, on the basis of the interiorization of countless cognitive tools, which are the historical products of human culture, particularly structured conceptual systems that are transmitted by formal education. Hence, human development may be characterized by a twofold dynamic: the externalization of memory—by the constitution of a cultural memory—and the recursive internalization by individuals of structuring elements of this cultural memory.",2
13.0,2.0,Mind & Society,17 October 2013,https://link.springer.com/article/10.1007/s11299-013-0133-5,On cognition and cultural evolution,November 2014,Shinji Teraji,,,Male,Unknown,Unknown,Male,"In The Sensory Order (1952), Friedrich A. Hayek provided a theory of the process by which the mind perceives the world around it. The Sensory Order can be regarded as one of the most creative attempts in Hayek’s writings.Footnote 1 Hayek’s The Sensory Order is in no way a direct economic or social analysis, but must be looked at in the context of the cognitive problems related to the conceptualization of institutions. Its publication links together the notions of evolution, spontaneous order, and the limits to explanation when dealing with complex phenomena.Footnote 2 The brain must digest information in a way to simplify data and stimuli that would otherwise be infinitely complex and confusing. The Sensory Order distinguishes between the physical order existing in the external world and our phenomenal experience of it. The essence of Hayek’s attempt in theoretical psychology is to show how a structure can be formed which discriminates between different physical stimuli and generates the sensory order that we actually experience. The sensory order is a system of qualities that do not simply represent physical properties of the external world. The subjectivity of individual knowledge finds its foundation in the construction of the mind. The mind operates by assembling new sensory data into associations with our accumulated inventory of knowledge. Knowledge is forged by the connection of new sensory information to previous sensory experiences. Hayek’s The Sensory Order aims to explain human perception as well as human action as a subjective phenomenon.Footnote 3 Perception is not a mirror of reality; it is always an interpretation of the external world. For Hayek, “psychology must start from stimuli defined in physical terms and proceed to show why and how the senses classify similar physical stimuli sometimes as alike and sometimes as different, and why different physical stimuli will sometimes appear as similar and sometimes as different” (Hayek 1952, pp. 7–8). According to Hayek, knowing the world is a classification of sensory qualities by the mind. “[T]he classification of the stimuli performed by our senses will be based on a system of acquired connections which reproduce, in a partial and imperfect manner, relations existing between the corresponding physical stimuli” (Hayek 1952, p. 145). What we know at any moment about the external world is determined by the order of the apparatus of classification which has been built up by previous sensory linkages. The qualitative differences in perceptions that we experience depend on the specific pattern of neuron firings that a given stimulus produces within various neural networks. The experiences of individuals will differ according to the pattern of neuron firings that each develops. Every perception of external data depends on the subjective characteristics, which in turn depends on original paths of interpretation. Hence, human perceptions emerge according to self-reinforcing processes that are likely to accumulate in a specific path of action. The possibility of further steps along the same path increases with each move down that path. To put it a different way, individuals’ decision processes continue to reproduce one particular choice pattern, not switching to some previously plausible alternatives. Hayek’s cognitive theory explains how different magnitudes of different pieces of cognitive information cause different perceptions and therefore actions. New linkages are established, depending on the pattern of ongoing neural activity. The structure of linkages governs our cognitive processes. For Hayek, the central element in the cognitive process is the feedback between individual and environment. The apparatus by means of which we learn about the external world “is shaped by the conditions prevailing in the environment in which we live, and it represents a kind of generic reproduction of the relations between the elements of this environment which we have experienced in the past; and we interpret any new event in the environment in the light of that experience” (Hayek 1952, p. 165). The classification apparatus provides a semi-permanent ‘map’ of the external world. The map “is formed by connections capable of transmitting impulses from neuron to neuron” (Hayek 1952, p. 115). Operating within the map is a dynamic and changeable ‘model.’ The model is “the pattern of impulses which is traced at any moment within the given network of semi-permanent channels” (Hayek 1952, p. 114). The map is something like a set of implications waiting to happen, and the model pulls out the implications relevant to the current environment from this set. As a person learns, a new interpretation of reality takes over the old one through re-classification. This paper examines two paths by which the work of Hayek has influenced the cognitive theory of institutions: cognition and cultural evolution. The paper argues that there is a relationship between the sensory order and the social order. It is organized as follows. The main aspects of Hayek’s theory of mind are related to the development of rules of conduct and to coordination problems. Section 2 offers a cognitive foundation for rules of conduct. Section 3 focuses on culture from a cognitive viewpoint, and Sect. 4 describes coordination problems. Based on these arguments, the paper discusses cultural evolution as an endogenous process. Section 5 depicts cultural evolution, and Sect. 6 presents a perspective on the relationship between cognition and cultural evolution. As a result, the paper develops an analytical framework for dealing with an interaction between individual’s perceptions of alternative rules and peoples’ action plans according to these rules. Section 7 concludes.",3
13.0,2.0,Mind & Society,01 January 2014,https://link.springer.com/article/10.1007/s11299-013-0135-3,Broader versus closer social interactions in smoking,November 2014,Rosa Duarte,José-Julián Escario,José-Alberto Molina,Female,Unknown,Unknown,Female,"According to the World Health Organization, smoking continues to be the leading global cause of preventable deaths, killing nearly 6 million people each year (WHO 2011). Moreover, this number of fatalities will surely increase if current trends continue. Thus, the same report pointed out that, by 2030, tobacco will kill more than 8 million people, worldwide, each year. Certain authors have claimed that cigarette smoking has become a “pediatric disease” given the high proportion of adolescents who report having smoked (Alexander et al. 2001; Ali and Dwyer 2009). The same claim could be made for Spain (Duarte et al. 2006), where more than 28 % of students aged 14–18 years declared having smoked in the prior month. It is, therefore, easy to understand the great efforts that most countries have made to reduce tobacco consumption in recent decades. At the same time, the majority of studies have estimated and analysed the efficacy of a range of policy measures in order to improve policy decisions. An important topic for policy makers is social interaction or peer effects. Thus, although it is generally claimed that one of the key factors influencing whether adolescents smoke, or not, is the smoking behaviour of their peers, even though empirical evidence for the existence and magnitude of such peer effects in smoking is not conclusive. Some papers report positive and significant peer effects (Ali and Dwyer 2009; Gaviria and Raphael 2001; Clark and Lohéac 2007; McVicar 2011), but others argue that the peer effects of smoking are much weaker than found in previous studies (Krauth 2007; Duarte et al. 2013) or even insignificant (Soetevent and Kooreman 2007). The literature on peer effects, or social interactions, has grown significantly in recent years. A relatively recent review of theoretical work can be found in (Scheinkman 2008). An important landmark in this area is the work of (Manski 1993), who distinguishes three types of effects: endogenous, exogenous or contextual, and correlated effects. The endogenous effect, also known as peer effect, appears when the propensity to participate in a behaviour depends on the prevalence of this behaviour in the group. The contextual effects appear when the propensity to participate in a behaviour varies with the exogenous characteristics of the group. Endogenous and contextual effects are also known as social effects. Third, the correlated effects emerge because individuals in the same group tend to have similar behaviours, sharing similar characteristics or institutional environments. Few studies have examined simultaneously more than one measure of social interactions from the same data set (Holliday et al. 2010), although there are some notable exceptions (Holliday et al. 2010; De Vries et al. 2006). The purpose of this paper is to deepen the empirical analysis of peer effects on smoking by considering the simultaneous use of two alternative measures of peer influence. One is defined at the class level and the other takes into account the smoking behaviour of the group of friends. The contribution of this study to the empirical literature on smoking peer effects consists in helping to determine the relevant group in which to estimate peer effects, and we suggest that, once we have controlled for closer peer effects, there is no gain in controlling for class-based peer effects.",1
13.0,2.0,Mind & Society,22 April 2014,https://link.springer.com/article/10.1007/s11299-014-0144-x,What underlies the Great Gatsby Curve? Psychological micro-foundations of the “vicious circle” of poverty,November 2014,Arthur Sakamoto,Jason Rarick,Sharron X. Wang,Male,Male,Female,Mix,,
13.0,2.0,Mind & Society,15 October 2014,https://link.springer.com/article/10.1007/s11299-014-0157-5,"Symposium on “Fear, economic behavior and public policies” - Part I",November 2014,Mario A. Cedrini,Marco Novarese,Robin Pope,Male,Male,,Mix,,
13.0,2.0,Mind & Society,28 August 2014,https://link.springer.com/article/10.1007/s11299-014-0152-x,Fear of principles? A cautious defense of the Precautionary Principle,November 2014,Gloria Origgi,,,Female,Unknown,Unknown,Female,"The PP is the more and more referred to in debates relating to environmental and health risks. It appeared for the first time in public debates around ecological issues in Germany in the Sixties and was rapidly adopted by ecologists especially in northern European countries. It began to be alluded to, at least implicitly, in international declarations such as the Stockholm declaration of the United Nations Environment Program (UNEP) in 1972.Footnote 2 In 1982, The World Charter of Nature, sponsored by 34 “developing” countries was adopted by the United Nations. An open reference to the PP is made for the first time in this text at a global level. It is interesting to read through the text, because it shows very clearly the new philosophy of nature that underlies the endorsement of the principle. The charter was modeled on the UN declaration of Human Rights and structured in five principles and a series of recommendations. The preamble to the principles states that the General Assembly, aware that: “(a) Mankind is a part of nature and life depends on the uninterrupted functioning of natural systems which ensure the supply of energy and nutrients, and (b) Civilization is rooted in nature, which has shaped human culture and influenced all artistic and scientific achievement, and living in harmony with nature gives man the best opportunities for the development of his creativity, and for rest and recreation” and convinced that: “(a) Every form of life is unique, warranting respect regardless of its worth to man, and, to accord other organisms such recognition, man must be guided by a moral code of action, and (b) Man can alter nature and exhaust natural resources by his action or its consequences and, therefore, must fully recognize the urgency of maintaining the stability and quality of nature and of conserving natural resources”, declares that: 1. Nature shall be respected and its essential processes shall not be impaired.
 In 1987, the Montreal Protocol on Substances that Deplete the Ozone Layer, made the first explicit reference to a “precautionary approach” to the problem of the ozone layer. The PP took its first globally accepted definition in 1992, at the Rio Earth Summit, on of the major United Nations conferences on environmental issues, whose outcome was a Declaration on Environment and Development structured around 27 principles. The 15th principle is the following:",3
13.0,2.0,Mind & Society,06 August 2014,https://link.springer.com/article/10.1007/s11299-014-0151-y,"Emotion, utility maximization, and ecological rationality",November 2014,Yakir Levin,Itzhak Aharon,,Male,Male,Unknown,Male,"The aim of this paper is to examine the adequacy of an evolutionary-oriented notion of rationality—ecological rationality—that has recently been proposed in economics. Like the notion of rationality prevalent in economics (economic rationality), ecological rationality is concerned with what it is rational to do, or intend to do. In this sense, both notions are versions of what philosophers call ‘practical rationality’ (to be further explicated later on). Indeed, the question of the adequacy of ecological rationality as it is understood in the paper, is the question of whether ecological rationality is a genuine notion of practical rationality. Economic rationality and ecological rationality also resemble each other in taking rationality to be a matter of utility maximization. But the two notions differ in their respective conceptions of both utility and maximization. While our discussion will focus on the economic version of ecological rationality, our conclusions will apply just as well to other, related versions of the ecological notion that have been proposed in biology and evolutionary psychology. The structure of the paper is as follows: In Sect. 2 we explicate the notion of practical rationality by contrasting it with the notion of theoretical rationality, and by illustrating both the similarities and differences between these two notions as well as the variety of forms of practical rationality. This we do by outlining a central notion of practical rationality, and by showing how this notion may suggest both more specific and alternative notions. In Sect. 3 we outline an evolutionary approach to the emotions illustrating it mainly by the example of fear. On this basis we motivate and outline the economic version of ecological rationality: We show, first, why and how this version may be applied to the emotions, and then explain the reasons adduced in economics for considering it to be of a much wider scope of application. In Sect. 4 we explicate the notion of intentional agency cum action by contrasting it with the notion of non-intentional behavior, and by outlining its close connections with the notion of practical rationality. This discussion implies that a specific conception of practical rationality, which is exemplified by the central notion outlined in Sect. 2, is the most basic or primary. Relatedly, it grounds the claim that normativity is a constitutive feature of practical rationality. In Sect. 5 we argue that a key evolutionary notion—viz., fitness—in terms of which ecological rationality is defined, is not a normative notion. Based on Sects. 4 and 5, we conclude the paper by arguing in Sect. 6 for the inadequacy of the ecological notion of rationality: By Sect. 4, Any notion of practical rationality worthy of the name must be normative. But by Sect. 5, The notion of ecological rationality is not normative. Hence, the upshot of Sect. 6 that The ecological notion of rationality is not a genuine notion of practical rationality. 
In making this argument, we also highlight important similarities and dissimilarities between the fundamental notion of practical rationality outlined in Sect. 4, ecological rationality, and economic rationality.",5
13.0,2.0,Mind & Society,17 August 2014,https://link.springer.com/article/10.1007/s11299-014-0153-9,Modern institutions between trust and fear: elements for an interpretation of legitimation through expertise,November 2014,Sandro Busso,,,Male,Unknown,Unknown,Male,"Despite its relevance in contemporary public discourse, for many years fear has been understudied in the field of social sciences. Although often examined in relation to specific issues such as crime, the topic of fear has rarely been explored as a social phenomenon by itself, and thus remained relatively under-theorised for many years (Hankiss 2001; Tudor 2003; Furedi 2006). Nevertheless, important steps have been taken in order to consider fear not only as an individual, physical or evolutionary reaction, but as a social fact, and to distinguish it from the idea of risk. Considering fear as an autonomous phenomenon implies a shift from a strict focus on fear as a consequence of social dynamics or individual relationships, to the social consequences of fear. In this view: “Fear itself is a risk and must be included in risk-management policymaking” (Gray and Ropeik 2002, 114). Thus, sociology of fear mainly debates two dimensions: the first revolves around the way culture and social structure influence the experience of fear, whereas the second focuses on the consequences of fear on individual behaviour and social structure. As for the first dimension of the debate, scholars have underlined the role of culture and social structure in defining our fears, suggesting that the who, what and even how we fear is somehow socially constructed.Footnote 1 As suggested by Furedi (2006), our reactions to specific circumstances are mediated by cultural norms that suggest to people what kind of reaction is expected from them. Moreover, a number of studies (Altheide 2002, 2013; Glassner 1999) underlined the role of mass media in creating what Massumi defined a “mass production line of fear” (1993, VIII). Emotional status affecting a society can sometimes be so enduring as to become institutionalised models of social action: the so-called “emotional climates” (Tudor 2003). Such climates, according to Barbalet, are “significant in the formation and maintenance of political and social identities” and “differentiate social groups or categories by virtue of the fact that they are shared by their members and unlikely to be shared with non-members” (2001, 159). The role played by emotions in generating belonging and in defining ingroups and outgroups introduces the second dimension of the social nature of fear: the way in which fear affects social structure and individual behaviour. Considerations on emotional climates focus on macro effects of fear in society, but emotional climates also affect the micro level significantly. Both in contemporary and ancient societies, fear has always been used as an instrument of social control, either with repressive or pedagogical purposes, thus it also plays a pivotal role in interactions between individuals. In a game theory perspective, most dilemmas of social life are shaped by “fear of non cooperation” that determines the choice between cooperation or defection (Kuwabara 2005, 1258). However, it is important to underline that fear does not affect social behaviours in a particular way, since it can both cause or dampen action (Goodwin and Jasper 2006). Therefore, in a broader perspective, it can be seen as an engine to promote social, political or economic change (Barbalet 2001) or as an instrument to maintain the status quo (Federico and Deason 2012). Nevertheless, as the paper discusses, some aspects of the contemporary culture of fear create conditions that seem to be associated with the second option more strongly. The sociological approach to fear also contributes to the understanding of how fear has changed in contemporary highly mediated societies. Indeed, as an effect of the increasing level of mediation, fear is less and less a “first hand” experience but an abstract, general or discursive one (Furedi 2011). This trend can be linked to the evolution of risk towards a global dimension (Beck 1992), which is strictly connected to the modernisation process. If in earlier periods risks “assaulted the nose or the eyes, and were thus perceptible to the senses”, with modernisation they “escape perception”, as are “localized in the sphere of physical and chemical formulas” (ivi, 21). Fear and risk evolve in parallel towards an abstract and generalised level. However, such an evolution activated a process of autonomisation of fear from risk. Mediated fear no longer needs defined risks to be experienced or real danger to exist, or at least, there is little correspondence between real threats and their perception by public opinion (Altheide 2013; Pain 2012; Glassner 1999). In addition to its independence from actual threats, fear gained a pervasive nature as different scholars underline. As suggested by Altheide: “fear has become a dominant public perspective. Fear begins with things we fear, but over time, with enough repetition and expanded use, it becomes a way of looking at life. […] Fear is one of the few perspectives that citizens share today” (2002, 3). Furedi (2006) and Glassner (1999) focus on the “culture of fear” pervading contemporary societies, and Hubbard (2003) talks about fear that saturates spaces of everyday life. Massumi introduces the concept of “low-level fear”, as “a kind of background radiation saturating existence” (1993, 24). More recently, Svendsen (2008, 12) described fear as “the emotion that controls the public, […] a magnifying glass through which we consider the world”. Therefore, fear is no longer “fear of something” but it is a sort of generalised feeling attached to different issues. Quoting Furedi: “Today fear has an unpredictable and free-floating character” (2005, 4). Free-floating fear may be described as a sort of prediction of terrible and unforeseen events, and this distance from actual threats enhances its power since “fear is at its most fearful when it is diffuse, scattered, unclear, unattached, unanchored, free floating, with no clear address or cause; when it hunts us with no visible rhyme or reason, when the menace we should be afraid of can be glimpsed everywhere but it’s nowhere to be seen” (Bauman 2006, 2). As a consequence of this emerging scenario, fear nowadays has become a sort of perspective or frame—in Goffman’s (1974) terms—through which reality is interpreted. It became “a framework for developing identities and for engaging in social life” (Altheide 2002, 3). Moreover, in contemporary societies it grew into one of the major forces shaping, for example, political activitiesFootnote 2 (Onuf 2012) and the search for consensus, and it is mirrored in a number of areas of social organisation, such as space and the structure of our cities (Body-Gendrot 2012). Effects of fear transcend individual behaviour to be experienced by society as a whole. Even if contemporary fear has a clear and distinctive macro component, the feeling of anxiety and fright is often experienced at a very private level. Consistently with the individualization process that came with modernization, fears are rarely shared and faced collectively. This process of “privatisation” (Furedi 2011) of fear turns out to be one of its most paralysing features, responsible for the sense of impotence and for the feeling of the uselessness of reacting. In the transition from pre-modern or pre-industrial fear to contemporary fear, the micro–macro relation is reversed. In the past, micro determinants (personal experiences) provoked fears that were managed at the macro (community) level. Nowadays, the cultural and discursive (macro) dimension of fear creates the conditions for a private and individual (micro) experience of fear.",2
13.0,2.0,Mind & Society,07 October 2014,https://link.springer.com/article/10.1007/s11299-014-0154-8,Adaptation patterns and consumer behavior as a dependency on terror,November 2014,Aviad Tur-Sinai,,,,Unknown,Unknown,Mix,,
13.0,2.0,Mind & Society,05 August 2014,https://link.springer.com/article/10.1007/s11299-014-0150-z,The role of anxiety and anger traits in financial field,November 2014,Elisa Gambetti,Fiorella Giusberti,,Female,Female,Unknown,Female,"Buying a home is probably the biggest investment decision in the life of an individual. Because most buyers do not have the cash to pay the purchase price upfront, they are obliged to take out housing loans. Banks offer a wide range of products to suit borrowers’ varied cash flow and risk preferences. In this particular decision the buyers know the outcomes. However, they do not know how the consequences of outcomes will turn out. The anticipation of the consequences depends on an individual’s propensity to imagine and experience pleasure and/or pain in the future (Berns et al. 2007). It is well known that choices appear to be guided by individual attributes, such as personality traits, emotions, motivations, evaluations and attitudes more than by economic reasons based on gain maximization (e.g., Dahlbäck 1990; Kahneman and Tversky 2000). Studies about the influence that individual differences have on consumer behavior have increased a lot in the last 15 years and it is now the moment to test in real domains of human life the quality of these results (Gibson 2006). In this respect the housing market appears as one of the fields in which the impact of personality traits should be more evident because of the high uncertainty and consequences that this kind of decisions may have on people’s lives. This research tries to fill a void in the literature evaluating the impact of anger and anxiety traits on consumer behavior, with a special focus on housing loan perceptions and preferences. A variety of studies have attempted to explore the influence of personality risk-taking traits that are stable across situations. For example, Lauriola and Levin (2001) found that personality, assessed by the most common questionnaire, the Big Five, predicted risk-taking primarily in the domain of gains, where high scores on Openness to Experience, characterized by originality and curiosity, were associated with greater risk-taking and high scores on Neuroticism, characterized by being easily upset, were associated with less risk-taking. In the domain of losses, instead, high scores on Neuroticism were associated with greater risk-taking. The concept of risk-taking has been operationalized in different ways.Weber and Johnson (2008) distinguished measures of risk attitude into three categories: the first is constituted by behavioral measures, where an individual’s risk preferences are assessed from actual choices made in games or scenarios, both real and hypothetical, such as Balloon Analog Risk Task (Lejuez et al. 2002), Columbia Card Task (Figner et al. 2009), Cups Task (Levin and Hart 2003), and Iowa Gambling Task (Bechara et al. 1994); the second includes self-report questionnaires, such as the Choice Dilemmas Questionnaire (Kogan and Wallach 1964), Risk-taking Propensity (Jackson et al. 1972), Domain Specific Risk Task (Weber et al. 2002), which directly question an individual about risky situations or perceptions of risks and benefits in order to infer his/her preferred levels of risk; the third assess individuals’ self-reports of personality traits, such as impulsivity (e.g., Eysenck and Eysenck 1978), or motivations (e.g., Need for Arousal; Figner et al. 2009), related to risk-taking and aversion. In the finance literature, measures of risk attitude generally assess decision makers’ preferred levels of risk. In economics, studies that evaluated the relationship between personality traits and financial risk-taking are relatively fewer. For example, Carducci and Wong (1998) found that persons with a Type A personality are more willing to take higher levels of risk in all financial matters, though this may be correlated to Type A persons tending to have higher levels of income than Type B ones (Thoresen and Low 1990). There is also evidence of risk-taking tendencies for high sensation-seeker in terms of their everyday financial matters (Wong and Carducci 1991). Research also found that trait anger and trait anxiety appear to predispose, respectively, to risky and to risk-avoidant decisions (Gambetti and Giusberti 2009; Maner et al. 2007; Mitte 2007). There are specific reasons to think that individual differences in the tendency to feel anger and anxiety have opposite effects on economic decision-making. First, anger is associated with the perception of certainty about what happened and of ability to control and foresee situations that will have important consequences for decisional processes (e.g., Lerner and Keltner 2000). Whereas high levels of dispositional self-control have been found to be negatively correlated with trait anger and with outwardly directed aggression (Tangney et al. 2004), trait anxiety is associated with the perception of uncertainty, unpleasantness and low situational control (e.g., Wilt et al. 2011). Another reason is that anger makes people indiscriminately optimistic about their own chances of success and it activates optimistic beliefs about experiencing future life events (e.g., Fischhoff et al. 2005; Lerner and Tiedens 2006). On the contrary, anxiety promotes pessimistic appraisals of future events (e.g., Shepperd et al. 2005) and outcomes across a range of behavioral contexts (e.g., Maner and Schmidt 2006). In general, according to the cognitive approach (Beck 1999; Deffenbacher et al. 2000), the assessment of the environment, activated by state or dispositional emotions, has an important influence on decision-making: it has informational, motivational and processing functions and so specific impacts on outcome effects (e.g., Peters et al. 2006). In the specific case, unlike anxiety, which typically signals the presence of potential threats and promotes psychological and behavioural responses that help individuals to reduce their perception of vulnerability (e.g., Butler and Mathews 1987), anger tends to promote approach tendencies in the form of attack (e.g., Harmon-Jones and Allen 1998; Roseman et al. 1994). In fact, whereas the potential goals of highly anxious individuals appear to be reducing uncertainty (e.g., Bensi and Giusberti 2007) and protecting themselves from potential threats (Maner and Schmidt 2006), the functional objective of anger entails the taking of immediate risks to reduce or deter future assaults (e.g., Quigley and Tedeschi 1996; Lerner and Tiedens 2006). It must be emphasized that anxiety and anger feelings could be linked respectively with the behavioral inhibition system, resulting in a higher risk perception, and the behavioral activation system, as determinant of risk-taking (Carver and White 1994). Although these two systems could certainly be underlying the perception and decision-making processes, in this study we investigated trait anxiety, that is a dispositional way to react with feelings of stress, worry and discomfort across time and situations (Spielberger and Sydeman 1994), and trait anger, that is a general temperament of low threshold reactivity in which anger is experienced in response to several innocuous or not triggers (Spielberger 1999). Trait anger is different from impulsivity because in the first case the individual expresses his/her angry feelings inwardly or outwardly in a destructive or non-destructive manner, whereas in the second case the person tends to act on a whim, displaying behavior characterized by little or no forethought, reflection or consideration of the consequence and often in a dysfunctional manner (VandenBos 2007). Anger and anxiety traits can be considered “common currencies” that can be used to evaluate alternative courses of action. This suggests that they have an important role in decision making and risk taking processes. However, the impact of individual differences in anger or anxiety-proneness on real life consumer choices has till now been noticeably understudied, even though they are particularly noteworthy: they may be unconscious and unrelated to the decision at hand, nonetheless they can have the potential to influence the perception of mortgage products, and the consequent preferences, in important ways. The main purpose of the current study was to evaluate the effect of anger and anxiety traits on housing loan preferences and perceptions. We performed an experiment in which participants were confronted with their real mortgage preferences, checking for demographic characteristics, experience in economic/financial topics and amount of savings. We supposed that trait anger would predict risky preferences, that is a preference for adjustable-rate mortgages (the risk is determined by the volatility of interest rates and the consequent difficulty to accurately forecast future direction). On the other hand, trait anxiety would predict conservative choices, that is a preference for fixed-rate mortgages. In addition, we sought to evaluate the individual cognitive assessments, that is risk perception and housing loans predictability. The first refers to the likelihood of paying higher interests. The second factor measures the sense of control regarding the possibility to forecast the variation in time of the total cost. On the basis of previous research (e.g., Lerner and Keltner 2000; Wilt et al. 2011), we expected that trait anger would activate a low risk perception and high predictability of housing loans, whereas trait anxiety would activate a perception of high risks and low predictability of housing loans.",6
13.0,2.0,Mind & Society,27 June 2014,https://link.springer.com/article/10.1007/s11299-014-0147-7,Behavioral and emotional responses to escalating terrorism threat,November 2014,Anja S. Göritz,David J. Weiss,,Female,Male,Unknown,Mix,,
14.0,1.0,Mind & Society,19 September 2014,https://link.springer.com/article/10.1007/s11299-014-0155-7,The natural frequency hypothesis and evolutionary arguments,June 2015,Yuichi Amitani,,,Male,Unknown,Unknown,Male,"Over the past few decades, psychologists and philosophers have discussed a variety of experiments, which show that ordinary—and even well-educated—people frequently commit basic logical or probabilistic fallacies. Some psychologists (e.g., Tversky and Kahneman 1982) have interpreted these results as showing that human thinking generally does not comply with logical and probabilistic laws, and is therefore irrational. For example, numerous experiments have shown that subjects frequently make mistakes on Bayesian inference quizzes.Footnote 1 Psychologists and philosophers have typically taken this to mean that human reasoning generally fails to conform to the probability calculus: when human beings calculate probabilities, they frequently violate basic mathematical results, such as Bayes’ theorem. Gerd Gigerenzer argues that the conclusion of psychologists like Kahneman and Tversky is based on a limited, faulty view of rationality. Specifically, Gigerenzer claims that these experimental results are still consistent with the ecological rationality—the inferential and mathematical abilities taking advantage of and being tuned to the information structure in the environment—that humans have developed during the course of evolutionary history. Gigerenzer and his research group attribute the “errors” revealed by these studies to experimental conditions, which make it difficult for human beings to exert their intellectual abilities properly—conditions very different from the sort of environmental contexts in which those abilities were acquired. Thus, it is no wonder that subjects tend to perform poorly in these experiments. When experimenters do provide settings which have similar informational structure to the one in the environment where early humans evolved their intellectual abilities, then they perform quite well: they comply with the laws of logic and probability. Regarding probabilistic reasoning, Gigerenzer argues the problem stems from the way probabilities are presented in most experimental studies, namely, whether they are presented in percentages (such as ‘20 %’) or natural frequencies (‘1 success out of 5 hunting attempts’). Although probabilities have been presented in percentages in most experiments, Gigerenzer says presenting probabilities in the natural frequency formatFootnote 2 is more appropriate, and facilitates reasoning about those probabilities, because humans’ mental mechanisms for probability calculus are more tuned to or designed for natural frequencies and, in fact, that this constitutes a genuine evolutionary adaptation of our species. His hypothesis is that frequency format is the informational structure that our intellectual abilities involving probability are specific for, and they have an information-processing psychological mechanism tuned to or designed for this form of information structure [this is called the natural frequency hypothesis; see for example, Gigerenzer (1991, 1993, 2000), Gigerenzer (2008), Gigerenzer and Hoffrage (1995), Hoffrage and Gigerenzer (1998), Cosmides and Tooby (1996)]. Given this hypothesis, Gigerenzer and his colleagues predicted that if probability tests—such as the Bayesian inference quizzes—were presented in a natural frequency format, subjects would perform quite well on them. This prediction has been confirmed in multiple experiments (Gigerenzer and Hoffrage 1995; Gigerenzer 1991, 1993, 1998, 2000; Hoffrage and Gigerenzer 1998; Hoffrage et al. 2002; Cosmides and Tooby 1996). Naturally, Gigerenzer’s work on probabilistic reasoning has generated much controversy and discussion in recent years, and there is a developed literature to attest to that [for example, see Kahneman and Tversky (1996), Vranas (2000, 2001), Stanovich and West (2000, 2003), Over (2000a, b, 2003), Girotto and Gonzalez (2001, 2002), Johnson-Laird et al. (1999), Lewis and Keren (1999), Barbey and Sloman (2007); Neace et al. (2008), Polonioli (2012). For replies from the supporters of the natural frequency hypothesis, see for example Gigerenzer (2001), Hoffrage et al. (2002), Brase (2008, 2009), Gigerenzer and Hoffrage (2007). For possible philosophical implications of this on the so-called rationality debate, see Samuels et al. (2001, 2004)]. Despite this, few have examined his adaptationist hypothesis that possessing probability-reasoning mechanisms, specific for the frequency representation, is common among human beings today because it was more adaptive in the course of evolution than the percentage representation, and was consequently selected for. This neglect is unfortunate, for although the evolutionary appeal of his overall hypothesis is more tentative and somewhat independent of his experimental results (Gigerenzer and Hoffrage 2007; Gigerenzer 2008), Gigerenzer has, nevertheless, maintained it for a decade (Gigerenzer 1993; Hoffrage et al. 2002). Furthermore, leading evolutionary psychologists, such as Cosmides and Tooby (1996), also support the hypothesis. Still others also consider this hypothesis a serious idea worth pursuing. For example, Pinker (1997) says “the mind may have evolved to think of probabilities as relative frequencies in the long run, not as numbers expressing confidence in single event. [...] Our ancestors’ usable probabilities must have come from their own experience, and that means they were frequencies [...]” (p. 347).Footnote 3
 This paper is a critical and conceptual examination of evolutionary proposal put forward by the supporters of the natural frequency hypothesis. I shall argue that although the frequency format does have evolutionary advantages over the percentage format in some respects (e.g., sample size information), those advantages come at some cost in other respects (e.g., greater burden on memory). Then I will suggest some plausible alternative explanations (including the nested-sets hypothesis; Sloman et al. 2003) for the improved test performances of experimental subjects under the frequency format. Even if they may not explain away the natural frequency hypothesis, these alternative accounts undermine the need to postulate mental mechanisms for probabilistic reasoning tuned to the frequency format, and they at least demonstrate evolutionary arguments put forward by the supporters of the natural frequency hypothesis have more problems than have been recognized. Though I’m not the only one to support the nested-sets hypothesis, I will be replying to Gigerenzer’s and his colleagues’ recent responses to this criticism, which no or few supporters of the hypothesis have responded so far.",1
14.0,1.0,Mind & Society,27 September 2014,https://link.springer.com/article/10.1007/s11299-014-0156-6,Conscious belief as constructed memory: an empirical challenge to dispositionalism,June 2015,Vishnu Sridharan,,,Unknown,Unknown,Unknown,Unknown,,
14.0,1.0,Mind & Society,19 November 2014,https://link.springer.com/article/10.1007/s11299-014-0158-4,An explanatory coherence model of decision making in ill-structured problems,June 2015,M. Laura Frigotto,Alessandro Rossi,,Unknown,Male,Unknown,Male,"In changing and unfamiliar environments, strategy makers face unexpected competitive threats and work in business contexts that vary suddenly and often unpredictably. A rising number of decision-making problems concerns situations that are novel and difficult to interpret: strategic alternatives are blurred, causal relationships linking various events are not apparent, and the chain of consequences triggered by adopting a particular decision are largely unforeseeable. In decision theory, these situations signify ill-structured problems, (Simon 1973; Hayes and Simon 1974), i.e., problems in which alternatives, states of the world, and consequences are not given and their structure is the result of creative effort (Newell et al. 1958). In ill-structured problems, representations are continuously defined and redefined in an indefinite space (Hayes and Simon 1974; Ungson et al. 1981), and they require the decision maker to elaborate assumptions or theories that offer a structure to the problem such that it enables decision making. In approaching an ill-structured problem, decision makers may highlight different alternatives and propose different solutions, each with particular strengths and weaknesses. They address alternative points of view and create arguments justifying proposed solutions. While the decision on a well-structured problem is a right or wrong answer that derives from a given problem structure, i.e. an interpretation of the problem that is self-evident, the decision taken on an ill-structured problem requires the creation of a problem structure. We interpret this as deriving from the creation of a coherent interpretation of the problem context and we model decision as deriving from the most coherent interpretation of it. Even if expected utility theory (EUT) is still considered the reference model in economic theorizing (Thagard and Millgram 1997), decades of behavioral studies on economic choice highlighted several limitation of the EUT in explaining decisions. EUT is not suitable for ill-structured problems, in particular, because in such cases the problem components are incomplete and indefinite. Decision models such as case-based decision theory (CBDT) have been proposed for quasi well-structured problems, where the problem space of the problem cannot be presented as a complete set of alternatives, consequences and goals, but it can partially be built by drawing analogies with past similar problem cases. However, the extant literature has not proposed a model to address cases of true ill-structured problems i.e., novel problems where components cannot be reconstructed even by analogy. Such cases require radically different assumptions in order to be understood (Ungson et al. 1981) and they would also require radically different models. In this paper, we propose a model of decision making for novel ill-structured problems that builds on explanation-based coherence theories (Thagard 1989; Thagard and Verbeurgt 1998). Existing coherence models of decision making have focused on the case of deliberative coherence, where a decision is viewed in terms of a complex action plan which results from the assessment of competing actions and goals according to some coherence principle (Thagard and Millgram 1997; Millgram and Thagard 1996). This class of models particularly fits instances of decision making characterized by competing goals and the presence of a plurality of available actions viewed as building blocks of action plans. Conversely, we build on a similar, albeit distinct, class of coherence models (explanatory coherence) to address other instances of decision making characterized by a truly novel and ill-structured nature, such as choices concerning the investment in radically new technologies or the launch of breakthrough products. We explore the dimensions of the creative effort through which an ill-structured problem is given a structure. We represent the structure of such problem representations building on established psychological literature and the connectionist tradition on explanatory coherence. To illustrate the model, we consider a choice concerning investment in a new technology, and following the same approach adopted by Millgram and Thagard (1996), we use the model to discuss how changes in the structure of alternatives may impact on the final decision and how critical the single components are for the final decision. The remainder of the paper is organized as follows. In the next section we frame novel problems as ill-structured problems and we discuss the limits of EUT vis-à-vis modeling such problems. Through examples, we redraw the fit between well-structured problems and EUT, and between routine problems and quasi ill-structured problems with CBDT; we consider that despite their relevance, a model for ill-structured problem is absent. In section three we argue how our model contributes to the extant literature on decision making. In section four, we discuss the theoretical foundations of the model rooted in psychological and cognitive literature and describe the modeling of decision alternatives for ill-structured problems in novel settings. Moreover, we position our model within the constraint satisfaction tradition and provide a detailed and formal description of it. In section five we illustrate how changes in the structuring of alternatives may impact on decision making. We close with two sections, the discussion and conclusions respectively, in which we derive implications for the structuring of decision alternatives in novel problems and also propose some directions for future research.",8
14.0,1.0,Mind & Society,03 December 2014,https://link.springer.com/article/10.1007/s11299-014-0159-3,On both sides of the fence: perceptions of collective narratives and identity strategies among Palestinians in Israel and in the West Bank,June 2015,Adi Mana,Shifra Sagy,Serene Mjally-Knani,Male,Female,Unknown,Mix,,
14.0,1.0,Mind & Society,30 January 2015,https://link.springer.com/article/10.1007/s11299-015-0162-3,Excellence examined,June 2015,Nicholas Rescher,,,Male,Unknown,Unknown,Male,"Excellence is a key concept in quality assessment, standing at the extreme position in placement along a scale of more-or-less comparison ranging from the extremely negative (inferior) to ever-increasing superiority. The excellent is that which stands at or near the positive pole of such a grading schema as an index of superlative positivity. Quality assessment accordingly runs along a comparison scale of the format  which we shall here characterize in a quality order ranking from 1 to 5, with the excellent thus designated as first-class. Excellence is a matter of excelling other comparable items: the idea is only operative in contexts where a qualitative comparison of some sort—better/worse, inferior/superior, lesser/greater, lower/higher or the like—comes into play. In some circumstances the concept can have a negative application—say in the case of an accomplished liar or a skilled burglar. But even here it is the specifically positive aspect of skill and talent that stands in the foreground. To be excellent is to excel, and excelling is always a matter of respect: for one item to excel another is to exhibit this condition in this or that particular way, that is to achieve a qualitative superiority in one or another specific regard. There are, accordingly, many different ways of excelling and excellence can be achieved in many different modes: power (“your excellency”) wealth (“rich as Croesus”) performance (in music, sports, mathematics, etc.) social standing (“the top 100”) workmanship (“a masterpiece”) beauty (“Mirror, Mirror, on the Wall”) etc. Consider a taxonomic chain that cascades down from generality to particularity as per the following example: Each step along such a chain introduces certain definite features that something of that sort has or does. In just this basis will the idea of excellence—of doing one’s particular kind of thing well (effectively and efficiently—come into play with increasing prominence. Increasingly specific functionality makes all the difference: at the start excellence is scarcely there at all, at the end it is increasingly present. The idea of an excellent dog is problematic, but there is little difficulty with an excellent show dog or hunting dog or guard dog, etc. The things characterized as excellent will be so sui generis. Excellence, like quality assessment in general, always envisions a limited comparison range denominated by a duly subordinated taxonomic category. For there is no such thing as an excellent item or an excellent action; unlike the situation of an excellent hammer or an excellent musical performance. An item cannot just be excellent: it has to be an excellent something. Excellence cannot be detached from taxonomy. The only meaningful sort of excellence is that bound up with the proviso: of its kind. And this taxonomic aspect of excellence means that the conception can proliferate aspectivally. However, excellence is a highly syncategorematic concept: it cuts across the usual boundaries and comes into operation across a virtually endless range of different sorts of things. And there is effectively nothing in the way of descriptive properties that different kinds of excellent things—an excellent mathematical proofs, for example, and an excellent dachshund—need to have in common. Excellence is an evaluative rather than descriptive feature. And it can be exhibited across a wider or narrower range. One can be an excellent athlete or an excellent linguist, and there can be excellent roses and excellent hammers. To be sure, there are realms in which there is no room or place for quality assessment. Numbers, for example—one integer is not better (or worse) than another. Accordingly there simply are no excellent integers—one can be larger than another, but not better that it. Of course, this does not mean that there is on quality assessment in mathematics at all. This would assuredly be false, for clearly one proof of the thesis can be more elegant, direct, economical, accessible than another. There are no excellent integers, but there are certainly excellent proofs.",2
14.0,1.0,Mind & Society,22 April 2015,https://link.springer.com/article/10.1007/s11299-015-0166-z,The challenge of fear to economics,June 2015,Mario A. Cedrini,Marco Novarese,,Male,Male,Unknown,Male,,
14.0,1.0,Mind & Society,08 April 2015,https://link.springer.com/article/10.1007/s11299-015-0165-0,"Attention deficit hyperactivity disorders, panic attacks, epileptic fits, depressions and dementias from missing out on appropriate fears and hopes",June 2015,Robin Pope,,,,Unknown,Unknown,Mix,,
14.0,1.0,Mind & Society,14 July 2014,https://link.springer.com/article/10.1007/s11299-014-0148-6,Fearing fear: gender and economic discourse,June 2015,Julie A. Nelson,,,Female,Unknown,Unknown,Female,"Economic discourse—or the lack of it—about fear is gendered on at least three fronts. First, while masculine-associated notions of reason, mind, choice, control, and mechanism have historically been prioritized in mainstream economics, fear—along with other emotions, embodiment, vulnerability, and lived experience—has tended to be feminine-associated. Contemporary psychological and neuroscientific research on cognitive “gender schema,” then, may at least partly explain the near absence of discussions of fear within economic research. Second, in the extremely rare cases where fear and emotion are alluded to within the contemporary economics literature on risk aversion, there is a tendency to (overly-)strongly associate them with women. Finally, dominant Western cultural metaphors not only associate fear with femininity, but also with inferiority and a lack of control. Historians and philosophers of science have suggested that failures to acknowledge the full range of human emotions and experience may be themselves be rooted in fear: a fear of the feminine. This essay discusses these three gendered aspects of the relationship between the economics discipline and fear, and concludes with a brief discussion of the problems created. Economists’ aversion to discussing fear—especially fear as experienced by men—seriously hampers the discipline’s ability to generate useful knowledge in the face of financial market instability and ecological threats.",4
14.0,1.0,Mind & Society,30 September 2014,https://link.springer.com/article/10.1007/s11299-014-0149-5,There is nothing to fear but the amygdala: applying advances in the neuropsychiatry of fear to public policy,June 2015,Lawrence Amsel,Spencer Harbo,Amitai Halberstam,Male,Male,Male,Male,"Fear and threat-elicited defense responses, whether or not consciously perceived, influence human behavior at both the individual and societal level. In the globalized economy of the twenty first Century, these phenomena may manifest themselves in policies that affect the lives of billions of people. Yet, the biological origins of fear and defense responses and their effect on decision-making may not be well understood by policy makers. Moreover, the response to threat and the experience of fear may interact with the cognitive tasks involved in risk assessment in complicated ways that have not yet been fully explored. Through these interactions, fear may be among the most powerful phenomena guiding current economic and public policy, and yet its influence may go unacknowledged. Two factors have been responsible for this lack of understanding. On the one hand, from an empirical psychological perspective, there has only recently been a significant improvement in our knowledge concerning the biological underpinnings of fear and threat response and their potential impact on behavior. On the other hand, fear and other emotions have often been excluded from rationalist economic and public policy discourse. They were seen as non-rational and thus at best non-commensurable with rationalist models, and at worst as pathological barriers to good decision-making. Fortunately, the last several decades have seen increased attention to the empirical study of fear within the disciplines of neuroscience (e.g., LeDoux 1998, 2000), behavioral psychology (e.g., Marshall et al. 2007), and psychiatry (e.g., Schneier et al. 2012), as well as a greater acceptance within economics of the appropriate role of emotions, including fear, in models of decision-making (e.g., Bell 1981; Pope 2009). One of the difficulties of incorporating fear into our economic and public policy models is the multiplicity of definitions and conceptualizations of fear and its close relative, anxiety. For example, fear had often been thought of primarily as a conscious phenomenon that takes place within human awareness. However, with recent advances in neuroscience, researchers now conceptualize automatic behavioral responses to threat that bypass conscious thought processes (e.g., freezing, motor withdrawal) as part of the fear network (Ohman and Mineka 2001). However, as the precise brain mechanisms of these phenomena differ (LeDoux 2012), a distinction is called for. LeDoux (2013) suggests a separation of terms to define conscious, emotional states of ‘fear’ and the automatic ‘threat-elicited defense responses.’ We attempt to take the same approach here, distinguishing between the conscious state of fear and automatic defense responses, which may both be products of the brain’s threat-avoidance network, but are causally and representationally distinct (Mobbs et al. 2009). We are gradually learning to sort out those aspects of fear that are amenable to rational cognitive or computational modeling, from those that involve so-called emotional responses, which are an alternative mode of processing fear information. Some of these responses may involve somatic sensations, as described by Demasio in his Somatic Marker Hypothesis (Demasio et al. 1991), and may give rise to more dichotomous judgments that are inconsistent with more rational probability matching. Note, this processing mode is more consistent with the notion of fear as a response mechanism in a hostile world, rather than as a risk evaluation process, useful in a system seeking to maximize benefit. The recent advances in understanding fear in the human brain, though still tentative, may give rise to a new approach to this complex phenomenon, that has significant implications for economic and public policies. This paper will next briefly introduce some of the foundations of the new approach to fear and threat response in order to highlight its potential impact on future policy decisions. Although humans have a strong belief, based on conscious experience, that our mind (and therefore our brain) functions as a unitary entity working towards a single goal, there is growing evidence from neuroscience that there are many systems in the brain acting independently and even in competition with one another (Han et al. 2007; Kelly et al. 2008; Peters et al. 2004; Poldrack and Packard 2003). While the more primitive parts of the brain like the amygdala focus on automatic responses such as threat detection, advanced structures like the prefrontal cortex focus on cognitive control and decision-making. There are also specific brain circuits that work to resolve conflicts in a context-specific manner (Egner and Hirsch 2005; Egner et al. 2008; Etkin et al. 2006), whether the conflict lies in the stimuli being processed or the internal response to those stimuli (Egner et al. 2007). In light of these findings, the brain can be seen as a complex economy, consisting of many agents that may be pursuing divergent goals, but nonetheless need to interact with each other. The complex nature of the brain can be easily observed in the context of fear and threat response, as they can be experienced in several fundamentally different ways depending on the context and specific stimulus of threat (LeDoux 2012; Mobbs et al. 2009; Ponnusamy et al. 2007). There are mechanisms of fear that occur in response to external stimuli, both within and outside of awareness, that can in turn produce autonomic, automatic, or voluntary behavioral responses, some of which focus on the goal of avoiding danger (Ohman and Mineka 2001), while others may be assimilating information for future use. Fear can also be experienced without a stimulus as a conscious emotional state, e.g., fear of potential future events such as terrorism or environmental catastrophe, or fear related to memories of past traumatic events. Moreover, fear sometimes becomes self-referential, wherein the state of fear itself becomes something to fear, an aversive condition that is avoided as much as the external stimulus that initially caused the fear, leading to thought blocking, denial, and avoidance behaviors (Chambless and Gracely 1989). Mobbs et al. (2009) have observed that the higher brain regions process more abstract fear, while midbrain regions respond to imminent threat. The primitive threat response that occurs as a survival-based defense mechanism has an internal logic that is black-and-white and inflexible, while the conscious fear generated by deliberate reasoning is dynamic and depends on contextual factors, anticipated future conditions, emotions and memories. These separate systems of fear may act independently and their interaction can be synergistic, or potentially counterproductive. For instance, the response towards specific stimuli, e.g., snakes, can be useful for avoiding danger, but avoidance behaviors that become generalized, e.g., avoiding wooded areas for fear of encountering a snake, may prevent action and is therefore detrimental to the system as a whole. This dichotomy can be understood from an evolutionary perspective, as the human brain was not planned in advance, but rather evolved from pre-existing systems. Human cognition and behavior have evolved over time in tandem with changes to the structural composition and neurocircuitry of the brain. The evolution of the human brain has involved the preservation of primitive brain structures and their associated processes, while developing new structures and processes on top of these. Thus, the prefrontal cortex, which contributes to higher-level critical thinking and decision-making, literally sits atop more ‘primitive’ mid-brain regions and communicates with them. From the context of survival, the fear systems serve the purpose of identifying and attending to threats, initiating a defense response (fight-or-flight), and preparing an individual for the appropriate action that will preserve life (Perna 2013). As an example, a fear of snakes has evolved in humans and other primates as a survival mechanism, which is still active in many individuals (Ohman and Mineka 2003). While logic and critical thinking may be useful in planning for the future and improving general conditions of life, the autonomic (nervous system) and automatic (motor) products of threat response serve the vital purpose of shielding us from potential harm or death and favor immediate survival over long-term thriving. The capacity for conscious states of fear and the consolidation of fear memories may have developed in humans to help us better respond to repeated threats and plan for future danger. In some cases fear memories may lead to pathologies of fear and exaggerated caution (discussed below). Understanding the biological and evolutionary bases of fear, both in its helpful and destructive aspects, may help policymakers better anticipate reactions to catastrophic events in the future, while better preparing society to face risk and uncertainty. The current neuroscience perspective indicates that the brain regions involved in threat response and fear are not organized into a single fear circuit (LeDoux 2012). Rather, threat-related stimuli are processed by several circuits in parallel. These simultaneous streams of processing can help us understand the internal inconsistencies inherent in human fear and threat response. Here an example from the clinic may help. It is not uncommon for First Responders exposed to a particularly intense trauma to become avoidant of elements of the trauma as a result of fear conditioning, for example, tunnels, subways, or even elevators. If asked about the probabilities of danger from these circumstances, these individuals are able to give good estimates of the low probability of danger, but behaviorally they act as if their probability estimates were very high, or as if there were a binary threat detection labeling some things as completely dangerous and others as safe, without the nuance of probability. Such inconsistencies have been the source of the impression that the fear can be irrational, as it is not even internally consistent. Multiple lines of research within neuroscience (broadly defined here as the study of brain and nervous tissue by the observational methods of biological science) point to a set of brain regions organized into a set of circuits that underlie our complex response to threat. In a somewhat simplified model, the major regions include: the sensory cortex (stimulus awareness), hippocampus (memory of associated stimuli), amygdala (rapid evaluation of danger potential), bed nucleus of stria terminalis (relay of fear-related signal), prefrontal cortex (integration and conscious decision-making), and hypothalamus (release of stress hormones) (Sah and Westbrook 2008). Of these, the key structure now believed to be governing fear is the amygdala, which plays a central role in both automatic threat response and conscious fear processing (Davis 1992; LeDoux 2003; Rosen and Donley 2006; Shin and Liberzon 2010). Evidence for this central role comes from several convergent neuroscience approaches. Classical conditioning experiments have been used to demonstrate the role of the amygdala in fear learning and unlearning (extinction) (Davis 1992; Knight et al. 2004; LeDoux et al. 1990; Phillips and LeDoux 1992; Sotres-Bayon et al. 2006; Rogan et al. 1997). Functional MRI studies, while tentative and based on small samples, further suggest that the amygdala’s role in processing emotions is focused on fear. For example, increased activity within the amygdala has been observed when a person is presented with angry and fearful faces, but not happy or neutral ones (Breiter et al. 1996; Mattavelli et al. 2013). Moreover, damage to the amygdala has been shown to interfere with the conditioning of fear responses (LeBar et al. 1995; LeDoux et al. 1988) and cause deficits to the processing of negative emotions, such as fear (Adolphs et al. 1995; Sprengelmeyer et al. 1999; Tranel et al. 2006). Moreover, the amygdala participates in fear learning and memory formation (LeBar et al. 1998; LeDoux 1998; Rogan et al. 1997), so that particular events may have a fear “label” attached as they are filed in our memory banks, a process known as fear consolidation (LeDoux 2007). Heightened amygdala activity has also been observed in posttraumatic stress disorder (PTSD), specific phobias, and generalized anxiety disorder (Shin and Liberzon 2010). It is important to note, however, that the amygdala is not always involved in fear processing and response. For instance, it seems not to be involved when when reacting to a specific already frequently encountered threat (Ponnusamy et al. 2007). Behavioral and FMRI studies have been instrumental in shedding light on the mechanisms of fear learning and the origins of fear-related pathologies, specifically how fear is spread from a dangerous stimulus to a neutral one, a property known as conditioning. Much of the research on fear and its mechanisms in the brain has focused on experiments that employ classical conditioning, modeled after Pavlov’s (1927) initial experiments. Classical conditioning models, when applied to fear, are known as fear learning models and operate through the association of a fear response to a conditioned, innocuous stimulus. Common fear responses elicited by a conditioned stimulus, such as a bell that rings just before an electric shock, include increased heart rate, release of hormones, and freezing (LeDoux 2000). It is important to note that while the association of conditioned stimuli with a threat responses is learned, the responses themselves are not learned but are pre-existing, species-specific responses to danger (LeDoux 1998). Fear conditioning has been shown to alter the amygdala’s response to threat in the same way as the induction of long-term potentiation, an experience-dependent form of neuroplasticity involved in memory formation (Rogan et al. 1997). Unlike other memories however, fear memories may elicit more than just a fear response and may actually re-ignite the whole negative experience of the original dangerous event. For example, in response to a small cue, individuals with PTSD may experience physiological reactions (increased heart rate, trouble breathing, or sweating), but may also experience a whole re-experiencing of the traumatic event (American Psychiatric Association 2013). This may be the result of the focusing of attention towards fearful aspects of a situation or the selective clarity of fear memory. Both the focusing and the memory may help an individual more efficiently avoid future threats. But the avoidance of future dangers may come at a cost to individuals suffering from pathologies of fear. Similar to other types of emotional and behavioral conditioning, fear conditioning is initially context specific, but can generalize through over-learning to include a wide variety of stimuli that do not pose any significant danger. For example, people in a car accident may first learn to fear cars, but this may generalize to a fear of any mode of transportation. In this way fear can lead to a more general state of anxiety, resulting in detrimental avoidance behaviors. Fear-based anxiety disorders, such as panic disorder, phobias, and PTSD, affect a large percentage of the population, estimated at 28.8 % in the United States (Kessler et al. 2005). Such fear-related pathologies can arise from contextual factors (Marshall et al. 2007) or from individual traits, as some persons may be more susceptible to developing fear-related disorders than others (Amstadter et al. 2009; Kendler et al. 2001). Pathologies of fear may be the result of increased fear learning through reconsolidation, which leads to enhanced retention of fear memories and a resistance to unlearning the fear (Blechert et al. 2007; Davis 1992; Myers and Davis 2007; Rothbaum and Davis 2003; Sotres-Bayon et al. 2006). In healthy individuals, the behavioral and autonomic responses to once-feared stimuli or past trauma diminish naturally over time, a naturally occurring form of extinction. Similar to fear learning, extinction occurs not through the forgetting of fear, but the overriding of fear memories (Bouton 2004). The parts of the brain identified in the encoding and expression of fear extinction are similar to those identified in fear learning, indicating that fear extinction is a process of “over-learning”—the new interpretation overrides but does not eliminate the earlier interpretations of stimuli as threatening. The parts of the brain identified in this process include cortical and subcortical brain areas, especially the interaction between the medial prefrontal cortex, amygdala, and hippocampus (Sotres-Bayon et al. 2006). In individuals who develop fear-related pathologies such as PTSD, the extinction of the fear memory may be inhibited, resulting in persistent symptoms such as re-experiencing a traumatic event (Rothbaum and Davis 2003). Advances in our understanding of fear learning and extinction mechanisms in the brain have implications for the treatment of fear-based pathologies through structured programs of fear extinction (Garakani et al. 2006). Exposure therapy, which aids the reduction of fear memories by reintroducing a conditioned fear stimulus in a safe context, has been used as an effective treatment to alleviate those suffering from persistent fear memories (Adenauer et al. 2011; Dias et al. 2013; Rothbaum and Schwartz 2002; Schneier et al. 2012). This can also occur without formal therapy (Davis et al. 2005; Hauner et al. 2013), and therefore this process can be applied widely. The treatment methods described above have implications for individuals and society, as deficits caused by fear conditioning can also impact larger groups. Collective trauma can be experienced by those outside the spatial proximity of a traumatic event, manifesting in anxiety, PTSD symptoms, and increased substance use (Marshall and Galea 2004; Marshall et al. 2007). This can occur through media exposure, especially among children (Custers and Van den Bulck 2012). The emotion of fear has even been observed to override positive emotions, like hope, in societies exposed to years of conflict (Bar-Tal 2001). This is not surprising, given our current understanding of fear consolidation and memory formation. In order to improve the response to mass traumatic events, like terror attacks or economic depression, policy makers should pay attention to images and information contributing to fear memories, as these fears may generalize and lead to disruptions to normal behavior and the avoidance of healthy risk taking. At the same time, increasing awareness of potential threats may also be beneficial, as mental health at individual and societal levels may be dependent on the correct mix of hope and fear (Pope 2009). With the absence of appropriate fear individuals may be inadequately prepared for future disasters. As learned responses to threat have been shown to circumvent certain fear circuits in the brain (Ponnusamy et al. 2007). The purposeful introduction of fearful cues when the severity of a potential threat cannot be detected, may help. This approach has been effectively used by government agencies that place graphic warnings on cigarette packs to prevent smoking, and could be used similarly to promote other healthy behaviors (Field 2013). Thus, building on our current understanding of fear learning and extinction is important for addressing issues of public policy that may influence or be influenced by individual or societal fear. Risk assessment is an important decision-making process that has implications for both survival and wellbeing. Animal studies show that risk taking is influenced by a multitude of factors (Stankowich and Blumstein 2005), and obviously many more factors may influence risk-taking behaviors in humans (Marshall et al. 2007). Risk taking has an evolutionary basis (Ellis et al. 2012) and may be dependent on the interaction between logical reasoning and psychosocial factors in the brain (Steinberg 2007). For instance, Slovic and Peters (2006) describe dichotomous states of experiencing risk as a feeling and perceiving risk analytically using logic and computation. The reasoned ways of perceiving risk can be interpreted as a step in decision making process whereby the possible outcomes of a threat or stressor are formulated and the appropriate action is taken (Pope 2009). Recent advances in the neuroscience of fear, on the one hand, and the behavioral turn in economics, on the other, have resulted in an increase of interest in the role of fear on risk-taking. Risk assessment may be unduly influenced by a fear of negative outcomes produced by the threat-detecting processes of the fear networks in the brain (Camerer et al. 2005). At the same time, there may often be a conflict between fear and constructive risk-taking behaviors, which could be explained by the divergent goals of different brain circuits. For example, the risk-taking behaviors of adolescents can be beneficial to society, such as military heroism or the romantic drive, may be carried out despite the mechanisms of fear, that would normally inhibit such behavior. However, in many cases fear may deter risk-taking behavior. This can occur either directly, through the anticipation of an unpleasant outcome that causes a person to choose a safer option, or indirectly by causing the person to put extreme weight on the probability of the negative outcome (Pope 1995). In putting more weight on a negative outcome than is appropriate for its likelihood of occurrence, one may be deterred from risk taking. Such caution may be driven by fear circuits that have been recently identified (Tom et al. 2007). On whether it is reasonable to be cautious in this sense, Keynes (1921) notes that it is unclear who will profit more the cautious or the risk-taker. In their early work, Kahneman and Tversky (1979) classify people who put more weight on bad outcomes than their probability of occurring as irrational, whereas Allais 1979 argues it is reasonable to put more weight on bad outcomes. Caution can also arise from loss aversion, that is from people taking their current situation as an overvalued reference point, and putting more weight on future possible losses than on future possible gains. Kahneman (2003, p. 1457) later work can thus be interpreted as saying that taking a reference point, as in loss aversion, is not invariably irrational. However, these models may not focus sufficiently on the period when fear would be most experienced, namely in the pre-outcome period (Pope 1983). Potential negative outcomes or losses may be more prominent in the decision-making process when a threat is felt at the individual level (Bohm and Pfister 2008), and may often be ignored completely by those who have never themselves faced that loss, even where the losses may be horrific as in the case of natural disasters like floods (Kunreuther 1984). However, spatial or temporal proximity to prior event are not the only determinants of risk-taking behavior. Many other factors may influence individuals’ decision making in response to perceived risk, contributing to their ‘relative risk appraisal’ (Marshall et al. 2007). Media coverage, statements by government officials, or other aspects of the cultural milieu may lead people to have an exaggerated fear or perception of risk related to a potential threat, thus deterring them from risk-taking behaviors (Fischhoff et al. 1978). Caution is not necessarily bad; in some environments, it enhances goals and assists in survival. However, when caution is excessive and becomes pathological, neuroscience’s recent discoveries may assist in its reversal. Conversely, advances in neuroscience and our understanding of fear conditioning networks may help to balance risk taking in the face of extreme threat, when taking a cautious approach may lead to a better outcome. Thus, given the complex nature of fear and its multiple instantiations, policymakers need to be able to identify what kind of fear is being activated in order to be able to adequately respond to the related changes in group behavior (e.g., economic), and this is especially true in times of crisis and subsequent periods of recovery.",3
14.0,2.0,Mind & Society,25 September 2015,https://link.springer.com/article/10.1007/s11299-015-0184-x,Special issue on “Complexity modeling in social science and economics”,November 2015,Itzhak Aharon,Sacha Bourgeois-Gironde,Yakir Levin,Male,,Male,Mix,,
14.0,2.0,Mind & Society,05 May 2015,https://link.springer.com/article/10.1007/s11299-015-0167-y,Network science: a useful tool in economics and finance,November 2015,Dror Y. Kenett,Shlomo Havlin,,Male,Male,Unknown,Male,"Network science has grown exponentially as a novel tool for the study of complex systems. A huge number of results have established this approach as a new kind of science, where theoretical results are integrated with empirical big data analysis. Many unexpected phenomena of real world systems have been discovered, and increasingly sophisticated system structures are studied. Examples include linked molecular or cellular structures, climate networks, communication and infrastructure networks, but also social and economic networks. An understanding of the growth, structure, dynamics, and functioning of these networks and their mutual interrelationships is essential in order to find precursors of changes, to make the systems resilient against failures, or protect them against external attacks. The interrelationship between structure (topology) and dynamics, function and task performance in complex systems represents the focus of many studies in different disciplines of research with important scientific and technological applications. Complex networks have been recognized as the leading framework to describe the behavior of physical, chemical, biological, technological and social networks. Network science has greatly evolved in the twenty-first century, and has become one of the most active fields of interdisciplinary science (Newman 2009; Boccaletti et al. 2006; Newman et al. 2011; Cohen and Havlin 2010; Havlin et al. 2012; Barabási et al. 2014; Barrat et al. 2004; Helbing 2013; Caldarelli 2007; Song et al. 2005). Famous examples include the network of sexual partners (Liljeros et al. 2001), internet and WWW (Faloutsos et al. 1999; Barabási and Albert 1999; Cohen et al. 2000; Pastor-Satorras and Vespignani 2007), epidemic spreading (Pastor-Satorras and Vespignani 2001), immunization strategies (Cohen et al. 2003), citation networks (Radicchi et al. 2008), structure of financial markets (Bonanno et al. 2003), social percolation and opinion dynamics (Solomon et al. 2000; Shao et al. 2009), dynamics of physiological networks (Bashan et al. 2012), protein networks (Milo et al. 2002; Sendiña-Nadal et al. 2011), organization and functioning of the brain (Reis et al. 2014), structure of mobile communication network (Onnela et al. 2007), climate networks (Yamasaki et al. 2008; Ludescher et al. 2014), transportation systems (Li et al. 2015) and many others. Among the phenomena that have been shown to fall in this conceptual framework are: cascading failures, blackouts, crashes, bubbles, crises, attacks and defense against them, introduction of new technologies, understanding measuring and predicting the emergence and evolution of networks and their stylized features, spreading phenomena and immunization strategies, as well as the stability and fragility of airline networks (Cohen and Havlin 2010). Current and past research has shown that in real life systems, there is a strong feedback between the micro states and macro states of the system. This description of nature can be well represented by network science—in which the micro is represented by the nodes of the network and the links between them, and the macro by the network itself, its topology, dynamics and function. Thus, network science is a leading framework to investigate real life systems. For example, as opposed to physical systems where the dynamics is usually bottom-up, in social and economic systems there are interplays on all levels with singular top-down feedbacks. Thus, in many practical realizations, in addition to the bottom-up contagion propagation mechanisms one finds that there is a global-to-local feedback: individuals, their interdependence and behaviors build up the system’s cooperative behavior that finally affects back on individuals’ choices. It has been proposed that the bottom/up–top/down feedback has the capability to change completely the character of a phase transition from continuous to discontinuous, thus explaining the severity of the economic crises in systems where the collective interacts as such with its own components (Cantono and Solomon 2010) [see extended discussion in Havlin et al. (2012)]. The latest financial crisis has enhanced the emphasis of the importance of collective connectivity effects in the evaluation of financial fragility and for the probability of default. While traditional evaluations of the probability of default use only global information (general situation of the economic branch to which a company belongs) and point information (companies’ balance sheet, profit margins, etc.), the latest events show that effects can and do propagate over many intermediate connections. In fact, the cascading effect has been a crucial element of the fast and devastating impact of the crisis. Thus, it is not realistic to separate the stability of a company from the collective dynamics taking place in its economic neighborhood. Access to time and conditions of capital flows between companies allows one in principle a capability to monitor in detail the working of the economy. One example is to express the fragility of a node in terms of the probabilities of default of its clients and suppliers. By conducting experiments on the network of financial transfers, one can probe the probability and size of such cascading events. Financial systems are perhaps the best example of a complex adaptive system, in which the micro interacts through bottom-up mechanisms with the macro. This is followed by top-down feedback between the macro and the micro. One immediate example is a market index and the stocks that makeup this index. The stocks represent the micro, while the index represents the macro. Kenett et al. (2011, 2012b) have recently shown that the index has a stronger influence on the stocks than vice versa, which is neither a trivial or intuitive result. Thus, the micro and the macro continuously interact. These interactions are best characterized by network science, one which addresses dynamic and coupled networks. Recent work on individual strategies as subtracted from detailed financial data (Lillo et al. 2008; Mu et al. 2010) enables to identify groups of players on the market and their role in stabilization and destabilization. One of the main sources of the intrinsic instability of financial markets is the almost entire absence of negative feedback loops. Appropriate network models with signed (positive and negative) links will help in finding the optimal balance between stability and liquidity and contribute this way to the solution of the major problem of market regulation.",42
14.0,2.0,Mind & Society,08 May 2015,https://link.springer.com/article/10.1007/s11299-015-0168-x,Models of complex adaptive systems in strategy and organization research,November 2015,Oliver Baumann,,,Male,Unknown,Unknown,Male,"The development of new theory is often spurred by novel techniques that provide better answers to existing questions, or that allow asking new ones (Greenwald 2012). In the field of strategy and organization science, this is true for research on the long-standing notion that firms need to establish fit with their environments in order to prosper and survive (Cyert and March 1963; Lawrence and Lorsch 1967). While extensive early work had sought to establish links between environmental contingency factors and elements of organization design (Galbraith 1973; Burton and Obel 1998), the emphasis has shifted in recent years from a static to a more dynamic perspective, and a considerable amount of research has focused on gaining insight into how organization design affects processes of organizational adaptation (Levinthal 1997; Siggelkow and Levinthal 2003; Gulati et al. 2005). To a large degree, this research was sparked by scholars conceptualizing organizations as complex adaptive systems (CAS) and adopting various models from “complexity science” (Holland 1975; Kauffman 1995; Miller and Page 2007). A key driver of this adoption was the fact that many modeling techniques require specifying system-level behavior upfront, which may be impossible if the purpose of the research is to better understand some system-level property. Models of CAS, in contrast, allow pursuing a bottom-up approach: the modeler specifies the agents of interest (e.g., a network of decision makers that makes up an organization); the agents are faced with some (task or competitive) environment; and they are equipped with behavioral rules to process information and act in their environment. System-level behavior, in turn, emerges from the local interactions of the agents and is often explored through Monte-Carlo simulation methods. Using this approach, extant research has uncovered many non-obvious yet convincing insights into relevant organizational systems. This paper discusses the role of such complexity-based modeling for strategy and organization science, in particular for work that is concerned with how organization design drives organizational adaptation and performance. In doing so, the paper has three objectives: (1) to briefly highlight some domains where models of organizations as CAS have made substantial contributions; (2) to point to challenges and tradeoffs that the current modeling enterprise is facing; and (3) to suggest possible future directions of research that would further push our understanding of organizations as CAS. The following sections address these aspects in turn. Given the scope of the topics that are covered, my arguments inevitably represent a personal perspective and must remain on a general level. By focusing on modeling work in the strategy and organization field, for instance, the paper does not consider CAS-based approaches in related domains.Footnote 1 Moreover, given the ambiguous use of the topics, some initial definitions are appropriate. For the purpose of this paper, I define organizational adaptation broadly as a change in some significant structural or behavioral attribute that affects an organization’s fit with its environment (Cyert and March 1963; Levinthal 1997).Footnote 2 Similarly, I apply an equally broad definition of what denotes a CAS (Simon 1962; Axelrod and Cohen 2000; Mitchell 2009): a system that comprises multiple interdependent components and nested levels of dynamics; that produces, acquires, and processes information about its internal and external environment; and that acts in the external environment based on this information. Finally, because research in strategy and organization science is often concerned with business firms, I use the terms “organizations” and “firms” interchangeably.",12
14.0,2.0,Mind & Society,27 June 2015,https://link.springer.com/article/10.1007/s11299-015-0170-3,"Emergence, group judgment and the discursive dilemma",November 2015,Joel Walmsley,,,Male,Unknown,Unknown,Male,"The concept of “emergence” is one of those philosophical ideas whose popularity has waxed and waned over its rich history. Whilst popular in the early part of the twentieth century, scientific developments largely led to its rejection (or, at least, neglect) shortly thereafter (see McLaughlin 1992). But the last couple of decades have seen a laudable resurgent interest in both defining the concept in general, and applying it to specific cases in particular. This reflects the fact that the would-be emergentist faces two challenges: the first is to define the concept in a way that coherently allows it to do what it is supposed to, the second is to identify cases that exemplify the concept thus defined. In this paper, I want to outline what I take to be the most defensible concept of emergence and show that it provides a good analysis of a particular example of the relationship between individual- and group-cognition; specifically, I shall argue that the concept of emergence outlined and defended by Broad (1925) provides a good analysis of group judgment in the situation that has come to be known as the “discursive dilemma.” Interestingly, although the concept of emergence often features in discussions of complexity (and vice versa), I will argue that this particular case of emergent group cognition (or judgment) is one that is best analysed in terms that need not draw upon the resources of complexity theory.",
14.0,2.0,Mind & Society,30 June 2015,https://link.springer.com/article/10.1007/s11299-015-0171-2,Complexity and individual psychology,November 2015,Yakir Levin,Itzhak Aharon,,Male,Male,Unknown,Male,"(a) What is the scope of complexity explanations-cum-modeling of human behavior? (b) How far can they be applied in economics? (c) To what extent can they be applied to other social phenomena? (d) Can complexity-like explanations-cum-models be applied to the psychology of individuals? (e) What are the implications of the answer given to (d) for questions (a)–(c)? In this paper we focus on question (d), but also briefly address question (e). We start with an outline of the classic representational-cum-computational model of human psychology, its main motivations, and the main challenges that it faces (Sect. 2). We then outline two alternative models that have been proposed in response to these challenges within the embodied mind movement: First, a rather moderate alternative—moderate embodied cognition—which, switching to notions of representation and computation other than those assumed by the classic model, also gives the body and the environment a role to play in our cognitive processes (Sect. 3). Second, a radical alternative—radical embodied cognition—which, giving up the notions of representation and computation, and thus also going against the moderate alternative, has significant affinities with complexity explanations (Sect. 4). Deploying neo-Kantian considerations, we then argue that due to the conceptual dimension of our cognitive system—what Kant (1998, A 24-25/B 39, A 31/B 47, A 68/B 93) called “discursivity”—the radical alternative must be incorrect insofar as humans are concerned; indeed, human psychology requires, at least partly, representational understanding of the sort provided by the classic model. Relatedly, we show how the discursive dimension of human cognition complicates our psychology and makes it rather varied and difficult to account for (Sect. 5). Finally, we briefly address the question of how the complicated nature of individual psychology implied by human discursivity may affect complexity explanations of social behavior (Sect. 6).",1
14.0,2.0,Mind & Society,22 July 2015,https://link.springer.com/article/10.1007/s11299-015-0176-x,The emergence of macroscopic regularity,November 2015,Meir Hemmo,Orly Shenker,,Male,Female,Unknown,Mix,,
14.0,2.0,Mind & Society,06 August 2015,https://link.springer.com/article/10.1007/s11299-015-0177-9,Why lay social representations of the economy should count in economics,November 2015,Elisa Darriet,Sacha Bourgeois-Gironde,,Female,,Unknown,Mix,,
14.0,2.0,Mind & Society,31 July 2015,https://link.springer.com/article/10.1007/s11299-015-0178-8,A call for complexity: integrated models to solve complex policy problems,November 2015,Liz Johnson,,,Female,Unknown,Unknown,Female,"The purpose of this research is to argue complexity theory can play a comprehensive and integral role in policy research providing an interconnected, systems worldview that should inform and underlie methods, methodology, and analysis (Johnson 2013). Theoretically, the approach offers unique opportunities to explore the non-linear nature of systems, as well as the interrelated, interdependent variables that ultimately impact outputs, outcomes, and externalities. Complexity theory and its fledgling paradigm of complexity science, however, need specified explication in order to effectively apply to policy research and policy analysis (Johnson 2015). Ramalingam et al. (2008) asserted complexity science is a loosely organized set of ideas and principles that have emerged resulting in a more nuanced means to describe and understand the processes and dynamics of change. Casti (1994) added complexity provides the means to represent the experience of change by descriptions of “our collective reality as a process” (p. 273). Researchers like Berreby (1998) and Pagels (1988) claimed complexity science is at the frontier of science, computing, and mathematics. Capra (2005) argued complexity is a new way to understand life as living systems comprised and based on the context, patterns, and processes of relationships. He further claimed, complexity can be understood as non-linear phenomena, translatable into a mathematical language, with a possibility of unifying life through the integration of cognitive, biological, and social dimensions (Capra 2002). Additionally, Casti (1994) asserted complexity science is a means to assess the limitations of reductionism as the standard, accepted problem-solving approach to solving problems. Johnson (2009b) viewed complexity science metaphorically, as an umbrella science, or the Science of all Science. Still more expansive, Pascale et al. (2000) described complexity as a broad-based inquiry into the mutual properties of all living things with the capacity to cooperate, regenerate, decline, die, compete, and thrive (Johnson 2015). Complexity science as a frontier in research is intriguing, but is inherently challenging conceptually on a variety of levels. One rationale for the conceptual challenge is the inherent nature of complexity is expansive and clambers up upon itself to achieve higher levels of organization (Rescher 1998). Another rationale is comprehending associated terminology can be challenging and confusing (Johnson 2009a; Wolf-Branigin 2013); however, there are those like Rescher (1998), who insisted complexity is the inverse of simplicity, yet provides limited explication (Rescher 1998). Still there are currently no agreed upon standard or rigorous definitions of complexity and what it entails (Johnson 2009a; Rescher 1998; Wolf-Branigin 2013). Page (2009) contended complexity has many dimensions for exploration. Complexity means unique and varied things to various people across and within disciplines (Johnson 2009a, b). What complexity could means to the discipline of policy research will be explored as well as what it could mean in the future.",2
14.0,2.0,Mind & Society,26 August 2015,https://link.springer.com/article/10.1007/s11299-015-0179-7,Contemporary finance as a critical cognitive niche,November 2015,Tommaso Bertolotti,Lorenzo Magnani,,Male,Male,Unknown,Male,"Complexity and Life, at every stage of evolution, are two notions that are never that far one from the other. The very origin of life on Earth, and the fact that so far there is no strong evidence of life having originated anywhere else in the Universe, are considered as the epitome of a random and unpredictable (and hardly reproducible) series of conditions. 
The past century, also because of the development of quantum physics, has witnessed a growing awareness about how complexity and unpredictability are dominant constraints in ontogenesis and phylogenesis, that is affecting not only the development of species as we know them, but also of every single individual (Longo and Buiatti 2013). In other words, given an initial set of genetic codes, there is no way to predict their giving rise to a potentially infinite number of new species; similarly, given the genetic code of an embryo, there is no way to reliably predict (or compute) what the phenotypic expression will be in n years, because of the unpredictable effects of a highly unpredictable environment. Indeed, from the perspective of system studies, life emerged and prospered in a non-deterministic, open system. Such systems are characterized by a significant level of uncertainty, in the sense that given current conditions future outcomes can be foreseen rightly to certain extents, or wrongly guessed, but they cannot be deterministically computed. This is essential to the nature of the system. The relationship between an organism and its environment is indeed one of coping with the uncertainty of its system, and that—we contend—is what sparked the origin of the multifaceted phenomenon known as cognition. Any definition of cognition is conceptually human-centered, and has been only subsequently extended—at least possibly—to animals. This is why we suggest that adopting a barely-essential definition of cognition as the one offered by the Stanford Philosophy Encyclopedia might serve our scope, and let us rely on something that is not overly biased towards animals, but neither excessively human centered to begin with. Cognition is constituted by the processes used to generate adaptive or flexible behavior. The adaptive behavior implied by cognitive capabilities is a response to the uncertainty of the environment in which the organism must survive. We can wonder: what is, pragmatically speaking, the ultimate goal of cognition? If by introducing a difference between unpredictability and uncertainty we mean to stress the qualitative gap between, respectively, quantifiable (unpredictable) and unquantifiable (uncertain) risk, a stimulating way to answer might be the following: cognition aims at predicting what is predictable, making predictable what is unpredictable, and making just unpredictable what is uncertain. The two first thirds of this sentence are clear enough, as they frame the cognitive drive towards making sense of one’s surroundings (Magnani 2007b, chapter 3). The latter, conversely, considers how survival, at any level (from the humblest ant to the Wolf of Wall Street) is mostly about achieving second bests. In this sense, we can understand cognition as the strife to pull within the range of predictability what is uncertain. With this respect, the form of inference best characterizing the organism’s approach to its surroundings is abduction, which in this case accounts for the attempt (sometimes successful) to elaborate and enact hypotheses about the behavior of relevant objects in an organism’s environment (Magnani 2009). We shall limit our analysis of abduction to what is requested by our current goal, but it is interesting from the beginning to contrast it with a better known form of inference, that is deduction. This very brief example should make the distinction clear, and show why the notion is so relevant for our task. Consider a train running on a rail track. If you know the speed of the train, the path of the tracks and so on, it is relatively straightforward ceteris paribus
Footnote 1 to calculate the position of the train at any given time. In a way, the position of the train can be deduced from its speed, its table and the geography of the track. If these are true, that is in accordance with the related state of things in the world, then the train will be where we compute it to be at a given time. Consider a lion chasing a gazelle. The lion must anticipate the prey’s movement in order to tackle it to the ground and kill it. Still there is no way for the lion, or for any hunter, to deduce the position of the escaping prey at any given time. The lion must perform and enact immediately a quick appraisal on whether the gazelle will jerk right or left, basing on speed, maybe past experiences, terrain conformation, presence of other lions and so on: the future position of the gazelle is quite unpredictable, therefore an abductive hypothesis, enacted at once, is the best the lion can rely on to manage its pursuit. What is the difference between the two systems described in the examples above? Standard models, ideologically assuming learning processes to be deterministic, would suggest that there is hardly any qualitative difference, and a complex model would explain the lion and gazelle system as well, making it predictable once the various factors are appropriately quantified. Translated into popular culture, this kind of assumption is precisely that lead to the bloodshed on Isla Nublar, in the novel and blockbuster Jurassic Park: such remark shows how common sense is quite clear about the difference between closed systems (more or less complicatedly deterministic), and open systems (uncertain and not necessarily quantifiable).Footnote 2 Going back to our examples, train circulation is a globally deterministic system inasmuch as it is entirely man-made: ruling out major break-ins of life in the system, for instance in the form of a suicidal will in the conductor, a human error, a terrorist attack etc., it is generally simple to foresee a schedule for the train also quantifying minor unpredictable disturbances. This is why, on the overall, the train schedules are quite reliable and indeed we usually experience pragmatically positive outcomes if we assume them to be reliable, and behave consequently. Conversely, the lion chasing the gazelle requires an entirely different paradigm to be described. Indeed, even something as simple as chasing a gazelle is indeed an open system, characterized by a great degree of uncertainty. Not only elements of unpredictability are hard to quantify, but the uncertainty also concerns the individuation of relevant factors and their interactions. Physical systems, no matter how complex, may display some non-computable statuses, but their dynamics are clear and can be actually modeled as far as they are governed by a series of physical causations. As soon as we move into the biological and then psycho-cognitive plan, we are not dealing with causations but with much looser (and hence uncertain) forms determined by the interactions between the two, or more, elements: we can speak of enablements (Longo and Montévil 2014) or, as the cognitive aspects are concerned, of affordances (Gibson 1977). Many of the phenomena a cognizing organism has to cope with are much more similar to the second example (lion-gazelle) than to the first. Survival (encompassing both the notions of fitness and welfare) rests on the possibility of continuously appraising unpredictable situations and making the best judgment out of them. Nevertheless, it is commonly agreed that higher cognitive capabilities are essentially about saving the cognitive effort, and one of the best ways to do this is to realize that unpredictability and similarity are not mutually exclusive and that, even if every biological (and then social) phenomenon is a priori unique, it is possible to elaborate certain heuristics in order to exploit the approximate-cause-effect relationships nested in biological unpredictability and randomness. This is what Tooby and De Vore described as accessing the “cognitive niche.” At the core of this lies a causal or instrumental intelligence: the ability to create and maintain cause-effect models of the world as guides for prejudging which courses of action will lead to which results. Because there are an infinitely large number of possible sequences of behavior (of which the overwhelming majority are maladaptive) “behavioral flexibility” is an insufficient characterization of our innovative adaptive pattern. Our cognitive system is knowledge or information driven, and its models filter potential responses so that newly generated behavioral sequences are appropriate to bring about the desired end. Of course, exploration, trial and error, and feedback are essential to the system, but, by themselves, they are inadequate to construct or maintain it [Tooby and DeVore (1987), p. 210, added emphasis]. If Tooby and De Vore [and subsequently Pinker (2003)] understand the cognitive niche as a kind of stage of cognition, in their opinion exclusive to human beings, other scholars such as Clark (2005) and Magnani (2009, chap. 6) have a much more local and objectified vision of a specific cognitive niche as something “constructed”, which can be defined as follows: Cognitive niche construction is the process by which organisms modify their environment to affect their evolutionary fitness by introducing structures that facilitate (or sometimes impede) the persistent individuation, the modeling, and the creation of cause-effect relationships within some target domain or domains. These structures may combine with appropriate culturally transmitted practices to enhance problem solving, and (in the most dramatic cases) they afford potential whole new forms of thought and reason (Bertolotti and Magnani 2015). Along the next sections we will better define the notion of cognitive niche (in its “constructed” connotation), and see how it can account for the way human “cultural” traits (ranging from hunting to advanced economy) can be seen as increasing welfare by reducing unpredictability, but also how such activities may hinder survival by introducing a new, more subtle level of uncertainty. In a nutshell, a cognitive niche consists in a series of externalizations of knowledge into the environment, for instance through material culture, resulting in a modification of the selective pressure that an organism has to face (Odling-Smee et al. 2003; Magnani 2009). The fact of championing cognitive niche construction could be seen as what intrinsically characterizes human beings (which are individuated by the theory as eco-cognitive engineers). The rest of the paper will then focus on the notion of terminator niche: a cognitive niche that becomes maladaptive because of the externalized knowledge structures that primarily did (or were thought to) cause the beneficial trade-off in selective pressure. Since the dawn of cognition, we have been acting on our surrounding ecologies in oder to make them easier to live in, and we engineered niches ranging from basilar sociality (Dunbar 2004) to material culture (Mithen 1996), through agricultural and hunting abilities. Every single step of development can be framed within the concept of eco-cognitive engineering: we engineer our environment by externalizing and manipulating pieces of knowledge. Otherwise said, humans (like other creatures) do not simply live in their environment, but they actively shape and change it while looking for suitable chances. In doing so, they construct cognitive niches—through which what the environment offers in terms of cognitive possibilities is appropriately selected and/or manufactured—to enhance their fitness as chance seekers (Tooby and DeVore 1987; Pinker 1997, 2003). Lessening the selective pressure means, for our cognitive efforts, to lessen the complexity of the external world by developing simpler models of how the environment works, and to enact them making the world a less unpredictable place to live in. A recent book by Odling-Smee et al. (2003) offers a full analysis of the concept of niche construction from a biological and evolutionary perspective. “Niche construction should be regarded, after natural selection, as a second major participant in evolution. […] Niche construction is a potent evolutionary agent because it introduces feedback into the evolutionary dynamics” (Odling-Smee et al. 2003, p. 2). By modifying their environment and by their affecting, and partly controlling, some of the energy and matter flows in their ecosystems, organisms (not only humans) are able to modify some of the natural selection pressure present in their local selective environments, as well as in the selective environments of other organisms. This happens particularly when the same environmental changes are sufficiently recurrent throughout generations and selective change. In summary, general inheritance (natural selection among organisms influencing which individuals will survive to pass their genes on to the next generation) is usually regarded as the only inheritance system to play a fundamental role in biological evolution; nevertheless, where niche construction plays a role in various generations, this introduces a second general inheritance system (also called ecological inheritance by Odling-Smee). In the life of organisms, the first system occurs as a one-time, unique endowment through the process of reproduction (sexual for example); on the contrary, the second system can in principle be performed by any organism towards any other organism (“ecological” but not necessarily “genetic” relatives), at any stage of their lifetime. Organisms adapt to their environments but also adapt to environments as reconstructed by themselves or other organisms.Footnote 3 From this perspective, acquired characteristics can play a role in the evolutionary process, even if in a non-Lamarckian way, through their influence on selective environments via cognitive niche construction. Phenotypes construct niches, which then can become new sources of natural selection, possibly responsible for modifying their own genes through ecological inheritance feedback (in this sense phenotypes are not merely the “vehicles” of their genes). It has to be noted that cultural niche construction alters selection not only at the genetic level, but also at the ontogenetic and cultural levels. For example, the construction of various artifacts challenges human health: Humans may respond to this novel selection pressure either through cultural evolution, for instance, by constructing hospitals, medicine, and vaccines, or at the ontogenetic level, by developing antibodies that confer some immunity, or through biological evolution, with the selection of resistant genotypes. As cultural niche construction typically offers a more immediate solution to new challenges, we anticipate that cultural niche construction will usually favor further counteractive cultural niche construction, rather than genetic change (Odling-Smee et al. 2003, p. 261). With a broader explanatory reach than sociobiology and evolutionary psychology, the theory of niche construction simultaneously explains the role of cultural aspects (transmitted ideas), behavior, and ecologically persistence inheritance. Of course niche construction may also depend on learning. It is interesting to note that several species, many vertebrates for example, have evolved a capacity to learn from other individuals and to transmit this knowledge, thereby activating a kind of proto-cultural process which also affects niche construction skills: it seems that in hominids this kind of cultural transmission of acquired niche-constructing traits was ubiquitous, and this explains their success in building, maintaining, and transmitting the various cognitive niches in terms of systems of coalition enforcement. “This demonstrates how cultural processes are not just a product of human genetic evolution, but also a cause of human genetic evolution” (Odling-Smee et al. 2003, p. 27). From this viewpoint the notion of docility (Simon 1993) acquires an explanatory role in describing the way human beings manage ecological and social resources for decision-making. Finally considering economy, it can be rightfully seen as relating to cognitive niches from its very etymology. The word derives its current seventeenth century meaning from the Greek “oikonomia,” household management, based on “oikos,” house + “nemein,” manage. The ancient notion of household incorporates the idea of cognitive niche, indicating at the same time the ecological diffusion of the family (the physical estate), and the maintained set of rules (shared to different extents) that regulate it. Speaking of household management epitomizes the idea of cognitive niche, by resuming all of the heuristics and problem-solving activities aimed at facing the cause of problems, that is the unpredictable conjunctures of the “external” world, upon which all households would survive. It is easy to extend the discourse of households to the actual economical discourse including firms, stakeholders and other actors. In its more actual significance, economic systems can still be seen as a broad cognitive niche, where knowledge representing cause-effect patterns in the unpredictable and complex economic world are individuated, in order for the actors to manage the available resources pursuing a determinate goal. Indeed, economics, as a discipline and a practice, consist in a niche construction and regulation activity supposedly making sure that the adopted strategies are coherent with the state of resources and with the goals they aim at producing. Indeed, as Dow observed, “the emergence of institutions more generally can be understood as a means of reducing uncertainty […]. The existence of the firm itself (indeed, of any contractual arrangement) can be understood as a mechanism to reduce uncertainty for some parties involved, providing a pool of liquidity and a basis for action” (Dow 2015, p. 40). Several perspectives on uncertainty in economics have already approached the matter in a way that is compatible with cognitive niche construction theories: consider the instructionist approach focusing on firms, claiming that the latter avoid uncertainty by restricting to ecological niches that are simple and change very slowly (Anderson and Paine 1975; Pich et al. 2002), and of course the evolutionary approachFootnote 4 (Nooteboom 2008). Therefore, assuming that the description so far satisfies the connection between economics and cognitive niche construction theories, what we aim at analyzing are critical economic conditions through the latter theory: in order to do that, we will first briefly spell out key features of high-technology cognitive niches (of which contemporary finance is a clear example), and then analyze the latest economical crisis in the light of the developed argument. Before moving on, the fundamental issue of open and closed systems has to be brought to attention once again, to understand the full impact of cognitive niche construction for the economic discourse. As shown by the already mentioned Cecile Dow, the understanding of uncertainty and its impact on the economical practice is a largely epistemological issue (Dow 2015). The economists’ assumption and the behaviors they prescribe depend on their beliefs concerning the capability of cognition to cope with uncertainty and unpredictability. As a powerful cognitive niche, the economy and economics play a crucial role in improving the profitability of a system by reducing and computing aspects otherwise contemplated as non-computable. At the same time, it must be remembered that the construction of a cognitive niche takes place over a given system, as a form of scaffolding [to rely on Clark’s most fitting metaphor (Clark 2005)]. This provides niche users with a closed, man-made system that allows better predictions and better control over an open, highly uncertain system. Nevertheless, this does not imply that niche construction delivers a 1:1 control over the original system. Consider agriculture, which clearly instantiates a good idea of cognitive niche construction: the invention of agriculture turned the environment into a much more reliable provider of food through a vast amount of shared knowledge and heuristics, but this did not guarantee the end of famines and the optimal yield in every crop forevermore. Economics share the main goal of cognitive niche construction, which is to improve cognitive capabilities by reducing unpredictability and uncertainty: because of the shift between the niche-system and what the niche is constructed upon, assuming that economics (qua cognitive niche construction) may utterly remove elements of uncertainty is a powerful and dangerous misunderstanding. Such misconception, we will argue, can be fostered by particular structures embedded in the niche. In the economical discourse, the problem relies in the mainstream assumption that “uncertainty represents a lack (of certainty or certainty-equivalence) which prevents agents from fully informed rational optimisation, so that it is seen as anathema (particularly in financial markets)” (Dow 2015, p. 43). It should also be kept in mind that—given its strong link to a traditional idea of logicality, in which the need of formalization and quantification dominates—economic language appears to be compelled to ineluctably depicting unpredictability as an effect of “uncertainty”. In this perspective concepts like preference, utility, welfare or uncertainty are at the core of economic mainstream analysis, just because they need to be quantified. A wider attention to recent logical developments could help to favor a relaxed but more fruitful consideration of the problem of uncertainty: new non standard logical perspectives, which are no more endowed, like in the case of classical logic, with universal characteristics, could offer a better logical-epistemological framework for economic cognition. For example, the current interest in the so called “naturalization of logic” (Magnani 2015a, b; Woods 2013) and in abductive reasoning certainly stresses the urgent need of abandoning a global arithmomorphic epistemological attitude reverting to a more “anthropomorphic” one, in which logical modeling is closer to human local cognitive ways of producing inferences and argumentations in the so-called “eco-cognitive perspective.”Footnote 5
 The theory of cognitive niches is extremely valuable because it allows not only to understand human cultural development in its traditional meaning, but its frame can be extended to comprehend hyper-technological cultures as well: the situation we sketched out so far could be say to work at least until the Fifties of past century. Then something changed: so far, a cognitive niche could be described as a relationship between biota, abiota and dead organic matter. Either you are alive, and then you can be a constructor, or you are not, and then you are a constructed. What is constructible is the object of cognitive niche construction: it is the target and the materiel on which the externalization of knowledge was built. And that was it. Since the computational revolution, though, cognitive niche construction was enhanced by something that was neither a biota, nor an abiota or dead organic matter: it was the category of constructed constructors. The most important breakthrough of high-tech niche construction involves the production of more or less complicated “artificial minds” (Magnani 2007a). The notion of artificial mind can be seen as an help, or as a “maid-mind,” but the aim is the same, that is to obtain a new kind of eco-cognitive engineer that contributes to the activity of niche-construction. Virtual niches, and high-technological niches, are populated by a number of constructed constructors, that is by agencies that were constructed (or programmed) externalizing knowledge on abiota materiel, but can actively engage a more or less extended range of active behaviors within the niche. These new actors can either chiefly serve either as assessors, maintainers and mediators of existing externalizations, or as engineers of new externalizing solutions in the niche, or as full-right agents in the cognitive niche. These actors need not be “material:” those interacting within traditional cognitive niches (such as driving supporting systems) tend to be material, but they can also reside in a bit of coding, such as a data mining software, and yet be able of causing significative modification to the global structure of the niche. In all of these cases, the crucial feature is the presence of non-human cognitive agents, usually embedded within a cognitive niche, that are able to: Assess a situation. Make an appraisal. Take a decision based on that appraisal. The final decision, which is usually the contribution to the cognitive nice (for instance in the shape of an affordance) is meant to be for the good of the human user—or at least of some human users, as in the case of “intelligent” weaponry (Krishnan 2009). As we stated several times in this subsection, the revolutionary steps consisted in the assumption of non-biological material to the status of actor in a cognitive niche: it is not the same as stating that, for the first time, the new status was given to something different than a human being: animals have traditionally been actors of cognitive niches, also as assessors and decision makers (a trivial example: watchdogs are expected to be able to tell a friend from a foe), but animals are part of the biota, they are trained and not constructed, and do sometimes actively resist niche construction activity. Conversely, in high-tech cognitive niches new actors are introduced, and they are shaped precisely as their creators want them to be. Another relevant feature of high-tech cognitive niches is the presence of cyborgs (Clark 2003; Magnani 2007b). This is not the place for a discussion of cyborgs, but they are worth mentioning because not only we witness the delegation of cognitive niche construction to artificial agency, but also biological agents, the traditional constructors, are further and further hybridized with the technological artifacts, so that the limit situation could be described as a combination of automatic niche construction activity and cyborg niche construction activity. In other words, the high-tech cognitive niche could be seen as supporting artificial decision maker and hybridized (part biota and part abiota) decision maker.",4
15.0,1.0,Mind & Society,14 December 2014,https://link.springer.com/article/10.1007/s11299-014-0160-x,"Empathy, mirror neurons and SYNC",June 2016,Ryszard Praszkier,,,Male,Unknown,Unknown,Male,"We sometimes feel as if we just resonate with something or someone, and this feeling seems far beyond mere intellectual cognition. It happens in various situations, for example while watching a movie or connecting with people or groups. What is the mechanism of this “resonance”? Let’s take the example of watching and feeling a film, as movies can affect us deeply, far more than we might realize at the time. It’s intriguing to pry open the filmmaker’s workshop and take a closer look at the methods used to influence the spectators. Let’s uncover this magic, using Dr. Karen Pearlman’s book Cutting Rhythms: Shaping the Film Edit (Pearlman 2009). The author, well known for her pioneering work in articulating the underlying principles of rhythm in film is, says: Rhythm shapes cycles of tension and release by shaping time, energy, and movement through the film in patterns designed to provoke and modulate particular qualities of empathetic response. I emphasize empathetic here, because rhythm is a felt phenomenon; the spectators’ experience of rhythm [—] is an embodied, physiological, temporal, and energetic participation in the movement of images, emotions, and events in the film. (p. 62–63) The author refers then to rhythms that evoke empathetic responses. This is done by creating cycles of tension and release, synchronizing the spectator’s rhythms to the film’s pulse and its fluctuations. Synchronization (SYNC) is perceived as pivotal for shaping the film’s rhythms into a vehicle that triggers an empathetic resonance. Taking a closer look, Pearlman mentions physiology and neuroscience: The mirror neurons embedded in our brain reflect the movement and sounds seen on the screen and beef up the spectator’s empathy. More than that, a body-based, empathy-kindling path (called kinesthetic empathy) induces an inner image of movements seen onscreen. The observer essentially “internally simulates” the observed movements and, without actually moving, feels his own body configuration change in response. Both those paths (mirror neurons and kinesthetic interaction) make us experience physiological tension and release virtually simultaneously, as we perceive the movie’s patterns of intensity and relaxation; and thus we enter the universe of synchronization. According to Pearlman, by modulating tension and release, rhythm acts on the observer as a generative aspect of his or her comprehension of a film, regardless of its genre, topic or quality. The role of the editor is to determine the timing, pacing, and trajectory phrasing of its movement, and spectators’ bodies respond to this rhythm [—] and SYNC up into a physiological phenomenon of feeling with (Pearlman ibid, p. 68). The film editor’s insights are supported by the conclusions of various researchers. For example, Gallese and Guerra (2013), as a result of discoveries in neuroscience, propose a novel approach to cinema that considers cognition but also incorporates the body expression. The triad: film, body and brain, can be a new basis for understanding film theory. With the advancement of new digital technologies for analyzing movies and individuals’ reactions to them (Shimamura 2013a), there opens a new approach in cognitive neuroscience, the embodiment of which is applied to film studies (Gallese and Guerra 2012). New disciplines are emerging: Neuroaesthetics (Chatterje 2012; Kirk 2012) and Psychocinematics (Shimamura 2013b). It seems worthwhile to take a closer look at our film editor’s conclusions, reached through professional experience and intuition. Are her conjectures supported in research outside the realm of cinema? If so, does that mean that this influencing-through-rhythm mechanism works outside the movie-watching experience? And finally, captivating notion: Maybe this isn’t an isolated phenomenon. Maybe people also communicate through rhythm in everyday life? The departure point for addressing these questions will be the notion of empathy. Next we will document the neuroscience behind this phenomenon, and finally, we’ll look at the synchronization processes based on mirror neurons and empathy.",24
15.0,1.0,Mind & Society,20 January 2015,https://link.springer.com/article/10.1007/s11299-015-0161-4,Probabilities as potentially problematic,June 2016,Nicholas Rescher,,,Male,Unknown,Unknown,Male,,
15.0,1.0,Mind & Society,31 January 2015,https://link.springer.com/article/10.1007/s11299-015-0163-2,Germany versus China: How does social distance influence public good behavior?,June 2016,Dung V. Vu,,,,Unknown,Unknown,Mix,,
15.0,1.0,Mind & Society,30 January 2015,https://link.springer.com/article/10.1007/s11299-015-0164-1,More than 20 years of chaos in economics,June 2016,Marisa Faggini,Anna Parziale,,Female,Female,Unknown,Female,"
Researchers from many different disciplines have been working for decades to understanding, predicting, and influencing the behaviours of complex systems giving birth to a new and exciting domain of research called complexity science. Also some economists have been involved in this change of perspective starting from recognition that economies are complex systems. They are composed of a large number of interacting components and of the relationships between them. The goal of complex systems research is to explain in a multidisciplinary way how complex and adaptive behavior can arise in systems composed of large numbers of relatively simple components, with no central control, and with complicated interactions. Not more aggregate reduced to the analysis of a single, representative, individual, ignoring by construction any form of heterogeneity and interaction, but the aggregate emerging from the local interactions of agents. Complexity theory describes how people and organizations respond to the chaos around them. Within complexity theory, chaos does not mean disarray or being out of order; rather, it means order emerging. In fact, chaos means order with no predictability. Events appear to be random; hence, confusion is erroneously inferred. It has been more than 20 years since ideas from deterministic chaos began appearing in the economics literature. Economists began to look at chaotic analyses of the late 1970s and the 1980s, including such important works as those by Medio (1979), Stutzer (1980), Benhabib and Day (1981), Day (1982), and Grandmont (1985), just to name few. A common feature of chaos models is that nonlinear dynamics tend to arise as the result of relaxing the assumptions underlying the competitive market general equilibrium approach (Faggini 2009). This interdisciplinary spread of ideas was accompanied by expectations that many major problems in the economics could be easily solved using chaos-inspired techniques. It is probably true that many early expectations for chaos have not been fulfilled. An assessment of the impact that chaotic dynamics has had on economics requires an understanding of the paradigm of research dominant in this area in which the generic method of isolation, of inclusion and exclusion, of focusing on key elements and neutralizing the rest, of simplification and idealization are applied. This paradigm is based essentially on the following assumptions: In the absence of exogenous shocks economy tends towards a determinate and intrinsically stable equilibrium as its natural end. The economic agents, resumed in the behaviour of representative agent, are described as rational calculating individual which maximize their utility or profits on the basis of complete information about the quantities, costs, prices, and demand of their products. They have extraordinary capacities, particularly concerning the area of information processing and computation. Linear models or at least the linearization of models have been traditionally preferred by economists. This is why the linear models with one solution or one equilibrium position can be solved explicitly without using numerical procedures. So described Economics is largely a matter of formalized thin fiction and has little to do with the wonderful richness of the facts of the real world. The idea that markets are inherently dynamically unstable has always played a minor role in studies of economic phenomena, and this has changed only marginally with diffusion of chaos theory. This is because chaos theory has stimulated the search for a mechanism that generates observed movements in real economic data and that minimises the role of exogenous shocks. If stochastic models explain many of these sudden fluctuations caused by external random shocks, in a chaotic system these abrupt fluctuations are considered to be internally generated as part of the deterministic process (Gilmore 1996). The fluctuations are within the system. They are the result of complex interactions among the system’s elements, and although it is difficult to predict system behaviour, the same cannot be said of the process that created it, as it is deterministic. In this sense, chaos theory represents a shift in thinking about methods for studying economic activity and in the explanation of many economic phenomena. The theory of chaos stresses that the world does not necessarily work as a linear relationship with perfectly defined or with direct relations in terms of expected proportions between causes and effects. Therefore researchers in economics and finance have been interested in testing nonlinear dependence and chaos in economic models and data. A wide variety of reasons for this interest have been suggested, including an attempt to improve the forecasting accuracy of linear time series models and to better explain the dynamics of the underlying variables of interest using a richer class of models than that permitted by limiting the set to the linear case. But chaos theory has not had the same impact in the economics as it has in the hard sciences like physics. As highlighted by Ramsey et al. (1990, p. 991) “…while empirical studies in the natural sciences are characterized by large data sets, […], data sets in applications consist of less than one thousand observations. Consequently statistical procedure designed in the former context may not appropriate in the latter”. So the search for chaos in economics has gradually became less enthusiastic, as no empirical support for the presence of chaotic behaviors in economics has been found. The literature did not provide a solid support for chaos as a consequence of the high noise level that exists in most economic time series, the relatively small sample sizes of data, and the weak robustness of chaos tests for these data. 
In economics data sets are the outcome of a complex process including institutional or structural changes and monetary regime switches, shocks, wars, political crises etc. The rich nature as well as the impact of those changes reveals interesting features in time series (structural instability and nonlinearity) that needs to be studied by developing new techniques, able to filter these complex dynamics (Kyrtsou and Vorlow 2009). The following sections contain a survey of empirical works on economic and financial data performed to uncover evidence of chaotic dynamics. The aim of this paper is to show why chaotic processes in the economic data series has proved to be problematic.",7
15.0,1.0,Mind & Society,21 June 2015,https://link.springer.com/article/10.1007/s11299-015-0169-9,Emergence of complex social behaviors from the canonical consumption model,June 2016,Fausto Cavalli,Ahmad Naimzada,Marina Pireddu,Male,Male,Female,Mix,,
15.0,1.0,Mind & Society,24 July 2015,https://link.springer.com/article/10.1007/s11299-015-0173-0,ACACIA-ES: an agent-based modeling and simulation tool for investigating social behaviors in resource-limited two-dimensional environments,June 2016,Elisabetta Zibetti,Simon Carrignon,Nicolas Bredeche,Female,Male,Male,Mix,,
15.0,1.0,Mind & Society,17 July 2015,https://link.springer.com/article/10.1007/s11299-015-0174-z,Cycles of maximin and utilitarian policies under the veil of ignorance,June 2016,Darya V. Filatova,Sacha Bourgeois-Gironde,Jing Shao,Female,,,Mix,,
15.0,1.0,Mind & Society,26 July 2015,https://link.springer.com/article/10.1007/s11299-015-0175-y,Stubbornness as an unfortunate key to win a public debate: an illustration from sociophysics,June 2016,Serge Galam,,,Male,Unknown,Unknown,Male,"Public opinion is today a key player in most issues a society in facing. Policy makers cannot ignore the current standing of public opinion while making up a decision. Yet, the underlining mechanisms driving opinion dynamics are still not well understood. To single out possible artifact, biases and manipulation in the making of a public opinion is thus a major challenge to avoid eventual misconducts in tackling sensitive issues to which societies are nowadays confronted. It happens that physicists have been dealing with this topic for many years within the frame of sociophysics (Galam 2005a, 2008a, b, 2012; Stauffer et al. 2006; Castellano et al. 2009; Sznajd-Weron and Sznajd 2000; Mobilia and Redner 2003; Behera and Schweitzer 2003; Tessone et al. 2004; Gonzlez et al. 2004; Schneider and Hirtreiter 2005; Sousa and Sanchez 2006; Lambiotte and Ausloos 2007; Contucci and Ghirlanda 2007; Kulakowski and Nawojczyk 2008; Martins 2008; Martins et al. 2009). Most models deal with a dynamics of opinion formation, which follows a deterministic flow driven by repeated local discussions among agents. The flow direction is determined by the existence of attractors and tipping threshold. To produce a threshold dynamics for two competing opinions A and B requires to have at least two attractors aA et aB separated by a tipping threshold pc. The dynamics landscape is asymmetric with respect to both opinions and ac is not necessarily located at 50 %. Initial supports determine which opinion eventually wins the associated public debate. For each opinion, to start from an initial public support beyond its tipping threshold is therefore what matters for this opinion to win a public debate (Galam 2002). Once this goal is reached, even if ac, the internal dynamics of the debate does the work for bringing that opinion over 50 %. While several discrete models exist, each one being rooted on a psycho-sociological hypothesis, they were shown to belong to one single probabilistic sequential scheme (Galam 2005b). The sociophysical approach was proven realistic with the successful prediction of a highly improbable political vote outcome in 2005 (Galam 2005a). The prediction was made several months ahead of the actual vote against all polls and analyses forecasts (Galam 2005c). In this paper we investigate means to reshape the topology of an opinion dynamics to suppress the tipping threshold, which in turn makes irrelevant the initial conditions. One given opinion is predetermined to win the associated debate before it starts since only one single attractor drives the opinion dynamics. The challenge is investigated using the Galam sequential probabilistic majority two state opinion dynamics model (Galam 2002, 2005a). It is found that to restrict interactions to pairs or to include inflexible agents does suppress the tipping threshold to make the dynamics free of the initial supports. The results shed a new, counter intuitive land disturbing light to design winning strategies in public issues. The case of the global warming issue is discussed within that frame (Galam 2008a, b; Gerlich and Tscheuschner 2009; Knutti 2008; IPCC, www.ipcc.ch; Wikipedia (a); Wikipedia (b)).",16
15.0,1.0,Mind & Society,19 September 2015,https://link.springer.com/article/10.1007/s11299-015-0183-y,The simplicity of complex agents: a Contextual Action Framework for Computational Agents,June 2016,Corinna Elsenbroich,Harko Verhagen,,Female,Male,Unknown,Mix,,
15.0,1.0,Mind & Society,30 January 2016,https://link.springer.com/article/10.1007/s11299-016-0189-0,"Wolfram Elsner, Torsten Heinrich, and Henning Schwardt: The microeconomics of complex economies: evolutionary, institutional, neoclassical and complexity perspectives",June 2016,Marco Novarese,,,Male,Unknown,Unknown,Male,,1
15.0,2.0,Mind & Society,31 December 2015,https://link.springer.com/article/10.1007/s11299-015-0186-8,Inference from the best systematization,November 2016,Nicholas Rescher,,,Male,Unknown,Unknown,Male,"In recent decades, “inference to the best explanation” has become a very fashionable mode of reasoning in epistemology, metaphysics, and especially the philosophy of science.Footnote 1 It is predicated on the idea that the account which affords the best explanation of some fact is thereby to be endorsed as correct. And on its basis we are enjoined to accept as actual that one among the available possibilities which best explains the facts. Much of the present-day espousal of scientific realism—with its insistence on the reality of unobservable entities such as subatomic particles—has found its prime support in the idea that the reality of such objects is to be inferred from the fact of their figuring in what we regard as the best available explanation of the observable facts. However, while such a procedure of “inference to the best explanation” seems exerts much appeal on first view, on closer scrutiny it encounters serious difficulties. Let us begin at the beginning here. The format of an “inference to the best explanation” is as follows: For example, when we want to explain why it is that the light went out, there are among the available prospects such explanations as: “someone turned it off.” “there was a power failure”, “some other appliance blew the fuse,” etc. Now it is, presumably, the first of these that offers the best explanation under the circumstances, and so the principle at issue would have it that this explanation is to be inferred as providing the answer to our question. But absent further information, this seems decidedly premature. There is surely something overly optimistic to the idea that we are entitled to maintain that the best alternative among the available explanations of some fact is correct. The senior housemaid was killed in the drawing room with a blunt instrument. Who did it? The best explanation points to the butler. He had motive (she had upset him), means (the fire poker), and opportunity (he was in the drawing room at some point around the time of the murder). His committing the crime is its best explanation: with none of the others in the house do we know for sure that those three crucial factor are in place. But even though the butler’s guilt is our best available explanation, it may well be something we would be ill advised to conclude. All too readily, the best available explanation could miss the boat. The best (and indeed correct) explanation of the fact that Abraham Lincoln died on April 15, 1865 is that he was shot by John Wilkes Booth on the day before. But it would be a daring theorist who would propose inferring this best explanation from that given fact in and of itself. Or consider another case. A plane crashes. The board of inquiry issues its report. Its bottom line is that there are various potential explanations: mechanical failure with a probability of 45 %, human error with a probability of 35 %, sabotage with 10 %, and other possibilities with an aggregate probability of 10 %. And so it emerges that mechanical failure looks to be the best single explanation. But this would hardly suffice to warrant our accepting this explanation, considering that in the circumstances it may even be more likely false than not.Footnote 2 In the end, there is no good reason to think that the explanation is true—or even highly practicable. In the end, even with best explanations we have some daunting problems. After all, that explanation may be best alright, but the best of a very bad lot. Or again, it may be optimal with respect to one fact, but the best explanation of another fact may require its denial. Moreover, it is far from clear what the best explanation is. One explanation can be better than another only in this or that regard, and this plurality of considerations cannot be transmuted into a single overall aggregation. Thus consider the following factors, each of which is critical to the merit of explanations: The security of the explanatory premisses. The tightness of the reasoning that links the explanatory premisses to the facts being explained; for instance, whether airtight demonstrability is at issue or merely probabilistic reasoning. The generality of the explanatory mechanisms used in terms of their applicability to different sorts of explanatory situations. (Avoidance of idiosyncratic ad-hoc-ness.) The naturalness or simplicity of the explanatory account; its avoidance of needless complexity.Footnote 3
 The uniformity of the explanatory proceedings through harmonization with this used in kindred problem-settings. The harmony or fit of the explanation with our broader understanding of how things work. To be sure, if “the best explanation” happens to be the only possible explanation then all is well. But this concession is clearly not particularly helpful. At the most someone may—and doubtless will—offer the following objection. “Your critique overlooks an important point. Best explanation there was devised to handle complex scientific cases—and not the sort of commonplace situations that are at issue in your simple-minded counter-examples.” But this objection clearly invites the following reply: If a theory cannot even manage to accommodate simple cases, how can one possibly expect it to be adequate when more complex situations are at issue.",1
15.0,2.0,Mind & Society,11 July 2015,https://link.springer.com/article/10.1007/s11299-015-0172-1,Information-driven network analysis: evolving the “complex networks” paradigm,November 2016,Remo Pareschi,Francesca Arcelli Fontana,,Male,Female,Unknown,Mix,,
15.0,2.0,Mind & Society,18 September 2015,https://link.springer.com/article/10.1007/s11299-015-0182-z,Sleep and the management of alertness,November 2016,Tinna Laufey Ásgeirsdóttir,Sigurður Páll Ólafsson,Gylfi Zoega,Female,Male,Male,Mix,,
15.0,2.0,Mind & Society,03 October 2016,https://link.springer.com/article/10.1007/s11299-016-0195-2,Symposium on “Multidimensional subjective well-being”,November 2016,Riccardo Viale,,,Male,Unknown,Unknown,Male,,
15.0,2.0,Mind & Society,23 November 2015,https://link.springer.com/article/10.1007/s11299-015-0185-9,A preliminary investigation about the relationship between well-being and fertility status in different menstrual cycle phases,November 2016,Paola Iannello,Daniela Villani,Gaia Bruschi,Female,Female,Female,Female,"The ovulatory cycle is a monthly occurrence for most women for a substantial part of their life (Bunting and Boivin 2008). The cycle is marked by specific hormonal changes and a growing body of research indicates that the female ovulatory cycle has important consequences on how women think, feel and behave (see Neave 2008). Across different phases of the monthly ovulatory cycle women live nonconscious shifts (Durante et al. 2011) that have been studied in relation to several behaviors. For example, a number of studies found that women are attracted to men with different features across the ovulatory cycle. Specifically, at peak fertility (near ovulation) women are attracted to potential sex partners who show classic biological indicators of male genetic fitness (e.g., symmetry, facial masculinity, vocal masculinity, smell, stature, etc.; Frost 1994; Johnston et al. 2001; Penton-Voak and Perrett 2000; Penton-Voak et al. 1999; Feinberg et al. 2006; Pawlowski and Jasienska 2005). These nonconscious changes in preferences for masculine men are potentially adaptive and could be explained at the light of the Ovulatory Shift Hypothesis (Gangestad and Thornhill 1998; Gangestad 2008). This hypothesis proposes that natural selection may shape aspects of women’s psychology to shift during the brief window within each cycle when conception is possible. According to this evolutionary logic, several authors have verified the existence of other shifts, not directly related to the sexual and reproductive goals. These changes go either to influence specific behaviors, such as products choice (Durante et al. 2011), voting preference and political attitudes (Durante et al. 2013), risk-taking behavior (Iannello et al. 2015), and the evaluation that women have of themselves (Hill and Durante 2009). In particular, Hill and Durante (2009) found that women experience an intraindividual self-esteem decrease at high fertility. Authors explained this fluctuation in self-esteem stating that women may feel worse when they should appear at their best to get the reproductive success. Nevertheless, it is important to underline that this relationship is not necessarily a causal one (Antonietti and Iannello 2011) and that there could be several psychological changes responsible for the relationship between self-esteem and fertility. Among the psychological changes that have been linked to hormonal changes during the menstrual cycle there are significant emotional and physical symptoms. Even if no consensus exists about the association between mood changes and menstrual cycle, some authors found changes related to anxiety, depression, irritability (Bains and Slade 1988; Corney and Stanton 1991; Ainscough 1990; Laessle et al. 1990) and how women respond to stress (De Ronchi et al. 2000). These results are primarily related to the pre-menstrual phase that occurs the few days prior to menstruation (Ramacharan et al. 1992). In general, negative affect is more commonly experienced when estrogen levels are declining or low (Cockerill et al. 1994), such as within follicular phase, whereas positive affect is associated with high estrogen and progesterone levels (Wang and Johnston 1993), such as in luteal phase. Despite researchers’ attention began to focus on the implications of hormonal fluctuations across the menstrual cycle for subjective well-being, generally employing measures of individuals’ experience of positive and negative affect (Karademas 2007), to our knowledge, no research exists directly addressing the implications for others aspects of well-being. Subjective well-being (SWB) concerns people’s cognitive and affective evaluations of the quality of their lives (Diener et al. 1998). Specifically, the affective (emotional) component is related to the balance between positive and negative affect, and the cognitive component is related to judgments about one’s life satisfaction. SWB does not reflect a stable inner state of well-being; rather, it can vary according to context effects (Schwarz and Strack 1999). Within the study of well-being, however, several authors have claimed that positive functioning should be conceived in terms of psychological well-being (PWB) (Keyes et al. 2002; Ryff 1989; Ryff and Keyes 1995), which concerns the full growth and self-realization of the individual through the resolution of existential life challenges. This multidimensional model of well-being (Ryff 1989; Ryff and Keyes 1995) comprises six distinct components of positive psychological functioning: being independent and self-determining (autonomy), having a sense of control over the external world (environmental mastery), maintaining positive relationships with others (positive relations), finding meaning in one’s life (purpose in life), seeing oneself as growing and expanding (personal growth), and feeling good about oneself (self-acceptance). Keyes and colleagues have found that subjective and psychological well-being represent distinct, though correlated, components of individuals’ well-being (Keyes et al. 2002). The relevance of the menstrual cycle to the level of well-being of women remains an open issue and represents the goal of this preliminary study. Specifically, the present study aims at exploring whether the level of well-being varies in different menstrual cycle phases, that is fertile and non-fertile phases. We investigated well-being through a multidimensional assessment of well-being that included: (1) the cognitive component of SWB related to judgments about one’s life satisfaction, (2) the psychological well-being (PWB) concerning the full growth and self-realization of the individual in several components of positive psychological functioning, and (3) self-esteem, that can be defined as the personal judgment of overall self-worth and is recognized as one indicator of well-being (e.g., Harter 1993; Baumeister et al. 2003; Mann et al. 2004).",1
15.0,2.0,Mind & Society,25 August 2015,https://link.springer.com/article/10.1007/s11299-015-0180-1,The meaning of happiness: attention and time perception,November 2016,Viviana Di Giovinazzo,Marco Novarese,,Female,Male,Unknown,Mix,,
15.0,2.0,Mind & Society,08 August 2016,https://link.springer.com/article/10.1007/s11299-016-0194-3,"The subjective well-being of women in Europe: children, work and employment protection legislation",November 2016,Tatiana Karabchuk,,,Female,Unknown,Unknown,Female,"A sharp decrease in fertility rates within the last 3–4 decades and an aging population in many European countriesFootnote 1 urges us to pay attention to the issue of the subjective well-being of women who usually take care of children and combine family responsibilities with employment. Moreover, women play a crucial role in family planning and the decisions to have children. The focus of this paper is to look at the factors of subjective well-being of mothers with a comparison with non-mothers in order to contribute to family policy development in countries with low fertility rates. Improvement of subjective well-being might be one of the ways to stimulate further childbirth and family growth and prosperity. Subjective well-being has many dimensions and is affected by many aspects of a person’s life, wherein work and the raising of children are the most important. On the one hand, subjective well-being is strongly determined by job satisfaction and self-fulfillment at the workplace (Kalleberg 1977; Rode 2004). Indeed the increasing numbers of working mothers in the world labor markets speak to the importance of work itself and job satisfaction for women (Hanson and Sloane 1992; Francesconi 2002; Gregg and Washbrook 2003; Hofferth and Curtin 2003; Dex et al. 2005; Berger et al. 2005). On the other hand, home/family life contribute to females’ satisfaction not less than work (Kiecolt 2003). Moreover, previous studies showed that parenthood by itself increases subjective well-being (Frey and Stutzer 2006; Haller and Hadler 2006; Pollmann-Schult 2014). Childless women report significantly lower life satisfaction and self-esteem while motherhood is strongly associated with positive significant gains in subjective well-being (Hansen et al. 2009; Hansen 2012; Baetschmann et al. 2012; Nelson et al 2013). Thus we can suggest that the lack of one of these two elements (work or children) results in a lowering of females’ subjective well-being (Kravdal 2014). Unpleasant experiences from a stressful return back to work after maternity leave may discourage women from having more children. Therefore, it is important to pay attention to the subjective well-being of working mothers in particular. Often female workers became vulnerable group in the labour market after childbirth, as employers consider human capital and productivity of mothers to be less efficient compared with men or non-mothers (Rogers and May 2003; Hill et al. 2004a, b; Domenech 2005). Additionally, mothers are usually paid less because of either fewer working hours or wage discrimination (Waldfogel 1997; Budig and England 2001; Anderson et al. 2002, 2003; Napari 2007). On top of this, women often sacrifice their jobs and careers for their families. They resign; change professions; shift to other positions; reduce working hours and agree to lower wages. These job changes, unfair treatment and wage cuts may cause a decrease of subjective well-being. The issue discussed here is highly relevant for all countries, not only for Europe. Family policies like child allowance, parental leave policies, and child-care provisions and etc. could help in solving the work-family conflict and increase the subjective well-being of mothers (Hoem and Hoem 1989; Kravdal 1992; Parasuraman and Simmers 2001; Francesconi 2002; Hill et al. 2004a, b; Kalwij 2010; Rindfuss et al. 2010). Along with particular family policy programs there are main labour market regulations that define overall job stability through the level of security provided to the employed and the level of support to the unemployed (Sjöberg 2010). However, employment protection regulations are often cited as the main cause for the large cross-country differences in labor market performance and labour market rigidity (OECD 2004), previous empirical research on fertility and work-family balance was concentrated predominantly on the family policies outcomes and much less on the labour regulations themselves. This study is seeking to fill the gap and focuses on employment protection legislation’s effect on mothers and non-mother subjective well-being across Europe. Life satisfaction, happiness and subjective well-being were drastically investigated within last 15 years (Easterlin 2003; Veenhoven and Hagerty 2006; Kahneman and Krueger 2006; Frey and Stutzer 2006; Haller and Hadler 2006; Inglehart et al. 2008; Sarracino 2012, 2014; Bartolini and Sarracino 2014; Pollmann-Schult 2014; Stanca 2016). At the same time little attention was given to the subjective well-being of women in the cross-national perspective with respect to labor market regulations. This paper helps to shed light on the determinants of the subjective well-being of working and not working mothers and non-mothers under different employment protection legislation. In which European countries are the working mothers better off in comparison to working non-mothers: under more liberal or stricter employment legislation? Thus, the paper adds to the literature on subjective well-being, work-family conflict and employment protection legislation discussions.",5
15.0,2.0,Mind & Society,03 August 2016,https://link.springer.com/article/10.1007/s11299-016-0193-4,Nudge to the future: capitalizing on illusory superiority bias to mitigate temporal discounting,November 2016,Davide Pietroni,Sibylla Verdi Hughes,,Male,Female,Unknown,Mix,,
15.0,2.0,Mind & Society,12 September 2015,https://link.springer.com/article/10.1007/s11299-015-0181-0,"Well-being, happiness and the structural crisis of neoliberalism: an interdisciplinary analysis through the lenses of emotions",November 2016,Marc Pilkington,,,Male,Unknown,Unknown,Male,"For some, neoliberalism is simply a form of market fundamentalism as well as a political project (Strauss 2013, p. 2) i.e. the outcome of the free-market revolution originating in the 1970s and the 1980s (Peck 2010, p. xi). Neoliberalism is the by-product of the faltering aura and reduced effectiveness of post-war Keynesianism, also known as embedded liberalism. Neoliberalism remains a rather diffuse doctrine that aims to renovate classical liberalism, by restoring or maintaining the free interplay of market forces, while accepting a certain degree of State intervention. Neoliberalism is in the first instance a theory of political economic practices that proposes that human well-being can best be advanced by liberating individual entrepreneurial freedoms and skills within an institutional framework characterized by strong private property rights, free markets and free trade. The role of the state is to create and preserve an institutional framework appropriate to such practices. It must set up military, defence, police and legal structures and functions required to secure private property rights and to guarantee the proper functioning of markets. If markets do not exist, they must be created, by state action. But beyond these tasks, the state should not venture. State interventions in markets (once created) must be kept to a bare minimum because the state cannot possess enough information to second-guess market signals (prices); powerful interest groups will inevitably distort and bias state interventions (particularly in democracies) for their own benefit (Harvey 2005, p. 2). Neoliberalism is not merely an intellectual edifice, but a continuous process of constructing a shifting reality, that of neoliberalization (Peck 2010, p. xiii). Scholars thus constantly trace new «threads of connection across the polymorphic phenomenon of neoliberalism» (ibid., p. xvii). In this article, we analyze the links between emotions, culture and social well-being before examining the learning lessons of the global financial crisis (GFC) within the framework of a renewed conceptual articulation between social well-being, the perception of emotions in others, and finally the emotional substrate of contemporary finance. Eventually, we hope that these insights will help delineate the structural crisis of neoliberalism.",4
16.0,1.0,Mind & Society,21 December 2015,https://link.springer.com/article/10.1007/s11299-015-0187-7,Randomness: off with its heads (and tails),November 2017,Aleksandar Aksentijevic,,,Male,Unknown,Unknown,Male,"Randomness plays an important role in a number of theoretical and applied contexts, from experimental design and quantum mechanics to games of chance (Noether 1987). A positive conceptualization of randomness allows scientists to treat noise as a distinct quantity and remove bias. Yet, the debate on the use of randomness in science and psychology has been characterized by a lack of understanding of its meaning and its manifestations. This could be because randomness represents a negative abstract idealization that is completely inaccessible to the human mind. By its very definition, it lies outside the domain of the knowable. It is the assumption of infinity that is responsible for much of the confusion. Randomness, like infinity, can only be defined in terms of its opposites (Falk 1991). Positing an infinite set or sequence involves permanent expansion of a finite concept which results in paradoxes (Falk 2010). Infinity cannot be grasped in its simultaneity because it has no boundaries. As noted by Péter (1957/1976), “the infinite in mathematics is conceivable by finite tools” (p. 51). At the same time, abstract definitions of infinity (which are easy to manipulate mathematically) are taken as the benchmark against which subjective randomness behavior is judged. Recent psychological evidence suggests that the mismatch between subjective and objective definitions of randomness tends to disappear once the unrealistic premise of infinity is removed (Hahn and Warren 2009). According to a strict definition, it is not possible to know whether a source is random because such a source can produce any output. We cannot know anything about the properties of a truly random source—its distribution parameters for instance. A truly random process can change randomly and is completely unknowable (Chaitin 2001). The existence of randomness cannot be proved (Ayton et al. 1989; Bar-Hillel and Wagenaar 1991; Chaitin 1975). Randomness, in its strict meaning can never be achieved (Calude 2000). Randomness refers to an ideal state which is described in different ways—a state in which all symbols and combinations of symbols are mutually independent and/or represented equally, a state that is perfectly unpredictable. To illustrate, Falk (1991) defines a random process as “…a mechanism emitting symbols… with equal probabilities, independently of each other and of previous history of any length” (p. 215). Straight away, we encounter problems. If symbols are generated by a single agent, they can never be completely independent.Footnote 1 The defining property of randomness, namely equiprobability, implies that all possibilities (combinations of elements) of any length are equally represented. Equiprobability represents an attempt to reconcile the strict frequents definition of randomness which requires infinity and the limited informational reach of the human observer. It cannot be achieved in practice because it relies on infinity. Specifically, for finite sequences of reasonable complexity it is not possible for all elements (bigrams, trigrams, n-grams) to be equally represented (Hahn and Warren 2009). A strict definition of randomness involving infinity cannot be used to create probabilistic models, which must be sufficiently complex to simulate the richness of real-life processes while remaining simple enough to be amenable to mathematical treatment (Volchan 2002). Shannon’s information theory (Shannon 1948) posits a known random source. Yet, if the source is known, it cannot be random. If a particular probability distribution is adopted, the source is constrained in a way that negates its randomness. In order to make any sense of presumably random data, one has to assume that the source does not change in any appreciable way during production. This was clearly illustrated by Attneave (1959, p. 13) who admitted that it would be impossible to say anything about a message if the generating probability distribution changed over time (i.e. if it was not ergodic). The infinity requirement informs the distinction between random processes and random products (e.g. Brown 1957; Nickerson 2002). Some authors define randomness as a property of the generating process (Wagenaar 1991). As shown above, defining a random process is strewn with unknowns. What about the output? According to Falk (1991), it is not possible to give a positive definition of the output of a random process. Lopes (1982) pointed out the problem which faces anyone who has ever had to sample a random sequence, namely, should one accept the output of a presumably random process even if the output itself is structured? The paradox of engineering randomness has been highlighted by Gardner (1989): “The closer we get to an absolutely patternless series, the closer we get to a type of pattern so rare that if we came on such a series we would suspect it had been carefully constructed by a mathematician rather than produced by a random procedure” (p. 165). The process/output distinction is ultimately unhelpful because of the absence of a tractable relationship between the two domains. Seemingly random outputs can be produced by deterministic processes. Here, a well-known example is the decimal expansion of pi, which successfully passes most tests of randomness (Jaditz 2000). Conversely, random (or highly complex) processes can and do produce structured outputs. A similar problem exists in the domain of algorithmic complexity (Li and Vitanyi 1997). In a recent review of complexity research (Aksentijevic and Gibson 2012b), it has been suggested that algorithms do not represent a useful starting point in complexity research because apparently simple algorithms can produce complex outputs and vice versa. If mathematical randomness represents a negative idealization that is completely inaccessible to the observer, what about the output of random number generators? It is clear that such programs increase the complexity of the generating process to the point at which no pattern, order or regularity can be detected within a time window of reasonable size. Eagle (2005) described this more precisely: “This is the idea that in constructing statistical tests and random number generators, the first thing to be considered is the kinds of patterns that one wants the random object to avoid instantiating” (p. 10). As long as there is any connection between the human agent and the process, the process cannot be called random, because its output will always contain faint echoes of structure (e.g. Lopes 1982, p. 629). To recapitulate, randomness in its strict meaning can never be related to the human observer because it represents a transcendental abstraction involving infinity.Footnote 2 The difficulty in reconciling the mathematical concept of randomness and subjective complexity has led to the gradual weakening of the former. In order to be used at all, processes under investigation have to be simple and ergodic. Departures from simplifying constraints complicate mathematical treatment to the extent that the models become unusable. It is easy to see that the formulation of stochastic models is governed by the limited ability of the human observer (statistician) to assimilate and describe long and/or complex patterns. Theoretical distributions represent simplified idealizations of imperfect and noisy empirical information. As the sample size increases, individual cases lose on importance and the data are eventually fitted to one of a number of smooth, mathematically and cognitively tractable statistical models (Hammond and Householder 1962). Somewhat paradoxically, such human creations are taken as objective reference in randomness research. Randomness is used in science to denote disorder and absence of pattern or meaning. As argued in this section, numerous investigations of subjective randomness behavior have come to the same conclusion. Specifically, human observers associate randomness with high complexity. Two well-documented phenomena reflecting the mismatch between subjective predictive behavior and objective definitions of randomness are the “hot hand” (Bar-Eli et al. 2006; Gilovich et al. 1985) and “gambler’s fallacy” (e.g. Ayton and Fischer 2004; Tune 1964). These two biases reflect the tendency respectively to overestimate and underestimate the length of a sequence of identical outcomes relative to that predicted by some probabilistic model and are often used to illustrate the lack of statistical sophistication of observers faced with highly complex situations (Alter and Oppenheimer 2006). They reflect the two complementary facets of human response to complexity and are both underpinned by the limited analytical ability of the observer. “Hot hand” describes the belief that a player’s chances of scoring in a basketball game are greater following a hit than following a miss. In other words, runs of hits are assumed to reflect improvements in performance and not chance fluctuations. This belief is prevalent (91 % of people questioned by Gilovich et al. subscribed to it) and persistent despite the overwhelming evidence that no pattern could be discerned in the statistical analysis (e.g. Dawes 1988). Yet, a game of basketball is not a random process governed by chance. A large number of highly interrelated factors conspire to generate outcomes that might or might not produce small-scale patterning accessible to human observers. Players do experience surges of concentration which can improve performance (Burns 2004; Gilden and Wilson 1995). Observers are aware of some of these factors and base their predictions on their limited ability to cope with complexity. More precisely, they apply small-scale patterning to a large-scale context. They cannot comprehend the totality of the situation and naturally seek meaning in partial causal snapshots (Keren and Lewis 1994). Gambler’s fallacy describes the opposite tendency of gamblers in a game of roulette to bet on red after the run of blacks and vice versa and is also called negative recency (Bar-Hillel and Wagenaar 1991; Rapoport and Budescu 1997).Footnote 3
 These behaviors are not caused by some peculiarity of the human cognitive system but reflect a misunderstanding of the relationship between the observer and the abstract concept of randomness. There is nothing unsophisticated or pathological about favoring patterns (hot hand) or associating randomness with change (gambler’s fallacy). The fact that the processes under consideration are too complex to be described in terms of simple patterns should not be viewed as a shortcoming because the urge to find order in complexity is the prime motivation for science and other socially desirable forms of abstraction. The two biases can be explained in terms of a (sensible) strategy of meaning seeking. When presented with complex information, observers search for clues that could help them assimilate it. One important clue is the source of information. Patterned information is correctly associated with human generators and disordered information, again correctly, with non-human sources (coins, roulette; Ayton and Fischer 2004; Boynton 2003; Burns and Corpus 2004; Matthews 2013). Human observers fare equally poorly in the numerous studies of subjective randomness. Since the 1950s, psychologists have devoted a great deal of effort to investigating subjective response to randomness (see e.g. Oskarsson et al. 2009 for a review). The results of this research can be summarized thus: humans are poor at perceiving, understanding and producing randomness. In the words of Gilovich: “People’s intuitive conceptions of randomness depart systematically from the laws of chance” (Gilovich et al. 1985, p. 296). According to Hahn and Warren (2009), observers associate randomness with irregularity (disorder; see also Kahneman and Tversky 1972), equiprobability and alternation. These three properties are indicative of the decreased tolerance for long runs (gambler’s fallacy) in response to a non-human agent. If we remove the ideal of perfect unpredictability, they also provide a good description of a random pattern. Disorder is an important factor in objective measures of entropy and randomness (e.g. Feynman et al. 1963). Human predilection for pattern which is at the core of the biases described above, underpins perception and cognition. It also explains the striving for equiprobability (which incidentally is demanded by mathematical definitions of randomness). Humans conceive of randomness in terms of change which involves a balanced yet irregular representation of different symbols within a short time window. This is the cause of negative recency (gambler’s fallacy) effects (e.g. Tyzska et al. 2008). Unequal representation of different outcomes implies imbalance and patterning—after all, this is how informational redundancy is objectively defined and computed (see Attneave 1959). Finally, the subjective concept of complexity as change explains observers’ predilection for alternation. With the exception of strictly periodic outliers (e.g. 01010101…) simple patterns tend to contain few runs or alternations (e.g. 0000001 as opposed to 01100101). To summarize, observers associate randomness with complexity, that is, irregular change (Aksentijevic and Gibson 2012a). As shown further in the text, the notion of change underpins complexity and ultimately randomness.",3
16.0,1.0,Mind & Society,18 January 2016,https://link.springer.com/article/10.1007/s11299-016-0188-1,A recap on Italian neurolaw: epistemological and ethical issues,November 2017,Elisabetta Sirgiovanni,Gilberto Corbellini,Cinzia Caporale,Female,Male,Female,Mix,,
16.0,1.0,Mind & Society,07 June 2016,https://link.springer.com/article/10.1007/s11299-016-0190-7,Xenophobia is really that: a (rational) fear of the stranger,November 2017,Guido Ortona,,,Male,Unknown,Unknown,Male,"Xenophobic attitudes and xenophobic behaviours are very common, and have always been. The intensity of attitudes and the seriousness of behaviours apparently span through an enormous range. It’s a general feeling that there is something common that lies behind the tut-tutting of a British gentleman who observes a rave music party of young migrants as well as behind the hate of, for instance, Muslims towards Hindus (and vice versa) in Gandhi’s India; and that there is something that explains both the jokes against the Irish in England, and the suicide bombings against the Jews in Israel. Examples are uncountable, with reference both to attitudes and actions. If we try moving from the common feeling towards a rigorous explanation, we face from the very start two serious theoretical problems. The first is that the common explanation we are looking for should explain the quasi-universality of negative attitudes and of aggressive behaviours towards the aliens, while at the same time providing an explanation for their differences (which are extremely large, as illustrated by the previous examples). In other terms, the common explanation we are looking for must be able not only to explain why—for instance—Hutus hate and often killed Tutsis, and vice versa, but also why members of some peoples hate members of other ones, but do not kill them, and also why members of some peoples do not hate members of other ones, but prefer not to make business with them, and finally why members of some peoples like members of some others. The second problem is that our explanation, provided that we may find one, must explain why the members of an ethnic group should avoid, or despise, or hate, or even kill the members of another one as such, i.e. in absence of a personal conflict. If a member of the ethnic group A hates a member of the ethnic group B because he stole his wallet, this has little to do with xenophobia; what we should be able to explain is another general case, that of a member of ethnic group A who hates a member of the ethnic group B simply because s/he belongs to it, even if there is no chance that the second may steal something from the first. A possible explanation is biological, or to be more precise sociobiological. The argument goes this way. Given our past of social animals living in hordes, we are programmed to be highly cooperative with the members of our horde, and to be highly competitive towards the members of other ones. These attitudes developed into ethnocentrism on one side, and into xenophobia on the other. This explanation is the one that we most frequently meet among laypersons and in “laymedia”; but we will see in next section that this explanation fails both the tests we require for an overall theory of xenophobia, i.e. that of the explanation of a variety of behaviours and that of the hostility without a specific conflict. In Sect. 3 we will provide a definition of xenophobia, and in Sect. 4 we will provide a behavioural economic explanation for it. In the following sections we will discuss with more detail the case of the basic xenophobia of the layperson, that I will label “white noise xenophobia”. A brief conclusive section is devoted to policy and political issues.",4
16.0,1.0,Mind & Society,21 June 2016,https://link.springer.com/article/10.1007/s11299-016-0191-6,How reason confronts experience: on naturalist accounts of reason,November 2017,Sheldon J. Chow,,,Male,Unknown,Unknown,Male,"The main purpose of this paper is to explore several aspects of naturalist accounts of reason.Footnote 1 Through this exploration, I hope to motivate the naturalist project, as well as reveal some of its important consequences. Rather than offer a thorough survey of the literature, however, my tack will be to focus on a select few naturalist treatments of reason.Footnote 2 This will allow me to pick out (and pick on) specific issues of interest. To this end, I will concentrate the discussion on some recent works by Hooker (1994, 1995, 2011), Hoffmaster and Hooker (2009), Morton (2010, 2012) and Bishop and Trout (2005, 2008). There are, to be sure, various other influential authors whose work can contribute to the discussion. But I have chosen the indicated authors in particular to serve the overarching aims of this paper. Let me briefly explain. First, Hooker’s effort at developing a naturalistic philosophy for scientific and quotidian reason is formidable and one of the most developed in current philosophy. Despite this, however, Hooker’s work is not often addressed in the reason and rationality literature. I hope to initiate corrective measures to this. Interestingly, with Barry Hoffmaster, Hooker has recently expanded his naturalism to encompass moral reason and moral epistemology (Hoffmaster and Hooker 2009). This latter work is particularly important, since Hooker there applies his naturalistic theory of reason to a real life example of moral decision-making. Theorists of both naturalistic and non-naturalistic stripes often talk of how their theories would apply in concrete circumstances, and they sometimes provide empirical data and offer various thought experiments to confirm their claims, but it’s rare that any philosopher addresses and applies her theory to an actual case in all its complexity. Hoffmaster and Hooker’s work thus presents a unique opportunity to examine and assess a thoroughgoing naturalism applied to a concrete, complex case. As we will see, the complexity of many real life problems has important consequences for naturalism about reason, and I shall explore these with an eye to sketching a novel take on the nature of problem-solving. Second, Morton’s naturalism offers a distinctive characterization of reason in terms of intellectual virtue. Of course, virtue theory applied to reason and epistemology has a growing history of its own. Morton’s work is unique, however, since it presents an externalist naturalistic framework within which he attempts to deal with (if I may steal a phrase) the buzzing and blooming confusion of many real life complex problems. Although I generally refrain from invoking virtue theory terminology, I will rely heavily on Morton’s naturalism to critically evaluate Hooker’s framework. Morton’s naturalism will serve as a general critique of internalist accounts of reason, and it will form a basis from which general lessons for naturalism can be drawn. Finally, the latter part of this paper will refer to Bishop and Trout’s controversial approach to reason and epistemology. I won’t address any of the controversies, but I will use Bishop and Trout’s framework to highlight a common theme among naturalists to assess reasoning strategies in terms of reliability in producing true beliefs—their Strategic Reliabilism is explicit in this respect. My aim will be to provide a rough sketch of an analysis of problems and problem-solving, which illustrates that any notion of reliability fails when confronted with sufficiently complex problems (of which the moral decision-making case considered by Hoffmaster and Hooker is paradigmatic). I should like to be clear that this will be just a rough sketch—a fully detailed account will have to wait for another occasion. These three bodies of work provide plenty of material to serve the central aims of this paper. Again, there certainly is other research that can just as well contribute, but it is not possible to discuss here all relevant works. Indeed, a thorough survey would require a book-length manuscript. However, as we go along I will have opportunity to give remarks about how certain other research, such as Gerd Gigerenzer’s ecological rationality (e.g., Gigerenzer and Todd 1999, 2012; Todd and Gigerenzer 2012), relates to some of the central ideas developed below. This paper is organized as follows. In Sect. 2, I discuss Hoffmaster and Hooker’s moral decision-making case, which they use to motivate and expound Hooker’s account of reason. Section 3 offers a critique of Hoffmaster and Hooker’s analysis by arguing that it considers only explicit conscious experience and fails to consider underlying (generally nonconscious) psychological processes. This reveals a danger for naturalism of mischaracterizing the activities and/or cognitive processes of agents when evaluating their performance, which can cloud the targets of normative analysis. In Sect. 4, I explicate Morton’s externalist shift for naturalist theories of reason. I show that, whereas Hooker’s internalist naturalism recommends a wide range of reasoning methods to be included in a theory of rationality, externalism offers a more encompassing perspective of reason. This provides the material to reconsider the moral decision-making case discussed by Hoffmaster and Hooker in an externalist light. In Sect. 5, I give my sketch of the aforementioned analysis of problems and problem-solving to illustrate the importance of externalism, and indeed why it is needed for dealing with problems with a certain level of complexity. Section 6 gives some concluding remarks.",1
16.0,1.0,Mind & Society,01 July 2016,https://link.springer.com/article/10.1007/s11299-016-0192-5,"Disability, economic agency, and embodied cognition",November 2017,Thomas Abrams,,,Male,Unknown,Unknown,Male,"Actor-network theory (ANT) perspectives on disablement argue that disability is not an immutable attribute of problem bodies, but rather a sociomaterial assemblage, a temporary organization of myriad natural, social, material, human and nonhuman forces (Moser and Law 1999; Schillmeier 2010; Winance 2006).Footnote 1 ANT studies on economic rationality interpret market performances as network outcomes, whereby economic agencies are formed and ordered ‘in the wild’ (Callon 1998; Callon and Rabeharisoa 2008). Recently, the twin ANT projects on disability and economic agency have merged into an economic sociology of disability, whereby both embodied and economic capacities are made, enacted and distributed within socioeconomic networks (Abrams 2015; Callon 2008). In this paper, I want to suture this economic sociology of disability with some recent work in the cognitive sciences, on embodied and distributed cognition (Clark and Chalmers 1998; Hutchins 1995; Thompson and Stapleton 2009). By doing so, we are able to entertain the problem of meaning, a neglected but emerging theme in ANT literature, and something deeply important to phenomenologists, disability studies, and to disabled persons more generally. Phenomenologists, in the cognitive sciences or otherwise, want to interrogate the structure of meaningful existence; disabled persons, myself included, want a meaningful existence in independence. First, I review the ANT economic sociology of disability. Next, I introduce the so-called extended mind literature, and its phenomenological critique by Thompson and Stapleton (2009). Finally, to ground this theoretical discussion in a concrete example, I look to the example of a government-assisted savings scheme in Canada, the Registered Disability Savings Plan. I argue that the low uptake for the plan can be explained, in part, through the theory explored in this paper. I end with suggestions for future research.",
16.0,1.0,Mind & Society,03 January 2017,https://link.springer.com/article/10.1007/s11299-016-0196-1,Understanding coevolution of mind and society: institutions-as-rules and institutions-as-equilibria,November 2017,Shinji Teraji,,,Male,Unknown,Unknown,Male,"This paper discusses the interaction between individuals and institutions. Theories of institutions can be classified into two broad approaches: institutions-as-rules and institutions-as-equilibria. According to the institutions-as-rules approach, institutions are conceived as rules that guide the actions of individuals engaged in social interactions. On the other hand, the institutions-as-equilibria approach views institutions as behavioral patterns or regularities. Institutional change covers both the process of changing an existing institution and the establishment of a new institution. In any case, institutional structures and individual actions coevolve. In order to have a complete picture of institutions in interactions between structure and agency, we need to take both approaches, institutions-as-rules and institutions-as-equilibria, into consideration. Coevolution between two different systems is a key concept in this paper. Norgaard (1994) argues that social and ecological systems should be studied as coevolving systems. For Norgaard (1994), coevolution involves relationships between entities which affect the evolution of the entities. Norgaard (1994) describes the coevolutionary process as follows: Thinking of the changes in social and environmental systems over time as a process of coevolution acknowledges that cultures affect which environmental features prove fit and that environments affect which cultural feats prove it. In this sense, coevolution accepts both environmental determinisms and cultural determinism while recasting them as a selection process. (Norgaard 1994, p. 71) Norgaard (1994) criticizes economics (neoclassical and Marxist) for treating nature as a passive pool exploited by advancing technologies. Human beings adapt to their environments, actively transform them, and adapt to their transformations. Coevolution denotes the fact that evolutionary changes in one subsystem are a response to changes in the other subsystem with which it interacts. Changes in subsystems, such as knowledge, values, technologies, organizations, and environments, are analyzed as evolutionary processes. These subsystems interact and coevolve. Coevolutionary change is path dependent, which highlights the lock-in of technologies, institutions, and environments, and explains why it is difficult to escape from prevailing practices. Gene-culture coevolution refers to the interaction between cultural evolution and biological evolution of the human species (Boyd and Richerson 1985). Cultural traits have an impact on the survival and reproduction, or the fitness of individual, and are, in turn, influenced by these. Successful practices are passed through tradition, learning, and imitation. It is often refereed to as the Lamarckian aspect of cultural evolution, because it is a source of purposeful creation of variety and acquired characteristics may be passed on. Moreover, according to Vatiero (2016), there is a coevolutionary process between corporate governance (i.e., species) and politics (its environment). Nelson and Winter’s (1982) evolutionary theory is grounded on an explicit dynamic account of the interaction between mechanisms of variation (which constantly introduce variety, novelty, and heterogeneity among routines) and mechanisms of selection (which tend to reduce heterogeneity among routines). Furthermore, Nelson (1995) argues technological systems change arising through the coevolution of technologies, industrial organizations, and institutions. For Nelson (1995), this is the fundamental process underlying economic progress. As a consequence, a large variety of institutions and industrial organizations can influence the growth trajectories across advanced economies. In The Sensory Order (1952), Friedrich A. Hayek proposed a brain theory essentially based on a neural network model. The Sensory Order distinguishes between the physical order existing in the external world and our phenomenal experience of it. The subjectivity of individual knowledge finds its foundation in the construction of the mind. The mind operates by assembling new sensory data into associations with our accumulated inventory of knowledge. According to Hayek (1952), knowing the world is a classification of sensory qualities by the mind. What we know at any moment about the external world is determined by the order of the apparatus of classification which has been built up by previous sensory linkages. The differences in perceptions that we experience depend on the specific pattern of neuron firings that a given stimulus produces within various neural networks. The experiences of individuals will differ according to the pattern of neuron firings that each develops. According to Caldwell (2000, p.13), in Hayek’s papers (‘Rules, perception and intelligibility,’ ‘The theory of complex phenomena,’ and ‘Degrees of explanation’), the following three salient points emerge: Complex orders occur within a variety of phenomena, from the individual brain all the way on up to a society. Orders typically arise when elements contained in them follow abstract rules. Rules are often followed unconsciously; that is, the ‘agents’ following them (even in cases where the ‘agent’ is a human, so is capable of speech) cannot explain what the rule is or why he is following it. Hayek explicitly points to the mind-institution link. Perception involves the capacity to identify regularities or patterns. Cognitive activity functions as a mechanism of adaptation. Expectations are formed endogenously by virtue of individual adapting behavior to achieve a closer fit with external reality. Therefore, expectations will ordinary correspond to rules of conduct geared toward an individual’s successful adaptation to the environment. Rizzello and Turvani (2000) contend that institutions serve the individual cognitive capacity which develops in interaction with the institutions and determines their path-dependent character: The process of knowledge acquisition as conceived by Hayek is path-dependent. The external data and information are recognized, interpreted, and learned on the basis of cognitive paths that ‘depend’ on genetic features and the previous paths of neuronal classification. … When we are faced with new information or a new external stimulus, which we are unable to interpret immediately, we tend to refer it to similar previously experienced situations. All of these paths are both tacit and idiosyncratic, but also heterogeneous and personal, since they depend on the previous experience of the individual. (Rizzello and Turvani 2000, p. 176) Thus, human cognitive activity can be understood as an adaptive system which is input-transforming and knowledge-generating. The mind constructs a representation of the external world by organizing and interpreting sensory data. As Hayek (1952) suggests, the connections between the physiological elements are the primary phenomena which create the mental phenomena. The mind is an adaptive system interacting with and adapting to its environment by performing a multi-level classification on the stimuli it receives from the environment. Evolution establishes a set of possible patterns of connections on the basis of the organism’s history. Individuals construct mental models to interpret and produce expectations about their social environment. Mental models are defined as the “subjective perceptions (models, theories) all people possess to explain the world around them” (North 1990, p. 23). Subjectivity in human interpretation of reality is a key factor in understanding individual actions. Due to the variety of mental models, there are multiple possible framings of any given situation. There are different consequences depending on the way people frame the situation. Shared belief systems are considered as the result of individual attempts to cope with complexity with the help of simplifying mental models. Institutions facilitate social interaction itself since they restrict the individual agents concerning their dispositions to behave. The constraints in the perception of alternatives can be expected to result in some similarities of individual choices. Each agent has little motivation to deviate from such similarities as long as the consequences of similar behavior do not systemically diverge. The resulting belief systems constrain the repertoire of possible reactions to changes in the environment. Individuals can confirm or modify their mental models. This adjustment process takes place through learning. Collective learning arises from the interaction between individuals, which makes it possible the sharing of mental models. Mental models are considered a crucial factor in institutional change. This paper draws on ideas of coevolution of individual mental models and institutions. The mind is itself a complex adaptive system. At the level of the individual, the cognitive processes enable the individual to adjust one’s actions to external reality. Individuals adjust their actions to achieve a better fit with reality. In this sense, the mind is endogenous to the individual’s environment. A society is also a complex adaptive system; it is composed of a set of agents that are related to one another in a particular way. Relying on rules is a device we have learned to use because our reason is insufficient to master the detail of complex reality. If rules are recognized as recurrent patterns of behavior, individuals act according to rules of conduct. Shifts in mental models change individual actions, which in turn leads to institutional evolution. Institutional evolution is an endogenous phenomenon with a cognitive dimension. Section 2 considers the institutions-as-rules approach. According to this approach, institutions are conceived as rules that guide the actions of individuals. Institutions structure human agency. Section 3 examines the institutions-as-equilibria approach. The institutions-as-equilibria approach views institutions as behavioral patterns or regularities. Such a regularity is described as an equilibrium in strategic, non-cooperative games. Section 4 shows that institutional structures and individual actions coevolve. Institutions are both the rules that underlie individual behavior and patterns of behavior. Section 5 discusses some implications of this study. Section 6 concludes.",2
16.0,1.0,Mind & Society,16 March 2017,https://link.springer.com/article/10.1007/s11299-017-0197-8,The winner’s curse in auctions with losses,November 2017,Matteo Migheli,,,Male,Unknown,Unknown,Male,"The winner’s curse (Bazerman and Samuelson 1983; Milgrom and Weber 1982) represents an inefficiency in auction mechanisms, and an economic loss for the winner incurring it. For this reason, the economics literature has studied the issue widely in order to understand the mechanisms through which it arises. As the next section will show, not only is this phenomenon pervasive throughout all the sectors in which auctions are used, but its causes are still not precisely known. For these reasons, the present article aims to shed some light on whether the phenomenon may depend on the magnitude of the potential loss, a point which the literature has not addressed. In other words, a non-tested hypothesis which this paper aims to test is whether people are less prone to bid high, and consequently to incur the risk of overbidding, when the value, and therefore the winning bid, of the auctioned item is high. The hypothesis tested in this article is that people become more and more cautious as the potential loss increases. Indeed, relatively high bids may depend on the individual’s utility of winning per se (Malhotra 2010). However, as the damage (i.e. the winner’s curse) caused by paying more for the auction item than its market value increases, the willingness to incur such a loss should become weaker and weaker. The absolute size of the winner’s curse is likely to increase with the value of the auctioned item, and thus the willingness to overbid (and consequently the winner’s curse) should decrease with it. The reasoning behind this hypothesis follows the work of Bowman et al. (1999), Kahneman (2003) and Vendrik and Woltjer (2007), who theorise that individuals have value functions that are convex for losses. To investigate this, the present article proposes an experiment based on a modified dollar game (see Shubik 1971 for the original design and Migheli 2012 for the modified setting), in which the two treatments used are such that in one case the potential loss is much larger than in the other. This paper contributes to the extant literature in three respects: (1) it queries whether the winner’s curse depends (negatively) on the (absolute magnitude of the) potential loss; (2) it sheds light on the causes of the winner’s curse, suggesting that it may arise also from non-irrational behaviours; and (3) it uses an experimental technique in a field (auctions) in which this approach is still largely underused. The results show that the winner’s curse occurs only in the treatment in which the potential loss is small. In the other treatment, all the bids are consistent with the hypothesis of rationality and are within the range that characterises the Nash equilibrium of the modified dollar auction (Migheli 2012). We can therefore conclude that the subjects understand the magnitude of the potential loss and take their decisions accordingly, reducing their bids, and consequently the probability of incurring the winner’s curse and its intensity, when the loss is potentially high.",1
17.0,1.0,Mind & Society,15 March 2019,https://link.springer.com/article/10.1007/s11299-019-00199-z,Nudge of shared information responsibilities: a meso-economic perspective of the Italian consumer credit reform,November 2018,Umberto Filotto,Caterina Lucarelli,Nicoletta Marinelli,Male,Female,Female,Mix,,
17.0,1.0,Mind & Society,13 March 2019,https://link.springer.com/article/10.1007/s11299-019-00200-9,"Budging beliefs, nudging behaviour",November 2018,Oliver P. Hauser,Francesca Gino,Michael I. Norton,Male,Female,Male,Mix,,
17.0,1.0,Mind & Society,16 March 2019,https://link.springer.com/article/10.1007/s11299-019-00201-8,Designing effective nudges that satisfy ethical constraints: the case of environmentally responsible behaviour,November 2018,Denis Hilton,Nicolas Treich,Philippe Tendil,Male,Male,Male,Male,"Nudges have a lot to offer. They are usually inexpensive, while they can be very impactful. They are often seen as “safe bets”; if the nudge does not work, nothing seems to be harmed. They provide a promising alternative or complement to conventional regulatory instruments that often work imperfectly (Halpern 2016). They are not constraining, unlike laws and fiscal measures, and are usually simple to understand. They do not seem to raise sensitive equity issues, unlike taxes on energy which may hit the poor harder than the rich. Nudges may help people to act in line with their preferences and interests by leading them towards favoured options, yet always allow them the choice to reveal their own preferences by rejecting this option if the decision-maker so prefers. Given all these advantages, it is not surprising that nudges have attracted a lot of attention both in academia and in practice. Many countries have now created their own nudge unit, and we may expect other public actors to follow, such as municipalities. Nudges are perhaps popular because of a low-hanging fruit or even a fad phenomenon. However, their demonstrated success in domains such as savings, health, tax collection, job seeking and the environment makes their potential as policy instruments clear. It therefore seems to be an opportune moment to review what is known about the use of behavioural science in designing instruments of public policy, and to initiate a discussion about how governments and other public actors can use “nudges” in a systematic and ethical way. What makes a “good” environmental nudge? We will begin by clarifying the philosophical justifications for public policy interventions based on nudges before moving on to discuss what makes for effective and ethical environmental nudges. We discuss the behavioural effect of nudges, namely their effects on the targeted behaviour. We also discuss their heterogeneous and paradoxical effects, and identify a positive or negative “collateral” effect on the decision-maker’s wellbeing. We show how integrating ethical concerns, typically by using truthful and reliable information, into the design of nudges may not only make them more acceptable to the public, but more effective in the long run. We finally discuss ways in which governments can act to ensure that nudges respect ethical and legal constraints, and thus remain effective and socially useful.",5
17.0,1.0,Mind & Society,13 March 2019,https://link.springer.com/article/10.1007/s11299-019-00202-7,Assessing social care policy through a behavioural lens,November 2018,Adam Oliver,,,Male,Unknown,Unknown,Male,"There is a view held by many that social care, like health care, is a special good, the financing and consumption of which ought not to be left entirely to users themselves. If left entirely to the private sphere, social care will be inequitably distributed due to unaffordability and myopia, and yet its appropriate provision is perhaps crucial to the human experience. If delivered well, it can offer mobility, compassion and dignity to the relatively infirm, both young and old. If attempting to offer dignity to all people who have social care needs is not in some sense special, then we can leave social care to those who have the assets to pay for it. Those who cannot afford it would then have to rely on their families, friends or charity to help them, to an even greater extent than they do now. However, assuming that most of us believe that social care is a special good, then, ideally, a public or private insurance system needs to underpin it, preferably one in which people who are both good and bad risks are willing and able to participate so that premiums are affordable for all. However, in England the state does not provide financial support for universal social care, and private social care insurance does not exist. There are a number of reasons why insurance companies have not entered this sector: for instance, there is great uncertainty over how long people will live in the future, as well as over changing care and support needs and costs. Lack of supply is not the only issue; there is also a lack of demand for social care insurance, again for multiple possible reasons. For example, there is a general lack of understanding of how the social care system works in England, with many perhaps overestimating the extent to which care is publicly financed and provided when needed. People might often also be reluctant to address an issue, such as infirmity when older, that is unpleasant to think about, and some will prefer to take a risk than to save to pay to cover a distant and unpredictable, if potentially high, cost. The challenges facing the present and future financing and provision of social care in England and elsewhere are therefore substantial and are expected to be compounded over time by further pressures on social care from an aging population. It appears likely that more care and support will have to be provided in the future, whether from increased public spending, private contributions and/or unpaid care. Key issues, then, are how policy may be designed to counter what many believe to be profound aspects of human irrationality that cause insufficient financial planning for future needs, and to motivate the best possible provision of care from available resources. This article will endeavour to offer some food for thought on how behavioural economics might be used as an input into the social care policy discourse. My task is to outline some of the behavioural economic phenomena that might be most relevant to policy makers and practitioners in this area, to describe various policy approaches that are informed by behavioural economic findings, and to relate some broad social care policy interventions to these approaches.",
17.0,1.0,Mind & Society,23 March 2019,https://link.springer.com/article/10.1007/s11299-019-00207-2,The normative and descriptive weaknesses of behavioral economics-informed nudge: depowered paternalism and unjustified libertarianism,November 2018,Riccardo Viale,,,Male,Unknown,Unknown,Male,"A public policy intervention is classified as a nudge when it is not a coercive measure, retains freedom of choice, is based on automatic and reflex responses, does not involve methods of direct persuasion, does not significantly alter economic incentives, and does revise the context of choice according to the discoveries of behavioural economics (Thaler and Sunstein 2008; Oliver 2013). What is proposed is therefore a form of libertarian paternalism that has a dual valency. As paternalism, it aims to make up for citizens’ irrational and self-harming tendencies by “gently nudging them” to decide rationally for their own good. In its libertarian form it aims to give the last word to the outcome of the conscious and deliberative processes of the individual citizen who can always choose to resist the nudge. This paper aims to demonstrate that the nudge theory suffers from three main weaknesses stemming from its theoretical dependence on behavioural economics. The first two weaknesses endanger the paternalistic goal, whereas the third does not justify the libertarian attribute. The first weakness lies in the incomplete realistic characterisation of behavioural economics theory that is the central theoretical pillar of Nudge theory. The hypotheses that are put forward, for example the Prospect Theory (Kahneman and Tversky 1979), the Social Preference Model (Fehr and Schmidt 1999) and the Model of Impulsiveness in Consumption (Laibson 1997), while representing a significant step towards the descriptivist objective of economic behaviour, overestimate in fact the computational capacity of the human mind. They risk becoming as-if hypotheses as with the conventionalist approach of Milton Friedman (1953). Since they are not realistic hypotheses on economic behaviour, they are not able to contribute significantly to forecasting human behaviour for the prescriptive and paternalistic objectives of the policy and architecture of decision-making of the Nudge theory. The second weakness is even more relevant. The normative model of behavioural economics is neoclassical rationality. It can be applied to choices in conditions of risk, when it is possible to forecast the probability of the outcomes of one’s choices. The Heuristics & Bias programme, the theoretical pillar of behavioral economics, pursued the objective of identifying situations in which human judgement diverges from its rational indications. Errors, bias and effects are the empirical result (in some cases the experimental artefact) of such anomalies of rationality. On the contrary the real life problems are inside a complex and uncertain environment. They are typically ill-defined problems; that is, the goals are not definite; we don’t know what counts as an alternative and how many alternatives there are; it’s unclear what the consequences might be and how to estimate their probabilities and utilities. This environment might be called also as Large World (Savage 1954) and it is characterized by uncertainty. An environment is uncertain when it is not possible to know the outcomes of one’s choices and the relative probabilities. Therefore in an uncertain environment the neoclassical algorithms are no longer valid, in that they postulate the knowledge of the probability of the outputs. Since the vast majority of the situations in which a citizen is called to make decisions is characterised by uncertainty, it is not justified to resort to the indications of neoclassical rationality to judge human behaviour. Thus errors and bias, as observed in behavioural economics, are not such in many instances, but they are instead adaptive and therefore rational responses to the context of decision-making. If rationality is to be judged from an ecological point of view, the nudge theory that pursues the normative paternalistic objective of neutralising the bias and judgement errors discovered by behavioural economics and putting paternalistically the citizen on the right track, is in fact proceeding in the wrong direction. The last weakness concerns an assumption of Nudge theory that stems from behavioral economics. As Kahneman (2011) maintains, there are two systems in the mind. System 1 represents the intuitive mind, while System 2 corresponds to the analytical one. System 1 tends to make decisions quickly and unconsciously, while System 2 follows a slow and conscious process. Biases and judgement errors are mainly generated by System 1. However, System 2 may later intervene to correct them. From this premise, the Nudge theory affirms its libertarian character. Even if paternalistically the government is nudging the citizen, for example by placing him/her in a default state, it is always possible that this choice becomes reversible, because System 2 may subvert it and decide otherwise. In this sense paternalism is libertarian. In this paper I will argue that the dualism of the mind in behavioural economics does not appear to be confirmed by psychological and neurocognitive research. Moreover, even if it were correct, it would not entail the reversibility of some nudges, particularly of default states. In this way the fundamental pillar is weakened to justify the libertarian character of the nudge theory.",4
17.0,1.0,Mind & Society,13 March 2019,https://link.springer.com/article/10.1007/s11299-019-00204-5,Identifying bounded rationality with panel data: evidence from the labor markets of Italy and Germany,November 2018,Bruno Contini,Toralf Pusch,,Male,Male,Unknown,Male,"This paper is not on job changing behavior per se. Nor do we explain the process by which choices take place. It is a novel empirical exploration on field data attempting to test bounded rationality against full rationality. In a different, unpublished paper of few years ago (Contini and Morini 2007) one of us attempted to solve the identification problem using an analytical strategy that turned out to be unsuccessful: many of the results were in line with hypotheses of bounded rationality, but could have been generated also by fully rational individuals. In this study we are not suggesting a specific theory of job change behavior. Nor, we believe, is precision essential if the aim is to show that some forms of bounded rationality provide a more plausible explanation of behavior than full rationality.",1
17.0,1.0,Mind & Society,14 March 2019,https://link.springer.com/article/10.1007/s11299-019-00206-3,"Bounded rationality, scissors, crowbars, and pragmatism: reflections on Herbert Simon",November 2018,Thomas Nickles,,,Male,Unknown,Unknown,Male,"All real-world systems have bounded rationality. Most Enlightenment thinkers knew this, despite their inflated view of the powers of reason. Yet virtually all decision theorists up to Herbert Simon retained a “theological” conception of genuine rationality as requiring “omniscience,” that is, perfect knowledge of relevant future states of the world, their probabilities and a well-defined preference function that furnished corresponding utilities. While omniscience permitted mathematically elegant models, Simon showed that more realistic models are possible, with better empirical results. Simon contended that traditional rationality theory had mislocated its target. What tradition counted as rational behavior was usually impossible in real life (and thus, ironically, irrational as a goal), whereas the vast realm of real choices by humans, animals, and machines was precisely what tradition treated as nonrational or irrational. Simon turned the traditional picture inside out. In expanding the scope of rational behavior, Simon also expanded the scope of epistemology. Godlike epistemological models are exactly the wrong starting point for epistemology, for an omniscient being, already knowing everything, cannot genuinely inquire, cannot learn, and is never faced with a decision under uncertainty or even risk. It is risk and uncertainty that define the human condition—and that of all finite cognizers. Traditional economists and philosophers up through the logical positivists often regarded thinking and acting under uncertainty as something other than rational, hence, as something other than genuine reasoning worthy of study by epistemologists and cognitive scientists. Even innovative scientific work fell into the prohibited area of “context of discovery,” as opposed to “context of justification,” and similarly for the decisions of business executives. By contrast, Simon regarded our cognitive capacities as evolutionary endowments that enable us to move forward through an uncertain world. The pragmatist, John Dewey, had made “we live forward” a slogan of his position, after Kierkegaard’s “We only understand backward, but we must live forward.” Simon quite agreed. I contend that we should regard Simon as one of the great pragmatic philosophical thinkers. Like the pragmatists introduced below, Simon regarded the function of cognition as problem solving. Problem solving requires search. An economical search usually requires heuristics (Newell and Simon 1972; Simon 1973). So Simon, unlike the logical positivists that he otherwise admired, made the use of heuristics essential to rational behavior, not merely temporary scaffolding. I here mean heuristics in a broad sense. For example, most or all models are heuristic tools in that they involve shortcuts of one kind or another—simplifications, abstractions, etc.,—that aspire to guide further work. Breaking with tradition, Simon said that models were normally good enough. The basic idea of satisficing means modeling a situation and the resulting decision-making process well enough to get results sufficient to take the next step. Thanks to Simon’s mathematical skills and modeling orientation (apparent in the titles of many of his books), he was a leader in developing rigorous, testable modeling and simulation methods for the social and behavioral sciences of his day. He recognized that models often stimulate cognition by employing rhetorical tropes, analogy, metaphor, simile, etc., again a departure from the logical and probabilistic ideal of Enlightenment thinkers and from twentieth-century positivists.",
17.0,1.0,Mind & Society,14 March 2019,https://link.springer.com/article/10.1007/s11299-019-00203-6,"Herbert Simon, innovation, and heuristics",November 2018,Reza Kheirandish,Shabnam Mousavi,,,Unknown,Unknown,Mix,,
18.0,1.0,Mind & Society,16 April 2019,https://link.springer.com/article/10.1007/s11299-019-00208-1,Are measures of life satisfaction linked to admiration for celebrities?,June 2019,Mara S. Aruguete,Ho Huynh,Emilia Flint,Female,,Female,Mix,,
18.0,1.0,Mind & Society,11 May 2019,https://link.springer.com/article/10.1007/s11299-019-00214-3,Brexit behaviourally: lessons learned from the 2016 referendum,June 2019,Tessa Buchanan,,,Female,Unknown,Unknown,Female,"Three days before the EU referendum, an interview with Richard Thaler was posted on the financial information website MarketWatch.com. He acknowledged the Leave campaign was appealing to people’s “guts”, and thought few voters were engaged in cost–benefit analyses, but on balance he expected a win for Remain. “I am not a prognosticator,” he said, “but I would bet on them staying. And I think that there is a tendency, when push comes to shove, to stick with the status quo” (Thaler 2016). The aim of this study, carried out in September 2017, was to understand what behavioural lessons communicators could learn from the campaign. Harold Clarke, Matthew Goodwin and Paul Whiteley in their 2017 book ‘Brexit’ said that the outcome surprised journalists, academics and pollsters alike. The margin of victory was narrow. If only a small fraction of those who turned out (approximately 635,000 people) had cast their votes the other way, the result would have been different. In their 2008 book ‘Nudge’, Cass Sunstein and Richard Thaler say that when it comes to politics: “voters… seem to rely primarily on their Automatic System.” The assumption behind this study is not that behavioural science can fully explain the result, but rather that in a close race even marginal gains can make a difference. It asks what happened to the ‘status quo bias’. It considers the importance of messengers, the use of loss aversion in the campaign, and whether positive immigration arguments might have affected the outcome. It also notes how behavioural science was successfully used to increase turnout. The research outlined here is taken from a broader study based on the MINDSPACE framework (Table 1) that was developed in 2010 by the Cabinet Office and the Institute for Government, and co-authored by Paul Dolan, Michael Hallsworth, David Halpern, Dominic King and Ivo Vlaev. The mnemonic was intended to raise awareness among civil servants of “nine of the most robust (non-coercive) influences on our behaviour” and to encourage the take-up of behavioural science by practitioners. Each of the MINDSPACE influences was considered, and five of the lessons learned are presented here.",4
18.0,1.0,Mind & Society,16 April 2019,https://link.springer.com/article/10.1007/s11299-019-00209-0,Meta-moral cognition: an introduction,June 2019,Reena Cheruvalath,,,Female,Unknown,Unknown,Female,"Moral cognitive neuroscientists provide evidence for the involvement of brain functions in moral cognition, yet many influencing factors pose a challenge in constructing an unbiased moral judgment by the moral agent. Understanding the roles of situation, culture, emotion, and other factors in the human moral condition are relevant for moral cognitive neuroscience (Moll et al. 2005) and it invites a philosophical upkeep. Philosophical underpinnings, in the form of meta-moral analysis, can reduce the intricacies that arise while formulating a moral judgment. Although the available literature presents glimpses of meta-moral cognition, it does not give due credit to the subject. “Meta-moral cognition is the process of understanding and regulating the cognitive process regarding moral judgment” (Cheruvalath 2019). People do engage in metacognition, but not necessarily in meta-moral cognition. Taking this extra step requires an ability to reflect on one’s own thoughts, and the rules or strategies followed to formulate a moral judgment. The objective of this paper is to provide an overview of the studies concerning the processes involved in meta-moral cognition.",
18.0,1.0,Mind & Society,30 May 2019,https://link.springer.com/article/10.1007/s11299-019-00216-1,"Scientific discovery, causal explanation, and process model induction",June 2019,Pat Langley,,,,Unknown,Unknown,Mix,,
18.0,1.0,Mind & Society,08 May 2019,https://link.springer.com/article/10.1007/s11299-019-00210-7,The Role of the Brand on Choice Overload,June 2019,Raffaella Misuraca,Francesco Ceresia,Palmira Faraci,Female,Male,Female,Mix,,
18.0,1.0,Mind & Society,20 April 2019,https://link.springer.com/article/10.1007/s11299-019-00211-6,Inconsistency is not pathological: a pragmatic perspective,June 2019,Mario J. Rizzo,,,Male,Unknown,Unknown,Male,"Observed choices may well be inconsistent in some way. In contrast to the behavioral strategy outlined above, we do not take these as indications of some sort of derangement of the individual’s true preferences. We simply look at them as phenomena that may not be fully understood. Furthermore, a fundamental difficulty in assessing choice inconsistencies lies in the ambiguity of the term inconsistency itself. In colloquial English inconsistent behavior sometimes means inconstant behavior. Sometimes a person may be polite and considerate while at other times he may be impolite and selfish. This may be interesting for some purposes but it avoids the obvious questions of whether circumstances have changed or whether there is some level at which the changing behavior is consistent with an unchanging objective, rule or strategy in a given environment.Footnote 3 I discuss three types of circumstances in which behavior may violate the standard consistency axioms: first, the pursuit of an internally consistent objective when the options available now create expectations of future opportunities and costs; second, the adherence to rules at a higher level of cognition than that at which consistency is tested; third, the use of fast and frugal heuristics to make decisions. Imagine a foraging individual who chooses options in the environment so as to maximize its long-term rate of energy gain (McNamara, Trimmer and Houston 2014). At no point in this process does this objective change. In that sense, the individual is consistently or constantly pursuing one goal. Do such an individual’s particular choices satisfy the consistency requirement of transitive preferences? Perhaps surprisingly, the answer is, under general conditions, no. When that is the case, the individual will appear to be inconsistent and yet, from the standpoint of pragmatic rationality, will be successfully achieving its goals. Thus concern with the consistency of particular choices is misplaced. In the standard intellectual and actual experiments about preference transitivity, individuals are presented with various options seriatim. There is no thought “allowed” about whether in the next round a certain option will reappear or not. The individual is supposed to simply accept the options presented and make a choice. Under more realistic conditions, however, individuals do give thought to whether options will continue to be available. Therefore suppose that there are three sources of food: A, B, and C. They may all be simultaneously available, or only one or two or none may be available. Each option has a definite energy content and a “handling time” to obtain, process and consume the food. During the handling time no other food source can be tapped. Other options can appear and disappear with definite probabilities during this period. The options available during the handling time that could have been chosen constitute an opportunity cost of choosing in the initial period. At the conclusion of the handling time the individual may or may not have the opportunity to choose among those options and new ones may have appeared. A simple case of options with a certain probability of disappearing can make the essential point. Assume the reappearance probabilities are negligible. The table below presents the basic data. Consider an individual who is presented by nature with A and B. She chooses B because it offers greater energy per unit of time. In the second period she has options B and C. Here C is the better choice on the same basis. In the third period the options are C and A. Since in the ten or twenty minutes it takes to process these two options respectively all of the three options are likely to be gone, A is the better choice because it has the highest absolute energy gain.Footnote 4 In the fourth period, the individual is presented with all three options. The best choice is C. Handling time is critical to this choice. Although A yields the most energy, it takes 20 min to process. In 20 min, C can be chosen and then B can be chosen twice.Footnote 5 Then the total gain would be 34 units.Footnote 6 The preference ordering when two options are presented is BpA, CpB, and ApC. Transitivity would imply that CpA. Thus the transitivity axiom is violated.Footnote 7 Furthermore, ApC when B is not available (as in the third period set of options) while CpA when B is also available (as in the fourth period set of options). From this we see that the Independence of Irrelevant Alternatives axiom is also violated.Footnote 8 Nevertheless, the individual maximizes her long-term goal. She follows a contingent rule: At each decision point, gain as much energy as feasible during the handling time while reckoning the opportunity costs attached to the disappearance and reappearance of other options during that time. This rule is implemented as her strategy in the concrete cases of choice that nature presents. The rule is internally consistent and it is applied constantly. The violation of consistency properties therefore occurs at the level of the particular choice of options but this is not important to the individual. She is pragmatically rational in maximizing long-term energy gain. Option Energy content, e Handling time, h P (disappear), μ P (reappear), λ A 20 20 0.5 0.001 B 8 5 0.001 0.001 C 18 10 0.5 0.001 This illustration departs only slightly from the standard rational choice framework. There are no substantial cognitive limitations at play. In fact the agent makes no mistakes at all. There is only the introduction of two realistic features. First, there is an opportunity cost while the agent is handling the currently chosen option. Second, the energy-maximizing strategy does not depend simply on the individual options but on other options that may become available or disappear. A revealed preference approach is too impoverished to see this (at least without considerable “epicycles”). When individuals follow customary, moral or legal rules that are hierarchical in nature, intransitive choices or preferences can be the consequence (Katz 2014). In these cases the focus on inconsistency among the choices may miss the essential meaning and rationality of the rule. Furthermore, the revealed preference approach is so boxed in by methodological strictures of just-looking-at-behavior that it can, and often does, miss coherence at a higher level of abstraction which may be less obvious. Let us consider the negligence rule for imposing risk on innocent individuals. In one application of the rule (Sandroni and Katz, no date, pp. 12–13), we can imagine two individuals who have just been in a traffic accident. In order to prevent the worsening of their injuries and possible grave harm, they rush to the nearest emergency room in their car. In so doing they impose risks of injury to innocent others who happen to be on the road along their route. Assume, for the purposes of this example, the cost of risk imposed on the others is less than the cost of risk that would be sustained by those headed to the ER if they were to slow down to normal speed. So far, so good. It is acceptable under the negligence rule to rush thereby imposing smaller risk-costs on others to avoid greater risk-costs to oneself. However, the individuals who were in the traffic accident routinely accept risks well in excess of these “greater risk-costs” when they engage in their favorite sport of mountain climbing. Therefore, they are perfectly willing to incur high risks, such as those in the accident situation here and more, for sport but not for protecting innocent people along the route to the hospital. Whatever else we think of these individuals, if they follow the rule, their preferences will be intransitive. To see this, let us assume that the dangers of mountain climbing include not only the risks to the climbers but also risks to those people who live nearby because the climbing might cause certain boulders to be loosened thus putting the residents in danger. However, this risk is the same as the risk to pedestrians (and other drivers) in the case of rushing to the hospital. Nevertheless, the legal rule does not permit them to impose that same risk on others in the furtherance of sport. Yet the utility the climbers derive from mountain climbing is greater than the utility they derive from avoiding their risk of harm in slow-driving to the hospital (as evidenced by the even higher self-imposed risks they accept in climbing). Therefore if the climbers choose in accordance with the legal rules they will display the following preferences. Let A = getting to the hospital quickly, B = avoiding a given level of risk (= x) imposed on othersFootnote 9 and C = mountain climbing. Then the pattern of preferences is: ApB as the drivers rush to the hospital imposing risks on others; BpC because the climbers will not impose the same risk as A on the residents beneath the mountain; but CpA as is revealed by the willingness to climb mountains which is riskier than not getting to the hospital quickly. To the observer who has no knowledge of the legal rule, the series of choices may seem puzzling. Why does the series of choices reveal a changed valuation? And indeed at the level of the choices themselves there is a change in valuation. But at a “higher” level, the series reveals behavior in conformity with an unchanged rule. For better or worse we are allowed to impose certain risks on others to protect our own health or safety but not to indulge in the pleasure of mountain climbing which we may value more highly. Evaluation of the “rationality” of their behavior should not focus on the intransitivity of choices under the rule but on the characteristics of the rule itself and what it is supposed to accomplish. The rule may be a reflection of the social prioritization of certain values. The physical integrity of the person may be a more important social value than entertainment even if the individual himself does not see it that way. This a comparative social valuation and not an individualistic one. Whether the reader concurs with this valuation is not the point. It seems to be at least a reasonable one. Compliance with the rule that is consistent with it results in intransitive choices. Thus it would be naïve or just silly to try to fix the behavior of these individuals based on the supposed incoherence of their behavior. The rationale for the behavior is in the rule.Footnote 10 Heuristics are short-cuts that enable people to make quick and often accurate decisions when both cognitive limitations of the agent and the structure of the environment warrant them. One of the most important of these is “Take the Best” (Gigerenzer 2000, pp. 171–187). TTB involves ordering cues in terms of their perceived validity (usefulness) and evaluating objects of choice in terms of the presence or absence of the characteristic the cue represents. The stopping rule used here is that search is terminated when one positive and one negative cue values are ascertained, but not when one positive and an unknown value is found.Footnote 11 Let us apply it to pairwise comparisons. Suppose an individual wants to decide which city (a, b, c) to live in. She may use certain cues to help her. There may be things like whether it is a national capital, the presence of a soccer team or a philharmonic orchestra. If the decisionmaker ascertains that as to the first cue object a is positive while object b is negative, the heuristic requires her to choose a. If, however, as in the table below, in a choice between b and c the agent cannot ascertain the presence or absence of the first cue with respect to object c, the individual goes on to the next cue. In that case, then b is chosen. When the she must choose between c and a he must go down to the third cue to get a determinate result. Here she will choose c.Footnote 12 a b c Cue 1 + − ? Cue 2 − + − Cue 3 − − + Unfortunately, for those who focus on transitivity as a core rationality criterion, the results of following this heuristic in this case are intransitive choices. In the first binary decision a is chosen over b. In the second, b is chosen over c. Finally, c is chosen over a. This “incoherence” might be interesting if it were the case that the heuristic is a bad rule to follow. But then the root of the problem would be the inadequacy of the heuristic and not the intransitivity of the series of choices.Footnote 13 In fact, however, TTB is not a bad heuristic. For example, in the original (intransitivity-generating) form presented here, the data on “overconfidence” seen in predictions of such things as which city has the greater population was better explained by the assumption that people use TTB to generate the answers (Gigerenzer et al. 1991).Footnote 14 Most recently, another heuristic—even more frugal than TTB—that can yield intransitive choices has been tested against standard inductive decision-making methods. This is the Minimalist heuristic (Gigerenzer 2000, pp. 188–190, 194–195). The Minimalist shares much in common with TTB except for the fact that the agent does not have to rank the cues in order of validity. In this heuristic a cue is drawn randomly from memory.Footnote 15 How does it perform? In a very large simulation study (Gigerenzer 2000, pp. 166–200) in which various methods of inductive inference were used to predict which of two cities had the larger population, the Minimalist did extremely well compared to the classical methods that use all of the available information (“integration strategies”). Specifically, its average accuracy was only one percentage point below that of a linear regression model (64.7% vs. 65.7%) but it used much less information. Interestingly, the top performer—if only by 0.1 of a percentage point—was TTB in a somewhat less strict form than that illustrated above.Footnote 16 Heuristics that may lead to intransitive choices can be as accurate and yet more frugal than standard integrative strategies that avoid intransitive choices. From a pragmatic perspective, it is clear that focus on transitivity is irrelevant and, more importantly, distracting. We are interested in consequences, not in constitutive rationality. In the long development of his thought, the economist and political theorist Friedrich Hayek shifted away from the pure logic of choice to rule-following as the most interesting and fruitful way to characterize human behavior (Hayek 1973; Rizzo 2016). He did not argue that people always follow rules but that they very often do, especially in the areas of moral and legal conduct, and under conditions of uncertainty. These rules may be explicit. In such cases individuals are rule following in a literal sense. Or they may be implicit. In these cases individuals are rule conforming. In this short article I have not emphasized this distinction nor have I inquired into the origin of the rules—whether they are the results of individual discovery, top-down orders or an evolutionary process of trial and error. All of these distinctions are important. My purpose, however, has been broader: to show that sensible rules which can advance the individual’s (or social) goals can nonetheless result in inconsistent or intransitive individual choices. This should serve as a corrective lesson for the discussion of behavioral welfare economics and paternalistic policy. We would do better to focus on the rules that people follow in solving their problems in particular environments. Instead of seeing “pathology” in inconsistent choices, we should view that as a challenge to uncover the rules that such choices instantiate. By so doing we may unlock a higher level of rationality than we had previously understood.",2
18.0,1.0,Mind & Society,23 April 2019,https://link.springer.com/article/10.1007/s11299-019-00213-4,Is irrational thinking associated with lower earnings and happiness?,June 2019,Shoko Yamane,Hiroyasu Yoneda,Yoshiro Tsutsui,Female,Male,Male,Mix,,
18.0,1.0,Mind & Society,25 April 2019,https://link.springer.com/article/10.1007/s11299-019-00212-5,The quantum-like approach to modeling classical rationality violations: an introduction,June 2019,Franco Vaio,,,Male,Unknown,Unknown,Male,"Since the early years of quantum physics, physicists have speculated that there might be some implications of quantum theory that go beyond physics and involve, among other fields, also psychology (e.g.: Bohr 1948; Schrödinger 1944; Whitehead 1929, 1933). One of the most famous attempts to connect the worlds of physics and psychics is the unus mundus conception, explored by Wolfgang Pauli and Carl Gustav Jung in their correspondences that lasted more than 25 years (Meier 1992; see also: Atmanspacher 2012; Atmanspacher and Fach 2013; Atmanspacher and Primas 2009; Jung and Pauli 1952). Niels Bohr in 1927 introduced the concept of complementarity in physics, partially borrowed from psychologist James (1890; see also: 1884). It soon became a fundamental principle of quantum physics to explain the dual aspect that natural phenomena show, notably at the atomic and subatomic level: an electron and a photon (in principle, anything in nature) behave both as a particle and as a wave, depending on the experimental set up in which they are studied. Complementarity has led to major debates, still ongoing, on two possible interpretations: ontic, on the one side—i.e., nature works “like that”—, and epistemic, on the other side—i.e., it is intrinsic in the limitations of our senses an mind—. In Bohr’s conception, complementarity is more a true law of nature than an idea of (auto)epistemic accessibility as it was for James.Footnote 1 Bohr wrote in a letter to Pauli: It seems that we here meet with an unavoidable dilemma, […] the question being not of a choice between two different rivalizing concepts but rather of the description of two complementary sides of the same phenomenon (quoted in: Murdoch 1987, p. 55, emendations are original in Bohr’s manuscript). Elsewhere, Bohr wrote: “There is no quantum world. There is only an abstract quantum physical description. It is wrong to think that the task of physics is to find out how nature is. Physics concerns what we can say about nature” (quoted in: Petersen 1963). Bohr himself speculated about the possibility that the principle of complementarity could provide insights not only into nature’s functioning, but also into human psychology (Bohr 1948). However, he never concretized his speculation into a research program. The question as to whether connections exist between the quantum world and psychology (human behavior) has resurfaced and has been explored in the last three decades in the new research field of quantum cognition. This research field has emerged in the wake of a new culture in theoretical physics, less application-oriented and more prone to science per se and to the analysis of its foundations and philosophy, that a new generation of theoretical physicists, mostly coming from the west-coast universities of California, gave birth to in the 1970s (Kaiser 2011). Several approaches to quantum cognition have been put forward, each of them based on peculiar assumptions (see e.g.: Aerts 2009; Atmanspacher et al. 2002; Atmanspacher and Rotter 2008; Barrett 1999, 2006; Bohm and Hiley 1993; Butterfield 1998; Hagan et al. 2002; Hameroff 1994, 1998, 2007, 2012; Haven and Khrennikov 2013; Khrennikov 2010; Khrennikov and Haven 2008, 2009; Mohrhoff 2005, 2006, 2011; Penrose 1989, 1994; Squires 1990; Stapp 1979, 1993, 1998, 2001, 2004, 2005, 2007a, b, 2008). Among them, the so-called quantum-like approach attracted most attention due to requiring less initial assumptions than other approaches. This approach does not attempt to explain cognitive processes in terms of quantum chemical processes in the neuron synapses, nor does it assume the idea of a “quantum brain” as a whole. Both of these views are considered at the moment as plausible conjectures, while lacking the support of reasonable experimental evidence. The quantum-like approach, on the other hand, assumes that the mathematical apparatus of quantum physics is effective in describing (at least some of the) cognitive processes, e.g. choice processes, a task at which real analysis and classical probability fail. Importantly, the quantum-like approach does not intend to replace psychological interpretations and theories. Neither it provides a basis for simplistic explanations of brain functioning in terms of supposed quantum structures, nor for any nonscientific (in the currently accepted meaning of this term) “suggestive” approach. The quantum-like approach aims, instead, to develop an effective mathematical theory, i.e., an instrument to help better see possible connections among phenomena, and to acquire a deeper comprehension of facts through providing an effective description of human behavior on a quantitative basis: an instrument to build models and to support the development of specific theories that can further extend the interpretations provided by the so-called fast and frugal heuristics (Gigerenzer and Goldstein 1996; see also: Gigerenzer et al. 2011; Neth and Gigerenzer 2015).Footnote 2 This claim is clarified next by examining the quantum-like approach to the psychological descriptions for two famous examples of the violations of rational choice assumptions: “Linda test” and “Hawaii test”.",2
18.0,1.0,Mind & Society,11 May 2019,https://link.springer.com/article/10.1007/s11299-019-00215-2,Rationality in a fatalistic world: explaining revolutionary apathy in pre-Soviet peasants,June 2019,Jessica Howell,Flagler College,Nikolai G. Wenzel,Female,Unknown,Male,Mix,,
18.0,2.0,Mind & Society,25 October 2019,https://link.springer.com/article/10.1007/s11299-019-00217-0,A behavioral approach to economic analysis,December 2019,Hugh Schwartz,,,Male,Unknown,Unknown,Male,,
18.0,2.0,Mind & Society,08 November 2019,https://link.springer.com/article/10.1007/s11299-019-00218-z,Architecture of the mind and libertarian paternalism: is the reversibility of system 1 nudges likely to happen?,December 2019,Riccardo Viale,,,Male,Unknown,Unknown,Male,"The publication of Nudge by Thaler and Sunstein (2008) has given rise in the last few years to an interesting debate on the role of behavioural sciences in government and in public policymaking. On the one hand, most have agreed that public policy-making needs a better understanding of the human mind and decision-making in order to achieve policy goals. However, on the other, a sceptical minority has underlined the illiberal danger of manipulation contained in the measures outlined in Nudge and, in general, in the behavioural approach to policy-making. Thaler and Sunstein subsequently contested the charge of manipulation and tried to prove the libertarian attribute of their paternalism (Thaler 2015; Sunstein 2016). An implicational network for reasoning about wet grass The present article does not aim to criticise the whole theory of nudge and neither to assess its practical efficacy as a policy-making tool.Footnote 1 In the present article I will try to assess only the claim made by Thaler and Sunstein regarding some aspects of the libertarian attribute of nudge theory. Broadly speaking their claim relies on the reversibility of choice made under the influence of nudging. In particular, they focus on System 1 nudges (Sunstein 2016) that exploit automatic, unconscious decisions based on psychological mechanisms, such as inertia, status quo, framing, time discounting, loss aversion, and priming, and so on, as the default states and on the ability to overcome them through the deliberative attitude of the human mind. An intervention is classified as a nudge when it is not a coercive measure, when it retains freedom of choice, is based on automatic and reflex responses, does not involve methods of direct persuasion, does not significantly alter economic incentives, and when it revises the context of choice in line with the discoveries of behavioural economics (Thaler and Sunstein 2008; Oliver 2013). What Thaler and Sunstein proposed is a form of libertarian paternalism that has a dual valence. As paternalism, it aims to make up for citizens’ irrational and self-harming tendencies by “gently nudging them” to decide rationally for their own good. As libertarian it aims to give the last word to the outcome of the conscious and deliberative processes of the individual citizen who can always choose to resist the nudge. Some crucial questions emerge from this particular theory. First, what rationality model is used by “nudgers” to intervene in the choices made by citizens? Second, which is the ideal phase of the citizens’ decision-making process to intervene in? Namely, what sort of paternalism should be adopted, and what would be the consequences for the libertarian tenet? This article will focus only on the second question (while the first is analysed in Viale, forthcoming). In the following pages I will analyse the grounds for the justification of the libertarian tenet. I will do so by expanding the neurocognitive model underpinning the libertarian claim of nudge theory. First, nudge theory is not a compact theory but rather a set of different tools and aims (Sunstein 2016, pp. 26–7): default options, feedback, warnings, reminders, simplification, priming, uses of social norms, public recognition, pre-commitment strategies, framing and timing, all of which aim to neutralise errors, map future welfare and structure complexity. In any case there are at least two big inclusive categories of nudges: those that exploit automatic unaware decisions based on psychological mechanisms and biases such as inertia, status quo, framing, time discounting, loss aversion, priming and so onFootnote 2; those that increase autonomous and aware decision-making and deliberation. Following the same lines, Sunstein (2016) distinguishes System 1 nudges from System 2 nudges. Second, there are various levels of possible behavioural intervention in citizen choice that are characteristic of paternalism. In the first place, we need to understand whether or not any kind of paternalism can be completely libertarian. Thaler and Sunstein hold that every choice architecture must allow the individual to make a conscious decision to accept or reject the proposed nudge. At any rate the attribution of libertarian to paternalism alters depending on what stage of the decisionmaking process it is applied to. We can identify three key stages by working backwards, from the downstream end back upstream. At the furthest point downstream we have the choice architecture of a state of wellbeing. This set of interventions could be called hedonic paternalism. The most important tool of nudge theory is the default state.Footnote 3 Various criticisms can be put forward to contest the non-libertarian dimension of this paternalism, or in other words the tendency of nudgers to turn into preachers and technocrats of reason. For example, being subject to weakness of will and the status quo bias, citizens, in some countries like France, are automatically enrolled by default as organ donors, although they can consciously decide to “opt out”. It has been shown that this method leads to a considerable increase in the number of donations. Using the language of System 1 and System 2 (Sloman 1996; Kahneman 2011), the citizen automatically finds herself in a default state, but she can use the analytical and reflective System 2 to evaluate and decide whether to accept this state. Nevertheless, the reversibility of the default state is difficult because of the System 1 tendency towards inertia, procrastination and status quo bias. It becomes hard to sustain the libertarian attribute when the nudge intervention is aimed primarily at exploiting our System 1 reasoning and the deliberative “opt out” clause becomes secondary and unpleasant. If, as Sen asserts (2009), no moral end and no question of justice can be founded on Machiavellian grounds, namely using a process that contradicts the principle itself, then it is legitimate to ask how much liberty of choice is protected by an approach based on non-deliberative automated processes. The possibility of deliberative choice exists in abstract terms, but the choice architecture is biased towards the maintenance of the default state due to the prevalence of our System 1 over System 2.,Footnote 4Footnote 5 A similar argument may be applied to other System 1 nudges, such as those exploiting framing effect, priming, time discounting, and loss aversion. The prevalence of automaticity and unawareness in the actions driven by System 1 nudges makes them hard to reverse. The libertarian attribute presupposes the possibility of autonomous, aware deliberation. Any unaware, automatic, heteronomous choice may not be considered libertarian. Therefore the libertarian attribute of paternalism is justified if the System 1, unaware, automatic, heteronomous decision can be changed and overridden by an autonomous, conscious, System 2 deliberation. It is the mere fact of conscious, analytical deliberation to revert to the default state and to the other System 1 nudges that fit the libertarian attribute. It is not sufficient to substitute—impulsively, without conscious deliberation—a System 1 default or decision with another System 1 default or decision. Changes based on competitive elements mostly positioned within System 1 are not an example of autonomous, aware deliberation to reverse System 1 nudges. Therefore they cannot support the libertarian claim of nudge theory. Conversely, and further upstream, there are System 2 nudges designed to reinforce and empower the capacity for reasoning and judgement, thereby leading to the choice of what solution to adopt. In this case, we can talk of cognitive paternalism. This paternalism seems libertarian given that it strengthens the individual’s deliberative capacity. For example, faced with a complex problem, a simplification is proposed by highlighting only the relevant variables and drawing attention to the underlying structure. Support for using a combination of lexicographical heuristics and satisficing is also a highly effective way of reducing the computational burden when choosing between alternatives. Faced with the medium- and long-term effects of one’s own choices, the proposal to highlight and simulate the relationship between an individual’s choices and their effects on his wellbeing over the medium term is also another form of paternalism that is acceptable from a libertarian point of view. For example, asking someone who is taking out a loan to try and anticipate its effects on their objective financial solvency, when that person finds themselves having to pay increasing monthly instalments for the coming years, is certainly an acceptable paternalism in terms of deliberative freedom. So are proposals that increase environmental feedback on personal choices or that help by providing appropriate “warnings” to neutralise misunderstandings or errors in computational calculation. This type of paternalism should, however, also include the advice of bounded and adaptive rationality (Hertwig and Grüne-Yanoff forthcoming; Viale forthcoming). In many contexts, especially where there is uncertainty, it is better that nudges do not target an increase in pointless (in the sense of hopeless) computational skills when dealing with forecasting problems where the alternatives are not known and there is no possibility of estimating the relative probabilities. Better, in this case to encourage decisions based on simple, fast heuristics, some of which, but not all, are intuitively made (Gigerenzer et al. 1999; Gigerenzer 2015; Hertwig and Grüne-Yanoff, forthcoming; Viale forthcoming). Lastly, upstream we have the most important paternalistic intervention that a government can make to improve the decision-making processes of its citizens: we can call this educational paternalism, namely the attempt “to give anyone who wants to catch fish a good fishing rod”. Behavioural economists do not believe much in economic education. They use a significant analogy that likens bias to optical illusion. In the same way that you can’t see certain shapes in experiments using the ambiguous shapes of gestalt psychology, so it is not possible to avoid making certain mistakes in probabilistic and logical judgement. The use of education to strengthen metacognitive and “debiasing” skills is, to them, a hopeless task. As Richard Thaler affirmed in an interview to Michael Bond: “Our ability to de-bias people is quite limited” (Bond 2009, p. 1191). Hence the need for hedonic and cognitive paternalism. Now, the problem of training in statistical thinking and logical reasoning, both of which are essential for economic and social decisions, has been examined in some past and recent studies (Fong et al. 1986; Nisbett 2009; Gigerenzer 2014). These have ascertained whether it is possible to teach the statistics of natural frequencies, or the law of large numbers, and whether it is possible to reduce the propensity for confirmation bias in real-life-type settings. This type of training is very useful in risk situations. In those dominated by uncertainty, where the principle of optimality does not hold, it is better to teach some simple fast heuristics and to pay attention to how the problems are framed (Viale forthcoming; Hertwig and Grüne-Yanoff forthcoming). In any case the behavioural economics representation of statistical and probabilistic human reasoning as deeply flawed is not true. On the contrary, many cognitive scientists conclude that people’s judgements are largely consistent with Bayesian reasoning (e.g., Chater et al. 2006; Chater and Oaksford 2008; Edwards 1968). Anderson’s (1990) rational analysis programme models both conscious processes, such as causal inference and problem-solving and unconscious processes, such as memory and forgetting, as Bayesian inference (Schooler and Anderson 1997). Similarly, in “Bayesian Models of Cognition”, Griffith et al. (2008) argued that both unconscious perceptual processes and high-level cognition are consistent with Bayesian models. As a final case, there is an influential Bayesian programme in neuroscience that assumes a “Bayesian brain” (e.g., Friston 2010). Moreover, the mind’s statistical reasoning processes have evolved to operate on natural frequencies and Bayesian computations are simpler to perform with natural frequencies than with probabilities. It is well known that if information is presented as the outcome of learning from experience, known as natural frequencies, and not as conditional probabilities, the proportion of people reasoning by Bayes’ rule increases considerably (Gigerenzer and Hoffrage 1995). Statistics expressed in terms of natural frequencies improve Bayesian inferences in decision-making (Gigerenzer and Hoffrage 1995; Hoffrage et al. 2000). From this point of view nudge theory informed by behavioural economics offers a mistaken representation of human competences as being deeply biased as far as concerns Bayesian reasoning. Nudge theory has two features. It is paternalist because it nudges people to make decisions that are in their own interest. It is libertarian because people are able to ignore the nudge and decide differently. In other words, the theory is paternalist because it uses the System 1 of the mind to drive the individual automatically towards a choice that is good for him or her, and it is libertarian because it relies on the System 2 of the mind to consciously analyse and evaluate the choice with the possibility of opting out. According to Thaler and Sunstein, System 1 and 2 dichotomy is a necessary condition in order to claim the libertarian feature of nudge theory. In most of their writings the defenders of nudge theory are strongly committed to a dual model of mind (Thaler and Sunstein 2008: pp. 19–22, 25, 42–44, Sunstein 2013: pp. 42–47, 60–65, 83–86, 100–25, 151–155, 2014: pp. 26–34, 151–59, 2016: pp. 27–31, 89–96, 140–50; Thaler 2015: p. 103109). Moreover, they seem aware of the difficulties of reversing and overriding System 1 automatic decisions by using System 2 aware deliberations. For example, Sunstein (2016) writes “With the distinction between deliberative and automatic processing in mind, we might want to distinguish between nudges that address System 1 and nudges that address System 2″ (p. 140). He also states that “We should distinguish between System 2 disclosures, designed simply to give people information and to ask them to process it, and System 1 disclosures, designed to work on the automatic system” (2016, p. 30). And finally he says it is “helpful to suggest that many actions count as manipulative because they appeal to System 1, and because System 2 is being subverted, tricked, undermined, or insufficiently involved or informed. Consider the case of subliminal advertising, which should be deemed manipulative because it operates ‘behind the back’ of the person involved, without appealing to his conscious awareness. People’s decision are affected in a way that entirely bypasses their own deliberative capacities” (2016, p. 90). Humans often are not able to override an automatic choice, such as those prompted by System 1 nudges. Since humans are cognitive misers, they follow the rule of engaging the brain only when all else fails and usually not even then (Krueger and Funder 2004). Cognitive misers have three rules: default to System 1 processes whenever possible; when that is not possible and analytical processing is necessary, default to serial associative cognition with a focal bias (as in the case of matching bias in selection tasks); when the reflective mind wants to start cognitive simulation by decoupling, not to complete it, or in other words override failure (Stanovich 2009, p. 69). Moreover, if the analytical processes try to override the autonomous processes, it is possible they do not succeed because the mindware is not available or is contaminated. In any case, the possibility of making a conscious analysis of the nudged choice (above all, if there is a default state and if it involves a System 1 nudge), and, if it is not acceptable, to reverse and reject it and to make a different choice, underlies the libertarian justification of choice architecture. The individual is not hemmed in by the specific choice architecture and is not obliged to choose what the behavioural technocrats want, but rather he has substantial, not just formal, freedom to reject the nudges. This is a fundamental political and philosophical point because, if this were not the case, then there would be no basis or justification for the libertarian attribute. Instead, the situation would be one of authoritarian and non-liberal manipulation of citizens’ choices using behavioural techniques. If it can be shown that the default states created in choice architecture are not easily modified and, on the contrary, that the aim of this architecture is that they cannot be modified, then any libertarian claim would be weakened. By way of example, think of the default states of welfare programmes, like “Save more tomorrow”Footnote 6 (Thaler and Sunstein 2008), or of the opting-out version of organ donation.Footnote 7 If, in these cases, situations of choice are produced that are difficult to reverse owing to the “status quo bias” and the effects of inertia and weakness of will, on the basis of which the nudge was justified and constructed, then this cannot be justified as a libertarian intervention.Footnote 8 Libertarian paternalists argue that individuals, while nominally free to exercise the “good choice”, de facto fail to do so for a variety of cognitive shortcomings. Their freedom of choice is therefore only nominal and they need the help of libertarian paternalists. If one accept this argument, “what is the value of the reversibility of the choices set up by the libertarian paternalists if freedom of choice is not exercised, that is, if the nudges of the libertarian paternalists are so effective?” (Rebonato 2012, p. 133). In fact according to Rebonato (2012) reversibility of choice is nominal, but not real and substantial. As the effectiveness of the nudges approaches 100%, real freedom of choice disappears. For example, almost 100% of Austrians are organ donors as against 12% of Germans. The simple explanation is that Germans must opt in and Austrians must opt out. Both Germans and Austrians can exercise their nominal freedom of choice to change their default at very little cost. In practice, however, many of them do not do so precisely because of the System 1 cognitive reasons, like inertia and status quo bias, that libertarian paternalists are so smart at identifying and exploiting in their architectures (Rebonato 2012, pp. 8–9). Lastly, a question to be answered by libertarian paternalists is why “the same individual who could not make an effective choice in the pre-nudge condition would have to miraculously acquire high critical (System 2) abilities, to consider the choice towards which she has been nudged, to see that it is good and to accept it” (Rebonato 2012, p. 134). To the extent that individuals pervasively suffer from cognitive biases, the nominal ability to reverse a nudge will amount to very little. In this sense, Rebonato (2012, p. 208) concludes: “And, perversely, the only (Homo-Economicus-like) individuals who are little affected by irrelevant frames and who can therefore easily reverse the libertarian paternalist nudge are those who could unshackle themselves from irrelevant framing in the first place—those, that is, for whom libertarian paternalism is, on its own terms, not needed.” Decades of research in judgement and decision-making have yielded an array of debiasing strategies that can improve personal and professional decisions. Deliberate System 2-style thinking plays a critical role in debiasing. Being “decision ready” is “when System 2 is capable of performing its functions, in terms of monitoring, suspending decisions, and correcting judgments” (Soll et al. 2015, p. 929). Unfortunately, many times a person is not decision ready. Tasks that require effort and attention and rely on self-control become more difficult (Baumeister et al. 2007). People are often distracted, aroused, fatigued and depleted, which constrains the ability to monitor decisions and to reverse them. Emotions and other visceral experiences do sometimes negatively impact decisions. According to Lowenstein (1996), people behave as if present desires will remain the same in a different context. People differ in their intelligence, thinking styles, mindware, training and “need for cognition” (Cacioppo and Petty 1982; Petty and Cacioppo 1986). Some biases, and the possibility to reverse them, such as hindsight bias, correlates with cognitive ability, but many others do not, such as anchoring (Stanovich and West 2008). Can people recognise their own unreadiness? “Research on self-awareness suggests that we should be pessimistic. People have a blind spot for their own biases” (Soll et al. 2015, p. 924). Many of the debiasing techniques address unreadiness. They differ in which sources of unreadiness they address, and whether they attempt to increase readiness or modify the environment so that readiness matters less. Following the categorisation of debiasing methods built on Fischhoff’s distinction (1982), it is possible to dub one approach “modify the decision maker”, and the other approach “modify the environment” (Soll et al. 2015, p. 926). The first relies on education and training about thinking strategies, heuristics and rules of thumb, as well as more formal aids (e.g., risk literacy) that people can be taught to use. It corresponds to the educational paternalism, previously introduced. The second seeks to alter the environment to achieve a better match with people’s way of thinking. One approach is to change the task in order to spur people to process information better. This corresponds to the cognitive paternalism previously introduced. The second is to adapt the environment to people’s biases. The adaptation should exploit automatic bias to influence behaviour for the good of the individual or society. This corresponds to the hedonic paternalism outlined earlier. An alternative way to represent debiasing by nudging distinguishes between outcome nudges, which push toward a uniform outcome, and process nudges, which help people employ decision strategies most likely to lead to their personal preferred outcome (Dietvorst et al. 2014). Hedonic paternalism corresponds to the first type of nudges, whereas educational and cognitive paternalism conform to the second type. Default states are the most important example of “modify the environment” debiasing techniques and outcome nudges. They rely on powerful and robust automatic mechanisms like inertia, procrastination, loss aversion, time discounting, and so on. Their difficult reversibility is acknowledged by concerns about their one-size-fits-all nature, which does not typically account for preference heterogeneity. They may benefit some people at the expense of harming others who are incapable of overriding the default (Smith et al. 2013). Defaults and other System 1 nudges have an important place in the debiaser’s toolkit, particularly owing to their robustness and difficult reversibility. That makes them a preferred tool for policy makers aiming to manipulate citizens’ behaviour.",5
18.0,2.0,Mind & Society,24 October 2019,https://link.springer.com/article/10.1007/s11299-019-00219-y,Implications of smart decision-making and heuristics for production theory and material welfare,December 2019,Morris Altman,,,Male,Unknown,Unknown,Male,"There is a consensus amongst behavioural economists that decision-makers often deviate from the behavioural norms identified as optimal by conventional (neo-classical) economics in both the domain of consumption and production, with a more recent focus on consumption. The focus of this article is on choice behaviour inside of the firm. Conventional economics typically maintains that decision-makers not only should but do behave in a manner consistent with the neoclassical norms for optimal behaviour. This behaviour ultimately results in economic efficiency. More specifically it is maintained that economic agents within the firm are behaving as if they are optimizing thereby maximizing profits and minimizing average production costs (Altman 1999; Friedman 1953). Any other behaviour (such as the use of heuristics—decision-making shortcuts) would see the firm decimated by market forces. Moreover, non-optimizing behaviour would not be rational (where rationality is defined as including, as a necessary condition, optimizing behaviour). And irrationality of this type is ruled out, by assumption, by the conventional wisdom. But the facts suggest otherwise. What actually transpires in the black box of the firm often is persistent sub-optimal behaviour. The firm is not as productive as it might otherwise be given the traditional inputs into the firm’s production function. Or, when firms are performing efficiently or optimally this is not necessarily consistent with firm members adhering to the optimal conventional behavioural norms of conventional economic theory, where the latter I refer to as rational or smart behaviour. What conventional economics recognizes is the type of economic inefficiency that is given ‘exogenously’ by market structure, referred to as allocative inefficiency. This is a function of distorted market prices relative some competitive ideal. This conventional type of inefficiency tends to be relatively trivial. But another type of inefficiency, referred to as x-inefficiency by Leibenstein, in his classic 1966 article, appears to be quite important when analysts bother to test for its substantive significance (Frantz 1997; Tong 2020). X-inefficiency is defined as not being as productive as possible given traditional factor inputs. Being as productive as possible, given these inputs is referred to as x-efficiency. Conventional economics assumes x-efficiency dominates in the long run. X-inefficiency is assumed away by the conventional economic modeling of the firm. But x-efficiency very much depends on the choices made within the black box of the firm. Conventional economics assumes that the choices made are consistent with x-efficiency. However, this need not be the case. I argue, unlike in the conventional modeling of the firm, that there is no guarantee that x-efficient choices will be the norm. Moreover, smart or rational agents can be expected to make choices that will result in the firm being x-inefficient. The x-inefficiency is part of the behavioural economics methodological narrative pioneered by Simon (1959, 1978, 1986, 1987). Generally speaking, behavioural economics argues for modelling assumptions that align with the facts and theoretical narratives that build upon how individuals or economic agents make decisions in the real world. It challenges the conventional economics assumption that individuals tend, even in the long run, to behave in accordance to conventional or neoclassical behavioural norms. A critical and general methodological point raised by behavioural economists is the importance of determining benchmarks for best-practice choice behaviour of individuals in a variety of decision-making environments. Most recent attention has been directed to the choices of consumers and households and the psychological, sociological and institutional determinants of these choices; with some discussion on the neurological determinants. Little emphasis has been directed on choices made inside the firm, which has significant effects on the level of material wellbeing achievable in a society. The great methodological debate relates to whether general ‘neoclassical’ benchmarks are optimal (and assumed achievable by reality-based decision-makers) or whether alternative benchmarks make more sense. In the heuristics and biases approach to behavioural economics (Kahneman 2003; Kahneman and Tversky 1979; Thaler and Sunstein 2008; Tversky and Kahneman 1981), humans tend not to meet achievable optimal neoclassical benchmarks and therefore make suboptimal choices. This is often a product of using heuristics or decision-making shortcuts. In the alternative fast and frugal approach, alternative optimal benchmarks are and should be adopted, based on context specific heuristics. This evolutionary approach to the use of heuristics is referred to as ecological rationality (for an early rendition of this see, March 1978). This can take the form of fast and frugal heuristics (Gigerenzer 2007; Mousavi et al. 2017; Mousavi 2018). Or, more broadly, it can take the form of heuristics that are frugal but not necessarily fast (slow thinking) (Kahneman 2011).Footnote 1 This is very much in contrast to heuristics and biases approach where heuristics yield sub-optimal choices and outcomes. Following from some of the key points articulated in x-efficiency theory, we argue that firms are efficient only if firm decision-makers choose to behave in a manner consistent with maximizing productivity given the traditional inputs at hand. However, such behaviour takes place only under a specific incentive environment and typically also requires particular preferences by firm decision-makers. It is not a matter of firms simply attempting to maximize profits as per conventional economics or adopting fast and frugal heuristics in the place of profit maximization. If profits are being maximized, it becomes a matter of how profits are being maximized. The how is not addressed in the conventional narrative and the same is the case with the fast and frugal narrative with regards to the organizational structure of the firm and the embedded incentive environment. We argue that economic efficiency is achieved when firms adopt heuristics that are nuanced to the particular circumstances of the firm, where these may not be consistent with neoclassical behavioural norms, for example. This type of smart decision-making, more in line with ‘satisficing’ behaviour (which one can refer to as frugal or simple or simplified heuristics), is context dependent and contributes towards achieving economic efficiency. In this context, fast and frugal heuristics, can yield higher levels of x-efficiency than following neoclassical behavioural norms. But blindly following any particular set of theoretical norms to achieve efficiency can be predicted to yield sub-optimal economic outcomes. Using heuristics as opposed to neoclassical behavioural norms is only one component of the economic efficiency narrative. Economic efficiency also critically depends on the preferences of decision-making, the firm’s state of industrial relations, the organizational form of the firm, as well as market forces. Optimal behaviour is considered to be behaviour which achieves economic efficiency or x-efficiency in production. What this, in effect, means is that firm members are working as smart and as hard as they can (maximizing effort inputs), given the totality of traditional factor inputs. This ‘ideal’ scenario is what Leibenstein (1966) refers to as x-efficiency in production. Conventional economics assumes that firm members (economic agents) are maximizing effort inputs at all times. Market forces ensure this outcome which can be reinforced by the assumed aligned profit maximizing preferences of firm members (Altman 1999; Friedman 1953). Hence, problems with firms’ performance are not scrutinized in terms of potential organizational, power, or preference related issues inside of the black box of the firm (Cyert and March 1963; Tong 2020). The conventional economic wisdom does not strictly specify how economic efficiency is achieved, but it does relate closely to profit maximizing behaviour and related intensive no cost or very low cost information search behaviour as well as detailed marginal calculations. And it is assumed that economic agents actually behave in this manner or it is assumed that they behave as if they are engaging in such profit maximizing behaviour (Berg and Gigerenzer 2010; Friedman 1953). In this modeling framework there is no analysis of how firms are actually run as functioning and successful economic entities. In reality, no successful firm’s members invest their time and effort engaged in intensive marginal calculations combined with intensive information search activities. They engage in different and varied activities to make their firms successful (Cyert and March 1963; Frantz 1997; Gordon 1996; Lazonick 1991 Leibenstein 1966, 1979; Levine and Tyson 1990; Penrose 1959; Sheffrin 2008; Tong 2020). In the heuristics and biases approach to behavioural economics pioneered by Kahneman and Tversky the optimal normative behavioural benchmark is the one identified in conventional ‘neoclassical economics’. Deviations for this decision-making behavioural norm is thought to be yield sub-optimal choices which reduce the decision-makers’ welfare from what it could otherwise be. Although Kahneman and Tversky’s focus was not on firm behaviour, it logically follows from their argument that such deviant sub-optimal behavioural will result in lower levels of economic welfare (lower levels of GDP per person, for example). Critical to this approach is the assumption (backed up with evidence largely from experiments on consumer behaviour) that decision-makers tend to deviate, often persistently (no or little Bayesian updating) from neoclassical behavioural norms (Kahneman 2003, 2011; Kahneman and Tversky 1979; Thaler 2015). Individuals tend not to behave neoclassically. This is a fundamentally important finding as it undermines the conventional economics assumption that not only should economic agents behave neoclassically, but they actually do. This point is reiterated by Smith (2003, 2006), who maintains that such deviations from neoclassical norms represent rational and optimal behaviour from the perspective of firm performance. What are the implications of ecological rationality and fast and frugal heuristics in the context of x-efficiency theory for production theory and our understanding of material welfare? I argue that they suggest alternatives to the neoclassical production function and that the neoclassical production function per se yields a misleading benchmark of what is maximum (efficient) output. Building upon the insights from behavioural economics, I present an alternative, more realistic and analytically precise specification of the production function which I referred to as the Satisficing, Boundedly Rational and Smart Fast and Frugal, X-Efficient production function or SAXE. The latter incorporates an understanding of the appropriate procedures, organization variables and end-goals required to achieve optimality in the realm of production. Satisficing (developed by Simon 1978, 1987) refers to doing the best one can, given one’s decision-making capabilities and the decision-making environment. This is opposed to the conventional economics concept of maximizing. Bounded rationality (also developed by Simon 1978, 1987) refers to rational or smart behaviour that is fit for purpose. It is behaviour suited to one’s decision-making capabilities and environment. And, bounded rationality invariably involves the development and use of heuristics or decision-making shortcuts. Fast and frugal (Gigerenzer 2007) refers to heuristics that are minimalist and quickly executed, potentially generating savings in time and money. Smart fast and frugal heuristics makes reference to such heuristics that are well thought out and fit for purpose and relates to Simon’s concept of procedural rationality. This alternative specification of the production function is ecologically rational in that it evolves based on decision-makers’ capabilities and their decision-making environment (related to bounded rationality). But this does not entirely dismiss neoclassical specifications, specifically that efficiency requires that output is maximized given traditional factor inputs. Rather, it places the neoclassical specification of efficiency in the context of bounded rationality. Conventional theory simply assumes that the rational firm (its agents) will make decisions abiding by conventional behavioural norms (or behaving as if they do) to achieve economic efficiency, de facto x-efficiency. The SAXE specification, makes no such assumption. Conventional norms are not assumed to be ideal or even achievable. Although output maximization is required to achieve x-efficiency, it is understood that the means to do so are complex and context dependent. X-efficiency is not guaranteed by assumption. It is achieved only when appropriate context dependent behavioural norms are developed and adhered to within a facilitating institutional environment. The neoclassical assumption of the necessary conditions (such as effort ‘maximization’) that need be met for economic efficiency to be realized is an important one. But the neoclassical necessary conditions are often mis-specified or not specified; they are implicitly assumed. A critical contribution of neoclassical methodology is to underline that there are objectively determinable standards for production efficiency to be achieved and for individual utility to be maximized. Not all behaviours can be deemed efficient or utility enhancing. However, I also argue that the neoclassical normative argument that individuals behave in a manner consistent with economic efficiency is highly misleading. I also make the case that rational choice behaviour can deviate significantly from benchmark efficient behaviour. Individual or group (firm) based rational choice behaviour can result in rational inefficiency in the domain of production. Inefficient choice is not equivalent to lacking in intelligence (Altman 2017). Rationality is consistent with both efficiency and inefficiency in production. I argue that “ecological rationality” need not be economically efficient (can be x-inefficient). An important component of my discourse is the argument that production efficiency is not always consistent with neoclassical norms. Moreover, production efficiency is critically dependent upon the incentive cum institutional environment faced by economic agents within the firm. This takes us beyond a discourse on heuristics and decision-making. Economic efficiency requires the realization of specific institutional norms (related to firm governance). The ‘right’ institutional framework need not be in place, yielding rational production inefficiencies. Conventional theory assumes that rational agents produce efficiently by minimizing costs and maximizing profits. It is further implicitly assumed that economic agents behave in a manner consistent with effort maximization. Economic agents of the firm behave as if they are all characterized by such behavioural arguments. Such efficiency enhancing behaviour is enforced through the survival principle. Inefficient firms are eliminated in the long run by the competitive process (Friedman 1953). Consistent with x-efficiency theory and the related evidence, I argue that conventional economics incorrectly assumes that all agents maximize effort inputs, which is fundamental to generate production efficiency (Frantz 1997; Leibenstein 1966; Tong 2020). This incorrect behavioural assumption is reinforced by the incorrect institutional assumption of perfect or highly competitive product markets. But if one assumes that neoclassical behavioural assumptions prevail, one assumes that firms are always operating along their respective optimal production possibility frontiers (PPFs) whereas they might very well be operating in the interior of their PPFs. Following the findings of fast and frugal heuristics research, I also argue that economic agents using clearly non neoclassical methods (fast and frugal heuristics) can yield superior results. Hence, such heuristic behaviour facilitates achieving higher levels of effort input required to realize production efficiency. But note that heuristics need not always be ‘fast’; they can involve slow thinking (Kahneman 2011). Given that effort is a scarce resource, what one can argue from the fast and frugal approach is that adopting a heuristics approach to decision-making saves this scarce effort resource as compared to what would transpire if neoclassical behavioural norms are adopted. The accumulated effort savings could then, potentially, be used to invest in increasing firm productivity thereby increasing its level of x-efficiency. Moreover, adopting fast and frugal heuristics need not sacrifice on the overall efficiency of choice outcomes with regards, for example, the operation of the shop floor or investment decisions.Footnote 2 This point is illustrated in Fig. 1, where effort input is on the vertical axis and productivity is on the horizonal. Line 0 k represents conventional behavioural norms, whilst 0 g represents heuristics based behavioural norms incorporating fast and frugal heuristics. 0 g yields higher levels of productivity than does 0 k—0f productivity compared to 0d. 0 g (heuristics) represents a more efficient use of effort. Looked at from an effort-use perspective, heuristics requires less effort (0b) to yield a given level of productivity (0d) than conventional behavioural norms (0a). In this scenario, maximum effort is given by 0 m, where conventional norms yield a much lower level of productivity (0f) than what could be generated using heuristics in the decision-making process. This level of productivity requires a much lower level of effort (0 m). The surplus effort could be invested in generating a higher level of productivity. But the savings in effort need not be invested to increase productivity or the level of x-efficiency. Much depends on the preferences of firm members, how the firm is organized, and market forces. Effort, decision-making norms and labour productivity Therefore, unlike with the heuristics and biases approach to behavioural economics neoclassical behavioural norms should not be the benchmark by which to measure optimal behaviour. One cannot assume a priori that efforts to maximize profits or minimize costs will yield economic efficiency irrespective of the processes adopted to realize this end. But this does not vitiate the critical point made in conventional economics that effort maximization is an essential determinant of economic or x-efficiency. The simple point made here is that neoclassical behavioural norms cannot be expected to yield economic efficiency. The type of decision-making norms one adopts critically affects the firm’s level of x-efficiency by impacting on the efficiency by which scarce effort resources are employed. This narrative yields the hypothesis that non-neoclassical (SAXE) heuristic based behavioural norms, in an appropriate institutional environment, can yield higher levels of productivity generating higher levels of x-efficiency. This hypothesis is reinforced by the experimental work of Smith (2005, 149). Smith concludes (2005, 149–150): It is shown that the investor who chooses to maximize expected profit (discounted total withdrawals) fails in finite time. Moreover, there exist a variety of nonprofit-maximizing behaviors that have a positive probability of never failing. In fact it is shown that firms that maximize profits are the least likely to be the market survivors. My point is simple: when experimental results are contrary to standard concepts of rationality, assume not just that people are irrational, but that you may not have the right model of rational behavior. Listen to what your subjects may be trying to tell you. Think of it this way. If you could choose your ancestors, would you want them to be survivalists or to be expected wealth maximizers? This SAXE approach to the theory of the firm has implications for one common interpretation of bounded rationality theory, which infers that boundedly rational behaviour yields sub-optimal decisions since bounded rationality imposes a constraint on rational (‘unboundedly’ rational) neoclassical behaviour (Conlisk 1996). Being boundedly rational results in firms performing in the interior of their production possibility frontier. In this approach the neoclassical narrative is retained. The benchmark for optimality is the neoclassical norm. But from the SAXE approach, bounded rationality simply implies that we live in a non-neoclassical world and that this affects the decisions that rational-smart decision-makers make. And, these rational decisions, in a world of bounded rationality, can’t be the same as what might transpire in a neoclassical world. Heuristics can, therefore, yield better decisions and better outcomes (higher levels of x-efficiency) that can be achieved by abiding by neoclassical behavioural norms. This point elaborated with regards to fast and frugal heuristics specifically, by Todd and Gigerenzer (2003, 160–161): Studying ecological rationality enables us to go beyond the widespread fiction that basing decision making on more information and computation will always lead to more accurate inferences. There is a point at which increasing information and information processing can actually do harm…there are cases where cognitive limitations actually seem to be beneficial, enabling new functions that would be absent without them, rather than constraining the possible behaviors of the system… given that cognitive capacities are free parameters that have been adjusted in the course of evolution, the ability to use simple heuristics may have required the evolution of no more than a certain limited amount of cognitive capacity necessary to execute those heuristics. This argument requires that simple heuristics had a selective advantage over more complex cognitive strategies (which would have required more processing power). But we have already seen that such advantages do exist, in terms of speed and robustness and enabling new functionality. Thus, the benefits of simple limited decision mechanisms may actually partly underlie the emergence of bounded rationality itself. A key point made in this ecological rationality narrative is that given the cognitive wiring of the decision makers and information imperfections the standard for unbiased behaviour often should not be neoclassical norms. What often appears to be an error or bias using neoclassical norms may in fact be a form of optimal behaviour. Neoclassical behaviour would be irrational or biased here. And, therefore, neoclassical behavioural norms should not be used as the norm for optimal behaviour. But the rational use of heuristics need not yield x-efficiency in production. The appropriate heuristics need be chosen to elicit economic efficiency or x-efficiency. Being fast and frugal need not translate into being x-efficient. Much critically depends upon the heuristics adopted inside of the firm and related institutional environment. An important point made by Simon (1987) and Leibenstein (1966) is that firm decision-makers attempt to maximize (or satisfice) in terms of their preferences. But satisficing behaviour, although rational and utility maximizing, need not be consistent with generating economic efficiency. And such utility maximizing behaviour can persist if relatively x-inefficient firms are somehow protected from market forces. Therefore, sub-optimal heuristics (behavioural norms) can persist given appropriate environment conditions (Altman 1999). In this case, even adopting non-neoclassical norms can result in the firm operating in the interior of its production possibility frontier. And we have here a case of rational inefficiency (Altman 2005, 2006, 2017). Ecological rationality is consistent with both rational inefficiency an x-efficiency in production. X-efficient production results from best practice management practices, preferences of decision-makers, the manner in which the firm is organized, and market forces. It takes a micro perspective on firm decision-making and organization and is consistent with the notion of bounded and ecological rationality as well as with smart and rational decision-making (Frantz 1997; Leibenstein 1966, 1979). When a firm is x-inefficient its production possibility frontier lies in the interior of the x-efficient PPF (Altman 2005, 2006). If x-inefficiency is assumed away, as it is in conventional or neoclassical theory, then the revealed production possibility frontier is assumed to be the optimal x-efficient PPF. Following upon a SAXE narrative, attempting to achieve x-efficiency by applying simplistic neoclassical behavioural norms to the firm embedded in the real world of bounded rationality would be one sure method of assuring firm underperformance and, therefore, x-inefficiency in production. In the original x-inefficiency model (Leibenstein 1966), x-inefficiency results in higher unit production costs since managerial decisions in and of themselves result in lower effort inputs and, therefore, lower levels of productivity. This can be illustrated in a one factor input model of the firm, where w is unit labor costs (a proxy for all labour costs), Q is output, and L is labor input: Average cost increases as labour productivity falls as a function of a reduction in effort inputs. In this case x-inefficient firms survive only if all firms are equally x-inefficient or if they are protected from competitive pressures. If the level of x-efficiency is contingent upon the costs of labor inputs, such as incentives to labor and management, different levels of x-inefficiency can be balanced by countervailing differences in production costs. Relatively x-efficient firms would be characterized by the same average costs as x-inefficient firms. In this case, x-inefficient firms can survive even in the face competitive product markets. In Eq. 1, changes in w can be neutralized by changes in productivity. This is important to note since firms need not be economically efficient to survive on the market even when product markets are relatively competitive since x-inefficiency need not generate higher average costs. From the fast and frugal narrative, one would hypothesize that neoclassical behaviour will yield a lower level of output than is feasible—one would be in the interior of the production possibility frontier. Based on our modeling above, fast and frugal heuristics, which can be removed from optimal neoclassical behaviour, should yield higher levels of productivity, higher levels of x-efficiency than neoclassical norms. The fast and frugal production possibility frontier would, therefore, lie above the neoclassical PPF. However, there might be an array of non-neoclassical behavioural norms and parameters that yield x-efficiency in production. Specifying what they are, requires a nuanced understanding of the firm and its current institutional and market context. For example, x-efficiency in production might require frugal heuristics, efforts to minimize moral hazard, and more fairness introduced into the incentive environment of the firm. A production possibility frontier based on this more holistic approach to the firm (going beyond, but incorporating the insights from the fast and frugal narrative) could reflect an even higher level of x-efficiency and should, therefore, lie even above the fast and frugal PPF. Which decision-making norms firm members adopts, however, plays an important role in explaining a firm’s level of x-efficiency. Some of these points are illustrated in Fig. 2. If the neoclassical or conventional behavioural norms is the benchmark for achieving optimal firm performance, given by PPF CONXI (conventional norms-x-inefficiency), then the heuristics and biases narrative suggests that the human preference for heuristics yield a PPF, H&B, below CONXI reflecting a lower level of productivity and a higher level of x-inefficiency. The fast and frugal narrative suggests a superior PPF given by the Heuristics PPF. Therefore, the conventional PPF, CONXI, reflects x-inefficiency in production not economic efficiency as typically assumed. Going beyond the important heuristics narrative, adding a more appropriate decision-making environment and decision-maker preferences into the decision-making mix yields an even higher PPF given by SAXE, the Satisficing Boundedly Rational and Smart Fast and Frugal, X-Efficient production function. The Satisficing Boundedly Rational and Smart Fast and Frugal, X-Efficient production function Applying neoclassical behavioural norms and ignoring the intricacies of firm decision-making in the real world of bounded rationality would be expected to shift the firm’s production possibility frontier inward as it engenders an inefficient use of scarce effort resources in the decision-making process, thereby reducing output per unit of factor input. This pushes the firm further from its potential of producing at economic or x-efficiency.",3
18.0,2.0,Mind & Society,15 November 2019,https://link.springer.com/article/10.1007/s11299-019-00220-5,Business education: Does a focus on prosocial values increase students’ pro-social behavior?,December 2019,Malte Petersen,Monika Keller,Wasilios Hariskos,Male,Female,Unknown,Mix,,
18.0,2.0,Mind & Society,04 November 2019,https://link.springer.com/article/10.1007/s11299-019-00221-4,Heuristics as tales from the field: the problem of scope,December 2019,Simone Guercini,,,Female,Unknown,Unknown,Female,"The “scope” of heuristic rules refers to the fit of the rule based decision-making with the context. “Scope” is the (extention of the) field in which a heuristic rule can be applied (with success). Scope of the decision-making approach has been considered in the studies of decision-making processes that generally divide them into two, mutually exclusive types: rational decision-making versus rule-based decision-making (March 1994). Recently, Gigerenzer (2019) discusses the issue of axiomatic rationality, delivering as a first conjecture that the normative power of axiomatic rationality is limited to those that Savage called “small worlds”. Focusing on the concept of ecological rationality, he states that “a heuristic is ecologically rational to the degree that it is adapted to the structure of an environment. The study of the ecological rationality requires formal models of heuristics and an analysis of the structures of environments these can exploit. It lays the foundation of a moderate naturalism in epistemology, providing statements about heuristics we should use in a given situation” (p. 2). This paper focuses on the concept of “scope” as an attribute of the heuristic rule that defines the boundaries of the field where a specific heuristic “should” be used (Gigerenzer 2019). In rule-based decision making the logic of appropriateness and identity contrasts the logic of consequences and preferences of rational decision making (March 1994). Heuristics define solutions to decision problems starting from one or a few “cues”. They can perform better than complex and “information-intensive” decision models (Gigerenzer et al. 1999). Looking at the scope means shifting the attention from the accuracy of the decision-making model to the borders of the context where it is effective (task environment). The scope is related to the structure of the adaptive toolbox (Gigerenzer and Selten 2001) adopted by the actor to get effectiveness: for their tasks decision makers can use few heuristics with a large scope or many with a narrow scope. With reference to this subject, this paper is based on evidence collected through researches conducted in almost 20 years in the textile industry in the context of a specific European industrial district. It proposes examples from ethnographic interviews and observation, to discuss the scope of heuristic rules in decision-making. The role of heuristics has been studied in cognitive sciences and applied psychology. Recently it has been object of attention on different perspectives in organization behavior, strategic management, business and entrepreneurial decision-making (Artinger et al. 2015; Bingham et al. 2015; Guercini et al. 2014; Loock and Hinnen 2015; Shepherd et al. 2015). Starting from a literature review and an ethnographic research, this paper discusses the scope of heuristic rules in decision-making. Different approaches to the problem of scope emerge in order to applying heuristic rules to tackle the cognitive limitations when facing decision-making problems. In preliminary terms, we have to say how the problem of scope can be defined in the heuristic rules adopted by the actors.",13
19.0,1.0,Mind & Society,25 May 2020,https://link.springer.com/article/10.1007/s11299-020-00232-6,The story of the pandemic: navigating our way between optimism and pessimism,June 2020,Rona Unrau,,,Female,Unknown,Unknown,Female,,1
19.0,1.0,Mind & Society,24 April 2020,https://link.springer.com/article/10.1007/s11299-020-00226-4,The new frontiers of AI in the arena of behavioral economics,June 2020,Mario Rasetti,,,Male,Unknown,Unknown,Male,,2
19.0,1.0,Mind & Society,21 May 2020,https://link.springer.com/article/10.1007/s11299-020-00233-5,Triple trouble,June 2020,Steven Jon Kaplan,,,Male,Unknown,Unknown,Male,,
19.0,1.0,Mind & Society,05 February 2020,https://link.springer.com/article/10.1007/s11299-019-00222-3,Arrow’s impossibility theorem as a special case of Nash equilibrium: a cognitive approach to the theory of collective decision-making,June 2020,Edgardo Bucciarelli,Andrea Oliva,,Male,Female,Unknown,Mix,,
19.0,1.0,Mind & Society,27 February 2020,https://link.springer.com/article/10.1007/s11299-020-00223-7,‘Zero-error’ versus ‘good-enough’: towards a ‘frugality’ narrative for defence procurement policy,June 2020,Kapil Patil,Saradindu Bhaduri,,Male,Unknown,Unknown,Male,"Public procurement in recent years has emerged as an essential innovation policy tool across countries. The strong demand pool exerted by such public procurement serves as an important incentive for firms to undertake risks of R&D and innovation (Edler and Georghiou 2007; Lember et al. 2014). Traditionally, governments around the world allocate substantial resources to procure new weapon systems and military equipment through either ‘off the shelf’ purchase or indigenous development. The defence procurement environment, however, is complex. On the one hand, there are uncertainties and severe resource constraints due to regularly changing threat perceptions, limited flow of information about new technologies, and the growing demand to reduce defence related expenses (Dertouzos 1994; Sceral et al. 2018). On the other hand, several stakeholders remain pre-occupied with the demand for ‘zero-error’ technologies (See, Davis 2015; Gholz and Sapolsky 2000). The decisions concerning the indigenous development and procurement of complex military systems (CoPS), in particular, have generated much interest owing to considerable uncertainty over technological choices, innovation, and their end-user acceptance. By CoPS, we refer to the systems, networks, infrastructure, engineering constructs and services that are highly costly and technology-intensive (Hobday 1998). Due to their ability to provide comprehensive situational awareness, the demand for newer and cutting edge CoP systems has intensely grown over the years. The procurement of such systems, however, is frequently marred with problems of time and cost overruns, performance gaps, and, at times, even a complete failure, resulting in substantial financial losses (Flyvbjerg 2011; Schwartz 2014; Bogan et al. 2017). Allegedly, the ‘optimisation driven’ end-user requirements and the demand for ‘zero-error’, often, lie at the core of such problems of time and cost overruns (Park 2012; Smith and Tranfield 2005).Footnote 1 The paper examines the extent to which a heuristics-based frugal decision-making process can offer an efficient response to these problems. Under uncertainty, the ‘frugality’ approach emphasises on taking decisions based on heuristics and ‘simple rules of thumb’. A frugal approach prioritises “what works in the actual environment’ over “what ought to work” (Gigerenzer and Todd 1999). In CoPS, such strategies might enable system designers, and procurement practitioners to respond to uncertainty emerging from lack of data for making ‘reliable’ predictions about cost, schedule and performance levels vis-à-vis the ‘highest’ specifications, interpreted to mean a preference for ‘satisficing’ over ‘optimization’ (Klein 2002). Here, frugality helps to yield satisficing solutions where a payoff reasonably meets the “aspiration” level (Simon 1967; Gigerenzer 2010). Over time, however, such solutions can be further improved through adaptive learning and incremental changes. Section 2 analyses the institutional superstructure of defence procurement mechanism with a view to underline ‘optimisation’ as the underpinning dynamic. The necessities for an alternative framework, and the challenges to achieving it, are briefly pointed out too in this section. Section 3 elaborates on the conceptual framework of frugality and its importance in innovation and technological capability building efforts in CoPS. Section 4 outlines the research method and data sources, and analyses case studies of CoPS acquisition of missile system, fighter aircraft and weapon locating radar by the Indian authorities using the framework of frugality. The last section presents an analytical ‘rounding up’ and lays out a few policy instruments to infuse a frugality narrative in defence procurement.",2
19.0,1.0,Mind & Society,08 April 2020,https://link.springer.com/article/10.1007/s11299-020-00224-6,"Relations between type of army service, incidental emotions and risk perceptions",June 2020,Sharon Garyn-Tal,Shosh Shahrabani,,Female,Unknown,Unknown,Female,"Military service in general and combat service in particular can be physically and psychologically stressful. Hence, type of service may have significant implications regarding soldiers’ emotions, judgments and risk perceptions during and after their military service. Over the last two decades, many studies have focused on how combat experiences are related to post-trauma, time preferences, difficulties in adjusting to a non-wartime environment, and the tendency to engage in high-risk behaviors (Brænder 2015; Browne et al. 2008; Fear et al. 2007; Killgore et al. 2008; Wilk et al. 2010). Several studies have suggested that combat deployment may affect propensity to risk, and risk behaviors among soldiers and ex-soldiers (e.g., Kelley et al. 2012; Killgore et al. 2008; Thomsen et al. 2011; Garyn-Tal and Shahrabani 2015). For example, Killgore et al. (2008) found that US soldiers with acute and intense combat experience were at greater risk of engaging in high-risk behaviors post-deployment. Thomsen et al. (2011) found that those who had been deployed in combat engaged in more risky behaviors than those who had never been deployed in combat. In addition, Garyn-Tal and Shahrabani (2015) found that among male Israeli soldiers, combat experience significantly and positively correlated with consumption of illegal substances, though this effect was mitigated after army discharge. Nevertheless, while many previous studies have focused on risk propensity and risky behavior among soldiers, knowledge is still lacking regarding the impact of specific type of army service on soldiers’ risk perceptions. Examining risk perceptions among those exposed to life-threating situations is important since such situations may influence their propensity to take risks and their decision-making. In addition, previous studies placed less emphasis on the impact of soldiers’ incidental and unrelated emotions during their service on their risk perceptions. Loewenstein and Lerner (2003) distinguish between integral emotions and incidental emotions. Integral emotions are those targeted at an immediate interaction. During a social interaction, for example, an individual may express anger toward someone else’s behavior. In contrast, incidental emotions are triggered by an event or object that is unrelated to the situation at hand (Loewenstein and Lerner 2003). For example, an argument with a spouse before leaving the house may influence that individual’s feelings in a later and unrelated interaction. Despite originating in unrelated sources, incidental emotions come into play in subsequent situations that involve judgment and decision-making. The current study focuses on soldiers actively serving in the Israeli army in combat or non-combat units. The study adds to the existing literature by examining how type of army service, risk perceptions and negative incidental emotions interact when soldiers are not engaged in war or threatened in some other way. The objective of the current study was to examine the following three questions: (a) Does type of army service (combat vs. non-combat) correlate with risk perceptions? (b) Do levels of incidental emotions differ between combat soldiers not under immediate threat and non-combat soldiers? (c) Do levels of incidental emotions among currently serving combat and non-combat soldiers correlate with their risk perceptions? Compulsory army service in Israel, and especially combat service, constitutes a focused threat to soldiers’ safety. Among other things, this threat may elevate negative incidental emotions, which in turn may influence soldiers’ everyday judgment, risk perceptions and decision-making in different domains of life. In fact, individuals have been known to engage in activities to ease their negative emotions (Isen 1997). For example, the incidental negative emotions elicited by a life-threatening situation may in turn activate thoughts, specific motives and goals (e.g., death anxiety) that may affect behavioral motivations. Ben-Zur and Zeidner (2009) found that a perceived threat to one’s life increases the tendency to take risks, at least in some decision domains. An examination of the factors affecting risk perceptions among soldiers can provide a better understanding of young people’s propensity to engage in risky behavior during their army service. The results of the study can have implications for policy steps to help soldiers during and after their army service. In fact, the results may be relevant not only for soldiers but also for any group that faces high-risk situations (e.g., firefighters, police officers, healthcare workers). Based on the study by Benzion et al. (2009), the current study focuses on assessing three domain-specific risks that soldiers may face: (1) their personal risk of being involved in a terrorist incident unrelated to their army service, (2) their risk of being injured in a car accident, and (3) their risk of being the victim of a violent crime. We decided to exclude routine hazards that soldiers may be less concerned about, such as the risk of catching the flu, as well as matters that are not relevant for soldiers, such as the risk of traveling to destinations with travel advisories. The current study is unique in that, unlike previous studies, it uses data from individuals whose army service was compulsory rather than voluntary. The unique attributes of soldiers that voluntarily join the military may be biased. For example, previous research suggests that individuals who volunteer for military service may be relatively more prone to engage in risk-taking behaviors (Killgore et al. 2008). In Israel, however, compulsory military service results in a broad and diversified population of soldiers. National military service is compulsory for all Israeli citizens over the age of 18, with exemptions given on religious, physical or psychological grounds (Cohen 1995). In the Israel Defense Forces (IDF) men serve 32 months, while women serve 2 years, though women who volunteer for combat positions often serve longer. The IDF’s decision regarding where to place recruits takes into account the needs of the military, the recruit’s health profile and other behavioral and educational profiles, and finally, though not necessarily, the recruit’s preferences. Combat service is compulsory for men, with the exception of a few special sub-units for which men must volunteer. In contrast, women must volunteer for combat service. During their compulsory military service, soldiers are paid a symbolic sum per month (a few hundred Israeli Shekels, equivalent to about $200). Combat soldiers receive slightly higher salaries than non-combat soldiers. Several types of soldiers serve in the IDF: (1) soldiers doing their regular mandatory service; (2) soldiers serving in the standing army; (3) reserve soldiers. The standing army consists of soldiers who have chosen to continue serving in the military after their compulsory service ends. Many of these choose to make the military their career. Soldiers in the standing army receive full wages and are entitled to an army pension. Reserve soldiers are citizens who have been called up for active duty that can include training and fighting. Standard annual reserve service usually lasts a few weeks. Israeli citizens usually continue to serve in the reserves until the age of 43–45. Previous studies have proposed many factors that may affect individuals’ risk perceptions. For example, studies of risk perceptions examined attributes or descriptions of a specific event that may affect individuals’ judgments of the likelihood such an event will occur (e.g., Lichtenstein et al. 1978; Tversky and Koehler 1994). According to the availability heuristic (Tversky and Kahneman 1974), a vivid event is easier to recall or imagine and this ease of recall inflates likelihood judgments. Therefore, individuals judge vivid events as more likely to occur than pallid events (Johnson et al. 1993; Sherman et al. 1985). Yet, the findings of Barron and Erev (2003) indicate that personal experience with similar events reduces an individual’s sensitivity to risk. Consistent with the personal experience hypothesis, Rosenboim et al. (2012) found that those who lived outside the range of the rockets fired at southern Israel and had little or no experience with terror attacks were more pessimistic than who lived in the area and had more experience with terror attacks. Further, Barnett and Breakwell (2001) examined the extent to which differences in risk experiences can explain individual variability in risk assessments. They found that this relationship depends on whether or not the risk experiences were voluntary (see also Garvin 2001). In addition, individuals view new risks differently than they view those that are familiar. Once individuals have assessed a particular risk, their opinions can be difficult to change (Siegrist and Cvetovich 2000). Other studies have examined the relationship between emotions and risk perceptions (e.g., Slovic et al. 2004; Slovic and Peters 2006; Västfjäll et al. 2014; Benzion et al. 2009; Lerner et al. 2003; Shahrabani et al. 2009). For example, Benzion et al. (2009) and Shahrabani et al. (2009) empirically examined the effect of negative emotions on risk perceptions in the context of the 2006 Israel-Lebanon war. Shahrabani et al. (2009) found that higher scores on the Negative Emotions index (higher levels of fear and anger) correlated with an increase in general perceived self-risk and risk of being in a terrorist incident among people who experienced missile attacks. These results support the valence approach (Johnson and Tversky 1983), according to which fear and anger lead to pessimistic risk perceptions. In addition, Benzion et al. (2009) showed that people who were fearful and angry because of the war were more pessimistic in their risk estimates. Moreover, Lerner et al. (2003) found that as a result of the events of September 11, 2001, fearful people made relatively pessimistic risk assessments, while angry people made relatively optimistic risk assessments, consistent with the appraisal-tendency approach (Lerner and Keltner 2000). Several studies have examined incidental emotions, which include all factors that elicit affect but are unrelated to the judgmental target (Loewenstein and Lerner 2003). In fact, according to the valence approach, priming of negative incidental mood increases judgments regarding the probability of negative events, even when the content of the priming is not similar to that of the events in question (Johnson and Tversky 1983; Wright and Bower 1992). For example, by reminding Swedish participants of the 2004 tsunami in Thailand, Västfjäll et al. (2014) elicited negative affect, which in turn resulted in more pessimistic expectations about their future. The relationship between emotions and decision-making has also been examined under conditions of everyday life. For example, the “affect heuristic” (Slovic et al. 2007) describes the use of affect in everyday decision-making. The authors claimed that images marked by positive and negative affective feelings guide judgment and decision-making. For example, Slovic et al. (2007) mentioned that according to Zajonc (1980), all perceptions contain some affect. “We do not just see ‘a house’: We see a handsome house, an ugly house, or a pretentious house” (Zajonc 1980, p. 154). In addition, the findings of Andrade and Ariely (2009) indicate that decisions based on incidental emotions can affect future decisions even after the initial emotion has subsided. Hogarth et al. (2011) showed that perceptions of risks encountered in everyday life are related to deviations in participants’ mood states and emotional reactions, demonstrating that mood state and emotions can explain significant variance in risk perception. In this study, respondents were asked to assess their personal risk of being involved in a terrorist incident (unrelated to their army service), being injured in a car accident and being the victim of a violent crime. They were also asked to assess the extent of their negative incidental emotions (fear and anger) during the past week under non-threatening conditions. Soldiers serving in combat units usually find themselves in intense and harsh situations that often evoke high levels of fear and anger. On the one hand, those who find themselves in such harsh situations may tend to experience high levels of negative emotions and even post-trauma when they are not on duty. On the other hand, these individuals may tend not to be easily frightened or angry since they can distinguish between routine and unusual negative situations. Therefore, we expect that: Under routine conditions, people with combat experience will tend to report levels of negative incidental emotions that differ from those who do not have combat experience. The impact of combat experience on risk perceptions may go in two opposite directions. On the one hand, direct experience has been found to have a positive effect on risk perception (for review, see Wachinger et al. 2013). For example, Ruin et al. (2007) found that individuals without any actual experience with floods tended to underestimate their danger, whereas individuals who had directly experienced floods tended to overestimate the danger. In addition, according to the availability heuristic (Tversky and Kahneman 1974), a vivid event is easier to imagine or recall, thus increasing the likelihood of judgments. According to these theories, we would expect people with combat experience to have higher perceptions of risk (more pessimistic views) than those who were not in combat, since those in combat units were exposed to intense or “vivid” events and realized that “things can happen.” On the other hand, according to the personal experience hypothesis (Barron and Erev 2003; Yechiam et al. 2005), personal experience with similar events reduces an individual’s risk sensitivity. Empirical studies support the personal experience hypothesis. Individuals that previously experienced a hazardous event but were not harmed personally are more likely to believe that a future event is unlikely to affect them, thus decreasing their risk perception (Halpern-Felsher et al. 2001; Scolobig et al. 2012; Wachinger et al. 2013). Therefore, according to these theories we would expect individuals with combat experience to be less pessimistic regarding risky events than those serving in non-combat units. Since the direction of the impact of type of army service on risk perception is not clear, we hypothesize: People with combat service experience will tend to report levels of risk perception that differ from the levels reported by those without combat experience. The valence approach (Johnson and Tversky 1983; Wright and Bower 1992) and the appraisal-tendency framework (Lerner and Keltner 2000) both maintain that fear leads to pessimistic risk perceptions. Yet while the valence approach predicts that angry people will also tend to be more pessimistic, the appraisal-tendency theory predicts that angry people will tend to be more optimistic (Lerner and Keltner 2000, 2001). Several empirical studies conducted in Israel showed that people who were fearful and angry because of the war had more pessimistic risk estimates (Benzion et al. 2009; Shahrabani et al. 2009, 2012). Since the current study was conducted in Israel, we do not expect to find differences between the impact of fear and that of anger on risk perceptions. For the most part, previous research has manipulated emotions and measured them in the wake of extreme events (e.g., the events of September 11 or forest fires). In the current study, in contrast, we measured negative incidental emotions under non-threatening conditions (e.g., not related to major events). Thus, as we do not expect to find any differences in the impact of different negative incidental emotions, we hypothesize that: Fearful people will make pessimistic estimates with respect to their personal risk of being involved in a terrorist incident, being injured in a car accident and being harmed by a violent crime. Angry people will make pessimistic estimates with respect to their personal risk of being involved in a terrorist incident, being injured in a car accident and being harmed by a violent crime. We added two control variables to the analytical model used to examine the relationship between type of army service and risk perceptions: gender and participants’ risk attitude. Garyn-Tal and Shahrabani (2015) found positive and significant correlations between risky behavior and gender, and between risky behavior and participants’ risk attitude. In particular, these researchers found that men are more likely to travel to a destination with a travel advisory and that individuals with high Evaluation of Risks (EVAR) scores have a greater probability of engaging in risky behavior, whether legal or illegal. Therefore, we hypothesize that gender and risk attitude may exhibit correlations with participants’ risk perceptions.",
19.0,1.0,Mind & Society,07 April 2020,https://link.springer.com/article/10.1007/s11299-020-00225-5,Agreement by conduct as a coordination device,June 2020,Arnald J. Kanning,,,Unknown,Unknown,Unknown,Unknown,,
19.0,1.0,Mind & Society,18 April 2020,https://link.springer.com/article/10.1007/s11299-020-00227-3,Problems and solutions: an ecological view,June 2020,Warren Thorngate,,,Male,Unknown,Unknown,Male,"Most psychologists agree that our behaviour (B) is influenced both by our personal characteristics (P) and by characteristics of our environment (E) (see, for example, Lewin 1936; Barker 1968; Bronfenbrenner 1979; Gibson 1966; Mischel 1973; Todd and Gigerenzer 2012). Yet progress in understanding how P, E and B interact to influence each other has been slow. Many researchers study how P influences B or how E influences B, but not both. Those who do study both sample P, B and E variables idiosyncratically. Thus, for example, one researcher might observe that students’ anxiety (P) and class size (E) influence students’ grades. Another researcher might observe that risk-taking (B) in a simulated stock market increases with market volatility (E) but only for amateur investors (P). Without a theoretical framework to guide interpretations of their results (Brunswik 1954), it is difficult to detect any commonalities in such findings. The purpose of the present article is to sketch a theoretical framework—a perspective—for interpreting many of the relationships among B, P and E. Seminal ideas for the framework came from Powers’ (1978, 1989) theory of perceptual control, from Bandura’s (1989) discussions of human agency, and from Slobodkin and Rapoport’s (1974) concept of biological rationality (see also, Thorngate and Tavakoli 2005). The ideas were further shaped by Kelley et al.’s (2003) classifications of social interdependence, Maynard Smith’s (1982) concepts of biological interdependence, and by Merton’s (1936) discussion of side effects and unintended consequences. The proposed framework was constructed over several years of ruminations about two universal literary themes: possession and desire. These themes appear in all stories of protagonists who want something they do not have, or who have something they do not want. The subsequent plot almost always describes how the protagonists attempt to obtain or retain what they desire, or to remove or prevent what they don’t. We call these attempts problem solving. What do we know about them? Almost all psychological research on problem solving “zooms in” on the thinking secreted between problems and their solutions (e.g., Bruner et al. 1956; Newell and Simon 1980). But there is more to problem solving than thought. The framework proposed here adds motivation, emotion, attention and interpersonal relations to the mix. It also “zooms out” to address the consequences, intended and otherwise—of problems created by problems solved. The framework offers, in effect, a problem-solving perspective on B, P, E relations. The perspective allows us to consider not only how persons and environments can influence behaviour, but also how behaviour can influence persons and environments. I call it a problem ecology perspective. The present article offers a brief overview of problem ecology concepts and some of their implications. Like all brief overviews, details are sparse. Still, if the article directs more attention to B, P and E relationships, its purpose will have been served.",1
19.0,1.0,Mind & Society,20 April 2020,https://link.springer.com/article/10.1007/s11299-020-00228-2,"Conservation of behavioral diversity: on nudging, paternalism-induced monoculture, and the social value of heterogeneous beliefs and behavior",June 2020,Nathan Berg,Yuki Watanabe,,Male,,Unknown,Mix,,
19.0,1.0,Mind & Society,29 April 2020,https://link.springer.com/article/10.1007/s11299-020-00230-8,From Pan to Homo sapiens: evolution from individual based to group based forms of social cognition,June 2020,Dwight Read,,,Male,Unknown,Unknown,Male,"The evolution from pre-human primates to modern Homo sapiens through the hominins over the time period from around 10 mya to the time of the Upper Paleolithic is a complex one, involving many domains, both at the individual and the collective levels. As indicated in Fig. 1, these domains include morphological, technical, social, behavioral, linguistic and cognitive dimensions undergoing change as the evolutionary pathway from the hominins to modern Homo sapiens played itself out. Not only are there evolutionary changes along each of these dimensions, in many instances exaptation (see Gould and Vrba 1982) has led to evolutionary changes in one domain also being taken up in a different domain (examples include: language syntax [Fitch 2011], human speech production [MacLarnon 2012], and hominin memory systems [Murray, Wise and Graham 2017]). Rather than a single line of evolution, the evolution of the hominins leading to Homo sapiens involves different, but intertwined, evolutionary threads (Foley 2016), with Homo sapiens arising as a taxonomically distinct primate species within which are found a suite of traits otherwise distributed individually across different primate species (Chapais 2008). Evolutionary events from 2 mya to present leading to the relation-based systems of social organization that arose during hominin evolution to Homo sapiens. Events are divided into (from bottom to top): brain, technology, food, body (morphology), male/female (relations), language and culture. Rectangular panels identify relevant major behavioral events that occurred during this time period. No change in a single kind of event is a driver for the evolutionary changes in other kinds of events. The interplay among events has been left implicit One of these intertwining evolutionary threads—the focus of this article—relates to a qualitative transformation in the social domain realized during the evolutionary trajectory going from a chimpanzee-like common ancestor of Pan and Homo to Homo sapiens. The transformation is from social relations being worked out individually at the phenomenal level through face-to-face interaction to the construction of relation-based social systems formulated culturally and at the ideational level, thereby enabling social relations to be expressed through language and worked out collectively (Read 2012). A quintessential example of what is meant here by relation-based systems of social interaction is the universal system of kinship relations found in human societies and expressed and understood through the kin terms making up what anthropologists refer to as kinship terminologies. In general, relation-based social systems universally provide the framework within which social interaction in human societies takes place and contrast sharply with the non-human primate social systems they replaced, which are based on extensive face-to-face interaction. By the latter is meant that an individual works out and develops understanding of the behavior of other group members through face-to-face interaction with those group members, thereby making their individual behavior more predictable, with the latter a pre-requisite for the formation of coherent and stable patterns of social interaction. In contrast, the transformation to relation-based social systems enabled collective, and not just individual, understanding of the expected behavior patterns of group members. As will be discussed in this article, the evolutionary transition of the hominins from individual to relation-based systems of social interaction resolved the opposition faced by the non-human primates between, on the one hand, maintaining coherent forms of social organization as group size increased and, on the other hand, coping with the increase in social complexity introduced by selection for the individualistic behavior that was part of the phylogenetic transition going from the Old World monkeys to the great apes. Individualistic behavior on the part of group members is the antithesis of coherent social groups: The essence of social existence is not to be found in the instincts of isolated individuals but in … the social group … thought of as enormously more significant than the individual; and social behavior and social institutions must be recognized as more permanent than any individual traits. (Judd 1925–1926:154–155). The tension between individualistic behavior and the coherency of a social unit such as the family can be seen in the observation regarding married couples in the United States that there is a “standing tension between the principle of individualism and the demands of marriage” (Quinn 2018:152). This opposition between individualistic behavior and social behavior was resolved culturally during hominin evolution, it will be argued, by working out a means to accommodate socially the highly individualistic behavior that would also have been a part of the behavioral repertoire of the hominins since highly individualistic behavior characterizes both the extant great apes and humans. The accommodation to individualistic behavior was achieved by developing a cultural framework that, among other things, defines a system of kinship relations linking group members to one another, along with expected positive and supportive behavior on the part of those who are mutual kin according to this system of kinship relations. This led to formation of cultural frameworks that enabled realization of the. … cultural goal of controlling, regulating, and where necessary or expedient, suppressing the genetic program so as to allow an otherwise fractious group of individuals to maintain themselves as a functioning sociocultural system … [through] the desired cultural kinship that makes social harmony possible … and [is] maintained over time. (Paul 2018: 65) The cultural framework that came into play and will be discussed below begins with the cognitive innovation during hominin evolution of the concept of a relation abstracted from patterned behavior, such as the concept of a mother relation abstracted from the mammalian pattern of female mothering behavior directed towards her offspring. The concept of a relation was subsequently extended further through the cognitive innovation of forming a new relation recursively from an already abstracted relation by forming the relation of a relation. This led to forming systems of new relations from already understood relations such as the relations representing the structural organization of a family as a social unit (see Read 2015). Initially, this gave rise to genealogical systems of relations, and subsequently to symbolic, computational systems of relations expressed through the kin terms making up what is referred to by anthropologists as a kinship terminology (Read 2019). A terminology expresses the kinship relations central to the systems of social organization that replaced face-to-face interaction as the means to work out, within a group, individualistic social relations among group members. Kinship relations are also the means by which an offspring of a female is identified as a group member through groups becoming organized around a conceptual system of relations and expected patterns of behavior. Groups formed in this manner also incorporate collectively the knowledge individuals gain pragmatically through their interactions, both socially and with their physical and ecological environments, with this knowledge transmitted across generations through enculturation (Leaf and Read 2012). In the hunter-gatherer, relation-based systems that came into play during the Upper Paleolithic and are based on resource procurement rather than resource production, the knowledge called upon by individuals in their daily activities expanded from behavior at the phenomenal level to include behavior represented culturally at the ideational level and transmitted to the offspring of group members through enculturation. The enculturation undergone by a newborn as he or she matures necessarily involves interaction in a social context with other individuals, themselves already enculturated into the cultural milieu of that group. This is analogous to the initial learning of a language since language learning involves daily linguistic interaction with the members of a language community. Neither language learning nor cultural enculturation can be reduced to just being an instance of phenotypic transmission of a phenotype trait from one individual to another through (but not exclusively) imitation. The cultural systems that began to be developed during hominin evolution leading to Homo sapiens and transmitted through enculturation are what makes us human and not just a smarter, more social ape (Read 2012).",2
19.0,1.0,Mind & Society,08 May 2020,https://link.springer.com/article/10.1007/s11299-020-00229-1,"On the category adjustment model: another look at Huttenlocher, Hedges, and Vevea (2000)",June 2020,Sean Duffy,John Smith,,Male,Male,Unknown,Male,"Psychologists have understood that judgments are a topic worthy of study, notably because perception and memory are imperfect. One clever method for studying judgments is to present participants with stimuli that have objectively measurable properties. For example, the stimulus could be a line with certain dimensions. We refer to a specific stimulus as a target. The target then disappears and participants are asked to reproduce some aspect (such as hue, volume, or length) of the target. We refer to this as the response. This task is repeated for targets of various characteristics. It has been known for some time that when participants perform repeated judgment tasks there is a bias toward the mean of the distribution of targets (Hollingworth 1910; Poulton 1979). For instance, in judgments of the length of lines, longer lines tend to be underestimated and shorter lines tend to be overestimated. This effect is sometimes referred to as the central tendency bias.Footnote 1 Because participants imperfectly remember and perceive targets, Huttenlocher et al. (2000), hereafter referred to as HHV, propose that participants use information about the distribution of targets to improve their judgments. HHV name this the category adjustment model, hereafter referred to as CAM. CAM predicts that judgments will be a weighted average of the imperfect memory of the target and the mean of the distribution of targets.Footnote 2 In the description of CAM, the authors state,Footnote 3 “Our model is a precisely specified Bayesian model…The mean of this posterior distribution is called the Bayesian estimate; it has the property of being the ‘most accurate’ estimate in the sense that it minimizes average error”. (p. 221). Since judgments will be an optimal weighted average of the imperfect memory of the target and the mean of the distribution, CAM offers a Bayesian explanation of the central tendency bias. In order to test the predictions of CAM, HHV perform three experiments. Participants perform a series of judgment tasks on the fatness of computer generated images of fish (Experiment 1), the greyness of squares (Experiment 2), and the lengths of lines (Experiment 3). In each of these experiments, participants perform these judgments under four different distributions of targets, which exhibit different means and standard deviations. HHV conduct their analyses on data that had been averaged across trials and averaged across sets of previous targets. HHV conclude by stating, “The experiments verified that people’s stimulus estimates are affected by variations in a prior distribution in such a manner as to increase the accuracy of their stimulus reproductions”Footnote 4 (p. 220). However, despite the assertion of HHV to the contrary, one simple alternate hypothesis is that there is a bias toward a set of recent targets rather than a bias toward the mean of the distribution. We note that this is a non-Bayesian explanation because participants are not predicted to exhibit learning. CAM asserts that participants have beliefs of the distribution of targets. While we are not able to observe the mean of the beliefs the distribution, well-known results show that, under mild assumptions, Bayesian learners will have beliefs that converge to the truth (Savage 1954; Blackwell and Dubins 1962). Therefore, in the analysis that follows, we use the running meanFootnote 5 of the targets as a proxy for the participant’s beliefs of the mean of the distribution of targets. We note that sets of recent targets are simply noisy versions of the running mean. As such, tests involving averaged data will not be able to distinguish between the hypothesis that there is a bias toward the running mean and the hypothesis that there is a bias toward a set of recent targets. Unfortunately, HHV only analyze averaged data and therefore these two hypotheses are not distinguishable.Footnote 6 In this paper, we explore the extent to which the data can be explained by this alternate hypothesis. Since the authors could not locate their datasets, we replicated the conditions in Experiment 3 from HHV. In our data, we find strong evidence of a bias toward recent targets and not toward the running mean of the distribution. This result is not consistent with CAM. In order to address the concern that our methods might not be able to detect a bias toward the running mean, we simulate data that exhibits a bias toward the running mean and not toward recent targets. Our methods correctly identify a bias toward the running mean and not toward recent targets in this simulated data. Further, CAM is a mathematical model and this allows the researcher to devise non-obvious predictions that are consistent with the model. As Bayesian learners will have beliefs that converge to the true distribution, if participants are Bayesian then we should observe evidence of learning across trials.Footnote 7 We subject the data to several tests of learning. We do not find evidence of the joint hypothesis that participants learn the distribution and employ this information in their judgments. These results are not consistent with CAM. Further, since there is no evidence of learning, it is difficult to see how this is consistent with any Bayesian model of judgment. It is worth noting that the entire contents of publications are assumed to be correct unless stated otherwise. Further, science can only be self-correcting if mistakes are identified and suggestions are made in order to avoid such mistakes in the future. A corollary to this is that declining to mention errors serves to prevent the progress of science. We therefore point out several problems with HHV, including dividing by zero. The alternative is to have incorrect aspects of a publication that are incorrect and assumed to be correct. To our knowledge, Duffy and Smith (2018) is the only other paper on CAM to use the methods that we employ here. Duffy et al. (2010) claim that the results of their experiments are consistent with CAM. Duffy and Smith (2018) reexamine the data from Duffy et al. (2010) by analyzing judgment-level data rather than analyzing averaged data. Duffy and Smith do not find evidence of CAM in the Duffy et al. (2010) data. As we do here, Duffy and Smith find that there is a bias toward recent stimuli rather than toward the running mean of the distribution. Duffy and Smith also test whether there is evidence of learning the across trials and they fail to find evidence of learning. Duffy and Smith conclude that the Duffy et al. judgments are not consistent with CAM.Footnote 8 Here we perform an analysis similar to that of Duffy and Smith (2018). The contributions of our paper are as follows. Contrary to the conclusions of HHV, we do not find evidence that the judgments are consistent with CAM. We do not find evidence of a bias toward the running mean and we also do not find evidence of the joint hypothesis that participants learned the distribution and employed this information in their judgments. It seems that evidence for CAM is a statistical artifact that appears when researchers analyze data averaged across trials and do not consider a recency bias. We also hope that our efforts lead to improved statistical techniques, including running multiple specifications, before arriving at strong conclusions. Additionally, it is our hope that the Bayesian judgment literature begins to include the insights from Savage (1954) and Blackwell and Dubins (1962) in the analysis of Bayesian models of judgment, including CAM. We point out specific technical problems with CAM in order to illustrate both the fundamental flaws of HHV and to explain the apparent lack of scrutiny that the paper received. Finally, our paper illustrates the importance of saving and sharing datasets. We note that the Journal of Experimental Psychology: General (the outlet for HHV) declined to publish a previous version of this paper. Therefore, readers will conclude that HHV is entirely correct. It is therefore disappointing to us that the journal did not remedy any subset of the serious problems that we describe. A contribution of this paper is the description of the numerous and fundamental flaws—including dividing by zero—that continue to exist in the pages of a top psychology journal. We hope that our efforts will lead to more forthcoming behavior from journals in admitting and correcting their flawed publications. In Sect. 2, we describe CAM and its impact. In Sect. 3, we describe our replication of the conditions in Experiment 3 from HHV and in Sect. 4 we analyze the data. In Sect. 5, we discuss perhaps the most egregious mathematical errors in CAM. Section 6 concludes.",3
19.0,1.0,Mind & Society,24 May 2020,https://link.springer.com/article/10.1007/s11299-020-00231-7,Heuristics in fantasy sports: is it profitable to strategize based on favourite of the match?,June 2020,Vojtěch Kotrba,,,Male,Unknown,Unknown,Male,"Fantasy sports as a form of entertainment and betting activity has established itself as an integral part of the sports environment (Schirato 2012; Lopez-Gonzalez and Griffiths 2018). Fantasy sport users can choose between free leagues with rather symbolic rewards, versus leagues with an entrance fee and considerable monetary prizes (Musco et al. 2017). Whether the user’s goal is to win a large amount of money or just the thrill of defeating opponents, the search for the optimal squad remains to be an essential part of the process. While luck plays a role in the results (Holleman 2006; Trippiedi 2014; Leishman 2016; Taggart 2016; Wisniewski 2016; Getty et al. 2018), a well-chosen squad can present a crucial advantage to the user. To find an optimal squad, the user must invest enormous effort and spend a large amount of time analysing data to spot the best performing players for a reasonable price. Some skilled users may use sophisticated strategies through software that provide advice regarding the choice of players. These strategies might be based on the drafting processes in real sports environments (Summers et al. 2007; Massey and Thaler 2013) or they may be formed specially for the purpose of fantasy sports (Fry et al. 2007; Bonomo et al. 2014; Anagnostopoulos et al. 2016; Becker and Sun 2016; Hunter et al. 2016). Most users primarily rely on their own skills for assessment (Farquhar and Meeds 2007; Billings and Ruihley 2013; Weiner and Dwyer 2017). Unprofessional users too spend a considerable amount of time analysing performance and price of different players (Strumbelj and Robnik-Sikonja 2015). All users, professional or not, seem to compensate for the lack of time with the use of heuristics (Smith et al. 2006) such as: the best hockey players are from Canada; the best football players are from Brazil; the players from Barcelona will gain more points, because Barcelona wins more often. These types of assumptions and stereotypes that might influence the users can be helpful only if certain conditions are met. For example, using FC Barcelona players might be instructive for users as to where to purposefully gain inspiration for simplification of the process of constructing their squads. The reason is that if a player’s team is successful, then the player should also perform well in a fantasy league. The importance of the consideration of the opponent has been shown in qualitative research in the previous literature (Smith et al. 2006). Even though the result of a match influences the point-gaining of players, the effect is not universal. When choosing a strategy that is in odds with another, not enough attention is paid to the relevant statistics which is ineffective. If Barcelona wins a game but its defence concedes more goals than other teams in the Spanish league, then it is not a good idea to choose the Barcelona’s defenders but choosing a forward from the same team would work. It may be a better strategy for the game of football to select a winning team’s forwards and midfielders with better individual statistics, but in case of defenders or goalkeeper choose a team with less losses and a lower number of goals conceded, and then look for good individual statistics of a player. Relatedly, research performed in the heuristics and biases framework of Tversky and Kahneman (1974) has shown the betting market can be inefficient due to the use of representativeness heuristic (Tassoni 1996; Woodland and Woodland 2015). A similar mechanism could be at work in fantasy sports. In this paper, the favourite of a match, or the respective chance of winning, is determined by using the betting odds from betting offices. Betting odds provide a way to quantify the probability of a team’s victory as the betting offices’ performance is assessed by correct determination of the probability of a team’s winning and the related winning bets. Thus, these offices tend to produce quite precise estimates (Strumbelj and Robnik-Sikonja 2015) or they will go out of business. The scope of this analysis is limited, as the use of betting odds for individual matches only applies to certain fantasy leagues, where users can choose a new squad each round. In this form of fantasy sports, the user is always able to adapt the squad to the situation in the next round. Alternatively, when the user drafts a squad only once, and subsequently can only alternate configuration and sell/buy players, it would not make sense to use the betting odds for individual matches rather analysis must account for the betting odds of the overall victory. The model presented in this paper concerns the former and therefore is more sophisticated as it allows for the change of squads at each round. An important query in this analysis is whether simple betting odds can really help users to better choose their squads. In other words, whether these exert statistically significant influence in overall point-gaining of an athlete in a given match. Previous studies often examined the strategies and procedures that are efficient in fantasy sports. Nevertheless, it remains unclear whether fantasy sports users actually use those strategies. This paper is different as it aims to study a heuristics that evidence suggest users apply to simplify the process of choosing their squads. In addition it assesses whether this heuristic strategy has rational basis, therefore, could be sensible to use. This investigation comprises of two steps. First, the paper asks whether betting odds have a significant influence on the point-gain of a certain player in a given match. Subsequently, the focus turns to whether users take this feature as a heuristic when choosing their squads, and if so does the real influence of the heuristic merits the degree to which users account for it. It is essential for the gambling industry to know how individuals choose to act when gaming. However, it is not always easy to obtain relevant data. Moreover, laboratory experiments for eliciting these choices can be distorted by participants who change behaviour because of being aware of the fact that they are being tested. Fantasy sports have proven to be an interesting alternative for gambling-related research (Ryan et al. 2018). Gamblers in fantasy sports take part in the competition and as the game is recorded on the Internet, it produces data that reveal users’ natural behaviours. By using such data, the current paper demonstrates the potentials of this new environment as a new device for gambling-related research. Furthermore, this paper uses a concrete heuristic to illustrate that gamblers do not act fully rational, but use shortcuts to simplify the decision-making process. Data is obtained from a fantasy league ran by the company Seznam, Inc., which is related to the highest English football competition Premier League (period of 2015–2016 season). The website fantasy.sport.cz was available in Czech, meaning the majority of its users are Czechs, due to the language barrier. Betting odds from the betting office Tipsport Inc. have been added to the dataset. Each football player was assigned the betting odds for the players’ club in the respective match, provided that the athlete had already started in a match for the given club prior to the respective match. Based on these datasets, several econometric models are compiled. The model for analysing the influence of betting odds on the success rate of an individual football player includes the number of points the player gained in the given round as a response variable. The control variable is represented by the player’s past performance. The response variable of the main model is the number of individual football players chosen into squads in a given round. The main examined variable is represented by the betting odds of the player’s club, with his performance, price and game position as control variables. A variable for Czech nationality has also been added to the model, as a partiality for Czech players was expected. Fans throughout history tend to prefer players of their own nationality (Wilson and Ying 2003). The results indicate that the influence of betting odds on the players’ point-gain is a statistically significant variable. Evidently, the players who are favoured tend to actually gain more points. However, this effect is low compared to the effect of past performance. Past performance is the main factor influencing a player’s probable future performance, as it is related to his talent. When it comes to how users select players for their squads, the effect of betting odds is statistically significant as well. Users at the very least tend to take the favourite of a match into account. This effect is also significantly larger compared to performance, specifically in the model which examines the direct effect of betting odds on performance. This shows that users overestimate the real effect of this heuristic, that is, they weigh it higher than it merits in their choices.",2
19.0,2.0,Mind & Society,16 June 2020,https://link.springer.com/article/10.1007/s11299-020-00234-4,"Confidence, power and distributive preferences",November 2020,Yoshio Iida,,,Male,Unknown,Unknown,Male,"Economic inequality is a serious problem in many countries. Cingano (2014) reports recent increasing trends in income disparity across the Organisation for Economic Co-operation and Development (OECD) countries and discusses the possible economic harm this has caused. Although, people are unaware of this fact, successes and failures in life, as well as income inequality, considerably depend on chance, which is something individuals cannot control by themselves (e.g., the social class into which they were born, the educational opportunities they have had, whether they joined the job market in the right place and at the right time, and so on) (OECD 2018; Frank 2017; Taleb 2001). Nevertheless, there are hardly any studies that have investigated the way in which people’s confidence in their good fortune influences their redistributive preferences. Such an investigation is important because of the implications for income inequality. People who are in a position to determine the income of others, like policy makers or company executives, may be overly confident in their luck to be elected and their success in business. By extension, if confident individuals in positions of authority are more acquisitive than employees in subordinate positions, the income gap may widen. Some psychological studies showed that people’s dispositional traits affect the decisions they make regarding distributive justice. For example, donors with high self-esteem are more likely to allocate rewards according to the performance-based equity principle than are donors with low self-esteem (Brockner et al. 1987). Major and Adams (1983) revealed that individuals with high Interpersonal Orientation (IO) (i.e., individuals who are interested in the interpersonal aspects of their relationships) tended to allocate rewards more equally than those with low IO (i.e., individuals who do not attach much importance to the interpersonal aspects of relationships and are interested in maximizing their own gain). In a similar vein, Greenberg (1983) showed that participants high in private self-consciousness who were attuned to their own feelings and internal standards distributed rewards in accord with the equity principle, whereas participants high in public self-consciousness who were concerned about others’ impressions of their personality tended to divide the rewards more equally. More recently, Iida (2015) found that donors who expected to receive a higher initial income based on their performance on a task (that required intellectual abilities or consistent efforts for better performance) made more self-serving income redistribution decisions than did those who did not expect to be nominated as donors in the dictator game (DG). Although the importance of such confidence on one’s performance was found in ex-post investigation (not hypothesized ex-ante based on the theory), considering the above-mentioned psychological studies, this finding is understandable. As Brockner et al. (1987) elucidated, individuals with high self-esteem tend to be confident in their own abilities, expect to perform better, and win higher initial incomes. Once they are appointed a donor role, they believe that they have executed the task assigned to them better than their partners and consequently, allocate higher rewards to themselves based on the equity principle. This perspective drives us to ask what would happen if the initial income distribution was determined completely by chance. Would participants who expected to be nominated as donors also act more selfishly than those who did not expect the donor nomination even though they would know that the donor roles would be allocated purely by chance? As opposed to an income gap caused by the participants’ innate abilities or efforts, the disparity created by an act of providence would provide no reason for them to receive higher rewards or invoke the equity principle. If confident participants are those individuals whose principles of distributive justice are founded on equity, they would not reserve more for themselves than for their partners based on this premise. However, studies concerning the better-than-average effect (e.g., Taylor and Brown 1988) suggest another possibility. Most people generally evaluate themselves more favorably than their peers. This better-than-average effect is observed not only in people’s evaluations of their behaviors and traits but also in their expectations regarding future life events, such as winning the lottery, deriving job satisfaction, and having an enduring marriage (Baker and Emery 1993; Weinstein 1980). Brown (1986) found that individuals with high self-esteem tended to evaluate themselves more favorably than they evaluated others. Such positive self-evaluations suggest that being allocated the donor role by chance can make confident individuals who have a strong tendency to rate themselves as better-than-average to believe that they have qualities superior to their partners and that they thus are more deserving of higher rewards. In addition, their confidence with regard to luck has less basis than their self-assurance about their capabilities. Most people have experienced competitive situations with others in a school and/or an office setting and had the opportunity to realize their own relative edge over others in terms of their abilities. However, no one can provide objective information or factual proof regarding his/her relatively better fortune compared to others. Some studies demonstrated that individuals are more self-serving when making subjective or ambiguous judgments (Allison et al. 1989; Rothbart and Park 1986). Therefore, the self-serving behavior of donors who are optimistic about their likelihood of receiving a fortuitous windfall can be expected to be conspicuous. This study controlled for the donors’ power over the redistribution of resources to provide some policy implications about the income gap that would result from the decision-making of confident individuals. A dictator in a standard DG enjoys two advantages over the recipient, namely higher initial income (the role of donor) and the uncontested power to redistribute it (the role of dictator). This study compared the results of a DG and a random DG (RDG) to examine how the latter affects redistributive preferences. In the DG experiment, a participant possessing higher initial income (donor) is also empowered to redistribute it to a participant possessing lower initial income (recipient). In the RDG experiment, a donor and a recipient declare their redistributive preferences, but one of them is randomly chosen and implemented. Donors in DGs know that their redistributive power is unconditional; however, donors in RDGs know that this is not the case. Redistributive power potentially affects donors in contrasting ways. On the one hand, participants might regard their absolute redistributive power as a fortunate happenstance and not as a tool for self-benefit. Some experimental studies showed that donors who received higher incomes randomly were less greedy than were those who earned higher incomes by performing simple tasks (Cherry et al. 2002; Hoffman et al. 1994; Oxoby and Spraggon 2008; Rousu and Baublitz 2011). As with participants who regard the higher initial amounts of income as a matter of good fortune, participants who are granted absolute redistributive power by the result of a lottery (DG experiment of this study) may be less likely to use it to primarily benefit themselves. On the other hand, donors may interpret their absolute power as their personal property and use it to benefit themselves by allocating less to others. Rode and Le Menestrel (2011) and Oxoby and Spraggon (2008) have shown that dictators (participants granted power) allocate considerable proportions of income to themselves even when income has been earned only by the recipients and that they distribute almost nothing of what they themselves have earned, to others. In other words, redistributive power can reinforce avariciousness and self-serving interpretations of fairness. The following section presents the design and procedure of the experiment. Section 3 reports the results and discusses the psychology of confident donors. Section 4 concludes the paper.",
19.0,2.0,Mind & Society,30 June 2020,https://link.springer.com/article/10.1007/s11299-020-00235-3,Urbanicity mental costs valuation: a review and urban-societal planning consideration,November 2020,Luca S. D’Acci,,,Male,Unknown,Unknown,Male,"Within the top twenty reasons of global burden of disease, there are five mental illness—depression (2nd), anxiety (7th), schizophrenia (11th), dysthymia (16th), and bipolar disorder (17th)—which in 2013, even if probably underestimated by more than 1/3, resulted to be the leading causes of years lived with disability (Vigo et al. 2016). Globally, in 2015, non-communicable diseases were 60% of total disability-adjusted life-years, of which 12% corresponded to mental disorders, neurological disorders, substance use disorders, and self-harm (Vigo et al. 2019); mental health disorders are also the primary cause of Disability-Adjusted Life Years worldwide (Bloom et al. 2012). Decades of empirical research shows an association between mental health and urbanicity, especially for the individuals genetically more inclined and those who lived in cities during their early life. Links, often proven to be causal by longitudinal and dose–response analysis, between urbanicity and mental illness have been greatly reported such as in these 89 studies: Coid et al. 2020; Evans et al. 2020; Vargas et al. 2020; Sampson et al. 2020; Lecic-Tosevski 2019; Reed et al. 2018; Evans et al. 2018; Castillejos et al. 2018; Kirkbride et al. 2006, 2018; Cooper et al. 2017; Besteher et al. 2017; Gruebner et al. 2017; Krzywicka and Byrka 2017; Vassos et al. 2016; Newbury et al. 2016; Brockmeyer and D’Angiulli 2016; Adli et al. 2016; Freeman et al. 2015, Wilker et al. 2015; Peterson et al. 2015; Haddad et al. 2015; Vaessen et al. 2015; Steinheuser et al. 2014; Haluza et al. 2014; Krabbendam et al. 2014; Streit et al. 2014; Calderón-Garcidueñas et al. 2013; Heinz et al. 2013; Bedrosian and Nelson 2013; Stevens et al. 2013; Tandon et al. 2012; Lederbogen et al. 2011; Fonken et al. 2011; Larson et al. 2011; Galea et al. 2011; McClung 2007, 2011; Meyer-Lindenberg 2010; Park et al. 2010; Bowler et al. 2010; Kelly et al. 2010; Mortensen et al. 2010; Levesque et al. 2011; Gwang-Won et al. 2010; Tae-Hoon et al. 2010; Peen et al. 2007, 2010; van Os et al. 2010; Kennedy et al. 2009; Bentall et al. 2008; March et al. 2008; Joens-Matre et al. 2008; Fuller et al. 2007; Graziano and Cooke 2006; Maas et al. 2006; Pedersen and Mortensen 2001a, b, 2006a, b; Weich et al. 2006; Krabbendam and van Os 2005; Tsunetsugu and Miyazaki 2005; Wang 2004; Sundquist et al. 2004; van Os 2004; van Os et al. 2004; McGrath et al. 2004; Harrison et al. 2003; Caspi et al. 2003; Frumkin 2001; Allardyce et al. 2001; Haukka et al. 2001; Torrey et al. 2001; van Os et al. 2001; Eaton et al. 2000; Schelin et al. 2000; Marcelis et al. 1999; Mortensen et al. 1999; Marcelis et al. 1998; Thornicroft et al. 1993; Lewis et al. 1992; Cohen 1982; Eaton 1974; Christmas 1973; Faris and Dunham 1939; White 1903. Paykel et al. (2000), analysing data from almost ten thousand individuals (Household Survey of the National Morbidity Survey of Great Britain) via a logistic regression, reported “a considerable British urban–rural differences in mental health, which may largely be attributable to more adverse urban social environments”. According to Vassos et al. (2016), the rate of incidence of nine types of psychiatric disorders is in average 1.6 times higher in the capital city than in the rural areas, with ‘schizophrenia and related disorders’ even almost double (1.83), while the review of McGrath et al. (2004) of 68 studies found a schizophrenia incidence rate 2 times higher in urban areas than in mixed rural/urban areas; a rate that rises up to a 2.75 times greater risk of schizophrenia when one has lived 15 years of her early life in a capital city rather than a rural area (Pedersen and Mortensen 2001a). Peen et al. (2007) reported an odds-ratio for mental disorders in very highly urbanized areas of 1.6 related to non-urbanized (1.8 when unadjusted by control variables). An approximatively twofold increase in psychosis risk associated with urbanicity is also confirmed in the following empirical studies: Marcelis et al. (1998, 1999), Mortensen et al. (1999), Schelin et al. (2000), Allardyce et al. (2001), Pedersen and Mortensen (2001a, b), van Os et al. (2001, 2004), Harrison et al. (2003), Sundquist et al. (2004), Pedersen and Mortensen (2006a, b), Kirkbride et al. (2006), Haukka et al. (2001) and Torrey et al. (2001). An increase as high as fourfold was found in Eaton et al. (2000). A meta-analysis review summarised that urban dwellers have a 1.4 times greater risk of mood disorders than non-urban (Peen et al. 2010). Due to the type of the analysis conducted, the causality (rather than a reverse causation) of the nature of this link, emphasising that urbanicity has an etiological effect on mental health, has been underlined, among many, by March et al. (2008), van Os et al. (2010) and Lederbogen et al. (2011). If we shift our attention to people’s preferences toward places to live their lives, “many surveys about quality of life in cities invariably suggest that it is in smaller cities that the highest quality of life is achieved” (Batty 2018, p. 95). Similarly, to European surveys, 44% of Americans voted small towns/rural environments as the best kind of places to live and only roughly one in five (20%) voted cities (Knox and Pinch 2006). Another questionnaire (D’Acci 2020) reported that only 32% of respondents prefers to live in a city rather than (ceteris paribus) in a natural environment (36%), in a town/village (24%), in a suburb (6%), while 2% of them were indifferent. In line with these stated residential preferences, happiness seems to decrease when urbanicity levels increase (Sander 2011; Lawless and Lucas 2010), and studies about self-declared life satisfaction, psychological well-being in rich countries systematically show lower levels of life satisfaction in urban areas compared to the rural or less urban areas (Viganò et al. 2019; Easterlin et al. 2011; Gilbert et al. 2016; Helliwell et al. 2018; Fassio et al. 2013; Lawless and Lucas 2010; Okulicz-Kozaryn and Mazelis 2018; Okulicz-Kozaryn 2017; Sørensen 2014; Berry and Okulicz-Kozaryn 2013, 2009). This discrepancy between rural and urban environments’ influence on mental health, life satisfaction and happiness suggest that by re-organizing our socioeconomic urban daily life and the physical urban-regional structure itself, there would be a potential margin of reduction in the urban mental illness rates and an increase of life satisfaction and daily mood of urban dwellers. To convince governments, urban and regional planners, stakeholders and the ordinary population about the relevance of the issue, an economic translation of the costs that psychological effects that cities have to us, might help to make the topic more tangible.",4
19.0,2.0,Mind & Society,19 June 2020,https://link.springer.com/article/10.1007/s11299-020-00236-2,Human-mind-inspired processing model for computing,November 2020,Chinthanie Weerakoon,Asoka Karunananda,Naomal Dias,Unknown,Unknown,Unknown,Unknown,,
19.0,2.0,Mind & Society,07 July 2020,https://link.springer.com/article/10.1007/s11299-020-00237-1,Digital signatures: a tool to prevent and predict dishonesty?,November 2020,Luka Koning,Marianne Junger,Joris van Hoof,Male,Female,Male,Mix,,
19.0,2.0,Mind & Society,22 July 2020,https://link.springer.com/article/10.1007/s11299-020-00239-z,Physics and decisions: an exploration,November 2020,Christian D. Schade,Shyam Sunder,,Male,Male,Unknown,Male,"Human decision making attracts intensive study from many disciplinary perspectives including economics, neurosciences, philosophy and psychology, but mostly without direct links to the physical context of decisions. Jerome Busemeyer’s (e.g., Busemeyer et al. 2006) work on quantum decision making and Haven and Khrennikov’s (2013) work on quantum social science are notable exceptions. However, they purposely use quantum calculus as a toolbox to model probabilities, and do not claim to build a theoretical model to capture other, more fundamental aspects of decision making. The physical basis of decision-making lies at the heart of the millennia-long debate over the existence of free will in philosophy, mostly to advocate determinism (such as within the ‘clockwork’ framework of Newtonian physics), but also to defend libertarianism as in the works of Kane (1985). Other novel proposals to link physics and decision making appear in Mousavi and Sunder (forthcoming) using classical physics, as well as in Schade (2018) which is based on the multiverse interpretation of quantum mechanics. The Humboldt-Kreis was organized to explore the role physical laws play in understanding conscious human behavior, especially decision making. On one hand, there is the possibility of laws whose applicability transcends the traditional divide between the inanimate and animate aspects of our world. What can we learn from them about a person’s decisions in isolation and in organization? On the other hand, there might be features/ abilities that are specific to the domain of consciousness, and it should then be interesting to understand the consequences. The intent behind organizing the Kreis was to explore these precepts. At this small event, we paired scholars to ask: (1) What ideas in the work of the other can better inform your own work? And (2) what aspects of your own work might be useful to the other participant/partner? The Humboldt-Kreis took place on December 4–6, 2019 at the School of Business and Economics, Humboldt-Universität zu Berlin (in Berlin, Germany). It was co-organized by Shabnam Moussavi (Max Planck Institute for Human Development or MPIB, Berlin, Germany), Christian D. Schade (Humboldt-Universität zu Berlin, Germany) and Shyam Sunder (Yale University, USA). Seven other scholars joined the organizers, listed here in alphabetic order: Guido Bacciagaluppi (Utrecht University, The Netherlands), Andrei Khrennikov (Linnaeus University, Sweden), Laura Martignon (Ludwigsburg University, Germany), Björn Meder (MPIB, Germany), Mehdi Moussaïd (MPIB, Germany), Saras Sarasvathy (Darden School, University of Virginia, USA), and Laurianne Vagharchakian (French Behavioural Insights Unit, Interdepartmental Directorate for Public Transformation, France). Eight participants presented their perspectives, while Meder and Vagharchakian participated in the discussions. Shyam Sunder presented a paper with Mousavi and another by himself. Christian Schade presented one paper but was granted a second timeslot to extend his explanation of the role of consciousness in his version of the quantum multiverse. Finally, the six short papers submitted by Khrennikov, Bacciagaluppi, Sarasvathy, Schade, Sunder and Mousavi with Sunder form this mini-symposium. With one exception, the papers are based on, or closely related to, the presentations at the Kreis. Saras Sarasvathy, inspired by ideas on the quantum multiverse before and during the Kreis, decided to write her reflections on the multiverse (instead of effectuation, the topic of her talk). The event ended with two parallel workshops, one led by Guido Bacciagaluppi and Christian Schade on “Is quantum theory relevant for macro domains?”, and the other by Shabnam Mousavi on the topic: “Complexity (in mathematics, natural and social sciences) and measurement”, with the participants split into two groups.",
19.0,2.0,Mind & Society,21 July 2020,https://link.springer.com/article/10.1007/s11299-020-00244-2,Physics and decisions: an inverted perspective,November 2020,Shabnam Mousavi,Shyam Sunder,,Unknown,Male,Unknown,Male,"A physical corpus is a common attribute of inanimate objects, animals, and humans, all subject to universal physical laws. Both a person and a stone fall to earth under the same laws of gravity independent of whether wind, accident, mischief, or suicidal intent is behind the event. Further, consider three examples of movement from a starting point to a target through a change in medium along the way (Fig. 1): where (a) a lifeguard runs over a sandy beach and then swims to reach a child; (b) an ant crawls between its nest located on a smooth surface and a food source located on a rough surface; and (c) a sunbeam travels from the sun to an eye of an underwater fish through vacuum and water. In all three cases, the fastest path (the kinked thick red line) is not the same as the shortest path (the broken straight line) connecting the starting point and target point. A lifeguard, an ant, and a sunbeam: all follow a kinked path across two media In all these three different contexts, problems, and actors, a single general physics principle—the principle of least action—provides a simple solution. Published by Maupertuis in 1744, this principle generates a path—the kinked line in these three examples—along which the integral of the difference between the kinetic energy and potential energy, at every point in time, is minimized (Feynman 1963). We propose using this principle to isolate the elements of all forms of action—human, animal, or inanimate—that arises from the physical corpus involved; for inanimate, we need to proceed no further, and explanation of the remainder for animals can be sought in their biology, and for humans in their biology and higher social-psychological faculties. 
Of all possible paths from a beginning point A to an end point B, the materially efficient path uses minimal action, where action is a scalar that corresponds to the dimension in which (appropriately defined) value has been conserved.
",2
19.0,2.0,Mind & Society,27 July 2020,https://link.springer.com/article/10.1007/s11299-020-00242-4,"Quantum mechanics, emergence, and decisions",November 2020,Guido Bacciagaluppi,,,Male,Unknown,Unknown,Male,"At the workshop on Physics and Decisions: An Exploration that took place at the Humboldt University in Berlin on 4–6 December 2019, quantum mechanics seemed to be relevant to several of the topics under discussion. In this symposium contribution I wish to try and summarise this in a somewhat systematic way, focusing in particular on the concept of emergence.
 I shall begin by distinguishing three aspects of the relation between quantum mechanics and the macroscopic world (as discussed in particular on the last morning of the workshop).
 The first is the prima facie qualitative tension between the quantum superposition principle and the definiteness of the macroscopic world (as in Schrödinger’s cat paradox). Various attractive strategies for overcoming this tension are available and have been worked out in detail, but I shall not review them here. The one we focused on was the idea of multiplicity provided by the multiverse (or Everett, or many-worlds) approach. If one accepts the idea that the reality that we observe corresponds only to one of many components of a superposition of quantum states, then that basic tension disappears. The second aspect is the detailed question of whether and how, even at the level of individual components (‘branches’, ‘worlds’), one can get an adequate description of macroscopic phenomena (e.g. the usual behaviour of a living cat, or the motions of the bodies in the solar system). Addressing this question provides the justification for the idea of multiplicity in the first place, i.e. how worlds may emerge as structures within the universal wave function. It is a technical question, and it has been studied very successfully for almost 50 years under the heading of ‘decoherence theory’. The third aspect, finally, is the question of whether, despite the emergence of worlds and of classical behaviour, quantum mechanics is in fact directly relevant to certain macroscopic phenomena. We shall see a number of ways in which this might be true, but at least one is obvious. Indeed, as Niels Bohr never tired to emphasise, the very notion of a ‘quantum phenomenon’ is defined in terms of macroscopic laboratory equipment: quantum mechanics impinges on the macroscopic world by making macroscopic objects behave in apparently probabilistic ways. The Born rule itself (the quantum mechanical recipe for assigning probabilities) is telling us about unexpected behaviour of macroscopic objects.Footnote 1 All three aspects relate to general questions of reduction and emergence. We shall now look at how quantum theory illuminates them, with particular regard to Everett worlds, classical behaviour, and the Born rule.",
19.0,2.0,Mind & Society,09 November 2020,https://link.springer.com/article/10.1007/s11299-020-00240-6,"Quantum-like modeling: cognition, decision making, and rationality",November 2020,Andrei Khrennikov,,,Male,Unknown,Unknown,Male,,5
19.0,2.0,Mind & Society,03 November 2020,https://link.springer.com/article/10.1007/s11299-020-00243-3,Choice matters,November 2020,Saras Sarasvathy,,,Unknown,Unknown,Unknown,Unknown,,
19.0,2.0,Mind & Society,20 July 2020,https://link.springer.com/article/10.1007/s11299-020-00245-1,Rational order from ‘irrational’ actions,November 2020,Shyam Sunder,,,Male,Unknown,Unknown,Male,,
19.0,2.0,Mind & Society,31 July 2020,https://link.springer.com/article/10.1007/s11299-020-00241-5,"Free will in the clustered-minds multiverse, and some comments on S. Sarasvathy’s ‘choice matters’",November 2020,Christian D. Schade,,,Male,Unknown,Unknown,Male,"When Shabnam Mousavi, Shyam Sunder, and I decided to organize the Humboldt-Kreis, I strongly believed in the multiverse interpretation of quantum mechanics (Everett 1957).Footnote 1 This interpretation is more convincing to me from both a physical as well as as a philosophy of science perspective then other interpretations. It might also be more fruitful for understanding how decisions are made, and that they are made at all, appreciating free will and its critical role in social science. At the end of the Kreis, I continued to hold these convictions. Does this mean that nothing happened during these three days? On the contrary, so much happened. First, I learned about linkages between physics and decisions I was not aware of, and more about effectuation (Sarasvathy 2001) than I knew before the Kreis. Secondly, I benefitted from comments and suggestions from the participants on my clustered-minds-multiverse (CMM) concept (monograph: Schade 2018; see also below). I also learned a lot in the aftermath, e.g., from discussions with Kathryn Laskey (GMU), a proponent of a quantum-based, singular-universe theory of free will (Laskey 2018), who was introduced to me by one of the participants in the Kreis, Laura Martignon. I also read the contributions submitted by the particpants with a lot of interest. Sarasvathy, e.g., who, after intensively studying my monograph (Schade 2018) over the summer 2019 and participating in the multiverse presentations by Guido Bacciagaluppi and myself during the workshop, decided to write a piece on the multiverse (and not on effectuation, the topic of her presentation at the Kreis). Two questions that the reader of this mini-symposium might have are: (a) why not just use a singular-universe version of free will, based on quantum mechanics, why do we need the multiverse for justifying free will?, and (b) how does  Sarasvathy’s contribution – replacing large part of physical by social-science based reasoning – relate to the other contributions on the multiverse? Therefore I decided to, besides explaining my version of the multiverse interpretation of quantum mechanics and why I think it is necessary for justifying free will, to briefly touch upon the topic of singular-universe free will à la Laskey and to spend even more ink on Sarasvathy’s ‘choice matters’ concept. The discussion of  Sarasvathy’s concept will also turn out to be helpful in demonstrating some of the specifics of my CMM concept.",
20.0,1.0,Mind & Society,21 October 2020,https://link.springer.com/article/10.1007/s11299-020-00263-z,"The relevance of anger, anxiety, gender and race in investment decisions",June 2021,Daniel M. V. Bernaola,Gizelle D. Willows,Darron West,Male,Unknown,Male,Male,"Much of the theory concerning financial investment assumes that investors behave in a rational manner and that all market information is embedded in the investment process (Bloomfield 2006). However, human beings are not always fully rational when making investment decisions. The investment decisions of individual investors are often driven by passion rather than by logic (Gibson 2006). In particular, the relevance of personality traits, such as anger and anxiety has been illustrated to influence financial decision-making (Gambetti and Giusberti 2009, 2012, 2014). A conceptual study by ul Hassan et al. (2013) showed that wrong decisions are made by angry investors. Such angry decisions create grim consequences because the decisions have been done without proper analysis and no rational judgement. Gambetti and Giusberti (2019) showed that anxious people tend to save money and avoid investments, perceiving high risks and low control and returns. This contrasted with extroverted, independent individuals with self-control who were more likely to make investments. This depth of research on personality traits has been prevalent in a European context. However, it does not inform the disparate nature and different environment of individuals in an emerging economy. Research within an emerging market is important, not only because of the substantial heterogeneity in the economic, cultural and social environments, but also because of the rapid change in these characteristics (Sudhir et al 2015). This study aims to add to the body of literature on the effect of two personality traits from psychology, anger and anxiety, and the way they relate to investment decisions that involve risk. This is done by not only assessing individuals in an emerging economy, but also, by assessing unique demographic characteristics such as race and gender. Specifically, the study is performed in South Africa, one of the BRICS nations, making a valuable contribution to emerging market findings. Sudhir et al (2015) encourage research that is country-specific to exploit unique characteristics of a specific emerging market, rather than diverse cross-country insights. The incorporation of gender and race as explanatory variables is also important, as research has illustrated both gender and race to influence financial knowledge (Willows 2019a), which is a notable indicator of financial behaviour and decision-making (Atkinson and Messy 2012). This study does not aim to be exhaustive. Pertinently, a student experimental cohort is used with hypothetical scenarios to allow for the assessment of any prima facie evidence of the merits of future research using more experienced participants and more realistic scenarios. A questionnaire was completed by 288 University of Cape Town (UCT) finance students. Trait anxiety was measured using the T-Anxiety subscale of the State-Trait Anxiety Inventory (STAI-Y2), which evaluates relatively stable aspects of anxiety proneness, such as general states of calmness, confidence and security. Trait anger was measured using the T-Anger subscale of the State-Trait Anger Expression Inventory-2 (STAXI-2), which measures how often angry feelings are experienced over time. The questionnaire was further supplemented with questions about investment decisions and demographics. Trait anger was found to be a significant predictor of an investment in equity. White participants (rather than Black, Indian or mixed-race participants) were also more inclined to invest in equity. Given that equity is considered a high-risk investment, the finding supports riskier investment decision-making by White individuals and individuals displaying higher levels of anger. Furthermore, women were more likely to select lower risk asset classes as investment than males (Montford and Goldsmith 2015). These findings are of importance, as literature has shown the impact of risk aversion on investment performance (Willows and West 2015). Given the uniqueness of this study’s inclusion of race as an explanatory demographic variable, this provides valuable information to financial advisors and investment managers, particularly those tasked with tailoring investment products for clients. It also highlights the necessity of including demographic variables when assessing the effects of personality traits. This study continues by reviewing literature on the influence of trait anger and trait anxiety on investment decision-making. Following that, the research method will be presented. To end, the results of both binary logistic regressions and multinomial logistic regressions will be discussed.",7
20.0,1.0,Mind & Society,07 January 2021,https://link.springer.com/article/10.1007/s11299-020-00268-8,"Correction to: The relevance of anger, anxiety, gender and race in investment decisions",June 2021,Daniel M. V. Bernaola,Gizelle D. Willows,Darron West,Male,Unknown,Male,Male,"The article ""The relevance of anger, anxiety, gender and race in investment decisions"", written by Daniel M. V. Bernaola · Gizelle D. Willows and Darron West, was originally published online on the publisher’s internet portal on 21 October 2020 with Open Access under a Creative Commons Attribution 4.0 International License. With the author’s/authors' decision to cancel Open Access the copyright of the article changed on 19 November 2020 to © Springer Science + Business Media, LLC, part of Springer Nature 2020 with all rights reserved. The original article was updated.",
20.0,1.0,Mind & Society,03 January 2021,https://link.springer.com/article/10.1007/s11299-020-00270-0,Sensing the self in spiritual experience,June 2021,V. Hari Narayanan,,,Unknown,Unknown,Unknown,Unknown,,
20.0,1.0,Mind & Society,02 January 2021,https://link.springer.com/article/10.1007/s11299-020-00269-7,A theory of sexual revolution: explaining the collapse of the norm of premarital abstinence,June 2021,Chien Liu,,,,Unknown,Unknown,Mix,,
20.0,1.0,Mind & Society,02 November 2020,https://link.springer.com/article/10.1007/s11299-020-00267-9,Reasonable bounds on rationality,June 2021,Igor Grossmann,Richard P. Eibach,,Male,Male,Unknown,Male,"What does it mean to say that rationality is bounded? Most previous work emphasizes how cognitive processing limitations, time constraints, and imperfect information cause people to fall short of the standard of optimal, utility maximizing decision-making (Simon 1983). Theory and research on bounded rationality indicates that decision-makers use strategies such as satisficing to compensate for these limitations. Many important insights have been gained using this approach. However, there is another sense in which human rationality may be bounded that has been relatively overlooked. In this other sense of the term, rationality is bounded because it is not the sole standard of judgmental competence that people consult when they decide what they should do (Tetlock 2002). A judgment may be considered sound because it is rational, or it may be considered sound because it is reasonable. Are rationality and reasonableness not identical standards? We maintain that if people internalize rationality and reasonableness as distinct standards for evaluating the competence of judgment then the standard of reasonableness may place important constraints on people’s pursuit of rationality. Thus, people sometimes may fail to make a rational decision not because they are unable to identify the rational option, as most work in the bounded rationality tradition has emphasized, but because choosing the rational option would violate their standards of reasonableness. Though the dissociation between rationality and reasonableness appear novel to economics and psychology, it builds on a rich tradition of scholarship in philosophy, legal studies, and political science which emphasizes that rationality and reasonableness are qualitatively distinct guides for human judgment (e.g., Graeber 2013; Rawls 2005; Toulmin 2001; also see some related ideas in behavioral economics: Shafir et al. 1993). Both rationality and reasonableness represent intellectual virtues, in the sense that they provide standards for assessing how well people use complex cognitive faculties, such as analytic reasoning and deliberative judgment, when they decide what to do. However, rationality and reasonableness set qualitatively different standards for evaluating how well people use their advanced cognitive faculties when they make judgments. Rationality is a relatively thin standard for assessing the quality of human judgment (Toulmin 2001). Judgments are considered rational to the extent that they follow the rules of formal logic. The substantive content of a judgment is usually considered irrelevant to assessing its rationality; instead what matters are the formal properties of the judgment such as its internal consistency and correct application of logical rules (Toulmin 2001). Assessments of rationality tend to be reductionist in the sense that they strip away the substantive content and most of the context of a given judgment and represent the judgment in abstract terms. In principle, the rationality of a given judgment should be able to be assessed by reducing it to the form of a mathematical expression (Toulmin 2001). For example, in economics instrumental rationality entails that an agent applies cost–benefit calculus to identify and then select the most efficient means to achieve an end. The substantive nature of the ends that agents pursue is typically irrelevant to assessing the rationality of their decision-making. By contrast, reasonableness is pragmatic, and represents an intellectual virtue akin to Aristotelian phronesis or practical wisdom (Grossmann 2017a, b; Grossmann et al. 2020b; Santos et al. 2017; Schwartz and Sharpe 2010). It is a richer and less formalized type of reasoning that involves balancing several distinct values and interests that are at stake in a given decision problem and trying to reconcile different points-of-view on that problem (Graeber 2013; Toulmin 2001). The reasonableness of a judgment cannot be represented in mathematical terms because, at its core, reasonableness involves deciding how to balance values, goals, and viewpoints that are inherently incommensurable and involve trade-offs, in the sense that these values cannot be mapped onto some common metric of valuation without removing some of their essential properties (Graeber 2013; Toulmin 2001). Therefore, unlike rationality, reasonableness cannot be reduced to a set of mathematical calculations and domain-general logical rules because reasonableness entails making apples-and-oranges comparisons and adjudicating between distinct perspectives. Reasonable judgment is thus an inherently fuzzier process than rational judgment. Yet, despite this fuzziness, some standard of reasonableness is necessary to guide everyday practical judgment because, as Graeber (2013) writes, “Most of life—particularly life with others—consists of making compromises that could never be reduced to mathematical models” (p. 200). To the extent that reasonableness draws attention to the fuzzier but nevertheless important considerations that are at stake when a judgment is being made, the reasonableness standard may provide an important check on people’s pursuit of rationality. If people were pure intuitive rationalists whose rational pursuits were not constrained by some standard of reasonableness then, to paraphrase Oscar Wilde, they might be able to calculate “the price of everything” but as a result they would understand “the value of nothing.” In contrast to this instrumental and reductionist approach associated with rationality, the standard of reasonableness aims to establish some practical guidance through balancing various values, goals, and interests (cf. “reflective equilibrium”; Rawls 1971).",1
20.0,1.0,Mind & Society,14 October 2020,https://link.springer.com/article/10.1007/s11299-020-00264-y,A small step towards unification of economics and physics,June 2021,Subhendu Bhattacharyya,,,Male,Unknown,Unknown,Male,"Integration of science obviously entails unification of natural science and social science. K. Popper was of the opinion that theories of social science more than often offer an approximation of the real picture (Popper 2002, p. 319); he referred to the concept of verisimilitude in this context. To him it was the Theory of Relativity which became victorious among Einsteinian mechanics, Marxian discourse, Freudian analysis and Adlerian activity. While working as an assistant of A. Adler he faced the propaganda ‘save the phenomenon’ personally for the first time in his life (Popper 2002, pp. 45–6). As an incisive critic Popper analytically contradicted the theory of historical inevitability, one of the main planks of Marxism, in his ‘Poverty of Historicism’ (Popper 1957). Status of biological science, in spite of itself being one of natural science, is yet to be completely grabbed in cause-and-effect flow-charts—theory of immunology and functioning of brain are cases in point. One interesting development took place in the field of science during seventies of the last century. It is the advent of chaos theory. This discipline brings together mathematical and non-mathematical subjects like physics and biology closer than ever before; moreover it elegantly bridges natural and social science by establishing similarity between them. If nothing else, chaos theory has shown its ability of halting rabid compartmentalisation of science (Gleick 1998, p. 5). Greek philosopher Anaximander was the first enquirer who was in search of an orderly arrangement of the universe (Munitz  1986, p. 21) and thus began the quest of pure knowledge by mankind, giving birth to science and unquestionably it was natural science. Intellectuals, centuries after centuries, have been deeply influenced by the organic view, as envisaged by Hegel, in order to muster a grand overview. A quick glance at the thought process of young Russell: “During this timeFootnote 1 my intellectual ambitions were taking shape. I resolved not to adopt a profession, but to devote myself to writing. I remember a cold, bright day in early spring when I walked by myself in the Tiergarten, and made projects of future work. I thought that I would write one series of books on the philosophy of sciences from pure mathematics to physiology, and another series of books on social questions. I hoped that the two series might ultimately meet in a synthesis at once scientific and practical. My scheme was largely inspired by Hegelian ideas. Nevertheless, I have to some extent followed it in later years, as much at any rate as could have been expected. The moment was an important and formative one as regards my purposes.” (Russell 1967, p. 116).Footnote 2 Historically speaking, social scientists tried to explain various social phenomena in post-Newtonian era, taking cue from physics and mechanics in particular; many of them were economists. Both Ricardo and James Mill endeavoured to fit political economy in a thoroughly deterministic framework, perhaps with an intention of creating reprints of contribution made by Euclid and Newton in their respective fields. W. S. Jevons was greatly influenced by laws of physics and essence of mathematical calculus while formulating theory of marginal utility. To him economic theory was a “calculus of pleasure and pain” (Schumpeter 1994, p. 1056). Principle of minimization of energy had an undeniable role in the formation of theory of maximization of utility (Sinha and Chakrabarti 2012, p. 46). The trend turned out to be almost opposite when we observe that some physicists started working on problems of economics in the last century. Actually attempts of elucidating various social phenomena by the accepted theories of physics have a long history indeed and the name of the discipline which deals with this is known as social physics. Statistics is used as the bridge joining economics and physics; econometrics, as a sub-field of statistics, has been found to be of much help in this process. This article picks up one particular item from the grand project of unification of natural and social science; it aims to highlight possibilities of synergy between physics and economics in solving some problems in economics. It gives an outline of mathematization of economics and devoted two sections on social physics and econophysics, separated by a discussion on statistical inference. Advent of statistical mechanics has opened up a vista so that physicists can make use of some statistical properties to explain some socio-economic phenomena. Affinity of econophysics with econometrics and stochastic processes demands a little more elaboration of econophysics and that is why this article treats social physics and econophysics in an almost mutually exclusive manner. Monte Carlo simulation is an important technique used not only by statisticians but econophysicists as well; methodology and limitations of this technique have been discussed in one section of this article. Global meltdown in the world of finance during the years 2007–08 has been mentioned as a case study to further the cause to be cautious in selection of models and assumption of (statistical) distributions. The application of model introduced by F. Black, M. Scholes and R. Merton (BSM model) for pricing of options is a purely technical issue; one of the sections of this article comments on tenability of this model against the backdrop of performances shown by stock markets. Finally the efficacy of statistics as a bridging discipline (between physics and economics) has been discussed with a conjectural hint at the prospect of mathematics for facilitating this bridging procedure more effective.",
20.0,1.0,Mind & Society,13 October 2020,https://link.springer.com/article/10.1007/s11299-020-00259-9,"Imitation, conscious will and social conditioning",June 2021,Daniel Rueda Garrido,,,Male,Unknown,Unknown,Male,"In this essay, I propose an account on human free will that is compatible with conditioning in social contexts. The argument that summarizes my claim is that the perception of other people’s behaviour conditions the agent in imitating that behaviour, as evidence from social psychology holds (Bargh and Chartrand 1999; Bargh and Ferguson 2000, 2004), and thus, what the agent perceives and experiences becomes the motive for her actions. However, in opposition to other interpretations, I endorse that although the actions of the agent have their potential motives in the perceived actions, these only become motives through the consciousness of the agent. For the agent, by willing to act in the way that she does, reveals an identification with those actions that she imitates. This essay takes up the experiments conducted by a group of social psychologists led by John Bargh, which endorse the imitation of other agents as a mechanical response. On the contrary, I propose that all imitation requires a pre-reflective consciousness of identification with the agents and behaviour perceived. If this identification is not given, the imitation is not obtained, which means that the experienced behaviour has not been taken as a motive of any behaviour. Identification would, therefore, be a sort of disposition to identify with certain people and attitudes. A disposition that, however, is only revealed in the imitation itself. The identification is not the motive, but the predisposition by which the agent determines her own action (for the concept of disposition, see Sartorio 2017, p. 160; and for the creation of causes, see Smith 2016). In this sense, the actions experienced by the agent are necessary, because she determines her acting by referring to some of the actions she perceived. Then, her action becomes understood as an imitation. When the action is not obtained, even if the agent has been exposed to perceptual stimuli, the imitation is not obtained either. This indicates that there has been no identification between the agent and the perceived action. In the same way, the experiments to which I have referred above informed us that the imitation of simple behaviour occurred depending on the degree of empathy that existed between the volunteer and the confederate (the researches do not consider this incompatible with imitation as a mechanism). I consider it common sense that the agent does not identify with all modes of being but only with those with which she has something in common and that equally she does not identify with all modes of acting but only with that mode in which she acts and which she shares with others. If we think, for example, of the act of opening the door for a woman to pass through (a sort of courtesy), it seems that it is an act (like that of all customs and habits) that has been perceived previously and with which the male agent has identified (carrying out that action is part of his self-image, because he identifies with it).Footnote 1 If the agent did not identify with this way of acting and being, even if he perceived this behaviour on a daily basis, he would not necessarily imitate it. He would not imitate it because pre-reflectively he does not identify with it. Let us say, that in such action this agent does not see himself reflected. Once the general line of thought has been set out and the fundamental concepts defined, I then move on to break down and carefully examine my arguments against the current literature on the subject. The thesis defended in this essay is twofold: first, it is argued that imitation is not possible without the pre-reflective consciousness of the agent, whereby the latter identifies herself with a certain way of being and acting. Secondly, as a consequence of the first, imitation requires perceptual stimuli and freedom. Thus, within the social context, perceptual conditioning requires equally the freedom and consciousness of the agent. That is, conscious will. In the following sections of the essay, I aim to explore this argument from several aspects related to the free will debate. In Sect. 2, I examine the role of consciousness in imitation within the agentive process as described above. In Sect. 3, I submit that the alternative possibility is unnecessary for claiming free will from the standpoint of a conscious recognition of a motive. In doing so, I emphasize the need to study perceptual stimuli and conscious will in connection. I thus suggest a version of the compatibilist approach, by which, although the agent is the cause of her action, her motives are linked to a necessary external conditioning. In Sect. 4, I develop my thesis at the level of social conditioning. In order to act freely, thus, the social agent requires both perceptual stimuli (in terms of social actions and situations) and her conscious will. In the final section, the essay concludes exploring some consequences of the view conveyed in previous sections.",
20.0,1.0,Mind & Society,11 October 2020,https://link.springer.com/article/10.1007/s11299-020-00260-2,Rationality and fatalism: meanings and labels in pre-revolutionary Russia,June 2021,Daniel W. Bromley,,,Male,Unknown,Unknown,Male,,
20.0,1.0,Mind & Society,10 October 2020,https://link.springer.com/article/10.1007/s11299-020-00265-x,Can affordances save civilisation?,June 2021,Darryl Penney,,,Male,Unknown,Unknown,Male,,
20.0,1.0,Mind & Society,14 August 2020,https://link.springer.com/article/10.1007/s11299-020-00248-y,Ten expert views on the COVID-19 pandemic,June 2021,Christian Wankmüller,Stefan Rass,Friederike Wall,Male,Male,Female,Mix,,
20.0,1.0,Mind & Society,10 September 2020,https://link.springer.com/article/10.1007/s11299-020-00254-0,Individual and community resilience in natural disaster risks and pandemics (covid-19): risk and crisis communication,June 2021,Panagiotis V. Katsikopoulos,,,Male,Unknown,Unknown,Male,,6
20.0,1.0,Mind & Society,09 August 2020,https://link.springer.com/article/10.1007/s11299-020-00251-3,"All policies are wrong, but some are useful—and which ones do no harm?",June 2021,Mario Brito,Maxwell Chipulu,Konstantinos V. Katsikopoulos,Male,Male,Male,Male,,
20.0,1.0,Mind & Society,05 September 2020,https://link.springer.com/article/10.1007/s11299-020-00255-z,"Words, numbers, warnings, tips, but still low risk perception",June 2021,Laura Macchi,,,Female,Unknown,Unknown,Female,,
20.0,1.0,Mind & Society,09 August 2020,https://link.springer.com/article/10.1007/s11299-020-00249-x,Judging the quality of (fake) news on the internet,June 2021,Stefan Rass,,,Male,Unknown,Unknown,Male,"Ever since the scandals around Cambridge Analytica, it has been recognized that the power of social media is useable to control people’s opinions by systematic spread of particularly crafted information to make people believe what they should believe. Various concepts like the “filter bubble” or others have been scientifically studied to describe the phenomenon that people may not be confronted with the full range of information, but only with a carefully selected subset of news that suits their interests, and possibly also their opinions. While there is nothing bad per se about giving people information that they are interested in, the ethical line not to be crossed is the point where the information that is given omits important information with the aim of “controlling” people’s opinions, fears, wishes or others. This would be fiddling with people’s free will, and leaving the philosophical questions aside here, let us adopt a technical perspective about how naturally grown internet connectivity has given birth to “Influencers”, and what theory and technology related to artificial intelligence can do here. First, let us distinguish a few terms for clarity (Rubin et al. 2015): false information is not necessarily the same as fake news, but the difference may be subtle: there is unverified information, which merely means that the information is passed onwards without verifying its quality or truth. There is not necessarily a manipulative intention behind this; in many cases, it is about gaining visibility (“clicks”, “likes”, …) or for economic purposes (say, to make ads visible more widely). Then, there is satire (or also sarcasm), which is also wrong information, but formulated in, often spoken, language that makes recognizing the incorrectness easy, so as to transport an underlying truth on a meta-level. Likewise, there is normally no manipulative intention behind. Exactly this distinguishes fake news, as being unverified (and in some cases even unverifiable) information, but coming in a jargon or language that makes it look trustworthy to serve a manipulative hidden agenda. Fake news can come in a variety of forms (Tan 2018), including the following: intended forgery (e.g., hand-crafted to look most realistic, whereas it is known to be false), selection of facts (e.g., taken out of the context or with details missing to change the understanding into wrong impression), conspiracy theories (e.g., presenting clear guilt of somebody about something, thereby avoiding all complexity of critical thinking), immoral arguments (e.g., alluding to the freeness of opinions), or manipulated audio/video. These are only a few selected forms of fake news, and not all of them are easy to discover. In some cases, we can discover a fake news by asking for more details about it. Intentional (not social) lying is very difficult, since the wrong information has to be consistent with a potentially huge number of related facts. Truth does not suffer from this problem, but a liar has to align its claims with an unlimited number of facts that could be brought into the picture. Thus, a first recommendation to recognize fake news is to ask for their origin and underlying evidence. For example, if an article claims a mortality rate of x% of COVID-19, one should ask about how this number has been calculated. There are different ways in which a mortality rate could be defined, for example, do we count people having had a disease at the time of death, or only those that were really dying from the disease (and not for a coincidental other reason). So, the mere statement that “the mortality rate is x” by itself is generally insufficient. An article speaking about such information should—to avoid the “fact selection” issue mentioned above, at least give the underlying numerical data and be specific on the details of the statistics that have been done.",2
20.0,1.0,Mind & Society,31 August 2020,https://link.springer.com/article/10.1007/s11299-020-00256-y,On the dynamics emerging from pandemics and infodemics,June 2021,Stephan Leitner,,,Male,Unknown,Unknown,Male,"The COVID-19 pandemic poses extreme and severe challenges to the society: There is a dramatic loss of lives worldwide, we experience and also expect for the future a multiplicity of challenges for economic systems, individuals are confronted with new and often very demanding situations, and public and private institutions are challenged to decide upon draconian measures under high time-pressure (Johns Hopkins University of Medicine 2020; Atkeson 2020; Xiang et al. 2020). This paper discusses emergent issues which appear to become particularly relevant in the context of the COVID-19 pandemic: Sect. 2 focuses on delayed effects of pandemics and Sect. 3 discusses adaptive societies and preparedness in the context of hazardous events. The understanding of the dynamics which unfold during and in the direct aftermath of a pandemic are key to effective and efficient management decisions. In additon, the insights into the dynamics of pandemics and infodemics can be employed for the good of the society in a long-term perspective as they might help minimize unwanted (and often delayed) effects.Footnote 1",4
20.0,1.0,Mind & Society,12 May 2021,https://link.springer.com/article/10.1007/s11299-021-00278-0,Democratic societies defeat (COVID-19) disasters by boosting shared knowledge,June 2021,Laura Martignon,Shabnam Mousavi,Joachim Engel,Female,Unknown,Male,Mix,,
20.0,1.0,Mind & Society,16 October 2020,https://link.springer.com/article/10.1007/s11299-020-00262-0,The epistemic uncertainty of COVID-19: failures and successes of heuristics in clinical decision-making,June 2021,Riccardo Viale,,,Male,Unknown,Unknown,Male,,2
20.0,1.0,Mind & Society,19 October 2020,https://link.springer.com/article/10.1007/s11299-020-00266-w,"“Trust me, I’m your neighbour” How to improve epidemic risk containment through community trust",June 2021,Silvia Felletti,,,Female,Unknown,Unknown,Female,"The situation created by the spreading of COVID-19 is a first in human history, with roughly a half of the world’s population subjected to isolation measures, which are to date the only practicable way to limit the spreading in absence of a cure or vaccine. Thus, the efficacy of the containment measures relies almost entirely on the citizens’ willingness to cooperate with institutions and with other members of society. For people to be actively committed, they first need to be aware of the risk, but risk communication could be useless or even harmful if it is not provided in a way that limits the likelihood of misconception and misbehaviour. Information should be primarily clear and easy to understand, in order to avoid the negative effects of a scarce risk literacy (e.g. a scarce ability to understand data in form of frequencies or percentages, or displayed by means of graphs, Cokely et al. 2012). Also, it should be research-informed and designed in a way to minimize the emergency of biases and errors that affect even experts’ risk perception (Kahneman et al. 1982). A good strategy to avoid biases or even harness systematic and predictable reasoning errors is using nudges that modify the context or “frame” of the decision to help people make better decisions for their own and the whole society’s sake (Thaler and Sunstein 2008; Viale 2019). Even when correctly informed, citizens may not be interested in being involved in the process of risk management: a good public risk communication should also be able to engage population, increase their sense of community and their feelings of personal responsibility.",
20.0,1.0,Mind & Society,06 August 2020,https://link.springer.com/article/10.1007/s11299-020-00250-4,Compliance to “Unpleasant” actions of crisis management: some remarks from a management control perspective,June 2021,Friederike Wall,,,Female,Unknown,Unknown,Female,"In the Covid-19 pandemic a lot of actions are taken by policy makers to inhibit the further spread of the pandemic. These actions comprise “lock-downs” including not only, for example, universities and schools but also interventions in the personal freedom of people. These interventions are of a kind that was, so far, unseen in many countries and include limitations of free movement or meeting with family and friends. The consequences for most of us are, at least, “unpleasant”. At the same time, in many countries, the enforcement of these actions is not so strict that there would be absolutely no decision-making scope which means that to some extent individuals also face situations of voluntary cooperation (Holländer 1990). The private so-called “Corona parties” may serve as an example of non-cooperative behaviour in the Covid-19 crisis. However, the apparent compliance of a vast majority of people in Europe may be a worthwhile subject of research. In this short note, I take a “management control” perspective and—starting with some kind of “thought experiment” —the attempt is made to figure out some lines for future research related to the linkage of micro-level and macro-level perspective on compliance to “unpleasant” actions taken by authorities for crisis management.",
20.0,1.0,Mind & Society,08 September 2020,https://link.springer.com/article/10.1007/s11299-020-00252-2,European disaster management in response to the COVID-19 pandemic,June 2021,Christian Wankmüller,,,Male,Unknown,Unknown,Male,"Disasters of recent years have caused thousands of victims and long-term ecological, social and economic damage to the affected areas. The Indian Ocean Tsunami in 2004, the Horn of Africa malnutrition crisis in 2011 and the Nepal Earthquake in 2015 are examples of devastating disasters, which put the resilience of society to the test. Since December 2019, the world population is once again facing a disaster of unexpected magnitude caused by a newly discovered coronavirus (SARS-CoV2). This virus leads to a severe acute respiratory infection (COVID-19), comparable to pneumonia, characterized by fever, cough and shortness of breath (Liu et al. 2020). This virus is remarkably dangerous, due to its ease of person-to-person transmission even by infected individuals who do not have any perceivable symptoms (Bai et al. 2020). This novel virus first attracted increased international attention in December 2019, when the Chinese government announced the first cases of infection in Wuhan, the capital city of Hubei, China. Since then, the virus has spread globally, resulting in the ongoing coronavirus pandemic. According to the Center for Systems Science and Engineering at Johns Hopkins University, a total number of 1,850,966 confirmed cases with 114,290 deaths in 185 countries worldwide had been reported by April 16th 2020 (Johns Hopkins University 2020). It is assumed that the number of unrecorded cases is considerably higher due to inconsistent testing and the non-availability of testing equipment in certain parts of the world. During the first weeks of the outbreak it quickly turned out that COVID-19 is much more dangerous than the conventional flu and therefore systematic interventions to contain the epidemic are urgently needed. At this point, problems and inefficiencies experienced in recent mega disasters have to be avoided in order to efficiently contain the virus’ spreading across Europe. This paper sheds light on the lessons learned by European disaster management in response to COVID-19 and asks for major drawbacks that still occur. The article continues with a short introduction to disaster management and observed inefficiencies of past disaster response (Sect. 2). Section 3 then reflects on the lessons learned by European governments and discusses alternative approaches to handle the situation. Section 4 concludes the paper with final remarks related to the topic.",8
20.0,2.0,Mind & Society,15 October 2020,https://link.springer.com/article/10.1007/s11299-020-00261-1,Tempest in a teacup: pandemic resilience in a Canadian small town,November 2021,Warren Thorngate,,,Male,Unknown,Unknown,Male,,
20.0,2.0,Mind & Society,25 July 2020,https://link.springer.com/article/10.1007/s11299-020-00247-z,Some reflections about diverse responses to the COVID-19 pandemic,November 2021,Hersh Shefrin,,,Unknown,Unknown,Unknown,Unknown,,
20.0,2.0,Mind & Society,31 July 2020,https://link.springer.com/article/10.1007/s11299-020-00246-0,Building up financial literacy and financial resilience,November 2021,Annamaria Lusardi,Andrea Hasler,Paul J. Yakoboski,Female,Female,Male,Mix,,
20.0,2.0,Mind & Society,19 November 2020,https://link.springer.com/article/10.1007/s11299-020-00253-1,Covid-19: equal response and unequal interests,November 2021,Hartmut Kliemt,,,Male,Unknown,Unknown,Male,,
20.0,2.0,Mind & Society,03 January 2021,https://link.springer.com/article/10.1007/s11299-020-00273-x,Opening up is not showing up: human volition after the pandemic,November 2021,Daniel W. Bromley,,,Male,Unknown,Unknown,Male,"We have never done this before. Habituated protocols and heuristics for human action are of no value. Bayesian algorithms offer no help. Ordinary risk assessment is of middling assistance. Uncertainty reigns. We are confused. This novel mélange of decision points—motivated by a different sort of novelty—is layered and contested. Who may (must) decide when a private business should (must) close? Who must (may) decide what to do when that act is contested by others? How do those questions vary when the subject is not a private business but is a religious house, a municipal facility, or a local chess club? The menu of necessary or volitional actions in the face of an approaching threat of a completely unknown nature is a wonder to behold. When consideration is then paid to an individual perspective versus a collective perspective, clarity dissipates. We have never done this before. But this is the easy part. After all, the above actions are motivated by a shared purpose to contain impending harm. There is a common enemy to be vanquished. Once all of the manifold considerations have been debated, weighed, reconsidered, disputed, and then finally undertaken, a new constellation of actions descends—and the wait for deliverance begins. All participants in this imposed re-constitution of familiar social arrangements—of new habits—will hold an array of views regarding the multitude of necessary adjustments. Each of us, with little residual autonomy in a global pandemic, is pushed and pulled, with varying degrees of mental reservation (and some minor bouts of open resistance), into a rather harmonized corps. We become coerced comrades—conscripts—in the struggle. Very few individuals agreed to those new protocols—local and national officials were the authoritative agents in the closing down. But virtually everyone must come to agree with those protocols. After all, we have little choice. Out of this social experiment, new habits will give rise to new shared norms of behavior. As John Dewey reminded us, we do not “take” new habits. We are our habits.",
20.0,2.0,Mind & Society,14 September 2020,https://link.springer.com/article/10.1007/s11299-020-00257-x,Covid-19 crisis in the Netherlands: “Only together we can control Corona”,November 2021,Gerrit Antonides,Eveline van Leeuwen,,,Female,Unknown,Mix,,
20.0,2.0,Mind & Society,27 October 2020,https://link.springer.com/article/10.1007/s11299-020-00258-w,Plastics and the coronavirus pandemic: a behavioral science perspective,November 2021,Fadi Makki,Anna Lamb,Rouba Moukaddem,Male,Female,Female,Mix,,
20.0,2.0,Mind & Society,08 February 2021,https://link.springer.com/article/10.1007/s11299-020-00271-z,The adaptive moral challenge of COVID-19,November 2021,Lindsay J. Thompson,,,,Unknown,Unknown,Mix,,
20.0,2.0,Mind & Society,23 December 2020,https://link.springer.com/article/10.1007/s11299-020-00272-y,On Kantian tendencies during the early corona pandemic in Germany,November 2021,Markus A. Feufel,Christine Schmid,Viola Westfal,Male,Female,Female,Mix,,
20.0,2.0,Mind & Society,08 February 2021,https://link.springer.com/article/10.1007/s11299-021-00275-3,Coronavirus in Ireland: one behavioural scientist’s view,November 2021,Peter D. Lunn,,,Male,Unknown,Unknown,Male,"On 29 February 2020, the first case of COVID-19 in Ireland was confirmed. Any hopes that this deadly virus would not reach our “island behind an island” died. Battle commenced. This article offers some reflections on what has followed from the perspective of a behavioural scientist with a government advisory role. It describes how Ireland has fared and highlights factors that perhaps contributed. It then offers some lessons that might be learned and which may generalise beyond this island. The contribution is unashamedly parochial and mixes claims for which there is clear evidence with personal observations for which there is not.",2
20.0,2.0,Mind & Society,17 September 2021,https://link.springer.com/article/10.1007/s11299-021-00282-4,"Biowarfare conspiracy, faith in government, and compliance with safety guidelines during COVID-19: an international study",November 2021,Olga Khokhlova,Nishtha Lamba,Marina Vinogradova,Female,Unknown,Female,Female,"Large scale events of global relevance are particular targets of conspiracy theorists (Prooijen and Douglas 2017), due to the proportionality bias–the belief that a large and significant event can only be explained by a proportionally large reason (Leman and Cinnirella 2007). The onset of the global COVID-19 pandemic has allowed for a whole range of conspiracy theories to emerge, indulging in speculations about the origin, spread, diagnosis, and treatment of the illness. These continue to remain misleading and are often devoid of a sound scientific basis (Craft et al. 2017). Numerous conspiracy theories have emerged during the pandemic, for instance—the link between 5G and coronavirus (Ahmed et al. 2020), the involvement of Bill Gates in causing the pandemic (Shahsavari et al. 2020) and the belief that the virus itself does not exist (Freeman et al. 2020). Of these, the most noteworthy in relation to global governmental influence on healthcare is the belief that the origin and spread of COVID-19 is deliberately engineered as a form of bioterrorism for socio-political advantage. Different conspiracists affiliated with a variety of ideologies report this to be the doing of one or more countries, in this case, bioterrorism is most commonly attributed to the Chinese government (Imhoff and Lamberty 2020; Schild et al. 2020). Globally, belief in biowarfare conspiracy has already resulted in hostility and mistrust, potentially leading to changes in international policies (Ling 2020). Conspiracy theories may function to decrease anxiety by providing an explanation for seemingly inexplicable phenomenon that is perceived as intriguing or mysterious, while also making individuals feel like they are holders of privileged information and create a sense of belongingness to an in-group with similar ideologies (Cichocka et al. 2015; Swami et al. 2016). However, Prooijen (2020) suggests that conspiracy theories may in fact further contribute to existential anxiety, thus reinforcing conspiratorial thinking when faced with anxiety-inducing events. Believing in a single conspiracy theory has been found to be predictive of belief in a variety of conspiracy-related ideas, which implies that a set of people with shared characteristics may be more susceptible to being influenced by suggestions of connivance (Swami et al. 2010; Prooijen and Acker 2015). Amongst a variety of personality profiles, previous research has shown that higher levels of education (Prooijen 2017), high self-esteem (Cichocka et al. 2015), and critical thinking (Swami et al. 2014) have been associated with lower conspiracist beliefs. Moreover, cognitive sophistication (Pennycook et al. 2020) and trust in science (Plohl and Musil 2021) was predictive of reduced COVID-19 misperceptions. Several research studies have also shown a high correlation between paranoid ideation, paranormal beliefs, schizotypy, and endorsement of conspiracy beliefs such as bioterrorism (Barron et al. 2014; Darwin et al. 2011; Georgiou et al. 2019). At the core of it, both paranoia and the biowarfare theory share a belief in external malicious or predatory intent and mistrust of individuals, organizations or outside forces such as the Chinese government (Larsen et al. 2020). Findings from a study by Oleksy et al. (2021) indicate that there is a greater need to focus on the content of the COVID-19 conspiracy theories, as those specifically related to the government may have the potential to cause greater harm by fostering public mistrust, in comparison to other general conspiracy beliefs. This is in turn could impact the perceived risk of the virus and the subsequent adherence to public health guidelines, which means that seemingly harmless speculation about the origins of the virus, can in fact have disastrous behavioral implications (Imhoff & Lamberty 2020). The rapidly changing guidelines and government measures to manage the spread of the illness have increased individual speculation about the government, subsequently affecting people’s faith in their political institutions. The systems justification theory posits that individuals strive to maintain the political status quo, and in the context of the pandemic, increased system justification may promote the endorsement of governmental measures by the public (Cichocka and Jost 2014; Jutzi et al. 2020). In the pre-pandemic world, differences in faith and trust in the government had varying patterns across the globe, depending on national political ideology and censorship (Ward et al. 2016). Cross-cultural differences are apparent, for example—countries such as China, India, Indonesia, UAE, Saudi Arabia and Malaysia have been found to have high trust in their government while, US, Germany, France, Ireland, UK and Russia rank the lowest (De Bruin et al. 2020; Gheorghe 2020). Other reports in line with these views (Fournier et al. 2011; Holbrook 2004 also discussed how countries such as Russia, Canada, Singapore, US and UK held pessimistic views regarding the economic prospects of their country which led to decreasing faith in their government systems. Political ideologies have already been shown to influence the perceived threat posed by the coronavirus (Calvillo et al. 2020). A pattern of ‘conspiratorial style of reasoning’, political extremism and psychopathology (Georgiou et al. 2020) is observed in people that have shown conspiracy theory endorsement during the pandemic. Negative attitudes towards government-issued guidelines during the pandemic have also been shown to be associated with individual endorsement of conspiracy beliefs about COVID-19 (Freeman et al. 2020; Georgiou et al. 2020). For example, Oleksy et al. (2021) reported that skepticism towards the government was predictive of lesser engagement with protective measures such as hand washing, while Swami and Barron (2020) found that rejection of COVID-19 conspiracy theories was associated with higher compliance with social distancing measures. It appears that differences in attitudes towards government measures across countries, can be attributed to the overall management of the spread of COVID-19, actual risk posed by the virus (in terms of number of cases relative to population), pre-pandemic levels of government trust, and national socio-economic inequality (Freeman et al. 2020; Plohl and Musil 2021). However, more research is needed to better understand factors which determine trust in political institutions during such public health emergencies and how it may have changed during the course of the pandemic. With the start of the pandemic, countries across the world have put a wide range of guidelines to ensure public safety and to curb the spread of the virus. Unless enforced with strict penalties, compliance to the measures has varied greatly, with many downplaying the risks and ‘ignoring’ the guidelines or contributing to the spread of both the virus and virus-related misinformation (Plohl and Musil 2021). The lack of a mutually agreed understanding of the pandemic has left space for radical polarizing beliefs to emerge, the behavioral manifestations of which can have a devastating effect on the public health measures that are put in place to protect people (Freeman et al. 2020). This may also mean that relying only on fact-based coverage of COVID-19 guidelines by the media, is insufficient and likely to fall on deaf ears, when it comes to individuals prone to conspiracy beliefs. In the current global climate, it is imperative to understand individual motivations to follow guidelines, so that targeted behavioral changes can be elicited. Recent research suggests that motivating factors to follow public health guidelines include—general health beliefs, peer influence, prosocial behavior, estimation of personal vulnerability and fear/perceived threat of the virus (Andrews et al. 2020; Clark et al. 2020). Of these, Harper et al. (2020) have found that the strongest motivator for compliance with safety standards was the personal fear of contracting COVID-19. Further investigation can help establish recommendations for strategies to mitigate the virus (and future outbreaks) and how they are communicated to the public in a way that accounts for individual differences and promotes compliance. Moreover, exploring the cross-cultural differences between governmental attitudes also has implications for adherence to guidelines that can ultimately increase or decrease the spread of the virus across the world (Hasan et al. 2020). Given the novelty of the current pandemic, this multi-national study was designed to explore several relationships between faith in government during COVID-19, belief in conspiracy theories and bioterrorism, compliance with governmental measures, paranoia and other cognitive factors like critical thinking. The study also aimed to understand country-wise differences in faith in government during the pandemic and endorsement of biowarfare conspiracy using a multi-national sample, and differences between high and low-risk countries in perceived compliance with public health measures. In addition to that, we aimed to understand the most motivating factors for following the guidelines.",2
20.0,2.0,Mind & Society,02 September 2021,https://link.springer.com/article/10.1007/s11299-021-00280-6,Economic behavior and behavioral economics at times of COVID-19 pandemic,November 2021,Doron Kliger,,,Male,Unknown,Unknown,Male,"FORECASTS from recent Oxford Economics reports depict three scenarios Footnote 1 for the world GDP (Oxford Economics2020 ).Footnote 2 The red column in Fig. 1 on the first page of the Oxford Economics report shows the baseline, pre-coronavirus, scenario of projected real GDP growth. The blue column there is the updated baseline, i.e., taking into account COVID-19, indicating about 2.5% decline in real GDP growth, and the yellow column presents the ""downside"" or ""worst-case"" scenario of about 4% decline, relative to the pre-coronavirus scenario. Figure 1 on page 3 of the report presents a longer look into the future. It reveals a bright side to this grim projection, as it implies that the main damage is only temporary. This is reflected in a 0.2% projected permanent damage to the baseline, which can expand in the worst-case scenario to a roughly 1% decline in real GDP. Other projections were developed by the OECD (OECD 2020).Footnote 3 Their ""contained"" scenario, by now obsolete, was portrayed under the hope that the epidemic would be restricted more or less to the Far East, in which case the prediction was a − 0.5% full-year impact on 2020 world GDP; and their “downside” scenario, which is unfortunately coming true, predicts a corresponding impact of − 1.5%. Moving along the worst-case scenario is the bad news. The good news, as the projections claim, is that the economy is expected to recover from the crisis as soon as toward the end of the current year 2020. IMMEDIATE REACTIONS to COVID-19 disrupted the “normal” flow of events around the world. Inspecting the website of Flight Radar 24 showed a sharp drop in air-flight departures from mainland China’s busiest airports by mid February, 2020.Footnote 4 Clearer skies have two major and opposing consequences: direct damage to the economy from the lost revenue in the airline industry and related economic activity, versus significantly cleaner air, reflected in the lower concentration of nitrogen dioxide in the air (among other pollutants). This is clearly (pun intended) seen in the map of mainland China just before the major air-traffic pandemic-related reduction, as opposed to during the pandemic, at the nitrogen dioxide emissions heatmaps provided by the NASA Earth Observatory.Footnote 5 Worth viewing are photos of New Delhi that were taken from the same vantage points “then and now”, showing the dramatic reduction in water and air pollution—the kind of changes resulting from the pandemic which we would want to preserve for the future to come (The Guardian 2020).Footnote 6",
20.0,2.0,Mind & Society,06 July 2020,https://link.springer.com/article/10.1007/s11299-020-00238-0,How did the U.S. stock market recover from the Covid-19 contagion?,November 2021,Shyam Sunder,,,Male,Unknown,Unknown,Male,,3
21.0,1.0,Mind & Society,04 March 2022,https://link.springer.com/article/10.1007/s11299-022-00287-7,Multi-player electoral engineering and COVID-19 in the polish presidential elections in 2020,June 2022,Jarosław Flis,Marek Kaminski,,Male,Male,Unknown,Male,"The government’s fast reaction, combined with the citizens’ discipline, resulted in the suppression of the virus’s spread in Poland. Its containment was especially striking when compared with other large EU countries or the United Kingdom (see Fig. 1). The physical mobility index indicated that Poles quickly limited their activity at low levels of infections, thereby reducing the nation’s infection rate (see Fig. 2). Poland, however, was not the only one; other Central-Eastern European countries, such as Romania, also managed to keep their curves’ flat.Footnote 1 Daily increase in the number of cases (weekly average) per million. (Note. The United Kingdom and the six largest EU countries (Italy, France, Germany, Poland, Romania, and Spain) are represented. The grey shading represents the period after April 5. Source: Johns Hopkins CRC (2020); CIA Factbook (2020).) Change in physical mobility by the number of cases per million. (Note. “Physical social mobility” is shown as a percentage of such average mobility in the week from January 27 to February 2, 2020. Source: Weekly averages of the variable “walking” found on Apple Mobility Trends from Apple Maps (2020). The graphs end when minimum mobility was reached.) It is possible that Poland’s and other Central-European nations’ determination was strengthened by the grim news coming from Italy and Spain, where the pandemic had developed two to three weeks earlier. French, Germans, and Spaniards waited the longest after the virus took hold before limiting their movements, but then the progression of movement restrictions was rapid. Italians and Britons started quite early limiting their movements but then changed their behavior relatively slowly. In terms of public opinion, the Polish government’s resolve was rewarded with positive polling: 70% of respondents believed that the authorities had dealt well or quite well with the pandemic versus 6.3% who held the opposite view (United Surveys 2020). Confidence in PiS’s politicians skyrocketed. Łukasz Szumowski, the country’s no-nonsense minister of health, as well as an accomplished cardiologist, led as the country’s most trusted official with a 51.1% rating, versus 15% who said they distrusted him, followed quite closely by President Andrzej Duda and Premier Mateusz Morawiecki (Ibris 2020, April 7). Meanwhile, the various opposition leaders were behind or far behind in their approval ratings. In a representative electoral poll taken at the time, 54.6% of voters declared their support for President Duda against the fragmented opposition candidates’ support, which ranged from 5.5 to 15.5% (Ibris 2020, April 29). With these good numbers, Duda could expect a first-round win in the majority runoff election on May 10. The economic consequences of the lockdown in Poland were serious but far from disastrous. A consensus among economists was that the nation’s slump would be shallow. A typical article in Bloomberg anticipated a 4.3% fall in Poland’s GDP—the smallest change among all 27 EU economies (Bujnicki and Krajewski 2020). Again, the government’s fast and bold reaction was praised. Additionally, other factors, such as the low price of oil (with Poland as an importer) and the anticipated stay in the country of summer tourists who routinely vacationed in Greece, Turkey, Spain, and Italy, were also expected to help. That said, the taming of the pandemic—generally interpreted as the ruling government’s spectacular success—created an unexpected electoral dilemma. The ratings of President Duda were near their all-time highs. However, it was also reasonable to expect a future slow slide in the president’s popularity, caused by lockdown fatigue and growing economic distress. Moreover, the slow rise in COVID-19 cases suggested that Poland might need more time than other big EU countries to completely extinguish the pandemic. Any delay in the timing of the election could witness a reversed perception of the country’s comparative success and a dramatic fall in the president’s ratings. Under such circumstances, PiS had strong incentives to keep the original election date, while the opposition demanded a postponement.",
21.0,1.0,Mind & Society,08 March 2022,https://link.springer.com/article/10.1007/s11299-022-00288-6,Addressing threats like Covid: why we will tend to over-react and how we can do better,June 2022,Mark Pingle,,,Male,Unknown,Unknown,Male,"The possibility of becoming infected with the Sars-Cov2 virus and experiencing a Covid19 disease is a private good problem and a public good problem. Privately, you have reason to take action to mitigate the new threat to your health. Social distancing, mask wearing, sanitizing and shutting down your business are costly actions, but you might choose to engage in them to some degree because reducing your probability of being infected may provide net benefits. However, such private actions also generate positive externalities because they reduce the probability you will infect others, or we can say that acting to reduce virus spread is a contribution to a public good. By living less freely, you reduce the possibility that you emit a negative externality that will occur if you contract and spread the virus. Desires for justice and welfare motivate government intervention. Additional government restrictions and spending will tend to impose costs on you and me, but these additional costs are justified if they sufficiently reduce the negative externalities or sufficiently internalize them. The additional individual benefits you and I receive from the enhanced public good (i.e. the reduced likelihood of infection) will increase our individual welfare if they outweigh the additional costs we experience. However, as Coase (1960) well expressed, identifying the most effective way to address an externality is not easy. In part, this is because the social costs imposed by negative externalities are reciprocal: “To avoid the harm to B would inflict harm on A (Coase, 1960, p. 2).” Consequently, Coase (1960, p. 2) contends the first question we must answer is a rights question: “Should A be allowed to harm B or should B be allowed to harm A?” Coase’s Theorem is that, if we effectively specify rights and transactions costs are zero, then any assignment of rights will lead to the same outcome; so government need not intervene. Of course, transactions costs are normally not zero, so Coase (1960, p. 18) describes the general problem as “choosing the appropriate social arrangement for dealing with the harmful effects.” He concludes, “It is all a question of weighing up the gains that would accrue from eliminating these harmful effects against the gains that accrue from allowing them to continue (Coase 1960, p. 26).” This paper explains why particular behaviors studied behavioral economists and psychologists suggest government policy makers, along with private decision-makers, will tend to over react to any new threat like Covid 19. Over-reaction implies some government policy actions will generate costs greater than the benefits, but also over-reaction will also take the form of higher-level governments discounting the adaptive capabilities of individuals, private sector groups, non-profits, and lower level governments. Of course, this does not imply government should take no action, nor does it imply higher-level governments should assume lower level governments and others will respond sufficiently. The hope offered by this paper is that a more refined understanding of the factors motivating reactions to a very serious but low probability threat like Covid 19 will lead to better choices, individually and collectively.",
21.0,1.0,Mind & Society,26 March 2021,https://link.springer.com/article/10.1007/s11299-021-00277-1,Monty Hall three door ’anomaly’ revisited: a note on deferment in an extensive form game,June 2022,Philipp E. Otto,,,Male,Unknown,Unknown,Male,"Since Friedman (1998), the Monty Hall decision problemFootnote 1 was intensively discussed. While the experimental observations appear interesting, its behavioral explanation still remains disappointing. The investigated decision frame is constructed after a television show where three doors exist of which only one conceals the winning prize, while the other two equal zero profits. After you, as the contestant, have picked a door of your choice, the show master Monty opens one of the unchosen doors which does not reveal the prize. The question then is: do you want to switch to the remaining door or do you want to stick with your original choice. In other words, what are the winning probabilities for changing and not changing doors. In the standard construction, Monty always opens one of the unchosen doors (the one without the prize if the prize has not been chosen and otherwise one of the two randomly) and offers the contestant the option to change doors. Under these simplifying specifications the undisputed consent is to always change your door as the remaining door’s probability to be a winning door is (at least) larger than 1/3,Footnote 2 while the probability has not changed for the initially chosen door. Why is it then that so many of us stay with their initial choice and do not want to change to the other door with the higher winning probability? Is it then necessary to resort to things like reverse psychology as a possibility raised by Kevin Spacey in the role of MIT Professor Micky Rosa (in the movie “21” released 2008 by Columbia Pictures)?Footnote 3 Interestingly, not only the first intuition is to stick with the initially chosen door, but experimental investigations show that many participants remain reluctant to change and do not switch to the other unopened door. Though, playing the Monty game repeatedly documents a robust learning effect toward increased switching close to or slightly above 50% (i.e. Friedman 1998). Palacios-Huerta (2003) show that incentives, ability, and social interaction can further strengthen learning effects in the repeated game. In a similar vein, Slembeck and Tyran (2004) conclude that communication and competition between participants supports learning towards increased switching – especially over the first rounds. Repetitions seem to help, although do not lead to optimal behavior. Granberg (1999a) show in their cross-cultural comparison study that sticking with the initial choice in the Monty game is a rather universal phenomenon. Cognitive illusions (i.e. of control) or cognitive biases (i.e. status-quo) have been proposed as possible explanatory concepts for such kinds of behavior (compare Granberg and Brown 1995; Granberg 2014). Can game theory provide alternative solutions besides explanatory concepts and posthoc rationalizations?",
21.0,1.0,Mind & Society,24 September 2021,https://link.springer.com/article/10.1007/s11299-021-00281-5,Social robots and digital well-being: how to design future artificial agents,June 2022,Matthew J. Dennis,,,Male,Unknown,Unknown,Male,"Digital well-being will become an increasingly important measure of the quality of life in the 21st century. While this topic first emerged as a concern over social media use—especially screentime—increasingly theorists are employing the idea to evaluate our interactions with technology more widely (Burr et al. 2020; Floridi 2014; King 2019; Snow 2019). Understanding digital well-being involves picturing what the contemporary good life looks like, and focusing on how emerging technologies can best enhance human flourishing. Theorists who work on this topic apply character-orientated ethical theories to the technologies we live and interact with on a daily basis, showing how digital artefacts and services can contribute to living a good life, to cultivating character, and to achieving happiness or other eudaemonic states. Christopher Burr et al.’s literature review on recent research on digital well-being illustrates this approach. Burr begins by stating that digital well-being is ‘what it means to live a life that is good for a human being in an information society’, after which he and his co-writers move to show how this concept should inform our evaluation of future technologies (Burr et al. 2020, p. 1). Scholars who are sympathetic to prioritising digital well-being seek the resources to apply it to technology from diverse ethical theories of character and virtuous character traits: from neo-Aristotelianism (Coeckelbergh 2009; Elder 2019; Vallor 2011, 2012a, 2016) to Stoicism (Dennis 2019; Klincewiez 2019), from Confucianism to Buddhism (Vallor 2016; Wong 2016), to figures in post-Kantian philosophy (van de Poel 2012; Verbeek 2012). While each of these traditions propose their own conceptions of human flourishing,Footnote 1 they are united in their view that the good life includes appropriate social relationships—the most prized of which is friendship. Consequently, the capacity of social robots to provide either: (1) fully fledged virtue friendship (Danaher 2019), or (2) task-orientated companionship (De Graaf 2016; Elder 2019; Vallor 2012b)—sex robots, for example (Danaher and McArthur 2017; Devlin 2018; Peeters and Haselag 2019)—should be considered a vital issue in social robotics. This article evaluates the future morphology of social robots using a character-orientated framework, focusing on their ability to contribute to the good life by providing both friendship and social companionship. Instead of analysing social robots in terms of the specific moral harms they may (or may not) cause, my approach takes inspiration from Mark Coeckelbergh’s observation that the ‘focus on the morality of robots means that ethical questions concerning how humans interact with [them] remain out of sight’ (2009, p. 217; both emphases added). To do this, I engage with the empirical literature on this topic (where available), although my primary aim is to prepare the conceptual ground for an account of how we should design social robots in ways that prioritise digital well-being. Approaching social robotics from a digital well-being perspective involves focusing on the following questions: what design choices will faciliate living well with social robots? And how will these design choices impact on our digital well-being? In what follows, I argue that we can increase the capacity of social robots to exist harmoniously with the demands of our digital well-being by ignoring media hype and remaining open-minded about their likely future morphology. This means that we have to interrogate the design choices made by the creators of today’s social robots at the most elementary level. I begin by exploring the various forms that future social robots might take, emphasising that both philosophers of technology and the public often find it hard to predict the direction that future developments in technology may move. After this, I compare the advantages of embodied social robots (ESRs) and disembodied ones (DSRs).Footnote 2 To do this, I focus on what the character-orientated ethical traditions agree is a core aspect of the good life: friendship, and similar types of beneficial social relations. In conclusion, I lay out four key issues that circumscribe what we should consider when thinking about how to live well with social robots.",4
21.0,1.0,Mind & Society,24 November 2021,https://link.springer.com/article/10.1007/s11299-021-00284-2,The perception of doability and how is it measured,June 2022,Ryszard Praszkier,Agata Zabłocka,,Male,Female,Unknown,Mix,,
21.0,1.0,Mind & Society,28 February 2022,https://link.springer.com/article/10.1007/s11299-022-00285-9,Animal vs. human rationality-cum-conceptuality: a philosophical perspective on developmental psychology,June 2022,Yakir Levin,Itzhak Aharon,,Male,Male,Unknown,Male,"The main topic of this paper is the rationality of non-human animals and its connection with their conceptuality and the rationality-cum-conceptuality of human adults. When dealing with this topic with respect to non-human animals we shall eo ipso deal with it as regards human infants. The basis of our discussion will be Susan Carey’s seminal account of the origin of concepts. We shall first extract from this account a notion of rationality, which is (1) applicable to human infants and non-human animals; (2) significantly different from the notions prevalent in behavioral ecology and yet, like these notions, amenable to empirical testing; (3) conceptually more fundamental than the latter notions. Relatedly, this notion (4) underlies a proto-conceptuality ascribable, by a key component of Carey’s account, to human infants and non-human animals. Based on a Kantian-inspired analysis of fully-fledged conceptuality and the type of rationality underlying it, we shall then show (1) the profound difference between the type of rationality extracted from Carey’s account and the rationality of human adults; (2) related fundamental differences between the types of conceptual representation that these types of rationality respectively ground. By showing this, we shall highlight fundamental aspects of conceptual representations that are missing from Carey’s account of the origin of concepts. Based on this, we shall finally argue that, as ingenious and explanatorily valuable as Carey’s account of the origin of concepts is, it is only a partial account of this origin. To be a bit more specific, we shall start by outlining the classic empiricist picture of mental representations in Carey’s modern guise (Sect. 2). Next, we shall outline Carey’s rationalist alternative to the empiricist picture and its key theoretical notion—core cognition—that is also a cornerstone of her account of the origin of concepts (Sect. 3). We shall then exemplify Carey's rationalist model by outlining her main experimental methodology—the violation-of-expectancy looking-time methodology—and briefly showing how it applies to one specific domain of core cognition—the world of middle-size objects (Sect. 4). This methodology has been applied to non-human animals thereby showing, as we shall next point out, that some such animals share human core cognition (Sect. 5). Based on this, we shall then outline a notion of rationality underlying the notion of core cognition—core cognitive inferentiality—that applies to both infants and non-human animals (Sect. 6). Finally, based on a Kantian-inspired analysis of conceptual-thought-cum-rationality, we shall show the profound difference between infants-cum-non-human-animals and human adults with respect to these features, as well as how this difference reflects on Carey's account of conceptual representations and their development (the “origin of concepts”, in her terminology). In addition, we shall outline a methodology—heterophenomenology—for approaching empirically the Kantian-inspired analysis (Sect. 7).",
21.0,1.0,Mind & Society,20 March 2022,https://link.springer.com/article/10.1007/s11299-022-00286-8,Do self-talk phrases affect behavior in ultimatum games?,June 2022,Vincenz Frey,Hannah N. M. De Mulder,Vincent Buskens,Male,Female,Male,Mix,,
21.0,1.0,Mind & Society,27 October 2021,https://link.springer.com/article/10.1007/s11299-021-00283-3,A role-game laboratory experiment on the influence of country prospects reports on investment decisions in two artificial organizational settings,June 2022,Marco Castellani,Linda Alengoz,Flaminio Squazzoni,Male,Female,Male,Mix,,
21.0,2.0,Mind & Society,16 November 2022,https://link.springer.com/article/10.1007/s11299-022-00291-x,Scope of heuristics and digitalization: the case of marketing automation,November 2022,Simone Guercini,,,Female,Unknown,Unknown,Female,"“Marketing automation” means automatic support for marketing decisions in the digital task environment (Little 2001; Heimbach et al. 2015). The central idea of marketing automation is to use models to cope with the large amount of data produced automatically because of digitalization of process in the business environment to react adaptively to customers, competitors and influencers behavior, to produce effective proposals, and getting preferences (Bucklin et al. 2002). To date, the heart of marketing automation has been recognized in an automatic customization of the marketing activities, an element that allows strengthening areas such as direct marketing and interaction (Heimbach et al. 2015: 130). The great availability of data allows the taking of decisions for marketing actions in an automatic form starting from parameters set using specific software and through the adoption of algorithms. These algorithms use the data inputs to produce predictions and behavior, sometimes almost in real time with respect to the production of the data (for example, analytical data from social media or search engines to activate promotions to specific online customers). The adoption of this support is important to face the speed with which market data is formed and used by marketers in the digital task environment (Hirt and Willmott 2014). Heuristics define solutions to decision problems starting from one or a few “cues”. Research tested formal models of heuristics in specific cases showing they can perform better than complex and “information-intensive” decision models (Gigerenzer et al. 1999). Heuristics have been studied in the early research on artificial intelligence (Newell 1981; Simon 1995), and more recently they have been object of attention on different perspectives in organization behavior, strategic management, business and entrepreneurial decision-making (Artinger et al. 2015; Bingham et al. 2019; Guercini and Milanesi 2020; Guercini et al. 2015, 2022; Loock and Hinnen 2015; Luan et al. 2019; Picone et al. 2021; Shepherd et al. 2015; Sinyard et al. 2020; Sull and Eisenhardt 2015). In the literature, decision making processes include two, mutually exclusive types: rational decision making versus rule-based decision making (March 1994). The “scope” of heuristic rules refers to the fit of the decision making rules with the context in a rule-based decision making. “Scope” is (the extention of) the field in which a heuristic can be applied (with success). Looking at the scope means shifting the attention from the accuracy of the decision making model (Gigerenzer and Gaissmaier 2011) to the borders of the context where it is effective (task environment). Emphasizing this aspect is crucial for at least two reasons that are particularly relevant to highlight in introducing the contribution this article intends to make (Grant and Pollock 2011). First, having in mind that there is a scope for heuristic rules offers an important key to the literature and the debate about whether their use is biased or smart. Second, having in mind that there is such a scope immediately highlights the existence of a gap on a topic that is fundamental to the study of heuristics in decision making processes. If ""scope"" can be translated as ""rule environment,"" then one can viasualize and map the area of effectiveness as the intersection between task environment and rule environment, which is a terrain on which our study has the ambition to advance scholars' understanding (Grant and Pollock 2011). This paper intends to address this gap the context of marketing automation, focusing on the impact of digitalization and marketing automation on marketers’ heuristics scope. With reference to this subject, this paper is based on evidence collected with in-depth ethnographic interviews (Van Maanen 2011) with twenty-three marketers (entrepreneurs, managers, marketing consultants). We consider a set of heuristics emerged from previous explorative research (multipliers, thresholds, calends), to discuss the impact of marketing automation on the scope of heuristic rules in decision-making. The marketers interviewed make extensive use of heuristics to deal with their tasks. The paper discusses how the scope of marketers’ heuristics evolves. Questions for a research agenda are proposed: How digitalization and the adoption of marketing automation systems impact on the models of decision adopted by marketers and specifically the scope of marketers’ heuristics? The approach adopted in this paper is descriptive and non-prescriptive, taking into consideration how the decision changes ""in the wild"", understood as ""large world"", but in any case gathering useful elements for the subsequent formalization of the decision models adopted by decision makers (Katsikopoulos 2019), which are marketers in this case. The paper offers an exploration of the assessments made by managers, consultants and entrepreneurs, gathering clues on how digitalization, in the form of the adoption of automatic marketing systems impacts the decision models to which they can refer.",
21.0,2.0,Mind & Society,23 November 2022,https://link.springer.com/article/10.1007/s11299-022-00292-w,Mental footnotes in Socialism: the current social validity of the concept of bourgeoisie from the Marx’s and Engels’ “Manifesto of the communist party”,November 2022,Jose L. Vilchez,,,Male,Unknown,Unknown,Male,"In the modern era, socialist ideas became more present in a number of political systems and the common social imaginary (cf. Abreu 2010; Bravo 1976; Cole, 1953/1957; Fourquin 1973; Mackenzie 1969). However, this political/economic Philosophy had their beginnings in much earlier stages of history (e.g., Moro, 1516/2011), in which idealized social systems were described and pursued (Mackenzie 1969). The utopian Socialism (coined by Engels 1880/1892) was already dealt with by the eighteenth century socialists (such as Saint-Simon, Fourier or Owen; Abreu 2010). These pioneers were interested in optimal forms of social organization (Kirkup 1892). In these works, the creation of a perfect society was promulgated. A society in which the human being lives in peace, harmony and equality, and society itself develops peacefully through the simple will of people (De Cabo 1987). In the nineteenth century, the so-called scientific Socialism (term coined to distinguish it from the utopian Socialism) was developed based on the Marx’s and Engels’ theories (Ruiz 2011). Nowadays, the term scientific would make no sense in light of consolidated Philosophies of Science, such as Positivism (cf. Popper 1959). However, the “Manifesto of the communist party” is considered the genesis of scientific Socialism (Marx and Engels, 1848/1888). This work assumes the postulates of the Hegelian dialectic (Ruiz 2011). The logical dialectic represents a process to interpret reality through the spirit, the nature and all human history and culture (Harvey 2018). Hegel conceives thinking as two poles of reasoning, in which there must be a clause (thesis), its opposite clause (antithesis) and its unity through a dialectical process (synthesis; Vásquez 1993). On the other hand, Marx proposes a materialistic transformation of the world though a dialectical process between mind and matter (Harvey 2018). This manner, the human development is the result of the evolution of pre-existing social relations based on matter (its changes and the changes on the means of production of human beings). Marxism can be conceived as (Fau 2011; Harnecker 1976; Ormerod 2017): (a) a theory of historical materialism; (b) the gradual progress of society from a feudal monarchist state to a socialist society, in which there would be so much abundance that each person would contribute according to their capacity and take according to their needs; (c) the overcoming of capitalist relations of production which is the result of the class struggle; and (d) the end of all exploitation (and of all class conflicts) and the gradual dissolution of the state (since it will be no longer necessary). These Marxist ideas spread worldwide in the twentieth century. However, its theoretical, primitive proposal has not remained the same, experiencing multiple transformations throughout history (Ruiz 2011). In this sense, Lenin reformulated those ideas in the aim to implant Socialism as an economic, social and political model to struggle Capitalism (Harnecker 2000). For Lenin, Socialism was inconceivable without (Stalin 1924): (a) the latest discoveries of modern science; (b) a planned state organization able of compelling tens of millions of people to obey strictest standards in production and distribution of products; and (c) the domination of the proletariat of the state. Among the most important events of the twentieth century based on this Philosophy was the bolshevist coup in October 1917 (Laffont 1997), against the allegedly Tsarist autocracy (Resnick and Wolff 1994). We coin coup and allegedly because historical sources (e.g., Pipes 2016) affirm that, in February 1917, Tsar Nicholas II had already abdicated in his brother (Michael Aleksandrovich Romanov) and this, in turn, delegated to the Dumas the constituting process to set up a government (forming a republic government which president was the socialist Alexander Kerensky). In this political climate, textile workers in Petersburg began a strike that spread throughout the whole Russia (De la Cruz 2011). Shortly thereafter, the Bolsheviks (political party), led by Lenin, took over (Resnick and Wolff 1994), without winning the election organized by the republic government (Pipes 2016). This coup (called the dictatorship of the proletariat) replaced money with a barter system, nationalized companies, took over the agricultural production and strictly controlled workers (Velarde 2017). As a result of these actions, a civil war took place between 1918 and 1921, which result was Lenin being forced to establish the New Economic Policy (NEP), a mixed economy between capitalist and socialist measures (Resnick and Wolff 1994). Inspired by the Soviet experience, the so-called great proletarian cultural revolution took place in China, led by Mao Tse Tung in the 1960s (Dongli and Karl 2005). Over time, the economy collapsed, farms and factories extremely reduced their production and universities closed; the right to work, education, medical care, along with other socio-economic rights, were lost (Meyskens 2018). In this sense, the collapse of the Union of Soviet Socialist Republics (USSR) and the failure of the socialist model in the rest of countries have been suggested to be due to (Espinoza 2014; Gregor 2019; Paterman 2019): (a) an erroneous interpretation and implementation of Marxism, which took place in retrograde economic circumstances; (b) because of a democratic political deficit; (c) the incapacity of economic solvency; and (d) the dogmatization of thinking. However, new perspectives in social movements promotes a new Socialism today to prevent current social injustice and inequality (e.g., Harnecker 2010). It is proposed that the bad reputation (that Socialism has had and keeps having) strengthens Capitalism (Harnecker 2010), due to the concentration and abuse of power (Felber 2012).Consequently, the so-called twenty-first century Socialism is suggested as a new option. This political ideology has been promoted in Venezuela, Bolivia, Argentina or Ecuador, among others, and it hopes to differ from the Socialism developed in Eastern Europe in the second half of the twentieth century (Hamburger 2014). The reforms stated by this new perspective include the transformation of the state structure (and its relations with society), the nationalization of certain sectors of the economy, the centralization of the political power and the complete administration through the state (Monedero 2008; Ramírez 2017). Despite the attempt to implement this new socialist model, especially in the Hispanic-American countries, it has been reported (Ramírez 2017): (a) corruption; (b) populism; (c) abuse of power; (d) ineptitude in the application of their efforts; and (e) negligence in the implementation of public policies that violates fundamental rights. On the other hand, some individuals think that social movements are imaginary and they are taken in a purely noumenic sphere. People are the ones who have to carry out those ideologies that become heuristics for them. When humans reason, they envisage possible instances regarding to the meaning of a statement. This mental representation is as iconic as possible, in form of images which structure is analogous to the structure of the situation that they represent (Johnson-Laird 1983). When imagining, the information built up constitutes our conceptual truth (in terms of Johnson-Laird and Savary 1999). Mental models include the lowest common denominators that define a specific situation. For example, if the reader is asked to imagine a “football stadium”, it would be highly likely that there would be defining and common characteristics in their mental representation such as “green field”, “11 versus 11 players” (although there is a possibility that any player can be expelled from the game), “two goals”, “grandstands” or “spectators”. However, these mental images can be modulated by the context (e.g., Vilchez 2013). The simplest example of this influence would be negation. With negations, individuals represent what is possible to imagine and generate a mental footnote (in terms of Johnson-Laird and Byrne 1991) to deny that information. If the reader was instructed to “not think of an elephant”, they probably imagine an elephant with a mental footnote denying such possibility (e.g., Vilchez 2018). In this sense, the implicit information of mental footnotes constitutes our general cognitive framework to reason (Chomsky 1980; Vilchez 2016) and determines the outcome of our cognitive processing. Therefore, mental footnotes become heuristics that save mental resources (cf. Simon 1955) but could have a relevant impact in our attitude/behavior (in this case, in our political stand). This implicit meaning is conditioned (in terms of Pavlov, 1926/1927) and it can have other meanings associated (cf. Vilchez 2013). As an example, nowadays, the term “woman” has many socially associated meanings such as “historically cast out” or “we have an historical debt with them”; even if it could be true or not. The influence of mental footnotes is proposed to go further than the mere negation (Vilchez 2019a), they are a guidance for daily reasoning and decision making (Vilchez 2018); they also have a determining role in pathological, psychological syndromes (Vilchez 2019b, 2016) and even an effect on the mere movement (Vilchez 2015). Since the twenty-first century Socialism is a fact in our current society (Hamburger 2014), we want to study how far and how deep is the Marx’s and Engels’ (“Manifesto of the communist party”; Marx and Engels 1888/1848) influence on the current society.",
21.0,2.0,Mind & Society,24 November 2022,https://link.springer.com/article/10.1007/s11299-022-00290-y,﻿Learning with insufficient data: a multi-armed bandit perspective on covid-19 interventions,November 2022,Jean Czerlinski Whitmore Ortega,,,Male,Unknown,Unknown,Male,"On February 28, 2020, as covid-19 infections spread to more than fifty countries, the World Health Organization (WHO) increased the global risk level of covid-19 to “very high” (World Health Organization 2020). Public health officials needed to recommend how the public could protect themselves, balancing safety and urgency. But there was very little data since this novel virus had only been identified three months prior. How could public health officials decide with insufficient data? The multi-armed bandit problem of computer science offers adaptive decision-making procedures that can achieve both safety and urgency. These adaptive methods balance learning information (exploring) with using information (exploiting), adjusting the balance toward learning when uncertainty is high (March 1991; Kaelbling et al. 1996). Related methods are already used in adaptive clinical trials for pharmaceuticals (Pallmann et al. 2018). But we still need to develop these methods for non-pharmaceutical interventions, as I will illustrate with a case study of public mask-wearing to reduce the spread of covid-19. Public health officials recommended against face masks in February of 2020, which should have halted learning aside from randomized controlled trials. But rogue mask mandates in cities like Jena, Germany, enabled learning about the safety and efficacy of masks, leading officials to reverse course in support of face masks in June 2020. There was a randomized controlled trial of face masks, but it did not complete until September of 2021, fifteen months later. We need better methods of addressing public health emergencies than relying on rogue experiments like the one in Jena, Germany. Let me begin by reviewing public health’s historical tension between safety and urgency and then explain how adaptive methods help resolve this tension.",

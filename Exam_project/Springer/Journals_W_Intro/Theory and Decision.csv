Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
1.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132449,Editorial,October 1970,,,,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132450,Specification of objectives in decision problems,October 1970,Karl Borch,,,Male,Unknown,Unknown,Male,,
1.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132451,Critéres de choix en avenir incertain,October 1970,P. Malgrange,,,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132452,On subjective probability and related problems,October 1970,Günter Menges,,,Male,Unknown,Unknown,Male,,9
1.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132453,Cost-benefit versus expected utility acceptance rules,October 1970,Alex C. Michalos,,,Male,Unknown,Unknown,Male,,13
1.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132454,On the possibility of rational policy evaluation,October 1970,Thomas Schwartz,,,Male,Unknown,Unknown,Male,,57
1.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132455,Kant — A decision theorist?,October 1970,C. W. Churchman,,,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132456,Books received,October 1970,,,,Unknown,Unknown,Unknown,Unknown,,
1.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154002,"Science, reason and value",December 1970,Andrew McLaughlin,,,Male,Unknown,Unknown,Male,,14
1.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154003,The languages of science,December 1970,K. B. Madsen,,,Unknown,Unknown,Unknown,Unknown,,
1.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154004,A mathematical model of Churchmanian inquiring systems with special reference to Popper's measures for ‘The Severity of Tests’,December 1970,Ian I. Mitroff,Frederick Betz,Richard O. Mason,Male,Male,Male,Male,,5
1.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154005,On inference from inconsistent premisses,December 1970,Nicholas Rescher,Ruth Manor,,Male,Female,Unknown,Mix,,
1.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154006,Reviews,December 1970,J. E. White,Hubert Schleichert,Herbert Feigl,Unknown,Male,Male,Male,,
1.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139569,Rational choice and economic behavior,March 1971,Richard H. Day,,,Male,Unknown,Unknown,Male,,48
1.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139570,"The word ‘variable’ in logic, mathematics and economics",March 1971,Harald Dickson,,,Male,Unknown,Unknown,Male,,
1.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139571,Taxation and investment behaviour under uncertainty — A multiperiod portfolio analysis,March 1971,Kåre P. Hagen,,,Male,Unknown,Unknown,Male,,1
1.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139572,Applications of pseudo-Boolean methods to economic problems,March 1971,Peter L. Hammer,Eliezer Shlifer,,Male,Male,Unknown,Male,,6
1.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139573,"Uncertainty, the bargaining problem, and the Nash-Zeuthen solution",March 1971,Edward Saraydar,,,Male,Unknown,Unknown,Male,,2
1.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139574,Reviews,March 1971,Kevin C. Sontheimer,F. H. George,Willard F. Enteman,Male,Unknown,Male,Male,,
1.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140287,The extraordinary claim of praxeology,June 1971,Claudio Gutiérrez,,,Male,Unknown,Unknown,Male,,2
1.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140288,Constructs and empirical basis in theories of economic behavior,June 1971,W. Kroeber-Riel,,,Unknown,Unknown,Unknown,Unknown,,
1.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140289,A differential approach to the repeated prisoner's dilemma,June 1971,H. Edwin Overcast,Gordon Tullock,,Unknown,Male,Unknown,Male,,3
1.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140290,Remarks of a Polish praxiologist on the subject of a paper by C. Gutiérrez,June 1971,Jan Zieleniewski,,,Male,Unknown,Unknown,Male,,1
1.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140291,Ist eine neubegründung der verstehenden soziologie möglich?,June 1971,Gerald L. Eberlein,,,Male,Unknown,Unknown,Male,,2
1.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140292,On Scriven on ‘Verstehen’,June 1971,James W. Van Evra,,,Male,Unknown,Unknown,Male,,3
1.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140293,Verstehen again,June 1971,Michael Scriven,,,Male,Unknown,Unknown,Male,,2
1.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140294,Bemerkungen zum zweidimensionalen kontinuum induktiver methoden Von J. Hintikka,June 1971,E. Kronthaler,,,Unknown,Unknown,Unknown,Unknown,,
1.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140295,Inductive generalization and its problems: A comment on Kronthaler's comments,June 1971,Jaakko Hintikka,,,Male,Unknown,Unknown,Male,,1
1.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140296,Reviews,June 1971,Michael Ruse,Karl-Dieter Opp,H. W. Hetzler,Male,Unknown,Unknown,Male,,
2.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133135,Provability as a deontic notion,October 1971,Charles F. Kielkopf,,,Male,Unknown,Unknown,Male,,
2.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133136,A matrix method for deontic logic,October 1971,Edgar Morscher,,,Male,Unknown,Unknown,Male,,3
2.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133137,On a principle of contradiction in normative logic and jurisprudence,October 1971,Bernhard Schlink,,,Male,Unknown,Unknown,Male,,
2.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133138,The logic of cause,October 1971,Michael Scriven,,,Male,Unknown,Unknown,Male,,11
2.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133139,Deontic logic without deontic operators,October 1971,Hermann Vetter,,,Male,Unknown,Unknown,Male,,2
2.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133140,Kritik des normlogischen Schliessens,October 1971,Jürgen Rödig,,,Male,Unknown,Unknown,Male,,5
2.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133141,Reviews,October 1971,U. Kockelkorn,Reinhard Kamitz,K. Haag,Unknown,Male,Unknown,Male,,
2.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00148991,Counterintuitive behavior of social systems,December 1971,Jay W. Forrester,,,Male,Unknown,Unknown,Male,,153
2.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00148992,First step toward a computer model of human behaviour,December 1971,John H. King,,,Male,Unknown,Unknown,Male,,
2.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00148993,Systems and structures — toward bio-social anthropology,December 1971,Ervin Laszlo,,,Male,Unknown,Unknown,Male,,3
2.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00148994,The mastering of complexity as a problem of the Social Sciences,December 1971,Helmut Klages,Jürgen Nowak,,Male,Male,Unknown,Male,,5
2.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00148995,Reviews,December 1971,Willard F. Enteman,Herbert Stachowiak,Alex C. Michalos,Male,Male,Male,Male,,
2.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00137875,Singular causal explanations,March 1972,Raymond Martin,,,Male,Unknown,Unknown,Male,,1
2.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00137876,Prerequisites for quantification in the empirical sciences,March 1972,Ladislav Tondl,,,Male,Unknown,Unknown,Male,,1
2.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00137877,On the fugitive paradox of fugitive propositions,March 1972,Asa Kasher,,,Male,Unknown,Unknown,Male,,1
2.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00137878,Intensional semantics of vague constants,March 1972,Pavel Materna,,,Male,Unknown,Unknown,Male,,2
2.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00137879,The mythology of methodology,March 1972,Ian I. Mitroff,,,Male,Unknown,Unknown,Male,,5
2.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00137880,Reviews,March 1972,Scott A. Kleiner,Myles Brand,Hugh Lehman,Male,Male,Male,Male,,
2.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00137881,Notice,March 1972,,,,Unknown,Unknown,Unknown,Unknown,,
2.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160953,On having the opportunity,June 1972,Myles Brand,,,Male,Unknown,Unknown,Male,,5
2.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160954,An incompleteness problem in Harsanyi's general theory of games and certain related theories of non-cooperative games,June 1972,Edward F. McClennen,,,Male,Unknown,Unknown,Male,,2
2.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160955,Notes on the so-called incompleteness problem and on the proposed alternative concept of rational behavior,June 1972,John C. Harsanyi,,,Male,Unknown,Unknown,Male,,2
2.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160956,On the Onassis problem,June 1972,Helmut Laux,Hans Schneeweiss,,Male,Male,Unknown,Male,,13
2.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160957,Review,June 1972,Norman Stockman,Lothar Czayka,Lawrence L. Haworth,Male,Male,Male,Male,,
2.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160958,"Second International game theory workshop, Berkeley, 1970",June 1972,Heinz J. Skala,,,Male,Unknown,Unknown,Male,,
2.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160959,Call for papers,June 1972,,,,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139348,Editorial,October 1972,Werner Kroeber-Riel,,,Male,Unknown,Unknown,Male,,1
3.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139349,On some suggestions for having non-binary social choice functions,October 1972,Raveendran N. Batra,Prasanta K. Pattanaik,,Male,Female,Unknown,Mix,,
3.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139350,A graph-theoretical approach to the aggregation of individual preference orderings,October 1972,L. Czayka,H. Krauch,,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139351,Even-chance lotteries in social choice theory,October 1972,Peter C. Fishburn,,,Male,Unknown,Unknown,Male,,49
3.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139352,Metrizing social preference,October 1972,D. Gerber,,,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139353,"Liberalism, non-binary choice and Pareto principle",October 1972,V. S. Ramachandra,,,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139354,Democracy and interdependent preferences,October 1972,Frederic Schick,,,Male,Unknown,Unknown,Male,,10
3.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139355,Reviews,October 1972,Alex C. Michalos,M. E. Williams,R. N. Bronaugh,Male,Unknown,Unknown,Male,,
3.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139356,Hinweis auf eine tagung über rechtstheorie,October 1972,,,,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139357,Requisites for an economic system intended to satisfy the requirements of the collectivity,October 1972,B. De Finetti,,,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139358,Books received,October 1972,,,,Unknown,Unknown,Unknown,Unknown,,
3.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141050,"Conditional expected, extensive utility",December 1972,R. Duncan Luce,,,Unknown,Unknown,Unknown,Unknown,,
3.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141051,Decision-theoretic aspects of risk-taking behaviour,December 1972,Rainer Pötzsch,,,Male,Unknown,Unknown,Male,,
3.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141052,What if utility functions do not exist?,December 1972,Fred S. Roberts,,,Male,Unknown,Unknown,Male,,17
3.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141053,The logic of preference reconsidered,December 1972,G. H. Von Wright,,,Unknown,Unknown,Unknown,Unknown,,
3.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141054,Rescher's determination of a social preference ranking,December 1972,Mårten Ringbom,,,Male,Unknown,Unknown,Male,,1
3.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141055,Some observations on social consensus methodology,December 1972,Nicholas Rescher,,,Male,Unknown,Unknown,Male,,2
3.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141056,Reviews,December 1972,Sidney J. Herzig,Sidney J. Herzig,Sighard Roloff,,,Male,Mix,,
3.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141057,Conference for Formal Methods in the Methodology of Empirical Sciences,December 1972,,,,Unknown,Unknown,Unknown,Unknown,,
3.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141058,Books received,December 1972,,,,Unknown,Unknown,Unknown,Unknown,,
3.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139503,The role of forecast in planning,March 1973,Mario Bunge,,,Male,Unknown,Unknown,Male,,2
3.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139504,Problems of points of inflection in trend functions as described by a model for the forecast of brand shares,March 1973,Werner Kroeber-Riel,Sighard Roloff,,Male,Male,Unknown,Male,,
3.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139505,Invariant structures of prognosis curves in Forrester's World Dynamics,March 1973,Helmut Maier,Werner Hugger,,Male,Male,Unknown,Male,,3
3.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139506,Epidemical spread of scientific objects: An attempt of empirical approach to some problems of meta-science,March 1973,Maria Nowakowska,,,Female,Unknown,Unknown,Female,,9
3.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139507,Comments on Dr. Vetter's paper ‘deontic logic without deontic operators’,March 1973,Herbert Keuth,,,Male,Unknown,Unknown,Male,,
3.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139508,Bemerkungen Zu J. Rödig's ‘Kritik des normlogischen schliessens’,March 1973,Ota Weinberger,,,,Unknown,Unknown,Mix,,
3.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139509,Reviews,March 1973,Ekkart Zimmermann,K. B. Madsen,,Male,Unknown,Unknown,Male,,
3.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139510,Erratum,March 1973,,,,Unknown,Unknown,Unknown,Unknown,,
3.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00138191,A decision theoretic model of the American war in Vietnam,June 1973,Mario Bunge,,,Male,Unknown,Unknown,Male,,3
3.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00138192,A proof of Hume's separation thesis based on a formal system for descriptive and normative statements,June 1973,Arnold A. Johanson,,,Male,Unknown,Unknown,Male,,2
3.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00138193,On misunderstanding ‘understanding’,June 1973,Thomas McCarthy,,,Male,Unknown,Unknown,Male,,11
3.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00138194,A note on Marshallian and von Neumann-Morgenstern utility,June 1973,W. R. Hughes,,,Unknown,Unknown,Unknown,Unknown,,
3.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00138195,A comment on ‘the extraordinary claim of praxeology’ by Professor Gutiérrez,June 1973,Walter Block,,,Male,Unknown,Unknown,Male,,16
3.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00138196,Reviews,June 1973,Alex C. Michalos,Viktor Vanberg,Karl-Dieter Opp,Male,Male,Unknown,Male,,
3.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00138197,Books received,June 1973,,,,Unknown,Unknown,Unknown,Unknown,,
4.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133396,Positionalist voting functions,September 1973,Peter Gärdenfors,,,Male,Unknown,Unknown,Male,,79
4.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133397,The independence condition in the theory of social choice,September 1973,Bengt Hansson,,,Male,Unknown,Unknown,Male,,68
4.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133398,Process and paradox: The significance of Arrow's theorem,September 1973,Edward I. Friedland,Stephen J. Cimbala,,Male,Male,Unknown,Male,,9
4.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133399,Preference aggregation and statistical estimation,September 1973,Jean-Marie Blin,,,Male,Unknown,Unknown,Male,,7
4.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133400,The possibility of rational policy evaluation,September 1973,Ilmar Waldner,,,Male,Unknown,Unknown,Male,,1
4.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133401,Reviews,September 1973,Werner Leinfellner,Michael Martin,Wolf-Dieter Eberwein,Male,Male,Unknown,Male,,
4.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00145149,The limitations of the factor-analytical approach to psychology with special application to Cattell's research strategy,November 1973,Maria Nowakowska,,,Female,Unknown,Unknown,Female,,5
4.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00145150,Memory and cognition: An information processing model of man,November 1973,Kenneth Deffenbacher,Evan Brown,,Male,Male,Unknown,Male,,4
4.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00145151,Probability and logic in belief systems,November 1973,Bernard Grofman,Gerald Hyman,,Male,Male,Unknown,Male,,11
4.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00145152,Reviews,November 1973,B. Kanitscheider,Helga Nowotny,,Unknown,Female,Unknown,Female,,
4.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136647,Punishment as retribution,February 1974,Donald Wittman,,,Male,Unknown,Unknown,Male,,28
4.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136648,Utilities for distributive justice,February 1974,Geoffrey Ross,,,Male,Unknown,Unknown,Male,,4
4.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136649,"Altruism, the free rider problem and group size",February 1974,John W. Sweeney Jr.,,,Male,Unknown,Unknown,Male,,8
4.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136650,The ranking assumption,February 1974,Robinson A. Grover,,,Unknown,Unknown,Unknown,Unknown,,
4.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136651,Rawls versus Bentham: An axiomatic examination of the pure distribution problem,February 1974,Amartya Sen,,,Unknown,Unknown,Unknown,Unknown,,
4.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136652,A theory of justice?,February 1974,Philip Pettit,,,Male,Unknown,Unknown,Male,,8
4.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136653,On some problems arising from Professor Rawls' conception of distributive justice,February 1974,Partha Dasgupta,,,Unknown,Unknown,Unknown,Unknown,,
4.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136654,The utilitarian contract: A generalization of Rawls' theory of justice,February 1974,Dennis C. Mueller,Robert D. Tollison,Thomas D. Willett,Male,Male,Male,Male,,7
4.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136655,A note on deterministic and algorithmic behavior,February 1974,Pavel Materna,,,Male,Unknown,Unknown,Male,,
4.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136656,Reviews,February 1974,Ota Weinberger,Philip Pettit,Scott A. Kleiner,,Male,Male,Mix,,
5.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140297,Limited role of entropy in information economics,June 1974,Jacob Marschak,,,Male,Unknown,Unknown,Male,,3
5.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140298,Two concepts of information,June 1974,Klemens Szaniawski,,,Male,Unknown,Unknown,Male,,9
5.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140299,The validity of subjective information as a formal concept applied to empirical analysis,June 1974,Klaus Anderseck,Günter Hesse,,Male,Male,Unknown,Male,,1
5.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140300,A Brunswik Lens Model of Dialectical Inquiring Systems,June 1974,Ian I. Mitroff,,,Male,Unknown,Unknown,Male,,20
5.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140301,Subjective qualitative information structures based on orderings,June 1974,Hans-Werner Göttinger,,,Unknown,Unknown,Unknown,Unknown,,
5.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140302,Reviews,June 1974,Reinhard Kamitz,Veit Pittioni,Peter Posch,Male,Male,Male,Male,,
5.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140303,Books received,June 1974,,,,Unknown,Unknown,Unknown,Unknown,,
5.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00143244,Convex stochastic dominance with finite consequence sets,August 1974,Peter C. Fishburn,,,Male,Unknown,Unknown,Male,,9
5.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00143245,"The choice axiom, revealed preference, and the theory of demand",August 1974,Carl Halldin,H. S. Houthakker,,Male,Unknown,Unknown,Male,,12
5.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00143246,An axiomatic system for multidimensional preferences,August 1974,Oswald Huber,,,Male,Unknown,Unknown,Male,,7
5.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00143247,Axiomatic analysis of non-transitivity of preference and of indifference,August 1974,Raymond H. Burros,,,Male,Unknown,Unknown,Male,,13
5.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00143248,On utility functions,August 1974,Georges Bernard,,,Male,Unknown,Unknown,Male,,31
5.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00143249,Reviews,August 1974,Mario Bunge,Radu J. Bogdan,,Male,Male,Unknown,Male,,
5.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00164717,On a concept of representative democracy,October 1974,Manimay Sengupta,,,Unknown,Unknown,Unknown,Unknown,,
5.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00164718,Intergenerational justice and the social discount rate,October 1974,Dennis C. Mueller,,,Male,Unknown,Unknown,Male,,4
5.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00164719,Decision analysis model: An extension of the states of nature concept,October 1974,Marvin Berhold,,,Male,Unknown,Unknown,Male,,
5.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00164720,Party strategy and the relationship between votes and seats,October 1974,David Sankoff,,,Male,Unknown,Unknown,Male,,1
5.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00164721,Avoiding the voter's paradox democratically,October 1974,Michael Davis,,,Male,Unknown,Unknown,Male,,1
5.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00164722,Bare preference and interpersonal utility comparisons,October 1974,Ilmar Waldner,,,Male,Unknown,Unknown,Male,,4
5.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00164723,Difficulties with bare preferences,October 1974,Gordon Becker,,,Male,Unknown,Unknown,Male,,1
5.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00164724,Value neutrality,October 1974,Ilmar Waldner,,,Male,Unknown,Unknown,Male,,
5.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00164725,"Utopia, as a necessary presupposition for every significant foundation of economics",October 1974,Bruno de Finetti,,,Male,Unknown,Unknown,Male,,2
5.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00164726,Report: Pareto optimum in a different approach to economics,October 1974,Bruno de Finetti,,,Male,Unknown,Unknown,Male,,3
5.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00164727,Reviews,October 1974,Pavel Apostol,,,Male,Unknown,Unknown,Male,,
5.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00164728,Reviews,October 1974,Hans W. Gottinger,Wolf-Dieter Eberwein,Radu J. Bogdan,Male,Unknown,Male,Male,,1
5.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00167589,Kuhn's impossibility proof and the moral element in scientific explanations,December 1974,Tibor R. Machan,,,Male,Unknown,Unknown,Male,,4
5.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00167590,Preferred allocations with uncertain implementation,December 1974,Frank H. Trinkl,,,Male,Unknown,Unknown,Male,,
5.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00167591,"Objective knowledge out of ignorance: Popper on body, mind, and the third world",December 1974,Herbert Keuth,,,Male,Unknown,Unknown,Male,,2
5.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00167592,Cultural evolution,December 1974,Michael Ruse,,,Male,Unknown,Unknown,Male,,2
5.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00167593,Systems theory and objectivity,December 1974,Morton A. Kaplan,,,Male,Unknown,Unknown,Male,,1
5.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00167594,Reviews,December 1974,Ekkart Zimmermann,Maria Nowakowska,,Male,Female,Unknown,Mix,,
5.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00167595,Reviews,December 1974,Alex C. Michalos,Karel Berka,,Male,Male,Unknown,Male,,
6.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139816,Editorial,February 1975,,,,Unknown,Unknown,Unknown,Unknown,,
6.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139817,The nomological character of microeconomics,February 1975,Alexander Rosenberg,,,Male,Unknown,Unknown,Male,,2
6.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139818,Charles Taylor on purpose and causation,February 1975,George Sher,,,Male,Unknown,Unknown,Male,,1
6.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139819,A formulation of the determinism hypothesis,February 1975,Pavel Materna,,,Male,Unknown,Unknown,Male,,1
6.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139820,Scriven on The Logic of Cause,February 1975,John A. Barker,,,Male,Unknown,Unknown,Male,,1
6.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139821,The diagnostic process as a statistical-causal analysis,February 1975,Hans Westmeyer,,,Male,Unknown,Unknown,Male,,13
6.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139822,On the historical explanation of unique events,February 1975,James H. Fetzer,,,Male,Unknown,Unknown,Male,,7
6.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139823,Verstehen I and Verstehen II,February 1975,Theodore Abel,,,Male,Unknown,Unknown,Male,,7
6.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139824,Reviews,February 1975,Hermann Vetter,Radu J. Bogdan,Mario Bunge,Male,Male,Male,Male,,
6.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00169102,Personal probabilities of probabilities,May 1975,Jacob Marschak,Morris H. Degroot,Robert L. Winkler,Male,Male,Male,Male,,46
6.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00169103,Probabilities of probabilities,May 1975,Karl Borch,,,Male,Unknown,Unknown,Male,,7
6.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00169104,Newcomb's many problems,May 1975,Isaac Levi,,,Male,Unknown,Unknown,Male,,42
6.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00169105,Complementary properties of binary relations,May 1975,Raymond H. Burros,,,Male,Unknown,Unknown,Male,,2
6.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00169106,Preference and choice,May 1975,Aleksandar Kron,Veselin Milovanović,,Male,Male,Unknown,Male,,3
6.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00169107,Some criticism of stochastic models generally used in decision making experiments,May 1975,Dirk Wendt,,,Male,Unknown,Unknown,Male,,5
6.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00169108,Statistical quantifiers in observational calculi: An application in GUHA-methods,May 1975,Tomáš Havránek,,,Male,Unknown,Unknown,Male,,21
6.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00169109,Aggregation of individual preferences by voting,May 1975,Walburga Rödding,,,Female,Unknown,Unknown,Female,,3
6.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00169110,Reviews,May 1975,Karel Berka,,,Male,Unknown,Unknown,Male,,
6.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00169111,Reviews,May 1975,H. L. Berghel,Hermann Kotthoff,Mario Bunge,Unknown,Male,Male,Male,,
6.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136197,"Behaviorism, finite automata, and stimulus response theory",August 1975,R. J. Nelson,,,Unknown,Unknown,Unknown,Unknown,,
6.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136198,From behaviorism to neobehaviorism,August 1975,Patrick Suppes,,,Male,Unknown,Unknown,Male,,33
6.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136199,A theory of subjective expected utility with vague preferences,August 1975,Peter C. Fishburn,,,Male,Unknown,Unknown,Male,,11
6.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136200,Nonlinear social welfare functions,August 1975,John C. Harsanyi,,,Male,Unknown,Unknown,Male,,81
6.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136201,Epistemological problems and the personality and social system in social psychology,August 1975,Hans Lenk,Günther Lüschen,,Male,Male,Unknown,Male,,1
6.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136202,Introspective utility and the group choice problem,August 1975,Neil R. Paine,,,Male,Unknown,Unknown,Male,,1
6.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136203,The limits of uncertainty: A note,August 1975,Ernest R. Alexander,,,Male,Unknown,Unknown,Male,,5
6.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136204,Reviews,August 1975,George E. Yoos,Reinhard Kamitz,,Male,Male,Unknown,Male,,
6.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136205,Books received,August 1975,,,,Unknown,Unknown,Unknown,Unknown,,
6.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139422,"The three basic paradigms of macrosociology: Functionalism, neo-Marxism and interaction analysis",November 1975,Raymond Boudon,,,Male,Unknown,Unknown,Male,,2
6.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139423,Model for the distribution process of attitudes in a two-party-system,November 1975,Josef Zelger,,,Male,Unknown,Unknown,Male,,
6.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139424,Social choice in a sequential environment,November 1975,A. Camacho,,,Unknown,Unknown,Unknown,Unknown,,
6.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139425,Some criteria for making decisions concerning the distribution of scarce medical resources,November 1975,Robert Young,,,Male,Unknown,Unknown,Male,,5
6.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139426,Knots and blanks: The pragmatic foundation of logical principles,November 1975,Claudio Gutiérrez,,,Male,Unknown,Unknown,Male,,
6.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139427,"Justice, utility, and interpersonal comparisons",November 1975,Edward F. Becker,,,Male,Unknown,Unknown,Male,,3
6.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139428,Avoiding the voter's paradox democratically: Comment,November 1975,Gordon Tullock,,,Male,Unknown,Unknown,Male,,
6.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139429,Reviews,November 1975,E. Leinfellner,Edward D. Booth,,Unknown,Male,Unknown,Male,,
7.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141100,"Some formal problems with the Von Neumann and Morgenstern theory of two-person, zero-sum games, I: The direct proof",February 1976,Edward F. McClennen,,,Male,Unknown,Unknown,Male,,5
7.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141101,Power and size: A new paradox,February 1976,Steven J. Brams,Paul J. Affuso,,Male,Male,Unknown,Male,,110
7.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141102,On the inadequacy of the regression paradigm used in the study of human judgment,February 1976,Milan Zeleny,,,Male,Unknown,Unknown,Male,,7
7.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141103,Simulation methodology,February 1976,G. Arthur Mihram,,,Unknown,Unknown,Unknown,Unknown,,
7.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141104,How relevant are ‘Irrelevant’ Alternatives?,February 1976,Jean-Marie Blin,,,Male,Unknown,Unknown,Male,,6
7.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141105,Advantages and possibilities of application of linear logistic test models in the area of social sciences,February 1976,Schildkamp-Kündiger,,,Unknown,Unknown,Unknown,Unknown,,
7.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141106,On instructional science and instructional technology,February 1976,Knud Aagaard,,,Male,Unknown,Unknown,Male,,
7.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141107,A note on publication and the value of significance tests,February 1976,M. Bloxham,,,Unknown,Unknown,Unknown,Unknown,,
7.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141108,A note concerning the Epidemical Spread of Scientific Objects,February 1976,Maria Nowakowska,,,Female,Unknown,Unknown,Female,,
8.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133084,Preface,January 1977,Anatol Rapoport,,,Male,Unknown,Unknown,Male,,
8.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133085,Domination structures and multicriteria problems in n-person games,January 1977,K. Bergstresser,P. L. Yu,,Unknown,Unknown,Unknown,Unknown,,
8.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133086,"A test of the Core, Bargaining Set, Kernel and Shapley models in n-person quota games with one weak player",January 1977,Abraham D. Horowitz,,,Male,Unknown,Unknown,Male,,10
8.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133087,Experiments with cooperative 2 × 2 games,January 1977,Anatol Rapoport,Oded Frenkel,Josef Perner,Male,Male,Male,Male,,7
8.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133088,Books received (addenda),January 1977,,,,Unknown,Unknown,Unknown,Unknown,,
8.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133089,Announcement,January 1977,,,,Unknown,Unknown,Unknown,Unknown,,
8.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133407,When you don't need to join: The effects of guaranteed payoffs on bargaining in three-person cooperative games,April 1977,James P. Kahan,Amnon Rapoport,,Male,Male,Unknown,Male,,19
8.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133408,Game-theoretic analyses of coalition behavior,April 1977,Kenneth E. Friend,James D. Laing,Richard J. Morrison,Male,Male,Male,Male,,12
8.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133409,Dynamics of coalition formation: Prescription vs. reality,April 1977,R. Gordon Cassidy,Edwin H. Neave,,Unknown,Male,Unknown,Male,,1
8.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133410,Sequential strategies in dual control problems,April 1977,Richard M. Cyert,Morris H. Degroot,,Male,Male,Unknown,Male,,3
8.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133411,Axioms for Shapley values in games with quarrelling,April 1977,D. Marc Kilgour,,,Unknown,Unknown,Unknown,Unknown,,
8.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133442,An analysis of simple counting methods for ordering incomplete ordinal data,July 1977,William V. Gehrlein,Peter C. Fishburn,,Male,Male,Unknown,Male,,3
8.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133443,The structure of random utility models,July 1977,Charles F. Manski,,,Male,Unknown,Unknown,Male,,969
8.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133444,A proposed solution to the voters preference aggregation problem,July 1977,Dennis J. Packard,,,Male,Unknown,Unknown,Male,,3
8.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133445,The Jamaican fishing study reinterpreted,July 1977,Michael Walker,,,Male,Unknown,Unknown,Male,,2
8.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133446,On the concept of information synonymy,July 1977,Ladislav Tondl,,,Male,Unknown,Unknown,Male,,
8.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133447,Nozick and the opposed preferences theory of exchange,July 1977,,,,Unknown,Unknown,Unknown,Unknown,,
8.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133448,Information and strategy in iterated prisoner's dilemma,July 1977,,,,Unknown,Unknown,Unknown,Unknown,,
8.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133449,Complementary lemmas in the theory of binary relations,July 1977,Raymond H. Burros,,,Male,Unknown,Unknown,Male,,
8.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133450,Reviews,July 1977,,,,Unknown,Unknown,Unknown,Unknown,,
8.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133451,Announcement,July 1977,,,,Unknown,Unknown,Unknown,Unknown,,
8.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141530,Pre-testing innovation: Methodology for testing the design of management systems,October 1977,R. V. Brown,S. R. Watson,,Unknown,Unknown,Unknown,Unknown,,
8.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141531,Relative acceptability: A proposed strategy for the bargaining game,October 1977,Robert Lyle Butterworth,,,Male,Unknown,Unknown,Male,,
8.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141532,Majority rule and general decision rules,October 1977,Philip D. Straffin Jr.,,,Male,Unknown,Unknown,Male,,19
8.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141533,"Kuhn, paradigm choice and the arbitrariness of aesthetic criteria in science",October 1977,Tibor R. Machan,,,Male,Unknown,Unknown,Male,,2
8.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141534,Further generalizations of the cake-eating problem under uncertainty,October 1977,Murray C. Kemp,,,Male,Unknown,Unknown,Male,,11
8.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141535,A new concept of verisimilitude,October 1977,Hermann Vetter,,,Male,Unknown,Unknown,Male,,4
8.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141536,Young on decisions concerning medical aid,October 1977,Bruce Langtry,,,Male,Unknown,Unknown,Male,,
8.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141537,A note on Rawls' ‘decision-theoretic’ argument for the difference principle,October 1977,Nollaig Mackenzie,,,Female,Unknown,Unknown,Female,,
8.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141538,Nozick on Sen: A misunderstanding,October 1977,C. R. Perelli-Minetti,,,Unknown,Unknown,Unknown,Unknown,,
8.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141539,Reviews,October 1977,Heinz Skala,Philip Pettit,John Ferejohn,Male,Male,Male,Male,,
8.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141540,Announcement,October 1977,,,,Unknown,Unknown,Unknown,Unknown,,
8.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141541,Editorial advisors,October 1977,,,,Unknown,Unknown,Unknown,Unknown,,
9.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00138117,"Coherence, regularity and conditional probability",January 1978,Isaac Levi,,,Male,Unknown,Unknown,Male,,7
9.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00138118,Varieties of commitment,January 1978,Hans Lenk,,,Male,Unknown,Unknown,Male,,3
9.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00138119,A combined approach to the dynamics of theories,January 1978,Wolfgang Stegmüller,,,Male,Unknown,Unknown,Male,,17
9.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00138120,Limitations of methodological experiments,January 1978,Willem K. B. Hofstee,,,Male,Unknown,Unknown,Male,,
9.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00138121,On the limits of the statistical-causal analysis as a diagnostic procedure,January 1978,Kazem Sadegh-Zadeh,,,Male,Unknown,Unknown,Male,,7
9.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00138122,What can the regression model of human judgment learn from multi-attribute decision theory?,January 1978,John Fountain,,,Male,Unknown,Unknown,Male,,1
9.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00138123,Reviews,January 1978,Nicholas J. Moutafakis,S. James Hintze,Eckehart Köhler,Male,Unknown,Male,Male,,
9.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00131770,The chain store paradox,April 1978,Reinhard Selten,,,Male,Unknown,Unknown,Male,,549
9.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00131771,Mixture axioms in linear and multilinear utility theories,April 1978,Peter C. Fishburn,Fred S. Roberts,,Male,Male,Unknown,Male,,8
9.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00131772,Methodology of empirical goal research — On its way into a blind alley?,April 1978,Jürgen Hauschildt,Winfried Hamel,,Male,Male,Unknown,Male,,
9.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00131773,Lexicographic tradeoff structures,April 1978,R. Duncan Luce,,,Unknown,Unknown,Unknown,Unknown,,
9.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00131774,Optimal choice of reward levels in an organization,April 1978,Luigi M. Tomasini,,,Male,Unknown,Unknown,Male,,
9.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00131775,Note on two applications of the CEVR utility function,April 1978,Georges Bernard,,,Male,Unknown,Unknown,Male,,3
9.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00131776,Reviews,April 1978,Henry E. Kyburg Jr.,Zeno Vendler,H. L. Berghel,Male,Male,Unknown,Male,,1
9.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133452,Qualitative independence in probability theory,July 1978,R. Duncan Luce,Louis Narens,,Unknown,Male,Unknown,Male,,7
9.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133453,Consumption and saving: Models and reality,July 1978,Karl Borch,,,Male,Unknown,Unknown,Male,,
9.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133454,Rationality from a computational standpoint,July 1978,Donald E. Campbell,,,Male,Unknown,Unknown,Male,,5
9.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133455,Expected utility with ambiguous probabilities and ‘irrational’ parameters,July 1978,Günter Franke,,,Male,Unknown,Unknown,Male,,15
9.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133456,The marxian theories of value and exploitation axiomatised,July 1978,Peter Gibbins,,,Male,Unknown,Unknown,Male,,2
9.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133457,On arbitration schemes for a wealth distribution problem,July 1978,Edwin H. Neave,,,Male,Unknown,Unknown,Male,,
9.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133458,Reviews,July 1978,Hollis Martin,,,,Unknown,Unknown,Mix,,
9.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126471,Multiattribute utility theory: A survey,October 1978,Mustafa R. Yilmaz,,,Male,Unknown,Unknown,Male,,6
9.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126472,A critical discussion of the work of John C. Harsanyi,October 1978,Horace W. Brock,,,Male,Unknown,Unknown,Male,,1
9.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126473,Three books on rawls,October 1978,Mary Gibson,,,,Unknown,Unknown,Mix,,
9.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126474,Books received,October 1978,,,,Unknown,Unknown,Unknown,Unknown,,
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126328,Preface,January 1979,,,,Unknown,Unknown,Unknown,Unknown,,
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126329,A systems concept of society: Beyond individualism and holism,January 1979,Mario Bunge,,,Male,Unknown,Unknown,Male,,36
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126330,Towards a system-theoretical decision logic,January 1979,John W. Sutherland,King G. Yee,,Male,Male,Unknown,Male,,1
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126331,Reasonable beliefs,January 1979,Wlodzimierz Rabinowicz,,,Unknown,Unknown,Unknown,Unknown,,
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126332,The methodological status of Popper's rationality principle,January 1979,Noretta Koertge,,,Unknown,Unknown,Unknown,Unknown,,
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126333,On the foundations of mean-variance analyses,January 1979,Peter C. Fishburn,,,Male,Unknown,Unknown,Male,,9
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126334,Rationality of indecisive choice functions on triadic choice domains,January 1979,Tony E. Smith,,,Male,Unknown,Unknown,Male,,3
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126335,On cardinal utility,January 1979,A. Camacho,,,Unknown,Unknown,Unknown,Unknown,,
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126336,Nontransitive multidimensional preferences: Theoretical analysis of a model,January 1979,Oswald Huber,,,Male,Unknown,Unknown,Male,,28
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126337,Economics vs. moral philosophy,January 1979,,,,Unknown,Unknown,Unknown,Unknown,,
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126338,"Social decision making in the presence of complex goals, ethics and the environment",January 1979,,,,Unknown,Unknown,Unknown,Unknown,,
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126339,The physical and the social foundations of technology,January 1979,Friedrich Rapp,,,Male,Unknown,Unknown,Male,,1
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126340,Social evolution: Learning theory applied to group action,January 1979,Karl-Dieter Opp,,,Unknown,Unknown,Unknown,Unknown,,
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126341,Manipulation of social choice rules by strategic nomination of candidates,January 1979,Donald E. Campbell,,,Male,Unknown,Unknown,Male,,5
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126342,The role of ambiguity in manipulating voter behavior,January 1979,Raymond Dacey,,,Male,Unknown,Unknown,Male,,5
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126343,Individual rationality and the concept of social welfare,January 1979,Elisha A. Pazner,,,Male,Unknown,Unknown,Male,,1
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126344,A condition for Nash-stability under binary and democratic group decision functions,January 1979,Manimay Sengupta,Bhaskar Dutta,,Unknown,Male,Unknown,Male,,9
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126345,Extended sympathy comparisons and the basis of social choice,January 1979,,,,Unknown,Unknown,Unknown,Unknown,,
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126346,Galbraith's theory of the mature corporation,January 1979,Thomas Iwand,Henry Thomassen,,Male,Male,Unknown,Male,,
10.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126347,Skinner on the prediction and control of behavior,January 1979,Hugh M. Lacey,,,Male,Unknown,Unknown,Male,,3
11.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126688,Introduction,March 1979,H. L. Berghel,,,Unknown,Unknown,Unknown,Unknown,,
11.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126689,"Proper names, essences and intuitive beliefs",March 1979,Diana Ackerman,,,Female,Unknown,Unknown,Female,,3
11.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126690,Quotation,March 1979,Donald Davidson,,,Male,Unknown,Unknown,Male,,88
11.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126691,If and modus ponens,March 1979,Gilbert Harman,,,Male,Unknown,Unknown,Male,,3
11.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126692,The Lessons of the Liar,March 1979,Philip Hugly,Charles Sayward,,Male,Male,Unknown,Male,,
11.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126693,A note on the breadth and depth of terms,March 1979,Asa Kasher,Ruth Manor,,Male,Female,Unknown,Mix,,
11.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126694,Believing and necessity,March 1979,Leonard Linsky,,,Male,Unknown,Unknown,Male,,2
11.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126695,Referring to nonexistent objects,March 1979,Terence Parsons,,,Male,Unknown,Unknown,Male,,12
11.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126696,Translation theories and the decipherment of linear B,March 1979,John Wallace,,,Male,Unknown,Unknown,Male,,
11.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126697,Announcements,March 1979,,,,Unknown,Unknown,Unknown,Unknown,,
11.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134357,Foreword,June 1979,Horace W. Brock,,,Male,Unknown,Unknown,Male,,1
11.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134358,Don't shoot the trumpeter - he's doing his best!,June 1979,Brian Barry,,,Male,Unknown,Unknown,Male,,4
11.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134359,Welfare judgments and future generations,June 1979,Thomas Schwartz,,,Male,Unknown,Unknown,Male,,6
11.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134360,Moral structures and axiomatic theory,June 1979,Steven Strasnick,,,Male,Unknown,Unknown,Male,,4
11.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134361,A diagrammatic exposition of justice,June 1979,Donald Wittman,,,Male,Unknown,Unknown,Male,,4
11.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126380,A game theoretic account of social justice,September 1979,Horace W. Brock,,,Male,Unknown,Unknown,Male,,14
11.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126381,Disparate goods and Rawls' difference principle: A social choice theoretic treatment,September 1979,Allan Gibbard,,,Male,Unknown,Unknown,Male,,31
11.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126382,"Bayesian decision theory, rule utilitarianism, and Arrow's impossibility theorem",September 1979,John C. Harsanyi,,,Male,Unknown,Unknown,Male,,33
11.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126383,Decision-making under ignorance with implications for social choice,September 1979,Eric Maskin,,,Male,Unknown,Unknown,Male,,58
11.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139446,"An axiomatic derivation of subjective probability, utility, and evaluation functions",December 1979,Roger B. Myerson,,,Male,Unknown,Unknown,Male,,16
11.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139447,The binary search decomposition in a decentralized organization,December 1979,Sang M. Lee,B. H. Rho,,,Unknown,Unknown,Mix,,
11.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139448,Epistemic considerations in game theory,December 1979,Krister Segerberg,,,Male,Unknown,Unknown,Male,,
11.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139449,Ordered sum and tensor product of linear utility structures,December 1979,Zoltan Domotor,,,Unknown,Unknown,Unknown,Unknown,,
11.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139450,A note on manipulability of large voting schemes,December 1979,Bezalel Peleg,,,Unknown,Unknown,Unknown,Unknown,,
11.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139451,Why I am not an objective Bayesian; some reflections prompted by Rosenkrantz,December 1979,Teddy Seidenfeld,,,Male,Unknown,Unknown,Male,,71
11.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139452,Bayesian theory appraisal: A reply to Seidenfeld,December 1979,R. D. Rosenkrantz,,,Unknown,Unknown,Unknown,Unknown,,
11.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139453,A note on Abraham Lincoln in probabilityland,December 1979,Bernard Grofman,,,Male,Unknown,Unknown,Male,,
11.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139454,Preference anarchy,December 1979,Alfred Mackay,Edward Wong,,Male,Male,Unknown,Male,,1
11.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139455,Review,December 1979,Mark C. Timmons,Ross Harrison,Mustafa R. Yilmaz,Male,Male,Male,Male,,
12.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154655,Applying the Jeffrey decision model to rational betting and information acquisition,March 1980,Ernest W. Adams,Roger D. Rosenkrantz,,Male,Male,Unknown,Male,,16
12.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154656,Anonymity conditions in social choice theory,March 1980,Donald E. Campbell,Peter C. Fishburn,,Male,Male,Unknown,Male,,7
12.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154657,"Deterrence, utility, and rational choice",March 1980,Gregory S. Kavka,,,Male,Unknown,Unknown,Male,,14
12.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154658,Impartiality and equity,March 1980,R. Harrison Wagner,,,Unknown,Unknown,Unknown,Unknown,,
12.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154659,Personal probabilities of probabilities in the case of sampling without replacement,March 1980,A. I. Dale,,,Unknown,Unknown,Unknown,Unknown,,
12.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154660,A skeptical history of microeconomic theory,March 1980,Alexander Rosenberg,,,Male,Unknown,Unknown,Male,,10
12.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154661,Review,March 1980,Brian Barry,Bernard Grofman,,Male,Male,Unknown,Male,,3
12.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154357,"Rule utilitarianism, rights, obligations and the theory of rational behavior",June 1980,John C. Harsanyi,,,Male,Unknown,Unknown,Male,,105
12.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154358,An extension of the Nash bargaining problem and the Nash social welfare function,June 1980,Mamoru Kaneko,,,Male,Unknown,Unknown,Male,,66
12.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154359,Acts and conditional probabilities,June 1980,Henry E. Kyburg,,,Male,Unknown,Unknown,Male,,33
12.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154360,A model to support the assessment of subjective probabilities,June 1980,Klaus-P. Schütt,,,Unknown,Unknown,Unknown,Unknown,,
12.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154361,A conditional expected utility model for myopic decision makers,June 1980,Leigh Tesfatsion,,,,Unknown,Unknown,Mix,,
12.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154362,Review,June 1980,Philip Pettit,,,Male,Unknown,Unknown,Male,,
12.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00154363,Announcement,June 1980,,,,Unknown,Unknown,Unknown,Unknown,,
12.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135033,Several possible measures of risk,September 1980,R. Duncan Luce,,,Unknown,Unknown,Unknown,Unknown,,
12.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135034,Rational choice and public affairs,September 1980,Tibor R. Machan,,,Male,Unknown,Unknown,Male,,4
12.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135035,"A competitive test of the descriptive accuracy of the characteristic function, power function, and shapley value based function",September 1980,Melvin M. Sakurai,,,Male,Unknown,Unknown,Male,,4
12.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135036,Economics vs. moral philosophy: Comment,September 1980,Thomas Achatz,Franz Haslinger,,Male,Male,Unknown,Male,,
12.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135037,Review,September 1980,Paul Lyon,Brian Skyrms,,Male,Male,Unknown,Male,,
12.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00131314,Matching the organization's structure and its cooperative market relations,December 1980,Helmy H. Baligh,Richard M. Burton,,Female,Male,Unknown,Mix,,
12.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00131315,"Manipulation-proofness: A concept for widening the scope of Arrowian welfare economics, both practically and intellectually",December 1980,Peter Brand,,,Male,Unknown,Unknown,Male,,2
12.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00131316,Approaches to cardinal utility,December 1980,A. Camacho,,,Unknown,Unknown,Unknown,Unknown,,
12.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00131317,Advances in multiattribute utility theory,December 1980,Peter H. Farquhar,,,Male,Unknown,Unknown,Male,,11
12.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00131318,Inequality and social choice,December 1980,Peter J. Lambert,,,Male,Unknown,Unknown,Male,,1
12.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00131319,Review,December 1980,Christopher Hookway,,,Male,Unknown,Unknown,Male,,
12.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00131320,Books received,December 1980,,,,Unknown,Unknown,Unknown,Unknown,,
13.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF02342603,A general theory of ratio scalability with remarks about the measurement-theoretic concept of meaningfulness,March 1981,Louis Narens,,,Male,Unknown,Unknown,Male,,76
13.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF02342604,Harsanyi’s critical rule utilitarianism,March 1981,Richard J. Stefanik,,,Male,Unknown,Unknown,Male,,
13.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF02342605,Double deception: Two against one in three-person games,March 1981,Steven J. Brams,Frank C. Zagare,,Male,Male,Unknown,Male,,10
13.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF02342606,Announcement,March 1981,,,,Unknown,Unknown,Unknown,Unknown,,
13.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134212,Some strategic properties of plurality and majority voting,June 1981,Donald E. Campbell,,,Male,Unknown,Unknown,Male,,2
13.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134213,"Equality, risk-aversion and contractarian social choice",June 1981,,,,Unknown,Unknown,Unknown,Unknown,,
13.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134214,Some measures of closeness to unanimity and their implications,June 1981,Shmuel Nitzan,,,Male,Unknown,Unknown,Male,,40
13.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134215,Subjective expected utility: A review of normative theories,June 1981,Peter C. Fishburn,,,Male,Unknown,Unknown,Male,,188
13.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126280,Can there be a model of explanation?,September 1981,Peter Achinstein,,,Male,Unknown,Unknown,Male,,4
13.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126281,Inductive-nomological explanations and psychological laws,September 1981,Robert Audi,,,Male,Unknown,Unknown,Male,,2
13.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126282,Theory construction in psychology: The interpretation and integration of psychological data,September 1981,Gordon M. Becker,Eckehart Köhler,,Male,Male,Unknown,Male,,
13.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126283,Probabilistic and causal dependence structures,September 1981,Zoltan Domotor,,,Unknown,Unknown,Unknown,Unknown,,
13.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126965,Causes as explanations: A critique,December 1981,Jaegwon Kim,,,Unknown,Unknown,Unknown,Unknown,,
13.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126966,"Nature, culture, and persons",December 1981,Joseph Margolis,,,Male,Unknown,Unknown,Male,,4
13.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126967,On the language of causal talk: Scriven and Suppes,December 1981,R. M. Martin,,,Unknown,Unknown,Unknown,Unknown,,
13.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126968,The role of inductive generalizations in Sellars' theory of explanation,December 1981,Joseph C. Pitt,,,Male,Unknown,Unknown,Male,,
13.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126969,Scriven on Causation as Explanation,December 1981,Ernest Sosa,,,Male,Unknown,Unknown,Male,,1
13.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126970,Scientific causal talk: A reply to Martin,December 1981,Patrick Suppes,,,Male,Unknown,Unknown,Male,,1
13.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126971,Correction to ‘several possible measures of risk’,December 1981,R. Duncan Luce,,,Unknown,Unknown,Unknown,Unknown,,
13.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126972,Books received,December 1981,,,,Unknown,Unknown,Unknown,Unknown,,
14.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135141,The logical structure of applied social science,March 1982,Günther E. Braun,,,Male,Unknown,Unknown,Male,,
14.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135142,On the axiomatisation of subjective probabilities,March 1982,Simon French,,,Male,Unknown,Unknown,Male,,11
14.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135143,Comparison and choice,March 1982,M. R. Sertel,A. V. D. Bellen,,Unknown,Unknown,Unknown,Unknown,,
14.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135144,Models for fuzzy nominal data,March 1982,Michael Smithson,,,Male,Unknown,Unknown,Male,,6
14.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135145,The inferred referendum — A rule for committee decisions,March 1982,A. M. Wolsky,L. Sanathanan,,Unknown,Unknown,Unknown,Unknown,,
14.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135146,"Deterrence, utility, and rational choice — A comment",March 1982,Georges Bernard,,,Male,Unknown,Unknown,Male,,3
14.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135147,Deterrence and utility again: A response to Bernard,March 1982,Gregory S. Kavka,,,Male,Unknown,Unknown,Male,,
14.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135148,Review,March 1982,Gary Anthony Gigliotti,,,Male,Unknown,Unknown,Male,,
14.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135149,Corrigendum,March 1982,,,,Unknown,Unknown,Unknown,Unknown,,
14.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135150,Announcement,March 1982,,,,Unknown,Unknown,Unknown,Unknown,,
14.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133972,Improving judgment by reconciling incoherence,June 1982,R. V. Brown,D. V. Lindley,,Unknown,Unknown,Unknown,Unknown,,
14.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133973,On the derivation of majority rule,June 1982,Donald E. Campbell,,,Male,Unknown,Unknown,Male,,10
14.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133974,Majority efficiencies for simple voting procedures: Summary and interpretation,June 1982,Peter C. Fishburn,William V. Gehrlein,,Male,Male,Unknown,Male,,33
14.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133975,On the logical structure of some value systems of classical economics: Marx and Sraffa,June 1982,David Pearce,Michele Tucci,,Male,Female,Unknown,Mix,,
14.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133976,A dual approach to Bayesian inference and adaptive control,June 1982,Leigh Tesfatsion,,,,Unknown,Unknown,Mix,,
14.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133977,On the relevance of statistical relevance theory,June 1982,Stephen P. Turner,,,Male,Unknown,Unknown,Male,,6
14.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133978,"Allocation, Lehrer models, and the consensus of probabilities",June 1982,Carl Wagner,,,Male,Unknown,Unknown,Male,,45
14.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133979,Review,June 1982,,,,Unknown,Unknown,Unknown,Unknown,,
14.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133674,Inconsistency resolution and collective choice,September 1982,Dennis J. Packard,Ronald A. Heiner,,Male,Male,Unknown,Male,,2
14.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133675,Dynamic models as tools for forecasting and planning: A presentation and some methodological aspects,September 1982,Peter Gärdenfors,,,Male,Unknown,Unknown,Male,,3
14.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133676,Traditional theory and all the king's men,September 1982,,,,Unknown,Unknown,Unknown,Unknown,,
14.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133677,Rediscovering debates in the international studies: Morton Kaplan's system epistemology revisited,September 1982,Roger D. Spegele,,,Male,Unknown,Unknown,Male,,1
14.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133678,Announcement,September 1982,,,,Unknown,Unknown,Unknown,Unknown,,
14.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126172,On the structure of dialectical reasoning in the social and policy sciences,December 1982,Ian I. Mitroff,Richard O. Mason,,Male,Male,Unknown,Male,,10
14.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126173,Liberalism and individual preferences,December 1982,John Craven,,,Male,Unknown,Unknown,Male,,5
14.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126174,A limited possibility result for social choice under majority voting,December 1982,T. M. Fogarty,,,Unknown,Unknown,Unknown,Unknown,,
14.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126175,The ‘Marriage Game’: An assignment problem with indivisibilities,December 1982,Peter J. Dolton,,,Male,Unknown,Unknown,Male,,2
14.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126176,Methodology and finance,December 1982,Reinhard H. Schmidt,,,Male,Unknown,Unknown,Male,,4
14.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126177,Cyclical preference logic,December 1982,Dennis J. Packard,,,Male,Unknown,Unknown,Male,,7
14.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126178,Prof. Opp on evolution - Some critical comments,December 1982,Michael Schmid,,,Male,Unknown,Unknown,Male,,1
15.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133459,Rights as constraints: Nozick versus Sen,March 1983,,,,Unknown,Unknown,Unknown,Unknown,,
15.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133460,On the predictive efficiency of the core solution in side-payment games,March 1983,H. Andrew Michener,Kathryn Potter,Melvin M. Sakurai,Unknown,Female,Male,Mix,,
15.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133461,A theory of direct inference,March 1983,John L. Pollock,,,Male,Unknown,Unknown,Male,,10
15.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133462,Review,March 1983,,,,Unknown,Unknown,Unknown,Unknown,,
15.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133463,Announcement,March 1983,,,,Unknown,Unknown,Unknown,Unknown,,
15.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00143066,Call for papers,June 1983,,,,Unknown,Unknown,Unknown,Unknown,,
15.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00143067,Beyond contradiction and consistency: A design for a dialectical policy system,June 1983,Ian I. Mitroff,Harold Quinton,Richard O. Mason,Male,Male,Male,Male,,7
15.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00143068,Approximations of rational criteria under complete ignorance and the independence axiom,June 1983,Michèle Cohen,Jean-Yves Jaffray,,Female,Unknown,Unknown,Female,,13
15.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00143069,The reasonable man — a social choice approach,June 1983,Ariel Rubinstein,,,Male,Unknown,Unknown,Male,,2
15.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00143070,Condorcet's paradox,June 1983,William V. Gehrlein,,,Male,Unknown,Unknown,Male,,92
15.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00143071,Review,June 1983,Stephen Spielman,,,Male,Unknown,Unknown,Male,,
15.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125669,Alternative libertarian claims and Sen's paradox,September 1983,Lorenz Krüger,Wulf Gaertner,,Male,Male,Unknown,Male,,18
15.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125670,The behavioral function: An inquiry into the relation between behavior and utility,September 1983,Johan K. De Vree,,,Male,Unknown,Unknown,Male,,2
15.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125671,How we may have been misled into believing in the interpersonal comparability of utility,September 1983,Louis Narens,R. Duncan Luce,,Male,Unknown,Unknown,Male,,28
15.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125672,Thirteen theorems in search of the truth,September 1983,Bernard Grofman,Guillermo Owen,Scott L. Feld,Male,Male,Male,Male,,264
15.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125673,Staying power in sequential games,September 1983,Steven J. Brams,Marek P. Hessel,,Male,Male,Unknown,Male,,14
15.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125674,Anscombe's paradox and the rule of three-fourths,September 1983,Carl Wagner,,,Male,Unknown,Unknown,Male,,22
15.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00162110,A restriction on a theorem of Harsanyi,December 1983,Michael D. Resnik,,,Male,Unknown,Unknown,Male,,6
15.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00162111,Decision making under uncertainty: Starr's Domain criterion,December 1983,G. O. Schneller IV,G. P. Sphicas,,Unknown,Unknown,Unknown,Unknown,,
15.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00162112,Formulating Rawls's principles of justice,December 1983,Larry Hohm,,,Male,Unknown,Unknown,Male,,2
15.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00162113,On the representation of classificatory value structures,December 1983,Wolfgang Lenzen,,,Male,Unknown,Unknown,Male,,6
15.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00162114,Dimensions of election procedures: Analyses and comparisons,December 1983,Peter C. Fishburn,,,Male,Unknown,Unknown,Male,,12
15.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00162115,Newcomb's problem: Recalculations for the one-boxer,December 1983,Roy A. Sorensen,,,Male,Unknown,Unknown,Male,,3
16.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141672,Limited-move equilibria In 2 × 2 games,January 1984,Frank C. Zagare,,,Male,Unknown,Unknown,Male,,76
16.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141673,The general relevance of the impossibility teorem in smooth social choice,January 1984,Norman Schofield,,,Male,Unknown,Unknown,Male,,1
16.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141674,The information problem in decision making,January 1984,Günter Menges,Stefan Huschens,,Male,Male,Unknown,Male,,2
16.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141675,Newcomb's many solutions,January 1984,Ellery Eells,,,,Unknown,Unknown,Mix,,
16.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00141676,Utilitarianism and Pareto principle: A comment,January 1984,Eerik Lagerspetz,,,Male,Unknown,Unknown,Male,,1
16.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125874,"Lattices, bargaining and group decisions",March 1984,S. S. Sengupta,,,Unknown,Unknown,Unknown,Unknown,,
16.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125875,Equilibria for far-sighted players,March 1984,D. Marc Kilgour,,,Unknown,Unknown,Unknown,Unknown,,
16.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125876,Can egalitarianism be built into rationality theory?,March 1984,C. A. Hooker,,,Unknown,Unknown,Unknown,Unknown,,
16.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125877,A philosophical basis for decision aiding,March 1984,Anthony N. S. Freeling,,,Male,Unknown,Unknown,Male,,5
16.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125878,Announcement,March 1984,,,,Unknown,Unknown,Unknown,Unknown,,
16.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134645,Compatibility of multiple goal programming and the maximize expected utility criterion,May 1984,J. J. Buckley,,,Unknown,Unknown,Unknown,Unknown,,
16.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134646,Newcomb's paradox,May 1984,Maurice W. Sasieni,,,Male,Unknown,Unknown,Male,,
16.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134647,Comparative statics of mixed-strategy equilibria in noncooperative two-person games,May 1984,Vincent P. Crawford,Dennis E. Smallwood,,Male,Male,Unknown,Male,,11
16.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134648,Avoiding Anscombe's paradox,May 1984,Carl Wagner,,,Male,Unknown,Unknown,Male,,16
16.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134649,The geometry of justice: Three existence and uniqueness theorems,May 1984,Donald Wittman,,,Male,Unknown,Unknown,Male,,2
16.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134650,Concordia discors: Or: What do economists think?,May 1984,Werner W. Pommerehne,Friedrich Schneider,Bruno S. Frey,Male,Male,Male,Male,,12
16.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134651,Announcement,May 1984,,,,Unknown,Unknown,Unknown,Unknown,,
17.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140051,Money as a social contract,July 1984,Eerik Lagerspetz,,,Male,Unknown,Unknown,Male,,2
17.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140052,A note on the two person exchange game,July 1984,Warren R. Hughes,,,Male,Unknown,Unknown,Male,,
17.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140053,On Harsanyi's utilitarian cardinal welfare theorem,July 1984,Peter C. Fishburn,,,Male,Unknown,Unknown,Male,,39
17.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140054,Expected frequency and mean size of the paradox of new members,July 1984,Amnon Rapoport,Ariel Cohen,,Male,Male,Unknown,Male,,3
17.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140055,The significance of independent decisions in uncertain dichotomous choice situations,July 1984,Shmuel Nitzan,Jacob Paroush,,Male,Male,Unknown,Male,,33
17.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140056,Aggregating opinions through logarithmic pooling,July 1984,C. Genest,S. Weerahandi,J. V. Zidek,Unknown,Unknown,Unknown,Unknown,,
17.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140057,Metatickles and the dynamics of deliberation,July 1984,Ellery Eells,,,,Unknown,Unknown,Mix,,
17.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140058,On utility functions. The present state,July 1984,Georges Bernard,,,Male,Unknown,Unknown,Male,,6
17.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00140059,Review,July 1984,D. Marc Kilgour,,,Unknown,Unknown,Unknown,Unknown,,
17.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160977,"Coercion, deterrence, and authority",September 1984,Timo Airaksinen,,,Male,Unknown,Unknown,Male,,4
17.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160978,The stability of bargains behind the veil of ignorance,September 1984,James C. Gaa,,,Male,Unknown,Unknown,Male,,1
17.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160979,Good approximations to maximin allocations,September 1984,Paul Grout,,,Male,Unknown,Unknown,Male,,
17.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160980,Interpersonal level comparability implies comparability of utility differences,September 1984,Yew Kwang Ng,,,Unknown,Unknown,Unknown,Unknown,,
17.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160981,"Games, goals, and bounded rationality",September 1984,Leigh Tesfatsion,,,,Unknown,Unknown,Mix,,
17.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160982,"Rule utilitarianism, rational decision and obligations",September 1984,Lanning Sowden,,,Unknown,Unknown,Unknown,Unknown,,
17.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160983,The St. Petersburg gamble and risk,September 1984,Paul Weirich,,,Male,Unknown,Unknown,Male,,19
17.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160984,Books received,September 1984,,,,Unknown,Unknown,Unknown,Unknown,,
17.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132607,Welfare functions and group decisions,November 1994,Karl Borch,,,Male,Unknown,Unknown,Male,,
17.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132608,A general theorem and eight corollaries in search of correct decision,November 1994,Shmuel Nitzan,Jacob Paroush,,Male,Male,Unknown,Male,,25
17.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132609,Foundations for direct inference,November 1994,John L. Pollock,,,Male,Unknown,Unknown,Male,,8
17.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132610,A note on the relationship of interdependent action to the optimality of certain voting decisions,November 1994,Louis Putterman,,,Male,Unknown,Unknown,Male,,
17.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132611,Notes on Ostrogorski's paradox,November 1994,Fred M. Shelley,,,Male,Unknown,Unknown,Male,,5
17.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132612,Partial information on decisional competences and the desirability of the expert rule in uncertain dichotomous choice situations,November 1994,Shmuel Nitzan,Jacob Paroush,,Male,Male,Unknown,Male,,7
17.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132613,Changes in utility as information,November 1994,Morris H. Degroot,,,Male,Unknown,Unknown,Male,,32
17.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132614,Book review,November 1994,Hal Berghel,,,Male,Unknown,Unknown,Male,,
17.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132615,Announcement,November 1994,,,,Unknown,Unknown,Unknown,Unknown,,
17.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132616,Announcement,November 1994,,,,Unknown,Unknown,Unknown,Unknown,,
18.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134075,Acceptance of empirical statements: A Bayesian theory without cognitive utilities,January 1985,John C. Harsanyi,,,Male,Unknown,Unknown,Male,,22
18.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134076,Rules of behavior and expected utility theory. Compatibility versus dependence,January 1985,Ole Hagen,,,Male,Unknown,Unknown,Male,,14
18.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134077,Risk aversion in n-person bargaining,January 1985,Hans Peters,Stef Tijs,,Male,Male,Unknown,Male,,4
18.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134078,The ordinal utility under uncertainty and the measure of risk aversion in terms of preferences,January 1985,Aldo Montesano,,,Male,Unknown,Unknown,Male,,7
18.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134079,A new basis for decision theory,January 1985,Donald Davidson,,,Male,Unknown,Unknown,Male,,28
18.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134080,"Comment on Pommerehne et al., “concordia discors: Or: What do economists think?”",January 1985,Richard L. Meile,Stephanie L. Shanks,,Male,Female,Unknown,Mix,,
18.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134081,Announcement,January 1985,,,,Unknown,Unknown,Unknown,Unknown,,
18.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134067,Editorial,March 1985,,,,Unknown,Unknown,Unknown,Unknown,,
18.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134068,Rationality and uncertainty,March 1985,Amartya Sen,,,Unknown,Unknown,Unknown,Unknown,,
18.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134069,The value of information in Newcomb's Problem and the Prisoners' Dilemma,March 1985,Paul Snow,,,Male,Unknown,Unknown,Male,,
18.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134070,A remark on the connexion between procedure and value,March 1985,,,,Unknown,Unknown,Unknown,Unknown,,
18.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134071,No chain store paradox,March 1985,Lawrence H. Davis,,,Male,Unknown,Unknown,Male,,1
18.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134072,Assessment of social value in the allocation of CT scanners in the Munich Metropolitan Area (MMA),March 1985,H. W. Gottinger,,,Unknown,Unknown,Unknown,Unknown,,
18.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134073,"The Generalized Means Model (GMM) for non-deterministic decision making: Its normative and descriptive power, including sketch of the representation theorem",March 1985,Hector A. Munera,,,Male,Unknown,Unknown,Male,,10
18.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134074,Individual behavior under risk and under uncertainty: An experimental study,March 1985,M. Cohen,J. Y. Jaffray,T. Said,Unknown,Unknown,Unknown,Unknown,,
18.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126593,Timing contradictions in von Neumann and Morgenstern's axioms and in savage's ‘sure-thing’ proof,May 1985,Robin Pope,,,,Unknown,Unknown,Mix,,
18.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126594,Assessment response surface: Investigating utility dependence on probability,May 1985,Mark R. McCord,Richard De Neufville,,Male,Male,Unknown,Male,,13
18.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126595,On the risk-aversion comparability of state-dependent utility functions,May 1985,Gerald L. Nordquist,,,Male,Unknown,Unknown,Male,,5
18.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126596,Formal properties of interpersonal envy,May 1985,Adhip Chaudhuri,,,Unknown,Unknown,Unknown,Unknown,,
18.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126597,A mistaken argument against the expected utility theory of rationality,May 1985,John Broome,,,Male,Unknown,Unknown,Male,,3
18.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126598,Errata,May 1985,,,,Unknown,Unknown,Unknown,Unknown,,
19.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134352,A portfolio of risk measures,July 1985,Kenneth R. Maccrimmon,Donald A. Wehrung,,Male,Male,Unknown,Male,,55
19.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134353,Metapreferences and the reasons for stability in social choice: Thoughts on broadening and clarifying the debate,July 1985,Bernard Grofman,Carole Uhlaner,,Male,Female,Unknown,Mix,,
19.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134354,"The impact of risk attitude, uncertainty and disequilibria on optimal production and inventory",July 1985,Karl Aiginger,,,Male,Unknown,Unknown,Male,,4
19.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134355,"Regret, recrimination and rationality",July 1985,Robert Sugden,,,Male,Unknown,Unknown,Male,,102
19.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134356,Books received,July 1985,,,,Unknown,Unknown,Unknown,Unknown,,
19.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132438,On Harrod's first refining principle,September 1985,J. Moreh,,,Unknown,Unknown,Unknown,Unknown,,
19.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132439,"The foundations of decision theory: An intuitive, operational approach with mathematical extensions",September 1985,J. M. Bernardo,J. R. Ferrandiz,A. F. M. Smith,Unknown,Unknown,Unknown,Unknown,,
19.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132440,Sophisticated voting under the plurality procedure: A test of a new definition,September 1985,Richard G. Niemi,Arthur Q. Frank,,Male,Male,Unknown,Male,,12
19.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132441,Measurement of fuzziness: A general approach,September 1985,Satya R. Chakravarty,Tirthankar Roy,,Male,Unknown,Unknown,Male,,5
19.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132442,Ratificationism without ratification: Jeffrey meets Savage,September 1985,Wlodzimierz Rabinowicz,,,Unknown,Unknown,Unknown,Unknown,,
19.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132443,Interpersonal level comparability does not imply comparability of utility differences — A comment on Ng,September 1985,Jean-Yves Jaffray,,,Unknown,Unknown,Unknown,Unknown,,
19.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126415,"A comparison of models for strategic planning, risk analysis and risk management",November 1985,Janos Acs,,,Unknown,Unknown,Unknown,Unknown,,
19.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126416,Strict proportional power in voting bodies,November 1985,Manfred J. Holler,,,Male,Unknown,Unknown,Male,,20
19.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126417,The logical reconstruction of pure exchange economics: Another alternative,November 1985,Douglas W. Hands,,,Male,Unknown,Unknown,Male,,3
19.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126418,A macroeconomic potential describing structural change of the economy,November 1985,Günter Haag,Wolfgang Weidlich,Gerhard O. Mensch,Male,Male,Male,Male,,1
19.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126419,"Utilitarianism, rawls, and the relativism of absolute judgements",November 1985,Nollaig Mackenzie,,,Female,Unknown,Unknown,Female,,
20.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133633,Conceptual correlation: An example of two social psychological theories,January 1986,Martti Kuokkanen,,,Male,Unknown,Unknown,Male,,5
20.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133634,The repetitions approach to characterize cardinal utility,January 1986,Peter Wakker,,,Male,Unknown,Unknown,Male,,6
20.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133635,Liberals and information,January 1986,Michael Webster,,,Male,Unknown,Unknown,Male,,7
20.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133636,Harsanyi's aggregation theorem without selfish preferences,January 1986,Stephen Selinger,,,Male,Unknown,Unknown,Male,,6
20.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133637,Catastrophe avoidance and risk aversion: Implications of formal utility maximization,January 1986,Robert U. Ayres,Manalur S. Sandilya,,Male,Unknown,Unknown,Male,,5
20.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133638,Expected utility with perturbed lotteries,January 1986,Gilbert W. Bassett Jr.,,,Male,Unknown,Unknown,Male,,
20.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133639,Announcement,January 1986,,,,Unknown,Unknown,Unknown,Unknown,,
20.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135087,Non-strict ordinal 2 × 2 games: A comprehensive computer-assisted analysis of the 726 possibilities,March 1986,Niall M. Fraser,D. Marc Kilgour,,Male,Unknown,Unknown,Male,,16
20.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135088,Social Darwinist aspects of utility and probability,March 1986,Stig I. Rosenlund,,,Male,Unknown,Unknown,Male,,
20.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135089,Plural analysis: Multiple approaches to quantitative research,March 1986,Rex V. Brown,Dennis V. Lindley,,Male,Male,Unknown,Male,,17
20.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135090,Approval voting and strategy analysis: A Venetian example,March 1986,Marji Lines,,,Unknown,Unknown,Unknown,Unknown,,
20.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135091,"A discussion of the present state of utility theory (FUR II, Venice, 1984)",March 1986,Georges Bernard,,,Male,Unknown,Unknown,Male,,2
20.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135092,On the existence of stable states in game problems,March 1986,V. A. Gelovani,E. R. Smol'jakov,,Unknown,Unknown,Unknown,Unknown,,
20.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00135093,Announcement,March 1986,,,,Unknown,Unknown,Unknown,Unknown,,
20.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134038,Preface,May 1986,,,,Unknown,Unknown,Unknown,Unknown,,
20.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134039,Action theory as a resource for decision theory,May 1986,Robert Audi,,,Male,Unknown,Unknown,Male,,
20.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134040,Voluntary exertion of the body: A volitional account,May 1986,Carl Ginet,,,Male,Unknown,Unknown,Male,,4
20.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134041,Intrinsic intentionality,May 1986,Hugh J. McCann,,,Male,Unknown,Unknown,Male,,24
20.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134042,An action-plan interpretation of Purposive Explanations of Actions,May 1986,William P. Alston,,,Male,Unknown,Unknown,Male,,4
20.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134043,Formal logic and practical reasoning,May 1986,Bruce Aune,,,Male,Unknown,Unknown,Male,,1
20.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134044,Leading a rational life,May 1986,John G. Bennett,,,Male,Unknown,Unknown,Male,,
20.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134045,Call for papers,May 1986,,,,Unknown,Unknown,Unknown,Unknown,,
20.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134046,Announcement,May 1986,,,,Unknown,Unknown,Unknown,Unknown,,
21.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134168,General equilibrium with information sales,July 1986,Beth Allen,,,Female,Unknown,Unknown,Female,,5
21.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134169,Preference and the cost of preferential choice,July 1986,Carl Halldin,,,Male,Unknown,Unknown,Male,,7
21.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134170,An examination and critique of Harsanyi's version of utilitarianism,July 1986,Reidar K. Lie,,,Male,Unknown,Unknown,Male,,2
21.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134171,On absolute preference and stochastic dominance,July 1986,Hector A. Múnera,,,Male,Unknown,Unknown,Male,,4
21.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134172,Comment on Craven,July 1986,Gary Anthony Gigliotti,,,Male,Unknown,Unknown,Male,,2
21.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134173,Announcement,July 1986,,,,Unknown,Unknown,Unknown,Unknown,,
21.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00127190,Predictive superiority of the beta-characteristic function in cooperative non-sidepayment N-person games,September 1986,H. Andrew Michener,James M. Ekman,David C. Dettman,Unknown,Male,Male,Male,,4
21.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00127191,Two-component random utilities,September 1986,Fernando Menezes,Campello De Souza,,Male,Unknown,Unknown,Male,,1
21.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00127192,Group decision making under uncertainty a note on the aggregation of “ordinal probabilities”,September 1986,Jean Laine,Michel Le Breton,Alain Trannoy,Male,Male,Male,Male,,9
21.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00127193,The chain-store paradox revisited,September 1986,Walter Trockel,,,Male,Unknown,Unknown,Male,,7
21.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00127194,Conditions for the optimality of simple majority decisions in pairwise choice situations,September 1986,Mark Gradstein,,,Male,Unknown,Unknown,Male,,3
21.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00127195,Medicine and the paradigm of Neo-pragmatism a contribution to medical decision theory,September 1986,Herbert Stachowiak,,,Male,Unknown,Unknown,Male,,2
21.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00127196,Morality and welfare,September 1986,J. Moreh,,,Unknown,Unknown,Unknown,Unknown,,
21.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134096,One person/one vote is not efficient given information on factions,November 1986,Robert F. Bordley,,,Male,Unknown,Unknown,Male,,4
21.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134097,Noncompensatory and generalized noncompensatory preference structures,November 1986,Denis Bouyssou,Jean-Claude Vansnick,,Male,Male,Unknown,Male,,58
21.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134098,Bargaining strength in three-person characteristic-function games with v(i)> 0 a reanalysis of Kahan and Rapoport (1977),November 1986,Ronald Henss,,,Male,Unknown,Unknown,Male,,2
21.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134099,Decisions with indeterminate probabilities,November 1986,Ronald P. Loui,,,Male,Unknown,Unknown,Male,,6
21.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134100,A noncooperative solution to a two-person bargaining game,November 1986,R. Harrison Wagner,,,Unknown,Unknown,Unknown,Unknown,,
21.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134101,Announcement,November 1986,,,,Unknown,Unknown,Unknown,Unknown,,
22.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125654,A note on interpersonal comparisons of utility,January 1987,C. L. Sheng,,,Unknown,Unknown,Unknown,Unknown,,
22.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125655,Comparison of theories for payoff disbursement of coalition values,January 1987,Amnon Rapoport,,,Male,Unknown,Unknown,Male,,5
22.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125656,The fundamental uncertainty principle and the principle of non-additive emotional states,January 1987,Domingo Castelo Joaquin,,,Male,Unknown,Unknown,Male,,1
22.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125657,Difference logics for preference,January 1987,Dennis J. Packard,,,Male,Unknown,Unknown,Male,,2
22.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125658,Multivariate risk premiums,January 1987,R. Ambarish,J. G. Kallberg,,Unknown,Unknown,Unknown,Unknown,,
22.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00125659,Announcement,January 1987,,,,Unknown,Unknown,Unknown,Unknown,,
22.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126384,Publisher's announcement,March 1987,,,,Unknown,Unknown,Unknown,Unknown,,
22.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126385,An integrated axiomatic approach to the existence of ordinal and cardinal utility functions,March 1987,Robert Jarrow,,,Male,Unknown,Unknown,Male,,5
22.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126386,"Expectation dependence of random variables, with an application in portfolio theory",March 1987,Randall Wright,,,Male,Unknown,Unknown,Male,,73
22.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126387,Some extensions of Luce's measures of risk,March 1987,Rakesh K. Sarin,,,Male,Unknown,Unknown,Male,,22
22.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126388,Maximizing expected utility is a survival criterion,March 1987,Paul Snow,,,Male,Unknown,Unknown,Male,,6
22.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126389,Causality in the logic of decision,March 1987,Patrick Maher,,,Male,Unknown,Unknown,Male,,8
22.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126390,Learning with detachment: Reply to Maher,March 1987,Ellery Eells,,,,Unknown,Unknown,Mix,,
22.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126391,Announcement,March 1987,,,,Unknown,Unknown,Unknown,Unknown,,
22.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134082,Editorial,May 1987,Werner Leinfellner,,,Male,Unknown,Unknown,Male,,
22.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134083,Interpreting inequality measures in a Harsanyi framework,May 1987,B. G. Dahlby,,,Unknown,Unknown,Unknown,Unknown,,
22.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134084,Nozick on Sen: A reply to Perelli-Minetti,May 1987,Miles Sonstegaard,,,Male,Unknown,Unknown,Male,,4
22.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134085,"Choosing between choice models of ethics: Rawlsian equality, utilitarianism, and the concept of persons",May 1987,Stephen W. Ball,,,Male,Unknown,Unknown,Male,,5
22.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134086,"Updating, supposing, and maxent",May 1987,Brian Skyrms,,,Male,Unknown,Unknown,Male,,22
22.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134087,Indefinite terminating points and the iterated Prisoner's Dilemma,May 1987,John W. Carroll,,,Male,Unknown,Unknown,Male,,9
22.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134088,A new theory of voting: Why vote when millions of others do,May 1987,Amihai Glazer,,,Male,Unknown,Unknown,Male,,16
22.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134089,A stormy assembly: Electoral paradoxes,May 1987,J. L. Petit,E. Térouanne,,Unknown,Unknown,Unknown,Unknown,,
22.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134090,Announcement,May 1987,,,,Unknown,Unknown,Unknown,Unknown,,
22.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134091,Note from the publisher regarding volume contents list,May 1987,,,,Unknown,Unknown,Unknown,Unknown,,
23.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00127333,Editorial,July 1987,,,,Unknown,Unknown,Unknown,Unknown,,
23.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00127334,The tracing procedure: A self-correcting reasoning procedure,July 1987,John C. Harsanyi,,,Male,Unknown,Unknown,Male,,3
23.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00127335,Second-order probabilities and belief functions,July 1987,Jonathan Baron,,,Male,Unknown,Unknown,Male,,23
23.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00127336,Equivalent decision trees and their associated strategy sets,July 1987,Irving H. Lavalle,Peter C. Fishburn,,Male,Male,Unknown,Male,,10
23.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00127337,The justice and rationale of cost-benefit analysis,July 1987,,,,Unknown,Unknown,Unknown,Unknown,,
23.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00127338,Mean-risk decision analysis,July 1987,Paul Weirich,,,Male,Unknown,Unknown,Male,,2
23.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126301,Mean utility preserving increases in risk for state dependent utility functions,September 1987,Michel Demers,,,Male,Unknown,Unknown,Male,,1
23.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126302,The uncovered set in spatial voting games,September 1987,Scott L. Feld,Bernard Grofman,with the assistance of Nicholas Noviello,Male,Male,Unknown,Male,,39
23.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126303,On the impossibility of events of zero probability,September 1987,Asad Zaman,,,Male,Unknown,Unknown,Male,,4
23.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126304,A test of the characteristic function and the Harsanyi function in N-person normal form sidepayment games,September 1987,H. Andrew Michener,David C. Dettman,David C. Julseth,Unknown,Male,Male,Male,,3
23.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126305,Are the preference axioms really rational?,September 1987,Paul Anand,,,Male,Unknown,Unknown,Male,,41
23.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126306,Books received,September 1987,,,,Unknown,Unknown,Unknown,Unknown,,
23.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00129148,Choice and constraint in budget-making,November 1987,Jan-Erik Lane,Anders Westlund,,Unknown,Male,Unknown,Male,,1
23.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00129149,On arguments from self-interest for the Nash solution and the Kalai egalitarian solution to the bargaining problem,November 1987,Luc Bovens,,,Male,Unknown,Unknown,Male,,5
23.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00129150,Subjectively weighted linear utility,November 1987,Gordon B. Hazen,,,Male,Unknown,Unknown,Male,,17
23.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00129151,Selection by proxy: A model for the simplification of decision under risk and under uncertainty,November 1987,H. Blasche,E. Dorfner,,Unknown,Unknown,Unknown,Unknown,,
23.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00129152,Constraints on utilitarian prescriptions for group actions,November 1987,C. L. Sheng,,,Unknown,Unknown,Unknown,Unknown,,
23.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00129153,Announcement,November 1987,,,,Unknown,Unknown,Unknown,Unknown,,
23.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00129154,Announcement,November 1987,,,,Unknown,Unknown,Unknown,Unknown,,
24.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00137219,On some n-person games,January 1988,Marian MatŁoka,,,Male,Unknown,Unknown,Male,,1
24.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00137220,Microcosms and macrocosms: Seat allocation in proportional representation systems,January 1988,Amnon Rapoport,Dan S. Felsenthal,Zeev Maoz,Male,Male,Male,Male,,4
24.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00137221,The conflict between naive and sophisticated choice as a form of the ‘liberal paradox’,January 1988,Gary Anthony Gigliotti,,,Male,Unknown,Unknown,Male,,3
24.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00137222,Plato's “democratic man” and the implausibility of preference utilitarianism,January 1988,Tal Scriven,,,,Unknown,Unknown,Mix,,
24.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00137223,The impossibility of the Paretian liberal and its relevance to welfare economics,January 1988,Tuovi Allén,,,Female,Unknown,Unknown,Female,,10
24.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00137224,Social organizations and matching theory,January 1988,F. Masarani,S. S. Gokturk,,Unknown,Unknown,Unknown,Unknown,,
24.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00137225,Announcement,January 1988,,,,Unknown,Unknown,Unknown,Unknown,,
24.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00137226,Announcement,January 1988,,,,Unknown,Unknown,Unknown,Unknown,,
24.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132457,A taxonomy of all ordinal 2 × 2 games,March 1988,D. Marc Kilgour,Niall M. Fraser,,Unknown,Male,Unknown,Male,,23
24.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132458,How vicious are cycles of intransitive choice?,March 1988,Maya Bar-Hillel,Avishai Margalit,,Female,Male,Unknown,Mix,,
24.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132459,Science as a rational enterprise,March 1988,Arthur M. Diamond Jr.,,,Male,Unknown,Unknown,Male,,19
24.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132460,Choice under risk and the security factor: An axiomatic model,March 1988,Jean Yves Jaffray,,,Male,Unknown,Unknown,Male,,43
24.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132461,Announcement,March 1988,,,,Unknown,Unknown,Unknown,Unknown,,
24.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132462,Announcement,March 1988,,,,Unknown,Unknown,Unknown,Unknown,,
24.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00148953,Publisher's announcement,May 1988,,,,Unknown,Unknown,Unknown,Unknown,,
24.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00148954,Cooperation in the Prisoner's Dilemma,May 1988,J. V. Howard,,,Unknown,Unknown,Unknown,Unknown,,
24.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00148955,Rawls and Bentham reconciled,May 1988,Udo Ebert,,,Male,Unknown,Unknown,Male,,12
24.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00148956,"Justice, liberty, unanimity and the axioms of identity",May 1988,I. Macintyre,,,Unknown,Unknown,Unknown,Unknown,,
24.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00148957,The primary-goods indexation problem in Rawls's theory of justice,May 1988,Douglas H. Blair,,,Male,Unknown,Unknown,Male,,10
24.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00148958,The essential ranking of decision rules in small panels of experts,May 1988,Drora Karotkin,Samuel Nitzal,Jacob Paroush,Female,Male,Male,Mix,,
24.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00148959,The risk aversion measure without the independence axiom,May 1988,Aldo Montesano,,,Male,Unknown,Unknown,Male,,11
24.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00148960,Book review,May 1988,Luc Bovens,,,Male,Unknown,Unknown,Male,,1
25.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00129167,Different experimental procedures for obtaining valuations of risky actions: Implications for utility theory,July 1988,Graham Loomes,,,Male,Unknown,Unknown,Male,,36
25.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00129168,Consequentialist foundations for expected utility,July 1988,Peter J. Hammond,,,Male,Unknown,Unknown,Male,,227
25.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00129169,Ordinal utility models of decision making under uncertainty,July 1988,Charles F. Manski,,,Male,Unknown,Unknown,Male,,69
25.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00129170,Announcement,July 1988,,,,Unknown,Unknown,Unknown,Unknown,,
25.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134154,A paradox in decision theory and some experimental results: The relative nature of decisions,September 1988,Iain Paterson,Andreas Diekmann,,Male,Male,Unknown,Male,,12
25.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134155,Biting the Bayesian bullet: Zeckhauser's problem,September 1988,Richard Jeffrey,,,Male,Unknown,Unknown,Male,,5
25.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134156,Can good news lead to a more pessimistic choice of action?,September 1988,Giacomo Bonanno,,,Male,Unknown,Unknown,Male,,
25.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134157,Full belief,September 1988,,,,Unknown,Unknown,Unknown,Unknown,,
25.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134158,A simplified taxonomy of 2 × 2 games,September 1988,Bernard Walliser,,,Male,Unknown,Unknown,Male,,4
25.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134159,Discrepancies in the outcomes resulting from different voting schemes,September 1988,Hannu Nurmi,,,Male,Unknown,Unknown,Male,,23
25.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134160,Announcements,September 1988,,,,Unknown,Unknown,Unknown,Unknown,,
25.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134161,Announcements,September 1988,,,,Unknown,Unknown,Unknown,Unknown,,
25.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133162,On the equivalence between descending bid auctions and first price sealed bid auctions,November 1988,Edi Karni,,,Male,Unknown,Unknown,Male,,10
25.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133163,"Time, bounded utility, and the St. Petersburg paradox",November 1988,Tyler Cowen,Jack High,,,Male,Unknown,Mix,,
25.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133164,Representing the negotiation process with a rule-based formalism,November 1988,Gregory E. Kersten,Wojtek Michalowski,Stan Szpakowicz,Male,Unknown,Male,Male,,41
25.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133165,Perception and organizational crisis management,November 1988,Hooshang Kuklan,,,Unknown,Unknown,Unknown,Unknown,,
25.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133166,Communication and its cost in graph-restricted games,November 1988,Edward C. Rosenthal,,,Male,Unknown,Unknown,Male,,8
25.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133167,Limit theorems for Dempster's rule of combination,November 1988,John Norton,,,Male,Unknown,Unknown,Male,,6
25.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133168,Publisher's announcement,November 1988,,,,Unknown,Unknown,Unknown,Unknown,,
26.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134054,Resource allocation and project selection: Control of r & d under dynamic process of data improvement,January 1989,V. Z. Belenky,A. M. Belostotsky,,Unknown,Unknown,Unknown,Unknown,,
26.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134055,Endogenous risks and the risk premium,January 1989,Eric Briys,Louis Eeckhoudt,Henri Loubergé,Male,Male,Male,Male,,6
26.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134056,Pavlov and the prisoner's dilemma,January 1989,David Kraines,Vivian Kraines,,Male,Female,Unknown,Mix,,
26.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134057,Simultaneous moves multi-person continuous time concession game,January 1989,Chaim Fershtman,,,Male,Unknown,Unknown,Male,,3
26.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134058,Interpersonal level comparability implies comparability of utility differences: A reply,January 1989,Yew-Kwang Ng,,,Unknown,Unknown,Unknown,Unknown,,
26.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134059,Cooperation and recognition. A comment on “cooperation in the prisoner's dilemma” by J. V. Howard,January 1989,Lucian Kern,,,Male,Unknown,Unknown,Male,,
26.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134060,Announcement,January 1989,,,,Unknown,Unknown,Unknown,Unknown,,
26.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00159221,Coherent bets under partially resolving uncertainty and belief functions,March 1989,Jean-Yves Jaffray,,,Unknown,Unknown,Unknown,Unknown,,
26.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00159222,Refinements of Nash Equilibrium: A critique,March 1989,Robin Cubitt,,,,Unknown,Unknown,Mix,,
26.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00159223,The gradual decline of cooperation: Endgame effects in evolutionary game theory,March 1989,Rudolf Schuessler,,,Male,Unknown,Unknown,Male,,8
26.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00159224,Distributional equality in non-classical utilitarianism — A proof of Lerner's theorem for ‘utilitarianism incorporating justice’,March 1989,A. Schäfer,R. W. Trapp,,Unknown,Unknown,Unknown,Unknown,,
26.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00159225,Some quantitative concepts of value and utility from a utilitarian point of view,March 1989,C. L. Sheng,,,Unknown,Unknown,Unknown,Unknown,,
26.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00159226,"Liberals, information and Webster's principles",March 1989,Jerry S. Kelly,,,Male,Unknown,Unknown,Male,,
26.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00159227,Announcement,March 1989,,,,Unknown,Unknown,Unknown,Unknown,,
26.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134106,Comparative accuracy of value solutions in non-sidepayment games with empty core,May 1989,H. Andrew Michener,Mark S. Salzer,,Unknown,Male,Unknown,Male,,3
26.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134107,"Bribes, power, and managerial control in corporate voting games",May 1989,Robert A. Jarrow,J. Chris Leach,,Male,Unknown,Unknown,Male,,4
26.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134108,Estimation of a Bernouilli parameter: A normative approach to replace the Bayesian one,May 1989,Jean-François Laslier,,,Unknown,Unknown,Unknown,Unknown,,
26.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134109,Utility theory and the Bayesian paradigm,May 1989,Jordan Howard Sobel,,,Male,Unknown,Unknown,Male,,1
26.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134110,Consensus for belief functions and related uncertainty measures,May 1989,Carl G. Wagner,,,Male,Unknown,Unknown,Male,,21
26.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134111,On the existence of fair matching algorithms,May 1989,F. Masarani,S. S. Gokturk,,Unknown,Unknown,Unknown,Unknown,,
26.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134112,Announcement,May 1989,,,,Unknown,Unknown,Unknown,Unknown,,
27.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133985,Homo oeconomicus is also homo cogitans homo cogitans is also homo oeconomicus,July 1989,Paul Bourgine,,,Male,Unknown,Unknown,Male,,3
27.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133986,Instrumental rationality and cognitive rationality,July 1989,Bernard Walliser,,,Male,Unknown,Unknown,Male,,32
27.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133987,"Common knowledge, common sense",July 1989,Jean-Pierre Dupuy,,,Male,Unknown,Unknown,Male,,31
27.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133988,Mimetic contagion and speculative bubbles,July 1989,André Orléan,,,Male,Unknown,Unknown,Male,,56
27.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133989,Cognition and uncertainty,July 1989,Bertrand Munier,,,Male,Unknown,Unknown,Male,,
27.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133990,Natural and artificial computing and reasoning in economic affairs,July 1989,J. L. Le Moigne,,,Unknown,Unknown,Unknown,Unknown,,
27.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133991,Credit risk assessment and meta-judgment,July 1989,Suzanne Pinson,,,Female,Unknown,Unknown,Female,,13
27.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133992,Economics reasoning by econometrics or artificial intelligence,July 1989,Odile Paliés,Jacques Mayer,,Female,Male,Unknown,Mix,,
27.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133993,Knowledge representation by schemata in financial expert systems,July 1989,P. Lévine,J.-Ch. Pomerol,,Unknown,Unknown,Unknown,Unknown,,
27.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133994,Expert system projects at the Banque de France an experience in modeling and representing knowledge,July 1989,Duc Pham-Hi,,,,Unknown,Unknown,Mix,,
28.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139236,What are goals and joint goals?,January 1990,Raimo Tuomela,,,Male,Unknown,Unknown,Male,,8
28.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139237,Unique nontransitive measurement on finite sets,January 1990,Peter C. Fishburn,,,Male,Unknown,Unknown,Male,,3
28.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139238,Effects of fixed costs in two-person sequential bargaining,January 1990,Amnon Rapoport,Eythan Weg,Dan S. Felsenthal,Male,Unknown,Male,Male,,31
28.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139239,Disciplining qualitative decision exercises: Aspects of a transempirical protocol,January 1990,John W. Sutherland,,,Male,Unknown,Unknown,Male,,1
28.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139240,Book review,January 1990,Doris Ostrusska,,,Female,Unknown,Unknown,Female,,
28.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00139241,Call for papers,January 1990,,,,Unknown,Unknown,Unknown,Unknown,,
28.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160932,Minimal representation of a semiorder,March 1990,Marc Pirlot,,,Male,Unknown,Unknown,Male,,22
28.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160933,Decision problems under uncertainty based on entropy functionals,March 1990,Hans W. Gottinger,,,Male,Unknown,Unknown,Male,,
28.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160934,A synergetic view of institutions,March 1990,Peter Weise,Wolfgang Brandes,,Male,Male,Unknown,Male,,4
28.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160935,Indefinitely repeated games: A response to Carroll,March 1990,Neal C. Becker,Ann E. Cudd,,Male,Female,Unknown,Mix,,
28.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00160936,Announcement,March 1990,,,,Unknown,Unknown,Unknown,Unknown,,
28.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00162698,Introduction to the special issue on group decision and negotiation support systems (GDNSS),May 1990,Bertrand Munier,Melvin F. Shakun,,Male,Male,Unknown,Male,,1
28.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00162699,Persuasive argumentation in negotiation,May 1990,Katia P. Sycara,,,Female,Unknown,Unknown,Female,,132
28.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00162700,Structuring and simulating negotiation: An approach and an example,May 1990,G. E. Kersten,L. Badcock,G. R. Mallory,Unknown,Unknown,Unknown,Unknown,,
28.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00162701,"Group decision and negotiation support in evolving, nonshared information contexts",May 1990,Melvin F. Shakun,,,Male,Unknown,Unknown,Male,,23
28.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00162702,A decision support system for the graph model of conflicts,May 1990,D. Marc Kilgour,Liping Fang,Keith W. Hipel,Unknown,Unknown,Male,Male,,15
28.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00162703,Supporting individuals in group decision-making,May 1990,P. Korhonen,J. Wallenius,,Unknown,Unknown,Unknown,Unknown,,
28.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00162704,The impact of information and computer based training on negotiators' performance,May 1990,Stéphane Gauvin,Gary L. Lilien,Kalyan Chatterjee,,Male,Male,Mix,,
28.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00162705,The effect of computer intervention and task structure on bargaining outcome,May 1990,Beth H. Jones,M. Tawfik Jelassi,,Female,Unknown,Unknown,Female,,8
28.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00162706,Announcement,May 1990,,,,Unknown,Unknown,Unknown,Unknown,,
29.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134102,Paradigms of measurement,July 1990,Piotr Swistak,,,Male,Unknown,Unknown,Male,,3
29.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134103,Some notes on Church's thesis and the theory of games,July 1990,Luca Anderlini,,,Male,Unknown,Unknown,Male,,27
29.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134104,On the definition of risk aversion,July 1990,Aldo Montesano,,,Male,Unknown,Unknown,Male,,8
29.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134105,Self enforceable paths in extensive form games,July 1990,Jean-Pierre Ponssard,,,Male,Unknown,Unknown,Male,,7
29.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126588,Disciplining qualitative decision exercises: Aspects of a transempirical protocol,September 1990,John W. Sutherland,,,Male,Unknown,Unknown,Male,,1
29.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126589,Under stochastic dominance Choquet-expected utility and anticipated utility are identical,September 1990,Peter Wakker,,,Male,Unknown,Unknown,Male,,60
29.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126590,Uncertainty in moral theory: An epistemic defense of rule-utilitarian liberties,September 1990,Stephen W. Ball,,,Male,Unknown,Unknown,Male,,2
29.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126591,Book review,September 1990,Keith W. Hipel,,,Male,Unknown,Unknown,Male,,
29.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126592,Announcement,September 1990,,,,Unknown,Unknown,Unknown,Unknown,,
29.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126800,Binary 2 × 2 games,November 1990,Peter C. Fishburn,D. Marc Kilgour,,Male,Unknown,Unknown,Male,,3
29.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126801,An experimental analysis of risk taking,November 1990,Olof Dahlbäck,,,Male,Unknown,Unknown,Male,,4
29.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126802,A comparative study of multiattribute decision making methodologies,November 1990,Reuven Karni,Pedro Sanchez,V. M. Rao Tummala,Male,Male,Unknown,Male,,44
29.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126803,Implications of behavioral consistency in dynamic choice under uncertainty,November 1990,Valentino Dardanoni,,,Male,Unknown,Unknown,Male,,2
29.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126804,Choice procedure consistent with similarity relations,November 1990,Jose Maria Aizpurua,Jorge Nieto,Jose Ramon Uriarte,Male,Male,Male,Male,,6
29.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00126805,Call for papers,November 1990,,,,Unknown,Unknown,Unknown,Unknown,,
30.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134634,An outline of my main contributions to economic science,January 1991,Maurice Allais,,,Male,Unknown,Unknown,Male,,12
30.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134635,The geometry of legal principles,January 1991,Rolando Chuaqui,Jerome Malitz,,Male,Male,Unknown,Male,,
30.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134636,Aggregation of preferences: The fuzzy case,January 1991,Antoine Billot,,,Male,Unknown,Unknown,Male,,22
30.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134117,On some multiattribute value function generalizations of the EOQ model in the context of personal inventory decisions,March 1991,Marvin D. Troutt,,,Male,Unknown,Unknown,Male,,
30.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134118,A note on the permutationally convex games,March 1991,Bahram Alidaee,,,Male,Unknown,Unknown,Male,,1
30.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134119,Rawlsian Nash solutions,March 1991,Raul V. Fabella,,,Male,Unknown,Unknown,Male,,
30.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134120,The value of perfect information in nonlinear utility theory,March 1991,Edward E. Schlee,,,Male,Unknown,Unknown,Male,,12
30.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134121,Practical reason or metapreferences? an undogmatic defense of kantian morality,March 1991,Julian Nida-Rümelin,,,Male,Unknown,Unknown,Male,,10
30.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134122,Independence of irrelevant alternatives in the theory of voting,March 1991,Georges Bordes,Nicolaus Tideman,,Male,Male,Unknown,Male,,23
30.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132444,An alternative logical framework for dialectical reasoning in the social and policy sciences,May 1991,Ru Michael Sabre,,,,Unknown,Unknown,Mix,,
30.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132445,An axiomatic basis of accounting: A structuralist reconstruction,May 1991,Wolfgang Balzer,Richard Mattessich,,Male,Male,Unknown,Male,,16
30.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132446,Aggregation theorems and multidimensional stochastic choice models,May 1991,A. A. J. Marley,,,Unknown,Unknown,Unknown,Unknown,,
30.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132447,Is utilitarianism useless?,May 1991,Daniel M. Hausman,,,Male,Unknown,Unknown,Male,,3
30.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132448,Embedded probabilities,May 1991,J. Dubucs,,,Unknown,Unknown,Unknown,Unknown,,
31.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134130,The Pareto rule and strategic voting,July 1991,Ian MacIntyre,,,Male,Unknown,Unknown,Male,,4
31.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134131,A reconstruction of Jeffrey's notion of ratifiability in terms of counterfactual beliefs,July 1991,Hyun Song Shin,,,,Unknown,Unknown,Mix,,
31.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134132,The outranking approach and the foundations of electre methods,July 1991,Bernard Roy,,,Male,Unknown,Unknown,Male,,1038
31.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134133,Ordering pairwise comparison structures,July 1991,R. Delver,H. Monsuur,A. J. A. Storcken,Unknown,Unknown,Unknown,Unknown,,
31.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132988,Editor's preface,September 1991,,,,Unknown,Unknown,Unknown,Unknown,,
31.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132989,Cardinal utility,September 1991,Maurice Allais,,,Male,Unknown,Unknown,Male,,3
31.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132990,"Equality, responsibility, and justice as seen from a utilitarian perspective",September 1991,John C. Harsanyi,,,Male,Unknown,Unknown,Male,,3
31.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132991,Communication of ambiguous risk information,September 1991,W. Kip Viscusi,Wesley A. Magat,Joel Huber,Unknown,Male,Male,Male,,72
31.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132992,A foundation of Bayesian statistics (How to deal with fears),September 1991,R. Kast,,,Unknown,Unknown,Unknown,Unknown,,
31.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132993,"Arbitrage, rationality, and equilibrium",September 1991,Robert F. Nau,Kevin F. McCardle,,Male,Male,Unknown,Male,,61
31.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132994,Endogenous risk and protection premiums,September 1991,Jason Shogren,,,Male,Unknown,Unknown,Male,,22
31.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132995,Nonrational actors and financial market behavior,September 1991,Richard Zeckhauser,Jayendu Patel,Darryll Hendricks,Male,Unknown,Unknown,Male,,36
31.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132996,Announcements,September 1991,,,,Unknown,Unknown,Unknown,Unknown,,
31.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00132997,Announcements,September 1991,,,,Unknown,Unknown,Unknown,Unknown,,
32.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133625,Evaluating Cooperative Game Theory in water resources,January 1992,Ariel Dinar,Aharon Ratner,Dan Yaron,Male,Male,Male,Male,,58
32.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133626,"A deterministic event tree approach to uncertainty, randomness and probability in individual chance processes",January 1992,Hector A. Munera,,,Male,Unknown,Unknown,Male,,6
32.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133627,Healthy scepticism as an expected-utility explanation of the phenomena of Allais and Ellsberg,January 1992,Joseph B. Kadane,,,Male,Unknown,Unknown,Male,,16
32.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133628,Generalization of solution concepts in conflict and negotiation analysis,January 1992,M. A. J. J. Van Gastel,J. H. P. Paelinck,,Unknown,Unknown,Unknown,Unknown,,
32.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133629,Failures of the reduction principle in an Ellsberg-type problem,January 1992,Michele Bernasconi,Graham Loomes,,Female,Male,Unknown,Mix,,
32.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133630,Book review,January 1992,George Loewenstein,,,Male,Unknown,Unknown,Male,,
32.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133631,Erratum,January 1992,,,,Unknown,Unknown,Unknown,Unknown,,
32.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133632,Announcements,January 1992,,,,Unknown,Unknown,Unknown,Unknown,,
32.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134047,Agricultural management using the ADELAIS multiobjective linear programming software: A case application,March 1992,D. K. Despotis,J. Siskos,,Unknown,Unknown,Unknown,Unknown,,
32.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134048,The repeated public goods game: A solution using Tit-for-Tat and the Lindahl point,March 1992,Mark Irving Lichbach,,,Male,Unknown,Unknown,Male,,3
32.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134049,Folk theorems for the observable implications of repeated games,March 1992,Eric Rasmusen,,,Male,Unknown,Unknown,Male,,5
32.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134050,On the structural aspects of collective action and free-riding,March 1992,Raimo Tuomela,,,Male,Unknown,Unknown,Male,,12
32.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134051,Insurance buying gamblers,March 1992,George G. Szpiro,,,Male,Unknown,Unknown,Male,,2
32.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134052,Survival through the Allais paradox,March 1992,Ole Hagen,,,Male,Unknown,Unknown,Male,,2
32.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134053,Call for papers,March 1992,,,,Unknown,Unknown,Unknown,Unknown,,
32.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134150,Exploitation of a crisp relation in a ranking problem,May 1992,Philippe Vincke,,,Male,Unknown,Unknown,Male,,35
32.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134151,Two-person sequential bargaining behavior with exogenous breakdown,May 1992,Rami Zwick,Amnon Rapoport,John C. Howard,Male,Male,Male,Male,,20
32.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134152,A procedural model of voting,May 1992,Sven Ove Hansson,,,Male,Unknown,Unknown,Male,,6
32.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134153,Distributive justice of bargaining and risk sensitivity,May 1992,Marlies Klemisch-Ahlert,,,Female,Unknown,Unknown,Female,,2
33.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133980,Subjective expected utility theory revisited: A reductio ad absurdum paradox,July 1992,Paul J. H. Schoemaker,,,Male,Unknown,Unknown,Male,,3
33.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133981,Mean-preserving changes in risk with tail-dominance,July 1992,Louis Eeckhoudt,Pierre Hansen,,Male,Male,Unknown,Male,,6
33.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133982,On generalized measures of realization in uncertain environments,July 1992,Ronald R. Yager,,,Male,Unknown,Unknown,Male,,19
33.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133983,Risk and risk aversion for state-dependent utility,July 1992,David Kelsey,,,Male,Unknown,Unknown,Male,,4
33.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133984,On the foundations of decision making under partial information,July 1992,David Rios Insua,,,Male,Unknown,Unknown,Male,,16
33.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134092,"Security level, potential level, expected utility: A three-criteria decision model under risk",September 1992,Michèle Cohen,,,Female,Unknown,Unknown,Female,,43
33.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134093,Weakly implementable social choice rules,September 1992,Taradas Bandyopadhyay,Larry Samuelson,,Unknown,Male,Unknown,Male,,1
33.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134094,Rational beliefs in extensive games,September 1992,Giacomo Bonanno,,,Male,Unknown,Unknown,Male,,6
33.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134095,An axiomatic analysis of the Nash equilibrium concept,September 1992,Hannu Salonen,,,Male,Unknown,Unknown,Male,,3
33.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133640,The exchange and allocation of decision power,November 1992,Tomas Philipson,,,Male,Unknown,Unknown,Male,,3
33.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133641,On the gearing adjustment. An axiomatic approach,November 1992,J. M. Gutiérrez,,,Unknown,Unknown,Unknown,Unknown,,
33.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133642,Sequential voting by veto: Making the Mueller-Moulin algorithm more versatile,November 1992,S. Dan Felsenthal,Moshé Machover,,Unknown,Unknown,Unknown,Unknown,,
33.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133643,"Minimax, information and ultrapessimism",November 1992,Giovanni Parmigiani,,,Male,Unknown,Unknown,Male,,7
33.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133644,How can an expert system help in choosing the optimal decision?,November 1992,G. Coletti,G. Regoli,,Unknown,Unknown,Unknown,Unknown,,
33.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133645,Book review,November 1992,Mark J. Machina,,,Male,Unknown,Unknown,Male,,
34.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01076102,One-shot decisions under Linear Partial Information,January 1993,Edward Kofler,Peter Zweifel,,Male,Male,Unknown,Male,,8
34.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01076103,Revealed preference and linear utility,January 1993,Stephen A. Clark,,,Male,Unknown,Unknown,Male,,4
34.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01076104,Endogenizing the order of moves in matrix games,January 1993,Jonathan H. Hamilton,Steven M. Slutsky,,Male,Male,Unknown,Male,,20
34.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01076105,The indefinitely iterated prisoner's dilemma: Reply to Becker and Cudd,January 1993,John W. Carroll,,,Male,Unknown,Unknown,Male,,
34.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01076106,Sociology in the economic mode,January 1993,Maarten C. W. Janssen,,,Male,Unknown,Unknown,Male,,
34.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01074895,The Becker-DeGroot-Marschak mechanism and generalized utility theories: Theoretical predictions and empirical observations,March 1993,L. Robin Keller,Uzi Segal,Tan Wang,Unknown,Male,,Mix,,
34.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01074896,A note on the information content of a consistent pairwise comparison judgment matrix of an AHP decision maker,March 1993,Elizabeth E. Noble,Pedro P. Sanchez,,Female,Male,Unknown,Mix,,
34.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01074897,Local utility functions,March 1993,Peter Bardsley,,,Male,Unknown,Unknown,Male,,4
34.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01074898,The axioms and algebra of ambiguity,March 1993,Peter C. Fishburn,,,Male,Unknown,Unknown,Male,,28
34.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01074899,Planning in firms as an interactive process,March 1993,Jean-Pierre Ponssard,Hervé Tanguy,,Male,Male,Unknown,Male,,22
34.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01074900,Nondictatorial social welfare functions with different discrimination structures,March 1993,Francis Bloch,,,Male,Unknown,Unknown,Male,,
34.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075187,Introduction,May 1993,Bertram I. Spector,,,Male,Unknown,Unknown,Male,,2
34.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075188,Decision analysis for practical negotiation application,May 1993,Bertram I. Spector,,,Male,Unknown,Unknown,Male,,12
34.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075189,The role of multi-objective optimization in negotiation and mediation support,May 1993,Andrzej P. Wierzbicki,Lech Kruś,Marek Makowski,Male,Male,Male,Male,,13
34.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075190,Statistical analysis for negotiation support,May 1993,Daniel Druckman,,,Male,Unknown,Unknown,Male,,7
34.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075191,Are game theoretic concepts suitable negotiation support tools? From Nash equilibrium refinements toward a cognitive concept of rationality,May 1993,Bertrand R. Munier,Jean-Louis Rullière,,Male,Unknown,Unknown,Male,,5
34.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075192,Cognitive mapping as a technique for supporting international negotiation,May 1993,G. Matthew Bonham,,,Unknown,Unknown,Unknown,Unknown,,
34.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075193,"Analysis, modeling, and the management of international negotiations",May 1993,Dhanesh K. Samarasan,,,Unknown,Unknown,Unknown,Unknown,,
34.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075194,Negotiation support: Development of representations and reasoning,May 1993,G. E. Kersten,,,Unknown,Unknown,Unknown,Unknown,,
34.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075195,Information management support for international negotiations,May 1993,Stephen J. Andriole,,,Male,Unknown,Unknown,Male,,4
34.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075196,The use of analytical methods and tools in international negotiations: A practitioner's perspective,May 1993,Gerhard Hafner,,,Male,Unknown,Unknown,Male,,1
34.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075197,Decision support and negotiation research: A researcher's perspective,May 1993,I. William Zartman,,,Unknown,Unknown,Unknown,Unknown,,
35.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075233,Newcomb's paradox: A realist resolution,July 1993,N. Jacobi,,,Unknown,Unknown,Unknown,Unknown,,
35.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075234,On the structuralist constraints in social scientific theorizing,July 1993,Martti Kuokkanen,,,Male,Unknown,Unknown,Male,,3
35.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075235,Three pseudo-paradoxes in ‘quantum’ decision theory: Apparent effects of observation on probability and utility,July 1993,Louis Marinoff,,,Male,Unknown,Unknown,Male,,8
35.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075236,A test of generalized expected utility theory,July 1993,Barry Sopher,Gary Gigliotti,,Male,Male,Unknown,Male,,28
35.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01074955,Learning to cooperate with Pavlov an adaptive strategy for the iterated Prisoner's Dilemma with noise,September 1993,David Kraines,Vivian Kraines,,Male,Female,Unknown,Mix,,
35.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01074956,Reflective coherence and Newcomb problems: A simple solution,September 1993,Gary Malinas,,,Male,Unknown,Unknown,Male,,
35.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01074957,Manipulation under majority decision-making when no majority suffers and preferences are strict,September 1993,I. D. A. Macintyre,,,Unknown,Unknown,Unknown,Unknown,,
35.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01074958,Expert rule versus majority rule under partial information,September 1993,Daniel Berend,Jørgen E. Harmse,,Male,Male,Unknown,Male,,12
35.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01074959,Mean variance preferences and the heat equation,September 1993,Peter Bardsley,,,Male,Unknown,Unknown,Male,,2
35.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075198,Editorial foreword,November 1993,Bertrand Munier,,,Male,Unknown,Unknown,Male,,
35.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075199,Realism and antirealism in social science,November 1993,Mario Bunge,,,Male,Unknown,Unknown,Male,,58
35.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075200,Comparative probability for conditional events: A new look through coherence,November 1993,Giulianella Coletti,Angelo Gilio,Romano Scozzafava,Unknown,Male,Male,Male,,17
35.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075201,"Decision, irreversibility and flexibility: The irreversibility effect re-examined",November 1993,Shyama V. Ramani,Alban Richard,,Unknown,Male,Unknown,Male,,17
35.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075202,Five legitimate definitions of correlated equilibrium in games with incomplete information,November 1993,Françoise Forges,,,Female,Unknown,Unknown,Female,,80
35.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075203,Intransitive cycles: Rational choice or random error? An answer based on estimation of error rates with experimental data,November 1993,Barry Sopher,Gary Gigliotti,,Male,Male,Unknown,Male,,45
35.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075204,Empirical evidence of two-attribute utility dependence on probability,November 1993,Mark R. McCord,Oscar Franzese,,Male,Male,Unknown,Male,,2
35.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075205,First announcement and call for papers,November 1993,,,,Unknown,Unknown,Unknown,Unknown,,
36.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075296,Separating marginal utility and probabilistic risk aversion,January 1994,Peter Wakker,,,Male,Unknown,Unknown,Male,,198
36.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075297,Ordinal preference representations,January 1994,Niall M. Fraser,,,Male,Unknown,Unknown,Male,,37
36.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075298,Two envelopes,January 1994,Jordan Howard Sobel,,,Male,Unknown,Unknown,Male,,12
36.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01075299,A note on the two envelopes problem,January 1994,P. Rawling,,,Unknown,Unknown,Unknown,Unknown,,
36.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079209,Extrapolating human probability judgment,March 1994,Daniel Osherson,Edward E. Smith,Michael Stob,Male,Male,Male,Male,,15
36.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079210,"Opinion leaders, independence, and Condorcet's Jury Theorem",March 1994,David M. Estlund,,,Male,Unknown,Unknown,Male,,106
36.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079211,Focal points in pure coordination games: An experimental investigation,March 1994,Judith Mehta,Chris Starmer,Robert Sugden,Female,,Male,Mix,,
36.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079212,Bayesian boundedly rational agents play the Finitely Repeated Prisoner's Dilemma,March 1994,Fernando Vega-Redondo,,,Male,Unknown,Unknown,Male,,4
36.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079213,First announcement and call for papers,March 1994,,,,Unknown,Unknown,Unknown,Unknown,,
36.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079928,The use of costless inspection in enforcement,May 1994,D. Marc Kilgour,,,Unknown,Unknown,Unknown,Unknown,,
36.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079929,A note on the prisoner's dilemma,May 1994,C. L. Sheng,,,Unknown,Unknown,Unknown,Unknown,,
36.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079930,A note on Pollock's system of direct inference,May 1994,Stephen Leeds,,,Male,Unknown,Unknown,Male,,1
36.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079931,Rational behaviour: A comparison between the theory stemming from de Finetti's work and some other leading theories,May 1994,Guido A. Rossi,,,Male,Unknown,Unknown,Male,,1
36.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079932,On fair compensation,May 1994,Marc Fleurbaey,,,Male,Unknown,Unknown,Male,,91
37.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079203,Epistemic logic and the foundations of game theory,July 1994,Michael Bacharach,Philippe Mongin,,Male,Male,Unknown,Male,,7
37.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079204,The epistemic structure of a theory of a game,July 1994,Michael Bacharach,,,Male,Unknown,Unknown,Male,,19
37.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079205,On the evaluation of solution concepts,July 1994,Robert Stalnaker,,,Male,Unknown,Unknown,Male,,48
37.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079206,On the logic of common belief and common knowledge,July 1994,Luc Lismont,Philippe Mongin,,Male,Male,Unknown,Male,,35
37.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079207,Awareness and partitional information structures,July 1994,Salvatore Modica,Aldo Rustichini,,Male,Male,Unknown,Male,,133
37.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079208,Representing the knowledge of turing machines,July 1994,Hyun Song Shin,Timothy Williamson,,,Male,Unknown,Mix,,
37.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079264,Friedman's ‘instrumentalism’ and constructive empiricism in economics,September 1994,Maurice Lagueux,,,Male,Unknown,Unknown,Male,,4
37.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079265,The expected likelihood of transitivity: A survey,September 1994,William V. Gehrlein,,,Male,Unknown,Unknown,Male,,5
37.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079266,Divide the Dollar: Three solutions and extensions,September 1994,Steven J. Brams,Alan D. Taylor,,Male,Male,Unknown,Male,,23
37.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079267,Utility optimization when the utility function is virtually unknown,September 1994,Enrique Ballestero,Carlos Romero,,Male,Male,Unknown,Male,,19
37.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079911,Weak covering relations,November 1994,Gilbert Laffond,Jean Laine,,Male,Male,Unknown,Male,,7
37.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079912,A challenge to the compound lottery axiom: A two-stage normative structure and comparison to other theories,November 1994,Donald B. Davis,M.-Elisabeth Paté-Cornell,,Male,Unknown,Unknown,Male,,16
37.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079913,Bargaining with reasonable aspirations,November 1994,Johann K. Brunner,,,Male,Unknown,Unknown,Male,,1
37.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079914,A Paretian liberal dilemma without collective rationality,November 1994,S. Subramanian,,,Unknown,Unknown,Unknown,Unknown,,
37.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079915,The mutual determination of wants and benefits,November 1994,John Broome,,,Male,Unknown,Unknown,Male,,1
38.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01083166,Changes in preference,January 1995,Sven Ove Hansson,,,Male,Unknown,Unknown,Male,,46
38.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01083167,"Flexibility, endogenous risk, and the protection premium",January 1995,Sergio H. Lence,Bruce A. Babcock,,Male,Male,Unknown,Male,,2
38.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01083168,Bayesian inference given data ‘significant atα’: Tests of point hypotheses,January 1995,D. J. Johnstone,D. V. Lindley,,Unknown,Unknown,Unknown,Unknown,,
38.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01083169,Endogenous correlated equilibria in noncooperative games,January 1995,Peter Vanderschraaf,,,Male,Unknown,Unknown,Male,,7
38.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01083170,A bargaining model with players' perceptions on the retractability of offers,January 1995,Abhinay Muthoo,,,Unknown,Unknown,Unknown,Unknown,,
38.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01083171,On the difficulty of making social choices,January 1995,Hannu Nurmi,,,Male,Unknown,Unknown,Male,,11
38.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01083172,A note on the decidability of de Finetti's coherence,January 1995,Francesco Corielli,,,Male,Unknown,Unknown,Male,,1
38.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079497,A strong (Ross) characterization of multivariate risk aversion,March 1995,Simon Grant,,,Male,Unknown,Unknown,Male,,4
38.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079498,Economic choice in generalized expected utility theory,March 1995,John Quiggin,,,Male,Unknown,Unknown,Male,,6
38.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079499,A theory of forward induction in finitely repeated games,March 1995,Nabil Al-Najjar,,,Male,Unknown,Unknown,Male,,3
38.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01079500,Postulates and paradoxes of relative voting power — A critical re-appraisal,March 1995,Dan S. Felsenthal,Moshé Machover,,Male,Unknown,Unknown,Male,,64
38.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01362233,Editorial foreword,May 1995,Bertrand Munier,,,Male,Unknown,Unknown,Male,,
38.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01362234,Two errors in the ‘Allais Impossibility Theorem’,May 1995,Mark J. Machina,,,Male,Unknown,Unknown,Male,,3
38.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01362235,The real foundations of the alleged errors in Allais' Impossibility Theorem: Unceasingly repeated errors or contradictions of mark machina,May 1995,Maurice Allais,,,Male,Unknown,Unknown,Male,,1
38.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01362236,The two errors: A summary,May 1995,Mark J. Machina,,,Male,Unknown,Unknown,Male,,
38.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01362237,Allais' rejoinder,May 1995,Maurice Allais,,,Male,Unknown,Unknown,Male,,
38.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01362238,The impossibility of experimental elicitation of subjective probabilities,May 1995,Edi Karni,Zvi Safra,,Male,Male,Unknown,Male,,33
38.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01362239,Paradoxes of pure curiosity,May 1995,Neil Tennant,,,Male,Unknown,Unknown,Male,,
38.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01362240,"In memoriam Werner Kroeber-Riel December 4, 1934–January 16, 1995",May 1995,W. Leinfellner,G. Eberlein,B. Munier,Unknown,Unknown,Unknown,Unknown,,
39.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01078867,Effective choice in the single-shot Prisoner's Dilemma tournament,July 1995,Doede Nauta,Jeljer Hoekstra,,Male,Unknown,Unknown,Male,,3
39.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01078868,Stochastic dominance in multicriterion analysis under risk,July 1995,Jean-Marc Martel,Kazimierz Zaras,,Unknown,Male,Unknown,Male,,36
39.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01078869,Judged knowledge and ambiguity aversion,July 1995,Hans-Jürgen Keppe,Martin Weber,,Unknown,Male,Unknown,Male,,31
39.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01078870,A new axiomatic foundation of partial comparability,July 1995,Alexis Tsoukiàs,Philippe Vincke,,Male,Male,Unknown,Male,,48
39.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01078980,Any complete preference structure without circuit admits an interval representation,September 1995,Moncef Abbas,,,Male,Unknown,Unknown,Male,,9
39.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01078981,Measuring credibility of compensatory preference statements when trade-offs are interval determined,September 1995,Carlos A. Bana e Costa,Philippe Vincke,,Male,Male,Unknown,Male,,15
39.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01078982,Expected utility for decision making with subjective models,September 1995,Salvatore Modica,,,Male,Unknown,Unknown,Male,,1
39.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01078983,On admissible strategies and manipulation of social choice procedures,September 1995,Boniface Mbih,,,,Unknown,Unknown,Mix,,
39.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01078984,Refinements of the no-envy solution in economies with indivisible goods,September 1995,Koichi Tadenuma,William Thomson,,Male,Male,Unknown,Male,,25
39.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01078985,Pareto improvements by Pareto strategic voting under majority voting with risk loving and risk avoiding voters — A note,September 1995,I. D. A. Macintyre,,,Unknown,Unknown,Unknown,Unknown,,
39.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01078986,"Schur convexity, quasi-convexity and preference for early resolution of uncertainty",September 1995,Zvi Safra,Eyal Sulganik,,Male,Male,Unknown,Male,,1
39.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01082053,The incoherence of agreeing to disagree,November 1995,Robert F. Nau,,,Male,Unknown,Unknown,Male,,5
39.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01082054,Towards a more precise decision framework,November 1995,Robin Pope,,,,Unknown,Unknown,Mix,,
39.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01082055,Prudence and extensionality in theories of preference and value,November 1995,J. M. Vickers,,,Unknown,Unknown,Unknown,Unknown,,
39.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF01082056,Preference extension rules for ranking sets of alternatives with a fixed cardinality,November 1995,Walter Bossert,,,Male,Unknown,Unknown,Male,,25
40.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133158,On games under expected utility with rank dependent probabilities,January 1996,Klaus Ritzberger,,,Male,Unknown,Unknown,Male,,16
40.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133159,Can we rationally learn to coordinate?,January 1996,Sanjeev Goyal,Maarten Janssen,,Male,Male,Unknown,Male,,28
40.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133160,Experimental evidence on the irreversebility effect,January 1996,Alexandra Rauchs,Marc Willinger,,Female,Male,Unknown,Mix,,
40.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133161,Signed orders in linear and nonlinear utility theory,January 1996,Peter C. Fishburn,Irving H. La Valle,,Male,Male,Unknown,Male,,1
40.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133169,Axiomatic characterizations of solutions for Bayesian games,March 1996,Robert van Heumen,Bezalel Peleg,Peter Borm,Male,Unknown,Male,Male,,22
40.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133170,Mechanism robustness in multilateral bargaining,March 1996,Eyal Winter,,,Male,Unknown,Unknown,Male,,7
40.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133171,A stochastic behavioral model and a ‘Microscopic’ foundation of evolutionary game theory,March 1996,Dirk Helbing,,,Male,Unknown,Unknown,Male,,48
40.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133172,Combinatorial versus decision-theoretic components of impossibility theorems,March 1996,David Makinson,,,Male,Unknown,Unknown,Male,,4
40.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00133173,Intrinsic losses,March 1996,Christian P. Robert,,,Male,Unknown,Unknown,Male,,45
40.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134208,Foundations of the theory of evidence: Resolving conflict among schemata,May 1996,Bonnie K. Ray,David H. Krantz,,,Male,Unknown,Mix,,
40.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134209,Hidden information acquisition and static choice,May 1996,Timothy Van Zandt,,,Male,Unknown,Unknown,Male,,2
40.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134210,Rational preference: Decision theory as a theory of practical rationality,May 1996,James Dreier,,,Male,Unknown,Unknown,Male,,37
40.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134211,Dynamic focal points in N-person coordination games,May 1996,F. Kramarz,,,Unknown,Unknown,Unknown,Unknown,,
41.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134113,Independent social choice correspondences,July 1996,Donald E. Campbell,Jerry S. Kelly,,Male,Male,Unknown,Male,,3
41.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134114,Majority vote of even and odd experts in a polychotomous choice situation,July 1996,Louisa Lam,Ching Y Suen,,Female,,Unknown,Mix,,
41.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134115,Intransitive choices based on transitive preferences: The case of menu-dependent information,July 1996,Georg Kirchsteiger,Clemens Puppe,,Male,Male,Unknown,Male,,7
41.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134116,Aggregation of decomposable measures with application to utility theory,July 1996,D. Dubois,J. C. Fodor,M. Roubens,Unknown,Unknown,Unknown,Unknown,,
41.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134637,Editorial foreword,September 1996,,,,Unknown,Unknown,Unknown,Unknown,,
41.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134638,Markets with endogenous uncertainty theory and policy,September 1996,Graciela Chichilnisky,,,Female,Unknown,Unknown,Female,,10
41.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134639,Uncertainty aversion and aversion to increasing uncertainty,September 1996,Aldo Montesano,Francesco Giovannoni,,Male,Male,Unknown,Male,,9
41.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134640,Effects of information on assessment of probabilities,September 1996,A. Rapoport,,,Unknown,Unknown,Unknown,Unknown,,
41.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134641,A reply to Rapoport,September 1996,L. Marinoff,,,Unknown,Unknown,Unknown,Unknown,,
41.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134642,Evaluating second-order probability judgments with strictly proper scoring rules,September 1996,Kathleen M. Whitcomb,P. George Benson,,Female,Unknown,Unknown,Female,,
41.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134643,A sound interpretation of minimality properties of common belief in minimal semantics,September 1996,Vittoriomanuele Ferrante,,,Unknown,Unknown,Unknown,Unknown,,
41.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00134644,Prisoner's dilemma from a moral point of view,September 1996,John J. Tilley,,,Male,Unknown,Unknown,Male,,2
41.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136126,Metaconfirmation,November 1996,Denis Zwirn,Hervé P. Zwirn,,Male,Male,Unknown,Male,,8
41.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136127,The individual rationality of maintaining a sense of justice,November 1996,Eric M. Cave,,,Male,Unknown,Unknown,Male,,1
41.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136128,Interpersonal dependency of preferences,November 1996,Julian Nida-Rümelin,Thomas Schmidt,Axel Munk,Male,Male,Male,Male,,1
41.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/BF00136129,Expected utility without utility,November 1996,E. Castagnoli,M. Li Calzi,,Unknown,Unknown,Unknown,Unknown,,
42.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004923322134,LEXICOGRAPHIC ADDITIVITY FOR MULTI-ATTRIBUTE PREFERENCES ON FINITE SETS,January 1997,Yutaka Nakamura,,,,Unknown,Unknown,Mix,,
42.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004934730665,The sure thing principle and the value of information,January 1997,Edward E. SchleeE,,,Male,Unknown,Unknown,Male,,7
42.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004911026088,RISK AND THE VALUE OF INFORMATION IN IRREVERSIBLE DECISIONS,January 1997,Hans Gersbach,,,Male,Unknown,Unknown,Male,,2
42.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004959623042,The undecidability of the spatialized prisoner's dilemma,January 1997,Patrick Grim,,,Male,Unknown,Unknown,Male,,16
42.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004927830125,BARGAINING AND DELAY: THE ROLE OF EXTERNAL INFORMATION,January 1997,Charles E. Hyde,,,Male,Unknown,Unknown,Male,,2
42.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004907606204,Bargaining with Incomplete information an axiomatic approach,March 1997,Joachim Rosenmüller,,,Male,Unknown,Unknown,Male,,4
42.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004919326873,EVOLUTION AND ULTIMATUM BARGAINING,March 1997,William Harms,,,Male,Unknown,Unknown,Male,,13
42.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004905131574,FORWARD INDUCTION IN GAMES WITH AN OUTSIDE OPTION,March 1997,GONZALO OLCINA,,,Unknown,Unknown,Unknown,Unknown,,
42.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004991825894,CONGESTION MODELS AND WEIGHTED BAYESIAN POTENTIAL GAMES,March 1997,Giovanni Facchini,Freek van Megen,Stef Tijs,Male,Male,Male,Male,,55
42.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004907110913,Tempered Regrets Under Total Ignorance,May 1997,Mary H. Acker,,,,Unknown,Unknown,Mix,,
42.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004963010158,De minimis and equity in risk,May 1997,Karl Mosler,,,Male,Unknown,Unknown,Male,,1
42.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1017913032257,"Risk, uncertainty and hidden information",May 1997,Stephen Morris,,,Male,Unknown,Unknown,Male,,23
42.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004906403823,Correlated strategies as Institutions,May 1997,Daniel G. M. Arce,,,Male,Unknown,Unknown,Male,,6
42.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004962809537,Joint Beliefs in Conflictual Coordination Games,May 1997,Peter Vanderschraaf,Diana Richards,,Male,Female,Unknown,Mix,,
43.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004911723951,PAYOFF DOMINANCE AND THE STACKELBERG HEURISTIC,July 1997,ANDREW M. COLMAN,MICHAEL BACHARACH,,Unknown,Unknown,Unknown,Unknown,,
43.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004977910644,"IN DEFENSE OF A CONSTRUCTIVE, INFORMATION-BASED APPROACH TO DECISION THEORY",July 1997,M. R. YILMAZ,,,Unknown,Unknown,Unknown,Unknown,,
43.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004950613488,Violations of Present-Value Maximization in Income Choice,July 1997,GARY GIGLIOTTI,BARRY SOPHER,,Unknown,Unknown,Unknown,Unknown,,
43.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004918708964,THE GENERALIZED HARMONIC MEAN AND A PORTFOLIO PROBLEM WITH DEPENDENT ASSETS,July 1997,MASAAKI KIJIMA,,,Unknown,Unknown,Unknown,Unknown,,
43.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004995515111,NONPARAMETRIC STATISTICS IN MULTICRITERIA ANALYSIS,July 1997,Antonino Scarelli,Lorenzo Venzi,,Male,Male,Unknown,Male,,2
43.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004981430688,Stochastic evolution of rationality,September 1997,Jean-Claude Falmagne,Jean-Paul Doignon,,Male,Male,Unknown,Male,,18
43.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004943925179,Discount-neutral utility models for denumerable time streams,September 1997,Peter Fishburn,Ward Edwards,,Male,Male,Unknown,Male,,17
43.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004983003069,Preference for gradual resolution of uncertainty,September 1997,Martin Ahlbrecht,Martin Weber,,Male,Male,Unknown,Male,,20
43.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004970725802,Sequential asymmetric auctions with endogenous participation,September 1997,Flavio M. Menezes,Paulo K. Monteiro,,Male,Male,Unknown,Male,,9
43.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004966624893,"Two applications of a theorem of Dvoretsky, Wald, and Wolfovitz to cake division",September 1997,Julius B. Barbanel,William S. Zwicker,,Male,Male,Unknown,Male,,8
43.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004913518465,Order relations among efficient decision rules,November 1997,Jacob Paroush,,,Male,Unknown,Unknown,Male,,4
43.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004948310616,On Playing Fair: Professor Binmore on Game Theory and the Social Contract,November 1997,Mohammed Dore,,,Male,Unknown,Unknown,Male,,2
43.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004958909249,Economics of Radiation Protection: Equity Considerations,November 1997,Thierry Schneider,Caroline Schieber,Christian Gollier,Male,Female,Male,Mix,,
43.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004976227366,Perspectives on a Pair of Envelopes,November 1997,Piers Rawling,,,Male,Unknown,Unknown,Male,,7
43.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004920611437,On Seidenfeld‘s Criticism of Sophisticated Violations of the Independence Axiom 1,November 1997,Wlodek Rabinowicz,,,Unknown,Unknown,Unknown,Unknown,,
43.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004996226545,Hidden Assumptions in the Dutch Book Argument,November 1997,C. Waidacher,,,Unknown,Unknown,Unknown,Unknown,,
44.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1017106420417,Editorial,January 1998,,,,Unknown,Unknown,Unknown,Unknown,,
44.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005099909043,Multistage Game Models and Delay Supergames,January 1998,Reinhard Selten,,,Male,Unknown,Unknown,Male,,6
44.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004962423985,Nash Equilibrium with Lower Probabilities,January 1998,Ebbe Groes,Hans Jørgen Jacobsen,Torben Tranaes,Male,Male,Male,Male,,18
44.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004968625637,Impossibility theorems for normal form games,January 1998,David Squires,,,Male,Unknown,Unknown,Male,,5
44.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004914608055,The Bicameral Postulates and Indices of a Priori Voting Power,January 1998,Dan S. Felsenthal,Moshé Machover,William Zwicker,Male,Unknown,Male,Male,,35
44.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004972127482,Axiomatization of a class of share functions for n-person games,April 1998,Gerard van der Laan,René van den Brink,,Male,Male,Unknown,Male,,15
44.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004982328196,Game Trees For Decision Analysis,April 1998,Prakash P. Shenoy,,,,Unknown,Unknown,Mix,,
44.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004924211553,On Preference and Freedom,April 1998,Prasanta K. Pattanaik,Yongsheng Xu,,Female,Unknown,Unknown,Female,,82
44.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004944209707,Two-Person and majority continuous aggregation in 2-good space in Social Choice: a note,April 1998,I.D.A. Macintyre,,,Unknown,Unknown,Unknown,Unknown,,
44.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004925709056,A Shortcut Method of Calculating the Distribution of Election Outcome Types Under Approval Voting,June 1998,Miles H. Sonstegaard,,,Male,Unknown,Unknown,Male,,
44.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004953014736,A Note on the Computation of the Mean Random Consistency Index of the Analytic Hierarchy Process (Ahp),June 1998,V.M. Rao Tummala,Hong Ling,,Unknown,,Unknown,Mix,,
44.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004931018999,Ramsey's Foundations Extended to Desirabilities,June 1998,Jordan Howard Sobel,,,Male,Unknown,Unknown,Male,,4
44.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004990529105,Echelons in Incomplete Relations,June 1998,Robert Delver,Herman Monsuur,,Male,Male,Unknown,Male,,2
44.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004918905650,Consensus By Identifying Extremists,June 1998,Robin D. Hanson,,,,Unknown,Unknown,Mix,,
45.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005041228480,Rational choice on non-finite sets by means of expansion-contraction axioms,August 1998,M. Carmen Sánchez,,,Unknown,Unknown,Unknown,Unknown,,
45.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005094032398,The Optimality of the Expert and Majority Rules Under Exponentially Distributed Competence,August 1998,Luba Sapir,,,Female,Unknown,Unknown,Female,,14
45.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005035407253,An Empirical Comparison of Probabilistic Coalition Structure Theories in 3-Person Sidepayment Games,August 1998,H. Andrew Michener,Daniel J. Myers,,Unknown,Male,Unknown,Male,,3
45.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004930101743,Rewarding Trust: An Experimental Study,August 1998,Friedel Bolle,,,,Unknown,Unknown,Mix,,
45.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005083323183,Evolutionary Equilibria: Characterization Theorems and Their Implications,October 1998,Jonathan Bendor,Piotr Swistak,,Male,Male,Unknown,Male,,15
45.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004906307146,"Social Structure, Economic Performance and Pareto Optimality",October 1998,Paul D. Thistle,,,Male,Unknown,Unknown,Male,,2
45.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004938512267,On the Analysis of Negative Freedom,October 1998,Martin van Hees,,,Male,Unknown,Unknown,Male,,25
45.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1017103528547,The Sure Thing Principle and the Value of Information: Corrigenda,October 1998,,,,Unknown,Unknown,Unknown,Unknown,,
45.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004946714084,Long-Term Behavior in the Theory of Moves,December 1998,Stephen J. Willson,,,Male,Unknown,Unknown,Male,,16
45.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004998730922,Aspects of Arranged Marriages and the Theory of Markov Decision Processes,December 1998,Amitrajeet a. Batabyal,,,Unknown,Unknown,Unknown,Unknown,,
45.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005027105436,Simple Characterizations of the Nash and Kalai/smorodinsky Solutions,December 1998,Nejat Anbarci,,,Male,Unknown,Unknown,Male,,7
45.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005038921033,Backward Induction Is Not Robust: The Parity Problem and the Uncertainty Problem,December 1998,Steven J. Brams,D. Marc Kilgour,,Male,Unknown,Unknown,Male,,6
45.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005090905103,On Stalnaker's Notion of Strong Rationalizability and Nash Equilibrium in Perfect Information Games,December 1998,Giacomo Bonanno,Klaus Nehring,,Male,Male,Unknown,Male,,5
45.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005043021942,Response to Bonanno and Nehring,December 1998,Robert Stalnaker,,,Male,Unknown,Unknown,Male,,1
45.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1017168322289,Contents Volume 45 / List of Contributors,December 1998,,,,Unknown,Unknown,Unknown,Unknown,,
46.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004942613175,Bounds to Memory Loss,February 1999,Hans K. Hvide,,,Male,Unknown,Unknown,Male,,2
46.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004917722235,Incomplete Contracts and Complexity Costs,February 1999,Luca Anderlini,Leonardo Felli,,Male,Male,Unknown,Male,,48
46.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004984621705,"Probability Learning, Event-Splitting Effects and the Economic Theory of Choice",February 1999,Steven J. Humphrey,,,Male,Unknown,Unknown,Male,,7
46.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004950814993,Nondegenerate Intervals of No-Trade Prices for Risk Averse Traders,February 1999,Gerd Weinrich,,,Male,Unknown,Unknown,Male,,1
46.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005094023022,An Infinite Decision Puzzle,February 1999,Jeffrey A. Barrett,Frank Arntzenius,,Male,Male,Unknown,Male,,12
46.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1017115118359,Announcement,February 1999,,,,Unknown,Unknown,Unknown,Unknown,,
46.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1017101602430,Announcement,February 1999,,,,Unknown,Unknown,Unknown,Unknown,,
46.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004994630014,E-Capacities and the Ellsberg Paradox,April 1999,Jürgen Eichberger,David Kelsey,,Male,Male,Unknown,Male,,61
46.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004930205037,Cycling with Rules of Thumb: An Experimental Test for a new form of Non-Transitive Behaviour,April 1999,Chris Starmer,,,,Unknown,Unknown,Mix,,
46.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004990410083,Anxiety and Decision Making with Delayed Resolution of Uncertainty,April 1999,George Wu,,,Male,Unknown,Unknown,Male,,69
46.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005099306184,Pure-strategy Equilibria with Non-expected Utility Players,April 1999,Ho-Chyuan Chen,William S. Neilson,,Unknown,Male,Unknown,Male,,5
46.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005038628305,"Money Does Not Induce Risk Neutral Behavior, but Binary Lotteries Do even Worse",June 1999,Reinhard Selten,Abdolkarim Sadrieh,Klaus Abbink,Male,Unknown,Male,Male,,89
46.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005066504527,Testing the Effects of Similarity on Risky Choice: Implications for Violations of Expected Utility,June 1999,David E. Buschena,David Zilberman,,Male,Male,Unknown,Male,,18
46.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005068512551,Linear Aggregation of SSB Utility Functionals,June 1999,Arja H. Turunen-Red,John A. Weymark,,Female,Male,Unknown,Mix,,
46.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005079106344,Dynamic Choice Mechanisms,June 1999,Ludwig von Auer,,,Male,Unknown,Unknown,Male,,2
47.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005033020124,Endogenous Timing in a Gaming Tournament,August 1999,Kyung Hwan Baik,Todd L. Cherry,Jason F. Shogren,,Male,Male,Mix,,
47.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1004977019944,Conditional Desirability,August 1999,Richard Bradley,,,Male,Unknown,Unknown,Male,,19
47.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005008122139,The Independent Localisations of Interaction and Learning in the Repeated Prisoner's Dilemma,August 1999,Robert Hoffmann,,,Male,Unknown,Unknown,Male,,8
47.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005003012375,Mixture of Maximal Quasi Orders: a new Approach to Preference Modelling,August 1999,Jacinto González-pachón,Sixto Ríos-insua,,Male,Male,Unknown,Male,,8
47.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005086804195,Measures of Deprivation and their Meaning in Terms of Social Satisfaction,August 1999,Satya R. Chakravarty,Diganta Mukherjee,,Male,Unknown,Unknown,Male,,22
47.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005084823048,A strategic foundation for the cooperator's advantage,October 1999,Scott H. Ainsworth,,,Male,Unknown,Unknown,Male,,2
47.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005032806209,Alternating-Offer Bargaining and Common Knowledge of Rationality,October 1999,Vincent J. Vannetelbosch,,,Male,Unknown,Unknown,Male,,5
47.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005190513334,The Donation Paradox for Peremptory Challenges,October 1999,Joseph B. Kadane,Christopher A. Stone,Garrick Wallstrom,Male,Male,Male,Male,,8
47.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005173013606,Hopes and Fears: the Conflicting Effects of Risk Ambiguity,October 1999,W. Kip Viscusi,Harrell Chesson,,Unknown,Male,Unknown,Male,,112
47.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005075021366,Weak utilities from acyclicity,October 1999,J.C.R. Alcantud,,,Unknown,Unknown,Unknown,Unknown,,
47.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005102326115,Dominance and Efficiency in Multicriteria Decision under Uncertainty,December 1999,F. Ben Abdelaziz,P. Lang,R. Nadeau,Unknown,Unknown,Unknown,Unknown,,
47.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005107029264,Experiment-dependent priors in psychology and physics,December 1999,Robert F. Bordley,Joseph B. Kadane,,Male,Male,Unknown,Male,,13
47.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005177230444,Testing the Intransitivity Explanation of the Allais Paradox,December 1999,Ebbe Groes,Hans JØrgen Jacobsen,Torben Tranæs,Male,Male,Male,Male,,5
47.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005154608426,Dutch books and agent rationality,December 1999,Daniel Silber,,,Male,Unknown,Unknown,Male,,3
47.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005142630173,Dynamic stochastic dominance in bandit decision problems,December 1999,Thierry Magnac,Jean-Marc Robin,,Male,Unknown,Unknown,Male,,3
48.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005008412046,Extending the Reach of Collective Decision Support Systems: Provisions for Disciplining Judgment-Driven Exercises,February 2000,John W. Sutherland,,,Male,Unknown,Unknown,Male,,
48.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005236224786,"Generalized sharing, membership size and pareto efficiency in teams",February 2000,Raul V. Fabella,,,Male,Unknown,Unknown,Male,,2
48.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005171122710,The Backward Induction Argument,February 2000,John W. Carroll,,,Male,Unknown,Unknown,Male,,
48.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005275616301,When Pretence can be Beneficial,February 2000,Nava Kahana,Tikva Lecker,,Female,Female,Unknown,Female,,1
48.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005232928362,"Reduction, Supervenience, and the Autonomy of Social Scientific Laws",March 2000,Lee C. McIntyre,,,,Unknown,Unknown,Mix,,
48.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005257721910,Believe in Pascal's Wager? Have I Got a Deal for You!,March 2000,Alexander Tabarrok,,,Male,Unknown,Unknown,Male,,5
48.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005256424944,K–Player Additive Extension of Two-Player games with an Application to the Borda Electoral Competition Game,March 2000,Gilbert Laffond,Jean-François Laslier,Michel Le Breton,Male,Unknown,Male,Male,,5
48.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005298615336,A Contractarian Approach to Pareto Efficiency in Teams: A Note,March 2000,Raul V. Fabella,,,Male,Unknown,Unknown,Male,,3
48.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005274316908,Ultimatum decision-making: A test of reciprocal kindness,March 2000,David L. Dickinson,,,Male,Unknown,Unknown,Male,,22
48.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005142606780,On the coase theorem and coalitional stability: the principle of equal relative concession,March 2000,Partha Gangopadhyay,,,Unknown,Unknown,Unknown,Unknown,,
48.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005205630723,The Right to Remain Silent,March 2000,Joseph Greenberg,,,Male,Unknown,Unknown,Male,,17
48.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005298715864,A Theory of Rational Choice under Ignorance,May 2000,Klaus Nehring,,,Male,Unknown,Unknown,Male,,13
48.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005250525852,Culture and the Explanation of Choice behavior,May 2000,Donald W. Katzner,,,Male,Unknown,Unknown,Male,,9
48.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005298409014,Preference for Information and Dynamic Consistency,May 2000,Simon Grant,Atsushi Kajii,Ben Polak,Male,Male,Male,Male,,14
48.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005202626761,Choice Functions: Rationality re-Examined,May 2000,Begoña Subiza,Josep E. Peris,,Female,Male,Unknown,Mix,,
48.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005056315776,Substitution of indifferent options at choice nodes and admissibility: a reply to Rabinowicz,June 2000,Teddy Seidenfeld,,,Male,Unknown,Unknown,Male,,4
48.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005296119507,Preference stability and substitution of indifferents: a rejoinder to Seidenfeld,June 2000,Wlodek Rabinowicz,,,Unknown,Unknown,Unknown,Unknown,,
48.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005206710831,"The Independence Postulate, Hypothetical and Called-off Acts: a further reply to Rabinowicz",June 2000,Teddy Seidenfeld,,,Male,Unknown,Unknown,Male,,4
48.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005289611789,Stochastic Choice and Consistency in Decision Making Under Risk: An Experimental Study,June 2000,Sopher,Narramore,,Unknown,Unknown,Unknown,Unknown,,
48.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005278100070,"Nash Bargaining Theory, Nonconvex Problems and Social Welfare Orderings",June 2000,Vincenzo Denicolò,Marco Mariotti,,Male,Male,Unknown,Male,,18
48.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005212125699,Global Robustness with Respect to the Loss Function and the Prior,June 2000,Christophe Abraham,Jean-Pierre Daures,,Male,Male,Unknown,Male,,5
48.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005205315505,Identification of a princess under incomplete information: an amarna story,June 2000,Serdar Güner,Daniel Druckman,,Male,Male,Unknown,Male,,4
49.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005291216245,Developing Assessment Procedures and Assessing Two Models of Escalation Behavior among Community College Administrators,August 2000,David W. Hollar,John Hattie,James Lancaster,Male,Male,Male,Male,,7
49.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005155426945,The implications of irreversibility in emergency response decisions,August 2000,Noël Pauwels,Bartel Van De Walle,Karel Soudan,Male,Male,Male,Male,,16
49.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005223607947,"Symbolic Products: Prestige, Pride and Identity Goods",August 2000,Elias L. Khalil,,,Male,Unknown,Unknown,Male,,36
49.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005284314550,DeFinettian Consensus,August 2000,L.G. Esteves,S. Wechsler,V.A. González-López,Unknown,Unknown,Unknown,Unknown,,
49.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005288012381,On Elements of Chance,September 2000,R. Duncan Luce,A.A.J. Marley,,Unknown,Unknown,Unknown,Unknown,,
49.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005262832464,Decision Making in `Random in a Broad Sense' Environments,September 2000,V.I. Ivanenko,B. Munier,,Unknown,Unknown,Unknown,Unknown,,
49.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005234101607,An Ethical Interpretation of the Nash Choice Rule,September 2000,Marco Mariotti,,,Male,Unknown,Unknown,Male,,3
49.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005201231435,Revealed Preference and Expected Utility,September 2000,Stephen A. Clark,,,Male,Unknown,Unknown,Male,,4
49.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005257316414,"Scoring Rules, Condorcet Efficiency and Social Homogeneity",September 2000,Dominique Lepelley,Patrick Pierron,Fabrice Valognes,,Male,Male,Mix,,
49.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1026570914311,Evolutionarily Stable Co-operative Commitments,November 2000,Werner Güth,Hartmut Kliemt,,Male,Male,Unknown,Male,,32
49.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1026520128623,Reconciliation with the Utility of Chance by Elaborated Outcomes Destroys the Axiomatic Basis of Expected Utility Theory,November 2000,Robin Pope,,,,Unknown,Unknown,Mix,,
49.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1026566915626,An Analysis of Stability Sets in pure Coordination Games,November 2000,Walter Elberfeld,Andras Löffler,,Male,Unknown,Unknown,Male,,
49.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1026425015999,Majority voting on orders,November 2000,Gilbert Laffond,Jean Lainé,,Male,Male,Unknown,Male,,7
49.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1026503107494,A Flawed Infinite Decision Puzzle,November 2000,Myron L. Pulier,,,Male,Unknown,Unknown,Male,,
49.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1026507224333,Barrett and Arntzenius's Infinite Decision Puzzle,November 2000,Mark J. Machina,,,Male,Unknown,Unknown,Male,,2
49.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1026432128221,From separability to unweighted sum: A case for utilitarianism,December 2000,Yew-Kwang Ng,,,Unknown,Unknown,Unknown,Unknown,,
49.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1026549117322,Non-Additive Beliefs in Solvable Games,December 2000,Hans Haller,,,Male,Unknown,Unknown,Male,,15
49.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005228530526,Decision settings analysis – a tool for analysis and design of human activity systems,December 2000,Nils O. Larsson,,,Male,Unknown,Unknown,Male,,1
49.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1026585831556,Equilibrium Bidding without the Independence Axiom: A Graphical Analysis,December 2000,Veronika Grimm,Ulrich Schmidt,,Female,Male,Unknown,Mix,,
49.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005213501393,Externalities in a Bargaining Model of Public Price Announcements and Resale,December 2000,Maarten Cornet,,,Male,Unknown,Unknown,Male,,3
49.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1017290331601,Volume Contents and List of Contributors,December 2000,,,,Unknown,Unknown,Unknown,Unknown,,
50.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005261400484,Why to Buy Your Darling Flowers: On Cooperation and Exploitation,February 2001,Friedel Bolle,,,,Unknown,Unknown,Mix,,
50.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005213530647,On Bivariate Risk Premia,February 2001,Christophe Courbage,,,Male,Unknown,Unknown,Male,,12
50.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005219005058,Lottery Dependent Utility: a Reexamination,February 2001,Ulrich Schmidt,,,Male,Unknown,Unknown,Male,,11
50.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1005296023424,Positive confirmation bias in the acquisition of information,February 2001,Martin Jones,Robert Sugden,,Male,Male,Unknown,Male,,86
50.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010353814597,The Ecology of Cooperation,March 2001,Robert Hoffmann,,,Male,Unknown,Unknown,Male,,2
50.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010349014718,Rationalizing Focal Points,March 2001,Maarten C.W. Janssen,,,Male,Unknown,Unknown,Male,,67
50.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010390527827,Small Group Predictions on an Uncertain Outcome: The Effect of Nondiagnostic Information,March 2001,George R. Young II,Kenneth H. Price,Cynthia Claybrook,Male,Male,Female,Mix,,
50.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010301408403,What to Make of the Liberal Paradox?,March 2001,Mathias Risse,,,Male,Unknown,Unknown,Male,,4
50.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010311325241,`Sex-Equal' Stable Matchings,May 2001,Antonio Romero-Medina,,,Male,Unknown,Unknown,Male,,9
50.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010384118394,Constraints and the Measurement of Freedom of Choice,May 2001,Sebastiano Bavetta,Marco Del Seta,,Male,Male,Unknown,Male,,10
50.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010336203373,On S-Convexity and Risk Aversion,May 2001,Michel Denuit,Claude Lefèvre,Marco Scarsini,Male,,Male,Mix,,
50.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010325930290,A Process Approach to the Utility for Gambling,May 2001,Marc Le Menestrel,,,Male,Unknown,Unknown,Male,,23
50.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010342908638,A Test of the Principle of Optimality,May 2001,Enrica Carbone,John D. Hey,,Female,Male,Unknown,Mix,,
50.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010340726882,South-South Cooperation and Export,May 2001,Sugata Marjit,Hamid Beladi,,Female,Male,Unknown,Mix,,
50.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010363409312,Divide-the-Dollar Game Revisited,June 2001,Nejat Anbarci,,,Male,Unknown,Unknown,Male,,18
50.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010392320211,Instability and Convergence Under Simple-Majority Rule: Results from Simulation of Committee Choice in Two-Dimensional Space,June 2001,David H. Koehler,,,Male,Unknown,Unknown,Male,,2
50.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010344404281,A unified approach to restricted games,June 2001,E. Algaba,J.M. Bilbao,J.J. López,Unknown,Unknown,Unknown,Unknown,,
50.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010325001555,Games of Incomplete Information Without Common Knowledge Priors,June 2001,József Sákovics,,,Male,Unknown,Unknown,Male,,5
50.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010315526150,Measures of Powerlessness in Simple Games,June 2001,Thomas Quint,,,Male,Unknown,Unknown,Male,,
50.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1010335410952,Constrained Monotonicity and the Measurement of Power,June 2001,Manfred J. Holler,Rie Ono,Frank Steffen,Male,Female,Male,Mix,,
51.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1012273830440,Are Referees Sufficiently Informed About The Editor'S Practice?,August 2001,Ruth Ben-Yashar,Shmuel Nitzan,,Female,Male,Unknown,Mix,,
51.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1012244714511,Target Rules for Public Choice Economies on Tree Networks and in Euclidean Spaces,August 2001,Bettina Klaus,,,Female,Unknown,Unknown,Female,,11
51.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1012400922478,Inferring a linear ordering over a power set,August 2001,Ran Spiegler,,,,Unknown,Unknown,Mix,,
51.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1012498106199,A note on decisions under uncertainty: the impact of the choice of the welfare measure,August 2001,Andreas Lange,,,Male,Unknown,Unknown,Male,,2
51.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1012296730191,Capacity Updating Rules and Rational Belief Change,August 2001,Matthew J. Ryan,,,Male,Unknown,Unknown,Male,,2
51.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015525808214,De Finetti was Right: Probability Does Not Exist,December 2001,Robert F. Nau,,,Male,Unknown,Unknown,Male,,42
51.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015546825052,Production under Uncertainty and Choice under Uncertainty in the Emergence of Generalized Expected Utility Theory,December 2001,John Quiggin,,,Male,Unknown,Unknown,Male,,6
51.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015598809123,Lifetime Uncertainty and Time Preference,December 2001,Nicolas Drouhin,,,Male,Unknown,Unknown,Male,,6
51.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015550925961,A No-Trade Theorem under Knightian Uncertainty with General Preferences,December 2001,Chenghu Ma,,,Unknown,Unknown,Unknown,Unknown,,
51.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015503010031,Uncertainty with Partial Information on the Possibility of the Events,December 2001,Aldo Montesano,,,Male,Unknown,Unknown,Male,,3
51.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015555026870,On Preference for Flexibility and Complexity Aversion: Experimental Evidence1,December 2001,Doron Sonsino,Marvin Mandelbaum,,Male,Male,Unknown,Male,,13
51.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015507110940,Multiple-Stage Decision-Making: The Effect of Planning Horizon Length on Dynamic Consistency,December 2001,Joseph G. Johnson,Jerome R. Busemeyer,,Male,Male,Unknown,Male,,15
51.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015559127778,An Experiment on Rational Insurance Decisions,December 2001,Richard Watt,Francisco J. Vázquez,Ignacio Moreno,Male,Male,Male,Male,,6
51.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015511211848,The classification of parametric choices under uncertainty: analysis of the portfolio choice problem,December 2001,Sergio Ortobelli Lozza,,,Male,Unknown,Unknown,Male,,17
51.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015563228687,The Domain and Interpretation of Utility Functions: An Exploration,December 2001,Marc Le Menestrel,Luk Van Wassenhove,,Male,,Unknown,Mix,,
51.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015515312757,More nonconcavities in information processing functions,December 2001,Hagen LINDSTÄDT,,,Male,Unknown,Unknown,Male,,2
51.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015567329595,Conditioning Capacities and Choquet Integrals: The Role of Comonotony,December 2001,Alain Chateauneuf,Robert Kast,André Lapied,Male,Male,Male,Male,,24
52.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015503119317,Adequate Moods for non-eu Decision Making in a Sequential Framework,February 2002,Nathalie Etchart,,,Female,Unknown,Unknown,Female,,3
52.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015516217425,"Fast, frugal, and fit: Simple heuristics for paired comparison",February 2002,Laura Martignon,Ulrich Hoffrage,,Female,Male,Unknown,Mix,,
52.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015538123387,Another impossibility result for normal form games,February 2002,Antonio Quesada,,,Male,Unknown,Unknown,Male,,1
52.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015513000587,Even Risk-Averters may Love Risk,February 2002,Alfred Müller,Marco Scarsini,,Male,Male,Unknown,Male,,3
52.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015568206548,The effect of context variables on cognitive effort in multiattribute binary choice,March 2002,S. Iglesias-Parro,E.I. De la Fuente,A.R. Ortega,Unknown,Unknown,Unknown,Unknown,,
52.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015544715608,"Known, Unknown, and Unknowable Uncertainties",March 2002,Clare Chua Chow,Rakesh K. Sarin,,Female,Male,Unknown,Mix,,
52.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015564123344,Why the Infinite Decision Puzzle is Puzzling,March 2002,Jeffrey A. Barrett,Frank Arntzenius,,Male,Male,Unknown,Male,,3
52.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015560016516,Positivity of bid-ask spreads and symmetrical monotone risk aversion*,March 2002,Moez Abouda,Alain Chateauneuf,,Unknown,Male,Unknown,Male,,5
52.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1015551010381,Condorcet's paradox and the likelihood of its occurrence: different perspectives on balanced preferences*,March 2002,William V. Gehrlein,,,Male,Unknown,Unknown,Male,,53
52.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1019674323804,Loss Aversion and Bargaining,May 2002,Jonathan Shalev,,,Male,Unknown,Unknown,Male,,47
52.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1019602429921,"The Minimax, the Minimin, and the Hurwicz Adjustment Principle",May 2002,Bernhard F. Arnold,Ingrid Größl,Peter Stahlecker,Male,Female,Male,Mix,,
52.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1019656013992,A note on Chichilnisky's social choice paradox,May 2002,Luc Lauwers,,,Male,Unknown,Unknown,Male,,1
52.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1019678407874,Committee Decisions with Partisans and Side-Transfers,May 2002,Mehmet Bac,Parimal Kanti Bag,,Male,,Unknown,Mix,,
52.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1019620322895,Intradimensional Single-Peakedness and the Multidimensional Arrow Problem,May 2002,Christian List,,,Male,Unknown,Unknown,Male,,3
52.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1020298220758,Semicontinuous Representability of Homothetic Interval Orders by Means of Two Homogeneous Functionals,June 2002,Gianni Bosi,,,Male,Unknown,Unknown,Male,,2
52.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1020212401428,On the emptiness of the stability set of order d,June 2002,Mathieu Martin,,,Male,Unknown,Unknown,Male,,
52.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1020296514974,A possibilistic hierarchical model for behaviour under uncertainty,June 2002,Gert de Cooman,Peter Walley,,Male,Male,Unknown,Male,,31
52.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1020237003687,An Elementary Interpretation of the Gini Inequality Index,June 2002,S. Subramanian,,,Unknown,Unknown,Unknown,Unknown,,
52.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1020269501495,Power of Enforcement and Dictatorship,June 2002,Antonio Quesada,,,Male,Unknown,Unknown,Male,,
52.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1020240214900,On Asymptotic Strategy-Proofness of Classical Social Choice Rules,June 2002,Arkadii Slinko,,,Male,Unknown,Unknown,Male,,16
53.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1020811218051,A Non-cooperative Axiomatization of the Core,August 2002,Akira Okada,Eyal Winter,,,Male,Unknown,Mix,,
53.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1020891631738,Spatial Dispersion as a Dynamic Coordination Problem,August 2002,Steve Alpern,Diane J. Reyniers,,Male,Female,Unknown,Mix,,
53.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1020805106965,A Banzhaf share function for cooperative games in coalition structure,August 2002,Gerard van der Laan,René van den Brink,,Male,Male,Unknown,Male,,7
53.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1020807903939,Expected utility from additive utility on semigroups,August 2002,Juan C. Candeal,Juan R. De Miguel,Esteban Induráin,Male,Male,Male,Male,,5
53.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1021227106744,Can Bayes' Rule be Justified by Cognitive Rationality Principles?,September 2002,Bernard Walliser,Denis Zwirn,,Male,Male,Unknown,Male,,15
53.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1021255808323,A Rule For Updating Ambiguous Beliefs,September 2002,Cesaltina Pacheco Pires,,,Female,Unknown,Unknown,Female,,89
53.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1021215903030,On the (Sample) Condorcet Efficiency of Majority Rule: An alternative view of majority cycles and social homogeneity,September 2002,Michel Regenwetter,James Adams,Bernard Grofman,Male,Male,Male,Male,,14
53.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1022815832351,High Stakes and Acceptance Behavior in Ultimatum Bargaining:,November 2002,Bertrand Munier,Costin Zaharia,,Male,Male,Unknown,Male,,38
53.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1022841432562,Behavioral and Prescriptive Explanations of a Reverse Sunk Cost Effect,November 2002,David Johnstone,,,Male,Unknown,Unknown,Male,,9
53.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1022800413905,The Deep Side of Preference Theory,November 2002,Antoine Billot,,,Male,Unknown,Unknown,Male,,3
53.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1022811800577,The price of risk with incomplete knowledge on the utility function*,November 2002,Francisco J. Vázquez,Richard Watt,,Male,Male,Unknown,Male,,1
53.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1024189714814,Negotiation and Defeasible Decision Making,December 2002,Fernando Tohmé,,,Male,Unknown,Unknown,Male,,8
53.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1024158301610,Ordinal equivalence of power notions in voting games,December 2002,Lawrence Diffo Lambo,Joël Moulen,,Male,Male,Unknown,Male,,41
53.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1024162419357,Digraph Competitions and Cooperative Games,December 2002,René van den Brink,Peter Borm,,Male,Male,Unknown,Male,,24
53.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1024196415513,Sophisticated Voting Under the Sequential Voting by Veto1,December 2002,Fany Yuval,,,Female,Unknown,Unknown,Female,,18
53.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1024194725086,On the Inconsistency of Equilibrium Refinement,December 2002,Werner Güth,,,Male,Unknown,Unknown,Male,,4
53.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1024102216631,Belief system foundations of backward induction,December 2002,Antonio Quesada,,,Male,Unknown,Unknown,Male,,3
54.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1025088217300,The Trial and Crucifixion of Jesus: A Modest Proposal,February 2003,Ron E. Hassner,,,Male,Unknown,Unknown,Male,,
54.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1025091208248,Constrained egalitarianism in a simple redistributive model,February 2003,Jean-Yves Jaffray,Philippe Mongin,,Unknown,Male,Unknown,Male,,5
54.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1025095318208,Commonalities in Time and Ambiguity Aversion for Long-Term Risks*,February 2003,Harrell W. Chesson,W. Kip Viscusi,,Male,Unknown,Unknown,Male,,45
54.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1025043301370,A Note on Optimal Insurance in the presence of a Nonpecuniary Background Risk,February 2003,Béatrice Rey,,,Female,Unknown,Unknown,Female,,27
54.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1026243725082,Auditor Probability Judgments: Discounting Unspecified Possibilities,March 2003,Richard G. Brody,John M. Coulter,Alireza Daneshfar,Male,Male,Unknown,Male,,3
54.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1026251404228,"For Bayesian Wannabes, Are Disagreements Not About Information?",March 2003,Robin Hanson,,,,Unknown,Unknown,Mix,,
54.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1026212810214,Endogenous entry in auctions with negative externalities,March 2003,Isabelle Brocas,,,Female,Unknown,Unknown,Female,,9
54.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1026249026940,Newcomb's Hidden Regress,March 2003,Stephen Maitzen,Garnett Wilson,,Male,,Unknown,Mix,,
54.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1026256800461,Generalized externality games,March 2003,Paula Corcho,José Luis Ferreira,,Female,Male,Unknown,Mix,,
54.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1027353528853,Modification of Semivalues for Games with Coalition Structures,May 2003,Rafael Amer,José Miguel giménez,,Male,Male,Unknown,Male,,23
54.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1027385819400,Multiple Prisoner's Dilemma Games with(out) an Outside Option: an Experimental Study,May 2003,Esther Hauk,,,Female,Unknown,Unknown,Female,,25
54.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1027387507335,The Favorite-Longshot Bias in Sequential Parimutuel Betting with Non-Expected Utility Players,May 2003,Frédéric Koessler,Anthony Ziegelmeyer,Marie-Hélène Broihanne,Male,Male,Unknown,Male,,6
54.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1027375403877,The Instruction set of Questionnaires can Affect the Structure of the Data: Application to Self-Rated State Anxiety,May 2003,Stéphane Vautier,Etienne Mullet,Sylvie Bourdet-loubère,,Male,Female,Mix,,
54.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/A:1027378124173,On the Cycle-Transitivity of the Dice Model,May 2003,B. De Schuymer,H. De Meyer,S. Jenei,Unknown,Unknown,Unknown,Unknown,,
54.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000004350.81892.1b,Verbal and Behavioral Learning in a Probability Compounding Task,June 2003,Daniel John Zizzo,,,Male,Unknown,Unknown,Male,,10
54.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000004258.22525.f4,Doxastic Conditions for Backward Induction,June 2003,Thorsten Clausing,,,Male,Unknown,Unknown,Male,,9
54.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000004303.49899.50,Acting Autonomously Versus not Acting Heteronomously,June 2003,Martin Van Hees,,,Male,Unknown,Unknown,Male,,2
54.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000004340.74268.68,Bargaining and Strategic Demand Commitment,June 2003,Daniel Cardona-Coll,,,Male,Unknown,Unknown,Male,,3
54.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000004426.70133.33,Contents Volume 54,June 2003,,,,Unknown,Unknown,Unknown,Unknown,,
54.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000004427.37304.2f,Index of Authors,June 2003,,,,Unknown,Unknown,Unknown,Unknown,,
55.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000019052.80871.b3,Is the Person-Affecting Intuition Paradoxical?,August 2003,Melinda A. Roberts,,,Female,Unknown,Unknown,Female,,15
55.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000019077.14252.9c,Lottery-Dependent Utility via Stochastic Benchmarking,August 2003,Paola Modesti,,,Female,Unknown,Unknown,Female,,3
55.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000019078.87122.ce,Total and partial Bivariate Risk Premia: An Extension,August 2003,Béatrice Rey,,,Female,Unknown,Unknown,Female,,1
55.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000019053.53742.37,Objective Belief Functions as Induced Measures,August 2003,Yutaka Nakamura,,,,Unknown,Unknown,Mix,,
55.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000019121.48443.f2,Correction of 'On the Inconsistency of Equilibrium Refinement',August 2003,Werner Güth,,,Male,Unknown,Unknown,Male,,
55.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000024427.27715.0a,INCREASING INCREMENT GENERALIZATIONS OF RANK-DEPENDENT THEORIES,September 2003,R. Duncan Luce,,,Unknown,Unknown,Unknown,Unknown,,
55.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000024421.85722.0a,FAIR DIVISION OF INDIVISIBLE ITEMS,September 2003,Steven J. Brams,Paul H. Edelman,Peter C. Fishburn,Male,Male,Male,Male,,51
55.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000044625.73220.62,Super Majoritarianism and the Endowment Effect,November 2003,Uriel Procaccia,Uzi Segal,,Male,Male,Unknown,Male,,4
55.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000044601.83386.7d,Analysis of Intertemporal Choice: A New Framework and Experimental Results,November 2003,Gary Gigliotti,Barry Sopher,,Male,Male,Unknown,Male,,12
55.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000044623.38467.7c,Now or Later?--Endogenous Timing of Threats,November 2003,Siegfried K. Berninghaus,Werner Guth,,Male,Male,Unknown,Male,,3
55.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000044594.17658.40,Harsanyi's Social Aggregation Theorem and Dictatorship,November 2003,Osamu Mori,,,Male,Unknown,Unknown,Male,,1
55.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000044593.23976.07,Oligarchy for Social Choice Correspondences and Strategy-Proofness,November 2003,Yasuhito Tanaka,,,Male,Unknown,Unknown,Male,,2
55.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000047477.13948.ad,The Emergence of Reactive Strategies in Simulated Heterogeneous Populations,December 2003,Ilan Fischer,,,Male,Unknown,Unknown,Male,,6
55.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000047478.15253.1d,'From Exchange It Comes to Tears'. A Dutch 'Folk Theorem' Reconsidered,December 2003,Nicolaas J. Vriend,,,Male,Unknown,Unknown,Male,,
55.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000047490.12475.03,Olson VS. Coase: Coalitional Worth in Conflict,December 2003,Joan Esteban,József Sákovics,,Female,Male,Unknown,Mix,,
55.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000047489.82529.18,Bargaining Solutions as Social Compromises,December 2003,Andreas Pfingsten,Andreas Wagener,,Male,Male,Unknown,Male,,6
55.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000047618.50501.e1,Contents Volume 55,December 2003,,,,Unknown,Unknown,Unknown,Unknown,,
55.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1023/B:THEO.0000047619.95143.2d,Index of Authors,December 2003,,,,Unknown,Unknown,Unknown,Unknown,,
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5629-3,The Coming of Game Theory,February 2004,Gianfranco Gambarelli,Guillermo Owen,,Male,Male,Unknown,Male,,12
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5628-4,Foreword,February 2004,,,,Unknown,Unknown,Unknown,Unknown,,
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5630-x,Guillermo Owen's Proof Of The Minimax Theorem,February 2004,Ken Binmore,,,Male,Unknown,Unknown,Male,,2
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5631-9,Encouraging a coalition formation,February 2004,Michael Maschler,,,Male,Unknown,Unknown,Male,,1
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5633-7,A comparison of non-transferable utility values,February 2004,Sergiu Hart,,,Male,Unknown,Unknown,Male,,21
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5635-5,The P-value for cost sharing in minimum,February 2004,Stefano Moretti,Rodica Branzei,Stef Tijs,Male,Female,Male,Mix,,
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5636-4,A Unified Approach To The Myerson Value And The Position Value,February 2004,Daniel Gómez,Enrique González-Arangüena,Monica Del Pozo,Male,Male,Female,Mix,,
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5637-3,α -Decisiveness In Simple Games,February 2004,Francesc Carreras,,,Male,Unknown,Unknown,Male,,15
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5638-2,Monotonicity of power and power measures,February 2004,Manfred J. Holler,Stefan Napel,,Male,Male,Unknown,Male,,25
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5639-1,On The Meaning Of Owen–Banzhaf Coalitional Value In Voting Situations,February 2004,A. Laruelle,F. Valenciano,,Unknown,Unknown,Unknown,Unknown,,
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5640-8,"""Counting'' power indices for games with a priori unions",February 2004,Marcin Malawski,,,Male,Unknown,Unknown,Male,,10
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5641-7,The dynamic stability of coalitionist behaviour for two-strategy bimatrix games,February 2004,Ross Cressman,József Garay,Zoltán Varga,Male,Male,Male,Male,,4
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5642-6,Dynamic Coalition Formation in the Apex Game,February 2004,Emiko Fukuda,Shigeo Muto,,Female,Male,Unknown,Mix,,
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5643-5,How many people can be controlled in a group pursuit game,February 2004,Yaroslavna Pankratova,Svetlana Tarashnina,,Unknown,Female,Unknown,Female,,2
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5644-4,Relevance of winning coalitions in indirect control of corporations,February 2004,Enrico Denti,Nando Prati,,Male,Male,Unknown,Male,,4
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5646-2,Takeover prices and portfolio theory,February 2004,Gianfranco Gambarelli,Serena Pesce,,Male,Female,Unknown,Mix,,
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5647-1,A Note On The Owen Set Of Linear,February 2004,Vito Fragnelli,,,Male,Unknown,Unknown,Male,,3
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5649-z,On the Owen Set of Transportation Solutions,February 2004,N. Llorca,E. Molina,J. Sánchez-soriano,Unknown,Unknown,Unknown,Unknown,,
56.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-5650-6,The Lovász Extension of Market Games,February 2004,E. Algaba,J.M. Bilbao,A. Jiménez,Unknown,Unknown,Unknown,Unknown,,
57.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-3670-x,Pascal’s and Tabarrok’s Wagers,August 2004,Lars Peter Østerdal,,,Male,Unknown,Unknown,Male,,4
57.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-3671-9,A Comparison of Some Distance-Based Choice Rules in Ranking Environments,August 2004,Hannu Nurmi,,,Male,Unknown,Unknown,Male,,28
57.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-3672-8,"Fairness, Public Good, and Emotional Aspects of Punishment Behavior",August 2004,Klaus Abbink,Abdolkarim Sadrieh,Shmuel Zamir,Male,Unknown,Male,Male,,11
57.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-2859-3,Left-Side strong increases in risk and their comparative statics,August 2004,Suyeol Ryu,Iltae Kim,,Unknown,Unknown,Unknown,Unknown,,
57.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-004-2631-8,A Note On Impossibility Theorems and Seniority Rules,August 2004,Matthias Hild,,,Male,Unknown,Unknown,Male,,
57.0,2.0,Theory and Decision,21 March 2005,https://link.springer.com/article/10.1007/s11238-004-7318-7,Plans And Decisions,September 2004,John L. Pollock,,,Male,Unknown,Unknown,Male,,6
57.0,2.0,Theory and Decision,21 March 2005,https://link.springer.com/article/10.1007/s11238-004-4319-5,Finite Horizon Bargaining With Outside Options And Threat Points,September 2004,Randolph Sloof,,,Male,Unknown,Unknown,Male,,7
57.0,2.0,Theory and Decision,21 March 2005,https://link.springer.com/article/10.1007/s11238-004-3673-7,"Does trust matter for R&D cooperation?
A game theoretic examination",September 2004,Marie-Laure Cabon-Dhersin,Shyama V.  Ramani,,Unknown,Unknown,Unknown,Unknown,,
57.0,3.0,Theory and Decision,04 June 2005,https://link.springer.com/article/10.1007/s11238-005-0281-0,"Framing, Switching and Preference Reversals",November 2004,Michael J. Ryan,,,Male,Unknown,Unknown,Male,,5
57.0,3.0,Theory and Decision,04 June 2005,https://link.springer.com/article/10.1007/s11238-005-0285-9,Proper and Standard Risk Aversion in Two-Moment Decision Models,November 2004,Fatma Lajeri-Chaherli,,,Female,Unknown,Unknown,Female,,16
57.0,3.0,Theory and Decision,04 June 2005,https://link.springer.com/article/10.1007/s11238-005-0282-z,Two-Speed Evolution of Strategies and Preferences In Symmetric Games,November 2004,Alex Possajennikov,,,Male,Unknown,Unknown,Male,,4
57.0,3.0,Theory and Decision,04 June 2005,https://link.springer.com/article/10.1007/s11238-005-0280-1,A Learning-Efficiency Explanation of Structure in Language,November 2004,Andreas Blume,,,Male,Unknown,Unknown,Male,,10
57.0,4.0,Theory and Decision,26 July 2005,https://link.springer.com/article/10.1007/s11238-005-0138-6,Editorial Foreword To Three Ukrainian/ Russian Papers in Decision Science,December 2004,Bertrand Munier,,,Male,Unknown,Unknown,Male,,
57.0,4.0,Theory and Decision,26 July 2005,https://link.springer.com/article/10.1007/s11238-005-3219-7,Convexity and Differentiability of Controlled Risk,December 2004,L. I. Krechetov,,,Unknown,Unknown,Unknown,Unknown,,
57.0,4.0,Theory and Decision,26 July 2005,https://link.springer.com/article/10.1007/s11238-005-3211-2,Experiment in the General Decision Problem,December 2004,V. Ivanenko,V. Labkovskii,,Unknown,Unknown,Unknown,Unknown,,
57.0,4.0,Theory and Decision,26 July 2005,https://link.springer.com/article/10.1007/s11238-005-3217-9,Estimation of Reliability Parameters Under Incomplete Primary Information,December 2004,A. N. Golodnikov,P. S. Knopov,V. A. Pepelyaev,Unknown,Unknown,Unknown,Unknown,,
57.0,4.0,Theory and Decision,26 July 2005,https://link.springer.com/article/10.1007/s11238-005-2459-x,Policy Stable States in the Graph Model for Conflict Resolution,December 2004,Dao-Zhi Zeng,Liping Fang,D. Marc. Kilgour,Unknown,Unknown,Unknown,Unknown,,
57.0,4.0,Theory and Decision,26 July 2005,https://link.springer.com/article/10.1007/s11238-005-0121-2,Coping with Low Pay: Cognitive Dissonance and Persistent Disparate Earnings Profiles,December 2004,Duncan Watson,Robert Webb,Alvin Birdi,Male,Male,Male,Male,,1
57.0,4.0,Theory and Decision,26 July 2005,https://link.springer.com/article/10.1007/s11238-005-0283-y,Adaptation of Tastes to Constraints,December 2004,Heinz Welsch,,,Male,Unknown,Unknown,Male,,8
57.0,4.0,Theory and Decision,26 July 2005,https://link.springer.com/article/10.1007/s11238-005-0120-3,Infinite Exchange Problems,December 2004,Michael Scott,Alexander Scott,,Male,Male,Unknown,Male,,1
57.0,4.0,Theory and Decision,26 July 2005,https://link.springer.com/article/10.1007/s11238-005-0151-9,"Theory and Decision, Contents of Volume 57",December 2004,,,,Unknown,Unknown,Unknown,Unknown,,
57.0,4.0,Theory and Decision,26 July 2005,https://link.springer.com/article/10.1007/s11238-005-0150-x,List of contributors,December 2004,,,,Unknown,Unknown,Unknown,Unknown,,
58.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-1474-2,Editorial Statement,February 2005,Mohammed Abdellaoui,,,Male,Unknown,Unknown,Male,,
58.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-8320-4,The Likelihood Method for Decision under Uncertainty,February 2005,Mohammed Abdellaoui,Peter P. Wakker,,Male,Male,Unknown,Male,,21
58.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-2460-4,Independence Properties Vis-À-Vis Several Utility Representations,February 2005,A. A. J. Marley,R. Duncan Luce,,Unknown,Unknown,Unknown,Unknown,,
58.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-6594-1,Which Scoring Rule Maximizes Condorcet Efficiency Under Iac?,March 2005,Davide P. Cervone,William V. Gehrlein,William S. Zwicker,Male,Male,Male,Male,,46
58.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-6848-y,Potential and ‘Power of a Collectivity to Act’*,March 2005,Annick Laruelle,Federico Valenciano,,Female,Male,Unknown,Mix,,
58.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-5682-6,Choices Under Ambiguity With Familiar And Unfamiliar Outcomes,March 2005,Marcello Basili,Alain Chateauneuf,Fulvio Fontini,Male,Male,Male,Male,,13
58.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-2458-y,Quasi-Bayesian Analysis Using Imprecise Probability Assessments And The Generalized Bayes’ Rule,March 2005,Kathleen M. Whitcomb,,,Female,Unknown,Unknown,Female,,4
58.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-6593-2,Economic Modeling Triggers More Efficient Planning: An Experimental Justification,May 2005,Jean Pierre Ponssard,Olivier Saulpic,,Male,Male,Unknown,Male,,2
58.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-5063-1,Are More Alternatives Better for Decision-Makers? A Note on the Role of Decision Cost,May 2005,Huei-Chung Lu,Mingshen Chen,Juin-Jen Chang,Unknown,Unknown,Unknown,Unknown,,
58.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-6846-0,Equitable Selection in Bilateral Matching Markets,May 2005,Antonio Romero-Medina,,,Male,Unknown,Unknown,Male,,9
58.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-6849-x,Maximal-Element Rationalizability,June 2005,Walter Bossert,Yves Sprumont,Kotaro Suzumura,Male,Male,Male,Male,,11
58.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-0284-x,No Switchbacks: Rethinking Aspiration-Based Dynamics in the Ultimatum Game,June 2005,Jeffrey Carpenter,Peter Hans Matthews,,Male,Male,Unknown,Male,,2
58.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-3890-8,Cognitive Algebra in Sport Decision-Making,June 2005,Patricia Rulence-Pâques,Eric Fruchart,Etienne Mullet,Female,Male,Male,Mix,,
58.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-3856-x,List of Contributors,June 2005,,,,Unknown,Unknown,Unknown,Unknown,,
58.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-3855-y,Contents of Volume 58,June 2005,,,,Unknown,Unknown,Unknown,Unknown,,
59.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-0576-1,Unanimity and Resource Monotonicity,August 2005,Biung-Ghi Ju,,,Unknown,Unknown,Unknown,Unknown,,
59.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-3449-8,Is the Disposition of Constrained Maximization Chosen Rationally?,August 2005,Young-Ran Roh,,,Female,Unknown,Unknown,Female,,
59.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-7303-9,Willingness to Pay for Risk Reduction and Risk Aversion without the Expected Utility Assumption,August 2005,Eric Langlais,,,Male,Unknown,Unknown,Male,,6
59.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-8631-5,"On Decomposing Net Final Values: Eva, Sva and Shadow Project",August 2005,Carlo Alberto Magni,,,Male,Unknown,Unknown,Male,,13
59.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-5459-y,Borda and Condorcet: Some Distance Results,September 2005,Christian Klamler,,,Male,Unknown,Unknown,Male,,10
59.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-0944-x,The Multilinear Extension and the Symmetric Coalition Banzhaf Value,September 2005,J. M. Alonso-Meijide,F. Carreras,M. G. Fiestras-Janeiro,Unknown,Unknown,Unknown,Unknown,,
59.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-8632-4,Resolving the Trust Predicament: A Quantum Game-theoretic Approach,September 2005,Badredine Arfi,,,Unknown,Unknown,Unknown,Unknown,,
59.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-6847-z,A Note on Implementation of Bargaining Solutions,November 2005,Yusuke Samejima,,,Male,Unknown,Unknown,Male,,4
59.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-0945-9,Stability and Efficiency of Partitions in Matching Problems,November 2005,İpek Özkal-Sanver,,,Female,Unknown,Unknown,Female,,2
59.0,3.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-8633-3,Do Trade Union Leaders Violate Subjective Expected Utility? Some Insights From Experimental Data,November 2005,Anna Maffioletti,Michele Santoni,,Female,Female,Unknown,Female,,17
59.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-2883-y,Games of Competition in a Stochastic Environment,December 2005,Judith Avrahami,Werner Güth,Yaakov Kareev,Female,Male,Male,Mix,,
59.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-8634-2,A Note on the Portfolio Selection Problem,December 2005,Franco Pellerey,Patrizia Semeraro,,Male,Female,Unknown,Mix,,
59.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-0946-8,The Use of Base Rate Information as a Function of Experienced Consistency,December 2005,Philip T. Dunwoody,Adam S. Goodie,Robert P. Mahan,Male,Male,Male,Male,,4
59.0,4.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4568-y,Theory and Decission Contents of Volume 59 and List of Contributors,December 2005,,,,Unknown,Unknown,Unknown,Unknown,,
60.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4751-1,Continuously Representable Paretian Quasi-Orders,February 2006,Vicki Knoblauch,,,Female,Unknown,Unknown,Female,,2
60.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4753-z,Axiomatization of a Preference for Most Probable Winner,February 2006,Pavlo R. Blavatskyy,,,Male,Unknown,Unknown,Male,,20
60.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4752-0,On the Relative Strengths of Altruism and Fairness,February 2006,Jonathan H. W. Tan,Friedel Bolle,,Male,,Unknown,Mix,,
60.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-6014-6,The Evolution of Cooperative Strategies for Asymmetric Social Interactions,February 2006,Jörg Rieskamp,Peter M. Todd,,Male,Male,Unknown,Male,,9
60.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-5601-x,"Book Review: Serge-Christophe Kolm, Macrojustice. The Political Economy of Fairness, Cambridge University Press, 2005. ISBN 0 52183503 8. vi + 537 pp.",February 2006,D. M. Fleurbaey,,,Unknown,Unknown,Unknown,Unknown,,
60.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4569-x,When an Event Makes a Difference,May 2006,Massimiliano Amarante,Fabio Maccheroni,,Male,Male,Unknown,Male,,3
60.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4594-9,Countable Additivity and the Foundations of Bayesian Statistics,May 2006,John V. Howard,,,Male,Unknown,Unknown,Male,,1
60.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4570-4,Representability of Ordinal Relations on a Set of Conditional Events,May 2006,Giulianella Coletti,Barbara Vantaggi,,Unknown,Female,Unknown,Female,,19
60.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4591-z,A Philosophical Foundation of Non-Additive Measure and Probability,May 2006,Sebastian Maaß,,,Male,Unknown,Unknown,Male,,2
60.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4571-3,"Expectations, Disappointment, and Rank-Dependent Probability Weighting",May 2006,Philippe Delquié,Alessandra Cillo,,Male,Female,Unknown,Mix,,
60.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4592-y,Ratio-Scale Measurement with Intransitivity or Incompleteness: The Homogeneous Case,May 2006,Marc Le Menestrel,Bertrand Lemaire,,Male,Male,Unknown,Male,,6
60.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4596-7,A Deeper Look at Hyperbolic Discounting,May 2006,Barry Sopher,Arnav Sheth,,Male,Unknown,Unknown,Male,,11
60.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4573-1,An Experimental Test of Generalized Ambiguity Aversion using Lottery Pricing Tasks,May 2006,Michael Bleaney,Steven J. Humphrey,,Male,Male,Unknown,Male,,5
60.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4590-0,"Gender, Financial Risk, and Probability Weights",May 2006,Helga Fehr-Duda,Manuele de Gennaro,Renate Schubert,Female,Male,Female,Mix,,
60.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4593-x,Error Propagation in the Elicitation of Utility and Probability Weighting Functions,May 2006,Pavlo Blavatskyy,,,Male,Unknown,Unknown,Male,,21
60.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4595-8,The Endowment Effect on Academic Chores Trade-Off (ACTO),May 2006,Amira Galin,Miron Gross Sigal Sapir,Irit Kela-Egozi,Female,Male,Female,Mix,,
61.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-006-7868-y,The Equivalence of Bayes and Causal Rationality in Games,August 2006,Oliver Board,,,Male,Unknown,Unknown,Male,,5
61.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-4748-9,Finite Alternating-Move Arbitration Schemes and the Equal Area Solution,August 2006,Nejat Anbarci,,,Male,Unknown,Unknown,Male,,15
61.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-006-7191-7,A Method for Eliciting Utilities and its Application to Collective Choice,August 2006,Ilia Tsetlin,,,Male,Unknown,Unknown,Male,,2
61.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-005-6013-7,Communication Protocols with Belief Messages,August 2006,Ryuichiro Ishikawa,,,Unknown,Unknown,Unknown,Unknown,,
61.0,1.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-006-0005-0,Applying the Benchmarking Procedure: A Decision Criterion of Choice Under Risk,August 2006,Francesca Beccacece,Alessandra Cillo,,Female,Female,Unknown,Female,,3
61.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-006-8047-x,"Does Learning Diminish Violations of Independence, Coalescing and Monotonicity?",September 2006,Steven J. Humphrey,,,Male,Unknown,Unknown,Male,,9
61.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-006-0006-z,The Effectiveness of Assigned Goals in Complex Financial Decision Making and the Importance of Gender,September 2006,Megan Lee Endres,,,Female,Unknown,Unknown,Female,,8
61.0,2.0,Theory and Decision,,https://link.springer.com/article/10.1007/s11238-006-0007-y,Rethinking Risk Attitude: Aspiration as Pure Risk,September 2006,Greg B. Davies,,,Male,Unknown,Unknown,Male,,6
61.0,3.0,Theory and Decision,27 June 2006,https://link.springer.com/article/10.1007/s11238-006-0004-1,Formation of a Communication Network Under Perfect Foresight,November 2006,Frédéric Deroïan,,,Male,Unknown,Unknown,Male,,4
61.0,3.0,Theory and Decision,06 September 2006,https://link.springer.com/article/10.1007/s11238-006-9000-8,“Take-the-Best” and Other Simple Strategies: Why and When they Work “Well” with Binary Cues,November 2006,Robin M. Hogarth,Natalia Karelaia,,,Female,Unknown,Mix,,
61.0,3.0,Theory and Decision,06 September 2006,https://link.springer.com/article/10.1007/s11238-006-9001-7,How Does Separability Affect The Desirability Of Referendum Election Outcomes?,November 2006,Jonathan K. Hodge,Peter Schwallier,,Male,Male,Unknown,Male,,11
61.0,3.0,Theory and Decision,08 August 2006,https://link.springer.com/article/10.1007/s11238-006-9002-6,"Demons, Deceivers And Liars: Newcomb’s Malin Génie",November 2006,Peter Slezak,,,Male,Unknown,Unknown,Male,,3
61.0,4.0,Theory and Decision,06 September 2006,https://link.springer.com/article/10.1007/s11238-006-9003-5,Achievable Hierarchies In Voting Games,December 2006,Jane Friedman,Lynn Mcgrath,Cameron Parker,Female,,,Mix,,
61.0,4.0,Theory and Decision,06 September 2006,https://link.springer.com/article/10.1007/s11238-006-9004-4,Uncommon Priors Require Origin Disputes,December 2006,Robin Hanson,,,,Unknown,Unknown,Mix,,
61.0,4.0,Theory and Decision,13 August 2006,https://link.springer.com/article/10.1007/s11238-006-9005-3,Correlated Equilibrium in Games with Incomplete Information Revisited,December 2006,Françoise Forges,,,Female,Unknown,Unknown,Female,,30
61.0,4.0,Theory and Decision,19 October 2006,https://link.springer.com/article/10.1007/s11238-006-9014-2,Is There a “Pessimistic” Bias in Individual Beliefs? Evidence from a Simple Survey,December 2006,Selima Ben Mansour,Elyès Jouini,Clotilde Napp,Female,Unknown,Female,Female,,24
61.0,4.0,Theory and Decision,19 October 2006,https://link.springer.com/article/10.1007/s11238-006-9019-x,Deriving Harsanyi’s Utilitarianism from De Finetti’s Book-Making Argument,December 2006,Enrico Diecidue,,,Male,Unknown,Unknown,Male,,4
62.0,1.0,Theory and Decision,05 December 2006,https://link.springer.com/article/10.1007/s11238-006-9007-1,Socially Structured Games,February 2007,P. Jean-Jacques Herings,Gerard Van Der Laan,Dolf Talman,Unknown,Male,Male,Male,,5
62.0,1.0,Theory and Decision,05 December 2006,https://link.springer.com/article/10.1007/s11238-006-9022-2,On Myopic Stability Concepts for Hedonic Games,February 2007,Shao Chin Sung,Dinko Dimitrov,,,Male,Unknown,Mix,,
62.0,1.0,Theory and Decision,06 September 2006,https://link.springer.com/article/10.1007/s11238-006-9006-2,Economic Darwinism: Who has the Best Probabilities?,February 2007,David Johnstone,,,Male,Unknown,Unknown,Male,,9
62.0,2.0,Theory and Decision,05 December 2006,https://link.springer.com/article/10.1007/s11238-006-9018-y,Supermodularity and the Comparative Statics of Risk,March 2007,John Quiggin,Robert G. Chambers,,Male,Male,Unknown,Male,,3
62.0,2.0,Theory and Decision,05 December 2006,https://link.springer.com/article/10.1007/s11238-006-9016-0,Rationality and Order-Dependent Sequential Rationality,March 2007,HOUY NICOLAS,,,Unknown,Unknown,Unknown,Unknown,,
62.0,2.0,Theory and Decision,05 December 2006,https://link.springer.com/article/10.1007/s11238-006-9020-4,"Stubbornness, Power, and Equilibrium Selection in Repeated Games with Multiple Equilibria",March 2007,Kjell Hausken,,,Male,Unknown,Unknown,Male,,3
62.0,2.0,Theory and Decision,20 December 2006,https://link.springer.com/article/10.1007/s11238-006-9013-3,The Experimetrics of Public Goods: Inferring Motivations from Contributions,March 2007,Nicholas Bardsley,Peter G. Moffatt,,Male,Male,Unknown,Male,,53
62.0,3.0,Theory and Decision,05 December 2006,https://link.springer.com/article/10.1007/s11238-006-9012-4,Extreme Copulas and the Comparison of Ordered Lists,May 2007,B. De Schuymer,H. De Meyer,B. De baets,Unknown,Unknown,Unknown,Unknown,,
62.0,3.0,Theory and Decision,09 January 2007,https://link.springer.com/article/10.1007/s11238-006-9015-1,Ambiguity Aversion in the Field of Insurance: Insurers’ Attitude to Imprecise and Conflicting Probability Estimates,May 2007,Laure Cabantous,,,Female,Unknown,Unknown,Female,,74
62.0,3.0,Theory and Decision,02 November 2006,https://link.springer.com/article/10.1007/s11238-006-9017-z,On Robust Constitution Design,May 2007,Emmanuelle Auriol,Robert J. Gary-Bobo,,Female,Male,Unknown,Mix,,
62.0,3.0,Theory and Decision,14 March 2007,https://link.springer.com/article/10.1007/s11238-007-9031-9,Asking Price and Price Discounts: The Strategy of Selling an Asset Under Price Uncertainty,May 2007,Tapan Biswas,Jolian Mchardy,,Male,Unknown,Unknown,Male,,6
62.0,3.0,Theory and Decision,09 January 2007,https://link.springer.com/article/10.1007/s11238-006-9024-0,Book Review,May 2007,Kjell Hausken,,,Male,Unknown,Unknown,Male,,
62.0,4.0,Theory and Decision,14 March 2007,https://link.springer.com/article/10.1007/s11238-007-9032-8,Stochastic Evolution of Rules for Playing Finite Normal Form Games,May 2007,Fabrizio Germano,,,Male,Unknown,Unknown,Male,,18
62.0,4.0,Theory and Decision,14 March 2007,https://link.springer.com/article/10.1007/s11238-007-9039-1,Valuing Others’ Information under Imperfect Expectations,May 2007,Hagen Lindstädt,,,Male,Unknown,Unknown,Male,,
62.0,4.0,Theory and Decision,20 March 2007,https://link.springer.com/article/10.1007/s11238-007-9027-5,Individually Rational Collective Choice,May 2007,Andrés Carvajal,,,Male,Unknown,Unknown,Male,,1
62.0,4.0,Theory and Decision,12 December 2006,https://link.springer.com/article/10.1007/s11238-006-9021-3,"Samuel Bowles’ “Microeconomics: Behavior, Institutions and Evolution”",May 2007,Guillaume Hollard,,,Male,Unknown,Unknown,Male,,
63.0,1.0,Theory and Decision,17 April 2007,https://link.springer.com/article/10.1007/s11238-007-9028-4,The Welfare Consequences of Strategic Voting in Two Commonly Used Parliamentary Agendas,August 2007,Aki Lehtinen,,,Female,Unknown,Unknown,Female,,17
63.0,1.0,Theory and Decision,27 March 2007,https://link.springer.com/article/10.1007/s11238-007-9030-x,A recursive core for partition function form games,August 2007,László Á. Kóczy,,,Male,Unknown,Unknown,Male,,72
63.0,1.0,Theory and Decision,17 April 2007,https://link.springer.com/article/10.1007/s11238-007-9036-4,Strategic games with security and potential level players,August 2007,Alexander Zimper,,,Male,Unknown,Unknown,Male,,1
63.0,1.0,Theory and Decision,17 April 2007,https://link.springer.com/article/10.1007/s11238-007-9037-3,Choosing and Describing: Sen and the Irrelevance of Independence Alternatives,August 2007,Michael Neumann,,,Male,Unknown,Unknown,Male,,6
63.0,2.0,Theory and Decision,23 May 2007,https://link.springer.com/article/10.1007/s11238-007-9045-3,Evaluating Time Streams of Income: Discounting What?,September 2007,Manel Baucells,Rakesh K. Sarin,,Male,Male,Unknown,Male,,16
63.0,2.0,Theory and Decision,17 May 2007,https://link.springer.com/article/10.1007/s11238-007-9041-7,Better May be Worse: Some Monotonicity Results and Paradoxes in Discrete Choice Under Uncertainty,September 2007,Jörgen W. Weibull,Lars-Göran Mattsson,Mark Voorneveld,Male,Unknown,Male,Male,,14
63.0,2.0,Theory and Decision,10 May 2007,https://link.springer.com/article/10.1007/s11238-006-9023-1,The Value of a Probability Forecast from Portfolio Theory,September 2007,D. J. Johnstone,,,Unknown,Unknown,Unknown,Unknown,,
63.0,3.0,Theory and Decision,15 May 2007,https://link.springer.com/article/10.1007/s11238-007-9040-8,Prospect-theory’s Diminishing Sensitivity Versus Economics’ Intrinsic Utility of Money: How the Introduction of the Euro can be Used to Disentangle the Two Empirically,November 2007,Peter P. Wakker,Veronika Köbberling,Christiane Schwieren,Male,Female,Female,Mix,,
63.0,3.0,Theory and Decision,10 May 2007,https://link.springer.com/article/10.1007/s11238-007-9029-3,A Unified Bayesian Decision Theory,November 2007,Richard Bradley,,,Male,Unknown,Unknown,Male,,21
63.0,3.0,Theory and Decision,23 May 2007,https://link.springer.com/article/10.1007/s11238-007-9035-5,An Evolutionary Analysis of Buyer Insurance and Seller Reputation in Online Markets,November 2007,Werner Güth,Friederike Mengel,Axel Ockenfels,Male,Female,Male,Mix,,
63.0,3.0,Theory and Decision,03 April 2007,https://link.springer.com/article/10.1007/s11238-007-9038-2,An Experimental Evaluation of the Serial Cost Sharing Rule,November 2007,Laura Razzolini,Michael Reksulak,Robert Dorsey,Female,Male,Male,Mix,,
63.0,4.0,Theory and Decision,17 April 2007,https://link.springer.com/article/10.1007/s11238-007-9042-6,Games without Rules,December 2007,Flavio Menezes,John Quiggin,,Male,Male,Unknown,Male,,2
63.0,4.0,Theory and Decision,05 May 2007,https://link.springer.com/article/10.1007/s11238-007-9043-5,The Self-Fulfilling Property of Trust: An Experimental Study,December 2007,Michael Bacharach,Gerardo Guerra,Daniel John Zizzo,Male,Male,Male,Male,,92
63.0,4.0,Theory and Decision,10 May 2007,https://link.springer.com/article/10.1007/s11238-007-9034-6,On the Robustness of the Winner’s Curse Phenomenon,December 2007,Brit Grosskopf,Yoella Bereby-Meyer,Max Bazerman,Female,Unknown,Male,Mix,,
64.0,1.0,Theory and Decision,27 September 2007,https://link.springer.com/article/10.1007/s11238-007-9044-4,Great Expectations. Part I: On the Customizability of Generalized Expected Utility,February 2008,Francis C. Chu,Joseph Y. Halpern,,Male,Male,Unknown,Male,,10
64.0,1.0,Theory and Decision,30 May 2007,https://link.springer.com/article/10.1007/s11238-007-9046-2,"On Ordinal Utility, Cardinal Utility and Random Utility",February 2008,Richard Batley,,,Male,Unknown,Unknown,Male,,12
64.0,1.0,Theory and Decision,20 June 2007,https://link.springer.com/article/10.1007/s11238-007-9033-7,Is Context-Based Choice due to Context-Dependent Preferences?,February 2008,Kobi Kriesler,Shmuel Nitzan,,Male,Male,Unknown,Male,,1
64.0,1.0,Theory and Decision,06 June 2007,https://link.springer.com/article/10.1007/s11238-007-9026-6,An Alternative Model of the Formation of Political Coalitions,February 2008,Jan-Willem Van Der Rijt,,,Unknown,Unknown,Unknown,Unknown,,
64.0,2.0,Theory and Decision,26 May 2007,https://link.springer.com/article/10.1007/s11238-007-9059-x,Guest Editor’s Introduction,March 2008,John D. Hey,,,Male,Unknown,Unknown,Male,,
64.0,2.0,Theory and Decision,07 December 2007,https://link.springer.com/article/10.1007/s11238-007-9090-y,"Purity, Resistance, and Innocence in Utility Theory",March 2008,R. Duncan Luce,,,Unknown,Unknown,Unknown,Unknown,,
64.0,2.0,Theory and Decision,17 June 2007,https://link.springer.com/article/10.1007/s11238-007-9052-4,Preferences Representable by a Lower Expectation: Some Characterizations,March 2008,Andrea Capotorti,Giulianella Coletti,Barbara Vantaggi,Female,Unknown,Female,Female,,4
64.0,2.0,Theory and Decision,28 September 2007,https://link.springer.com/article/10.1007/s11238-007-9057-z,Ranking sets additively in decisional contexts: an axiomatic characterization,March 2008,José C. R. Alcantud,Ritxar Arlegi,,Male,Unknown,Unknown,Male,,3
64.0,2.0,Theory and Decision,19 June 2007,https://link.springer.com/article/10.1007/s11238-007-9061-3,Dynamic Decision Making when Risk Perception Depends on Past Experience,March 2008,Michèle Cohen,Johanna Etner,Meglena Jeleva,Female,Female,Female,Female,,51
64.0,2.0,Theory and Decision,13 June 2007,https://link.springer.com/article/10.1007/s11238-007-9054-2,Private Information and the ‘Information Function’: A Survey of Possible Uses,March 2008,Emmanuel Haven,,,Male,Unknown,Unknown,Male,,20
64.0,2.0,Theory and Decision,04 July 2007,https://link.springer.com/article/10.1007/s11238-007-9068-9,Temptations and Dynamic Consistency,March 2008,Enrica Carbone,,,Female,Unknown,Unknown,Female,,3
64.0,2.0,Theory and Decision,05 June 2007,https://link.springer.com/article/10.1007/s11238-007-9058-y,"Dynamic Choice, Independence and Emotions",March 2008,Astrid Hopfensitz,Frans Van Winden,,Female,Male,Unknown,Mix,,
64.0,2.0,Theory and Decision,29 June 2007,https://link.springer.com/article/10.1007/s11238-007-9051-5,Uncertainty Aversion Vs. Competence: An Experimental Market Study,March 2008,Carmela Di Mauro,,,Female,Unknown,Unknown,Female,,6
64.0,2.0,Theory and Decision,15 June 2007,https://link.springer.com/article/10.1007/s11238-007-9053-3,Granny Versus Game Theorist: Ambiguity in Experimental Games,March 2008,Jürgen Eichberger,David Kelsey,Burkhard C. Schipper,Male,Male,Male,Male,,40
64.0,2.0,Theory and Decision,30 June 2007,https://link.springer.com/article/10.1007/s11238-007-9055-1,Disappointment Aversion in internet Bidding-Decisions,March 2008,Doron Sonsino,,,Male,Unknown,Unknown,Male,,27
64.0,2.0,Theory and Decision,29 June 2007,https://link.springer.com/article/10.1007/s11238-007-9056-0,Risk Aversion when Gains are Likely and Unlikely: Evidence from a Natural Experiment with Large Stakes,March 2008,Pavlo Blavatskyy,Ganna Pogrebna,,Male,Female,Unknown,Mix,,
64.0,2.0,Theory and Decision,30 May 2007,https://link.springer.com/article/10.1007/s11238-007-9060-4,A Betting Market: Description and a Theoretical Explanation of Bets in Pelota Matches,March 2008,Loreto Llorente,Josemari Aizpurua,,Male,Unknown,Unknown,Male,,1
65.0,1.0,Theory and Decision,13 June 2007,https://link.springer.com/article/10.1007/s11238-007-9049-z,Choice Functions with States of Mind,August 2008,Houy Nicolas,,,Unknown,Unknown,Unknown,Unknown,,
65.0,1.0,Theory and Decision,13 October 2007,https://link.springer.com/article/10.1007/s11238-007-9079-6,Binary Relations: Finite Characterizations and Computational Complexity,August 2008,Vicki Knoblauch,,,Female,Unknown,Unknown,Female,,
65.0,1.0,Theory and Decision,08 September 2007,https://link.springer.com/article/10.1007/s11238-007-9074-y,A Parallel between Regret Theory and Outranking Methods for Multicriteria Decision Making Under Imprecise Information,August 2008,Gül Özerol,Esra Karasakal,,Female,Female,Unknown,Female,,22
65.0,1.0,Theory and Decision,30 May 2007,https://link.springer.com/article/10.1007/s11238-007-9050-6,Qualitative Heuristics For Balancing the Pros and Cons,August 2008,Jean-François Bonnefon,Didier Dubois,Sylvie Leblois,Unknown,Male,Female,Mix,,
65.0,2.0,Theory and Decision,19 January 2008,https://link.springer.com/article/10.1007/s11238-007-9095-6,Effects of Uncertainty Aversion on the Call Option Market,September 2008,Aldo Montesano,,,Male,Unknown,Unknown,Male,,8
65.0,2.0,Theory and Decision,06 December 2007,https://link.springer.com/article/10.1007/s11238-007-9087-6,"Altruism, Spite and Competition in Bargaining Games",September 2008,Maria Montero,,,Female,Unknown,Unknown,Female,,16
65.0,2.0,Theory and Decision,27 March 2008,https://link.springer.com/article/10.1007/s11238-008-9100-8,Using Turn Taking to Mitigate Coordination and Conflict Problems in the Repeated Battle of the Sexes Game,September 2008,Sau-Him Paul Lau,Vai-Lam Mui,,Unknown,Unknown,Unknown,Unknown,,
65.0,3.0,Theory and Decision,21 August 2007,https://link.springer.com/article/10.1007/s11238-007-9070-2,Consulting an Expert with Potentially Conflicting Preferences,November 2008,Thomas Lanzi,Jerome Mathis,,Male,Male,Unknown,Male,,5
65.0,3.0,Theory and Decision,01 December 2007,https://link.springer.com/article/10.1007/s11238-007-9085-8,How Risk Disciplines Pre-Commitment,November 2008,Christophe Caron,Thierry Lafay,,Male,Male,Unknown,Male,,4
65.0,3.0,Theory and Decision,22 December 2007,https://link.springer.com/article/10.1007/s11238-007-9093-8,A Theory of Agreements in the Shadow of Conflict: The Genesis of Bargaining Power,November 2008,Joan Esteban,József Sákovics,,Female,Male,Unknown,Mix,,
65.0,3.0,Theory and Decision,18 December 2007,https://link.springer.com/article/10.1007/s11238-007-9091-x,Coalitional Interval Games for Strategic Games in Which Players Cooperate,November 2008,Luisa Carpente,Balbina Casas-Méndez,Anne van den Nouweland,Female,Female,Female,Female,,7
65.0,4.0,Theory and Decision,25 April 2008,https://link.springer.com/article/10.1007/s11238-008-9103-5,Eliciting beliefs,December 2008,Robert Chambers,Tigran Melkonyan,,Male,Male,Unknown,Male,,2
65.0,4.0,Theory and Decision,06 June 2007,https://link.springer.com/article/10.1007/s11238-007-9048-0,Simple Majority Achievable Hierarchies,December 2008,Dwight Bean,Jane Friedman,Cameron Parker,Male,Female,,Mix,,
65.0,4.0,Theory and Decision,13 December 2007,https://link.springer.com/article/10.1007/s11238-007-9086-7,On the Representation of Incomplete Preferences Over Risky Alternatives,December 2008,Paola Manzini,Marco Mariotti,,Female,Male,Unknown,Mix,,
65.0,4.0,Theory and Decision,17 November 2007,https://link.springer.com/article/10.1007/s11238-007-9084-9,‘q-Pareto-Scalar’ Two-Stage Extremization Model and its Reducibility to One-Stage Model,December 2008,Fuad Aleskerov,Yetkin Çinar,,Male,Male,Unknown,Male,,6
65.0,4.0,Theory and Decision,26 May 2008,https://link.springer.com/article/10.1007/s11238-008-9111-5,Delay in a bargaining game with contracts,December 2008,Yi-Chun Chen,Xiao Luo,,,,Unknown,Mix,,
66.0,1.0,Theory and Decision,21 October 2007,https://link.springer.com/article/10.1007/s11238-007-9082-y,Choice under complete uncertainty when outcome spaces are state dependent,January 2009,Clemens Puppe,Karl H. Schlag,,Male,Male,Unknown,Male,,16
66.0,1.0,Theory and Decision,12 July 2008,https://link.springer.com/article/10.1007/s11238-008-9114-2,Monotonicity of power in games with a priori unions,January 2009,J. M. Alonso-Meijide,C. Bowles,S. Napel,Unknown,Unknown,Unknown,Unknown,,
66.0,1.0,Theory and Decision,20 November 2008,https://link.springer.com/article/10.1007/s11238-008-9123-1,Markov interactions in a class of dynamic games,January 2009,Charles Figuières,,,Male,Unknown,Unknown,Male,,9
66.0,1.0,Theory and Decision,24 July 2007,https://link.springer.com/article/10.1007/s11238-007-9075-x,The Effects of Beliefs Versus Risk Attitude on Bargaining Outcomes,January 2009,David L. Dickinson,,,Male,Unknown,Unknown,Male,,18
66.0,2.0,Theory and Decision,28 September 2007,https://link.springer.com/article/10.1007/s11238-007-9072-0,"Modus Ponens and Modus Tollens for Conditional Probabilities, and Updating on Uncertain Evidence",February 2009,Jordan Howard Sobel,,,Male,Unknown,Unknown,Male,,4
66.0,2.0,Theory and Decision,05 January 2008,https://link.springer.com/article/10.1007/s11238-007-9094-7,The Collapsing Choice Theory: Dissociating Choice and Judgment in Decision Making,February 2009,Jeffrey M. Stibel,Itiel E. Dror,Talia Ben-Zeev,Male,Male,Female,Mix,,
66.0,2.0,Theory and Decision,19 December 2007,https://link.springer.com/article/10.1007/s11238-007-9092-9,Why a Simple Second-Price Auction Induces Efficient Endogenous Entry,February 2009,Jingfeng Lu,,,Unknown,Unknown,Unknown,Unknown,,
66.0,3.0,Theory and Decision,04 June 2008,https://link.springer.com/article/10.1007/s11238-008-9112-4,Recursive expected utility and the separation of attitudes towards risk and ambiguity: an experimental study,March 2009,Sujoy Chakravarty,Jaideep Roy,,Unknown,Unknown,Unknown,Unknown,,
66.0,3.0,Theory and Decision,14 June 2007,https://link.springer.com/article/10.1007/s11238-007-9062-2,An Experimental Investigation of the Disparity Between WTA and WTP for Lotteries,March 2009,Ulrich Schmidt,Stefan Traub,,Male,Male,Unknown,Male,,26
66.0,3.0,Theory and Decision,17 July 2007,https://link.springer.com/article/10.1007/s11238-007-9067-x,A Characterization for the Spherical Scoring Rule,March 2009,Victor Richmond Jose,,,Male,Unknown,Unknown,Male,,7
66.0,3.0,Theory and Decision,24 October 2007,https://link.springer.com/article/10.1007/s11238-007-9081-z,Absolute and Relative Time-Consistent Revealed Preferences,March 2009,Thomas Demuynck,,,Male,Unknown,Unknown,Male,,2
66.0,4.0,Theory and Decision,23 March 2008,https://link.springer.com/article/10.1007/s11238-008-9099-x,On the Application of Multiattribute Utility Theory to Models of Choice,April 2009,Jeffrey Helzner,,,Male,Unknown,Unknown,Male,,4
66.0,4.0,Theory and Decision,14 December 2007,https://link.springer.com/article/10.1007/s11238-007-9088-5,Asymmetries in Information Processing in a Decision Theory Framework,April 2009,Luís Santos-Pinto,,,Male,Unknown,Unknown,Male,,
66.0,4.0,Theory and Decision,08 January 2009,https://link.springer.com/article/10.1007/s11238-008-9129-8,Dynamic consistency in extensive form decision problems,April 2009,Nicola Dimitri,,,Female,Unknown,Unknown,Female,,1
66.0,4.0,Theory and Decision,02 September 2007,https://link.springer.com/article/10.1007/s11238-007-9078-7,Reliability of Information Aggregation with Regional Biases: A Note,April 2009,Martina Behm,Hans Peter Grüner,,Female,Male,Unknown,Mix,,
66.0,4.0,Theory and Decision,24 July 2007,https://link.springer.com/article/10.1007/s11238-007-9071-1,Estimating Merging Costs by Merger Preconditions,April 2009,Jingang Zhao,,,Unknown,Unknown,Unknown,Unknown,,
67.0,1.0,Theory and Decision,17 October 2007,https://link.springer.com/article/10.1007/s11238-007-9080-0,Subjective Probability Weighting and the Discovered Preference Hypothesis,July 2009,Gijs van de Kuilen,,,Male,Unknown,Unknown,Male,,18
67.0,1.0,Theory and Decision,18 October 2007,https://link.springer.com/article/10.1007/s11238-007-9065-z,Utility of Gambling when Events are Valued: an Application of Inset Entropy,July 2009,C. T. Ng,R. Duncan Luce,A. A. J. Marley,Unknown,Unknown,Unknown,Unknown,,
67.0,1.0,Theory and Decision,18 July 2007,https://link.springer.com/article/10.1007/s11238-007-9069-8,Envy Freeness in Experimental Fair Division Problems,July 2009,Dorothea K. Herreiner,Clemens D. Puppe,,Female,Male,Unknown,Mix,,
67.0,1.0,Theory and Decision,04 December 2007,https://link.springer.com/article/10.1007/s11238-007-9089-4,A Simple Model of Secure Public Communication,July 2009,Hannu Vartiainen,,,Male,Unknown,Unknown,Male,,2
67.0,2.0,Theory and Decision,17 July 2007,https://link.springer.com/article/10.1007/s11238-007-9066-y,Expected Utility Consistent Extensions of Preferences,August 2009,Burak Can,Bora Erdamar,M. Remzi Sanver,Male,Male,Unknown,Male,,5
67.0,2.0,Theory and Decision,22 June 2007,https://link.springer.com/article/10.1007/s11238-007-9063-1,Plurality Rule Works In Three-Candidate Elections,August 2009,Bernardo Moreno,M. Socorro Puy,,Male,Unknown,Unknown,Male,,3
67.0,2.0,Theory and Decision,21 July 2007,https://link.springer.com/article/10.1007/s11238-007-9073-z,A Model of Minimal Probabilistic Belief Revision,August 2009,Andrés Perea,,,Male,Unknown,Unknown,Male,,3
67.0,2.0,Theory and Decision,27 June 2007,https://link.springer.com/article/10.1007/s11238-007-9064-0,The Evolution of Coding in Signaling Games,August 2009,Jeffrey A. Barrett,,,Male,Unknown,Unknown,Male,,53
67.0,3.0,Theory and Decision,22 February 2008,https://link.springer.com/article/10.1007/s11238-008-9096-0,Inequality and Political Consensus,September 2009,Hans Peter Grüner,,,Male,Unknown,Unknown,Male,,6
67.0,3.0,Theory and Decision,20 February 2008,https://link.springer.com/article/10.1007/s11238-008-9097-z,The Unexpected Behavior of Plurality Rule,September 2009,William V. Gehrlein,Dominique Lepelley,,Male,,Unknown,Mix,,
67.0,3.0,Theory and Decision,08 May 2008,https://link.springer.com/article/10.1007/s11238-008-9110-6,A characterization of majority voting rules with quorums,September 2009,Nicolas Houy,,,Male,Unknown,Unknown,Male,,8
67.0,3.0,Theory and Decision,06 February 2008,https://link.springer.com/article/10.1007/s11238-007-9083-x,Axiomatizations of a Class of Equal Surplus Sharing Solutions for TU-Games,September 2009,René van den Brink,Yukihiko Funaki,,Male,Male,Unknown,Male,,62
67.0,4.0,Theory and Decision,03 March 2009,https://link.springer.com/article/10.1007/s11238-009-9135-5,Interpersonal comparisons of utility in bargaining: evidence from a transcontinental ultimatum game,October 2009,Romina Boarini,Jean-François Laslier,Stéphane Robin,Female,Unknown,,Mix,,
67.0,4.0,Theory and Decision,22 June 2008,https://link.springer.com/article/10.1007/s11238-008-9113-3,Choosers as extension axioms,October 2009,Bora Erdamar,M. Remzi Sanver,,Male,Unknown,Unknown,Male,,7
67.0,4.0,Theory and Decision,13 February 2009,https://link.springer.com/article/10.1007/s11238-009-9133-7,Framing effects as violations of extensionality,October 2009,Sacha Bourgeois-Gironde,Raphaël Giraud,,,Male,Unknown,Mix,,
67.0,4.0,Theory and Decision,23 December 2008,https://link.springer.com/article/10.1007/s11238-008-9127-x,Living without state-independence of utilities,October 2009,Brian Hill,,,Male,Unknown,Unknown,Male,,5
67.0,4.0,Theory and Decision,21 April 2008,https://link.springer.com/article/10.1007/s11238-008-9105-3,Risk aversion in expected intertemporal discounted utilities bandit problems,October 2009,Jean-Philippe Chancelier,Michel De Lara,André de Palma,Unknown,Male,Male,Male,,6
68.0,1.0,Theory and Decision,15 December 2009,https://link.springer.com/article/10.1007/s11238-009-9188-5,Introduction to the special issue,February 2010,Manel Baucells,Antoni Bosch-Domènech,Franz H. Heukamp,Male,Male,Male,Male,,2
68.0,1.0,Theory and Decision,25 March 2009,https://link.springer.com/article/10.1007/s11238-009-9138-2,Interpersonal comparisons of utility for 2 of 3 types of people,February 2010,R. Duncan Luce,,,Unknown,Unknown,Unknown,Unknown,,
68.0,1.0,Theory and Decision,09 April 2009,https://link.springer.com/article/10.1007/s11238-009-9142-6,Expected utility theory under non-classical uncertainty,February 2010,V. I. Danilov,A. Lambert-Mogiliansky,,Unknown,Unknown,Unknown,Unknown,,
68.0,1.0,Theory and Decision,24 June 2009,https://link.springer.com/article/10.1007/s11238-009-9154-2,On the Conditional Value-at-Risk probability-dependent utility function,February 2010,Alexandre Street,,,Male,Unknown,Unknown,Male,,25
68.0,1.0,Theory and Decision,09 May 2009,https://link.springer.com/article/10.1007/s11238-009-9149-z,Invariant multiattribute utility functions,February 2010,Ali E. Abbas,,,Male,Unknown,Unknown,Male,,11
68.0,1.0,Theory and Decision,11 June 2009,https://link.springer.com/article/10.1007/s11238-009-9155-1,A descriptive multi-attribute utility model for everyday decisions,February 2010,Jie W. Weiss,David J. Weiss,Ward Edwards,,Male,Male,Mix,,
68.0,1.0,Theory and Decision,28 April 2009,https://link.springer.com/article/10.1007/s11238-009-9144-4,A parametric analysis of prospect theory’s functionals for the general population,February 2010,Adam S. Booij,Bernard M. S. van Praag,Gijs van de Kuilen,Male,Male,Male,Male,,165
68.0,1.0,Theory and Decision,04 January 2009,https://link.springer.com/article/10.1007/s11238-008-9130-2,Common ratio using delay,February 2010,Manel Baucells,Franz H. Heukamp,,Male,Male,Unknown,Male,,41
68.0,1.0,Theory and Decision,23 April 2009,https://link.springer.com/article/10.1007/s11238-009-9143-5,Reevaluating evidence on myopic loss aversion: aggregate patterns versus individual choices,February 2010,Pavlo R. Blavatskyy,Ganna Pogrebna,,Male,Female,Unknown,Mix,,
68.0,1.0,Theory and Decision,20 May 2009,https://link.springer.com/article/10.1007/s11238-009-9152-4,Endowment effects? “Even” with half a million on the table!,February 2010,Pavlo Blavatskyy,Ganna Pogrebna,,Male,Female,Unknown,Mix,,
68.0,1.0,Theory and Decision,27 May 2009,https://link.springer.com/article/10.1007/s11238-009-9153-3,Stability of risk preferences and the reflection effect of prospect theory,February 2010,Manel Baucells,Antonio Villasís,,Male,Male,Unknown,Male,,54
68.0,1.0,Theory and Decision,23 July 2009,https://link.springer.com/article/10.1007/s11238-009-9166-y,An experimental investigation of transitivity in set ranking,February 2010,Amélie Vrijdags,,,Female,Unknown,Unknown,Female,,3
68.0,1.0,Theory and Decision,02 April 2009,https://link.springer.com/article/10.1007/s11238-009-9139-1,Conflicting violations of transitivity and where they may lead us,February 2010,Brett Day,Graham Loomes,,Male,Male,Unknown,Male,,9
69.0,1.0,Theory and Decision,12 August 2008,https://link.springer.com/article/10.1007/s11238-008-9116-0,"An additive representation on the product of complete, continuous extensive structures",July 2010,Yutaka Matsushita,,,,Unknown,Unknown,Mix,,
69.0,1.0,Theory and Decision,13 July 2008,https://link.springer.com/article/10.1007/s11238-008-9115-1,Slutzky equations and substitution effects of risks in terms of mean-variance preferences,July 2010,Thomas Eichner,,,Male,Unknown,Unknown,Male,,1
69.0,1.0,Theory and Decision,15 May 2008,https://link.springer.com/article/10.1007/s11238-008-9107-1,Valuing future cash flows with non separable discount factors and non additive subjective measures: conditional Choquet capacities on time and on uncertainty,July 2010,Robert Kast,André Lapied,,Male,Male,Unknown,Male,,11
69.0,1.0,Theory and Decision,01 April 2009,https://link.springer.com/article/10.1007/s11238-009-9140-8,Dominance criteria for welfare comparisons: using equivalent income to describe differences in needs,July 2010,Udo Ebert,,,Male,Unknown,Unknown,Male,,1
69.0,1.0,Theory and Decision,23 April 2008,https://link.springer.com/article/10.1007/s11238-008-9109-z,A model of influence in a social network,July 2010,Michel Grabisch,Agnieszka Rusinowska,,Male,Female,Unknown,Mix,,
69.0,1.0,Theory and Decision,26 September 2008,https://link.springer.com/article/10.1007/s11238-008-9118-y,Information aggregation and preference heterogeneity in committees,July 2010,Elisabeth Schulte,,,Female,Unknown,Unknown,Female,,18
69.0,1.0,Theory and Decision,16 June 2009,https://link.springer.com/article/10.1007/s11238-009-9161-3,Information-driven coordination: experimental results with heterogeneous individuals,July 2010,Viktoriya Semeshenko,Alexis Garapin,Mirta B. Gordon,Female,Male,Female,Mix,,
69.0,1.0,Theory and Decision,24 February 2010,https://link.springer.com/article/10.1007/s11238-010-9200-0,Overconfidence in tournaments: evidence from the field,July 2010,Young Joon Park,Luís Santos-Pinto,,,Male,Unknown,Mix,,
69.0,2.0,Theory and Decision,22 December 2009,https://link.springer.com/article/10.1007/s11238-009-9190-y,Cycles versus equilibrium in evolutionary games,August 2010,Thomas W. L. Norman,,,Male,Unknown,Unknown,Male,,2
69.0,2.0,Theory and Decision,23 June 2009,https://link.springer.com/article/10.1007/s11238-009-9158-y,Perceptron versus automaton in the finitely repeated prisoner’s dilemma,August 2010,Sylvain Béal,,,Male,Unknown,Unknown,Male,,2
69.0,2.0,Theory and Decision,04 December 2008,https://link.springer.com/article/10.1007/s11238-008-9125-z,Population monotonic path schemes for simple games,August 2010,Barış Çiftçi,Peter Borm,Herbert Hamers,Male,Male,Male,Male,,4
69.0,2.0,Theory and Decision,30 October 2008,https://link.springer.com/article/10.1007/s11238-008-9121-3,Institutions matter! Why the Herder Problem is not a Prisoner’s Dilemma,August 2010,Daniel H. Cole,Peter Z. Grossman,,Male,Male,Unknown,Male,,10
69.0,2.0,Theory and Decision,23 July 2009,https://link.springer.com/article/10.1007/s11238-009-9167-x,Characterization of dominance relations in finite coalitional games,August 2010,Felix Brandt,Paul Harrenstein,,Male,Male,Unknown,Male,,8
69.0,2.0,Theory and Decision,05 March 2009,https://link.springer.com/article/10.1007/s11238-009-9136-4,Simple methods for evaluating and comparing binary experiments,August 2010,Thomas A. Weber,,,Male,Unknown,Unknown,Male,,5
69.0,2.0,Theory and Decision,11 December 2009,https://link.springer.com/article/10.1007/s11238-009-9187-6,On the stability of a triplet of scoring rules,August 2010,Mostapha Diss,Vincent Merlin,,Unknown,Male,Unknown,Male,,11
69.0,2.0,Theory and Decision,09 December 2008,https://link.springer.com/article/10.1007/s11238-008-9126-y,A linear generalization of Stackelberg’s model,August 2010,Thierry Lafay,,,Male,Unknown,Unknown,Male,,8
69.0,3.0,Theory and Decision,11 May 2010,https://link.springer.com/article/10.1007/s11238-010-9214-7,Choosing monetary sequences: theory and experimental evidence,September 2010,Paola Manzini,Marco Mariotti,Luigi Mittone,Female,Male,Male,Mix,,
69.0,3.0,Theory and Decision,26 September 2008,https://link.springer.com/article/10.1007/s11238-008-9119-x,Choice with imprecise information: an experimental approach,September 2010,Takashi Hayashi,Ryoko Wada,,Male,Female,Unknown,Mix,,
69.0,3.0,Theory and Decision,16 December 2008,https://link.springer.com/article/10.1007/s11238-008-9128-9,Betting on Machina’s reflection example: an experiment on ambiguity,September 2010,Olivier L’Haridon,Lætitia Placido,,Male,Unknown,Unknown,Male,,44
69.0,3.0,Theory and Decision,16 February 2010,https://link.springer.com/article/10.1007/s11238-010-9193-8,The influence of probabilities on the response mode bias in utility elicitation,September 2010,Christopher Schwand,Rudolf Vetschera,Lea M. Wakolbinger,Male,Male,Female,Mix,,
69.0,3.0,Theory and Decision,08 July 2009,https://link.springer.com/article/10.1007/s11238-009-9164-0,"Ambiguity, pessimism, and rational religious choice",September 2010,Tigran Melkonyan,Mark Pingle,,Male,Male,Unknown,Male,,4
69.0,3.0,Theory and Decision,16 June 2009,https://link.springer.com/article/10.1007/s11238-009-9159-x,A new monotonicity condition for tournament solutions,September 2010,İpek Özkal-Sanver,M. Remzi Sanver,,Female,Unknown,Unknown,Female,,3
69.0,3.0,Theory and Decision,27 September 2008,https://link.springer.com/article/10.1007/s11238-008-9120-4,Aggregation of correlated votes and Condorcet’s Jury Theorem,September 2010,Serguei Kaniovski,,,Male,Unknown,Unknown,Male,,31
69.0,3.0,Theory and Decision,12 February 2009,https://link.springer.com/article/10.1007/s11238-009-9131-9,Groups can make a difference: voting power measures extended,September 2010,Claus Beisbart,,,Male,Unknown,Unknown,Male,,2
69.0,3.0,Theory and Decision,05 May 2010,https://link.springer.com/article/10.1007/s11238-010-9215-6,Sleeping Beauty and the absent-minded driver,September 2010,Jean Baratgin,Bernard Walliser,,Male,Male,Unknown,Male,,4
69.0,4.0,Theory and Decision,18 February 2009,https://link.springer.com/article/10.1007/s11238-009-9134-6,Combining strength and uncertainty for preferences in the graph model for conflict resolution with multiple decision makers,October 2010,Haiyan Xu,Keith W. Hipel,Ye Chen,Unknown,Male,,Mix,,
69.0,4.0,Theory and Decision,30 April 2009,https://link.springer.com/article/10.1007/s11238-009-9148-0,Another characterization of the Owen value without the additivity axiom,October 2010,André Casajus,,,Male,Unknown,Unknown,Male,,11
69.0,4.0,Theory and Decision,28 November 2008,https://link.springer.com/article/10.1007/s11238-008-9122-2,Stability and Nash implementation in matching markets with couples,October 2010,Claus-Jochen Haake,Bettina Klaus,,Unknown,Female,Unknown,Female,,2
69.0,4.0,Theory and Decision,14 March 2009,https://link.springer.com/article/10.1007/s11238-009-9137-3,The self-dual serial cost-sharing rule,October 2010,M. J. Albizuri,,,Unknown,Unknown,Unknown,Unknown,,
69.0,4.0,Theory and Decision,13 February 2009,https://link.springer.com/article/10.1007/s11238-009-9132-8,Guilt and shame: an axiomatic analysis,October 2010,Raúl López-Pérez,,,Male,Unknown,Unknown,Male,,16
69.0,4.0,Theory and Decision,29 April 2009,https://link.springer.com/article/10.1007/s11238-009-9146-2,On social utility payoffs in games: a methodological comparison between Behavioural and Rational Game Theory,October 2010,Luca Zarri,,,Male,Unknown,Unknown,Male,,2
69.0,4.0,Theory and Decision,06 May 2009,https://link.springer.com/article/10.1007/s11238-009-9147-1,Testing transitivity in choice under risk,October 2010,Michael H. Birnbaum,Ulrich Schmidt,,Male,Male,Unknown,Male,,18
69.0,4.0,Theory and Decision,11 June 2009,https://link.springer.com/article/10.1007/s11238-009-9156-0,Nonbinding recommendations: the relative effects of focal points versus uncertainty reduction on bargaining outcomes,October 2010,David L. Dickinson,Lynn Hunnicutt,,Male,,Unknown,Mix,,
69.0,4.0,Theory and Decision,10 May 2009,https://link.springer.com/article/10.1007/s11238-009-9150-6,A model of influence with an ordered set of possible actions,October 2010,Michel Grabisch,Agnieszka Rusinowska,,Male,Female,Unknown,Mix,,
69.0,4.0,Theory and Decision,03 April 2009,https://link.springer.com/article/10.1007/s11238-009-9141-7,Values for rooted-tree and sink-tree digraph games and sharing a river,October 2010,Anna B. Khmelnitskaya,,,Female,Unknown,Unknown,Female,,38
70.0,1.0,Theory and Decision,06 October 2009,https://link.springer.com/article/10.1007/s11238-009-9174-y,A decision-theoretical view of default priors,January 2011,Stephen G. Walker,Eduardo Gutiérrez-Peña,,Male,Male,Unknown,Male,,1
70.0,1.0,Theory and Decision,17 June 2010,https://link.springer.com/article/10.1007/s11238-010-9222-7,Characterization of the existence of semicontinuous weak utilities for binary relations,January 2011,Athanasios Andrikopoulos,,,Male,Unknown,Unknown,Male,,4
70.0,1.0,Theory and Decision,08 October 2009,https://link.springer.com/article/10.1007/s11238-009-9176-9,Mixed risk aversion and preference for risk disaggregation: a story of moments,January 2011,Patrick Roger,,,Male,Unknown,Unknown,Male,,13
70.0,1.0,Theory and Decision,24 October 2009,https://link.springer.com/article/10.1007/s11238-009-9179-6,Individual and couple decision behavior under risk: evidence on the dynamics of power balance,January 2011,André de Palma,Nathalie Picard,Anthony Ziegelmeyer,Male,Female,Male,Mix,,
70.0,1.0,Theory and Decision,08 July 2009,https://link.springer.com/article/10.1007/s11238-009-9163-1,Does product complexity matter for competition in experimental retail markets?,January 2011,Stefania Sitzia,Daniel John Zizzo,,Female,Male,Unknown,Mix,,
70.0,1.0,Theory and Decision,13 September 2009,https://link.springer.com/article/10.1007/s11238-009-9172-0,A model of consumption-dependent temptation,January 2011,Wojciech Olszewski,,,Male,Unknown,Unknown,Male,,11
70.0,1.0,Theory and Decision,09 April 2010,https://link.springer.com/article/10.1007/s11238-010-9206-7,Remarks on the consumer problem under incomplete preferences,January 2011,Leandro Nascimento,,,Male,Unknown,Unknown,Male,,2
70.0,1.0,Theory and Decision,25 December 2009,https://link.springer.com/article/10.1007/s11238-009-9189-4,A model of procedural and distributive fairness,January 2011,Michal Wiktor Krawczyk,,,Male,Unknown,Unknown,Male,,67
70.0,2.0,Theory and Decision,05 September 2010,https://link.springer.com/article/10.1007/s11238-010-9227-2,Statistical decisions under ambiguity,February 2011,Jörg Stoye,,,Male,Unknown,Unknown,Male,,33
70.0,2.0,Theory and Decision,10 February 2010,https://link.springer.com/article/10.1007/s11238-010-9194-7,Elementary proof that mean–variance implies quadratic utility,February 2011,D. J. Johnstone,D. V. Lindley,,Unknown,Unknown,Unknown,Unknown,,
70.0,2.0,Theory and Decision,18 April 2010,https://link.springer.com/article/10.1007/s11238-010-9211-x,"Political influence in multi-choice institutions: cyclicity, anonymity, and transitivity",February 2011,Roland Pongou,Bertrand Tchantcho,Lawrence Diffo Lambo,Male,Male,Male,Male,,23
70.0,2.0,Theory and Decision,07 May 2010,https://link.springer.com/article/10.1007/s11238-010-9217-4,Portfolio allocation and asset demand with mean-variance preferences,February 2011,Thomas Eichner,Andreas Wagener,,Male,Male,Unknown,Male,,14
70.0,2.0,Theory and Decision,03 November 2010,https://link.springer.com/article/10.1007/s11238-010-9235-2,Beneficial safety decreases,February 2011,Till Grüne-Yanoff,Holger Rosencrantz,,Male,Male,Unknown,Male,,
70.0,2.0,Theory and Decision,17 June 2009,https://link.springer.com/article/10.1007/s11238-009-9160-4,Justification of functional form assumptions in structural models: applications and testing of qualitative measurement axioms,February 2011,John K. Dagsvik,Stine Røine Hoff,,Male,Female,Unknown,Mix,,
70.0,3.0,Theory and Decision,31 December 2010,https://link.springer.com/article/10.1007/s11238-010-9241-4,Interval scalability of rank-dependent utility,March 2011,Mikhail V. Sokolov,,,Male,Unknown,Unknown,Male,,
70.0,3.0,Theory and Decision,21 February 2010,https://link.springer.com/article/10.1007/s11238-010-9202-y,Decision theory with prospect interference and entanglement,March 2011,V. I. Yukalov,D. Sornette,,Unknown,Unknown,Unknown,Unknown,,
70.0,3.0,Theory and Decision,04 February 2010,https://link.springer.com/article/10.1007/s11238-010-9195-6,Performance of procrastinators: on the value of deadlines,March 2011,Fabian Herweg,Daniel Müller,,Male,Male,Unknown,Male,,15
70.0,3.0,Theory and Decision,08 July 2010,https://link.springer.com/article/10.1007/s11238-010-9225-4,The aggregation of preferences: can we ignore the past?,March 2011,Stéphane Zuber,,,,Unknown,Unknown,Mix,,
70.0,3.0,Theory and Decision,18 July 2009,https://link.springer.com/article/10.1007/s11238-009-9165-z,Economic Darwinism,March 2011,Birgitte Sloth,Hans Jørgen Whitta-Jacobsen,,Female,Male,Unknown,Mix,,
70.0,4.0,Theory and Decision,08 October 2009,https://link.springer.com/article/10.1007/s11238-009-9175-x,Context dependence and consistency in dynamic choice under uncertainty: the case of anticipated regret,April 2011,Takashi Hayashi,,,Male,Unknown,Unknown,Male,,14
70.0,4.0,Theory and Decision,24 July 2009,https://link.springer.com/article/10.1007/s11238-009-9169-8,Lottery pricing under time pressure,April 2011,Pavlo R. Blavatskyy,Wolfgang R. Köhler,,Male,Male,Unknown,Male,,4
70.0,4.0,Theory and Decision,15 October 2009,https://link.springer.com/article/10.1007/s11238-009-9177-8,How to consult an expert? Opinion versus evidence,April 2011,Thomas Lanzi,Jerome Mathis,,Male,Male,Unknown,Male,,
70.0,4.0,Theory and Decision,10 February 2010,https://link.springer.com/article/10.1007/s11238-010-9199-2,Beliefs about overconfidence,April 2011,Sandra Ludwig,Julia Nafziger,,Female,Female,Unknown,Female,,15
70.0,4.0,Theory and Decision,16 April 2010,https://link.springer.com/article/10.1007/s11238-010-9201-z,Common characterizations of the untrapped set and the top cycle,April 2011,Nicolas Houy,,,Male,Unknown,Unknown,Male,,2
70.0,4.0,Theory and Decision,10 September 2010,https://link.springer.com/article/10.1007/s11238-010-9228-1,Finding socially best spanning trees,April 2011,Andreas Darmann,Christian Klamler,Ulrich Pferschy,Male,Male,Male,Male,,4
70.0,4.0,Theory and Decision,12 September 2010,https://link.springer.com/article/10.1007/s11238-010-9231-6,Experiments on bilateral bargaining in markets,April 2011,Andreas Tutic,Stefan Pfau,André Casajus,Male,Male,Male,Male,,6
71.0,1.0,Theory and Decision,02 April 2010,https://link.springer.com/article/10.1007/s11238-010-9210-y,Tribute to Jean-Yves Jaffray,July 2011,Michèle Cohen,Alain Chateauneuf,Jean-Christophe Vergnaud,Female,Male,Unknown,Mix,,
71.0,1.0,Theory and Decision,31 March 2010,https://link.springer.com/article/10.1007/s11238-010-9209-4,Jaffray’s ideas on ambiguity,July 2011,Peter P. Wakker,,,Male,Unknown,Unknown,Male,,8
71.0,1.0,Theory and Decision,05 February 2010,https://link.springer.com/article/10.1007/s11238-010-9197-4,Are beliefs a matter of taste? A case for objective imprecise information,July 2011,Raphaël Giraud,Jean-Marc Tallon,,Male,Unknown,Unknown,Male,,9
71.0,1.0,Theory and Decision,13 June 2010,https://link.springer.com/article/10.1007/s11238-010-9219-2,PRM inference using Jaffray & Faÿ’s Local Conditioning,July 2011,Christophe Gonzales,Pierre-Henri Wuillemin,,Male,Unknown,Unknown,Male,,4
71.0,1.0,Theory and Decision,23 January 2010,https://link.springer.com/article/10.1007/s11238-009-9192-9,Risk aversion elicitation: reconciling tractability and bias minimization,July 2011,Mohammed Abdellaoui,Ahmed Driouchi,Olivier L’Haridon,Male,Male,Male,Male,,59
71.0,1.0,Theory and Decision,07 March 2010,https://link.springer.com/article/10.1007/s11238-010-9205-8,An experimental investigation of imprecision attitude and its relation with risk attitude and impatience,July 2011,Michèle Cohen,Jean-Marc Tallon,Jean-Christophe Vergnaud,Female,Unknown,Unknown,Female,,30
71.0,1.0,Theory and Decision,27 February 2010,https://link.springer.com/article/10.1007/s11238-010-9203-x,Regular updating,July 2011,Alain Chateauneuf,Thibault Gajdos,Jean-Yves Jaffray,Male,Male,Unknown,Male,,9
71.0,1.0,Theory and Decision,18 July 2009,https://link.springer.com/article/10.1007/s11238-009-9162-2,How to deal with partially analyzable acts?,July 2011,Jean-Yves Jaffray,Meglena Jeleva,,Unknown,Female,Unknown,Female,,4
71.0,2.0,Theory and Decision,01 July 2009,https://link.springer.com/article/10.1007/s11238-009-9157-z,Condorcet vs. Borda in light of a dual majoritarian approach,August 2011,Eyal Baharad,Shmuel Nitzan,,Male,Male,Unknown,Male,,
71.0,2.0,Theory and Decision,11 August 2009,https://link.springer.com/article/10.1007/s11238-009-9171-1,"Differential marginality, van den Brink fairness, and the Shapley value",August 2011,André Casajus,,,Male,Unknown,Unknown,Male,,46
71.0,2.0,Theory and Decision,01 April 2011,https://link.springer.com/article/10.1007/s11238-011-9246-7,Strategic behavior under partial cooperation,August 2011,Subhadip Chakrabarti,Robert P. Gilles,Emiliya A. Lazarova,Unknown,Male,Female,Mix,,
71.0,2.0,Theory and Decision,24 October 2009,https://link.springer.com/article/10.1007/s11238-009-9182-y,Actualist rationality,August 2011,Charles F. Manski,,,Male,Unknown,Unknown,Male,,12
71.0,2.0,Theory and Decision,30 October 2009,https://link.springer.com/article/10.1007/s11238-009-9181-z,Dynamic reasoning and time pressure: Transition from analytical operations to experiential responses,August 2011,Peter A. F. Fraser-Mackenzie,Itiel E. Dror,,Male,Male,Unknown,Male,,12
71.0,2.0,Theory and Decision,19 September 2009,https://link.springer.com/article/10.1007/s11238-009-9173-z,Risk vulnerability: a graphical interpretation,August 2011,Louis Eeckhoudt,Béatrice Rey,,Male,Female,Unknown,Mix,,
71.0,2.0,Theory and Decision,22 July 2009,https://link.springer.com/article/10.1007/s11238-009-9168-9,A note on negativity bias and framing response asymmetry,August 2011,Doron Sonsino,,,Male,Unknown,Unknown,Male,,
71.0,2.0,Theory and Decision,16 June 2010,https://link.springer.com/article/10.1007/s11238-010-9223-6,The stochastic component in choice and regression to the mean,August 2011,Aurora García-Gallego,Nikolaos Georgantzís,Gerardo Sabater-Grande,Female,Male,Male,Mix,,
71.0,2.0,Theory and Decision,05 February 2010,https://link.springer.com/article/10.1007/s11238-010-9196-5,Endogenous entry in lowest-unique sealed-bid auctions,August 2011,Harold Houba,Dinard van der Laan,Dirk Veldhuizen,Male,Unknown,Male,Male,,16
71.0,3.0,Theory and Decision,04 April 2010,https://link.springer.com/article/10.1007/s11238-010-9198-3,A representation of preferences by the Choquet integral with respect to a 2-additive capacity,September 2011,Brice Mayag,Michel Grabisch,Christophe Labreuche,Male,Male,Male,Male,,37
71.0,3.0,Theory and Decision,09 January 2010,https://link.springer.com/article/10.1007/s11238-009-9191-x,Solving the St. Petersburg Paradox in cumulative prospect theory: the right amount of probability weighting,September 2011,Marie Pfiffelmann,,,Female,Unknown,Unknown,Female,,13
71.0,3.0,Theory and Decision,25 October 2009,https://link.springer.com/article/10.1007/s11238-009-9183-x,Dynamic inconsistency and choice,September 2011,Isabelle Brocas,,,Female,Unknown,Unknown,Female,,1
71.0,3.0,Theory and Decision,15 June 2010,https://link.springer.com/article/10.1007/s11238-010-9224-5,"Marginality, differential marginality, and the Banzhaf value",September 2011,André Casajus,,,Male,Unknown,Unknown,Male,,11
71.0,3.0,Theory and Decision,17 November 2009,https://link.springer.com/article/10.1007/s11238-009-9184-9,"R&D cooperation in emerging industries, asymmetric innovative capabilities and rationale for technology parks",September 2011,Vivekananda Mukherjee,Shyama V. Ramani,,Unknown,Unknown,Unknown,Unknown,,
71.0,3.0,Theory and Decision,04 March 2010,https://link.springer.com/article/10.1007/s11238-010-9204-9,Coalitional stability and efficiency of partitions in matching problems,September 2011,Duygu Nizamogullari,İpek Özkal-Sanver,,,Female,Unknown,Mix,,
71.0,3.0,Theory and Decision,18 April 2010,https://link.springer.com/article/10.1007/s11238-010-9207-6,The willingness-to-accept/willingness-to-pay disparity in repeated markets: loss aversion or ‘bad-deal’ aversion?,September 2011,Andrea Isoni,,,Female,Unknown,Unknown,Female,,48
71.0,3.0,Theory and Decision,07 September 2010,https://link.springer.com/article/10.1007/s11238-010-9230-7,Empirical rules of thumb for choice under uncertainty,September 2011,Rolf Aaberge,,,Male,Unknown,Unknown,Male,,
71.0,4.0,Theory and Decision,30 July 2009,https://link.springer.com/article/10.1007/s11238-009-9170-2,Optimal jury design for homogeneous juries with correlated votes,October 2011,Serguei Kaniovski,Alexander Zaigraev,,Male,Male,Unknown,Male,,27
71.0,4.0,Theory and Decision,27 March 2010,https://link.springer.com/article/10.1007/s11238-010-9208-5,Making statements and approval voting,October 2011,Enriqueta Aragones,Itzhak Gilboa,Andrew Weiss,Female,Male,Male,Mix,,
71.0,4.0,Theory and Decision,13 April 2011,https://link.springer.com/article/10.1007/s11238-011-9248-5,Words versus actions as a means to influence cooperation in social dilemma situations,October 2011,Ganna Pogrebna,David H. Krantz,Claudia Keser,Female,Male,Female,Mix,,
71.0,4.0,Theory and Decision,23 June 2011,https://link.springer.com/article/10.1007/s11238-011-9261-8,"Pareto principles, positive responsiveness, and majority decisions",October 2011,Susumu Cato,,,Male,Unknown,Unknown,Male,,8
71.0,4.0,Theory and Decision,04 December 2010,https://link.springer.com/article/10.1007/s11238-010-9238-z,Signaling strength? An analysis of decision making in The Weakest Link,October 2011,Marco A. Haan,Bart Los,Yohanes E. Riyanto,Male,Male,Unknown,Male,,1
71.0,4.0,Theory and Decision,18 June 2011,https://link.springer.com/article/10.1007/s11238-011-9266-3,Learning strategic environments: an experimental study of strategy formation and transfer,October 2011,Andreas Nicklisch,,,Male,Unknown,Unknown,Male,,3
71.0,4.0,Theory and Decision,29 October 2009,https://link.springer.com/article/10.1007/s11238-009-9180-0,Equivalent comparisons of information channels,October 2011,Hiroyuki Nakata,,,Male,Unknown,Unknown,Male,,3
71.0,4.0,Theory and Decision,30 November 2010,https://link.springer.com/article/10.1007/s11238-010-9239-y,Stochastic revealed preference and rationalizability,October 2011,Jan Heufer,,,Male,Unknown,Unknown,Male,,2
71.0,4.0,Theory and Decision,29 May 2010,https://link.springer.com/article/10.1007/s11238-010-9218-3,Syntactic foundations for unawareness of theorems,October 2011,Spyros Galanis,,,Male,Unknown,Unknown,Male,,25
71.0,4.0,Theory and Decision,08 April 2010,https://link.springer.com/article/10.1007/s11238-010-9212-9,The ratio bias phenomenon: fact or artifact?,October 2011,Mathieu Lefebvre,Ferdinand M. Vieider,Marie Claire Villeval,Male,Male,Female,Mix,,
71.0,4.0,Theory and Decision,29 April 2010,https://link.springer.com/article/10.1007/s11238-010-9213-8,Risk-adjusted martingales and the design of “indifference” gambles,October 2011,Ali E. Abbas,,,Male,Unknown,Unknown,Male,,
71.0,4.0,Theory and Decision,13 June 2010,https://link.springer.com/article/10.1007/s11238-010-9221-8,Re-examining the law of iterated expectations for Choquet decision makers,October 2011,Alexander Zimper,,,Male,Unknown,Unknown,Male,,8
72.0,1.0,Theory and Decision,17 June 2011,https://link.springer.com/article/10.1007/s11238-011-9254-7,Indecisiveness aversion and preference for commitment,January 2012,Eric Danan,Ani Guerdjikova,Alexander Zimper,Male,Female,Male,Mix,,
72.0,1.0,Theory and Decision,19 June 2011,https://link.springer.com/article/10.1007/s11238-011-9259-2,Ambiguity aversion in multi-armed bandit problems,January 2012,Christopher M. Anderson,,,Male,Unknown,Unknown,Male,,25
72.0,1.0,Theory and Decision,10 November 2010,https://link.springer.com/article/10.1007/s11238-010-9236-1,Why does myopia decrease the willingness to invest? Is it myopic loss aversion or myopic loss probability aversion?,January 2012,Stefan Zeisberger,Thomas Langer,Martin Weber,Male,Male,Male,Male,,9
72.0,1.0,Theory and Decision,09 June 2011,https://link.springer.com/article/10.1007/s11238-011-9253-8,An experimental study of the generosity game,January 2012,Werner Güth,M. Vittoria Levati,Matteo Ploner,Male,Unknown,Male,Male,,17
72.0,1.0,Theory and Decision,11 June 2010,https://link.springer.com/article/10.1007/s11238-010-9220-9,The puzzle of cooperation in a game of chicken: an experimental study,January 2012,Marie-Laure Cabon-Dhersin,Nathalie Etchart-Vincent,,Unknown,Female,Unknown,Female,,5
72.0,1.0,Theory and Decision,05 May 2010,https://link.springer.com/article/10.1007/s11238-010-9216-5,Bounds on the competence of a homogeneous jury,January 2012,Alexander Zaigraev,Serguei Kaniovski,,Male,Male,Unknown,Male,,4
72.0,1.0,Theory and Decision,09 January 2011,https://link.springer.com/article/10.1007/s11238-010-9240-5,Beyond Condorcet: optimal aggregation rules using voting records,January 2012,Eyal Baharad,Jacob Goldberger,Shmuel Nitzan,Male,Male,Male,Male,,12
72.0,1.0,Theory and Decision,19 September 2010,https://link.springer.com/article/10.1007/s11238-010-9229-0,A characterization of the maximin rule in the context of voting,January 2012,Ronan Congar,Vincent Merlin,,Male,Male,Unknown,Male,,16
72.0,2.0,Theory and Decision,29 March 2011,https://link.springer.com/article/10.1007/s11238-011-9244-9,A short step between democracy and dictatorship,February 2012,Antonio Quesada,,,Male,Unknown,Unknown,Male,,1
72.0,2.0,Theory and Decision,10 April 2011,https://link.springer.com/article/10.1007/s11238-011-9249-4,Using turn taking to achieve intertemporal cooperation and symmetry in infinitely repeated 2 × 2 games,February 2012,Sau-Him Paul Lau,Vai-Lam Mui,,Unknown,Unknown,Unknown,Unknown,,
72.0,2.0,Theory and Decision,27 March 2011,https://link.springer.com/article/10.1007/s11238-011-9251-x,Implementation without rationality assumptions,February 2012,Ville Korpela,,,Male,Unknown,Unknown,Male,,8
72.0,2.0,Theory and Decision,13 April 2011,https://link.springer.com/article/10.1007/s11238-011-9247-6,Infinity in the lab. How do people play repeated games?,February 2012,Lisa Bruttel,Ulrich Kamecke,,Female,Male,Unknown,Mix,,
72.0,2.0,Theory and Decision,22 April 2011,https://link.springer.com/article/10.1007/s11238-011-9245-8,Reputation and influence in charitable giving: an experiment,February 2012,David Reinstein,Gerhard Riener,,Male,Male,Unknown,Male,,42
72.0,2.0,Theory and Decision,04 August 2010,https://link.springer.com/article/10.1007/s11238-010-9226-3,Revealed preference tests for consistency with weakly separable indirect utility,February 2012,Per Hjertstrand,James L. Swofford,,Male,Male,Unknown,Male,,4
72.0,2.0,Theory and Decision,21 June 2011,https://link.springer.com/article/10.1007/s11238-011-9255-6,Parametric multi-attribute utility functions for optimal profit under risk constraints,February 2012,Babacar Seck,Laetitia Andrieu,Michel De Lara,Unknown,Female,Male,Mix,,
72.0,2.0,Theory and Decision,02 October 2010,https://link.springer.com/article/10.1007/s11238-010-9232-5,Aspects concerning entropy and utility,February 2012,A. R. Hoseinzadeh,G. R. Mohtashami Borzadaran,G. H. Yari,Unknown,Unknown,Unknown,Unknown,,
72.0,3.0,Theory and Decision,29 November 2011,https://link.springer.com/article/10.1007/s11238-011-9283-2,"Strategy-proofness, tops-only, and the uniform rule",March 2012,Toyotaka Sakai,Takuma Wakayama,,Unknown,Male,Unknown,Male,,6
72.0,3.0,Theory and Decision,10 December 2011,https://link.springer.com/article/10.1007/s11238-011-9285-0,Multiple motives of pro-social behavior: evidence from the solidarity game,March 2012,Friedel Bolle,Yves Breitmoser,Claudia Vogel,,Male,Female,Mix,,
72.0,3.0,Theory and Decision,30 December 2011,https://link.springer.com/article/10.1007/s11238-011-9288-x,Quasi stable outcomes in the assignment game,March 2012,Raïssa-Juvette Samba Zitou,Rhonya Adli,,Unknown,Unknown,Unknown,Unknown,,
72.0,3.0,Theory and Decision,19 June 2011,https://link.springer.com/article/10.1007/s11238-011-9260-9,Is specialization desirable in committee decision making?,March 2012,Ruth Ben-Yashar,Winston T. H. Koh,Shmuel Nitzan,Female,Male,Male,Mix,,
72.0,3.0,Theory and Decision,26 November 2010,https://link.springer.com/article/10.1007/s11238-010-9234-3,Measuring the time stability of Prospect Theory preferences,March 2012,Stefan Zeisberger,Dennis Vrecko,Thomas Langer,Male,Male,Male,Male,,73
72.0,3.0,Theory and Decision,17 June 2011,https://link.springer.com/article/10.1007/s11238-011-9256-5,The γ-core in Cournot oligopoly TU-games with capacity constraints,March 2012,Aymeric Lardon,,,Male,Unknown,Unknown,Male,,22
72.0,3.0,Theory and Decision,02 July 2011,https://link.springer.com/article/10.1007/s11238-011-9268-1,Complementarity of behavioral biases,March 2012,Toru Suzuki,,,Male,Unknown,Unknown,Male,,1
72.0,4.0,Theory and Decision,16 September 2011,https://link.springer.com/article/10.1007/s11238-011-9278-z,"Order extensions, budget correspondences, and rational choice",April 2012,Susanne Fuchs-Seliger,,,Female,Unknown,Unknown,Female,,
72.0,4.0,Theory and Decision,13 October 2011,https://link.springer.com/article/10.1007/s11238-011-9281-4,Internal slackening scoring methods,April 2012,Marco Slikker,Peter Borm,René  van den Brink,Male,Male,Male,Male,,1
72.0,4.0,Theory and Decision,10 June 2011,https://link.springer.com/article/10.1007/s11238-011-9252-9,Core/periphery scientific collaboration networks among very similar researchers,April 2012,Antoni Rubí-Barceló,,,Male,Unknown,Unknown,Male,,5
72.0,4.0,Theory and Decision,08 October 2011,https://link.springer.com/article/10.1007/s11238-011-9280-5,Values with exogenous payments,April 2012,Harald Wiese,,,Male,Unknown,Unknown,Male,,
72.0,4.0,Theory and Decision,01 July 2011,https://link.springer.com/article/10.1007/s11238-011-9273-4,A revealed preference analysis of solutions to simple allocation problems,April 2012,Özgür Kıbrıs,,,Male,Unknown,Unknown,Male,,13
72.0,4.0,Theory and Decision,16 December 2011,https://link.springer.com/article/10.1007/s11238-011-9286-z,Pareto efficiency in multiple referendum,April 2012,Tuğçe Çuhadaroğlu,Jean Lainé,,Female,Male,Unknown,Mix,,
72.0,4.0,Theory and Decision,23 June 2011,https://link.springer.com/article/10.1007/s11238-011-9267-2,Choosing a gambling partner: testing a model of mutual insurance in the lab,April 2012,Daniela Di Cagno,Emanuela Sciubba,Marco Spallone,Female,Female,Male,Mix,,
73.0,1.0,Theory and Decision,25 June 2011,https://link.springer.com/article/10.1007/s11238-011-9275-2,Introduction to FUR special issue,July 2012,Glenn W. Harrison,Morten I. Lau,Daniel Read,Male,Male,Male,Male,,
73.0,1.0,Theory and Decision,02 August 2011,https://link.springer.com/article/10.1007/s11238-011-9272-5,Pure hyperbolic discount curves predict “eyes open” self-control,July 2012,George Ainslie,,,Male,Unknown,Unknown,Male,,34
73.0,1.0,Theory and Decision,31 July 2011,https://link.springer.com/article/10.1007/s11238-011-9269-0,Regret aversion in reason-based choice,July 2012,Terry Connolly,Jochen Reb,,,Male,Unknown,Mix,,
73.0,1.0,Theory and Decision,01 July 2011,https://link.springer.com/article/10.1007/s11238-011-9270-7,A choice for ‘me’ or for ‘us’? Using we-reasoning to predict cooperation and coordination in games,July 2012,David J. Butler,,,Male,Unknown,Unknown,Male,,13
73.0,1.0,Theory and Decision,14 July 2011,https://link.springer.com/article/10.1007/s11238-011-9277-0,Non-linear mixed logit,July 2012,Steffen Andersen,Glenn W. Harrison,E. Elisabet Rutström,Male,Male,Unknown,Male,,29
73.0,1.0,Theory and Decision,15 July 2011,https://link.springer.com/article/10.1007/s11238-011-9274-3,Behavioral biases and the representative agent,July 2012,Elyès Jouini,Clotilde Napp,,Unknown,Female,Unknown,Female,,6
73.0,1.0,Theory and Decision,12 July 2011,https://link.springer.com/article/10.1007/s11238-011-9271-6,The price for information about probabilities and its relation with risk and ambiguity,July 2012,Giuseppe Attanasi,Aldo Montesano,,Male,Male,Unknown,Male,,4
73.0,1.0,Theory and Decision,19 July 2011,https://link.springer.com/article/10.1007/s11238-011-9276-1,Inferring beliefs as subjectively imprecise probabilities,July 2012,Steffen Andersen,John Fountain,E. Elisabet Rutström,Male,Male,Unknown,Male,,13
73.0,2.0,Theory and Decision,03 November 2010,https://link.springer.com/article/10.1007/s11238-010-9233-4,Are bygones bygones?,August 2012,Robin Cubitt,Maria Ruiz-Martos,Chris Starmer,,Female,,Mix,,
73.0,2.0,Theory and Decision,18 March 2012,https://link.springer.com/article/10.1007/s11238-012-9296-5,Stability of risk preference measures: results from a field experiment on French farmers,August 2012,Arnaud Reynaud,Stéphane Couture,,Male,,Unknown,Mix,,
73.0,2.0,Theory and Decision,11 April 2012,https://link.springer.com/article/10.1007/s11238-012-9303-x,On the problem of network monopoly,August 2012,Jolian McHardy,Michael Reynolds,Stephen Trotter,Unknown,Male,Male,Male,,
73.0,2.0,Theory and Decision,23 June 2012,https://link.springer.com/article/10.1007/s11238-012-9316-5,Uncertain indemnity and the demand for insurance,August 2012,Kangoh Lee,,,Unknown,Unknown,Unknown,Unknown,,
73.0,2.0,Theory and Decision,14 December 2010,https://link.springer.com/article/10.1007/s11238-010-9237-0,Degrading network capacity may improve performance: private versus public monitoring in the Braess Paradox,August 2012,Eyran J. Gisches,Amnon Rapoport,,Unknown,Male,Unknown,Male,,22
73.0,2.0,Theory and Decision,01 May 2012,https://link.springer.com/article/10.1007/s11238-012-9305-8,"D-separation, forecasting, and economic science: a conjecture",August 2012,David A. Bessler,Zijun Wang,,Male,Unknown,Unknown,Male,,18
73.0,3.0,Theory and Decision,26 May 2012,https://link.springer.com/article/10.1007/s11238-012-9310-y,"The doctrinal paradox, the discursive dilemma, and logical aggregation theory",September 2012,Philippe Mongin,,,Male,Unknown,Unknown,Male,,28
73.0,3.0,Theory and Decision,04 January 2012,https://link.springer.com/article/10.1007/s11238-011-9289-9,Competing allocation principles: time for compromise?,September 2012,Lars Schwettmann,,,Male,Unknown,Unknown,Male,,12
73.0,3.0,Theory and Decision,01 April 2011,https://link.springer.com/article/10.1007/s11238-011-9243-x,Distributional orderings: an approach with seven flavors,September 2012,Yoram Amiel,Frank Cowell,Wulf Gaertner,Male,Male,Male,Male,,5
73.0,3.0,Theory and Decision,27 March 2012,https://link.springer.com/article/10.1007/s11238-012-9299-2,"Type composition, career concerns, and signaling efforts",September 2012,Chia-Hui Chen,,,Unknown,Unknown,Unknown,Unknown,,
73.0,3.0,Theory and Decision,21 June 2011,https://link.springer.com/article/10.1007/s11238-011-9257-4,Common knowledge and limit knowledge,September 2012,Christian W. Bach,Jérémie Cabessa,,Male,Male,Unknown,Male,,4
73.0,3.0,Theory and Decision,17 June 2011,https://link.springer.com/article/10.1007/s11238-011-9262-7,Minimum cost spanning tree games and spillover stability,September 2012,Ruud Hendrickx,Jacco Thijssen,Peter Borm,Male,Male,Male,Male,,
73.0,3.0,Theory and Decision,30 March 2011,https://link.springer.com/article/10.1007/s11238-011-9250-y,Attitudes toward uncertainty among the poor: an experiment in rural Ethiopia,September 2012,Alpaslan Akay,Peter Martinsson,Stefan T. Trautmann,Male,Male,Male,Male,,41
73.0,3.0,Theory and Decision,18 June 2011,https://link.springer.com/article/10.1007/s11238-011-9258-3,Conflicting evidence and decisions by agency professionals: an experimental test in the context of merger regulation,September 2012,Bruce Lyons,Gordon Douglas Menzies,Daniel John Zizzo,Male,Male,Male,Male,,5
73.0,4.0,Theory and Decision,18 June 2011,https://link.springer.com/article/10.1007/s11238-011-9264-5,History as a coordination device,October 2012,Rossella Argenziano,Itzhak Gilboa,,Female,Male,Unknown,Mix,,
73.0,4.0,Theory and Decision,26 October 2011,https://link.springer.com/article/10.1007/s11238-011-9284-1,Reasoning-based introspection,October 2012,Olivier Gossner,Elias Tsakas,,Male,Male,Unknown,Male,,2
73.0,4.0,Theory and Decision,13 October 2011,https://link.springer.com/article/10.1007/s11238-011-9279-y,On the interaction between heterogeneity and decay in two-way flow models,October 2012,Pascal Billand,Christophe Bravard,Sudipta Sarangi,Male,Male,Female,Mix,,
73.0,4.0,Theory and Decision,15 February 2012,https://link.springer.com/article/10.1007/s11238-012-9294-7,We-thinking and vacillation between frames: filling a gap in Bacharach’s theory,October 2012,Alessandra Smerilli,,,Female,Unknown,Unknown,Female,,22
73.0,4.0,Theory and Decision,20 January 2012,https://link.springer.com/article/10.1007/s11238-011-9290-3,Decentralization of contracts with interim side-contracting,October 2012,Bernd Theilen,,,Male,Unknown,Unknown,Male,,
73.0,4.0,Theory and Decision,01 April 2012,https://link.springer.com/article/10.1007/s11238-012-9302-y,Expanding state space and extension of beliefs,October 2012,Takashi Hayashi,,,Male,Unknown,Unknown,Male,,7
73.0,4.0,Theory and Decision,14 July 2012,https://link.springer.com/article/10.1007/s11238-012-9320-9,Combining expert probabilities using the product of odds,October 2012,Patrizio Frederic,Mario Di Bacco,Frank Lad,Male,Male,Male,Male,,1
73.0,4.0,Theory and Decision,22 July 2012,https://link.springer.com/article/10.1007/s11238-012-9324-5,Products of non-additive measures: a Fubini-like theorem,October 2012,Christian Bauer,,,Male,Unknown,Unknown,Male,,2
73.0,4.0,Theory and Decision,26 June 2012,https://link.springer.com/article/10.1007/s11238-012-9317-4,Intensity of preference and related uncertainty in non-compensatory aggregation rules,October 2012,Giuseppe Munda,,,Male,Unknown,Unknown,Male,,16
73.0,4.0,Theory and Decision,08 June 2011,https://link.springer.com/article/10.1007/s11238-011-9265-4,The Group Calibration Index: a group-based approach for assessing forecasters’ expertise when external outcome data are missing,October 2012,Ilan Fischer,Ravid Bogaire,,Male,Male,Unknown,Male,,1
73.0,4.0,Theory and Decision,26 May 2012,https://link.springer.com/article/10.1007/s11238-012-9314-7,Search and research: the influence of editorial boards on journals’ quality,October 2012,Damien Besancenot,Kim V. Huynh,Joao R. Faria,Male,,Unknown,Mix,,
74.0,1.0,Theory and Decision,04 December 2012,https://link.springer.com/article/10.1007/s11238-012-9344-1,An axiomatization of the kernel for TU games through reduced game monotonicity and reduced dominance,January 2013,Theo Driessen,Cheng-Cheng Hu,,Male,,Unknown,Mix,,
74.0,1.0,Theory and Decision,17 May 2012,https://link.springer.com/article/10.1007/s11238-012-9311-x,Cooperative provision of indivisible public goods,January 2013,Pierre Dehez,,,Male,Unknown,Unknown,Male,,4
74.0,1.0,Theory and Decision,01 August 2012,https://link.springer.com/article/10.1007/s11238-012-9326-3,Non-uniqueness of equilibrium action profiles with equal size in one-shot cheap-talk games,January 2013,Irene Valsecchi,,,Female,Unknown,Unknown,Female,,
74.0,1.0,Theory and Decision,13 December 2012,https://link.springer.com/article/10.1007/s11238-012-9341-4,"Quantity competition, endogenous motives and behavioral heterogeneity",January 2013,Alessandra Chirco,Caterina Colombo,Marcella Scrimitore,Female,Female,Female,Female,,13
74.0,1.0,Theory and Decision,12 December 2012,https://link.springer.com/article/10.1007/s11238-012-9340-5,Making Sen’s capability approach operational: a random scale framework,January 2013,John K. Dagsvik,,,Male,Unknown,Unknown,Male,,8
74.0,1.0,Theory and Decision,27 October 2012,https://link.springer.com/article/10.1007/s11238-012-9338-z,Practical beliefs vs. scientific beliefs: two kinds of maximization,January 2013,Elias L. Khalil,,,Male,Unknown,Unknown,Male,,7
74.0,1.0,Theory and Decision,31 October 2012,https://link.springer.com/article/10.1007/s11238-012-9336-1,Changes in multiplicative background risk and risk-taking behavior,January 2013,Octave Jokung,,,Male,Unknown,Unknown,Male,,5
74.0,1.0,Theory and Decision,15 September 2012,https://link.springer.com/article/10.1007/s11238-012-9327-2,Dynamic contractual incentives in the face of a Samaritans’s dilemma,January 2013,Josepa Miquel-Florensa,,,Unknown,Unknown,Unknown,Unknown,,
74.0,2.0,Theory and Decision,17 June 2011,https://link.springer.com/article/10.1007/s11238-011-9263-6,The predictive role of counterfactuals,February 2013,Alfredo Di Tillio,Itzhak Gilboa,Larry Samuelson,Male,Male,Male,Male,,3
74.0,2.0,Theory and Decision,07 December 2012,https://link.springer.com/article/10.1007/s11238-012-9343-2,Ambiguity in asset pricing and portfolio choice: a review of the literature,February 2013,Massimo Guidolin,Francesca Rinaldi,,Male,Female,Unknown,Mix,,
74.0,2.0,Theory and Decision,03 October 2012,https://link.springer.com/article/10.1007/s11238-012-9331-6,The irreversibility effect and agency conflicts,February 2013,Clemens Löffler,Thomas Pfeiffer,Georg Schneider,Male,Male,Male,Male,,2
74.0,2.0,Theory and Decision,10 August 2012,https://link.springer.com/article/10.1007/s11238-012-9325-4,When learning meets salience,February 2013,David Bodoff,,,Male,Unknown,Unknown,Male,,
74.0,2.0,Theory and Decision,19 May 2012,https://link.springer.com/article/10.1007/s11238-012-9307-6,Moment characterization of higher-order risk preferences,February 2013,Sebastian Ebert,,,Male,Unknown,Unknown,Male,,30
74.0,2.0,Theory and Decision,13 April 2012,https://link.springer.com/article/10.1007/s11238-012-9304-9,Intertemporal utility smoothing under uncertainty,February 2013,Katsutoshi Wakai,,,Male,Unknown,Unknown,Male,,
74.0,3.0,Theory and Decision,11 December 2011,https://link.springer.com/article/10.1007/s11238-011-9287-y,"Trust, inequality and the market",March 2013,Shaun P. Hargreaves Heap,Jonathan H. W. Tan,Daniel John Zizzo,Male,Male,Male,Male,,20
74.0,3.0,Theory and Decision,20 April 2012,https://link.springer.com/article/10.1007/s11238-012-9301-z,Sincerity and manipulation under approval voting,March 2013,Ulle Endriss,,,Unknown,Unknown,Unknown,Unknown,,
74.0,3.0,Theory and Decision,24 May 2012,https://link.springer.com/article/10.1007/s11238-012-9312-9,Parameters of social preference functions: measurement and external validity,March 2013,Christoph Graf,Rudolf Vetschera,Yingchao Zhang,Male,Male,Unknown,Male,,5
74.0,3.0,Theory and Decision,26 June 2012,https://link.springer.com/article/10.1007/s11238-012-9315-6,Norms and rationality. Is moral behavior a form of rational action?,March 2013,Karl-Dieter Opp,,,Unknown,Unknown,Unknown,Unknown,,
74.0,3.0,Theory and Decision,16 October 2011,https://link.springer.com/article/10.1007/s11238-011-9282-3,Do financial professionals behave according to prospect theory? An experimental study,March 2013,Mohammed Abdellaoui,Han Bleichrodt,Hilda Kammoun,Male,,Female,Mix,,
74.0,3.0,Theory and Decision,16 March 2012,https://link.springer.com/article/10.1007/s11238-012-9295-6,On the origin of the WTA–WTP divergence in public good valuation,March 2013,Emmanuel Flachaire,Guillaume Hollard,Jason F. Shogren,Male,Male,Male,Male,,8
74.0,3.0,Theory and Decision,28 March 2012,https://link.springer.com/article/10.1007/s11238-012-9297-4,A note on “Re-examining the law of iterated expectations for Choquet decision makers”,March 2013,André Lapied,Pascal Toquebeuf,,Male,Male,Unknown,Male,,4
74.0,3.0,Theory and Decision,30 March 2012,https://link.springer.com/article/10.1007/s11238-012-9298-3,Error and inference: an outsider stand on a frequentist philosophy,March 2013,Christian P. Robert,,,Male,Unknown,Unknown,Male,,
74.0,4.0,Theory and Decision,07 April 2012,https://link.springer.com/article/10.1007/s11238-012-9300-0,Invoking a Cartesian product structure on social states,April 2013,Herrade Igersheim,,,Unknown,Unknown,Unknown,Unknown,,
74.0,4.0,Theory and Decision,02 October 2012,https://link.springer.com/article/10.1007/s11238-012-9329-0,Communication compatible voting rules,April 2013,Mark Thordal-Le Quement,,,Male,Unknown,Unknown,Male,,7
74.0,4.0,Theory and Decision,12 January 2012,https://link.springer.com/article/10.1007/s11238-011-9291-2,Preferences over consumption and status,April 2013,Alexander Vostroknutov,,,Male,Unknown,Unknown,Male,,3
74.0,4.0,Theory and Decision,18 February 2012,https://link.springer.com/article/10.1007/s11238-011-9292-1,Is more health always better for society? Exploring public preferences that violate monotonicity,April 2013,Ignacio Abásolo,Aki Tsuchiya,,Male,Female,Unknown,Mix,,
74.0,4.0,Theory and Decision,28 June 2012,https://link.springer.com/article/10.1007/s11238-012-9319-2,Preferences and the price of stability in matching markets,April 2013,James W. Boudreau,Vicki Knoblauch,,Male,Female,Unknown,Mix,,
75.0,1.0,Theory and Decision,29 September 2012,https://link.springer.com/article/10.1007/s11238-012-9334-3,Information and ambiguity: herd and contrarian behaviour in financial markets,July 2013,J. L. Ford,D. Kelsey,W. Pang,Unknown,Unknown,Unknown,Unknown,,
75.0,1.0,Theory and Decision,27 December 2012,https://link.springer.com/article/10.1007/s11238-012-9348-x,Emotional balance and probability weighting,July 2013,Narat Charupat,Richard Deaves,Peter Miu,Unknown,Male,Male,Male,"We report the results of an experiment designed to explore whether a relationship exists between proxies for emotional balance and prospect theory’s probability weighting function parameters. There is abundant evidence that emotion and mood affect financial decision-making. Emotion markers are present when markets heat up (Lo and Repin 2002). People are more likely to insure against emotionally vivid events (Johnson et al. 1993). The disposition effect, the tendency to hold on to losing investments too long, seems best explained by emotion (Summers and Duxbury 2007). Positive affect influences investment (MacGregor et al. 2000). Visceral factors seem to play an important role in intertemporal choice and lack of self-control (Loewenstein 1996, 2000). National stock markets may even be impacted via such triggers as sunlight, day length and changes thereto (Hirshleifer and Shumway 2003; Kamstra et al. 2000, 2003), sporting success (Edmans et al. 2007), and aviation disasters (Kaplanski and Levy 2010). While there is evidence suggesting that emotions can play a positive role in decision-making (Bechara et al. 1997), evidence to the contrary (Lo et al. 2005; Shiv et al. 2005) and the simplifying invocation of rationality by conventional finance have led to the presumption that financial decisions are not only best made cognitively and without mediation by possibly bias-inducing emotional forces but also usually are. While decision-making has been traditionally viewed as a purely cognitive activity, and to the extent that emotions are considered, it is only through anticipated emotions (such as the regret that may be felt if a poor investment is made), in the “risk-as-feelings” view of Loewenstein et al. (2001), immediate visceral reactions concurrent with the decision itself may occur, and these may influence the cognitive process, leading to a feedback loop between cognition and emotion. If one subscribes to such a view, it is natural to suspect that emotion plays a role in decision-making under risk. Specifically, given that rationality and expected utility theory (EUT) go hand in hand, it is logical to consider the possibility that emotion is a key driver behind non-EUT-type behavior.Footnote 1 We focus our attention here on prospect theory (PT), as developed by Kahneman and Tversky (1979) and Tversky and Kahneman (1992). Loss aversion and probability weighting are the two most obvious markers of non-EUT-type behavior, and there is evidence that they are impacted by emotional forces. Dhar and Wertenbroch (2000) document this for loss aversion. Also, Rottenstreich and Hsee (2001) build a case for probability weighting. We focus on probability weighting here, not only because it is quite clear what the probability weighting function should look like under EUT but also because there is no unambiguous measure of loss aversion.Footnote 2 Rottenstreich and Hsee propose that the shape of PT’s probability weighting function can be deconstructed such that the steep slopes at each end of the curve reflect emotional reactions from movements from near-certainty to certainty and impossibility to near-impossibility. In their experiments, they show that if an outcome is affect-rich (or emotion-laden) greater curvature for the weighting function (implying greater steepness at the endpoints and less sensitivity in the mid-range) is required to explain behavior. While the latter study suggests that probability weighting can be affected by emotional factors which are situation-dependent, probability weighting may also be influenced by emotional factors which are individual-dependent. There is abundant evidence that various emotional predispositions and financial tendencies have a significant heritable component (Larsen and Buss 2008; Barnea et al. 2010; Cesarini et al. 2010), suggesting a mechanism for heterogeneity. Bruhin et al. (2010) find that while most exhibit PT-type behavior, a substantial minority seems to follow EUT. Given the role played by probability weighting in financial decision-making and differences in emotional characteristics among individuals, heterogeneous financial behavior may in part be the result of cross-sectional differences in probability weighting function parameters arising from individual-specific emotional characteristics. We conjecture that an important individual difference in this context is the affective influence regulation, which we call emotional balance (EB). Individual differences in affective information processing exist (Barrett 1998; Gohm and Clore 2000). In particular, variation among individuals exists in terms of feeling intensity and the degree to which people allow their feelings to be integrated with their judgments and decisions. While emotion properly harnessed can facilitate decision-making, given free rein it can lead to poor choices. Logically, those who are better able to harness their emotions should make better decisions. Traders able to do so perform better (Lo et al. 2005). In the context of decision-making under risk, those with better emotional self-awareness and control over their feelings (i.e., those with greater EB) might be expected to act in a manner more closely resembling normative EUT-type preferences. In this study, we measure the degree of resemblance to EUT-type preferences by examining the curvature and elevation of the probability weighting function.",12
75.0,1.0,Theory and Decision,26 January 2012,https://link.springer.com/article/10.1007/s11238-012-9293-8,Pareto utility,July 2013,Masako Ikefuji,Roger J. A. Laeven,Chris Muris,Female,Male,,Mix,,
75.0,1.0,Theory and Decision,27 April 2012,https://link.springer.com/article/10.1007/s11238-012-9306-7,Varieties of failure of monotonicity and participation under five voting methods,July 2013,Dan S. Felsenthal,Nicolaus Tideman,,Male,Male,Unknown,Male,,27
75.0,1.0,Theory and Decision,19 May 2012,https://link.springer.com/article/10.1007/s11238-012-9308-5,Justification of functional form assumptions in structural models: a correction,July 2013,John K. Dagsvik,,,Male,Unknown,Unknown,Male,,4
75.0,1.0,Theory and Decision,26 May 2012,https://link.springer.com/article/10.1007/s11238-012-9313-8,Best-of-two contests with psychological effects,July 2013,Alex Krumer,,,Male,Unknown,Unknown,Male,,19
75.0,1.0,Theory and Decision,28 June 2012,https://link.springer.com/article/10.1007/s11238-012-9318-3,Why do groups cooperate more than individuals to reduce risks?,July 2013,Min Gong,Jonathan Baron,Howard Kunreuther,,Male,Male,Mix,,
75.0,1.0,Theory and Decision,03 October 2012,https://link.springer.com/article/10.1007/s11238-012-9330-7,Strategic collusion in auctions with externalities,July 2013,Omer Biran,,,Male,Unknown,Unknown,Male,,
75.0,1.0,Theory and Decision,06 December 2012,https://link.springer.com/article/10.1007/s11238-012-9342-3,Are moral norms distinct from social norms? A critical assessment of Jon Elster and Cristina Bicchieri,July 2013,Benoît Dubreuil,Jean-François Grégoire,,Male,Unknown,Unknown,Male,,18
75.0,2.0,Theory and Decision,13 July 2012,https://link.springer.com/article/10.1007/s11238-012-9321-8,The impact of governmental assistance on insurance demand under ambiguity: a theoretical model and an experimental test,August 2013,Marielle Brunette,Laure Cabantous,Anne Stenger,Female,Female,Female,Female,,32
75.0,2.0,Theory and Decision,15 July 2012,https://link.springer.com/article/10.1007/s11238-012-9322-7,Individual vs. couple behavior: an experimental investigation of risk preferences,August 2013,Mohammed Abdellaoui,Olivier l’Haridon,Corina Paraschiv,Male,Male,Female,Mix,,
75.0,2.0,Theory and Decision,13 July 2012,https://link.springer.com/article/10.1007/s11238-012-9323-6,Aspirations as reference points: an experimental investigation of risk behavior over time,August 2013,Arvid O. I. Hoffmann,Sam F. Henry,Nikos Kalogeras,Male,,Male,Mix,,
75.0,2.0,Theory and Decision,06 November 2012,https://link.springer.com/article/10.1007/s11238-012-9333-4,Experimental evidence on case-based decision theory,August 2013,Wolfgang Ossadnik,Dirk Wilmsmann,Benedikt Niemann,Male,Male,Male,Male,,13
75.0,2.0,Theory and Decision,24 November 2012,https://link.springer.com/article/10.1007/s11238-012-9339-y,Min- and Max-induced rankings: an experimental study,August 2013,Amélie Vrijdags,,,Female,Unknown,Unknown,Female,,1
75.0,2.0,Theory and Decision,03 November 2012,https://link.springer.com/article/10.1007/s11238-012-9337-0,Not so cheap talk: costly and discrete communication,August 2013,Johanna Hertel,John Smith,,Female,Male,Unknown,Mix,,
75.0,2.0,Theory and Decision,30 August 2012,https://link.springer.com/article/10.1007/s11238-012-9328-1,"A note on local spillovers, convexity, and the strategic substitutes property in networks",August 2013,Pascal Billand,Christophe Bravard,Sudipta Sarangi,Male,Male,Female,Mix,,
75.0,3.0,Theory and Decision,14 February 2013,https://link.springer.com/article/10.1007/s11238-013-9351-x,Who gains from information asymmetry?,September 2013,Gil S. Epstein,Yosef Mealem,,Male,Male,Unknown,Male,"Asymmetry in the abilities of players in a contest can be found in many situations. Moreover, players do not always know the abilities of their opponents. For example, in a contest over monopoly regulation, where both the consumers and the producer invest effort to influence the decisions of politicians or regulators, it is not always clear how efficient the players are in using their resources. In this particular situation, the ability of consumers to influence politicians is usually known to both groups, while the ability of the producer is usually unknown to the consumers. The asymmetry is a result of the fact that the consumers do not know how efficient the producer is in using its resources: do the politicians support or oppose granting monopoly power to the producer; does the producer have direct access to the politicians; do the politicians receive donations from the producer, etc. In this situation, the producer is the informed player since it knows both its own abilities and those of the consumers, while the consumers know only their own abilities. Another example involves an individual claiming compensation from an insurance company following a car accident and an intermediary, such as a court, which will decide whether the individual is to receive compensation. Both parties invest resources in obtaining evidence to prove their case, while only the claimant knows his real situation. If the individual has been seriously injured in the accident then it will be easier for him to prove his case since for every unit of resources invested (to prove his case), he will have a higher probability of winning. On the other hand, if he is only lightly injured, it will be harder for him to prove his case and each unit of resources invested will have a lower return. In this case, the information regarding the individual’s real state is private and known only to him.Footnote 1 In the rent-seeking literature, it has been established that asymmetry between the contestants reduces wasteful lobbying efforts. The asymmetry can be in terms of the lobbying capabilities, wealth endowments, attitudes toward risk, or rent valuations of the contestants (see, for example, Allard 1988; Baik 1994; Epstein and Nitzan 2002, 2007; Gradstein 1994; Nitzan 1994). The analysis of situations in which players value the prizes differently and their values are private information can be found in the economic literature. Nti (1999) allows players to have different values but assumes that they are known. Malueg and Yates (2004), on the other hand, determine the Bayesian Nash equilibrium in a rent-seeking contest in which the players’ valuations of the prize are private information and determine the conditions under which the equilibrium exists. Wärneryd (2003) considers a two-player contest for a prize of common, but uncertain value. For settings in which one player knows the value of the prize, while the other knows only its prior distribution, he provides conditions for a situation in which the uninformed agent is ex-ante strictly more likely to win the prize than the informed agent. In the special case of the Tullock contest, equilibrium expenditures are lower under asymmetric information than when either both agents are informed or neither agent is informed. Hurley and Shogren (1998a) consider a model in which players value the stakes differently and there is one-sided private information, i.e. one player does not know the other’s value. They investigate how changes in the nature of the one-sided information asymmetry affect investment levels in the contest and show that the results are a function of the level of information uncertainty. In a different paper, Hurley and Shogren (1998b) consider a model in which both players’ values are private information and analyze the equilibrium numerically. They show that if the values (stakes) of the contestants are identical, then rent dissipation is higher in the complete information contest than in the one-sided asymmetric information contest. This paper extends the literature to any size of stakes and compares the Bayesian Nash equilibrium outcome of the one-sided private information contest to the outcomes of a contest in which both players know the abilities of all the players. In our paper, we show conditions under which uncertainty increases both the investment of the uninformed player and the contest’s rent dissipation. Clark (1997) considers a similar type of question to the one we present and examines a form of the Tullock imperfectly discriminating rent-seeking game in which the contestants are uncertain about the value of a bias parameter in the probability of winning function. Beliefs about this unknown parameter are not constrained to be static. He considers two methods by which the players’ prior beliefs on this parameter can be updated. First, he allows for the information to emerge by allowing the game to be played twice where the outcome of the first game is known before the second begins. The identity of the winner in the first contest represents information that emerges endogenously and which can be used to revise beliefs on the unknown bias parameter. Second, information can be produced outside the model by an external agency, which gives rise to exogenous learning. In this paper, we consider a different one-sided private information problem. In the Bayesian Nash equilibrium, one player possesses information regarding his own ability and that of his opponent, while the other player only knows his own ability. We compare this contest to the case of fully informed players, where each player knows his own ability and that of his opponent. The comparison is important in understanding to what extent the asymmetry affects the players. Would it be optimal for a central planner to invest resources to reduce the asymmetry between the players to minimize the waste of resources or to increase social welfare? In order to answer such questions, it is important to compare the outcome under full information with that under information asymmetry. We start by presenting a case in which the informed player’s ability can be one of the two types and derive conditions under which the uninformed player invests more (or less) effort in the one-sided private information contest than in a complete information contest (Result 1). We also derive conditions under which the expected rent dissipation in a contest with one-sided private information is larger (or smaller) than in a fully informed contest (Result 2). While it would appear at first glance that uncertainty may be an advantage to the informed player, we show general conditions under which incomplete information may in fact be harmful to him (Result 3). Surprisingly, we show that this condition is independent of the probability assigned by the uninformed player to the type of the informed player (or the proportion of each type in the population). Another question that we consider is whether ex-ante the informed player—before knowing his own type—prefers that the uninformed player knows his type and as such will play the game under full information (Result 4). We then turn to considering a two-stage game in which the informed player, in the first stage of the contest, declares his type (or does not declare) and in the second stage the players play according to the information available to them. We show that if the informed player can only tell the truth, then the game will turn into a full information contest; while if the informed player can lie, then a one-sided private information contest has meaning in this context (Result 5). We generalize Result 3 to allow for the informed player’s ability to be any one of \(N\) types (Result 6). The generalization enables us to show that in the case where one contestant has the same ability as the uninformed contestant, he will prefer a one-sided private information contest (Result 6).",14
75.0,3.0,Theory and Decision,24 December 2012,https://link.springer.com/article/10.1007/s11238-012-9347-y,Stability and efficiency in perfect foresight situation,September 2013,Yoshio Kamijo,,,Male,Unknown,Unknown,Male,"We consider a situation with perfect foresight that was introduced by Xue (1998) as a solution to fully take into account players’ foresight. We present efficiency results in a large domain, including any game represented in a normal form. Players’ foresight has been explored and incorporated into the models or solutions in several contexts. In non-cooperative game theory, a player who follows the subgame perfect equilibrium strategy anticipates the subsequent actions of other players. In cooperative game theory, the bargaining set, a solution concept in cooperative game theory, presumes that players who propose a payoff distribution expect a counter-proposal by other players. On the other hand, some traditional solution concepts or equilibrium concepts are also examined from the perspective of players’ foresight. For example, the gamma core (Chander and Tulkens 1995, 1997) is proposed as an alternative to the traditional alpha and beta cores, and the coalition-proof Nash equilibrium (Bernheim et al. 1987) is introduced as the refinement of the Nash equilibrium from the point of view of players’ brightness. Harsanyi (1974) criticizes the original definition of a stable set of von Neumann and Morgenstern (1953) because of players’ myopia, and proposes an alternative definition of the stable set. Inspired by Harsanyi (1974); Chwe (1994) formalizes players’ foresight. His solution can be applied to situations where coalitions can freely form, but cannot make binding agreements and where their actions are public. Since Harsanyi and Chwe’s model (HC model) can be applied to both cooperative and non-cooperative games, it is applied to many contexts (for the formation of a price-leadership cartel, Diamantoudi 2005; Kamijo and Nakanishi 2007; Kamijo and Muto 2010, and Nakanishi and Kamijo 2008; for hedonic coalition formation games, Diamantoudi and Xue 2003; for network formation games, Page et al. 2005; for prisoner’s dilemma games, Suzuki and Muto 2005 and Nakanishi 2009). Although Harsanyi and Chwe incorporate players’ foresight into their solution concepts, Xue (1998) points out that their notions capture only players’ partial foresight. Xue proposes an alternative model that describes players’ perfect foresight. A perfect foresight captured by Xue (1998) has the following characteristics (see also Xue 2000). First, a player with perfect foresight only considers the final outcomes that might result after his current decision. Thus, he compares his payoff in the current outcome with the one in the final outcome, not with the one in the outcome immediately after his action. This first point was also captured by Harsanyi (1974) and Chwe (1994). Second, a player with perfect foresight also considers the path from the current outcome to the final outcomes, i.e., he considers how the final outcomes can be reached. Thus, in the perfect foresight by Xue, we must consider deviations from the path to the final outcomes that were not considered in HC model. To demonstrate the second point, consider an example depicted in Fig. 1. Here, there are two players 1 and 2 and three outcomes \(a, b,\) and \(c\). Players 1 and 2 can jointly replace \(a\) by \(b\) and player 2 can, by himself, replace \(a\) by \(c\). The vector attached to each outcome denotes a payoff vector derived from the outcome if it prevails. Then, HC model prescribes that the unique set of stable outcomes is \(\{ b, c \}\) because outcome \(a\) is replaced by \(b\,(c)\) by unanimous agreement of players 1 and 2 (player 2) and they (he) will be strictly better off by doing so, and once either \(b\) or \(c\) is reached, there is no additional move. Thus, the predicted outcome from initial outcome \(a\) is either \(b\) or \(c\). However, is it reasonable that outcome \(b\) is realized from an initial outcome \(a\) by rational and farsighted individuals? Consider a path from the initial outcome \(a\) to the final \(b\) and suppose that the two players agree to this path. Do they, in particular player \(2\), have the actual incentive to follow this path? If we only consider the comparison between the initial outcome \(a\) and the final outcome \(b\), the incentive of the players is satisfied. However, player 2, if he is farsighted enough, should find that he can enjoy more profit by deviating from this agreement and instead replacing \(a\) by \(c\). In other words, even if both players agree to change \(a\) by \(b\), player 2 can deviate from this path from \(a\) to \(b\) because there is no binding contract. This point is not captured when we only consider the deviation from outcomes, and thus it is claimed that players’ farsightedness is limited in both Harsanyi (1974) and Chwe (1994). Two players game Although many works apply the HC model, there do not exist studies that apply the perfect foresight of Xue, except for Shino (2002) who applied it to the prisoner’s dilemma game. In this paper, we apply Xue’s model to a large class of games. We pose applicable assumptions on what individuals can do. Inducement relations, which specify what individuals can do or what outcomes individuals can induce from a status quo, are assumed to be invertible and coalition-free. The invertible inducement relations are such that if a set of individuals can change an initial outcome to another outcome, then the same set of individuals is also able to change the latter outcome to the former. The coalition-free inducement relations are such that if there is a sequence of moves that begins with outcome \(a\) and ends with outcome \(b\), there is another sequence from \(a\) to \(b\) such that each move is realized by only one individual. Examples satisfying these assumptions include normal form games where each player has to determine his own strategy, coalition formation games with free entry and exit, directed or non-directed network formation games with some link formation rule, and so on. Following the spirit of the theory of social situations (Greenberg 1990), two different stability notions are considered. First, when we consider the stability notion that incorporates players’ conservatism, every outcome can be stable. On the other hand, if we adopt the stability notion that incorporates players’ optimism, only the efficient outcomes can be stable. This means that we give the sufficient conditions on the existence of nonempty-valued optimistic stable standard of behavior for perfect foresight situation. Thus, the stability is compatible with the efficiency of the outcomes. We also discuss the limitations and extensions of our model and results. We first attempt to weaken the restrictions on the preference relations and the inducement relations. We propose quasi-strict preference relations, which are weaker than strict preference relations, and show that all of our results hold under this milder assumption on preferences. Then, we introduce many examples included in our model. As explained before, any normal form game is included, and thus any coalition formation game including network formation represented by a normal form game is also in the scope of our model. Another application of our results is pre-play negotiation before playing normal form games. Then, we provide a reasonable refinement of Nash equilibrium outcomes; all the Nash equilibria that are Pareto-efficient among the set of Nash equilibrium outcomes are predicted as agreements of the pre-play negotiation by farsighted players with optimism. We also apply our results to \(n\)-persons’ simple bargaining for the fixed amount of a pie. Finally, we examine an alternative assumption on the inducement relations, instead of invertible-ness and coalition-free-ness. The inducement relations satisfy the unanimity property if any status quo can be replaced by any other alternative by a unanimous agreement of all society members. It is shown that this natural assumption is sufficient for the existence of nonempty-valued optimistic/conservative stable standard of behavior for the perfect foresight situation. The rest of the paper proceeds as follows. In the next section, we present the basic model. A perfect foresight situation of Xue (1998) is explained in Sect. 3. In Sect. 4, we give our main results. In Sects. 5 and 6, we discuss the extensions and limitations of our results and show the applicability of our model to many economic examples. Section 7 concludes the paper.",
75.0,3.0,Theory and Decision,02 December 2012,https://link.springer.com/article/10.1007/s11238-012-9345-0,Optimal allocation mechanisms with type-dependent negative externalities,September 2013,Isabelle Brocas,,,Female,Unknown,Unknown,Female,,11
75.0,3.0,Theory and Decision,28 December 2012,https://link.springer.com/article/10.1007/s11238-012-9350-3,Endowment Effect in negotiations: group versus individual decision-making,September 2013,Amira Galin,,,Female,Unknown,Unknown,Female,"Imagine a scenario in which graduate students studying for their Master’s degree negotiate with university authorities in regard to their curriculum. At the beginning of the negotiation a status quo exists, as their curriculum includes a fixed number of elective courses and two seminars, which the students must complete in order to qualify for their Master’s degree. A seminar is considered much more demanding than an elective course, especially in terms of the student’s remaining leisure time. On the one hand, if the university authorities wish to negotiate a change in the status quo by adding an additional seminar (a third one) to the curriculum, in exchange for reducing the number of elective courses, the students, in effect, become “sellers” of their own leisure time. On the other hand, if the university authorities wish to negotiate a change by dropping one seminar in exchange for additional elective courses, the students then become “buyers,” in terms of buying more leisure time. This scenario raises several interesting questions: How many elective courses will each student demand to be dropped from his/her curriculum when a seminar is added? Will he/she accept adding the same number of elective courses (previously demanded to be dropped) when a seminar is dropped from the curriculum? Now imagine that a representative group of students conduct the negotiation instead of each individual student. Would the group of students conduct negotiations differently than an individual student? Would a group be more or less demanding than individuals in negotiating their leisure time (i.e., number of elective courses in exchange for a seminar)? Rational theories assume that in negotiation, under complete certainty, the “buying” and “selling” prices of the same object should be symmetrical, since ownership of an object should not affect its valuation. However, according to the Endowment Effect (EE), ownership of an object increases its value. Sell demands made by owners tend to be significantly higher than expected by buyers. The EE, by inducing high demands, can be an obstacle when negotiators attempt to reach a settlement. This is not necessarily so, however, when the negotiator is a group. On the one hand, groups’ overpricing induced by the EE might result in severe social welfare losses (Glöckner et al. 2009). On the other hand, groups’ moderating the EE impact as compared to individuals could lead to a higher probability of reaching an agreement. The aim of this study is to examine and compare the EE in and between groups and individuals. In other words, this study examines which type of negotiator—groups or individuals—has a better probability to moderate demands in negotiation, thus making a settlement more feasible. In addition, in order to get some insight into the procedure that motivates groups’ negotiation decisions, the research also focuses on the underlying mechanisms that motivate groups deliberating negotiation decisions.",9
75.0,3.0,Theory and Decision,12 December 2012,https://link.springer.com/article/10.1007/s11238-012-9346-z,On the independence of history: experience spill-overs between experiments,September 2013,Astrid Matthey,Tobias Regner,,Female,Male,Unknown,Mix,,
75.0,3.0,Theory and Decision,22 May 2013,https://link.springer.com/article/10.1007/s11238-013-9378-z,Varying the number of bidders in the first-price sealed-bid auction: experimental evidence for the one-shot game,September 2013,Sascha Füllbrunn,Tibor Neugebauer,,,Male,Unknown,Mix,,
75.0,3.0,Theory and Decision,03 March 2013,https://link.springer.com/article/10.1007/s11238-013-9359-2,On recursive solutions to simple allocation problems,September 2013,Özgür Kıbrıs,,,Male,Unknown,Unknown,Male,"
Revealed preference theory studies conditions under which by observing the choice behavior of an agent, one can discover the underlying preferences that govern it. Choice rules for which this is possible are called rational. Most of the earlier work on rationality analyzes consumers’ demand choices from budget sets (e.g., see Samuelson 1938, 1948). The underlying premise that choices reveal information about preferences, however, is applicable to a wide range of choice situations. For example, applications of the theory to bargaining games (Nash 1950) characterize bargaining rules which can be “rationalized” as maximizing the underlying preferences of an impartial arbitrator (Peters and Wakker 1991; Bossert 1994; Ok and Zhou 2000; Sánchez 2000). In this paper, we propose and axiomatically analyze a class of rational solutions to simple allocation problems. A simple allocation problem for a society \(N\) is an \(\left| N\right| +1\) dimensional nonnegative real vector \(\left( c_{1},\ldots ,c_{\left| N\right| },E\right) \) satisfying \(\sum \nolimits _{N}c_{i}\geqq E\), where \(E,\) the endowment has to be allocated among agents in \(N\) who are characterized by \(c\), the characteristic vector. Simple allocation problems have a wide range of applications some of which are discussed at the end of this section. We interpret an allocation rule on simple allocation problems as data on the choices of a policy-maker. As is standard in revealed preference theory, we say that a policy-maker’s choices are (i) rational, (ii) transitive-rational, and (iii) representable if they coincide, respectively, with the maximization of a (i) binary relation, (ii) transitive binary relation, and (iii) numerical function on the allocation space. For simple allocation problems, these concepts are discussed in detail by Kıbrıs (2012). Also see Thomson (2012). We propose a class of recursive rules. This is a large class of rules which mimic a recursive decision process where the policy-maker initially starts with a reference allocation of \(E\) in mind and then uses the data of the problem and his previous allocation decisions to recursively adjust his allocation choice. More specifically, the policy-maker first considers an allocation of \(E\) without paying attention to the data in \(c.\) This, for example, might simply be equal division of \(E\) among all agents. The policy-maker then checks if there are agents that are assigned shares exceeding their characteristic values.Footnote 1 If no such agent exists, the policy-maker is content with the initial proposal and concludes the decision algorithm. Otherwise, he decreases the shares of such agents to their characteristic values and reallocates the resulting surplus among the remaining agents, again without necessarily taking their characteristic values into account. For example, he might decide to allot all the surplus to one of the remaining agents. The new proposal is then checked against the characteristic vector and if necessary, updated again. This recursive updating continues until there remains no agent who is assigned a share that exceeds his characteristic value. Our main result, Theorem 1, is a characterization of recursive rules. It can be divided into the following two statements. First, recursive rules all satisfy three axioms: rationality, other-c monotonicity, and c-continuity. As noted above, rationality means that a recursive rule’s choices are consistent with the maximization of a binary relation. Given that the definition of a recursive rule makes no reference to such a binary relation, this is a surprising observation. The second axiom, other-c monotonicity, is a standard property which is satisfied by most allocation rules in the literature (Barberà et al. 1997; Kıbrıs 2012; Thomson 2003, 2007). It means that in a recursive rule, a change in an agent \(i\)’s characteristic value \(c_{i}\) affects the rest of the society in the same way, that is, no two other agents’ shares are affected in opposite directions. The third axiom, c-continuity, means that a recursive rule is continuous with respect to changes in the characteristic vector. The second, and more surprising statement of Theorem 1 is that recursive rules are the only rules to satisfy rationality, other-c monotonicity, and c-continuity. This paper is related to Kıbrıs (2012) which, for simple allocation problems, analyzes the logical relationships among the three central notions of revealed preference theory (rationality, transitive-rationality, representability) and other well-known axioms in the literature. A combination of Kıbrıs (2012) and Theorem 1 leads to two interesting observations. First, every recursive rule is  transitive-rational. That is, recursive rules never exhibit cyclic choice behavior and thus, can be rationalized by a transitive preference relation. Second, every recursive rule which is continuous with respect to \(E\) is additionally representable by a numerical function. For more on this discussion, please see Sect. 3. The analysis of recursive rules also contributes to two important questions in microeconomic theory. First, it is widely accepted that the standard model of rational choice makes very demanding assumptions about the decision-maker’s knowledge of its environment as well as his skills in computation. In the last century, these shortcomings lead to an increasing interest in decision theory as well as behavioral economics. Simon (1955) argued that a central quest in this literature should be “to replace the global rationality of economic man with a kind of rational behavior that is compatible with the access to information and the computational capacities that are actually possessed by organisms, including man, in the kinds of environments in which such organisms exist”. His well-known proposal, the “satisficing procedure” was meant as an attempt to provide such a choice procedure.Footnote 2
 Second, as recently noted by Rubinstein (2011, p. 33), to support the view that the scope of microeconomic models is wider than simply models in which agents carry out explicit optimization, we need examples of intuitive choice procedures that do not explicitly employ maximization of a preference relation but yet satisfy axioms of rationality. As an example of such a choice procedure, Rubinstein also discusses the “satisficing procedure” of Simon (1955). Much like the satisficing procedure, the recursive rules we propose contribute to both of the above objectives. That is, (i) they are not demanding in terms of the decision-maker’s access to information and computational abilities, (ii) they do not explicitly employ maximization of a preference relation, but (iii) they are consistent with the preference maximization model (i.e., their choices can equivalently be expressed as if the decision-maker is maximizing a preference relation). It is also interesting to note that the recursive algorithm shares some common features with the satisficing procedure. In both, the decision-maker does not consider all aspects of the choice problem at once (leading to low informational and computational requirements of both rules). Instead, he considers alternative choices one-by-one, checking if they satisfy certain criteria. In the satisficing procedure, the criterion is exceeding a cutoff utility level whereas in recursive rules, it is being consistent with the problem’s characteristic vector. If an alternative is not satisfactory, the decision-maker considers a revised alternative (produced by the recursive adjustment function in recursive rules and the sequencing of alternatives in the satisficing procedure). Our second result, Theorem 2, is a characterization of the Equal Gains (EG) rule. This rule is an important member of the family of recursive rules. It is called the uniform rule in the single-peaked allocation literature, the constrained equal awards rule in the bankruptcy literature, and the leveling tax in the taxation literature. The Equal Gains rule allocates the endowment in each problem equally, subject to no agent receiving more than his characteristic value. Being a recursive rule, the Equal Gains rule satisfies rationality, other-c monotonicity, and c-continuity. Theorem 2 shows that it additionally satisfies a well-known anonymity axiom called equal treatment of equals. This means that the Equal Gains rule always assigns identical shares to two agents with identical characteristics. Theorem 2 also shows that the Equal Gains rule is the only rule to satisfy rationality, c-continuity, and equal treatment of equals. Note that this statement does not include other-c monotonicity. The addition of equal treatment of equals makes it redundant. We conclude this section with some applications of simple allocation problems: (i) Taxation: a public authority is to collect an amount \(E\) of tax from a society \(N.\) Each agent \(i\) has income
\(c_{i}\) (e.g., Edgeworth 1898; Young 1987). (ii) Bankruptcy: a bankruptcy judge is to allocate the remaining assets
\(E\) of a bankrupt firm among its creditors, \(N.\) Each agent \(i\) has credited
\(c_{i}\) to the bankrupt firmFootnote 3 (e.g., O’Neill 1982; Aumann and Maschler 1985; Thomson 2003, 2007). (iii) Permit allocation: the Environmental Protection Agency is to allocate an amount \(E\) of pollution permits among firms in \(N\). Each firm \(i\), depending on its location, is imposed by the local authority an emission constraint
\(c_{i}\) on its pollution level (e.g., Kıbrıs 2003). (iv) Single-peaked or saturated preferences: a social planner is to allocate \(E\) units of a perfectly divisible commodity among members of \(N.\) Each agent \(i\) is known to have preferences with peak (saturation point)
\(c_{i}\)
Footnote 4 (e.g., Sprumont 1991). (v) Demand rationing: a supplier is to allocate its production
\(E\) among demanders in \(N\). Each \(i\in N\)
demands
\(c_{i}\) units (e.g., Cachon and Lariviere 1999). (vi) Bargaining with quasilinear preferences and claims: an arbitrator is to allocate \(E\) units of a numeriare good among agents who have quasilinear preferences with respect to it. Each agent holds a claim
\(c_{i}\) on what he should receive (e.g., Chun and Thomson 1992; Moulin 1985). (vii) Consumer choice under fixed prices and rationing: a consumer has to allocate his income
\(E\) among a set \(N\) of commodities. The prices are fixed and the consumer faces a “rationing constraint” \(c_{i}\) on his consumption of each commodity \(i\) (e.g., Bénassy 1993; Kıbrıs and Küçükşenel 2008).",8
75.0,4.0,Theory and Decision,05 October 2012,https://link.springer.com/article/10.1007/s11238-012-9332-5,Measuring risk aversion with lists: a new bias,October 2013,Antoni Bosch-Domènech,Joaquim Silvestre,,Male,Male,Unknown,Male,,12
75.0,4.0,Theory and Decision,19 October 2012,https://link.springer.com/article/10.1007/s11238-012-9335-2,Reining in excessive risk-taking by executives: the effect of accountability,October 2013,Mathieu Lefebvre,Ferdinand M. Vieider,,Male,Male,Unknown,Male,,16
75.0,4.0,Theory and Decision,15 February 2013,https://link.springer.com/article/10.1007/s11238-013-9355-6,A parsimonious model of subjective life expectancy,October 2013,A. Ludwig,A. Zimper,,Unknown,Unknown,Unknown,Unknown,,
75.0,4.0,Theory and Decision,14 May 2013,https://link.springer.com/article/10.1007/s11238-013-9376-1,Maximin play in completely mixed strategic games,October 2013,Vitaly Pruzhansky,,,Male,Unknown,Unknown,Male,"Recall how a typical justification of Nash equilibria runs: if somebody recommends equilibrium strategies to the players, they do not have incentives not to obey this recommendation. When it comes to completely mixed equilibrium strategies, this justification is somewhat weakened in the following two aspects. Even though no player has incentives to deviate, they still have no particular incentives to randomize exactly with the prescribed probabilities because, given the equilibrium strategy of the opponent, each player is indifferent as to what strategy to choose himself. In equilibrium a player’s expected payoff depends on the strategy of the opponent, as his own equilibrium strategy may not guarantee this payoff. In particular, if the player’s opponent deviates from his own equilibrium strategy, the player’s expected payoff may be lower than in equilibrium. As an illustration consider the 2-person strategic game \(\Gamma _{1}\) below. In \(\Gamma _{1}\) player 1 chooses rows, while player 2 chooses columns. Each player has two pure strategies (L)eft and (R)ight. The unique Nash equilibrium of the game is given by a pair of probability vectors: \(p^{*}\) for player 1 and \(q^{*}\) for player 2. It is easy to verify that \(p^{*}=\left( \frac{2}{3},\frac{1}{3}\right) \) and \(q^{*}=\left( \frac{1}{2},\frac{1}{2}\right) \), where the first number in each bracket equals the probability with which each player selects his first pure strategy. This Nash equilibrium yields the expected payoffs \( v_{1}^{*}=\frac{1}{2}\) and \(v_{2}^{*}=\frac{2}{3}\). To illustrate point (i) above, observe that given the equilibrium strategy of player \(2 - q^{*}\)—player 1 obtains the expected equilibrium payoff \(v_{1}^{*}\) regardless of his actual strategy: each pure strategy or any mixture of them yields him the expected payoff of \(\frac{1}{2}\). To see (ii) note that if player 2 deviates from \(q^{*}\) while player 1 still plays \( p^{*}\), player 1’s expected payoff can be anywhere between \(\frac{1}{3}\) and \(\frac{2}{3}\). In particular, it can be lower than \(v_{1}^{*}\). Similar results hold for player 2 as well.Footnote 1
 In spite of the above two weaknesses, completely mixed Nash equilibrium strategies have one important desirable property: they rule out the possibility of strategic outguessing by the opponent. For instance, if in \( \Gamma _{1}\) player 1 plays \(p^{*}\), then by definition player 2 does not have incentives to outguess him because \(p^{*}\) makes player 2 indifferent. If player 1 would deviate from \(p^{*}\) to say \(p=\left( \frac{3}{4},\frac{1}{4}\right) \), then player 2 would select \(R\), as a result player 1’s expected payoff would only be \(v_{1}=0\cdot \frac{3}{4} +1\cdot \frac{1}{4}=\frac{1}{4}<\frac{1}{2}\). Note that this logic rests on the assumption that player 2 not only knows that player 1 intends to deviate from his own equilibrium strategy, but also how he will deviate. Clearly without knowing that player 1 puts probability higher than \(\frac{2}{3}\) on his first pure strategy \(L\), player 2 will not choose \(R\) as his best response. In reality players are usually not recommended a strategy, but themselves have to figure out how to play. Suppose that a player discovers a strategy, different from the Nash equilibrium one, but which would guarantee him the expected equilibrium payoff, regardless of the strategy of the opponent. For instance, in \(\Gamma _{1}\) above these strategies would be \( \overline{p}=\left( \frac{1}{2},\frac{1}{2}\right) \) and \(\overline{q} =\left( \frac{1}{3},\frac{2}{3}\right) \), for player 1 and 2, respectively. Would the players use these strategies instead? This question is important given that players have such alternative strategies in a number of completely mixed strategic games. These strategies turn out to be equivalent to maximin equalizer strategies, see Pruzhansky (2011). Notably, there is no agreement on these conceptual issues in the literature. For instance Aumann and Maschler (1972), who appear to be the first to document the issue that maximin strategies may guarantee exactly the same expected payoff as Nash equilibrium strategies, admit that “We do not know what to recommend ..., since maximin strategies are not in equilibrium.” Harsanyi seems to prefer maximin over any other solution arguing in (Harsanyi (1977), p. 116), “If player \(i\) cannot hope to obtain more than his maximin payoff \(\overline{v}_{i}\), he must use the strategy that fully assures at least that much.”Footnote 2
 Standard game theory textbooks, e.g., Osborne and Rubinstein (1994) or Fudenberg and Tirole (1991) typically advocate the use of mixed equilibrium strategies based on what can be termed “large population” arguments. They include evolutionary dynamics, repeated interactions or purification arguments in the spirit of Harsanyi (1973). The common theme behind these arguments is that games are played not by individual players or decision makers but rather by (infinite) populations of players, and that the history of past plays is recorded and known at each round of play. The purpose of this paper is to argue that maximin strategies may be at least as attractive as Nash equilibrium strategies in games where the above large population arguments do not apply. Specifically, this will be the case in games where players cannot assess precisely the probabilities with which their opponents randomize, as well as in games that are played infrequently, where players each time face new opponents and the history of past play is not available. To sharpen the discussion, we restrict attention to 2-player strategic games possessing only mixed Nash equilibria. We also suppose that for each player mixed Nash equilibrium and maximin strategy do not coincide. Our findings have direct implications on how maximin strategies might be treated in applied research. For instance, currently we are not aware of any applied model where maximin strategies would be used, either as a refinement of Nash equilibrium strategies or as an alternative to Nash equilibrium strategies. Yet, there are interesting and relevant economic phenomena (e.g., tax collection and tax evasion) where an analysis of maximin strategies may be useful. We provide a simple example illustrating these ideas in the Appendix. The paper is structured as follows. Section 2 introduces basic definitions and concepts that will be used throughout the paper. Section 3 discusses the attractiveness of maximin versus Nash equilibrium strategies when players’ probability assessment are subjective. Section 4 proposes an analytical framework to model the choice of maximin versus Nash equilibrium strategies in a Bayesian setting. Section 5 concludes the paper by outlining several suggestions for future experimental research. We believe that this future work may shed more light on whether maximin behavior is more common than completely mixed Nash equilibria.",2
75.0,4.0,Theory and Decision,22 February 2013,https://link.springer.com/article/10.1007/s11238-013-9352-9,"Social choice, the strong Pareto principle, and conditional decisiveness",October 2013,Susumu Cato,,,Male,Unknown,Unknown,Male,"Arrow’s impossibility theorem states that there exists an individual who has dictatorial power if a social welfare function (or constitution) satisfies certain desirable conditions (Arrow 1951). The theorem can be regarded as a serious criticism of modern democracy, and it has an influence on welfare economics and other normative subjects. In the theorem, the weak Pareto principle is imposed as an underlying postulate of democracy. It requires that an alternative \(x\) is socially preferred to another alternative \(y\) whenever \(x\) is preferred to \(y\) by every individual in the society. There are other Paretian principles. One is the strong Pareto principle.Footnote 1 It requires that \(x\) is socially preferred to \(y\) whenever \(x\) is at least as good as \(y\) by every individual and \(x\) is preferred to \(y\) by some individual. The strong Pareto principle is satisfied by most democratic decision procedures, such as utilitarian criteria and majority rules.Footnote 2
 In this paper, we develop a tractable approach to analyze an aggregation rule satisfying the strong Pareto principle and binary independence. We propose the notion of conditional decisiveness: given a coalition \(A\) of individuals, a coalition \(B\) is \(A\)-conditionally decisive if and only if an alternative \(x\) is socially preferred to another alternative \(y\) whenever (i) \(x\) is indifferent to \(y\) by every individual in \(A\) and (ii) \(x\) is preferred to \(y\) by every individual in \(B\). We show that if social preferences are complete and transitive, then the family of \(A\)-conditionally decisive coalitions forms an ultrafilter for each non-empty subset \(A\) of the set of individuals. Given this result, we can derive the serial dictatorship when the set of individuals is a finite set. We also examine other rationality requirements on social preference and show the hierarchical structures of the family of conditionally decisive coalitions. Our approach has some advantages. First, the results hold regardless of the population structure. The set of individuals is allowed to be finite, countably infinite, or uncountably infinite.Footnote 3 Second, we can deal with not only transitivity but also with other social-rationality requirements: semi-transitivity, the interval-order property, quasi-transitivity, and acyclicity. For each rationality requirement, the power structure is clarified. Third, this approach smoothly connects the analysis of strongly Paretian rules with that of weakly Paretian rules by Kirman and Sondermann (1972), Brown (1974), Brown (1975), Hansson (1976), Banks (1995), Bossert and Suzumura (2009), Bossert and Suzumura (2010, 2011), and Cato (2012a); Cato (2012b); Cato (2013). The literature on Arrovian social choice theory is vast, but few works examine aggregation rules satisfying the strong Pareto principle.Footnote 4 Among others, Packel (1981) offers closely related results. Both his work and ours are concerned with the power structure of aggregation rules satisfying the strong Pareto principle. There are some significant differences. First, our main focus is to examine aggregation rules satisfying the strong Pareto principle and binary independence, while Packel’s is not. Second, we employ the notion of conditional decisiveness, while Packel employs the notion of strong decisiveness. Third, our analytical tools are traditional topological concepts, such as a filter and an ultrafilter, while his is what he calls a target. This paper is organized as follows. Section 2 presents the notation and definitions. Section 3 provides our main results. Section 4 contains discussions, and Sect. 5 concludes this paper. The proofs are relegated to the Appendix.",15
75.0,4.0,Theory and Decision,02 March 2013,https://link.springer.com/article/10.1007/s11238-013-9358-3,Characterizing referenda with quorums via strategy-proofness,October 2013,Marc Pauly,,,Male,Unknown,Unknown,Male,"Referenda are used in a number of countries on local, regional, and national levels. To give some examples from European countries, Italy knows referenda on a national level, whereas Germany only allows for referenda on the state level. In the Netherlands, in spite of the recent 2005 referendum on the European constitution, only referenda on the local city level are institutionalized. Besides this difference in political scope, referenda differ also on a number of other dimensions. Referenda can be initiated by either the citizens or by the elected representatives. They can be corrective in trying to correct a law that has been passed, or they can propose a new law. They can be binding or non-binding, in which case elected representatives are not bound to act or vote in accordance with the referendum. Finally, it matters how many options are available to choose from in a referendum. The most common case is the situation where a law is proposed (or opposed to) and eligible voters have the choice between voting yes, voting no, and abstaining. This is also the situation we will consider in this paper. A final difference concerns the quorum requirement that is imposed in many referenda. There are countries such as Estonia and Switzerland where a referendum is valid no matter how many people cast their vote in the referendum. However, in many cases a referendum is valid only if a certain participation quorum has been met. In Italy, 50 % of the eligible voters need to cast their vote in a referendum for the referendum to be valid, similarly in Poland and Portugal (Aguiar-Conraria and Magalhães 2010b). For local referenda in the Netherlands, the quorum depends on the city but is usually lower than 50 %, e.g., 30 % in the city of Rotterdam. Situations also differ in what happens when a referendum is invalid. In many cases, one of the two options is a default option that represents the status quo and that will be adopted (maintained) if the referendum turnout is too low. In other cases, the consequences of an invalid referendum are uncertain. In the case of Dutch local referenda, the city council will then vote on the issue with an (at least formally) uncertain outcome. This issue is discussed in more detail in Sect. 3 because it yields two different models of referenda, a symmetric model with three outcomes (treated in Sect. 5) and an asymmetric model with only two outcomes (treated in Sect. 6). A problem with the participation quorum is the so-called no-show paradox (Fishburn and Brams 1983): a voter who is against a proposed change in the status quo and who would thus naturally vote no, might be faced with a situation where a vocal minority is in favor of changing the status quo. If he also thinks that turnout for the referendum will be low because the issue at stake is not sufficiently important for most people to participate in the referendum, he might decide to abstain rather than to vote no because that way he thinks he is more likely to maintain the status quo, obtaining an invalid referendum rather than a valid referendum yielding no. This form of strategic manipulation evidenced by the no-show paradox is undesirable for at least four reasons. First, for an institutional designer, the information revealed by the referendum becomes highly ambiguous because we do not know how to interpret abstentions anymore. Second, this kind of manipulation increases the cognitive burdens for the agents involved because they will need to obtain information about how many people are likely to vote for and against, and in the end these strategic calculations may turn out to be wrong. Third, we would like a referendum procedure that induces people to honestly reveal their top preferences instead of voting strategically. Fourth, we would like a referendum procedure to encourage participation rather than abstention, also because in political practice, low participation rates are usually considered problematic for the legitimacy of the decisions reached. Hence, we would like a referendum procedure that is non-manipulable, a procedure where the no-show paradox cannot arise. As it turns out, there are referendum procedures which do not suffer from the no-show paradox. These procedures also work with a quorum but not with a participation quorum. Rather, in the case where we are dealing with the choice between change and the status quo, these procedures require that for change to be chosen, a certain minimal number of people needs to vote for change. In such an acceptance quorum procedure, the quorum requirement is thus not imposed on the number of total voters, but only on the number of voters who vote for change. Under this alternative quorum rule the voter we just considered cannot obtain a better result from strategically abstaining, for a no vote is not counted towards meeting the quorum. A number of countries work with acceptance quorums rather than participation quorums, such as Denmark and some German states. But the fact that a referendum working with an acceptance quorum is somewhat non-manipulable (in a sense to be made precise) and in that sense preferable to a participation quorum raises another question: are there possibly other referendum procedures that are non-manipulable as well? Which is the class of referendum procedures that is strategy-proof in situations like the one described? In what follows we will develop a formal model that allows us to answer this question. After reviewing previous formal work on referenda in Sect. 2, I will discuss how many policy outcomes we should use in our model of referenda (Sect. 3). The formal model of referenda and the main axioms are presented in Sect. 4, together with some preliminary results. The situation where an invalid referendum yields a genuine third alternative not equivalent to the status quo will be considered in Sect. 5. This model will be used to prove the first of our two main results, Theorem 1, which characterizes majority quorum rules in terms of strategy-proofness. Section 6 then deals with the situation where an invalid referendum means the adoption of the status quo and proves the second main result, Theorem 2, which provides a characterization of acceptance quorum rules. Proofs can be found in the appendix.",2
75.0,4.0,Theory and Decision,30 May 2013,https://link.springer.com/article/10.1007/s11238-013-9377-0,Market failure in light of non-expected utility,October 2013,Eyal Baharad,Doron Kliger,,Male,Male,Unknown,Male,"Akerlof’s seminal ”Market for Lemons” paper (Akerlof’s 1970) demonstrated that asymmetric information among Expected Utility (EU) maximizing agents might lead to market failure. In particular, buyers’ incomplete information on merchandise quality might result in a situation where high-quality products are ”under traded” and the potential gains from trade are not fully realized. About a decade later, Kahneman and Tversky (1979) challenged the paradigme of Expected Utility Theory (EUT, Von Neumann and Morgenstern 1944) with the introduction of Prospect Theory (PT) as an alternative, descriptive, theory of individuals’ decision making. Subsequently, further attempts to accommodate violations of EUT have been made, by substituting additive probabilities with non-additive Probability Weighting Functions (PWFs; e.g., Rank Dependent Expected Utility (RDEU, Quiggin 1982; Yaari 1987) and Cumulative Prospect Theory (CPT, Tversky and Kahneman 1992)). Our study revisits Akerlof’s ”Market for Lemons” in a setup where individuals’ behavior is governed by the above-mentioned behavioral decision-making theories. Specifically, we examine the effect of nonadditive PWF and loss aversion on the occurrence of market failure. We focus our attention on the effect of the decision-making parameters on prices in markets with different quality mixtures. We show that the PWF advocated by non-expected utility models boost market failure in markets characterized by high proportions of high-quality products, and reduce it in markets typified by low proportions of such products (assuming two quality types). More specifically, our study of RDEU with and without PWF is twofold: once for under risk neutrality, and then under risk aversion; We then analyze CPT with linear vs. subadditive PWFs. In addition, we concentrate on CPT, and show that the higher the loss aversion is, the more pronounced is the market failure. We explore the role of elevation (also referred to as ”attractiveness” , see Gonzalez and Wu 1999), finding that gain-domain elevation is negatively related to the extent of market failure, and complete the analysis by showing that the value function is (i) negatively monotonic in the gain-domain diminishing sensitivity parameter when the market is characterized by a high proportion of ‘peaches’, and (ii) positively monotonic in the loss-domain diminishing sensitivity parameter when the market is characterized by a high proportion of ‘lemons.’ The paper is organized as follows. In the next section we provide a literature review. Our general setting is presented in Sect. 3 , Sect. 4 provides a discussion on market failure in light of non-expected utility maximizing. Section 5 concludes. Figures and formal proofs are relegated to the Appendix.",1
76.0,1.0,Theory and Decision,04 January 2013,https://link.springer.com/article/10.1007/s11238-012-9349-9,Common consequence effects in pricing and choice,January 2014,Ulrich Schmidt,Stefan T. Trautmann,,Male,Male,Unknown,Male,"Common consequence effects—including the famous paradox of Allais (1953)—are the most prominent experimental design for observing violations of expected utility theory (Machina 1987; Birnbaum 2004). A common consequence effect (CCE) occurs if the preference between two lotteries changes if the same probability mass is shifted from one common outcome to a different common outcome in both lotteries. Numerous empirical studies reported this type of violation of expected utility, providing a motivation for the development of alternative theories like cumulative prospect theory, rank-dependent utility or the TAX model.Footnote 1 Given their theoretical importance and long tradition in empirical research on decision making under risk, it is surprising that almost all empirical studies have analyzed CCEs employing pairwise choice data. Work on CCEs with pricing data is virtually absent in the decision making literature. From an empirical perspective, however, pricing behavior is relevant in many market transactions, with agents stating buying and selling prices. If CCEs did (not) occur for pricing behavior, their economic relevance would be reinforced (challenged). In principle, pricing and choice data should reveal the same preference ordering. Many empirical observations show that this is not the case in practice. The most prominent example of such response mode effects is the preference reversal phenomenon (Lichtenstein and Slovic 1971). The preference between two alternatives elicited by a straight choice is the opposite of the preference elicited by minimal selling prices. In addition, for different pricing methods response mode effects can be observed, for instance the widely studied disparity between willingness-to-pay (WTP) and willingness-to-accept (WTA) (Knetsch and Sinden 1984). Given these response mode effects, we may expect that the incidence of CCEs also varies between choice task and pricing tasks, and between different pricing methods.Footnote 2
 The present paper studies the incidence of CCEs under direct pairwise choice, WTP elicitation (maximal buying price), and WTA elicitation (minimal selling price). We consider eight different pairs of choice situations involving common consequence probability shifts, and distinguish between two empirical patterns: the fanning out hypothesis and the fanning in hypothesis (discussed below). For the elicitation of WTP and WTA we employ incentive-compatible second-price auctions. The experimental design is presented in the next section. The following section presents the theoretical framework and the experimental results. The last section concludes.",3
76.0,1.0,Theory and Decision,24 March 2013,https://link.springer.com/article/10.1007/s11238-013-9361-8,Hyperbolic discount curves: a reply to Ainslie,January 2014,Andrew Musau,,,Male,Unknown,Unknown,Male,"(Ainslie (2012), p. 27) (hereafter simply Ainslie) questions our understanding of the properties of hyperbolic discount curves as modeled in Musau (2009) (hereafter [M]). We are grateful for the opportunity to provide a more structured discussion of our model and evaluate Ainslie’s points of contention. Evidently, our conclusions were not clearly stated. The paper is organized as follows: Sect. 2 provides a discussion of the emergence of hyperbolic discount functions in the behavioral economics literature and evaluates their properties. Section 3 highlights the relevance of calculating incentives to cooperate in the iterated prisoners’ dilemma (IPD) under non-exponential discounting. Section 4 presents a summarized version of the model and results in [M]. Section 5 responds to Ainslie’s points with reference to the previous sections. Eventually, Sect. 6 concludes.",3
76.0,1.0,Theory and Decision,07 February 2013,https://link.springer.com/article/10.1007/s11238-013-9354-7,Experimental Cournot oligopoly and inequity aversion,January 2014,Doruk İriş,Luís Santos-Pinto,,Male,Male,Unknown,Male,"Although quantity-setting oligopoly is one of the “workhorse models” of industrial organization, experimentally there is much ambiguity about its outcome. A recent survey by Georgantzis (2006) indicates that many experimental Cournot oligopoly games reject the hypothesis that the outcome is in line with the Cournot–Nash equilibrium of the corresponding one-shot game. Interestingly, however, outcomes on both sides of the Cournot–Nash outcome are found: some experiments result in higher than Cournot–Nash production levels while others result in lower production levels (Holt 1995).Footnote 1
 Why does the theory perform poorly in the experiments? One possibility is that players are averse to inequality in earnings, that is, they are concerned about their own material payoff but also about the consequences of their acts on payoff distributions. Inequity aversion has been shown to explain a broad range of data for many different games. The clearest evidence for these type of preferences comes from bargaining and trust games. For example, in ultimatum games offers are usually much more generous than predicted by subgame perfect equilibrium, and low offers are often rejected. According to the inequity aversion explanation, these offers are consistent with an equilibrium in which players make offers knowing that other players may reject allocations that appear unfair.Footnote 2
 In this paper, we study formally the role of inequity aversion on Cournot competition. We assume that a player cares about her own monetary payoff and, in addition, would like to reduce the difference between her payoff and those of her rivals. More specifically, a inequity averse player dislikes advantageous inequity: she feels compassion towards her rivals when the average material payoff of her rivals is smaller than her own material payoff. In addition, an inequity averse player also dislikes disadvantageous inequity: she feels envy towards her rivals when the average material payoff of her rivals is greater than her own material payoff. We find that inequity aversion can change the nature of the strategic interaction: quantities are strategic substitutes when players choose asymmetric output levels but strategic complements when they choose similar output levels. This can give rise to a continuum of equilibria. We show that the set of Nash equilibria of Cournot competition with inequity averse players changes monotonically with compassion and envy. If players’ degree of envy increases, then the largest Nash equilibria of the Cournot game moves closer to the Walrasian outcome. In contrast, if players’ degree of compassion increases, then the smallest Nash equilibria of the Cournot game moves closer to the collusive outcome. However, as the number of players grows the impact of inequity aversion vanishes. This happens because it takes only one self-interested player to destroy the continuum of equilibria generated by inequity aversion. We find that relatively low levels of inequity aversion generate less asymmetries in profits than those predicted when self-interested players play asymmetric Cournot oligopolies. We also show that relatively high levels of inequity aversion can explain why often players attain equal profits in asymmetric experimental Cournot oligopolies. The intuition for this result is straightforward. For relatively high levels of inequity aversion, attaining asymmetric profits imposes inequity costs that are too high in relation to the material benefits. Our findings are more relevant in experimental Cournot oligopolies with a small number of players. In fact, we show that increasing the number of players reduces the impact of inequity aversion on Cournot oligopoly. This finding is consistent with Huck et al. (2004) who find some collusion with two firms and no collusion as the number reaches four firms. Our findings are also more relevant in experimental Cournot oligopolies where individuals rather than teams play the role of firms. Hildenbrand (2012) studies a Stackelberg experiment in which the firms are either represented by individuals or teams. He finds that individuals exhibit more inequity aversion than teams.Footnote 3
 This paper is related to a recent strand of literature in economics that studies the consequences of relaxing the assumption of pure self interest. Rabin (1993) is the first using fairness considerations in game theory. Sappington and Desiraju (2007) study inequity aversion in adverse selection contexts. Biel (2008) studies how the optimal incentive contract in team production is affected when workers are averse to inequity. Santos-Pinto (2008) shows that inequity aversion is able to organize several experimental regularities of endogenous timing games. Englmaier and Wambach (2010) study optimal contracts when the agent suffers from being better off or worse off than the principal. The paper proceeds as follows. Section 2 sets-up the model. Section 3 characterizes equilibria of Cournot oligopoly with symmetric costs and inequity averse players. Section 4 considers Cournot oligopoly with asymmetric costs. Section 5 concludes the paper. All proofs are in the Appendix.",7
76.0,1.0,Theory and Decision,27 March 2013,https://link.springer.com/article/10.1007/s11238-013-9360-9,An experimental investigation of intrinsic motivations for giving,January 2014,Mirco Tonin,Michael Vlassopoulos,,Male,Male,Unknown,Male,"What motivates people to act generously, for instance, by making donations to charities? Besides extrinsic motives such as tax breaks, thank-you gifts, and various material rewards deriving from, for example, developing a reputation for being generous, there are intrinsic motives for giving. In particular, the literature has focused on a distinction between two types of intrinsic motivation: pure altruism and warm glow (Andreoni 1989, 1990). The crucial distinction is that people motivated by pure altruism care about the total amount of public good that is provided, for instance, because others’ well-being enters directly their utility function (Becker 1974), while people motivated by warm glow care about their own individual donation, which acquires properties of a private good. Of course, the two motives may be also operating simultaneously. Recently, various papers have argued that warm glow encompasses the signaling benefits of altruistic actions, including concerns for self image, social image, and esteem (Benabou and Tirole 2006; Ellingsen and Johannesson 2008, 2011; Andreoni and Bernheim 2009; Grossman 2010). Besides the need to obtain a cleaner picture of the motives for giving the interest in the distinction between warm glow and pure altruism stems from the fact that it has implications for the evaluation of a long standing idea in public finance, the so-called crowding out hypothesis: the possibility that private giving in support of charitable causes may be crowded out by public spending. As purely altruistic motivation is subject to crowding out while warm glow is not, empirical studies in the field and the lab have sought to estimate the extent of crowding out to provide evidence of the relative importance of warm glow and pure altruism.Footnote 1 Field studies have usually found little evidence of crowding out, however, interpretation of such findings may be difficult. For instance, a recent investigation using a panel of charities (Andreoni and Payne 2011) finds that crowding out is primarily due to reduced effort in fund-raising by charities. In laboratory experiments, on the other hand, crowding out is typically found to be significant (e.g., Eckel et al. 2005). This paper makes two main contributions to the literature that is concerned with understanding and measuring the motives for charitable giving. First, we demonstrate that the experimental design developed by Crumpler and Grossman (2008) [CG henceforth] does not offer a clean test of warm glow due to the presence of strong altruistic feelings toward the experimenter. This finding has implications for all designs in which a subject’s action has an impact on the total amount that is paid by the experimenter (Harrison and Johnson 2006). Our second contribution is to offer an alternative and novel test for detecting warm glow giving that is not based on crowding out. Also, under reasonable assumptions, we can provide bounds on the magnitudes of warm glow and pure altruism as motives that drive charitable giving. Our starting point is the CG design which entails a modified dictator game in which dictators could choose the recipient from a list of charities.Footnote 2 The chosen charity received a fixed amount from the experimenter and any amount the dictator decided to pass on to the charity crowded out one-for-one the experimenter’s contribution. This substitution removes the incentive to give for someone who is a pure altruist toward the charity, that is, someone who is exclusively concerned about the total amount that goes to the charity regardless of the identity of the giver. On the other hand, for a warm glow giver who derives utility from the act of giving per se, the incentive to give is still active. CG find that subjects in their experiment gave an average of 20 % of their endowment and attribute this to the warm glow motive for giving. Note, however, that other interpretations for this finding cannot be ruled out. As the authors themselves acknowledge: [\(\ldots \)] it is possible that participants are making contributions because they have some altruistic feelings for the experimenter. By giving to the charity, the subject is reducing the financial burden on the experimenter. (p. 1014) Our experimental design allows us to assess how serious this confounding factor is. In particular, in our experiment subjects make three decisions of how to allocate an endowment of \(\pounds 10\). We introduce a condition in which the recipient is the experimenter (we refer to it as T1), besides replicating the warm glow treatment of CG (we refer to it as T2). In T1, subjects may decide to share some of the endowment with the experimenter either because of purely altruistic or warm glow feelings or because they want to be kind to someone who has already been kind to them, that is, because of a concern for fairness or reciprocity (Rabin 1993; Fehr and Gachter 2000). Note that in terms of the impact of the subject’s decision on the final allocations for the subject and the experimenter, this condition is identical to the warm glow treatment implemented by CG, where the subject’s giving reduces the experimenter’s costs by the same rate. For example, if in T2 a subject gives \(\pounds 5\), then he or she receives \(\pounds 5\) and reduces the experimenter’s cost by \(\pounds 5\), that is, the subject in effect gives back to the experimenter \(\pounds 5\) out of the \(\pounds 10\) that were handed out as an endowment. Thus, T1 allows us to identify those subjects who may have altruistic feelings toward the experimenters that CG refer to in the quote. We find that a sizeable share of subjects is indeed of this type and we fail to reject the null that average giving in T1 is equal to T2, suggesting that the above-mentioned confounding effect is potentially serious. In the third condition that we implemented (we refer to it as T3), the recipient was again a charity of the subject’s choice only this time the amount passed on to the charity was not fixed but was determined by what the subject donated, if anything. In this third condition, both types of motives, warm glow and pure altruism, are operating while altruistic feelings toward the experimenter are not induced. In our experiment participants are exposed to these three treatments in random order, being aware that at the end of the experiment only one of the decisions will be selected at random to determine payoffs. We use the probabilistic implementation of the decisions in our design to perform a new test of detecting warm glow motivation. The idea behind our test is that purely altruistic motivation is by definition conditional on the donation being actually implemented, while for warm glow motivation this is not necessarily the case. If the warm glow of acting generously derives, for instance, from the benefit of self-signaling, as in Benabou and Tirole (2006), then the fact that the donation may not be implemented does not erase the signaling benefit of a donation, which implies that warm glow may accumulate by spilling-over across treatments. This, instead, is not the case with purely altruistic motivation. Consequently, if there is a warm glow component in the utility function, then the position in which a given treatment is taken within the sequence of decisions matters, as a subject’s warm glow gratification may be more satiated when a decision is taken later in the experiment. Indeed, we find average giving to be higher in decisions taken earlier in the experiment, thus providing evidence of warm glow as a motive that drives giving. We also discuss the implications of our findings for the interpretation of other experiments that use a probabilistic implementation of a sequence of choices. Finally, our design enables us to offer a lower bound estimate of warm glow giving—clean of any confounding altruistic concerns for the experimenters—by examining the behavior of those subjects who did not display any altruistic behavior in T1. For subjects undergoing T3 in the first decision we estimate that giving due to warm glow is between 22 and 28 % of endowment. We can also quantify giving due to purely altruistic motivation and we find that pure altruism accounts for a donation in the range of 20 and 26 % of endowment.Footnote 3 Thus, we find the two intrinsic motivations for giving to be roughly equally important in our experiment. The structure of the rest of the paper is as follows. Section 2 presents the experiment. Section 3 assesses the impact of altruism toward the experimenter; Sect. 4 introduces our test of warm glow giving, while Sect. 5 provides bounds for warm glow and pure altruism in our experiment. The last section concludes.",31
76.0,1.0,Theory and Decision,26 February 2013,https://link.springer.com/article/10.1007/s11238-013-9357-4,Equilibrium and potential in coalitional congestion games,January 2014,Sergey Kuniavsky,Rann Smorodinsky,,Male,Unknown,Unknown,Male,"Congestion games, introduced by Rosenthal (1973), form a very natural model for studying many real-life strategic settings: traffic problems, load balancing, routing, network planning, facility locations and more. In a congestion game players must choose some subset of resources from a given set of resources (e.g., a subset of edges leading from the source to the target on a graph). The congestion of a resource is a function of the number of players choosing it and each player seeks to minimize his total congestion across all chosen resources. In many modeling instances players and the decision making entity have been thought of as one and the same. However, in a variety of settings this may not be the case. Consider, for example, a traffic routing game where each driver chooses his route in order to minimize his travel time, while accounting for congestion along the route caused by other drivers. Now, in many cases drivers are actually employees in shipping firms, and in fact it is in the interest of the shipping firm to minimize the total travel time of its fleet. Similarly, routers in a communication network participate in a congestion game. However, as various routers may belong to the same ISP we are again in a setting where coalitions naturally form. This motivated Fotakis et al. (2006) and Hayrapetyan et al. (2006) to introduce the notion of Coalitional Congestion Games (CCG). In a CCG, we think about the coalitions as players and each coalition maximizes its total utility. The coalitional congestion game inherits its structure from the original game, once the coalitions of players from the original congestion games (now, becoming the players of the coalitional congestion game) have been identified. A most notable property of congestion games is that the set of pure Nash equilibria is nonempty. This has been shown by Rosenthal in (1973). Later, Monderer and Shapley (1996) formally introduce potential games and show the equivalence of these two classes. The fact that potential games posses a pure Nash equilibrium is straightforward. Unfortunately, the statement that a CCG is a potential game or that it possesses a pure Nash equilibrium is generally false. In this paper, we investigate conditions under which this statement is true. We focus on a subset of congestion games called simple congestion games, where each player is restricted to choose a single resource. Our main contributions are: Whenever each coalition contains at most two players a CCG induced from a simple congestion game possesses a pure-strategy Nash equilibrium (Theorem 1). If some coalition contains three players, then there need not exist a pure-strategy Nash equilibrium (Example 1). If a the congestion game is not simple then there need not exist a pure-strategy Nash equilibrium, even if the largest coalition is a pair (Example 2); and Suppose there exists at least one singleton coalition and at least one coalition composed of two players, then a coalitional congestion game induced from a simple congestion game is a potential game if and only if cost functions are linear (Theorem 2). Our results extend and complement the results in Fotakis et al. (2006) and Hayrapetyan et al. (2006). For example, Fotakis et al. (2006) show that if the resource cost functions are linear then the coalitional congestion game is a potential game. We show that, with some additional mild conditions on the partition structure, this is also a necessary condition. Hayrapetyan et al. (2006) shows that if the underlying congestion game is simple and costs are weakly convex then the game possesses a pure Nash equilibrium. We demonstrate additional settings where this holds. The structure of the paper is as follows: Sect. 2 provides a model of coalitional congestions games, Sect. 3 discusses the conditions for the existence of a pure Nash equilibrium in such games, and Sect. 4 discusses the conditions for the existence of a potential function. All proofs are relegated to an appendix.",3
76.0,1.0,Theory and Decision,07 February 2013,https://link.springer.com/article/10.1007/s11238-013-9353-8,Benchmark values for higher order coefficients of relative risk aversion,January 2014,Michel Denuit,Béatrice Rey,,Male,Female,Unknown,Mix,,
76.0,1.0,Theory and Decision,11 March 2013,https://link.springer.com/article/10.1007/s11238-013-9356-5,Different carrots and different sticks: do we reward and punish differently than we approve and disapprove?,January 2014,Andreas Leibbrandt,Raúl López-Pérez,,Male,Male,Unknown,Male,"In many social interactions, the availability of approval and disapproval seems to affect behavior.Footnote 1 On the one hand, social approval for generosity and courage may provide an incentive for charitable donations (Vesterlund 2006) and military service (Frey 2007). On the other hand, disapproval from work colleagues may prevent highly productive employees from exceeding the formally agreed rate of output (Homans 1961), or preclude workers from underbidding the prevailing wage in a community (Akerlof 1980). In a more anecdotic example, people sometimes take queue-jumpers to task, which probably attenuates such opportunistic behavior. Finally, some controlled lab evidence shows that approval and disapproval can foster pro-social behaviors (Masclet et al. 2003; Rege and Telle 2004; Noussair and Tucker 2005; Xiao and Houser 2005; Ellingsen and Johannesson 2008; Xiao and Houser 2009; Dugar 2010).Footnote 2
 In this paper, we report data from two experimental treatments which complement the previous literature. The research goal of our first treatment is to investigate the determinants of approval and disapproval. In this manner we can understand better what situations are more likely to trigger negative or positive reactions, an important issue if we want to explain behaviors like the ones cited above or, in general, if we believe that such reactions are deterrents of norm deviance or amplifiers of socially desirable behaviors like cooperation. Participants in this treatment play four simple games with a two-stage structure. In the first stage, the first mover A chooses between two allocations of monetary payoffs for her and the second mover B. In the second stage, the second mover can then assign negative/positive points to A conditional on A’s choice; these points do not affect A’s payoff but it is common knowledge that they respectively signal disapproval/approval by B. The four games differ systematically in the available allocations, which renders it possible to explore several potential determinants of disapproval and approval. In particular, whether the second mover disapproves/approves because the first mover (i) chose the allocation in the game with the minimum/maximum payoff for B—i.e., A harmed/helped B [see Holländer (1990) for a model in this line], (ii) deviated from/chose a strictly egalitarian allocation, or (iii) chose an allocation where she gets a larger/lower payoff than B. Our second treatment is identical to the first one, except that the negative/positive points here reduce/increase A’s payoff and are not explicitly described as disapproval/approval. By comparing the results from both treatments, we aim to compare the determinants of approval/disapproval with those of monetary punishment/reward, which directly affects the pecuniary payoff of the punished/rewarded agent. Are these determinants similar? This research question is motivated by two issues. On the one hand, a large experimental literature shows that the availability of monetary punishment and rewards can promote cooperation, generosity, and fairness (Güth et al. 1982; Ostrom et al. 1992; Andreoni et al. 2003; Fehr and Gächter 2000; Anderson and Putterman 2006; Gürerk et al. 2006; Walker and Halloran 2004; Sefton et al. 2007; Vyrastekova and van Soest 2008; Ertan et al. 2009). On the other hand, we suspect that these laboratory findings may be difficult to extrapolate to the field, where the use of monetary sanctions goes often against the law or prevailing social norms. For instance, destroying some of another person’s wealth, which is an extreme example of a monetary sanction, is condemned by many people. In contrast, the use of disapproval is arguably less regulated, and we conjecture that most non-institutionalized sanctions outside the lab take the form of disapproval. If there are substantial differences in the determinants of monetary and non-monetary punishment, therefore, we need to be particularly careful when extrapolating lab findings on monetary sanctions. Overall, our findings suggest some key differences in motivation for punishing/rewarding and disapproving/approving. On the one hand, we find that factors (i) and (ii) above play the key role for disapproval and approval: People disapprove choices that harm them or deviate from a strictly egalitarian allocation, and approve those that help them or achieve strict equality. On the other hand, we observe that monetary punishment and reward are most affected by factor (iii) and to some extent (i): Punishment/reward increases if player A gets a larger/lower payoff than player B (for related evidence see Zizzo 2003; Falk et al. 2005; Dawes et al. 2007) and if player A harmed/helped B. Thus, our study provides evidence that payoff comparisons are relatively more important to explain monetary punishment and reward suggesting that the punishment/reward technology determines which behaviors are punished and rewarded. Our paper is related to previous studies comparing monetary and non-monetary punishment and reward. Masclet et al. (2003), for example, find in the context of a repeated public goods experiment that non-monetary punishment can be similarly effective as monetary one to raise contributions and earnings. In a similar context, Noussair and Tucker (2005) show that contributions and overall welfare are higher when both types of punishment are available than when only one of the two types is available. Xiao and Houser (2005) study the role of emotion expression in ultimatum games and observe fewer rejections of unfair offers if recipients can send a message to the proposer—see also Ellingsen and Johannesson (2008) and Xiao and Houser (2009), who study emotion expression in a dictator game. In general, the focus of much if this literature is whether the availability of monetary and non-monetary punishment/reward increases cooperation, fairness, or social efficiency, whereas our focus is on motivations, comparing those behind monetary and non-monetary punishment/reward. For this reason, we consider one-shot games which allow us to disentangle between inequity aversion (IA) and reciprocity (RP) concerns and rule out reputation or temporal effects.Footnote 3
 The rest of the paper is organized as follows. The next section describes our experimental design and procedures. Section 3 presents and discusses our results. Section 4 concludes with some ideas for further research.",7
76.0,1.0,Theory and Decision,01 June 2012,https://link.springer.com/article/10.1007/s11238-012-9309-4,The econometric modelling of social preferences,January 2014,Anna Conte,Peter G. Moffatt,,Female,Male,Unknown,Mix,,
76.0,2.0,Theory and Decision,11 April 2013,https://link.springer.com/article/10.1007/s11238-013-9363-6,Theory and implementation of coalitional analysis in cooperative decision making,February 2014,Haiyan Xu,D. Marc Kilgour,Edward A. McBean,Unknown,Unknown,Male,Male,"The consideration of coalitions within a formal approach to investigating a conflict permits a given decision maker (DM) to decide if he or she can strategically improve via cooperation with others. In fact, as demonstrated by the real world dispute over the potential sale of Canadian water in bulk quantities in this research, a coalition that the Federal Government of Canada and the Provincial Government of Newfoundland and Labrador formed produced a resolution in which water exports were not allowed and, hence, negative environmental impacts were avoided. However, when the price of water increases in the future, it is more beneficial for the Provincial Government to cooperate with the proponents of exporting water to gain revenues for the province. The objective of this research is to produce new results in coalition analysis to permit the thorough investigation of real world conflicts in which coalitions may form. As depicted in Fig. 1, many models are available for describing strategic conflicts. The game theory techniques in the right branch, which were first proposed in the 1944 book of Von Neumann and Morgenstern (1944), employ cardinal preferences for each party such as cardinal utilities or money to represent preference. Within the left branch, metagame analysis was initially proposed by Howard (1971) and was subsequently expanded and improved by Fraser et al. (1984) to form a set of techniques called conflict analysis. Significant enhancements of conflict were made by Kilgour et al. (1987) and Fang et al. (1993) to create the methodology of the Graph Model for Conflict Resolution (GMCR) which, as the title implies, possesses a graph theoretic basis. The approaches given on the left in Fig. 1 only require relative preference information for each DM, while those in the right branch need cardinal preferences (Howard 1971; von Neumann and Morgenstern 1944). This is the main difference between game theory and conflict analysis. GMCR provides a convenient and effective means to model and analyze a strategic conflict. Genealogy of formal conflict models Previously, stabilities (solution concepts) in the graph model were traditionally defined logically, in terms of the underlying graphs and preference relations. Specifically, when determining the stability of a state for a given DM, a logical structure is employed for tracking the moves and countermoves that could take place if the DM decides to improve his or her situation. If the DM perceives that he or she will end up in a less preferred situation as a result of these potential interactions with others, the state is deemed to be stable. If a state is stable for all DMs, it is called an equilibrium. Many stabilities are available including limited-move stability definition (Zagare 1984). Individual stabilities consisting of Nash stability (Nash 1950, 1951), general metarationality (GMR) and symmetric metarationality (SMR) (Howard 1971), as well as sequential stability (SEQ) (Fraser and Hipel 1979) in GMCR are non-cooperative in that each DM is modeled as trying to do his or her best according to his or her own goals. But sometimes, several DMs form a coalition that is a subset of DMs to obtain mutual benefits for all coalition members. Indeed, cooperative behavior can lead a conflict in a positive direction toward what are called win–win resolutions in which different DMs’ interests can be satisfied, at least to some degree (Hipel et al. 1991). With this observation in mind, Kilgour et al. (2001) defined a coalitional stability and pointed out that a state that is not an equilibrium has no long-term stability because there is at least one individual DM who has the incentive to move away to upset the temporarily stable state. Therefore, the coalitional stability analysis within the graph model assesses whether states that are stable from individual viewpoints may be unstable for coalitions (Kilgour et al. 2001). Subsequently, Inohara et al. (2008a, b) expanded the four basic individual stabilities, Nash GMR, SMR, and SEQ to coalitional stabilities, CNash, CGMR, CSMR, and CSEQ, under simple preference. Analysis of a strategic conflict in a graph model involves searching paths in a graph but an important restriction of a graph model is that no DM can move twice in succession along any path. However, to make calculation easier, the coalitional stability based on Nash stability (Kilgour et al. 2001) and the study developed by Inohara et al. (2008a, b) are based on the assumption of transitive graphs that allow consecutive movements by the same DM. Prohibiting consecutive moves thus allows for graph models with intransitive graphs, which are sometimes very useful in practice. In addition, their studies (Kilgour et al. 2001; Inohara et al. 2008a, b) retain a logical structure that results in coalitional stabilities that have not been integrated into a decision support system (DSS). As was noted in the development of the DSS GMCR II (Fang et al. 2003a, b; Kilgour et al. 1990), the nature of logical representations makes coding difficult because stability analysis is related to tracking the moves and countermoves that could take place if the DM decides to improve his or her situation. This is equivalent to search paths with the restriction that the same DM cannot move in succession. Although some algorithms exist (Abouelaoualim et al. 2008), they cannot be used to trace the evolution of a conflict. To overcome this limitation, matrix representation of conflict resolution for simple preference, preference with uncertainty, three-level preference, and hybrid preference is formulated explicitly in terms of matrices by Xu et al. (2009, 2010b, c, 2011, 2013). The matrix representation has extended logical framework of GMCR into a new matrix formulation that allows the calculations to be determined using a number of simple matrix operations. It is natural to extend the matrix representation for conflict resolution to include coalitional stabilities. Therefore, Xu et al. (2010a) extended the coalitional Nash stability to include unknown preference (Li et al. 2004) and gave its matrix representation. The current status of coalitional stability in the graph model is summarized in Table 1. Coalition analysis originates from game theory (Aumann et al. 1994; van Deemen 1997). In this research, coalitional stabilities are still focused on the status quo states that are equilibria. A theory based on a logical representation is proposed in this article to define four basic coalitional stabilities, CNash, CGMR, CSMR, and CSEQ, which are subject to the important restriction that no DM can move twice in succession along any path in a graph for simple preference. Note that the coalitional SEQ stability includes two situations. One is \(CSEQ_{1}\) which treats coalition H’s opponents \(N-H\) as being cooperative, another is \(CSEQ_{2}\) to treat H’s opponents as being non-cooperative. The four basic coalitional stabilities in multiple DM graph models relax the assumption of transitive graphs in existing research (Inohara et al. 2008a, b; Kilgour et al. 2001) and generalize the four individual stabilities consisting of Nash, GMR, SMR, and SEQ (Fang et al. 1993). However, these logical representations of the four coalitional stabilities often require complex calculations and are difficult to code. To make the coalitional stabilities encode effectively within a DSS, algebraic expressions of the four basic coalitional stabilities, CNash, CGMR, CSMR, and CSEQ, are developed in this article to provide easier computer implementation and quicker calculations. The algebraical method possesses useful algebraic properties that can be exploited to produce improved algorithms for solving conflict problems using coalitional stabilities. A key advantage of having a DSS based on algebraic definitions is that the system can facilitate modification and extension of the definitions. The structure of this article is presented as follows. First, logical forms of the four basic coalitional stabilities for simple preference in the graph model are presented in Sect. 2, followed by the algebraic expressions of the four coalitional stabilities in Sect. 3. Within Sect. 4, a model of the Lake Gisborne conflict is analyzed to illustrate the practical application of coalitional stabilities under simple preference and to demonstrate their unique features. Finally, some conclusions and ideas for future work are presented in Sect. 5.",11
76.0,2.0,Theory and Decision,09 May 2013,https://link.springer.com/article/10.1007/s11238-013-9367-2,Randomized dictatorship and the Kalai–Smorodinsky bargaining solution,February 2014,Shiran Rachmilevitch,,,Female,Unknown,Unknown,Female,"The classic bargaining problem, originated in Nash (1950), is defined as a pair \((S,d)\), where \(S\subset \mathbb{R }^2\) is the feasible set, representing all possible (v-N.M) utility agreements between the two players, and \(d\in S\), the disagreement point, is a point that specifies their utilities in case they do not reach a unanimous agreement on some point of \(S\). The following assumptions are made on \((S,d)\): 
\(S\) is compact and convex; 
\(d<x\) for some \(x\in S\);Footnote 1
 For all \(x\in S\) and \(y\in \mathbb{R }^2\): \(d\le y\le x\Rightarrow y\in S\). Denote by \(\mathcal{B }\) the collection of all such pairs \((S,d)\). A solution is any function \(\mu :\mathcal{B }\rightarrow \mathbb{R }^2\) that satisfies \(\mu (S,d)\in S\) for all \((S,d)\in \mathcal{B }\). Given a feasible set \(S\), the weak Pareto frontier of \(S\) is \(WP(S)\equiv \{x\in S:y> x\Rightarrow y\notin S\}\) and the strict Pareto frontier of \(S\) is \(P(S)\equiv \{x\in S:y\gneqq x\Rightarrow y\notin S\}\). The best that player \(i\) can hope for in the problem \((S,d)\), given that player \(j\) obtains at least \(d_j\) utility units, is \(a_i(S,d)\equiv \text{ max }\{x_i:x\in S_d\}\), where \(S_d\equiv \{x\in S:x\ge d\}\). The point \(a(S,d)=(a_1(S,d),a_2(S,d))\) is the ideal point of the problem \((S,d)\). The Kalai–Smorodinsky solution, \(KS\), due to Kalai and Smorodinsky (1975), is defined by \(KS(S,d)=P(S)\cap [d;a(S,d)]\).Footnote 2 The Nash solution, \(N\), due to Nash (1950), is defined to be the unique maximizer of \((x_1-d_1)(x_2-d_2)\) over \(S_d\). Nash showed that this is the unique solution that satisfies the following axioms, in the statements of which \((S,d)\) and \((T,e)\) are arbitrary problems. 
Weak Pareto optimality (WPO): \(\mu (S,d)\in WP(S)\).Footnote 3
 Let \(F_A\) denote the set of positive affine transformations from \(\mathbb{R }\) to itself.Footnote 4
 
Independence of equivalent utility representations (IEUR): \(f=(f_1,f_2)\in F_A\times F_A\Rightarrow f\circ \mu (S,d)=\mu (f\circ S,f\circ d)\).Footnote 5
 Let \(\pi (a,b)\equiv (b,a)\). 
Symmetry (SY): \( [\pi \circ S=S] \& [\pi \circ d=d]\Rightarrow \mu _1(S,d)=\mu _2(S,d)\). 
Independence of irrelevant alternatives (IIA): \( [S\subset T] \& [d=e] \& [\mu (T,e)\in S]\Rightarrow \mu (S,d)=\mu (T,e)\). Whereas the first three axioms are widely accepted, IIA has been a target for criticism. The idea behind a typical such criticism is that the bargaining solution could, or even should, depend on the shape of the feasible set. In particular, Kalai and Smorodinsky (1975) noted that when the feasible set expands in such a way that for every feasible payoff for player 1 the maximal feasible payoff for player 2 increases, it may be the case that player 2 loses from this expansion under the Nash solution. Given \(x\in S_d\), let \(g_i^S(x_j)\) be the maximal possible payoff for \(i\) in \(S\) given that \(j\)’s payoff is \(x_j\), where \(\{i,j\}=\{1,2\}\). What Kalai and Smorodinsky noticed, is that \(N\) violates the following axiom, in the statement of which \((S,d)\) and \((T,d)\) are arbitrary problems with a common disagreement point. 
Individual monotonicity (IM): Furthermore, they showed that when one deletes the controversial IIA from the list of Nash’s axioms and replaces it by IM, a characterization of \(KS\) obtains. The class of solutions that satisfy the three common axioms to Nash and Kalai–Smorodinsky is large, and includes interesting and intuitive solutions. For example, it includes the Perles–Maschler solution (due to Perles and Maschler (1981)) and the equal area solution (due to Anbarci and Bigelow (1994)).Footnote 6 There is no wonder, then, that IIA is viewed as one of the most essential properties exhibited by \(N\), and the same is true for the relation between IM and \(KS\).",3
76.0,2.0,Theory and Decision,09 May 2013,https://link.springer.com/article/10.1007/s11238-013-9368-1,Mutual Fund Theorem for continuous time markets with random coefficients,February 2014,Nikolai Dokuchaev,,,Male,Unknown,Unknown,Male,"We study an optimal portfolio selection problem in a market model which consists of a risk-free bond or bank account and a finite number of risky stocks. The evolution of stock prices is described by Ito stochastic differential equations with the vector of the appreciation rates \(a(t)\) and the volatility matrix \(\sigma (t)\), while the bond price is exponentially increasing with a random risk free rate \(r(t)\). A typical optimal portfolio selection problem is to find an investment strategy that maximizes \(\mathbf{E}U(\widetilde{X}(T))\), where \(\mathbf{E}\) denotes the mathematical expectation, \(U(\cdot )\) is an utility function, \(X(T)\) represents the wealth at final time \(T\), and \(\widetilde{X}(T)=\exp \biggl (-\int _0^Tr(s)\mathrm{d}s\biggr )X(T)\) is the discounted wealth. There are many works devoted to different modifications of this problem [see, e.g., Merton (1969) and review in Karatzas and Shreve (1998)]. Dynamic portfolio selection problems are usually studied in the framework of stochastic control. To suggest a strategy, one needs to forecast future market scenarios (or the probability distributions, or the future distributions of \(r(t), a(t)\) and \(\sigma (t)\)). Unfortunately, the nature of financial markets is such that the choice of a hypothesis about the future distributions is always difficult to justify. In fact, it is still an open question if there is any useful information in the past prices that helps to predict the future. Respectively, there are serious reservations toward usual tools of stochastic control such as dynamic programing or stochastic maximum principle that require knowledge of future values of the process \(\mu (t)=(r(t),a(t),\sigma (t))\). It is why some special methods were developed for the financial models to deal with limited predictability. One of this tools is the so-called Mutual Fund Theorem that says that the distribution of the risky assets in the optimal portfolio does not depend on forecast of future values of \(\mu \) and on the investor’s risk preferences (or utility function). This means that all rational investors may achieve optimality using the same mutual fund plus a saving account, and this mutual fund does not need to use the market forecast. Clearly, calculation of the optimal portfolio is easier in this case. If Mutual Fund Theorem holds, then, for a typical model, portfolio stays on the efficient frontier even if there are errors in the forecast, i.e., it is optimal for some other risk preferences. This reduces the impact of forecast errors. This is another reason why it is important to know when Mutual Fund Theorem holds. Mutual Fund Theorem was established first for the single period mean variance portfolio selection problem, i.e., for the problem with quadratic criterions. This result was a cornerstone of the modern portfolio theory. In particular, the capital assets pricing model is based on it. For the multi-period discrete time setting, some versions of Mutual Fund Theorem were obtained so far for problems with quadratic criterions only (Li and Ng 2000; Dokuchaev 2010a). For the continuous time setting, Mutual Fund Theorem was obtained for portfolio selection problems with quadratic criterions as well as for more general utilities. In particular, Merton’s optimal strategies for \(U(x)=\delta ^{-1}x^\delta \) and \(U(x)=\log (x)\) are such that Mutual Fund Theorem holds for the case of random coefficients independent from the driving Brownian motion (Karatzas and Shreve 1998). It is also known that Mutual Fund Theorem does not hold for power utilities in the presence of correlations; see, e.g., Brennan (1998) and Feldman (2007). Khanna and Kulldorff (1999) proved that Mutual Fund Theorem holds for a general utility function \(U(x)\) for the case of non-random coefficient, and for a setting with consumption. Lim (2004, 2005) and Lim and Zhou (2002) found some cases when Mutual Fund Theorem holds for problems with quadratic criterions. Dokuchaev and Haussmann (2001) found that Mutual Fund Theorem holds if the scalar value \(\int _0^T|\theta (t)|^2\mathrm{d}t\) is non-random, where \(\theta (t)\) is the market price of risk process. Schachermayer et al. (2009) found sufficient conditions for Mutual Fund Theorem expressed via replicability of the European type claims \(F(Z(T))\), where \(F(\cdot )\) is a deterministic function and \(Z(t)\) is the discounted wealth generated by the log-optimal discounted wealth process. The required replicability has to be achieved by trading of the log-optimal mutual fund with the discounted wealth \(Z(t)\). It can be summarized that Mutual Fund Theorem was established so far for the following continuous time optimal portfolio selection problems: For \(U(x)\equiv \log (x)\) and general random coefficients \((r,a,\sigma )\); For \(U(x)=\delta ^{-1}x^\delta , \delta \ne 0\) and random coefficients \((r,a,\sigma )\) being independent from the driving Brownian motions; For problems with quadratic criterions; For general utility and non-random coefficients \((r,a,\sigma )\); For general utility when the integral \(\int _0^T|\theta (t)|^2\mathrm{d}t\) is non-random; For general utility when the claims \(F(Z(T))\) can be replicated via trading of a mutual fund with the discounted wealth \(Z(t)\), for deterministic functions \(F\). It can be noted that conditions (iv) implies (v), and (v) implies (vi). Extension of Mutual Fund Theorem on problems (i)–(vi) was not trivial; it required significant efforts and variety of mathematical methods. In this paper, we present one more case when Mutual Fund Theorem holds. More precisely, we found that it holds for general utility when the parameters \(r(t), a(t)\) and \(\sigma (t)\) are all random, they are independent from the driving Brownian motion, and they are currently observable. It is an incomplete market; it is a case of “totally unhedgeable” coefficients, according to terms from Karatzas and Shreve (1998, Chap. 6). In fact, we found that only a weakened version of Mutual Fund Theorem holds: the supremum of expected utilities can be achieved on a sequence of strategies with a certain distribution of risky assets that does not depend on utility.",4
76.0,2.0,Theory and Decision,27 March 2013,https://link.springer.com/article/10.1007/s11238-013-9365-4,Use of data on planned contributions and stated beliefs in the measurement of social preferences,February 2014,Anna Conte,M. Vittoria Levati,,Female,Unknown,Unknown,Female,"A vast amount of experimental evidence has shown that individuals contribute voluntarily to public goods, even if self-interest implies that free riding should be their dominant strategy. Several researchers explain this finding in terms of social preferences, such as altruism (Levine 1998), efficiency concerns (Charness and Rabin 2002), inequity aversion (Fehr and Schmidt 1999; Bolton and Ockenfels 2000), and conditional cooperation (Fischbacher et al. 2001). A handful of experimental studies have investigated the empirical validity of the various models of social preferences, mostly focusing on distribution games (e.g., Andreoni and Miller 2002; Charness and Rabin 2002; Cox et al. 2007). In the context of public goods games, the identification of social preferences seems to require a careful exploration not only of the decision maker’s behavior but also of his beliefs about others’ behavior. In a piece of early study, Offerman et al. (1996) use a step-level public goods game with binary contributions to provide insights into the relationship between expectations and behavior. In linear public goods games, Croson (2007) detects a significant and positive relationship between an individual’s own contribution and his beliefs about the contributions of the other members of his group. More recently, Fischbacher and Gächter (2010) and Ambrus and Pathak (2011) independently explain the decline of contributions in repeated public goods settings by combining the role played by beliefs in influencing contributions with the presence of different types of players. We depart from the aforementioned literature in that we identify people’s cooperative preferences by looking at the relationship between planned contributions and stated beliefs about the others’ actions in the following period. We consider a sequence of 15 one-shot two-person linear public goods games. In each game, subjects make two contribution decisions and specify two distributions of first-order beliefs: (a) they choose their contribution amount from a given set of 11 elements, and state their subjective probabilities that the participant they are currently matched with has opted for any element of the same set and (b) they choose from an identical discrete set the amount that they plan to contribute in the following period, and state their subjective probabilities that the participant they will be matched with then will opt for any element of the set. Our intuition is that, if other-regarding preferences are idiosyncratic and beliefs play a role in contribution determination, then a person should reveal his preferences not only in the way his contribution relates to his current beliefs but also in the way his planned contribution relates to his beliefs about the others contributions one-period-ahead. A major contribution of this article is to verify this intuition and thus to test whether plans convey accurate information about preferences and behavior. It is surprising that, apart from a few studies on individual decision making (e.g., Bone et al. 2003; Hey 2005; Bone et al. 2009), economists tend to ignore data on plans. The typical claim is that people cannot plan ahead, with the empirical divergence between plans and eventual behavior being interpreted as evidence of how poor predictors individuals are of their future acts. However, as Manski (1990, p. 934) points out, such a conclusion is unwarranted: planned and eventual behavior may differ as a consequence of events occurring between the time plans are elicited and the time actions take place. As we consider public goods games in which each individual receives feedback about other’s contribution after each period, what may cause one’s own final behavior to differ from one’s own earlier plan is a revision to first-order beliefs (which in turn captures the effect of the new information acquired in the interim period between elicitations of plans and final choices). To the best of our knowledge, this article is the first to condition on beliefs to assess whether the preference types estimated from plans are equal to those estimated from final contributions. We believe this issue to be important because intentions to contribute (mostly gathered through “consequential” survey questions)Footnote 1 are frequently used by businesses and governments to determine which product to offer and which policy to adopt. In addition, purchase intentions are routinely used in marketing research to predict whether or not consumers will purchase products (see Morwitz 1997; and references therein). As noticed by Hey (2005, p. 122), “central to effectively all economic theories of rational dynamic decision-making is the concept of a plan.” A public goods game lends itself to planning more naturally than other games because people are often asked to state how much they plan to contribute to, e.g., renewable energy sources, health, parks, or infrastructure before they specify their actual contributions. For instance, a government that wants to equip all houses with solar panels may ask people how much they plan to contribute to such energy production technology prior to starting the project; only if the reported planned contributions are substantial, the project is started and people are asked to actually contribute. Our experiment allows us to elicit, in a controlled laboratory environment, planned and final contributions to public goods, and to investigate whether and to what extent people’s preferences when planning differ from their preferences when finally contributing. The remainder of the article is organized as follows. After introducing the basic games, Sect. 2 details our experimental treatments and procedures. Section 3 describes the data and reports preliminary statistical tests. In Sect. 4, we define the mixture model, and in Sect. 5, we present and discuss the estimates of the model. Section 6 summarizes our central findings and concludes.",8
76.0,2.0,Theory and Decision,29 March 2013,https://link.springer.com/article/10.1007/s11238-013-9364-5,Entitlement and the efficiency-equality trade-off: an experimental study,February 2014,Agnes Bäker,Werner Güth,Manfred Stadler,Female,Male,Male,Mix,,
76.0,2.0,Theory and Decision,11 May 2013,https://link.springer.com/article/10.1007/s11238-013-9371-6,Motivation and mission in the public sector: evidence from the World Values Survey,February 2014,Edd Cowley,Sarah Smith,,Male,Female,Unknown,Mix,,
76.0,2.0,Theory and Decision,12 May 2013,https://link.springer.com/article/10.1007/s11238-013-9366-3,Stronger utility,February 2014,Pavlo R. Blavatskyy,,,Male,Unknown,Unknown,Male,"Set \(X\) is a non-empty set of outcomes (consequences) that is totally ordered under a preference relation \(\succsim \). Set \(X\) is not necessarily a subset of the Euclidian space \({\mathbb{R }}^{n}\). Lottery \(L\):\(X \rightarrow \) [0,1] is a probability distribution on set \(X\), i.e., \(L(x)\in \) [0,1] for all \(x\in X\) and \(\sum _{x\in X}L(x) = 1\). The set of all such lotteries is denoted by \(\mathcal{{L}}\). A degenerate lottery that yields one outcome \(x \in X\) with probability one is denoted by \(x\). Notation \(L\alpha L^{\prime }\) denotes a probabilistic mixture that yields outcome \(x \in X\) with probability \(\alpha \cdot L(x) +(1-\alpha ) \cdot L^{\prime }(x), \alpha \in \) [0,1]. For any lottery \(L\in \mathcal{{L}}\), cumulative distribution function \(F_{L}(x)\) is defined as Similarly, for any \(L\in \mathcal{{L}}\), decumulative distribution function \(G_{L}(x)\) is defined as For any \(L, L^{\prime }\in \mathcal{{L}}\), lottery \(L\vee L^{\prime }\) yields outcome \(x\in X\) with a probability Lottery \(L\vee L^{\prime }\) is the least upper bound on lotteries \(L\) and \(L^{\prime }\) in terms of first-order stochastic dominance. Lottery \(L\vee L^{\prime }\) stochastically dominates both \(L\) and \(L^{\prime }\), and there is no other lottery that stochastically dominates both \(L\) and \(L^{\prime }\) but that is stochastically dominated by \(L\vee L^{\prime }\). For any \(L, L^{\prime }\in \mathcal{{L}}\), lottery \(L\wedge L^{\prime }\) yields outcome \(x \in X\) with a probability Lottery \(L \wedge L^{\prime }\) is the greatest lower bound on lotteries \(L\) and \(L^{\prime }\) in terms of first-order stochastic dominance. Both \(L\) and \(L^{\prime }\) stochastically dominate lottery \(L \wedge L^{\prime }\), and there is no other lottery that is stochastically dominated by both \(L\) and \(L^{\prime }\) but that stochastically dominates \(L \wedge L^{\prime }\).",21
76.0,2.0,Theory and Decision,14 May 2013,https://link.springer.com/article/10.1007/s11238-013-9374-3,Decreasing higher-order absolute risk aversion and higher-degree stochastic dominance,February 2014,Michel Denuit,Liqun Liu,,Male,Unknown,Unknown,Male,"
Fishburn and Vickson (1978) showed that for random alternatives with an equal mean, 3rd-degree stochastic dominance expresses the common preference by all the decision makers with decreasing absolute risk aversion ( DARA, in short ) in the expected utility setting, i.e.,3rd-degree stochastic dominance and DARA stochastic dominance represent equivalent rules. Precisely, for two random variables \(X\) and \(Y\) such that \(E [ X ] = E [ Y ] \), \(E [ u ( X ) ] \le E [ u ( Y ) ] \) for every utility function \(u ( \cdot ) \) such that the first derivative \(u^{\prime }\), the second derivative \(u^{\prime \prime }\) and the third derivative \(u^{\prime \prime \prime }\) fulfill \(u^{\prime } ( x ) > 0\), \(u^{\prime \prime } ( x ) < 0\) and \(u^{\prime \prime \prime } ( x ) > 0\) for all \(x\) if, and only if, \(E [ u ( X ) ] \le E [ u ( Y ) ] \) for every utility function \(u ( \cdot ) \) such that \( u^{\prime } ( x ) > 0\), \(u^{\prime \prime } ( x ) < 0\) and \(A_u^{\prime } ( x ) \le 0\) for all \(x\), where \(A_u ( x ) = -u^{\prime \prime } ( x ) /u^{\prime } ( x ) \) is the classical Arrow–Prattabsolute risk aversion measure for \(u ( \cdot ) \). It is well-knownthat, assuming \(u^{\prime } > 0\), \( A_u^{\prime } \le 0\) is sufficient but not necessary for \(u^{\prime \prime \prime } > 0\). So 3rd-degree stochastic dominance implies DARA stochastic dominance but not vice versa. That DARA stochastic dominance also implies 3rd-degree stochastic dominance when they are applied to equal-mean random alternatives is because for every utility function \(u ( \cdot ) \) such that \(u^{\prime \prime \prime } > 0\), one can define an auxiliary utility function \(v ( \cdot ) \) as \(v ( x ) = u ( x ) + kx\) for some non-negative real constant \(k\). And one can easily see that \(v ( \cdot ) \) ranks any two equal-mean random variables the same way as \(u ( \cdot ) \), and \(A_v^{\prime } ( x ) \le 0\) when \(k\) is sufficiently large. Therefore, if \(Y\) does not dominate \(X\) by 3rd-degree stochastic dominance, then \(Y\) would not dominate \(X\) by DARA stochastic dominance either.Footnote 1
 This paper proves a result that simultaneously generalizes the Fishburn and Vickson theorem in two directions. First, maintaining the equal-mean condition, the equivalence between higher-degree stochastic dominances and their corresponding higher-degree DARA stochastic dominances is established. Second, the implications of a more general restriction on random alternatives—that they have equal first \(m\) moments, where \(m \ge 1\)—are also explored. Under these moment conditions, higher-degree stochastic dominance rules can be equivalently characterized using risk apportionment notions as in Eeckhoudt and Schlesinger (2006) or imposing that higher-degree coefficients of absolute risk aversion decrease with the initial wealth level. Higher-degree stochastic dominance rules are interesting because the higher the degree a stochastic dominance rule is of, the more random alternatives it can be used to rank. Moreover, the assumption of equal mean or equal first few moments does not seem to be overly restrictive in many situations. For example, the assumption of the existence of actuarially fair insurance ensures that insurance buyers’ choice is over random alternatives with an equal mean. And decisions involving third- or even higher-degree risk increases require comparing random alternatives with equal first two or more moments.Footnote 2
 In the sense that it sheds some light on the relationship between preference conditions \(u^{\prime \prime \prime } > 0\) and \(A_u^{\prime }(x) \le 0\), this paper is related to studies giving economic interpretations to these conditions and their higher-degree counterparts. Pratt’s (1964) famous justification of \(A_u(\cdot )\) as a local intensity measure of risk aversion makes \(A_u^{\prime } \le 0\) a condition of risk aversion decreasing in initial wealth. On the other hand, Eeckhoudt and Schlesinger (2009) revived an interpretation of \(u^{\prime \prime \prime } > 0\), i.e., of prudence or 3rd-order risk apportionment, as the pain from a (2nd-degree) risk increase decreasing in wealth. Similarly, Eeckhoudt et al. (2009) and Denuit and Rey (2010) provided interpretations of risk apportionment of order \(n+1\) as decreasing sensitivity to a detrimental \(n\)th-degree risk increase in the sense of Ekern (1980). The remainder of this paper is organized as follows. Section 2 relates DARA of higher degree to risk apportionment. Section 3 recalls stochastic dominance rules, paying particular attention to the cases where the first few moments of the random variables to be compared coincide. Section 4 contains the main results, establishing that higher-order stochastic dominance rules and common preference by all decision makers with decreasing higher-order absolute risk aversion coincide under appropriate constraints on the respective moments of the random variables to be compared. Several particular cases often encountered in the literature are discussed. All the random variables considered in this paper are valued in some interval \([a,b]\) of the real line. We denote as \(u^{\prime }\), \(u^{\prime \prime }\), and \(u^{\prime \prime \prime }\) the first derivative, the second derivative, and the third derivative of the utility function \(u\). More generally, we write \(u^{(n)}\) for the \(n\)th derivative of \(u\), \(n=1,2,3,4,\ldots \); the notations \(u^{\prime }\), \(u^{\prime \prime }\), and \(u^{\prime \prime \prime }\) and \(u^{(1)}\), \(u^{(2)}\), and \(u^{(3)}\), respectively, can be used interchangeably.",
76.0,3.0,Theory and Decision,17 May 2013,https://link.springer.com/article/10.1007/s11238-013-9375-2,If nudge cannot be applied: a litmus test of the readers’ stance on paternalism,March 2014,Chen Li,Zhihua Li,Peter P. Wakker,,Unknown,Male,Mix,,
76.0,3.0,Theory and Decision,23 March 2013,https://link.springer.com/article/10.1007/s11238-013-9362-7,On the significance of the prior of a correct decision in committees,March 2014,Ruth Ben-Yashar,Shmuel Nitzan,,Female,Male,Unknown,Mix,,
76.0,3.0,Theory and Decision,11 May 2013,https://link.springer.com/article/10.1007/s11238-013-9373-4,A geometric approach to revealed preference via Hamiltonian cycles,March 2014,Jan Heufer,,,Male,Unknown,Unknown,Male,"This article shows that within the standard framework of demand theory a revealed preference cycle of a certain length that does not contain cycles of shorter length must admit a Hamiltonian cycle on the convex monotonic hull of all commodity bundles involved in the cycle. Conversely, if the convex hull of a set of bundles does admit a Hamiltonian cycle, then there exist corresponding budget sets such that these bundles form a preference cycle. Hamiltonian cycles on convex hulls in the commodity space thus characterise revealed preference cycles, and therefore also violations of utility maximising demand behaviour. This result can be used to answer old questions in a new, geometric, intuitive way. It also highlights a connection between David Gale’s work on mathematics and revealed preference. Based on the work by, among many others, Afriat (1967) and Varian (1982, 1983), revealed preference has become operational and offers useful tools for the analysis of consumption data, which is also exemplified by the growing trend to collect consumption data in experimental settings.Footnote 1 These data can now be easily tested for consistency with revealed preference axioms, and thereby also for consistency with utility maximisation and other hypotheses. But for quite some time it had been an open question in economic theory whether the weak axiom of revealed preference (WARP) as introduced by Samuelson (1938) was actually sufficient to guarantee that a demand function maximises a utility function. Houthakker (1950) defined an apparently stronger condition, the strong axiom of revealed preference (SARP) and showed that this condition was indeed sufficient. 
Arrow (1959), however, remarked that there was still no proof ‘that the Weak Axiom is not sufficient to ensure the desired result. The question is still open’. Uzawa (1959) showed that the Weak Axiom combined with certain regularity conditions implies the Strong Axiom.Footnote 2 Meanwhile, Rose (1958) showed that the Weak Axiom implies the Strong Axiom for two commodities, extending a geometrical argument by Hicks (1965[1956], pp. 52–54). Finally, Gale (1960) constructed a counterexample for the case of three commodities: WARP was satisfied, SARP was violated. This, essentially, settled the question. Kihlstrom et al. (1976) provided a theoretical argument which yields an infinite number of demand functions that satisfy WARP but not SARP. Peters and Wakker (1994, 1996) showed how to embed Gale’s example in higher dimensions without relying on isomorphic extensions, that is, with strictly positive demand for every commodity for suitable budgets. John (1997) showed that there is a simpler proof of their results. 
Samuelson (1953) raised the question whether the exclusion of cycles of a certain length would be sufficient to imply SARP. Even if WARP does not generally imply SARP, the exclusion of cycles of length \(K\) could exclude the possibility of cycles of length greater than \(K\). This question was answered by Shafer (1977) who showed that, in the three dimensional case, for every positive integer \(K\), there exists a demand function which violates SARP, but has no cycles involving \(k\) or fewer observations. For \(K = 2\) this also proves that WARP does not imply SARP. Shafer’s result was also extended into more than three dimensions by Peters and Wakker (1994) and John (1997). In a recent paper, Deb and Pai (2012) examine the set of preferences than can be observed, depending on the dimension of the commodity space. They find that if the dimension of the commodity space is high enough relative to the number of observations, any revealed preference relation can arise. This paper develops a new geometric and intuitive approach to show that with more than two commodities there can be preference cycles of arbitrary finite length whereas for two commodities cycles can only be of length \(2\). From this it immediately follows (i) that WARP necessarily implies SARP for two commodities, (ii) that there exist finite sets of observations which satisfy WARP but not SARP. The approach here is an alternative to the proofs of Rose, Gale, Peters and Wakker insofar as it gives a condition which is both necessary and sufficient for the existence of cycles of length greater than two. It is then shown that the condition cannot be satisfied in two dimensions, whereas in more than two dimensions it can be satisfied. The proof technique is intuitively appealing: the condition states that for a set of \(K\) bundles one can find a set of \(K\) budget sets on which these bundles are chosen such that these observations form a preference cycle of irreducible length \(K\) if and only if the convex monotonic hull of these bundles admits a Hamiltonian cycle involving all \(K\) bundles. Thus, it admits an understanding by giving a geometric interpretation of preference cycles. Finite but otherwise arbitrarily long preference cycles in \(L > 2\) dimensions can then be constructed by embedding a cyclic \((L-1)\)-polytope into an \((L-1)\)-face of a convex monotonic hull in \(L\)-space. Cyclic polytopes always admit Hamiltonian cycles and have been described and analysed by Gale (1963) 3 years after he provided his famous example of a preference cycle in three dimensions. To the best of the author’s knowledge, Gale himself never mentioned this connection in his works.",4
76.0,3.0,Theory and Decision,09 May 2013,https://link.springer.com/article/10.1007/s11238-013-9372-5,Games with a local permission structure: separation of authority and value generation,March 2014,René van den Brink,Chris Dietz,,Male,,Unknown,Mix,,
76.0,3.0,Theory and Decision,08 May 2013,https://link.springer.com/article/10.1007/s11238-013-9370-7,Delegation and motivation,March 2014,Lukas Angst,Karol Jan Borowiecki,,Male,Male,Unknown,Male,"The idea of fostering motivation of the employees through empowerment is well established in management theory. It brings up the question of centralized decision-making versus delegation. The typical focus has been on a trade-off between two effects coming along with delegation: on one hand, delegation leads to a better utilization of information distributed across the lower levels of the firm’s hierarchy; on the other hand, it induces a loss of control for the upper-level managers. Despite the numerous implications identified in previous research, it is not entirely clear what role does motivation play. In particular, how is the motivation of an agent affected if he gets the decision rights assigned? This study is based on a delegation experiment that was conducted in order to shed light on changes in the motivation of individuals who experience choice over some aspects of a task. In a laboratory setting motives and other effort promoting elements were explored. The idea of the experimental design was to investigate effects of delegation by sticking closely to the base principal–agent model provided by Aghion and Tirole (1997). The subjects have the task to screen among three projects and determine the best of them. To learn the profits of all possible projects, both parties have to decide for an effort which equates to the probability of becoming completely informed (referred to as Searchintensity). By setting the parameters in a manner that an agent is even worse off in case of delegation in terms of expected payoff, a higher Searchintensity must arise due to the delegation act itself. Since there are no additional extrinsic incentives for the agents in case of delegation, a higher effort provided must be intrinsically motivated. Therefore, the benefits of the choice of the Searchintensity, as the central variable of this design, are twofold.Footnote 1 First, as it measures the offered stake to find a good project it allows shedding light on the absolute extent of intrinsic motivation. Second, estimating the sources of motivation, in particular the way Searchintensity is affected by other variables like responsibility or beliefs, becomes possible. The results indicate that in this kind of setting the agents actually do not favour a delegation and a decision is considered rather burdensome. The study uncovers further interesting ways of interaction between delegation motives, effort motivators, goals and other perceptions of the agents. The decision to transfer the decision rights is in accordance with the standard prediction applied. This proposition is also supported by the fact that 48 % of the principals actually delegated. The findings further indicate that principals take primarily the giftbelief into account, whereas agents consider their own perceived friendliness to determine the desirabilities of both situations. The hypothesis of a higher average Searchintensity provided by the agents than by the principals must be rejected. Neither higher goal attainment nor significantly stronger feelings of responsibility can be detected. Agents prefer unambiguously to be subordinate, rather than make a costly and apparently demanding decision. Although they do not appreciate a delegation, they accept a potential delegation decision and set goals upon which they make their effort contingent. This is in accordance to Locke and Latham’s (1990) goal-setting theory, which asserts that task performance is directly regulated by the conscious goals that individuals are going for on the task. The goal attainment on its side depends mainly on the assumed delegation motives and on the responsibility perceptions of the agents. Other than among principals, where the felt responsibility neither affects effort nor goal attainment, responsibility considerations affect at least goal attainment amongst agents. The suggested delegation motives seem to be important for the agents in their decision process. They associate the delegation motive ‘confidence in agent’s effort’ with positive feelings and cope with it by admitting responsibility, setting high goals and finally choosing a higher Searchintensity. With reference to the economic literature that assigns a signalling value to delegation, the positive character given to this delegation motive by the agents supports this valuation. The high rating of ‘relief’ and its correlation with the wish of centralization represents another interesting argument in the perception of a delegation, indicating that they do not necessarily favour a delegation and the necessity to take over decision is considered rather burdensome. Finally, the principals expect their agents to be more receptive for a delegation than they are in effect, and the agents in contrast assume their principals to associate friendlier intentions to their delegation decision than they actually do. Although the experiment could not give support for the behavioural hypothesis of higher effort provided by participants who receive choice subsequently, it gives interesting indication of how a delegation is perceived across individuals and the consequences these perceptions lead to. The costs and benefits affecting the delegation decision have been widely studied. In analysing the interaction between the two parties Dessein (2002) derives that under centralization the agent will always anticipate the discerning attitude of his principal and his information will be strategic towards the principal. Aghion and Tirole (1997) accordingly state that centralization may jeopardize communication between the agent and the principal when preferences are not sufficiently aligned. In reviewing capital allocation decisions Marino and Matsusaka (2005) found that under delegation the agent (assumed to be an empire builder) has room to overspend. Even when the principal keeps a hand in the decision, like it is assumed by Baker et al. (1999), the agent may distort his proposal to make the project look better than it is, resulting in an inefficiently large capital allocation and inefficient communication. On the other hand, there are substantial arguments for positive impacts of delegation. Bénabou and Tirole (2003) argue that through a delegation the principal demonstrates her confidence in the agent’s ability, and, therefore, higher efforts of the agent can be expected. The inherent signalling value of delegation is studied also by Swank and Visser (2006), who suggest that a principal can use delegation as a convincing communication device to signal his beliefs about the abilities of an agent. Bénabou and Tirole (2002) introduce a model which determines the valuation of self-confidence and demonstrate how it can influence the decision-making of individuals and eventually improve the welfare. Crémer (1995) argued that in principal–agent problems a credible commitment of a principal to not acquire information about the agent will strengthen his incentives for a positive result, overwhelming so the gains could be made from better information acquired by the principal. Nonetheless, not only the contingent additional information must be considered in the delegation decision, but also the fact that the agent works on his own idea and may be more optimistic about the possibility of success (Zabojnik 2002). A delegation again is the most credible commitment of the principal, and this sign of confidence might have even more positive consequences. The remainder of this article is organized as follows. The next section describes the experimental design. The third section provides a discussion of the behavioural preditions. In the fourth section the empirical findings are presented and discussed, and in the last section, concluding remarks are provided.",1
76.0,3.0,Theory and Decision,06 June 2013,https://link.springer.com/article/10.1007/s11238-013-9380-5,Costly and discrete communication: an experimental investigation,March 2014,Sean Duffy,Tyson Hartwig,John Smith,Male,Male,Male,Male,"The properties of words are very different from the properties of real numbers. For instance, it is not the case that there exists a word with a meaning between any two words. However, words are used to construct statements which convey information about a complex and nuanced reality. One can use words to express more detailed and nuanced information, but only at a cost to the sender. It is our view that language is an imperfect and coarse means of communicating information about a complex and nuanced world. We report on an experiment designed to capture this feature of communication. In our experiment, the language available to the sender imperfectly describes the state of the world. By this, we mean that the sender cannot fully and costlessly communicate. However, the sender can improve communication, at a cost, by increasing the complexity or elaborateness of the message. By way of example, suppose that your advisee has been invited to present at a conference. Your preferences and the preferences of your advisee are identical with regards to her performance at the conference: to sound competent, to receive helpful comments, etc. In order to facilitate this success, you wish to provide her with information about how to best have a successful conference. However, there is not a single word to convey the full extent of your knowledge regarding how best to present, how best to prepare the slides, how best to respond to potential questions, etc. You can increase the amount of information conveyed only by constructing additional statements. As a result, you are unlikely to communicate all of the relevant information. Further, the amount of information which you provide will be related to the costs which you bear in the construction of the statements. 
Hertel and Smith (2012) provide a theoretical account of such communication by adapting the uniform-quadratic version of Crawford and Sobel (1982) so that messages available to the sender are constrained to be costly and discrete. Although there are many equilibria, the authors employ an out-of-equilibrium condition which identifies the equilibria with the largest possible number of transmitted messages, which we refer to as most informative. The paper makes the prediction that more costly signals will be conserved (sent on smaller regions of the state space) and that the size of the language used will emerge in equilibrium as a function of the costs of communication. We design an experiment in order to investigate communication in the Hertel and Smith setting. The questions are then, how does the most informative equilibrium identified by Hertel and Smith (2012) perform in the laboratory, and, as predicted, does the size of the language emerge as a function of the communication costs. In this experiment, the subjects are anonymously divided into pairs, one as a sender and one as a receiver. The sender learns the state of the world then sends a message to the receiver. The receiver observes the message and selects an action which affects the payoffs of both players. The incentives of the players are aligned in the sense that both sender and receiver are paid an amount which is increasing in the accuracy of the receiver’s action. In our experiment, messages imperfectly describe the underlying state space. Specifically, due to the constraints of the message space, the sender is not able to fully and costlessly communicate. However, the sender is able to transmit more information by constructing an elaborate, but costly, message. Here the state space is an integer between \(-3\) and 3. The sender can send a costless message, which we refer to as the “Empty” message.Footnote 1 Additionally, the sender can compose a costly message consisting of two possible elements “High” and “Low.” These message elements would seem to provide a natural ordering given our state space. The cost of a message is then a function of the number of elements in the message. Therefore, the empty message can be transmitted at a cost of 0; the messages “High” and “Low” can be transmitted at a cost of \(c\); and the messages “High High,” “High Low,” “Low High,” and “Low Low” can be transmitted at a cost of \(2c\), where we vary \(c\). We find that the size of the language emerges endogenously as a function of the costs of communication. On the other hand, we find that the equilibrium predictions do not perform well. However, our experimental observations differ from the theoretical predictions in a manner consistent with other experimental communication papers: the senders overcommunicate. Previous experimental communication papers have found that senders often communicate more information than that which is in their best interest. In the context of cheap talk experiments, where senders and receivers do not have aligned preferences over the action of the receiver, overcommunication implies that the sender’s message is suboptimally precise in that it allows the receiver to select an action which is closer to the receiver’s preferred action than the action which the sender prefers. In other words, since the sender and receiver have different preferences over the action, overcommunication implies that the receiver selects an action which is suboptimally far from the action preferred by the sender. In our setting, where preferences over the action are aligned but sending messages is potentially costly, overcommunication implies that senders convey information which is suboptimally precise and, as a result, they incur excessive communication costs. Overcommunication in our setting implies that senders would be better off by communicating with a lower precision and, therefore, incurring smaller communication costs. In our setting, one way in which overcommunication could be detected would be the observation that senders experience worse outcomes as communication costs increase and the receivers experience better outcomes as communication costs increase. This would suggest that senders are not sufficiently sensitive to the costs of communication, and the receivers are benefiting from this behavior. We find that the sender’s payoffs, relative to the equilibrium payoffs, are decreasing in the cost of communication. We also find that the receiver’s payoffs, relative to the equilibrium payoffs, are increasing in the cost of communication. Finally, we find imperfections in coordination on the basis of the experimental labels. We find that subjects are better able to coordinate on some states rather than others, despite that there does not exist an a priori reason to expect such differences.",5
76.0,3.0,Theory and Decision,10 July 2013,https://link.springer.com/article/10.1007/s11238-013-9388-x,Hold or roll: reaching the goal in jeopardy race games,March 2014,Darryl A. Seale,William E. Stein,Amnon Rapoport,Male,Male,Male,Male,"Contest theory (Konrad 2009) examines multiple types of competitions that evolve over time between two or more contestants, who expend efforts or accumulate resources in order to get ahead of their rival. These competitions differ from one another on multiple dimensions including the number of players, their objectives, information about their rivals that the players gain and repeatedly update during the contest, and the rules that govern their interaction. Our interest here is in a class of dynamic tournaments in which the objective of each contestant is simply to win, rather than the more common objectives of maximizing expected utility, minimizing expected rank, or maximizing point difference (see, e.g., the tournament conducted by Axelrod 1984). Within this class, we focus on tournaments that combine skill and chance, allow for player roles to alternate over the course of the contest, and provide each contestant complete information about the relative positions of her rivals during the course of the competition. In some of these tournaments, the change in player roles during the game is determined exogenously. For example, the outcome of each play in Backgammon is determined probabilistically by simultaneously rolling two dice, whereas the change of player roles is fixed in advance by the rules of the game. In other tournaments of this type (e.g., military conflicts, Blackjack), the change in player roles is determined endogenously. In deciding whether to maintain her player role or cede it to her rival, each player considers her relative position in the tournament, the relative position of the other player(s), and how far all the players are from reaching a pre-specified target. She often does so by comparing two alternative courses of action. The first is a riskless course of action where the player cedes the initiative to her rival in order to consolidate her own position and transfer the risk associated with the stochastic nature of the contest to her rival. The second is a risky course of action where the player attempts to improve her position relative to the positions of her rivals while at the same time exposing herself to the possibility of a temporary setback. These two courses of action are often manifested in military conflicts where opposing armies alternate between pursuing an attack, which may or may not be successful, and regrouping to consolidate their forces, evaluate new information, and plan ahead their next move. R&D competitions to secure a contract often have a similar structure where each firm alternates between trying to get ahead of its rivals—an option that carries a risk of temporary setback—or consolidating its position, updating information about its rivals, and planning a new line of research. Our study aims to examine the balance that contestants must maintain between these two courses of action. Compared to the equilibrium solution—which we construct and then use as a benchmark of our analysis—do agents choose the risky option and thereby “push their luck” more often than they should, or do they err on the side of caution by stopping to pursue the risky course of action too early? In order to answer this question, we study a tournament that presents the participants with very simple rules of play and at the same time a non-trivial computational solution. Our study focuses on a jeopardy race game (JRG) that was first described in print by Scarne (1945), and later commercialized and used by mid-level mathematics teachers to illustrate probability concepts (Neller and Presser 2004). Variants of this game and methods of their solution have recently been presented and illustrated by Tijms (2012) in a brief and elegant introduction to stochastic games and dynamic programming. The basic rules of the JRG are quite simple. There are two players who start the tournament with commonly known numbers of points which may or may not be equal. On each turn, one of the players either rolls a die until she scores “1” or holds and scores the sum of the rolls (called the turn total). At any time during a player’s turn, she is faced with two options: 
roll If the player rolls the number 1: she scores nothing in this turn, and it becomes the next player’s turn; 2–6: this number is added to the player’s turn total and the player’s turn continues. 
hold The turn total is added to the player’s game score, and play is ceded to the player’s rival. The first player to score \(R\) or more points wins the contest. The value of \(R\) is predetermined and commonly known. We view this version of the JRG as a stylized model of a contest between two players that evolves over time. Hold is a riskless option of maintaining the resources she already has accumulated and ceding the initiative to her rival. Roll is a bolder option of taking the initiative to move ahead in achieving her goal by accumulating more points; it is up to the player who decides to roll how long to pursue this more aggressive course of action. The latter option comes with a risk of suffering a temporary setback. For an experimental implementation, the JRG has several advantages. It is one of the simplest games for subjects to understand, providing (i) a familiar task environment with binary decision choice on each turn, (ii) accurate and timely feedback on subjects’ progress in their competition to be the first to achieve a commonly known goal (“Winning isn’t everything; it’s the only thing”), and (iii) an equilibrium solution that serves as a benchmark for studying observed decision behavior. The JRG shares elements with other stylized models of dynamic decision making in a stochastic environment like variants of the one- and two-armed Bandit problems (Berry and Fristedt 1985) or the BART model that have been studied experimentally (e.g., Biele et al. 2009; Lejuez et al. 2002). It differs from these models because of the strategic element that is present in contests and other interactive decision making tasks but is absent from individual decision making tasks under uncertainty. Our paper extends the JRG from \(N=2\) to \(N>2\) players, and presents an algorithm for computing the equilibrium solution. Section 2 examines the relevant literature. Section 3 introduces the 2-player JRG, describes the equilibrium solution, outlines the decision method for conducting the experiment, and presents the major findings. A novel 3-player JRG is introduced in Sect. 4. We construct and illustrate the equilibrium solution to this new game, and then compare observed to predicted decision behavior in a second 3-player JRG experiment. Section 5 describes yet another version of the JRG, called the strategy method, where players must commit in advance to the number of dice rolled on each turn. Section 6 compares results across experiments, showing that the strategy method mitigates much of the conservative play observed under the decision method.",
76.0,4.0,Theory and Decision,31 May 2013,https://link.springer.com/article/10.1007/s11238-013-9379-y,Helping patients and physicians reach individualized medical decisions: theory and application to prenatal diagnostic testing,April 2014,Edi Karni,Moshe Leshno,Sivan Rapaport,Male,Male,Female,Mix,,
76.0,4.0,Theory and Decision,31 May 2013,https://link.springer.com/article/10.1007/s11238-013-9381-4,Ellsberg games,April 2014,Frank Riedel,Linda Sass,,Male,Female,Unknown,Mix,,
76.0,4.0,Theory and Decision,12 June 2013,https://link.springer.com/article/10.1007/s11238-013-9384-1,Independence of irrelevant alternatives revisited,April 2014,Susumu Cato,,,Male,Unknown,Unknown,Male,"
Fishburn (1987) distinguishes two classes of conditions for collective decision making. The first is a class of intraprofile conditions, which “consider one profile at a time” (Fishburn 1987, p. 15). The second is a class of interprofile conditions. Interprofile conditions require that if profiles “relate to each other in a certain way,” then collective decisions under the profiles “must relate to each other in certain ways” (Fishburn 1987, p. 15). In the Arrovian framework, the Pareto principle is an intraprofile condition and the independence of irrelevant alternatives is an interprofile condition. In this sense, Arrow’s impossibility theorem clarifies the existence of a trade-off between an intraprofile condition and an interprofile condition. In the original version, the independence of irrelevant alternatives is formulated as an axiom for social “choice functions,” not social “preferences.” This is described in Arrow’s own parlance as follows: 
Let
\(R_1,\ldots ,R_n\)
and
\(R^{\prime }_1,\ldots , R^{\prime }_n\)
be two sets of individual orderings and let
\(C(S)\)
and
\(C^{\prime }(S)\)
be the corresponding social choice functions. If, for all individuals
\(i\)
and all
\(x\)
and
\(y\)
in a given environment
\(S, xR_iy\)
if and only if
\(xR_i^{\prime }y\), then
\(C(S)\)
and
\(C^{\prime }(S)\)
are the same. (Arrow 1951, p. 27) The Arrow social welfare function maps a profile of individual preferences to a unique social preference, and thus, the axiom indirectly restricts the construction of social preference.Footnote 1
 Arrow’s independence of irrelevant alternatives has been reformulated in terms of social preferences as binary independence (May 1954; Vickrey 1960). Binary independence requires that the social ranking between two alternatives depends only on the individual rankings between the two. An implication of the independence of irrelevant alternatives is informational efficiency. That is, information of individual preferences must be efficiently used to construct social preferences under the axiom. 
Young (1995) argues for the relevance of the “informational efficiency” aspect of the independence of irrelevant alternatives in a political environment:Footnote 2
 [I]ndependence allows the electorate to make sensible decisions within a restricted range of choices without worrying about the universe of all possible choices. It is desirable to know, for example, that the relative ranking of candidates for political office would not be change if purely hypothetical candidates were included on the ballot. (Young 1995, p. 58) Moreover, any truthful social choice function must satisfy the independence of irrelevant alternatives.Footnote 3
 Many researchers have argued that the independence of irrelevant alternatives is responsible for the Arrow impossibility theorem. A main criticism is that under the axiom, the social preference is required to be constructed based on poor information with respect to individual preferences. That is, richer information is needed for aggregating preferences. Various authors formulate weakenings of the independence of irrelevant alternatives; these allow us to use more information on individual preferences. Some obtain possibility results and others obtain impossibility results (Hansson 1973; Young and Levenglick 1978; Campbell and Kelly 2000a; Fleurbaey et al. 2005a, b).Footnote 4
 Among others, Blau (1971) provides a seminal contribution in this line. He introduces \(m\)
-ary independence, which is a weakening of binary independence: an aggregation rule satisfies \(m\)-ary independence if and only if the social ranking over \(m\) alternatives depends only on the individual rankings over the \(m\) alternatives. It seems that an informational constraint of \(m\)-ary independence is substantially weaker than binary independence (especially when \(m\) is sufficiently large). As shown by Blau (1971), however, \(m\)-ary independence is equivalent to binary independence when \(m\) is smaller than the number of alternatives. In this paper, we introduce a generalized notion of the independence of irrelevant alternatives. Given an arbitrary collection \(\mathcal{S }\) of subsets of the set of alternatives, the social ranking over a menu \(S \in \mathcal{S }\) depends only on the individual rankings over the menu \(S\). The collection \(\mathcal S \) represents a treatment of information under a collective choice rule. We characterize a necessary and sufficient condition under which \(\mathcal S \)-independence is equivalent to binary independence. This condition is called the connectedness. Because \(m\)-ary independence satisfies the connectedness, Blau’s result follows from our results. The rest of this paper is organized as follows. Section 2 introduces notations and definitions. Section 3 examines binary independence and its generalizations. Section 4 considers the relevance of binary independence in the setting with infinite alternatives. Section 5 examines Arrow’s formulation of the independence of irrelevant alternatives. Section 6 discusses this paper’s relationship with the works of Campbell and Kelly (2000a, 2007).",8
76.0,4.0,Theory and Decision,31 May 2013,https://link.springer.com/article/10.1007/s11238-013-9382-3,Inequality aversion and antisocial punishment,April 2014,Christian Thöni,,,Male,Unknown,Unknown,Male,"A large strand of literature in experimental social psychology and experimental economics has demonstrated that informal sanctions can solve social dilemmas.Footnote 1 The upshot of this literature is that, when people are able to punish others dependent on their contributions, they do so in a way that free riding is no longer profitable and even selfish subjects find it worthwhile to contribute. The public goods game is then no longer a social dilemma and the groups manage to avoid the tragedy of the commons (Hardin 1968). This is very remarkable, because the punishment of free riders is per construction also plagued by free rider incentives: in the best world I would have others educate the free riders to become contributors and enjoy their contributions without engaging in costly punishment myself. Recent experimental evidence, however, puts the universality of these results into question. Gächter et al. (2005) began investigating public goods games with punishment in different cultures and found that the punishment option is hardly effective in enhancing cooperation in some of their subject pools. They presented preliminary evidence that the variation in cooperation across their subject pools is connected to the use of the punishment option. Interestingly, the differences are not in the way subjects treat free riders, but in the way they treat cooperative subjects. For punishment targeted to subjects with an equal or a higher contribution than the punisher Herrmann et al. (2008) coined the term ‘antisocial punishment’.Footnote 2 They investigate 16 culturally diverse subject pools and show that there is a clear-cut connection between the prevalence of antisocial punishment and the effectiveness of the punishment option in fostering cooperation. In subject pools where antisocial punishment is frequent subjects do not profit from the punishment option and earn lower profits than without the punishment option. Given that antisocial punishment is a major obstacle for cooperation it is important to understand the motives behind antisocial punishment. Fehr and Gächter (2000, p. 990) devote a footnote to the causes of antisocial punishment. They mention random error, improvement of the relative position (status preferences), and revenge for anticipated or past punishment. Herrmann et al. (2008) provide a more extensive account for the causes of antisocial punishment, adding ‘do-gooder derogation’ (Monin 2007), and a desire to punish non-conformists to the list. In the data they find evidence for the revenge explanation but their experiment is not designed to differentiate between various motives behind antisocial punishment. To date there is no experimental study that systematically explores the causes of antisocial punishment. This paper brings an additional, maybe surprising reason for antisocial punishment into the discussion. In the next section I use the model of Fehr and Schmidt (1999) to show that inequality aversion predicts antisocial punishment in many cases. Consider a situation where (i) cooperative players are faced with a free rider and (ii) not all cooperators are willing to punish the free rider. Inequality aversion predicts that those who punish do not only punish the free rider, but also the cooperative players who do not punish. This happens for purely material reason: it ensures that the punishing players do not fall behind the players who free ride on their punishment expenditures. Suspecting inequality aversion as a motivation for punishment acts is quite natural. For punishment of free riders Fehr and Fischbacher (2004) and Fowler et al. (2005)Footnote 3 provide evidence that egalitarian motives drive punishment decisions. In Sect. 3, I use data from three experimental studies on public goods games with punishment to investigate whether the same is true for antisocial punishment. The answer is no—a majority of antisocial punishment acts occur in situations which do not meet the conditions for antisocial punishment as explained by inequality aversion.",17
76.0,4.0,Theory and Decision,08 June 2013,https://link.springer.com/article/10.1007/s11238-013-9385-0,Fair division of indivisible items between two players: design parameters for Contested Pile methods,April 2014,Rudolf  Vetschera,D. Marc Kilgour,,Male,Unknown,Unknown,Male,"The existence of fair divisions, and algorithms to find them if they do, are topics of interest in a range of disciplines including mathematics, economics, computer science, and operations research. (For general references on fair division, see Brams and Taylor 1996; Brams 2006; Klamler 2010; Brams et al. 2012). A fair division problem has two essential components: whatever is to be shared, and two or more agents, whom we call “players”. The object(s) to be shared might consist of one divisible item, or of a set of divisible or indivisible items. Players have individual preferences over the allocations they might receive. A fair division procedure is a set of rules for the players to follow plus a method to convert their actions into an allocation that satisfies some fairness and efficiency properties, as long as the players have followed the rules sincerely. Among the many criteria that might be applied to the final partition are envy-freeness, Pareto-optimality, and (if comparable cardinal measures of value are available) max–minimization, or maximization of the utility of the worse-off player. This work concerns the fair allocation of indivisible items to two players. As examples, consider how two children can share a pile of hard candies, or how two political parties can allocate cabinet seats to which they are jointly entitled. We always assume that the players have equal entitlements and that there is no numéraire, i.e., that no compensation is possible. The outcome of a procedure can be considered to be the subset of items allocated to Player One, with the complement understood to be assigned to Player Two. Many procedures that can be applied to this problem fulfill some minimal fairness criteria; for example, each item can be assigned to a randomly chosen player. This article deals with Contested Pile procedures for the fair division of a finite set of indivisible items between two agents with different preferences (Brams et al. 2012; Vetschera and Kilgour 2013). A Contested Pile procedure consists of a Generation Phase followed by a Splitting Phase. In the Generation Phase, each player receives a number of items. If the players act sincerely, these items are allocated to the player who values them more. Items allocated during the Generation Phase remain with the player who receives them and cannot be reallocated. Usually, however, not all items are allocated in the Generation Phase; the unallocated items, which tend to be similarly valued by the players, constitute the Contested Pile. In the Splitting Phase, the players receive complementary subsets of the Contested Pile. In general, therefore, a player receives some items in both phases. Because a Contested Pile procedure requires a procedure for each of two phases, there are many possibilities. We believe that our project is the first comprehensive study of how this design tends to affect the allocation. We compare different design variants of the Generation Phase and the Splitting Phase in a simulation study that incorporates criteria such as fairness, efficiency, and robustness against strategic behavior. Our results can guide designers of fair division procedures in their search for solutions that are adequate in specific contexts. A particular focus of the present study is a procedure for splitting a Contested Pile, the Undercut procedure, recently introduced by Brams et al. (2012). By taking this explicit design perspective, the present study complements an earlier study (Vetschera and Kilgour 2013) in which we focused on the Generation Phase and analyzed the effects of strategic behavior by players on one particular design. Our methodology rests on a general computational model of Contested Pile methods. Results obtained with computational models always depend on the specific parameter settings used. We aim to achieve robust results by considering a wide range of settings. Furthermore, we performed a large number of replications to cancel out chance effects arising from the randomly generated problem data. The remainder of the paper is structured as follows. Section 2 presents the theoretical basis of Contested Pile methods, discusses their design parameters, and introduces the Undercut procedure in more detail. Section 3 then describes our research hypotheses and the computational model used to study them. Section 4 presents the results of our computational experiments, and Sect. 5 concludes by discussing our results and providing an outlook on future research.",7
76.0,4.0,Theory and Decision,28 June 2013,https://link.springer.com/article/10.1007/s11238-013-9386-z,On fairness of equilibria in economies with differential information,April 2014,Achille Basile,Maria Gabriella Graziano,Marialaura Pesce,Male,Female,Unknown,Mix,,
77.0,1.0,Theory and Decision,02 August 2013,https://link.springer.com/article/10.1007/s11238-013-9390-3,The bipolar Choquet integral representation,June 2014,Salvatore Greco,Fabio Rindone,,Male,Male,Unknown,Male,"Cumulative Prospect Theory (CPT) (Tversky and Kahneman 1992) is the modern version of Prospect Theory (PT) (Kahneman and Tversky 1979) and it is nowadays considered a valid alternative to the classical Expected Utility Theory (EUT) of Von Neumann and Morgenstern (1944). CPT has generalized EUT, preserving the descriptive power of the original PT and capturing the fundamental idea of Rank Dependent Utility (RDU) (Quiggin 1982) and of Choquet Expected Utility (CEU) (Schmeidler 1986, 1989; Gilboa 1987). In recent years CPT has obtained increasing space in applications in several fields: in business, finance, law, medicine, and political science (e.g., Benartzi and Thaler 1995; Barberis et al. 2001; Camerer 2000; Jolls et al. 1998; McNeil et al. 1982; Quattrone and Tversky 1988). Despite the increasing interest in CPT—in the theory and in the practice—some critiques have been recently proposed: Levy and Levy (2002), Blavatskyy (2005), Birnbaum (2005), Baltussen et al. (2006), Birnbaum and Bahra (2007), Wu and Markle (2008), Schade et al. (2010). In our opinion, the most relevant of these critique concerns the Gain-Loss Separability (GLS), i.e., the separate evaluation of losses and gains. More precisely, let \(P=(x_1,p_1;\ldots ;x_n,p_n)\) be a prospect giving the outcome \(x_i\in \mathrm{I }\!\mathrm{R }\) with probability \(p_i,\ i=1,\ldots ,n\) and let \(P^+ (P^-)\) be the prospect obtained from \(P\) by substituting all the losses (gains) with zero. GLS means that the evaluation of \(P\) is obtained as sum of the value of \(P^+\) and \(P^-\), i.e., \(V(P)=V(P^+ )+V(P^-)\). Wu and Markle (2008) refer to the following experiment: 81 participants gave their preferences as it is shown below (read \(H\succ L\) “the prospect \(H\) is preferred to the prospect \(L\)”.) 
\([52\%] \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [48\%]\)
 
\([15\%] \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [85\%]\)
 
\([37\%] \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [63\%]\)
 As can be seen, the majority of participants preferred \(H\) to \(L\), but, when the two prospects were split in their respective positive and negative parts, a relevant majority prefers \(L^+\) to \(H^+\) and \(L^-\) to \(H^-\). Thus, GLS is violated and CPT cannot explain such a pattern of choice. In the sequel, we will refer to this experiment as the “Wu-Markle paradox.” In the CPT model the GLS implies the separation of the domain of the gains from that of the losses, with respect to a subjective reference point. This separation, technically, depends on a characteristic S-shaped utility function, steeper for losses than for gains, and on two different weighting functions, which distort, in different way, probabilities relative to gains and losses. We aim to generalize CPT, maintaining the S-shaped utility function, but replacing the two weighting functions with a bi-weighting function. This is a function with two arguments, the first corresponding to the probability of a gain and the second corresponding to the probability of a loss of the same magnitude. We call this model the bipolar Cumulative Prospect Theory (bCPT). The bCPT will allow gains and losses within a mixed prospect to be evaluated conjointly. Let us explain our motivation. The basic one, stems from the data in Wu and Markle (2008) and in Birnbaum and Bahra (2007). Both papers, following a rigorous statistical procedure, reported systematic violations of GLS. Moreover, if we look through the Wu-Markle paradox shown above, we understand that the involved probabilities are very clear, since they are the three quartiles 25, 50, and 75%. Similarly, the involved outcomes have the “right” size: neither so small to give rise to indifference nor so great to generate unrealism. Now suppose to look at the experiment in the other sense, from non mixed prospects to mixed ones. The two preferences \(L^+\succ H^+\) and \(L^-\succ H^-\), under the hypothesis of GLS, should suggest that \(L\) should be strongly preferred to \(H\). Surprisingly enough, \(H\succ L\). What happened? Clearly, the two preferences \(L^+\succ H^+\) and \(L^-\succ H^-\) did not interact positively and, on the contrary, the trade-off between \(H^+,H^-\) and \(L^+,L^-\) was in favor of \(H\). These data, systematically replicated, seem to suggest that a sort of Gain Loss Hedging (GLH) appears in the passage from prospects involving only gains or losses to mixed ones. When the GLH phenomenon is intense enough to reverse the preferences, i.e. \((L^+\succ H^+\) and \(L^-\succ H^-)\) and also \(H\succ L\), then GLS is violated. Thus, the first motivation of the paper is to show how bCPT is able to capture, at least partially, these erroneous predictions of CPT. A second motivation for proposing bCPT, stems from the consideration that, in evaluating mixed prospects, it seems very natural to consider a trade-off between possible gains and losses. This, corresponds to assume that people are more willing to accept the risk of a loss having the hope of a win and, on the converse, are more careful with respect to a possible gain having the risk of a loss. Psychologically, the evaluation of a possible loss could be mitigated if this risk comes together with a possible gain. For example, the evaluation of the loss of $3,000 with a probability 0.5 in the prospect \(H=(0\ ,0.5; \$-3,000,\ 0.5 )\) could be different from the evaluation of the same loss within the prospect \(L=(\$4,200\ ,0.5;\ \$-3,000\ ,0.5 )\), where the presence of the possible gain of \(\$4,200\) could have a mitigation role. Why should be the overall evaluation of a prospects only be the sum of its positive and negative part? The last motivation has historical roots and involves the revolution given to the development of PT. Since when the theory has been developed (Kahneman and Tversky 1979), a basic problem has been to distinguish gains from losses. However, in the evolution of decisions under risk and uncertainty, the majority of data, regarded non-mixed prospects (see, e.g., Allais 1953; Ellsberg 1961; Kahneman and Tversky 1979). Many authors pointed out that the mixed case is still a little understood domain (Luce 1999, 2000; Birnbaum and Bahra 2007; Wu and Markle 2008). This paper is organized as follows. In Sect. 2 we describe the bCPT, starting from the CPT. In Sect. 3 we present several bi-weighting functions, generalizing well know weighting functions of CPT. Section 4 is devoted to the relationship between CPT and bCPT. In Sect. 5 we extend bCPT to uncertainty. Our main result, the characterization of the bipolar Choquet integral, is developed in Sect. 6. We conclude in Sect. 7. Some proofs, depending on the importance, are presented in the main text, while the remaining proofs are presented in Appendix. The Appendix also contains tests of bCPT on the previous data reported in the literature about the GLS violation.",10
77.0,1.0,Theory and Decision,20 September 2013,https://link.springer.com/article/10.1007/s11238-013-9391-2,Additive representation of separable preferences over infinite products,June 2014,Marcus Pivato,,,Male,Unknown,Unknown,Male,"The two main results of this paper (Theorems 1 and 2) yield additive utility representations for separable preference orders defined on a Cartesian product. There is a long history of such results in economic theory, starting with the well-known work of Debreu (1960). But most of these results concern preferences on finite-dimensional spaces, and require these preferences to satisfy a continuity or Archimedean condition. In contrast, this paper works with infinite-dimensional spaces, and requires no preexisting topological or algebraic structure at all. Importantly, Theorem 1 does not require any Archimedean or continuity condition, and Theorem 2 uses only a very weak ‘continuity’ condition (to approximate infinite-dimensional comparisons by finite-dimensional comparisons). The classical theory of additive representations is concerned with real-valued utility functions. Instead, this paper will work with utility functions ranging over linearly ordered abelian groups. The real number line is one such group. Other linearly ordered abelian groups can be thought of as ‘non-Archimedean’ generalizations of the real number line, which may contain ‘infinite’ or ‘infinitesimal’ values. A particularly important example is the group \({}^*\!\mathbb{R }\) of hyperreal numbers, an extension of the real number system which is the starting point of nonstandard analysis.Footnote 1 A linearly ordered abelian group is the minimal mathematical structure needed to define the range of a ‘cardinal’ utility function. Real-valued utilities emerge as a special case if we impose an Archimedean condition to exclude infinite and infinitesimal utility values (see Sect. 3). The use of (non-real) linearly ordered abelian groups is not an annoying technicality or a weakness of the model in this paper. It is a strength. As already noted, to get a real-valued utility representation, previous results invoked either a continuity axiom or an Archimedean axiom (see Sect. 7 for a review of this literature). These axioms are often presented as innocuous ‘technical’ conditions, but they are not. Empirically, they are almost impossible to test, and when combined with other axioms, their empirical implications are unclear. Normatively, it is not really clear why an agent should be Archimedean or continuous in her preferences. Thus, measurement theorists and decision theorists have long been uncomfortable with these axioms.Footnote 2 This makes it desirable to find an additive utility representation which eschews them. That is one of the major contributions of this paper. The limitations of a purely real-valued approach to utility are already visible in finite dimensions. For example, consider the leximin social welfare order on \(\mathbb{R }^N\). This order is separable, but it does not admit a real-valued utility representation. However, it can be represented with an additive utility function ranging over a linearly ordered abelian group [see Example 4(v) below]. The advantages of non-real-valued representations are even clearer in infinite-dimensional spaces. One application is intergenerational justice. Many contemporary policy problems have ramifications extending into the indefinitely far future. These include: disposal of nuclear waste, consumption of nonrenewable resources, irreversible environmental degradation and anthropogenic climate change. For such issues, the choices society makes now will have implications for people who will be born hundreds or thousands of years later. It may be sensible for an individual to discount her own future utility due to her impatience. But many authors have argued that it is not ethically defensible for us to discount the utility of future generations. In problems of intertemporal social choice, we must give future persons the same weight as current persons.Footnote 3
 But comparing non-discounted infinite utility streams is difficult. For example, consider the two futures portrayed in Fig. 1. Each of these plots portrays a possible sequence of social welfare levels over the next 300 years (and let us assume that the pattern in each sequence continues into the indefinite future). In both cases, social welfare is growing at an average annual rate of 2 % (e.g. due to population growth and/or economic development), but this growth is perturbed by a periodic fluctuation in social welfare (e.g. due to business cycles). In Fig. 1a, this fluctuation has shorter frequency and higher amplitude than in Fig. 1b. Neither sequence Pareto-dominates the other. Which sequence is socially preferable? The utilitarian criterion is unhelpful: the sum of (nondiscounted) future utilities is infinite for both sequences.Footnote 4 But it is not even clear that a utilitarian-style comparison would be appropriate. A sizeable literature has emerged to grapple with the problem of infinite-horizon, nondiscounted intertemporal social choice (see Sect. 7.2 for a review of this literature). This paper will propose a new solution to this problem [see Example 4(i) and Sect. 4.1]. Two possible futures. Which is better? Now consider a different problem. Let \(\mathcal{I }\) be an infinite set of possible states of nature, and suppose that we want to define a ‘uniformly distributed’ probability distribution on \(\mathcal{I }\), such that every state in \(\mathcal{I }\) gets the same (nonzero) probability. This may reflect the fact that we are totally ignorant about the true state, and, therefore, we think any state is just as likely as any other state. Using classical (i.e. real-valued) probability, such a distribution is impossible. This has ramifications for decision theory. Consider a lottery whose outcome depends on which state in \(\mathcal{I }\) is realized. How can we compute the expected utility of such a lottery? How can we compare two such lotteries? This paper will show how this can be done [see Example 4(ii) below]. Similar issues arise in game theory. Consider the following two-player zero-sum game (an abstract form of ‘Hide and Seek’). Row and Column each choose an integer. If their integers agree, then Row gets a payoff of \(-\)1 and Column gets a payoff of 1. If their integers disagree, then they both get a payoff of zero. Clearly, there is no pure strategy Nash equilibrium in this game. Furthermore, there is no mixed-strategy Nash equilibrium using real-valued probabilities. To see this, suppose that \(\rho \) was a probability density on the integers describing Row’s mixed strategy, while \(\kappa \) was a probability density describing Column’s mixed strategy. Neither density can be constant. Column’s best response to \(\rho \) is to concentrate all of \(\kappa \)’s mass on the integers where \(\rho \) is maximal. But Row’s best response to \(\kappa \) is to concentrate all of \(\rho \)’s mass on integers where \(\kappa \) takes the value \(0\). Thus, \(\rho \) and \(\kappa \) can never be mutual best responses; they cannot form a Nash equilibrium. To form a Nash equilibrium, it seems that \(\rho \) and \(\kappa \) must both be ‘uniformly’ distributed over the set of integers. But this is impossible as long as we use real-valued probabilities. This paper will show how to construct a mixed-strategy Nash equilibrium for this game using infinitesimal probabilities (see Sect. 4.2). Consider a closely related problem. If \(\mathcal{I }\) is an open interval in the real numbers, then the (normalized) Lebesgue measure is the canonical model of a ‘uniform’ probability distribution on \(\mathcal{I }\). But many nonempty subsets of \(\mathcal{I }\) have Lebesgue measure zero—for example, the set of rational points in \(\mathcal{I }\). Consider a uniformly distributed random variable ranging over \(\mathcal{I }\). It is logically possible that this random variable takes a rational value. But this event has (Lebesgue) probability zero. This paradoxical outcome might be dismissed as a mere technical annoyance—until we want to compute the conditional probability of some event, conditional on the random variable being rational. Such a conditional probability is not even well-defined in classical probability theory. This paper singles out a particular solution to this problem: a ‘uniform’ probability measure which assigns nonzero (but possibly infinitesimal) probability to every nonempty subset of \(\mathcal{I }\) (see Sect. 4.3). Of course, this idea is not new; non-Archimedean and/or nonstandard probability measures have been studied for several decades by both mathematicians and economists (see Sect. 7.1 for a summary of this literature). But this paper provides a new and very simple axiomatic characterization of such probability measures. Strangely, there has been very little application of non-Archimedean groups to infinite-horizon intertemporal choice. But the results of this paper are applicable in that context as well. The rest of this paper is organized as follows. Section 2 formally defines the model and states the main results, Theorems 1 and 2. These two theorems provide axiomatic characterizations for the existence of an additive utility representation for a preference order on an infinite-dimensional Cartesian product. Section 3 discusses when this utility representation is in fact real-valued. Section 4 illustrates the significance of these results by exploring the three applications briefly sketched above. Section 5 develops some background about nonstandard analysis, and states a series of lemmas which are the main steps in the proof of Theorem 2. Section 6 examines the meaning of the three axioms invoked in Theorems 1 and 2. Section 7 briefly reviews related literature. Finally, the Appendix contains the proofs of all results.",20
77.0,1.0,Theory and Decision,07 August 2013,https://link.springer.com/article/10.1007/s11238-013-9387-y,Sequential decision making without independence: a new conceptual approach,June 2014,A. Nebout,,,Unknown,Unknown,Unknown,Unknown,,
77.0,1.0,Theory and Decision,31 May 2013,https://link.springer.com/article/10.1007/s11238-013-9383-2,The potential of iterative voting to solve the separability problem in referendum elections,June 2014,Clark Bowman,Jonathan K. Hodge,Ada Yu,Male,Male,Female,Mix,,
77.0,1.0,Theory and Decision,12 September 2013,https://link.springer.com/article/10.1007/s11238-013-9392-1,Fairness motivation in bargaining: a matter of principle,June 2014,Sigbjørn Birkeland,Bertil Tungodden,,Male,Male,Unknown,Male,"Bargaining is an important mechanism in all realms of society and a prominent topic in the social sciences. Bargaining is typically studied in the context of self-interested individuals, but it is by now well established that people are also fairness motivated (Camerer 2003). How does fairness motivation affect the possibility of reaching a bargaining agreement?Footnote 1
 Economic experiments using the alternating offer protocol of Rubinstein (1982) have shown that when an agreement is reached, bargaining outcomes tend to cluster around equal division, even when researchers impose unequal bargaining power on the participants (Camerer 2003). This may suggest that fairness motivation not only affects the possibility of an agreement but also potentially the nature of an agreement. For example, in a two-person bargaining situation where none of the participants has a particular fairness claim to the endowment, which has been the predominant case in many bargaining experiments, the participants may find it fair to agree on an equal division, even when they have greater bargaining power than the other individual. The present paper studies formally the importance of fairness motivation in bargaining and thereby aims at contributing to better understanding of cooperation and conflict in society. We introduce a bargaining model that allows for both self-interest and fairness motivation, and analyze how these two motivational forces interact in the bargaining process. To introduce the possibility of a fairness motivated conflict, we enrich the context such that the individuals do not necessarily find it fair to share equally.Footnote 2 Recent economic experiments have, for example, shown that if the endowment to be shared is the result of individual contributions, people may have different views of what constitutes a fair division (Konow 1996; Gächter and Riedl 2005; Cappelen et al 2010). In the formal analysis, we provide a framework for studying bargaining situations both when people have compatible and incompatible views of what is a fair division of the endowment, and we argue that this framework accommodates existing experimental and field data on bargaining. In the first part of the paper, we study how fairness motivation influences the possibility of reaching an agreement. Proposition 1 shows that bargaining between two individuals who disagree about what is a fair division of the endowment may end in conflict. This result is shown for a general class of utility functions and illustrates the importance of allowing for heterogeneity in fairness views when studying bargaining. In contrast, Proposition 2 establishes that if two individuals agree on what is a fair division, it is always possible to reach an agreement. We illustrate these results by the use of a specific utility function that represents a standard social preference model (Bolton and Ockenfels 2000; Bruyn and Bolton 2008; Cappelen et al 2007), where we show that for realistic parameter values, fairness motivation can lead to disagreement. In the second part of the paper, we study the nature of the bargaining agreement by using the Nash bargaining solution. Propositions 3–5 show that both the weight attached to fairness and the fairness view are crucial in determining the allocation of the endowment in the bargaining agreement. These results demonstrate how fairness considerations may be important in shaping the bargaining agreement and may provide some justification for the advice about focussing on fairness put forward in much of the prescriptive bargaining literature (Fisher and Ury 1991). But, the analysis also highlights the fragileness of this advice—if each bargainer insists on a bargaining outcome that reflects what he considers fair, the bargaining may end up in disagreement. Finally, Proposition 6 shows that the introduction of fairness motivation does not affect the bargaining outcome if both individuals consider it fair to divide in proportion to their bargaining power.",26
77.0,2.0,Theory and Decision,05 October 2013,https://link.springer.com/article/10.1007/s11238-013-9396-x,"Risk behavior for gain, loss, and mixed prospects",August 2014,Peter Brooks,Simon Peters,Horst Zank,Male,Male,Male,Male,"Dominance criteria play an important role for decision making as they are used to eliminate inferior options and strategies. These priciples are used in economics, finance, management science and in many related disciplines (see Bawa 1982 for a listing of early literature, and the review of Levy 1992). We designed an experiment to study stochastic dominance principles and, within that design, to test for features of prospect theory (PT), currently the most popular descriptive decision theory for risk and uncertainty (Starmer 2000; Wakker 2010). PT incorporates reference dependence (i.e., outcomes are coded as gains and losses relative to a reference point), sign-dependence (i.e., probability weighting depends on the sign of outcomes), and loss aversion (LA) (losses weigh heavier than similar size gains). These features paired with extensive empirical evidence lead to predictions for binary choice that may disagree with traditional dominance criteria. In this paper we focus on second-order stochastic dominance (SSD) rules.Footnote 1 SSD has been introduced by Rothschild and Stiglitz (1970) and Hadar and Russell (1969) to develop a measure of “prospect \(Q\) is more risky than \(P\).” If \(Q\), compared to \(P\), assigns more probability mass to at least one lower ranked outcome and does not sufficiently compensate by assigning more probability mass to better ranked outcomes, \(Q\) is regarded as more risky, and thus is dominated by \(P\) in the SSD-sense. The theoretical implications of SSD are well understood for expected utility theory (an increasing and concave utility function; see Levy 1992) and for rank-dependent utility theory (Quiggin 1982; increasing and concave utility, and increasing and convex probability weighting function; see Chew et al. 1987; Ryan 2006). Schmidt and Zank (2008) provide an analysis of SSD under general PT: for gains, utility is concave and the probability weighting function is convex; for losses, utility is concave and the probability weighting function is concave. The theoretical implications of SSD for PT do not fit well with the empirical evidence on choice behavior, which suggests that utility is concave for gains but convex for losses and that the weighting functions have an inverse \(S\)-shape, being concave for small decumulative probabilities and convex for large ones (see Abdellaoui et al. (2007) and Wakker (2010) for summaries and discussions of recent empirical evidence).Footnote 2 However, the theoretical implications of SSD emerge naturally as the property requires a global form of risk aversion. Under PT risk behavior is not globally consistent in the former sense as outcomes are not interpreted as final wealth positions but, instead, as deviations from a reference point. A distinction between risk behavior for gain prospects and separately the risk behavior for loss prospects is meaningful. Additionally, for mixed prospects containing gains and losses, LA may imply additional aversion to increases in risk when gains are traded off against losses of similar size (Brooks and Zank 2005; Abdellaoui et al. 2007, 2008). Accounting for the empirical evidence regarding utility curvature for gains and that for losses, Levy and Levy (2002) proposed restrictions to the general SSD principle. Prospect stochastic dominance (PSD) requires SSD for the gain part of prospects but for the loss part of prospects the opposite of SSD (i.e., a preference for the dominated prospect) is demanded. Markowitz stochastic dominance (MSD), inspired by the utility curvature proposed in Markowitz (1952), requires SSD for the loss part of prospects and the opposite of SSD for the gain part of prospects. For prospects with equal means PSD predicts the opposite of MSD, but this may not hold for prospects that have different means. Levy and Levy ran experiments involving mixed prospects containing both gains and losses and interpreted their results as evidence for MSD and, hence, as evidence against PT. Accounting for probability weighting as in the modern PT model of Tversky and Kahneman (1992); Wakker (2003) demonstrated that the choice behavior observed by Levy and Levy is, however, consistent with PT. A similar result was found in the study of Baucells and Heukamp (2004). The latter authors argued for further adjustments of PSD that take into account inverse \(S\) probability weighting and also LA (Baucells and Heukamp 2006). While the study of Levy and Levy (2002) is inconclusive about violations of modern PT, the findings of Baltussen et al. (2006) show that choice behavior between mixed prospects may not always be in agreement with PT predictions. They reconsider a specific choice task of Levy and Levy that involved a choice between a mixed prospect F and a second one, G, that results from \(F\) through simultaneous increases in risk for the gain and the loss part of \(F\). They design an “intermediate” mixed prospect, \(H\), that agrees with \(F\) on the gain part but has increases in risk that only involve losses. As a result they obtain additional information about risk behavior among mixed prospects with common gains and among mixed prospects with common losses. This way they identify descriptive inaccuracies of PT and provide evidence regarding the PSD and MSD principles of Levy and Levy (2002). This paper presents new empirical tests of SSD and its restrictions PSD and MSD taking account of the empirical evidence regarding probability weighting and LA, and compares the findings with the predictions based on PT. We present data from a laboratory experiment involving binary choices over small to moderate scale prospects that involve real stakes. The 90 participants had to decide between a prospect and an SSD-dominated transformation of that prospect in 95 binary choices. We consider three broader conditions within which further refinements are identified. The broad conditions involve: (i) the gain condition: choices among prospects where no outcomes are losses, (ii) the loss condition: choices among prospects that involve no gains, and (iii) the mixed condition: choices among prospects that involve gains and losses of similar size. It is well-known that SSD implies aversion to mean preserving spreads (MPS), also known as strong risk aversion (the latter requiring SSD only if the prospects have the same mean). Within a choice task we present prospects that have the same mean, hence we obtain evidence about aversion to MPSs (Rothschild and Stiglitz 1970), and information about how this behavior may be affected by the nature of prospects (i.e., gain, loss, or mixed). Additionally our study accounts for potential biases caused by probability weighting, which is an important component of risk behavior (see Wakker 2001, 2010) and a key inovation of modern PT. Accordingly, within each broader condition, we employ MPSs which use small, medium and large decumulated probabilities. Our analysis, therefore, provides new evidence concerning the validity of the SSD criteria of Levy and Levy (2002) as discussed and extended in Baucells and Heukamp (2006). Most experimental studies on PT have focused on prospects with at most three outcomes and clear empirical evidence supporting inverse S probability weighting has been obtained (see Wakker 2010). To test PSD and MSD we require prospects with more than three outcomes. For this domain it is unclear if the shape of probability weighting functions and LA are as pronounced as the existing evidence suggests. For example, (Camerer (1995), p. 637), suggested that the performance of nonlinear probability weighting for prospects with a larger number of outcomes is an unsettled and fundamental empirical question. Lopes (1984) and Fennema and Wakker (1997) also used multiple outcome prospects and found support for inverse S probability weighting while the data of Payne (2005) suggests that an extension of the original PT-version of Kahneman and Tversky (1979), where actual instead of cumulated probabilities are transformed, gives a better fit. Baltussen et al. (2006) suggest that probability weighting for mixed prospects may be different than for gain or loss prospects (Baltussen et al., p. 1290) and call for more experimental tests in the domain of mixed prospects with more than three outcomes. Our study uses five outcome prospects and allows for comparison across the domains of gain, loss, and mixed prospects, therby supplementing the existing scarce evidence on probability weighting under PT on the domain of multiple outcome prospects. A further feature of our study is that it provides results at the aggregate level as well as at the individual level, as this distinction may be relevant when comparing behavior across domains (see Zeisberger et al. 2012). Our results indicate that behavior as predicted by PT extends to the domain of gain and loss prospects but not necessarily to the domain of mixed prospects. We find that MSD and PSD do not perform well but that the extension of PSD by Baucells and Heukamp (2006) gives better predictions. For mixed prospects we find some evidence that the sign of common outcomes and, hence, the likelihood of losing, plays a significant role for choice behavior, complementing results of Payne (2005) and Brooks and Zank (2005). The next section presents notation and is followed by a section with details of the experiment. The results are presented in Sect. 5 , with a discussion provided in Sect. 6. Concluding remarks are presented in Sect. 7.",8
77.0,2.0,Theory and Decision,13 September 2013,https://link.springer.com/article/10.1007/s11238-013-9393-0,Axiomatizing bounded rationality: the priority heuristic,August 2014,Mareile Drechsler,Konstantinos Katsikopoulos,Gerd Gigerenzer,Female,Male,Male,Mix,,
77.0,2.0,Theory and Decision,28 September 2013,https://link.springer.com/article/10.1007/s11238-013-9397-9,Multi-task agency with unawareness,August 2014,Ernst-Ludwig von Thadden,Xiaojian Zhao,,Unknown,Unknown,Unknown,Unknown,,
77.0,2.0,Theory and Decision,08 September 2013,https://link.springer.com/article/10.1007/s11238-013-9394-z,Implementing equal division with an ultimatum threat,August 2014,Esat Doruk Cetemen,Emin Karagözoğlu,,Male,Male,Unknown,Male,"The quote above refers to the punishment threat and egalitarian behavior in the celebrated Divide the Dollar (DD) game. The divide-the-dollar game is a special case of Nash’s (1953) demand game, where the bargaining frontier is linear. In the DD game, two players simultaneously demand shares of a dollar. If the sum of their demands is less than or equal to a dollar, they receive their corresponding demands; if the sum of their demands is greater than a dollar, each player receives zero. Since this simple game encapsulates important characteristics of most bargaining problems (e.g., parties can secure a surplus if they can agree on the way it is to be shared; otherwise they cannot), it received great interest (Binmore 1998; Kilgour 2003). However, despite its appealing features the DD game suffers from an essential drawback: the Nash equilibrium does not have much predictive power since any division of the dollar is a Nash equilibrium. Therefore, in its perfectly divisible dollar version, there are infinitely many Nash equilibria in this game. One may think that 50–50 division is the most likely outcome, possibly due to its prominence, symmetry, or presumed fairness. In fact, this is what is suggested theoretically (Schelling 1960; Sugden 1986; Young 1993; Huyck et al. 1995; Skyrms 1996; Bolton 1997)Footnote 1 and observed empirically (Siegel and Fouraker 1960; Nydegger and Owen 1975; Roth and Malouf 1979). Nevertheless, from a standard game theoretical point of view the 50–50 division is just one of the infinitely many Nash equilibria. This makes the research program on mechanisms that resolve the multiplicity problem and induce reasonable behavior an auspicious one. To the best of our knowledge, Brams and Taylor (1994) is the first study that aims to resolve the multiplicity problem in the DD game in favor of the equal division. They offer three modifications of the standard DD game, in which they change the payment rule (DD1), add a second stage (DD2), or do both (DD3). All of these mechanisms implement equal division in equilibrium. Brams and Taylor (1994) also list five characteristics that any reasonable mechanism should satisfy. These are (i) equal demands are treated equally; (ii) no player receives more than his demand; (iii) if the estate to be divided is sufficient to respect all players’ demands, then everyone should receive his demand; (iv) the estate should be completely distributed in any case; and (v) if each player’s demand is greater than \(E\)/\(n\) (\(E\) is the value of the estate to be divided and \(n\) is the number of players), then the player with the highest demand should not receive more than what the player with the lowest demand receives. They show that no reasonable mechanism can implement the equal division if the equilibrium concept used is the iterated elimination of weakly (or strongly) dominated strategies. In a similar fashion, Anbarcı (2001) modifies the payment rule in case the sum of players’ demands exceeds a dollar. The author interprets incompatible demands (i.e., \(d_{1}+d_{2}>1\)) as each player asking for a compromise from the other. Anbarcı (2001) mechanism requires each player to make a compromise identical to the one he asks from the other player. He shows that the iterated elimination of strictly dominated strategies attains a unique equilibrium that involves equal demands. Intuitively speaking, this mechanism requires each player to put himself in the other player’s shoes. Thus, it can be thought as a mechanism that implements equal division by forcing players to empathize. 
Ashlagi et al. (2012) also modify the payment rule in case the sum of players’ demands is greater than the estate. Instead of each player receiving zero in this case, the estate is completely distributed by using an allocation rule. The authors resort to bankruptcy (or conflicting claims) problems to find such allocation rules. They show that equal division prevails in all Nash equilibria of the modified game, if the allocation rule satisfies equal treatment of equals,
efficiency, and order preservation of awards (Theorem 1).Footnote 2 Alternatively, in case the rule satisfies equal treatment of equals, efficiency, claims monotonicity, and non-bossiness, the same result holds (Theorem 3).Footnote 3 Thus, Ashlagi et al. (2012) show that it is possible to implement equal division in the Nash equilibria of the modified DD game even after avoiding the punishment (i.e., everyone receiving zero in the case of incompatible demands) and using allocation rules that satisfy certain properties (e.g., claims monotonicity and order preservation of awards) that encourage greedy behavior. The class of rules they consider is large and contains—but is not restricted to—most of the prominent bankruptcy rules (e.g., proportional rule, constrained equal awards rule, constrained equal losses rule, and Talmud rule). A result by Trockel (2000) in a different line of research is also of interest. Trockel (2000) implements the Nash solution based on its Walrasian characterizations. He considers convex bargaining sets (i.e., including ones with non-linear frontiers). Although his focus is somewhat different than the papers mentioned above, if one limits attention to DD the methods Trockel (2000) employs can be seen as other ways to implement the equal division, because the Nash bargaining solution in DD is the 50–50 division.Footnote 4 Walrasian mechanisms that he employs and Brams and Taylor (1994) and Anbarcı (2001) offer share some common features: all of these mechanisms (i) alter the payment rule in case of incompatible demands and (ii) discriminate between players and favor the player with a more modest demand in this case. Our modification \((DD^{\prime })\) involves a second stage with a softer (in comparison to DD) punishment rule in the case of incompatible demands. In such a case, players have one more chance to divide the dollar. Specifically, they can avoid the excess they generated in the first stage with an ultimatum game in the second stage. In the two-player version, the player who demanded less (more) in the first stage is the proposer (responder). In case of a tie, a player will be chosen randomly. The proposer makes an offer, which suggests the part of the excess that will be deducted from his demand and the responder’s demand. If the proposal is accepted, then both players receive their corresponding (post-deduction) demands. In case of a rejection, the excess cannot be avoided and players do not stand another chance: they both receive zero. If one interprets the modifications to DD as guidelines of different types of arbitrators, the arbitrator type that corresponds to DD
\( ^{\prime }\) would be one that (i) does not immediately enforce harsh punishments, (ii) leaves the floor to bargaining parties as much as possible, and (iii) encourages agreeable behavior. Moreover, in the two-player version, \(DD^{\prime }\) attains a unique subgame perfect Nash equilibrium in pure strategies, in which players demand (and receive) an equal share of the dollar in the first stage (Theorem 1). In the \(n>2\) version, there are payoff-equivalent subgame perfect Nash equilibria with the same characteristics. In all three studies on the DD game mentioned above, the authors focus on pure-strategy equilibria. However, if one also considers mixed-strategy equilibria, the problem of multiplicity is even more severe, as hinted by Myerson (1991, p. 112) and illustrated in detail by Malueg (2010). In particular, Malueg (2010) shows that practically any set of  balanced demands can support a mixed-strategy Nash equilibrium. As we show below, the mechanism we propose not only eliminates all pure-strategy equilibria involving unequal division of the dollar but also eliminates all equilibria where players mix over different demands in the first stage (Theorem 2).Footnote 5
 The organization of the paper is as follows: In Sect. 2, we introduce the modified DD game \((DD^{\prime })\) and prove our results for two players. Section 3 concludes. Extensions of our results to \( n>2\) players are provided in the appendix.",7
77.0,2.0,Theory and Decision,08 October 2013,https://link.springer.com/article/10.1007/s11238-013-9395-y,The generalized homogeneity assumption and the Condorcet jury theorem,August 2014,Ruth Ben-Yashar,,,Female,Unknown,Unknown,Female,"The seminal Condorcet jury theorem (CJT, 1785) states that the likelihood of a correct majority decision becomes certain as the group size tends to infinity. According to CJT’s fundamental assumption, every voter has an independent probability \(p, p>1/2\), of choosing correctly, assuming binary choice. The theorem has previously been generalized in various ways.Footnote 1 One may wonder whether the CJT is valid when the average probabilities over the two possible states of nature (facing a good/bad alternative) for each voter is greater than 1/2. This study shows that, unexpectedly, the answer to this question is negative.Footnote 2
 Formally, enlargement of the decision-making group is defined as adding two members. Each of the members is associated with two probabilities of voting correctly; \(p_1 \) in one state of nature and \(p_2 \) in the other. Within this framework, we show that larger groups, in most cases, are less likely to reach a correct collective decision. Our conclusion is valid even if the average individual probability of making a correct decision exceeds 1/2. This result sheds some new light on the homogeneity assumption in Condorcet’s setting, according to which the addition of members increases the probability of making a correct collective decision. We now proceed to set out the model and prove our result.",8
77.0,2.0,Theory and Decision,10 November 2013,https://link.springer.com/article/10.1007/s11238-013-9398-8,Double or nothing?! Small groups making decisions under risk in “Quiz Taxi”,August 2014,Klemens Keldenich,Marcus Klemm,,Male,Male,Unknown,Male,"Risk attitudes are an integral part of many economic decisions and thus economic models. However, the empirical assessment of risk attitudes has proven to be very challenging because many confounding factors make isolating risk attitudes difficult. Previous economic studies have used several methods to quantify risk attitudes, including the estimation of life-cycle macro-models, self-reported measures from micro-surveys, or laboratory experiments. Each of these approaches has its shortcomings which include, among others, the failure to account for unobserved individual heterogeneity, sample selection, or lack of incentives to reveal true preferences. Since Gertner (1993) and Metrick (1995), economic research has been using data from TV game shows which are generated in a way that lends itself to measuring risk attitudes. Our study contributes to the economic literature on decision making under risk by analyzing the behavior of contestants in the German TV game show “Quiz Taxi.”Footnote 1 The “Quiz Taxi” is unique among game shows because contestants do not apply actively to be on the show, which reduces the potential selection bias. They participate in a quiz while driving in a cab. Having reached the destination, the candidates are asked if they want to play a final “master question.” If they answer correctly, their winnings will double; if they are wrong, they will lose all. The group decision about this master question is used to study risk attitudes, focusing on possible sources of heterogeneity pertaining to sex, age, and group size. As many economic decisions are made by groups rather than individuals and there might be a systematic difference in behavior between groups and individuals, this is an important aspect of the data. We devote particular attention to the interplay of risk attitude and the subjective probabilities of answering the master question correctly. In addition, the data also allow us to include aspects of the communication that leads to the final decision. The agents in this particular setup are groups that must reach a consensus and must, therefore, communicate their preferences, which in most other settings remain unobservable to researchers. The empirical analysis reveals that contestants behave in a risk averse manner. Despite fairly high chances of answering correctly, only one third of all groups decides to play the master question. The tendency to opt for the risky gamble decreases with the winnings at stake, but increases with the performance during the cab ride. With regard to observable characteristics, all-female groups act much more cautious. Furthermore, the decision-making behavior also varies across groups that differ with regard to size, sex and age composition, prior performance, season aired on TV, or initial agreement. Therefore, the group composition is not only directly related to risk attitude, but also interacts with other relevant aspects of the decision. Accounting for communication characteristics and content highlights the importance of the subjective context for decision-making processes in situations involving risky choices. In particular, the propensity to gamble rises as discussions last longer. The analysis suggests that the contestants’ immediate reactions to the master question are dominated by risk aversion, which decreases when the decision is given more time and thought. The remainder of the paper is organized as follows: Sect. 2 briefly discusses the empirical background of the assessment of risk attitudes, focusing on research based on TV game shows. The “Quiz Taxi” and its contestants are described in Sect. 3. Our empirical strategy and results are presented in Sects. 4 and 5, respectively. Section 6 concludes.",3
77.0,2.0,Theory and Decision,29 November 2013,https://link.springer.com/article/10.1007/s11238-013-9401-4,Condorcet’s principle and the strong no-show paradoxes,August 2014,Conal Duddy,,,Male,Unknown,Unknown,Male,"Condorcet’s principle, proposed in the eighteenth century by the Marquis of Condorcet, is one of the most important normative principles in the theory of voting. A Condorcet winner is a candidate for election who is preferred by a majority in all pairwise comparisons with the other candidates. Condorcet’s principle says that a Condorcet winner must be elected whenever there is one (Condorcet, Marquis de 1785).Footnote 1
 However, Moulin (1988) shows that Condorcet’s principle entails a surprising and troubling paradox for voting rules, called the no-show paradox. This paradox arises when the addition of a ballot that ranks candidate \(x\) above candidate \(y\) may take victory away from \(x\) and give it to \(y\). A voting rule that is free from this paradox, so that no voter is made worse off for having voted sincerely rather than abstaining, is said to satisfy the participation principle.Footnote 2 Moulin proves that a voting rule cannot satisfy both Condorcet’s principle and the participation principle when there are four or more candidates. If we are to satisfy Condorcet’s principle then we must tolerate the no-show paradox. However, we may consider some instances of the paradox to be more severe than others. In that case, we may have reason to prefer some “Condorcet-consistent” voting rules over others, since some of them might at least be free from these severe cases of the paradox. In this paper we consider two special cases of the no-show paradox. These would seem to be especially bizarre instances. One arises when the casting of a ballot that ranks a candidate in first place causes that candidate to lose the election, superseded by a lower-ranked candidate. The other arises when a ballot that ranks a candidate in last place causes that candidate to win, superseding a higher-ranked candidate. We call these the strong no-show paradoxes after Pérez (2001) and Nurmi (2002). One or both of these special cases of the paradox are also considered by Smith (1973), Richelson (1978), Brams and Fishburn (1983), Saari (1995), and Lepelley and Merlin (2001).Footnote 3
 
Pérez (2001) demonstrates that Condorcet-consistent voting rules do exist that are free from one or even both of the strong no-show paradoxes, no matter the number of candidates. One of these is the Simpson–Kramer Min–Max rule. That rule is free from both of the strong no-show paradoxes. Young’s rule is free from one of the two paradoxes; a ballot that ranks a candidate in last place can never cause that candidate to win under Young’s rule. See Pérez (2001) for definitions of both of these rules and several other Condorcet-consistent rules. 
Moulin (1988) considers the aggregation of linear orderings or, in other words, the case where voters do not express indifference. An inspection of Moulin’s proof is sufficient to confirm that his impossibility result continues to hold true for the aggregation of weak orderings. It is unsurprising that expanding the domain of voting rules by permitting voter indifference does not lead to a possibility result. Typically, in social choice theory, the emergence of possibility is associated with the contraction of a domain rather than the expansion of one (see Gaertner 2001).Footnote 4
 
Pérez (2001) considers the aggregation of linear orderings and the aggregation of weak orderings. It is only in the case of linear orderings that he finds compatibility between Condorcet’s principle and freedom from the strong no-show paradoxes. He shows that this compatibility vanishes when we move from linear to weak orderings. In fact, even when there are just three candidates there is an impossibility. Recall that Moulin’s impossibility result applies just when there are four or more candidates. Voting rules that are free from the strong no-show paradoxes satisfy conditions called Positive Involvement and Negative Involvement. In the case of weak orderings, Positive Involvement says that if candidate \(a\) is a winning candidate and we add a voter who ranks \(a\) in first place then \(a\) must remain a winning candidate. Negative Involvement says that if candidate \(a\) is a losing candidate and we add a voter who ranks \(a\) in last place then \(a\) must remain a losing candidate. To help motivate what comes next, we reproduce here Pérez’s proof of this impossibility in the case of weak orderings. Suppose that there are three candidates \(a, b\), and \(c\), and 15 voters. The voters’ preferences are described in Table 1. Each number above the horizontal line indicates the number of voters with the preference ordering given below that number. Letters are written next to each other to indicate indifference. This profile of preferences is “cyclical” in the sense that each candidate pairwise defeats one of the others by a margin of one vote: \(a\) beats \(b, b\) beats \(c\), and \(c\) beats \(a\). Suppose, without loss of generality, that \(a\) is the unique winning candidate or is tied for victory with one or both of the other candidates. In the case of a tie, all of the tied candidates are said to be winning candidates. A tie-breaking mechanism such as a lottery may be used to elect one from among the winning candidates. Suppose that we add two new voters who are indifferent between \(a\) and \(c\), and prefer both of those to \(b\). Let us write that ordering as \(a\sim c\succ b\). Then \(c\) becomes the Condorcet winner. So \(a\) is no longer a winning candidate, despite \(a\) being ranked in first place by the new voters. Now suppose that instead of adding those two voters we exclude the two voters with preference ordering \(b\succ a\sim c\). Then, again, \(c\) becomes the Condorcet winner. So when we readmit these two voters \(a\) becomes a winning candidate despite both voters ranking \(a\) in last place. This completes the proof of the impossibility. In that first case, candidate \(a\) may well object to being removed from the set of winners as a result of the addition of two new voters who rank \(a\) in first place. However, from the perspective of the new voters, or of a welfarist social planner, it is not clear that there is really a problem. The new voters are indifferent between \(a\) and \(c\) and so they are no worse off. Indeed, if the original result was a tie between all three candidates and if a lottery was to be used to break that tie, then the new voters are better off for having voted. They have ensured victory for one of their most preferred candidates, ruling out the chance that \(b\) might be elected by lottery. Forbidding a scenario in which the new voters are better off for having voted is arguably not in keeping with the spirit of the participation principle. Let us return to the second scenario where we removed two voters with preference ordering \(b\succ a\sim c\). When those two voters abstain \(c\) is the unique winner. When they cast their ballots the set of winning candidates contains \(a\). These two voters then are no worse off for having voted. Indeed, they may well be better off. Suppose, again, that the result at the 15-voter profile is a three-way tie with a lottery to be used for tie-breaking. Then these two voters benefit from voting. They have given \(b\) a chance of being elected by lottery. An impartial observer may see no reason to object to this change in the outcome, and these voters do not appear to have a strategic incentive to abstain in this case. For simplicity, let us now restrict our attention to deterministic voting rules that always choose a single winner. Perhaps ties are broken by lexicographic order rather than by lottery, for example. Under this restriction, the Positive Involvement condition can be decomposed into two parts. Suppose that \(a\) is the original winner and we add a new voter who ranks \(a\) in first place. Part (i) says that \(a\) must not be superseded by another candidate also ranked in first place by the new voter. Part (ii) says that \(a\) must not be superseded by a candidate ranked below \(a\) by the new voter. In the proof given above, part (ii) of the Positive Involvement condition is entirely redundant. Perhaps by deleting part (i) and retaining only part (ii), which we might call the welfarist part, we can regain the compatibility with Condorcet’s principle that was lost in the move from linear orderings to weak orderings. Similarly, Negative Involvement can be decomposed into two parts. Suppose that \(a\) is not the original winner and we add a new voter who ranks \(a\) in last place. Part (i) says that \(a\) must not take victory from another candidate also ranked in last place by the new voter. Part (ii) says that \(a\) must not take victory from a candidate ranked above \(a\) by the new voter. Again, only part (i) is invoked in the proof argument above. When part (i) of each of these conditions is deleted we call the resulting conditions Weak Positive Involvement and Weak Negative Involvement. We find that Condorcet’s principle is compatible with both of these conditions when there are three candidates, and that it is compatible with neither of them when there are at least four candidates. The case of linear orderings that Moulin considers is standard in the theory of voting. However, in economic theory more generally it is usual to model preferences by weak orderings. Notably, individual indifference is permitted both in the case of Arrow’s theorem (Arrow 1963) and the Gibbard–Satterthwaite theorem (Gibbard 1973; Satterthwaite 1975). Both of those theorems have sometimes been presented in the literature in their linear-ordering forms. This is done for the sake of simplicity as the differences between the two forms of either theorem are considered to be minor. Yet, it turns out that moving to the weak-ordering setting has an important consequence for Moulin’s theorem. When individuals can be indifferent, the requirement that a voter must never be made worse off by voting sincerely than by abstaining is significantly more demanding than is necessary. This requirement can be weakened to either Weak Positive Involvement or Weak Negative Involvement, thereby strengthening Moulin’s critique of Condorcet’s principle.",10
77.0,3.0,Theory and Decision,14 August 2014,https://link.springer.com/article/10.1007/s11238-014-9459-7,Preface,October 2014,M. I. Lau,T. Neugebauer,U. Schmidt,Unknown,Unknown,Unknown,Unknown,,
77.0,3.0,Theory and Decision,12 August 2014,https://link.springer.com/article/10.1007/s11238-014-9464-x,My experimental meanderings,October 2014,John Hey,,,Male,Unknown,Unknown,Male,,3
77.0,3.0,Theory and Decision,07 June 2014,https://link.springer.com/article/10.1007/s11238-014-9444-1,Recall searching with and without recall,October 2014,Daniela Di Cagno,Tibor Neugebauer,Abdolkarim Sadrieh,Female,Male,Unknown,Mix,,
77.0,3.0,Theory and Decision,22 May 2014,https://link.springer.com/article/10.1007/s11238-014-9443-2,Comparing behavior under risk and under ambiguity in a lifecycle experiment,October 2014,Enrica Carbone,Gerardo Infante,,Female,Male,Unknown,Mix,,
77.0,3.0,Theory and Decision,08 July 2014,https://link.springer.com/article/10.1007/s11238-014-9456-x,"Reconsidering the common ratio effect: the roles of compound independence, reduction, and coalescing",October 2014,Ulrich Schmidt,Christian Seidl,,Male,Male,Unknown,Male,"The common ratio effect became one of the prime examples of the failure of expected utility theory and has motivated a substantial amount of literature analyzing the descriptive validity of expected utility (see, e.g., Hey and Orme 1994).Footnote 1 For two monetary prizes, \(a>b\), and a probability \(p, \; 1\ge p \ge 0\), of winning \(a\) and \((1-p)\) of winning \(b\), we denote the respective lottery by \(\varPhi =(a,p;b,(1-p))\). Suppose that there is also another lottery \(\varPsi =(c,q;b,(1-q))\), \(c>a,\; p>q\ge 0\), at choice. Reducing now the winning probabilities to \(\lambda p\) and \(\lambda q\), \(1>\lambda >0\), defines new lotteries \(\varPhi '=(a,\lambda p; b, (1-\lambda p))\) and \(\varPsi '=(c,\lambda q;b, (1-\lambda q))\). Let \(\succsim \) denote a subject’s preference ordering among lotteries and suppose \(\varPhi \succsim \varPsi \). Then this subject acts in conformity with expected utility theory, if However, experimental research, using both hypothetical and real payoffs, showed that many subjects decide according to This is the common ratio effect. It holds in particular for \(p=1\) and \(q<1\).Footnote 2
 Obviously, \(\varPhi '\) and \(\varPsi '\) can also be established by two-stage lotteries \(\varPhi ''=(\varPhi , \lambda ; b, (1-\lambda ))\) and \(\varPsi ''=(\varPsi , \lambda ; b, (1-\lambda ))\). In the first stage, a lottery is played which accords a payoff of \(b\) with probability \((1-\lambda )\) and a lottery \(\varPhi \) or \(\varPsi \) with probability \(\lambda \).Footnote 3 If the subject ignores the first stage, then the isolation effect or the pseudocertainty effect
Footnote 4 is at work, and the subject decides as if only \(\varPhi \) and \(\varPsi \) were at choice. The isolation effect and the pseudocertainty effect then induce subjects to satisfy compound independence
 which was introduced by Segal (1990) and demands the independence axiom only for two-stage lotteries. In general terms, the reduced form of \(\varPhi ''\) is given by the three-part lottery 
Reduction of compound lotteries holds if we always have \(\varPhi ''' \sim \varPhi '\). 
Event splitting or branch splitting means that the probabilities of some payoffs are split up, so that the respective payoff is accorded with two or more probabilities which sum up to the original probability. Suppose \(1\ge p>r \ge 0, \; 1\ge p+r\ge 0\), then event splitting of payoff \(b\) is given by the three-part lottery The inverse operation, viz. unifying the split probabilities to a single probability for the particular payoff, is called coalescing. Note that there are many ways of event splitting, but only one way to coalesce events to one event with the respective compound probability. If coalescing holds, we have for this example Summarizing, we have Compound independence holds if \(\varPhi \succsim \varPsi \Leftrightarrow \varPhi '' \succsim \varPsi ''\). Reduction of compound lotteries holds if \(\varPhi '' \succsim \varPsi '' \Leftrightarrow \varPhi ''' \succsim \varPsi '''\). Coalescing holds if \(\varPhi ''' \succsim \varPsi ''' \Leftrightarrow \varPhi ' \succsim \varPsi '\). If all three axioms hold, we have Logical ratiocination implies and common ratio effects would be ruled out. In other words, at least one of these axioms should be violated in order to generate a common ratio effect. The experiment in the present paper analyzes which failure of these axioms is concomitant with the empirical observation of common ratio effects. Similar analyses were performed by Cubitt et al. (1998a) and by Birnbaum (2004). Cubitt et al. (1998a, pp. 1364–1365) investigated a five-step decomposition of the common ratio effect. Whereas our decomposition is basically static, their decomposition contains inter alia a precommitment lottery, i.e., a two-stage lottery such that subjects were required to choose their option for the second stage before the initial lottery was resolved; they called that dynamic choice. They (p. 136) referred to orderly precommitment as timing independence. They tied down violation of timing independence as the major cause of the common ratio effect. According to Cubitt et al. (1998a, p. 1378), subjects may be tempted to make more risky decisions under precommitment or they may experience endogenous preference shifts which they failed to anticipate. Note that Cubitt et al. (1998a) had no lotteries with event splitting (and coalescing) in their experimental design. Also note that all theories for explaining the common ratio effect which they discuss in their Table 1 cannot explain splitting effects and, therefore, cannot reconcile our results. Birnbaum (2004) performed a similar analysis for common consequence effects and identified violations of coalescing as their major source. We will, in particular, investigate whether his results translate into common ratio effects. Event splitting and coalescing effects were observed and investigated by Starmer and Sugden (1993), Neilson (1992), Humphrey (1995, 1966, 1998, 2001), and Birnbaum (1999a, b, 2004). Birnbaum (2004) provided comprehensive theoretical and empirical analyses of the common consequence effect, in which he, inter alia, studied event splitting. Different evaluation of split events must be either traced to the associated probabilities or the utility of multiple events has to be adapted accordingly. Explaining the effects of event splitting thus presupposes either probability weighing or utility dependence on event frequency. In an early study on price determination, Birnbaum and Stegner (1979) found that buyers place more weight on the lower estimates and sellers on the higher estimates of value. These are expressions of risk aversion to prevent determining faulty prices. Before embarking on our experiment, some remarks on experimental incentives are appropriate. Smith (1982) proclaimed a list of sufficient conditions for microeconomic experiments. We single out saliency and dominance. Saliency demands that subjects’ rewards are increasing (decreasing) in good (bad) outcomes, and dominance demands that the rewards should dominate any subjective costs associated with participation in the activities of an experiment. This requires real rather than hypothetical payoffs unless subjects are interested in the substance of the experiment. As applied to Allais’ experiments, this would mean in the strict sense that only persons like Bill Gates, Warren Buffett, or George Soros could afford making respective experiments. There are several escapes from this impediment: first, the experimenter can rely on subjects who are so much interested in the particular experiment that they appreciate participation in the experiment higher than the subjective costs of it. Then hypothetical payoffs are appropriate. Second, the experimenter scales down the payoffs, and, third, the experimenter uses somewhat higher payoffs but resorts to the random-lottery incentive system. With respect to Allais’ lotteries all three methods were applied. Each one has pros and cons, but ideological attitudes are misplaced in this respect.Footnote 5 We decided in favor of the second method following Conlisk (1989), Battalio et al. (1990, p. 37), Starmer and Sugden (1991), Harrison (1994, p. 231)Footnote 6, Burke et al. (1996), Beattie and Loomes (1997), Cubitt et al. (1998a), and Fan (2002), to mention only some authors. Conlisk, Harrison, Burke et al., and Fan observed a dramatic reduction of Allais-type responses for comparatively small real payoffs. Conlisk (1989, pp. 401–403) provided two explanations for that: (i) subjects did “reason more carefully and thus discover the appeal of responses consistent with expected utility theory” (for this view cf. also Slovic (1969)), (ii) subjects tend to switch to the maximization of expected value, since there are no great fortunes at stake such as one million for sure. Alas, when administering hypothetical payoffs to his pilot subjects, Conlisk (1989, p. 406) observed the same drop in Allais-type behavior as for the treatment with real payoffs. This seems to suggest that it is small rather than real payoffs which caused Allais-type behavior to disappear. As for our experiment, comparison of Experiments 1 and 3 shows distinct Allais-type behavior in spite of low and real payoffs as used in our experiment, which is in line with the results of Battalio et al. (1990, p. 37), Starmer and Sugden (1991), Beattie and Loomes (1997), and Cubitt et al. (1998a). The third method was applied inter alia by Birnbaum (2004), who also observed Allais-type behavior. Seminal work in comparing these methods was done by Starmer and Sugden (1991) and Beattie and Loomes (1997). Starmer and Sugden (1991) investigated the common consequence effect for real payoffs for all lotteries and for the random-lottery incentive system. For both treatments they evidenced significant Allais-type behavior. Beattie and Loomes (1997) investigated the common ratio effect (their Questions 1 and 3) under three treatments, viz. hypothetical payoffs (subjects received only a modest show-up fee), real payoffs under the random-lottery incentive system, and real payoffs for all lotteries. For all three treatments Beattie and Loomes (1997, p. 163, Table 1) observed a substantial common ratio effect with no significant differences among the three treatments. This result allowed them to state (p. 164) that “the salience hypothesis appears to be comprehensively rejected.”",9
77.0,3.0,Theory and Decision,28 May 2014,https://link.springer.com/article/10.1007/s11238-014-9446-z,Risk and time preferences of entrepreneurs: evidence from a Danish field experiment,October 2014,Steffen Andersen,Amalia Di Girolamo,Morten I. Lau,Male,Female,Male,Mix,,
77.0,3.0,Theory and Decision,19 August 2014,https://link.springer.com/article/10.1007/s11238-014-9465-9,What price compromise?,October 2014,John Bone,John D. Hey,John Suckling,Male,Male,Male,Male,"This paper reports on an experimental investigation based on the following decision problem. There is a (desirable) prize that may be allocated to either of two individuals, J and K. The allocation is to be decided randomly, by drawing a coloured ball from an opaque bag. The prize goes to J if the drawn ball is yellow, and to K if blue. If the drawn ball is red, however, then they each receive nothing. The problem for J and K is that they have to agree which one of three bags is to be used for the draw, with contents as shown in Table 1. If they fail to do so then by default no draw occurs and they each receive nothing, an outcome we denote as z. If they are both self-interested then J and K have opposing preferences, ex ante, over the three bags. Bag C, being the middle-ranked alternative for both partners, represents a compromise. However, this compromise comes at a price, in the form of the red ball. Five or more red balls in Bag C, yellows and blues being unchanged throughout, would be too high a price, in that both partners would be better off, ex ante, with either of Bags A and B (and thus, of course, C would no longer be a compromise). We can say that, in that case, agreement on Bag C would be collectively irrational for the two partners.Footnote 1 But what if, as here, Bag C contains only one red ball? Is this an irrationally high price to pay for the compromise? It would be so if J and K could bindingly agree to toss a coin to decide between Bags A and B, since this would give them each, ex ante, a 50 % chance of winning the prize. It would similarly be so if they could agree to share the prize afterwards, either by direct division or indirectly through side-payments or further randomisation, since each of Bags A and B delivers with certainty the prize thereby to be shared. But suppose that neither type of agreement is possible, so that the only options for J and K are just as initially described. Might they then reasonably agree to choose the costly compromise in the form of Bag C? Our conjecture was that reasonable, self-interested people probably would do so. However, as we show in Sect. 2, such an agreement is precluded by the standard Nash axioms, according to which just one red ball is indeed too high a price to pay. This provides the basis for an experimental test of the Nash axioms, as reported in Sects. 3 and 4. Other than very abstractly, the decision problem here bears no resemblance to any real-world bargaining problem. As with the familiar tests of Expected Utility (EU), our experimental design reflects a prior conjecture that, in a particular contrived situation, subjects will behave in a way that violates the Nash axioms, taken together. Also as with such tests of EU, in itself our experiment does not provide any explanation of such behaviour,Footnote 2 or identify which specific axiom is violated. The value of such a test depends on its novelty and strength, and on the currency of the theory in question. As noted below in Sect. 2, Nash bargaining theory has been subject to previous experimental testing, with broadly unfavourable results. As a pure theory of bargaining it has been largely superseded, although probably as much for conceptual as for empirical reasons, by the non-cooperative game theoretic approach. As with EU, however, the Nash theory is still commonly employed in economic modelling,Footnote 3 thus justifying its continued testing. Furthermore, as explained in Sect. 2, our test is not only novel but also relatively strong, in that it is directed at a relatively weak version of the theory.",3
77.0,3.0,Theory and Decision,20 June 2014,https://link.springer.com/article/10.1007/s11238-014-9449-9,Does consultation improve decision-making?,October 2014,Alessia Isopi,Daniele Nosenzo,Chris Starmer,Female,Female,,Mix,,
77.0,3.0,Theory and Decision,12 July 2014,https://link.springer.com/article/10.1007/s11238-014-9448-x,Assortative mating on risk attitude,October 2014,Philomena M. Bacon,Anna Conte,Peter G. Moffatt,Female,Female,Male,Mix,,
77.0,3.0,Theory and Decision,10 April 2014,https://link.springer.com/article/10.1007/s11238-014-9431-6,Estimating individual and group preference functionals using experimental data,October 2014,A. Morone,P. Morone,,Unknown,Unknown,Unknown,Unknown,,
77.0,3.0,Theory and Decision,12 June 2014,https://link.springer.com/article/10.1007/s11238-014-9447-y,Experimental payment protocols and the Bipolar Behaviorist,October 2014,Glenn W. Harrison,J. Todd Swarthout,,Male,Unknown,Unknown,Male,"The independence axiom (IA) plays a central role in most formal statements of expected utility theory (EUT), as well as popular alternative models of decision-making under objective or subjective risk. One such alternative is rank-dependent utility (RDU) theory (Quiggin 1982), which assumes that the IA is invalid in a certain way. The axiom also plays a central role in virtually every experiment used to characterize the way in which risk preferences deviate from EUT, through the use of the random lottery incentive mechanism (RLIM). For example, if someone claims that individuals behave as if they “probability weight” outcomes, and hence violate the IA, it is almost always on the basis of experiments and theories that assume the IA if the incentives are to be taken seriously. But there is an obvious inconsistency with saying that individuals behave as if they violate the IA on the basis of evidence collected under the maintained assumption that the axiom is magically valid. This inconsistency has long bothered theorists confronted with experimental data, and there have been responses from theorists and experimentalists. The primary theoretical response has been to argue that there is a way to write out a model of decision-making under risk that allows one to relax the IA but to still allow risk preferences to deviate from EUT predictions and for RLIM to be valid. In effect, to argue an existence proof: even though the inconsistency is real for the most popular alternatives to EUT, there exists a formal alternative to EUT where there is no inconsistency. We discuss this theoretical response later in Sect. 6. The primary experimental response has been to develop some direct tests and some ingenious designs intended to trap the IA under some circumstances.Footnote 1 But these direct and indirect experimental tests of the IA have been inconclusive. This is frustrating: either the axiom applies or it does not. The uneasy state of the literature has evolved to assuming the axiom for the purposes of making the payment protocol of an experiment valid, but rejecting it when characterizing the risk preferences exhibited in the same experiment using the standard alternatives to EUT.Footnote 2 Those characterizations seem to show evidence of rank-dependent probability weighting, when that very evidence calls into question a maintained assumption of the payment protocol used to generate the evidence. We refer to someone who holds this view as a BipolarFootnote 3 Behaviorist, exhibiting pessimism about the IA when it comes to characterizing how individuals directly evaluate two lotteries in a binary choice task, but optimism about the IA when characterizing how individuals evaluate multiple lotteries that make up the incentive structure for a multiple-task experiment. The standard payment protocol in individual risky choice experiments involves a subject making \(K>1\) binary choices over objective lotteries, and then selecting one choice at random for payment. We call this protocol 1-in-K. Following Conlisk (1989), Starmer and Sugden (1991), Beattie and Loomes (1997), Cubitt et al. (1998), and Cox et al. (2011), an alternative payment protocol, which we call 1-in-1, involves a subject making only one choice, and then being paid with certainty for the single choice.Footnote 4 The IA can have no role to play in the validity of the 1-in-1 protocol per se if we restrict choice to simple lotteries, but plays a defining role in the 1-in-K protocol. And the role that the IA plays in the theoretical and behavioral validity of the experimental payment protocol is quite distinct from the role that it might play in evaluating the actual binary choice or choices. Even with the 1-in-1 protocol being used, it is possible to ask if behavior is better characterized by violations of IA or not. Indeed, the whole point of our design is to highlight the dual role of the IA in 1-in-K protocols that seek to test violations of IA. Testing the manner in which the IA interacts with payment protocols used to collect data on observed choice behavior is complicated by the possibility that the reduction of compound lotteries (ROCL) axiom may be invalid behaviorally. This possibility lies at the heart of the theoretical response to the hypothesis about subject behavior underlying the stance of the Bipolar Behaviorist. If the objects of choice are themselves compound lotteries, as is the case in some famous experimental tasks such as the “preference reversal” experiments of Grether and Plott (1979), then one has to take a stand on the validity of ROCL anyway.Footnote 5 But if the objects of choice are simple lotteries, as here, then one can test the implications of RLIM for the standard alternatives to EUT without taking a position on the validity of ROCL.Footnote 6
 We offer direct tests of the effect of payment protocols on preferences for risk in general, and the evidence for probability weighting in particular. We do find statistically significant evidence for a difference in estimated risk preferences deriving from the use of different payment protocols and experimental tasks. Using choices over simple lotteries, we find evidence of RDU probability weighting with the 1-in-1 protocol that does not rely on the validity of the IA. So this result establishes that there is theoretical and behavioral “cause for concern” when one assumes the validity of the IA for the 1-in-K protocol. We then find that this theoretical concern is empirically relevant. Estimated RDU risk preferences are different depending on whether one infers them from data collected with the 1-in-1 payment protocol or the 1-in-K payment protocol. It is not the existence of evidence for probability weighting that is the issue, it is the fact that the nature of probability weighting differs in the 1-in-1 versus 1-in-K protocol. In order to justify the use of the 1-in-K payment protocol, many studies appeal to the “isolation effect.” This effect is often presented as a behavioral assertion that a subject views each choice in an experiment as independent of other choices in the experiment. When stated formally, the isolation effect is often expressed the same as the IA, and is indeed exactly the same as the IA in our choice context. We recognize that the isolation effect is often invoked informally as “an empirical matter,” with either an appeal to prior evidenceFootnote 7 or simply a conjecture that the isolation effect is a reasonable description human behavior. Given limited empirical support from prior studies and the tautology of support via conjecture, we present an experiment which provides a new test of the isolation effect. In Sect. 2 we describe the theoretical constructs needed for our design, in particular the various axioms that are at issue. In Sect. 3 we present our experimental design, which allows comparison of risk preferences obtained from choice tasks over simple lotteries that do not require the IA with risk preferences obtained from tasks that do require the assumption. We also explain why we focus on differences in estimated preferences across treatments rather than just examine raw choice patterns. In Sect. 4 we develop the econometric model used to estimate preferences. We pay particular attention to the manner in which between-subject heterogeneity is modeled. The reason for this attention is that the simplest way of avoiding reliance on the IA is to give some individuals only one choice to make, necessitating the pooling of choices across different individuals. In the absence of an assumption of homogeneity of risk preferences, or samples of sufficient power to allow randomization to mitigate the need for that assumption, we must address the econometric modeling of heterogeneity. In Sect. 5 we examine the data from our experiments, and present the econometric analysis of hypotheses. In Sect. 6 we draw some general implications of our results, and in Sect. 7 offer general conclusions.",44
77.0,4.0,Theory and Decision,21 January 2014,https://link.springer.com/article/10.1007/s11238-013-9409-9,Introduction to FUR XV special issue,December 2014,James C. Cox,Glenn W. Harrison,Vjollca Sadiraj,Male,Male,Female,Mix,,
77.0,4.0,Theory and Decision,12 December 2013,https://link.springer.com/article/10.1007/s11238-013-9410-3,Probabilistic risk attitudes and local risk aversion: a paradox,December 2014,Vjollca Sadiraj,,,Female,Unknown,Unknown,Female,"The first paradox to challenge expected utility theory was offered by Allais (1953). The Allais patterns violate the independence axiom, which gives the expected utility functional its idiosyncratic feature of linearity in probabilities. In order to evade the Allais paradox, theories of decision under risk that relax the independence axiom were developed (see Starmer 2000, for an accessible presentation). The idea of representing risk aversion with nonlinear probability transformations originated in the psychology literature about mid-twentieth century (Preston and Baratta 1948; Edwards 1954) and entered the economics literature in late seventies (Handa 1977; Kahneman and Tversky 1979). Some early models of probability weighting (Handa 1977; Kahneman and Tversky 1979) were shown to violate first order stochastic dominance. Subsequent models with rank dependence of prizes avoid that problem (Tversky and Kahneman 1992; Quiggin 1982). Further development and applications of rank-dependent models, and other alternatives to expected utility, have continued to the present. Wakker (2010) provides a comprehensive presentation of the literature including the use of rank-dependent probability in cumulative prospect theory. Rank-dependent utility models have been relatively successful in explaining several behavioral anomalies that have been observed in the laboratory and in the field which accounts for their recent widespread use in the field applications. This paper, however, is concerned with the implications of probabilistic sensitivity and rank dependence for risk aversion. I explain that attributing local (with respect to probabilities) risk aversion to attitudes towards probabilities can produce paradoxical risk aversion. This calls into question the ability of rank- dependent utility models to rationalize risk aversion on domains that preserve the size of payoffs (small prizes only or large prizes only). It also challenges these models’ ability to rationalize risk aversion on domains with mixed scale of payoffs (small prizes and large prizes).",5
77.0,4.0,Theory and Decision,10 December 2013,https://link.springer.com/article/10.1007/s11238-013-9407-y,The role of intuition and reasoning in driving aversion to risk and ambiguity,December 2014,Jeffrey V. Butler,Luigi Guiso,Tullio Jappelli,Male,Male,Male,Male,"In many situations individuals behave as if they are able to estimate the probabilities associated with specific outcomes and to calculate the expected payoff of their decisions. In other situations the probabilities of outcomes are unknown. Economists define as risk averse individuals who prefer a certain payment rather than a gamble with the same expected payoff, and as ambiguity averse individuals who would rather choose an option with fewer unknown elements than one with many unknown elements.Footnote 1 Individuals’ attitudes towards risk and ambiguity are prime candidates to explain behavior in financial markets. For instance, aversion to risk and to ambiguity can explain why people are reluctant to invest in stocks and therefore demand an equity premium (Epstein and Schneider 2010). In this paper, we undertake a systematic study of risk and ambiguity aversion, how they correlate with observable characteristics and how they correlate with each other. To make the two concepts operational, we use measures of attitudes towards risk and ambiguity from a sample of retail investors as well as from experimental evidence. We complement survey data drawn from a representative sample of Unicredit retail investors (the Unicredit Client Survey, or UCS) with corroborating experimental data.Footnote 2 The UCS survey contains detailed demographic and financial information as well as a section devoted explicitly to obtaining measures of attitudes towards risk and ambiguity. We find that individuals are heterogeneous along both dimensions and that their attitudes towards risk and ambiguity exhibit a common pattern: those who dislike financial risk are also more likely to dislike ambiguity. While this correlation has been documented before, we go further to show that attitudes toward risk and ambiguity can be traced back to the way individuals approach decisions. Research in psychology suggests that people rely on two modes of thinking when making decisions.Footnote 3 In the terminology of Stanovich and West (2000), the first mode of decision making (System 1) is intuitive thinking, while the second mode (System 2) is based on effortful reasoning and systematic processing of information. System 2 is calculative, analytical and controlled and involves systematic conscious comparisons of different alternatives. While such deliberative reasoning is slow, System 1 is quick, automatic and can even be unconscious.Footnote 4
 In the UCS we obtain information on individuals’ predispositions to rely on both decision modes. This allows us to classify respondents into three groups: those who rely mostly on intuition, those who use both intuition and reasoning, and those who rely predominantly on deliberative reasoning. We find that attitudes towards risk and ambiguity vary significantly with the way individuals make decisions. The survey shows that, relative to individuals who use both modes, those who decide predominantly using intuition are less likely to be averse to risk and ambiguity. We replicate this finding in two separate experiments involving over 1,300 participants from universities in Rome, using incentive compatible measures of risk and ambiguity aversion and an alternative behavioral measure of decision mode suggested by previous research (Rubinstein 2007; Achtziger and Alós-Ferrer 2012). One interpretation of our results is that intuitive thinkers have a comparative advantage in dealing with situations involving risk and uncertainty, as they can reach decisions more promptly and on the basis of less information—two factors which may be crucial for performance in many ambiguous situations. This conjecture was raised by Klein (1998, 2003) and indirectly supported by a handful of studies providing evidence across various types of decisions that effortful, deliberative, decision-making processes often yield lower quality decisions or predictions than intuition or its close cousins (see, e.g., Dijksterhuis 2004; Lee et al. 2009, Pham et al. 2012). We contribute to this burgeoning literature by providing field-based and experimental evidence in the specific context of decision making in highly ambiguous environments of the comparative performance advantage that reliance on intuition provides. For experimental evidence on this performance advantage, we invited a randomly chosen subset of participants in our first two experiments to participate in an Iowa Gambling Task (Bechara et al. 1994) involving 100 sequential choices under uncertainty. We found that participants who relied more on intuition performed significantly better in the task. For field-based evidence, we follow the trading strategies of our UCS sample of investors around the time of the 2008 stock market crash and show that investors who rely mainly on intuition were better able to time the market, exiting at a faster pace before the stock market crashed than deliberative thinkers. The remainder of the paper proceeds as follows. Section 2 describes our two main data sources: the UCS survey and corroborating experiments. Sections 3 and 4 focus on the UCS survey, describing our measures of risk and ambiguity aversion as well as decision mode before presenting our main empirical evidence of the effect of intuition and reasoning on preferences for risk and uncertainty. Section 5 discusses our experimental evidence corroborating the correlation between decision mode and risk and ambiguity preferences found in the survey data. Section 6 presents evidence on the performance advantage of intuitive thinking in the form of, first, results from an Iowa Gambling Task experiment and, second, behavior of investors around the time of the 2008 stock market crash. In Sect. 7, we relate our findings to the existing literature. Section 8 concludes.",34
77.0,4.0,Theory and Decision,04 February 2014,https://link.springer.com/article/10.1007/s11238-013-9406-z,Eliciting ambiguity aversion in unknown and in compound lotteries: a smooth ambiguity model experimental study,December 2014,Giuseppe Attanasi,Christian Gollier,Noemi Pace,Male,Male,Female,Mix,,
77.0,4.0,Theory and Decision,10 December 2013,https://link.springer.com/article/10.1007/s11238-013-9405-0,Relative performance of liability rules: experimental evidence,December 2014,Vera Angelova,Olivier Armantier,Yolande Hiriart,Female,Male,Female,Mix,,
77.0,4.0,Theory and Decision,17 December 2013,https://link.springer.com/article/10.1007/s11238-013-9408-x,Differences in cognitive control between real and hypothetical payoffs,December 2014,Ralf Morgenstern,Marcus Heldmann,Bodo Vogt,Male,Male,Male,Male,"An open question in experimental economics is how to verify observed behavior from an experiment in relation to behavior in the real world. It is a general goal in economic research to reduce biasing effects of a lab environment and its specific circumstances, such as the reward structure for decisions in experiments. Smith (1982) established the condition of “saliency” for microeconomic experiments, proposing a monetary reward function that varied payoffs with the responses of subjects. This “salience condition” provides a necessary condition for incentive-compatible reward structures. Subjects can be incentivized to respond truthfully according to their real preferences. In this context, a hypothetical reward structure does not satisfy saliency, and subjects are not incentivized to give truthful responses. For this reason, hypothetical decisions are viewed as unreliable even though a hypothetical reward structure may not change the basic decision direction in general. Camerer and Hogarth (1999, p. 17) stated that in “\([\ldots ]\) games, auctions, and risky choices the most typical result is that incentives do not affect mean performance, but incentives often reduce variance in responses. In situations where there is no clear standard of performance, incentives often cause subjects to move away from favorable ‘self-presentation’ behavior toward more realistic choices.” However, overall evidence that hypothetical choices differ from real choices is ambiguous. Studies by Kühberger et al. (2002) and Beattie and Loomes (1997) could not confirm a general difference between hypothetical and real choices. However, Slovic (1969) discussed differential effects in real and hypothetical payoffs, and the studies of Holt and Laury (2002); Holt and Laury (2005) found differences in risk attitudes for high-stakes lotteries. Holt and Laury (2002); Holt and Laury (2005) have shown that real decisions in high-stakes lotteries evoke more risk averse choice behavior compared to hypothetical decisions. The difference in risk attitude is ascribed to an incentive effect. This incentive effect was reviewed by Harrison (2006a). He examined studies by Battalio et al. (1990), Holt and Laury (2002), and Harrison et al. (2005) with reference to a hypothetical bias over uncertain outcomes. Harrison (2006a) confirmed a difference in the choice behavior of subjects between hypothetical and real decisions in those studies. These findings generally support the unreliability of hypothetical decisions. However, we also have to consider that there are special cases in which a realization of decision outcomes is not possible. For instance, outcomes related to questions of environmental damages, moral conflicts, losses, or very high stakes are often not realizable. In those cases, hypothetical decisions may still provide valuable information as good forecast indicators. This topic is discussed under the term “statistical calibration” (see Blackburn et al. 1994; Harrison 2006b). Although the behavioral effects induced by hypothetical bias are known, the factors causing this incentive effect have not been fully investigated. It can be assumed that differences in risk attitude are evoked by differences in the process of evaluation of a decision task. In standard behavioral experimental settings, preceding processes resulting in a decision cannot directly be observed. Because specific decision-related processes are known to have a neural correlate, event-related potentials (ERPs) derived from an electroencephalogram (EEG) can potentially reveal these hidden processes. The spontaneous EEG shows a signal originating from the brain’s fast oscillating electrical activity. This EEG is typically recorded from the scalp using a set of electrodes placed at standardized positions. ERPs are neural reactions embedded in the spontaneous EEG, and time-locked to motor, sensory, or cognitive events. They are characterized by their temporal appearance, their polarity, and the location of their appearance. Just a few processes result in ERPs, which can be observed in a single trial; most ERPs are extracted from the spontaneous EEG by way of standard averaging techniques. In comparison to functional magnetic imaging, the striking advantage of ERPs is the temporal resolution. Because ERPs reflect the fast changing electrical sum potential of neocortical neuron ensembles, they are able to differentiate between processes in the range of milliseconds (ms). In contrast, functional magnetic resonance imaging (fMRI), which relies on the relatively slow hemodynamic response to neural processes, has a temporal resolution of at least 1 s. In return, the spatial resolution of functional fMRI, used primarily for processes occurring in non-cortical/subcortical brain sites, is superior to an EEG’s spatial resolution. Here, we used ERPs that were time-locked to the presentation of a decision-requiring stimulus in order to reveal the hypothesized differences in cognitive processes taking place before a decision is made. In ERP research, the control of decisive behavior is indicated by a negative deflection at fronto-central electrode sites occurring approximately 200–300 ms after stimulus presentation. This ERP component, labeled as N2 or N200 because of its negative deflection at about 200 ms, is driven by activity in the fronto-medial part of the anterior cingulate cortex (ACC) and the prefrontal cortex (PFC). The N2 is assumed to reflect the neural underpinnings of cognitive control (Veen and Carter 2002; Folstein and Petten 2008). The latent construct “cognitive control” describes the ability of humans to control one’s own behavior and to adapt it to changing environmental demands in a flexible manner (Veen and Carter 2002; Wendt et al. 2007). Several investigations have shown that in decision tasks, cognitive control increases when subjects have to choose between two or more competing alternatives (Bland and Schaefer 2011; Neys et al. 2011). Accordingly, the N2 should be able to reveal differences in the neural underpinnings of processes related to decisions in a lottery task when comparing hypothetical against real payoffs. We applied a standard method for eliciting certainty equivalents in a binary lottery choice paradigm (see Farquhar 1984) in order to investigate the differences in risk attitudes between hypothetical and real decisions. Eliciting certainty equivalents for hypothetical and real high-payoff choices in a within-subject design allow for an analysis of a potential hypothetical bias. According to findings in the literature (e.g., Holt and Laury 2002), we expect a higher degree of risk aversion for real payoff choices compared to hypothetical payoff choices. Thus, we hypothesize that elicited certainty equivalents are smaller for real decisions. Moreover, the existence of a hypothetical bias in our data indicates that differences in the evaluation process of hypothetical and real choice tasks probably exist. As a consequence, we expect differences in the appearance of the N2 component, which reflect a neural correlate of an evaluation process during decision making, since the N2 component is affected by processes of cognitive control indicating a distinct level of conflict in the decision-making process. An assumption of differences in the appearance of the N2 has to be related to the expected level of conflict between hypothetical and real choices. An fMRI study by Kang et al. (2011) investigated hypothetical and real choices for consumer goods. Kang et al. revealed an increased activity in cognitive control areas for real choices. The authors conjectured that this finding could refer to a more careful comparison process between products and prices when real choices are made. Hence, the salience of real choices may lead to more careful decision making, which would result in a higher level of cognitive control. According to this implication for consumer goods, real choices in a lottery choice paradigm should also provoke a more careful comparison process. We also expect to observe a higher level of cognitive control for real decisions because these decisions are more salient for a subject. Thus, we hypothesize that higher N2 amplitudes will be observed for real choices.",6
78.0,1.0,Theory and Decision,27 December 2013,https://link.springer.com/article/10.1007/s11238-013-9404-1,A Bayesian model of Knightian uncertainty,January 2015,Nabil I. Al-Najjar,Jonathan Weinstein,,Male,Male,Unknown,Male,"
Knight (1921)’s idea of a fundamental difference between “measurable risk and an unmeasurable uncertainty” has generated both interest and controversy. Standard models in economics assume that agents use probabilities to quantify all uncertainties regardless of their source or nature. No distinction is drawn between actuarial and strategic risks, or between risks associated with repetitive versus singular events. Yet there is a compelling intuition that some probability judgments are less “obvious” or “objective” than others. Betting on events like the unraveling of the European Monetary Union or global warming seems qualitatively different from betting on the outcome of a coin toss or whether it rains tomorrow. The first type of event represents, in Knight’s words, unmeasurable uncertainties that should be treated differently from measurable risks. This paper suggests that a Bayesian framework can capture the distinction between situations with known probabilities, or “risk,” and Knightian uncertainty where objective probabilities are unknown.Footnote 1 We illustrate the distinction in a consumption–saving model where uncertainty is represented by the agent’s subjective belief about the parameter governing future income. We relate the impact of uncertainty to the agent’s ability to intertemporally smooth consumption and show that this leads to precautionary behavior that may appear overly conservative to an outside observer. We also point to the potential tension between subjective uncertainty and empirical methods that use rational expectations for econometric identification. Consider an agent with time-separable utility over (finite or infinite) consumption streams. The agent receives an i.i.d.income stream with unknown parameter \(\theta .\)
Footnote 2 Uncertainty is lack of knowledge represented by a prior belief \(\mu \) over \(\theta .\) Uncertainty does not necessarily lead to measurable consequences on behavior. For example, if uncertainty is defined on consumption streams directly, then discounted expected utility is unaffected by replacing the uncertain belief \(\mu \) by a parameter with the same marginal as \(\mu .\) In this case, uncertainty reduces to ordinary risk. The role of uncertainty is manifested in the indirect utility the agent derives from income streams when he has (perhaps limited) freedom to save or borrow. We first illustrate this point in a simple setting where consumption occurs after an initial phase of payoff accumulation. For example, a retirement portfolio generates dividends each period, but the agent cares only about its value at the time of retirement. Another example is a start-up that accumulates gains and losses over a period of time, but whose value is realized only when the entrepreneur sells the firm. In these examples, separating consumption from payoff accumulation simplifies the calculation of indirect utility, making the impact of uncertainty transparent and striking. We then turn to a richer consumption–saving problem where consumption and wealth can change as uncertainty about income resolves. Using exponential utility for tractability, we derive the evolution of consumption and show that uncertainty results in precautionary behavior that may appear overly conservative to an outside observer. Consumption is more volatile under uncertainty because the agent perceives short-run income variations to be potentially informative about his long-run income prospects. Under risk, by contrast, all income realizations are viewed as transitory. A Bayesian framework represents any lack of knowledge in terms of probabilities. What then justifies treating uncertainty differently from other situations of imperfect information? Our main point is that the key difference between risk and uncertainty is that uncertain beliefs are not empirical. Section 4 introduces two arguments to support this point. First, we formalize the intuition that the probabilities of uncertain events are subjective opinions about which, in Keynes’ words, “there is no scientific basis to form any calculable probability whatever.” We capture this intuition using statistical tests that compare agents’ subjective beliefs with the actual sequence of realized outcomes. Consider asymptotic tests for simplicity. A natural property to require in such tests is to be free of Type I error: if the agent knows the true probabilities, he must pass the test almost surely. Proposition 4.1 says that an agent who is uncertain about the true parameter, and who has subjective belief \(\mu ,\) must also believe that there is an alternative belief \(\mu '\not =\mu \) such that no Type I error free test could reject \(\mu '\) regardless of the amount of data used. Bayesian agents assign probabilities to all events, whether risky or uncertain. What distinguishes beliefs about uncertain events is that they cannot be objectively tested to determine whether they are right or wrong. By contrast, it is easy to test beliefs under risk by comparing them with the observed frequencies. The second sense in which uncertain beliefs are not empirical concerns the difficulty of estimating their impact using standard econometric methods. Beliefs influence decisions regardless of whether they reflect risk or uncertainty. But since beliefs are not directly observable, econometric identification assumptions are needed to recover them from data. A standard assumption is rational expectations which identifies beliefs with the observed empirical frequencies. While this assumption offers considerable advantages, it also rules out subjective model-uncertainty as a factor in decisions, a point made by Weitzman (2007) among others. In Sect. 4 we suggest that the perceived failure of equilibrium models to capture Knightian uncertainty may have more to do with the use of rational expectations in their econometric estimation than with the Bayesian rational choice paradigm. There is a growing interest in the economic role of Knightian uncertainty.Footnote 3 One motivation is the discrepancy between the observed behavioral patterns (e.g., in asset prices) and the predictions of models where agents are assumed to know the true data generating process. Introducing uncertainty about fundamentals is a natural way to bring models closer to reality. Pástor and Veronesi (2009) survey asset pricing anomalies that could be explained with the introduction of uncertainty about fundamental parameters. They conclude that “[m]any facts that appear baffling at first sight seem less puzzling once we recognize that parameters are uncertain and subject to learning.” Hansen and Sargent (2001) discuss the importance of model mis-specification and parameter-uncertainty in macroeconomic modeling. Connections to these works are discussed in greater detail in Sect. 4. Two seminal papers, Gilboa and Schmeidler (1989) and Bewley (1986), formalize the concept of Knightian uncertainty as lack of full Bayesian belief. In Bewley (1986), uncertainty is modeled as an incomplete ranking over acts. Gilboa and Schmeidler (1989)’s ambiguity averse agents use a maxmin criterion with respect to a set of priors to incorporate caution in their decisions. Both approaches focus on the typical pattern of choices in static Ellsberg experiments as a key behavioral manifestation of uncertainty. The present paper argues that a distinction between risk and Knightian uncertainty can be made within the Bayesian framework. This point of view follows a number of authors, including Halevy and Feltkamp (2005) and Weitzman (2007), who pursue Bayesian approaches to uncertainty. LeRoy and Singell (1987) suggest that such approach can be traced to Knight (1921)’s original work, noting that “Knight shared the modern view that agents can be assumed always to act as if they have subjective probabilities.” Similarly, Keynes (1937) writes that, even in situations of uncertainty, “the necessity for action and for decision compels us [..] to behave exactly as we should if we had [...] a series of prospective advantages and disadvantages, each multiplied by its appropriate probability, waiting to be summed.” Knight and Keynes, writing decades before modern subjective expected utility theory, seemed to believe that decision making under uncertainty is not necessarily in conflict with probabilistic reasoning. We find it useful to distinguish Knightian uncertainty from ambiguity aversion. We take uncertainty to mean probabilities that cannot be objectively measured, an intuition we formalize in terms of statistical tests. Ambiguity aversion, on the other hand, refers to non-probabilistic beliefs, exemplified by the static Ellsberg choices. Although both lead to precautionary behavior, there are profound differences. In a Bayesian model, the implications of uncertainty appear in connection with intertemporal choice and the constraints on consumption smoothing. In static settings, such as Ellsberg’s choices, risk and uncertainty are indistinguishable. This is consistent with Knight (1921)’s view: “when an individual instance only is at issue, there is no difference for conduct between a measurable risk and an unmeasurable uncertainty. The individual [...] throws his estimate of the value of an opinion into the probability form of ‘a successes in b trials’ [...] and ‘feels’ toward it as toward any other probability situation.” In modern Bayesian language, agents care only about the prizes they receive, not whether they were the result of risk rather than uncertainty. The outline of the rest of the paper is as follows. Section 2 introduces the basic setup and defines the uncertainty premium. This section also introduces a very simple model where the value function can be easily computed and the impact of uncertainty is obvious. This simple model is inspired by Halevy and Feltkamp (2005), which we discuss in detail below. Section 3 introduces a more complete consumption–saving model and derives the stochastic laws of consumption under risk and uncertainty. Uncertainty leads to precautionary behavior and greater sensitivity to information. Section 4 discusses the empirical implications of uncertainty. We begin with a simple argument illustrating that uncertain beliefs are not testable, then discuss the potential tension between uncertainty and rational expectations econometrics. We discuss in detail the relationship of our work to Weitzman (2007) and Cogley and Sargent (2008).",12
78.0,1.0,Theory and Decision,27 December 2013,https://link.springer.com/article/10.1007/s11238-013-9412-1,Collectively ranking candidates via bidding in procedurally fair ways,January 2015,Werner Güth,,,Male,Unknown,Unknown,Male,"Institutional design in the private as well as in public sphere can be consequentialistic, e.g., in the sense of trying to achieve the best allocation outcome. Such consequentialism requires information allowing to predict allocation outcomes, specifically about how these are evaluated by those involved. Since such information is usually private, one either tries to guarantee incentive compatibility in weakly dominant strategies or to apply the revelation principle (see for example, Myerson 1979). However, the former is often impossible or suffering from serious difficulties like inviting unwarranted collusion and the latter presupposing the common knowledge assumptions of game theoretic equilibrium analysis and not at all practically implementable (see Güth 2014). Here, we therefore rely on institutional design via guaranteeing desirable properties like the “one person, one vote” principle in democratic politics. The basic idea is to avoid any arbitrary (dis)favoring of any of the concerned parties according to interpersonally comparable and objectively verifiable characteristics. Rather than asking the concerned parties, e.g., individuals or groups, to vote, they are required to submit monetary bids which are obviously interpersonally comparable and objectively verifiable. Non-discrimination is required with respect to these monetary bids rather than according to the idiosyncratic and usually at best privately known true evaluations. Like the “one person, one vote” principle, such institutional design, based on regulatory or procedural fairness, relies on a basic equality postulate. Before discussing the collective action problem, considered in this study, namely collective ranking of candidates, let us note that the basic approach is also applicable to collective provision of goods and services (Güth and Kliemt 2013), but also to private dealings (Güth 2011). Like the mechanism proposed here also these institutional arrangements are already experimentally tested and surprisingly functional (Güth et al. 2014). When facing several candidates we often engage in ranking them. Examples are sports contests like the Olympics, where the task is easy when there exists some numerically measurable success like distance, speed, weight, etc. But there are other sports like gymnastics where different evaluators may rank differently, and where collective ranking depending on divergent individual rankings poses quite a challenge. This is also true for competitions in the arts etc., where one does not only care who wins but also for the entire ranking of candidates. Similarly, in national concourses of candidates, e.g., in France or Italy when hiring young professors, one may not only want to find out who is first but also cares for the entire ranking of all candidates. To have something more specific in mind think about a group of individuals who have to agree on some program which they will jointly experience, e.g., a work group performing some boring task which they try to render more enjoyable by listening to music.Footnote 1 Each individual would like the most preferred type of music most of the time. However, the collective ranking distributes the shares such that the collectively most preferred music is played longer than the collectively rated second best etc. What is analyzed is how voters by submitting monetary bids do not only determine the top listed candidate but also the entire collective ranking. If evaluators care for the whole collective ranking, it makes sense to incentivize them by making them “pay” for whatever they choose. Here “pay” may also mean to be monetarily compensated when having to accept a collective ranking which one does not like at all. We therefore assume that all evaluators submit a bid for all possible rankings of candidates, i.e., each evaluator \(j\) submits a list \(b_j\) of monetary bids, one for each possible collective ranking. The rules have then to determine for all possible vectors of individual lists or bid vectors the collective ranking and the individual “payments” implied by this bid vector and its resulting collective ranking. Using similar axioms as in Güth (2011) and Güth and Kliemt (2013), we derive the rules solving these two tasks for all possible bid vectors. Due to the non-discriminatory nature of these rules, we refer to them as a procedurally fair mechanism to collectively rank candidates. “Incentivizing” assumes some transferable commodity like money and is not to be misunderstood to claim incentive compatibility.Footnote 2 Actually, what the three axioms determine is the game form assigning the collective ranking and the individual payments for all possible bid vectors. This is in line with legal institutional design, where one determines legal rulesFootnote 3 applying to many, mostly unknown cases. To yield a well defined game for a specific situation, one has to add the true evaluations of the interacting parties as well as their beliefs concerning the values of others. Satisficing in addition to the three axioms incentive compatibility in the sense of dominance solvability (truthful bidding is weakly undominated) is usually impossibleFootnote 4 and will therefore be neglected. The revelation principle (see Myerson 1979) avoids impossibility but requires the common knowledge assumptions of game theoretic equilibrium analysisFootnote 5 that questions its applicability to unknown future situations. Section 2 presents the requirements allowing to derive the procedurally fair mechanism. The example of a collective ranking task, presented in Sect. 3, has already been explored experimentally (Güth et al. forthcoming). In Sect. 4, we describe how to extend our approach to situations, where one endogenously wants to determine which sets of candidates are acceptable and collectively rank only the acceptable candidates. Section 5 concludes this paper.",
78.0,1.0,Theory and Decision,27 September 2013,https://link.springer.com/article/10.1007/s11238-013-9399-7,A note on equivalent comparisons of information channels,January 2015,Luís Fernando Brands Barbosa,Gil Riella,,Male,Male,Unknown,Male,"
Nakata (2011) presents a model of acquisition of information where the agent does not know what pieces of information she is missing. His results are heavilly based on Dekel et al. (2001)—henceforth DLR. In this note, we point out some problems in a few of his results and show how to correct them. In the next section, we present the formal setup and introduce the main representation concepts we are going to work with. In Sect. 3, we discuss the problem in Nakata’s Theorem 1. We observe that the continuity axiom used by Nakata is not strong enough to imply the desired representation. In fact, we give an example that shows that Nakata’s axioms are not sufficient for the relation to have a representation by a utility function. Later, we show how to strenghen Nakata’s continuity axiom in order to obtain the desired representation. In Sect. 4, we argue that Nakata’s Theorem 2 also suffers from the same problem as his first theorem. This problem can be fixed the same way we fixed the first result. In Sect. 5, we discuss Nakata’s Theorem 3, which deals with Monotone Additive EU representations. Again, this result suffers from the same problem we have pointed out above. However, in this case it is not enough to strengthen the continuity axiom. We show through an example that even if we strengthen Nakata’s continuity axiom the same way we did before we still do not obtain the desired representation. We then present an additional postulate and show that when we add it to the other axioms we obtain the correct version of Nakata’s Theorem 3. Finally, in Sect. 6, we introduce an additional postulate that guarantees that the sets of information nodes in Theorems 2 and 3 are all finite.",
78.0,1.0,Theory and Decision,29 September 2013,https://link.springer.com/article/10.1007/s11238-013-9400-5,Cooperation and signaling with uncertain social preferences,January 2015,John Duffy,Félix Muñoz-García,,Male,Male,Unknown,Male,"A large body of experimental evidence suggests that many individuals exhibit concerns for fairness in the income distribution, a characteristic that is also referred to as “social” preferences. Fehr and Schmidt (1999) and Bolton and Ockenfels (2000), among others, have provided models where the incorporation of such social preferences can help to explain experimental findings, in particular, greater-than-predicted “cooperative” behavior, that would be difficult to rationalize under standard, “selfish” preferences. This paper contributes to the literature on the role of social preferences in fostering cooperative behavior by studying settings where players have incomplete information about other players’ social preferences, specifically, incomplete information about their degree of inequity aversion. Surprisingly, most theoretical analyzes of social preferences have been developed in complete information settings, where players can perfectly observe one another’s concerns for fairness, e.g., the extent of their inequity aversion. While such an assumption may be reasonable in contexts where players have interacted with one another for several previous periods, complete information about other players’ social preferences seems less sensible if players are unfamiliar with the strategic environment, or have had no prior interactions with their opponents, a situation that characterizes the initial round(s) of play of many experimental games. We study incomplete information about other’s social preferences by considering signaling games, where players interact in either the simultaneous- or sequential-move versions of the prisoner’s dilemma (PD) game.Footnote 1
 We first investigate the role of information transmission in the twice-repeated version of the simultaneous-move PD game. We show that no separating strategy profile can be sustained in equilibrium, thereby limiting the information transmitted about players’ social preferences. By contrast there always exists a pooling equilibrium where the uninformed player is unable to distinguish the type of his opponent from the initial period action choice. If priors are sufficiently high, the uninformed player is attracted toward cooperation in the second and final period, at which point an opponent with low concerns for fairness takes the opportunity to defect, i.e., he “stabs the uninformed player in the back.” (We refer to this strategy profile as the “backstabbing” equilibrium.) Interestingly, this equilibrium provides an explanation for a relatively common observation in experimental settings, wherein subjects defect in the last period of their interactions, despite a previous history of cooperation as shown, e.g., in Selten and Rolf (1986) and Andreoni and Miller (1993) for the PD game, McKelvey and Palfrey (1992) for the centipede game, Camerer and Weigelt (1988) and Brandts and Figueras (2003) for the borrower-lender game, and Anderhub et al. (2002) for the finitely repeated trust game.Footnote 2 Importantly, this informational explanation, does not rely on subjects’ inability to understand the rules of the game, or a failure to backward induct, but rests instead on the existence of incomplete information about other players’ social preferences. We further demonstrate that in the backstabbing equilibrium of our incomplete information set-up, the payoffs of both the defecting player and the player suffering the defection, are actually greater than in the equilibrium under complete information. Therefore, the introduction of incomplete information entails a Pareto improvement under certain conditions, thus implying that all individuals should prefer interacting in incomplete rather than complete information settings. 
Healy (2007) identifies a similar pooling equilibrium in the context of a finitely repeated gift-exchange game where the firm manager does not observe the worker’s type.Footnote 3 To facilitate a comparison of our results with those in Healy (2007), we modify the above signaling game in order to make it strategically equivalent to the gift-exchange game.Footnote 4 We show that a pooling equilibrium also emerges in the sequential version of that game. We demonstrate, nonetheless, that this “backstabbing” equilibrium can be supported under different parameter conditions in the simultaneous- and sequential-move versions of the PD game, depending upon the first mover’s concern for fairness, which provides a set of testable results in controlled experiments of the simultaneous and sequential PD game.",
78.0,1.0,Theory and Decision,30 November 2013,https://link.springer.com/article/10.1007/s11238-013-9402-3,Properties based on relative contributions for cooperative games with transferable utilities,January 2015,Yoshio Kamijo,Takumi Kongo,,Male,Male,Unknown,Male,"Efficiency and fairness (or equity) are important criteria when allocating something among multiple agents. In cooperative games with transferable utilities, efficiency is a usual requirement for the worth of the total cooperation to be fully allocated to existing players. While, fairness is considered in various manners, a kind of fairness requires that the players who make equal contributions should receive equal payoffs. This requirement is called symmetry. In symmetry, we focus on a player’s contribution to each coalition and measure it as the worth generated by the player together with the coalition; that is, the difference in the worth of the coalition with and without that player. This difference is called the player’s marginal contribution to the coalition, and symmetry requires that two players whose marginal contributions are equal for any coalition that does not contain the two players be treated in the same manner with respect to their payoffs. Hence, symmetry is often called the “equal treatment of equals.” Equal treatment of players with respect to payoffs based on players’ marginal contributions is also considered in the marginality introduced by Young (1985) in a different manner. This property requires that if a player’s marginal contribution to any coalition is the same for two different games, then that player’s payoff too should be the same in both games. Symmetry and marginality are logically independent. However, both are based on players’ marginal contributions to coalitions. In Myerson (1980), another kind of players’ marginal contributions is discussed. In Myerson’s balanced contributions property (BC), we consider the marginal contribution of one player to any other player in the game. This is determined by the difference between player’s payoffs in the original game and in the game where the first player leaves. BC requires that for any pair of players, a player’s marginal contribution to the other player in a game should be equal to the other player’s marginal contributions to the player. Later, Kamijo and Kongo (2010) examines a milder condition than BC. In their new property, contributions between two players may no longer be equalized, however, contributions among all players are equalized in a cyclical manner. This new property is called the balanced cycle contributions property (BCC). Although contributions to coalitions and to players are different, the above kinds of fairness-related properties are based on players’ marginal contributions (to coalitions/other players). Unlike the above marginal contributions approach, Ortmann (2000) adopts an alternative approach to capture players’ contributions to other players. He defines the relative contributions of one player to any other player not by the difference in payoffs in the original game and in the game where the player leaves, but by the ratio of the former to the latter. Ortmann (2000) also requires the balancedness of relative contributions for any pair of players in a game. As illustrated by Ortmann (2000, p.231) this requirement is a principled approach when we generalize the “proportional division of surplus” between two persons to that among three or more persons. In addition, Ortmann’s property can be viewed as a multiplicative variation of Myerson’s property, and hence, can also be interpreted as a kind of fairness condition. Ortmann’s new property characterizes the proportional value (Ortmann 2000; PV) which is a counterpart of the Shapley value (SV) characterized by the abovementioned Myerson’s property. In this paper, following Ortmann’s approach, we consider players’ contributions to coalitions/other players in a relative manner. We first consider relative versions of symmetry and marginality, and study the relationships between efficiency and such kind of fairness property. Unlike the case of Ortmann (2000), replacing properties based on players’ marginal contributions to coalitions with players’ relative contributions to coalitions does not lead to corresponding characterizations of PV. Rather, the relative version of marginality is incompatible with efficiency for positive games. Next, we consider a corresponding relative version of Kamijo and Kongo’s (2010) BCC property. It can be seen as a generalization of Ortmann’s fairness condition. In particular, we require the balancedness of “relative” contributions in a cyclical manner. With results parallel to those of Kamijo and Kongo (2010); Kamijo and Kongo (2012), we establish new axiomatizations of the proportional and of the egalitarian value (the equal division of the worth of all players’ cooperation; EV) in a unified manner and provide an impossibility result. The rest of the paper is structured as follows. Section 2 provides the definitions and notation. Section 3 studies properties based on players’ relative contributions to coalitions. Section 4 examines properties based on players’ relative contributions to other players. Section 5 mentions some remarks.",4
78.0,1.0,Theory and Decision,12 February 2014,https://link.springer.com/article/10.1007/s11238-013-9403-2,Catastrophe insurance equilibrium with correlated claims,January 2015,Radoslav S. Raykov,,,Male,Unknown,Unknown,Male,"In 1981, Hoy and Robson (1981) raised the question whether insurance can be a Giffen good violating the law of demand. This paper revisits the same topic, but from a different angle: the angle of catastrophe insurance. Catastrophe insurance differs from regular insurance in that individual claims are correlated and insurers have to pay more clients at once, thereby often facing a large liquidity strain. A case in point is the aftermath of hurricane Katrina in 2005. After the hurricane caused insured damages in excess of $40 billion, many insurance companies refused to pay claimants, resulting in one of the largest insurance-related class-action lawsuits in the US.Footnote 1 In this paper, I show two related findings: first, that when customers know their risks are correlated, this correlation can cause positive-sloping demand at low prices, and second, that because of this, a catastrophe insurance market can fail. Market failure is a stable equilibrium in this model, which provides a better understanding of the frequent failures in this type of insurance market. The traditional literature ascribes such market failures mostly to the supply-side (nobody is selling), but in contrast, this model shows that an insurance market can also fail because nobody is buying, even when prices are low. The intuition is that when consumers recognize the liquidity pressures facing insurers and use the price as a signal of solvency, competitive price-cutting strategies may backfire and instead of increasing demand, reduce it. One historical example is the nineteenth-century US market for fire insurance. Before the advent of modern fire-fighting technology, fire insurance was effectively a form of catastrophe insurance: urban fires used to spread quickly and affect many adjacent properties, often inflicting damage comparable to that from a natural disaster. When fire insurers were first permitted to operate in more than one state, competition between incumbent firms and new entrants drove prices below average cost, leading to several large bankruptcies that severely impaired the functioning of the market (Cummins and Venard 2007). A more recent example of an underpriced insurance product are AIG’s credit default swaps. A credit default swap (CDS) is a derivative that functions like an insurance policy against a company defaulting on its debt. In a recession, company defaults are correlated, thus making a CDS very similar to catastrophe insurance. By the mid-2000s, selling credit default swaps had become a profitable business with a large market volume and an appetite for competition.Footnote 2 According to academics and Wall Street analysts, AIG credit default swaps were underpriced relative to the true risk involved (Adamo 2009; Sjostrom 2009). Similar to the US fire insurance example, underpricing combined with the correlation of credit defaults resulted in large losses ($ 32.4 billion from January 2007 through September 2008).Footnote 3 Soon after the underpricing became apparent, AIG’s credit default swaps sales collapsed regardless of the attractive pricing, resulting in a low-price, low-demand situation. AIG’s credit rating was also downgraded three notches by S&P, resulting in an inability to access capital markets while triggering $20 billion worth of extra collateral calls (Sjostrom 2009, p. 962). The framework presented here accounts for low-price, low-demand equilibria associated with a high chance of default, and shows that they arise naturally when buyers are aware of the correlation of risks.",5
78.0,1.0,Theory and Decision,24 December 2013,https://link.springer.com/article/10.1007/s11238-013-9411-2,An axiomatization of Choquet expected utility with cominimum independence,January 2015,Takao Asano,Hiroyuki Kojima,,Male,Male,Unknown,Male,"A lot of work has been done since Ellsberg (1961) casts doubts on the validity of the Subjective Expected Utility (henceforth, SEU) theory axiomatized by  Savage (1972). One of the seminal works is Schmeidler (1989) that shows that the decision maker’s beliefs are captured by a capacity, and her preferences are represented by the Choquet integral if she satisfies a set of axioms. As a related paper, Schmeidler (1986) proposes the notion of comonotonic additivity and provides some characterization of comonotonic additive operators. The notions of comonotonic independence and comonotonic additivityFootnote 1 are a key to deriving the axiomatization theorem in Gilboa and Schmeidler (1989). The comonotonic independence axiom that is weaker than the independence axiom in Anscombe and Aumann (1963) enables us to overcome Ellsberg’s paradox and to represent the decision maker’s preferences in the abovementioned way. As a generalization of Schmeidler (1986), Kajii et al. (2007) propose the concept of cominimum additive operator that is weaker than the concept of additive operator but is stronger than the concept of comonotonic additive operator considered by Schmeidler (1986)Footnote 2 and provide some characterization of cominimum additive operators. However, two problems remain to be solved. First, Kajii et al. (2007) do not provide axiomatizations of the decision maker’s preferences. From a normative point of view, it is intriguing to shed some light on cominimum additivity, which illuminates the meaning of cominimum additivity in decision theory. Second, since Kajii et al. (2007) analyze only a finite state space, the framework discussed in Kajii et al. (2007) is not suitable for axiomatizations of simple acts comparable to the ones introduced by Schmeidler (1989). Moreover, analyses in an infinite state space are technically difficult. Therefore, the analysis of Kajii et al. (2007) within an infinite state space is interesting not only from a mathematical viewpoint but also from the point of view of decision theory. The purpose of this paper is to generalize Kajii et al. (2007) into an infinite state space and to provide a new axiomatization of simple acts based on the notion of \({\mathcal {E}}\)-cominimum independence within the framework of the Choquet expected utility (henceforth, CEU) theory.Footnote 3
 Our axiomatization theorem can be also interpreted as an axiomatization theorem of totally monotone games. Although only a subset of the set of all totally monotone games is axiomatized, our axiomatization theorem enables us to derive Gilboa (1989) and Eichberger and Kelsey (1999) as a corollary. Furthermore, we show that our axiomatization theorem within CEU can be related to multi-prior expected utility (or maxmin expected utility; henceforth, MEU) through the core of a capacity.Footnote 4 In addition to the existence of the core, our result clarifies how the core can be represented. The organization of this paper is as follows. Section 2 provides motivations and examples. Section 3 provides the notions of cominimum additivity and cominimum functions. Section 4 presents the axioms and the main result of this paper. Section 5 extends the result of Sect. 4. Section 6 concludes this paper. Proofs of Lemmas 1 and 2 and of Theorem 3 are relegated to the Appendix section.",3
78.0,1.0,Theory and Decision,02 February 2014,https://link.springer.com/article/10.1007/s11238-014-9415-6,The lattice structure of the S-Lorenz core,January 2015,Vincent Iehlé,,,Male,Unknown,Unknown,Male,"The Lorenz criterion is widely accepted to compare profiles of revenues on the ground of egalitarianism. A profile Lorenz dominates another if its cumulative distribution, from rich to poor, is lower than the other one. But, the use of that rule as a social value may be conflicting with the maximizing behavior of agents. The cooperative game theory provides an appropriate framework to assess whether individual interest and egalitarianism can be accommodated together (see, e.g., Arin and Feltkamp 2002; Arin and Inarra 2001; Dutta 1990; Dutta and Ray 1989, 1991; Hougaard et al. 2001; Jaffray and Mongin 2003; Llerena et al. 2008; Roth et al. 2005) and complements the classical analysis of inequality measurement (see, e.g.,Atkinson and Bourguignon 1982; Dasgupta et al. 1973; Kolm 1977).Footnote 1
 We consider here the solution concepts defined by Dutta and Ray (1989, 1991) in cooperative games with transferable utility (TU games). The root concept is given by the notion of Lorenz core, defined recursively, that embodies egalitarianism and maximizing behavior together with the property of robustness against credible multilateral deviations of coalitions of players. The Lorenz core is defined in two manners depending whether the weak or strong domination relation is chosen in the definition of deviation (W-Lorenz core and S-Lorenz core in the remainder). The central solution in the analysis of Dutta and Ray (1989, 1991) is given by the Constrained Egalitarian Allocation, also defined in two manners according to the chosen relation. A WCEA (resp. SCEA) is a Lorenz-undominated allocation in the W-Lorenz core (resp. S-Lorenz core). Hence, the distinction between the two definitions of a constrained egalitarian allocation is tight. However, it leads to contrasted results. The sets of WCEAs and SCEAs are not comparable in general (see Dutta and Ray 1991,for a list of examples). Both solutions have also different qualitative properties. For the WCEA, Dutta and Ray (1989) find that the solution is unique, while its existence is not granted, except for convex games. In addition, they show that the solution is the Lorenz greatest allocation of the core in convex games, but not necessarily the greatest of the W-Lorenz core. For the SCEA, Dutta and Ray (1991) find that the solution exists under the mild assumption of weak superadditivity but is not unique in general. The outcome is, therefore, very sensitive to the choice of the domination criterion, and, in practice, the trade-off between uniqueness and existence may lead to a dilemma. We complement some of the findings of Dutta and Ray (1991). Our conclusions derive from an abstract (though not difficult!) result which underpins the structure of the S-Lorenz core. For any TU game and any ranking of players, the set of all preimputations compatible with the ranking, equipped with the Lorenz order, is a bounded join semi-lattice. Furthermore, the set admits as sublattice the S-Lorenz core intersected with the region compatible with the ranking (Theorem 2). The result allows to compare easily S-Lorenz core allocations and to construct new allocations that Lorenz dominate others in the S-Lorenz core. As an immediate corollary, we obtain that the set of SCEAs is either empty or a singleton on each rank-preserving region (Corollary 1). As a by-product result, we also obtain that the set of SCEAs is finite, and what is more, the cardinality of the set can be made more precise according to the location of some SCEAs (Corollary 2). The underlying property beyond these results is that each SCEA is the Lorenz greatest element of the S-Lorenz core on the rank-preserving region of the allocation. The note is organized as follows: Sect. 2 is devoted to notations and basic definitions; Sect. 3 presents the lattice structure result and its corollaries; in Sect. 4, we discuss briefly the implications of the lattice structure property; the proof of our main result, Theorem 2, is postponed to Appendix.",1
78.0,1.0,Theory and Decision,11 July 2013,https://link.springer.com/article/10.1007/s11238-013-9389-9,Gender differences when subjective probabilities affect risky decisions: an analysis from the television game show Cash Cab,January 2015,Matthew R. Kelley,Robert J. Lemke,,Male,Male,Unknown,Male,"The risk literature has long been interested in how decisions are made under uncertainty, and how those decisions differ across the sexes. In the present study, the television show Cash Cab is used as a natural experiment to investigate decision making under uncertainty when the probability of success is unknown and the decision maker has an opportunity to update her subjective probability of success. To our knowledge, this paper is the first to explore subjective updating within the context of a game show. The literature largely agrees that women are more risk averse than men (e.g., for surveys, see Byrnes et al. 1999, Eckel and Grossman 2005). Booij et al. (2010), Fehr-Duda et al. (2006), and Jianakoplos and Bernasek (1998) find women to be more risk averse than men when making financial decisions. Cohen and Einav (2007) demonstrate similar gender differences in risk preferences when looking at choices of car insurance deductible amounts, and Hersch (1996) shows that women are less likely than men to engage in risky social behavior such as smoking and not wearing a seat belt. Two reasons are commonly given for this difference in risk aversion across the sexes: (1) men and women value payoffs differently, and (2) men and women differ in how they evaluate or process probabilities. Of these two, there is general consensus that the second is more important than the first. Moreover, Bruhin et al. (2010) show that women’s weighting curves of probabilities are more nonlinear than men’s (e.g., women seem to over-estimate small probabilities), which results in differing assessments of the expected value of a gamble. Borghans et al. (2009), however, argue that the difference in probability assessment across sexes diminishes as the uncertainty of the gamble increases (called ambiguity aversion by Ellsberg 1961). Beginning with Gertner (1993), game shows have been used to investigate decision making under uncertainty. Game shows offer two main benefits: payouts are substantially larger than what can usually be offered in a laboratory environment, and most game shows involve straightforward, simple probabilities. Cash Cab, however, is different. While answering a series of general knowledge questions (ungrouped by category), contestants on Cash Cab can learn about their ability to answer the types of trivia questions asked on the show. At the end of each game, a successful contestant is presented with a gamble: take her winnings (which average more than $850) and leave the show, or risk her winnings double-or-nothing on one last trivia question (topic not indicated). When presented with this gamble, a contestant has the opportunity to assess her subjective probability of being able to answer the question correctly, as she decides to either accept or reject the final gamble. To preview, the results of the present study show clear differences across the sexes in their willingness to risk their winnings on the double-or-nothing gamble. In general, men are much more likely than women to accept the gamble. We also find that contestants appear to update their subjective probabilities based on the confidence of their previous answers rather than relying solely on whether their previous answers were correct. Most importantly, however, men and women seem to focus on different aspects of their game play when considering the final gamble. Female contestants are less likely to accept the gamble if they were previously asked a question for which they had no confidence in their answer while male contestants show no such tendency. Furthermore, female contestants appear to consider how confident they were in correctly answering their more recent questions (which according to Cash Cab are the hardest), whereas male contestants consider their confidence in all of their previous questions, including the easier ones. Given the structure of Cash Cab, therefore, women appear to condition their decision making on the most relevant data available to them, whereas men tend to consider less relevant information when updating. These results run counter to previous findings that women are more likely than men to form biased assessments of probabilities. The paper is organized as follows. In the next section, we describe the television game show Cash Cab. Following that, we detail the data collection methodology and report the descriptive statistics. Section 4 discusses how decisions made on trivia game shows potentially rely on subjective probabilities and the updating of those probabilities. The empirical results are presented in Sect. 5 and the general discussion is Sect. 6.",7
78.0,2.0,Theory and Decision,21 January 2014,https://link.springer.com/article/10.1007/s11238-013-9413-0,Risk-neutral equilibria of noncooperative games,February 2015,Robert Nau,,,Male,Unknown,Unknown,Male,"Game theory occupies the large middle ground of rational choice theory: the problem of “2, 3, 4... bodies” in which agents must reason about the strategic behavior of the other rational agents as well as reflect on their own preferences and compete in markets. The modeling of interactive decisions of this kind requires strong assumptions. First, the rules of the game are (in the most general case) parameterized in units of von Neumann–Morgenstern utility rather than money or material goods in order to allow for differences in tastes and attitudes toward risk. Second, the utility payoffs of different players are assumed to be common knowledge, enabling them to model each other’s decisions as well as their own, and to all know that they can all do this, and so on. Third, common knowledge of rationality and common knowledge of the rules of the game are assumed to lead to an equilibrium, usually a Nash equilibrium or one of its refinements or extensions, in which the decision of each player is individually rational given the decisions simultaneously made by the other players, and randomization (if any) is performed independently. Fourth, when there is uncertainty about any of the game parameters, the beliefs of the players are assumed to be consistent with a common prior distribution, which generates an infinite hierarchy of mutually consistent reciprocal beliefs. These assumptions are often applied at maximum strength in order to tightly constrain the solution, yet each of them is problematic in its own way. Common knowledge of real-valued von Neumann–Morgenstern utilities among potentially risk averse players is an idea that is rather tricky to make precise, let alone credible, except in simple settings where it is true by construction. In textbooks on game theory (e.g., Fudenberg and Tirole 1991, p. 4) an informal analogy may be drawn in which players are seated at separate computer terminals where they all see each other’s utility payoffs displayed. Formal treatments of common knowledge generally refer to knowledge of the occurrence of events, following Aumann (1976), and where this concept is applied to common knowledge of real-valued payoffs or reciprocal uncertainty about payoffs, it is done in terms of events that select elements from a set of possible payoff functions whose parameters are already given. In experimental games, common knowledge of utilities may be induced by having the players read from a common rulebook in which their payoffs are specified in units of the probability of winning a single large prize (e.g., Cooper et al. 1989; Ochs 1995). By definition this yields commonly known utilities, but the players’ risk attitudes are rendered irrelevant by the fact that there are only two terminal outcomes. A modeling approach that gives a behavioral role to risk aversion in simple games is the concept of quantal response equilibrium (McKelvey and Palfrey 1995) in which the players’ behavior has a noise component that is distinct from randomization in the usual sense of deliberately mixed strategies. Goeree et al. (2003) show that when this solution concept is applied to generalized matching pennies games in the presence of risk aversion, it leads players to prefer moves that are safe in the sense of having payoffs that exhibit little variation in response to the moves of others, which is incompatible with Nash equilibrium behavior in that setting. This paper asks a more fundamental question, namely: how does risk aversion affect the amount of information about the rules of a game that can be made commonly known via material communication, and what are its implications for the determinacy of the game’s solution, even in a simple case such as matching pennies? The modeling approach follows that of Nau and McCardle (1990) and Nau (1992), which is a multiplayer extension of de Finetti’s (1937); de Finetti’s (1974) operational approach to defining subjective probabilities,Footnote 1 which in turn is a microcosm of a financial market. Its behavioral primitives are offers to accept small conditional bets whose payoffs depend on the outcome of the game. This leads to solutions of games that are characterized by exactly the same rationality conditions as those of individual decisions and competitive markets, and they are not necessarily uniquely determined nor probabilistically independent between players: they may consist of convex sets of correlated equilibria rather than unique Nash equilibria. It will be shown here that in the case of risk averse players the common priors and equilibria are expressed in terms of sets of “risk-neutral probabilities” that need not represent the players’ true subjective beliefs. In strictly competitive games such as matching pennies, players may be able to hedge some of their risks and alter the game’s rules to their mutual benefit by additionally accepting finite bets that partially reveal the solution of the game, i.e., bets that are rationalized by their present beliefs in addition to the bets that are rationalized by the game’s rules.",1
78.0,2.0,Theory and Decision,29 January 2014,https://link.springer.com/article/10.1007/s11238-014-9417-4,"Skewness seeking: risk loving, optimism or overweighting of small probabilities?",February 2015,Thomas Åstebro,José Mata,Luís Santos-Pinto,Male,Male,Male,Male,"In settings where risky decisions have to be made many people favour riskier options which offer a small probability of large gains, that is, where the distribution of payoffs has positive skew. For example, people tend to overbet on the long-shot horse with low probability of winning large returns rather than the favourite with the greatest expected return (Golec and Tamarkin 1998). When people buy lottery tickets, Forrest et al. (2002) and Garrett and Sobel (1999) show that, people are more concerned with the size of the top prize than the expected value of the lottery. Positive skew affects other economic choices besides gambling. Three-quarters of all people who enter self-employment face higher variance and skew but lower expected return than in employment (Hamilton 2000). Further, 97 % of inventors will not break even on their investments but face a very skew distribution of returns conditional on succeeding (Åstebro 2003). Financial markets also provide evidence that is consistent with skew seeking choices. For example, risk aversion implies that people should hold diversified portfolios. However, Blume and Friend (1975) find that most households hold undiversified portfolios with a high proportion of stocks with high positive skew. Also, securities that make the market portfolio more negatively skewed earn positive ‘abnormal’ average returns (see e.g. Kraus and Litzenberger 1976). How can one explain these choices favouring options with positive skew, high risk and low expected return? Within the context of the classical expected utility theory (EUT from now on) such choices may be explained by love for risk due to convex utility of wealth. EUT also allows risk-averse individuals to have a preference for positive skew and, indeed, Ebert (2013) shows that all commonly used risk-averse utility functions exhibit skewness preference. However, because EUT’s preference for positive skew is a third-order effect, it cannot lead to choices of alternatives that have lower expected value and higher risk (Chiu 2010). Choices such as those discussed above can also be determined by pleasure from gambling (Conlisk 1993) or anticipatory feelings (Caplin and Leahy 2001). The choice of entrepreneurship over wage work may also be a result of a preference for being one’s own boss (Benz and Frey 2008) or of overconfidence about skill (Wu and Knott 2006). Another alternative is that these choices result from overweighting the probability of attaining favourable outcomes due to optimism or likelihood insensitivity. Optimism refers to overweighting the probability of getting larger prizes and underweighting the probability of getting lower prizes regardless of the probabilities of the prizes. Likelihood insensitivity refers to the tendency to overweight small probabilities and underweight large ones regardless of the magnitude of the prizes (see Wakker 2010). Field data are not practical to distinguish between these alternative explanations, since risk and skew are typically positively correlated. Both risk loving and optimistic persons respond similarly to increases in skew. The risk lover will favour greater skew because greater skew is associated with greater risk. The optimist will favour the same option because he will overweigh the probability of favourable outcomes. To evaluate the role of love for risk, optimism and likelihood insensitivity as explanations for favouring positive skew we, therefore, use a laboratory experiment. In this experiment we ask subjects to make choices between pairs of lotteries for which we manipulate the expected value, risk and skew. Comparing choices between pairs of lotteries allow us to rule out explanations related to the pleasure of gambling. In addition, our design rules out explanations that rely on overconfidence about skill since the probabilities associated with success are objectively determined, known in advance, and do not depend on one’s actions. Our experimental design, inspired by Holt and Laury (2002), offers individuals sets of 10 choices between a safe and a risky lottery, constructed in such a way that the number of times the safe lottery is chosen is indicative of either risk-averse, risk-neutral or risk-seeking choices. We depart from Holt and Laury (2002) design in one critical aspect. We consider three skew conditions for the risky lotteries. In the first, second and third skew conditions the risky lotteries have zero, intermediate and maximum skew, respectively. In all three conditions the safe lotteries are kept fixed as well as the mean and variance of the risky lotteries. Observed changes in the number of safe choices across the three skew conditions thus reveal the impact of positive skew upon decisions. Making fewer safe choices when the skew is higher is an indication that subjects make skew seeking choices, that is, that they are willing to take more risk in exchange for positive skew. While laboratory experiments typically use college students as they are cheap and conveniently available, it has been argued that studying real decision makers with significant incentives is more relevant for testing theory (Larrick 2004). We, therefore, perform the experiment on a sample of 131 experienced French executive education participants (hereon ‘executives’). For comparison, the same experiment was performed on a standard sample of 148 Canadian college students. In both cases, the experiment was performed under both low and high stakes (20 times higher than low stakes). We find that, in both samples, subjects make riskier choices when the choice task includes positively skewed lotteries. We estimate structural decision models that allow for utility curvature, optimism and likelihood insensitivity. We find no evidence of love for risk (convex utility) and we find that both optimism and likelihood insensitivity contribute to skew seeking choices. Students display linear utility of wealth and executives concave utility of wealth. Students display more likelihood insensitivity and are less optimistic than executives. Differences in optimism and utility curvature between executives and students disappear when we control for differences in age.",43
78.0,2.0,Theory and Decision,05 February 2014,https://link.springer.com/article/10.1007/s11238-014-9418-3,Within- versus between-country differences in risk attitudes: implications for cultural comparisons,February 2015,Ferdinand M. Vieider,Thorsten Chmura,Adewara Sunday,Male,Male,Unknown,Male,"The question of culture’s influence on behavior is interesting both because it may reveal differences in behavioral patterns previously thought universal, and because of the underlying determinants of behavior a cultural comparison may shed light on. In a provocative recent paper, Henrich et al. (2010) called attention to the scientific shortcomings that derive from constructing theories of behavior exclusively on observations obtained with Western university students. Indeed, the study of cultural differences is enjoying increasing popularity in economics (e.g., Bohnet et al. 2008; Herrmann et al. 2008; Oosterbeek et al. 2004). There are several papers that compare risk attitudes between countries (Bruhin et al. 2010; Rieger et al. 2011; Weber and Hsee 1998). Although all of these are controlled experiments, cultural studies hold additional pitfalls. The treatment in cultural comparisons consists by definition of the different subject pools from which samples are drawn. Cultural comparisons must thus by necessity abandon one of the elements that make the experimental method so powerful—the random assignment to treatments of subjects drawn from one and the same subject pool. Instead, it is now the subject pool itself that becomes the element of interest. In order to be sure that the effects picked up in a country comparison are really driven by the specific characteristics of a country, it is not sufficient to find differences between countries. We must also make sure that experiments run in different locations within a country produce the same or reasonably close results. We here address the issue of the extent to which between-country differences in risk attitudes can clearly be ascribed to culture (loosely “a particular form or stage of civilization”). We do not focus on the cultural determinants per se, but rather on whether inferences on some type of cultural effects can be drawn at all. Specifically, it may just be possible that differences observed between countries derive from (observable or unobservable) differences in the subject pool, or that within-country differences are just as large as between-country differences, given that samples are drawn from different subject populations. This objection is an important one, and previous studies have not ruled out this potential explanation. To address this issue we ran the same experiment in Beijing and Shanghai, China, and at two different campuses in Addis Ababa, Ethiopia. Controlling for observable heterogeneity in the subject pool, differences in risk preferences measured in the two Chinese cities are marginal, and differences between the two campuses in Addis Ababa are inexistent. This indeed points in the direction that giving up random allocation to treatments is not to blame for differences found between countries, at least once one controls for observable subject characteristics. We are also able to show that differences between the two countries are large. These differences should, however, be interpreted with caution, and do, in any case, not constitute the main point of this paper. We will return to this point in the discussion. This paper proceeds as follows. Section 2 introduces the experimental method, as well as the theory and econometric estimation techniques. Section 3 presents the results. Section 4 discusses the results and concludes the paper.",22
78.0,2.0,Theory and Decision,06 February 2014,https://link.springer.com/article/10.1007/s11238-014-9419-2,A simple stress test of experimenter demand effects,February 2015,Piers Fleming,Daniel John Zizzo,,Male,Male,Unknown,Male,"This paper presents a stress test of experimenter demand effects, which refer to changes in behaviour by experimental subjects due to cues about what constitutes an appropriate behaviour (Zizzo 2010). The idea behind experimenter demand effects is that subjects try to make sense of an unfamiliar experimental environment in order to decide an appropriate response, and in doing so they may be particularly sensitive to whatever cues are provided in such an environment. Our stress test of experimenter demand effects is of particular relevance to the interpretation of results in settings, such as public good games, trust games or bargaining games, that are characterized by variable surplus. That is, how the overall amount is split across the subjects depends on the actions by the subjects. We run an experiment where subjects can physically destroy money-equivalent coupons awarded to themFootnote 1 as well as, in a different task, explicitly return money to the experimenter. Subjects affected by experimenter demand effects will consider both tasks to be identical where, given their stylized nature and the absence of any simple alternative schema to make sense of them, there is an expectation that they should physically destroy or return some of the assets given to them. The distinctive feature of the destruction task is that the coupons’ destruction could not directly benefit the experimenter. This is in contrast to the cash return task or, indeed, any experiment where a money transfer to the budget of the experimenter implicitly takes place if experimental surplus is destroyed or not obtained. In the cash return task, unlike the destruction task, there can be a money transfer towards the experimenter, so altruism towards the experimenter may potentially affect behaviour and provide an alternative explanation of behaviour relative to experimenter demand effects. We are aware of two papers that have tested the possibility of altruism towards the experimenter. In an insightful contribution, Frank (1998) did so in the context of ultimatum games: in the experimental treatment, if the proposer’s offer was rejected, the money-equivalent currency (stamps) was physically burned; they found that receivers did not behave differently in this treatment relative to a control where the money implicitly went back to the experimenter. However, the strategic nature of ultimatum games may have meant that feelings of anger out of unfairness may have reduced the relevance of altruism towards the experimenter in this context; furthermore, this experiment did not control for the possibility that some subjects would gain utility from seeing stamps physically burned in the laboratory. Harrison and Johnson (2006) considered the effect of changing the recipients in standard dictator games, and found that giving to the experimenter was intermediate between giving to a charity and giving to another subject.Footnote 2 This could be interpreted as evidence of altruism towards the experimenter, but it is equally possible that returning money to the experimenter by the implied transfer of unexploited experimental surplus could in itself be due to experimenter demand, i.e. being due to the demands of the experimental decision environment, rather than being due to altruism towards the experimenter.Footnote 3
 We use an extremely simple setup with minimal strategic concerns or potential for misunderstanding to identify experimenter demand effects against alternative explanations. By providing an equivalent physical task to all subjects regardless of whether coupons are destroyed, we control for the pleasure of physical activity. Our experiment also controls for the potential benefits of coupon destruction for the coupons provider and for the clarity of the instructions. We are then able to separate out experimenter demand effects from altruism towards the experimenter as possible explanations of behaviour. If subjects are driven by altruism towards the experimenter in deciding what to do in experiments with variable surplus tasks, we would expect subjects to return money to the experimenter but not to destroy the coupons as this could not benefit the experimenter. Conversely, if experimenter demand effects drive both physical coupon destruction and returning money to the experimenter, both should be positive and should be positively correlated to each other. We have a social information treatment manipulation where we provide summary information about how subjects behaved in a pilot. We also have a partial, albeit imperfect, measure of sensitivity to social pressure by using the Stöber (2001) social desirability scale.Footnote 4 Intuitively, under social information the experimental norm (what is expected of subjects in the experimental environment) should be clearer and, therefore, subjects who are more responsive to social pressure should destroy more (conversely, subjects who are resistant to social pressure may destroy less). This provides a further stress test of experimenter demand effects.Footnote 5
 Section 2 presents the experimental design, Section 3 presents the results and Section 4 briefly concludes. The experimental instructions are available in the appendix.",12
78.0,2.0,Theory and Decision,13 February 2014,https://link.springer.com/article/10.1007/s11238-014-9416-5,Confidence and competence in communication,February 2015,Kohei Kawamura,,,Male,Unknown,Unknown,Male,"When we learn new information and access its reliability, we often take into account the information provider’s competence. However, the information provider and his audience may not always agree on how competent or how well-informed he actually is. This disagreement can be understood as a manifestation of over- and underconfidence in communication. For example, various experts (consultants, securities analysts, lawyers, etc.) are often accused of being overconfident about their ability or the quality of information they have. On the other hand, underconfidence is yet another type of obstacle to communication that troubles a number of individuals in society. Such communication apprehension can occur when a sender underestimates his ability or the relevance of his knowledge, and becomes excessively afraid of the audience’s reaction to his message. How do over- and underconfidence affect the nature of communication? Do they enhance or diminish information transmission? If so, how? Is more informative communication always better? How do over- and underconfidence interact with the sender’s (intrinsic) “bias,” which has attracted much attention in the literature on strategic information transmission? This paper addresses these questions by incorporating asymmetric beliefs (“confidence”) on the quality of the sender’s information (”competence”) into the standard cheap talk model of Crawford and Sobel (1982). We find that overconfidence leads to the sender’s incentive to “exaggerate” and send an extreme report, while underconfidence gives rise to incentive to “moderate” his report.Footnote 1 When communicating with an overconfident sender, the receiver discounts the quality of the sender’s report. That is, in making her decision the receiver puts less weight on the sender’s message than the sender wants the receiver to. In order to influence the decision in the face of the discounting, the sender is tempted to send an extreme message, but in equilibrium this results in the feature that extreme messages are less informative than moderate ones. In contrast, the receiver values the information held by an underconfident sender more than the sender himself does. As a result, the sender wants to weaken his influence on the receiver’s decision, and hence the sender has incentive to “moderate” his report relative to the actual information he has. In equilibrium, moderate messages become less informative than extreme ones. At the same time, we show that severe overconfidence and severe underconfidence have a marked similarity as they both may lead to the use of binary communication (e.g., “yes or no,” “agree or disagree”), despite the very different nature of the informational distortions they cause. Binary communication is shown to be robust to both over- and underconfidence since it suppresses any incentive to “exaggerate” or “moderate.” We also introduce an intrinsic bias of the sender, which represents the difference between the ideal decision of the sender and that of the receiver (who makes the decision) if they shared the same complete information on the state. It is well known that such bias reduces information transmission when there is no over- or underconfidence. In this paper, we demonstrate that while both overconfidence and underconfidence also reduce the quality of communication in the absence of bias, slight overconfidence on the part of the sender enhances information transmission whenever he is moderately biased. Moreover, overconfidence may increase the prospect of informative communication when he is severely biased. One striking feature of underconfidence in communication is that, when their expected utilities are computed according to their (asymmetric) beliefs, the sender may be better off with no communication than with informative communication even if both are perfect Bayesian equilibria. The receiver is always better off with more information transmission, so that multiple equilibria may not be Pareto ranked in communication with an underconfident sender. This may provide a strategic basis for “communication apprehension” (or “communication avoidance”) by people with underconfidence or low self-esteem. That is, if an underconfident agent can choose whether to participate in a communication game, he may strictly prefer not to do so, since equilibrium communication in the game may hurt himself. This is in contrast to the standard cheap talk models, where the worst equilibrium is “babbling” (no information transmission) so that the sender weakly prefers playing the communication game to staying out of the game. Another interesting characteristic of communication with an underconfident sender is that the receiver too may be better off in an equilibrium where a smaller number of messages is used. Specifically, we show that the receiver may prefer the equilibrium with binary messages to an equilibrium with more messages. The intuition is very simple. Consider a situation where three messages are used in equilibrium (“yes, I’m not sure, or no”). When a sender is severely underconfident, he sends a moderate message (“I’m not sure”) most of the time, which makes communication hardly informative from the receiver’s viewpoint. On the other hand in a binary communication equilibrium the sender has to choose either of the two messages (“yes or no”) and the receiver may find this more informative than the three-message equilibrium where the agent sends the moderate message (“I’m not sure”) with a very high probability. Roughly speaking, when communicating with an underconfident person, one may prefer not to give him a chance to send a moderate message. Overconfidence has been attracting much attention from psychologists and economists. The literature on judgement under uncertainty has found that people tend to be overconfident about the information they have, in that their subjective probability distributions on relevant events are too tight (Kahneman 1982; Cesarini et al. 2006). Overconfidence has been found in various professions such as lawyers (Wagenaar and Keren 1986), policy experts (Tetlock 1999), and security analysts (Chen and Jiang 2006). The implications of overconfidence for economic choices and especially for financial markets have been studied recently by numerous researchers (e.g., Kyle and Wang 1997; Gervais and Odean 1998; Daniel et al. 1998; Scheinkman and Xiong 2003). Hvide (2002) offers a theoretical argument for the endogenous emergence of overconfidence. Despite its prevalence, underconfidence is much less pronounced in the economic literature, and this paper provides a first approach to study both over- and underconfidence within a simple framework. Psychologists have analyzed how low self-esteem and shyness, which can be considered as an important aspect of underconfidence, become an obstacle to communication (e.g., Richmond and McCroskey 1997; Zimbardo 1990). We are able to offer a game theoretic analysis of the nature of difficulties in communication with underconfident individuals, and also can explain their tendency to avoid communication as we show that an underconfident sender may indeed be better off with no communication than with informative communication, even if engaging in communication itself is completely costless. In experimental settings, Griffin and Tversky (1992) document how overconfidence and underconfidence arise systematically in the context of evidence assessment. Hoelzl and Rustichini (2005) have found that people tend to be underconfident particularly with unfamiliar tasks. Underconfidence can potentially be important even for professional experts. In medical profession, Friedman et al. (2004) have found that physicians are more likely to be underconfident about their diagnoses than to be overconfident. In order to model the level of confidence, we allow for asymmetric beliefs on the probability that the sender observes the true state of nature. In other words, the receiver and the sender may not share the same “confidence” on the sender’s “competence” though they are fully aware of the difference in beliefs. Since we do not have to specify which player a priori has the correct belief, our framework can be applied to communication with “over-reliance” or “under-reliance” on the sender’s information, for which a sender is often supposed to hold the correct belief and his audience does not. Recent papers that involve asymmetric beliefs (non-common priors) include Admati and Pfleiderer (2004), Fang and Moscarini (2005), Van den Steen (2005), and Che and Kartik (2009), among others. Fang and Moscarini (2005) consider the effect of workers’ overconfidence in their skills on wage policies, and Van den Steen (2005) examines worker incentives when a worker may disagree with the manager regarding the best course of action. Che and Kartik (2009) develop a verifiable disclosure model between a decision maker and agent who have asymmetric prior beliefs about the state, and study the agent’s incentive for information acquisition. The closest to the present paper is Admati and Pfleiderer (2004) who study an information transmission game where the sender can be overconfident in his ability to observe the true state. They argue that in communication with an overconfident sender extreme messages or ratings are less precise because if the sender reports the truth honestly, from his viewpoint the receiver’s reaction is too weak and the sender has incentive to send an extreme report. Admati and Pfleiderer (2004) focus on overconfidence and do not consider underconfidence. Also they assume that the sender is otherwise unbiased: if the sender and the receiver agree on the sender’s ability the parties’ interests are perfectly aligned. Thus, they are unable to address the question how the intrinsic bias of a sender, which has been a focus of attention in the information transmission literature (and an important concern in practice), interacts with his confidence in his competence. In the present paper, we analyze underconfidence as well as overconfidence in a systematic way. Moreover, we explicitly illustrate the interaction between confidence and the intrinsic bias. Also, while Admati and Pfleiderer (2004) restrict the message space in such a way that the sender chooses one of an exogenously given finite set of messages, we do not impose such a restriction on the message space. Hence we are able to study more naturally how much information a sender can possibly communicate credibly, depending on his confidence and bias. This also enables us to examine interesting issues with multiple equilibria, especially in communication with an underconfident sender. Although our model is an extension of the canonical model of Crawford and Sobel (1982), we cannot simply adopt their equilibrium characterization because the sender’s incentive to misreport may not point in the same direction. Recently, Gordon (2010) has provided the general characterization of a class of cheap talk equilibria where the sender’s bias can depend on his information. While he is mainly concerned with equilibrium characterization itself, our focus in the present paper is on how parameters regarding confidence and intrinsic bias alter the nature of communication. We observe that overconfidence and underconfidence lead to different structures of informative equilibria. The informative equilibria with overconfidence are closely related to communication with noise (Krishna and Morgan 2004; Blume et al. 2007; Goltsman et al. 2009), where the receiver puts less weight on a message than in communication without noise, because due to noise the received message may not necessarily be informative about the state. In the absence of intrinsic bias, weak response to a message leads to the sender’s incentive to “exaggerate” his message relative to the prior expected state. A similar intuition has been developed by Kawamura (2011) in a multiple sender setting. This type of informational distortion appears in communication with an overconfident sender too, as we have already seen above. On the other hand, informative equilibria in communication with an underconfident sender shares an important feature with “reputational cheap talk” (Ottaviani and Sørensen 2006a, b; Gentzkow and Shapiro 2006), where the sender attempts to look more able when his quality is unknown to the receiver but known to the sender himself. In order to appear to be of high quality, the sender with reputational concerns has incentive to report messages that are closer to the prior, which also arises in communication with an underconfident sender for a different reason. The rest of this paper is organized as follows. The next section describes the model, and Sect. 3 characterizes the equilibria. The effects of over- and underconfidence in the absence of bias are studied in Sect. 4 and Sect. 5 examines the interplay between confidence and bias. Section 6 concludes.",6
78.0,2.0,Theory and Decision,16 January 2014,https://link.springer.com/article/10.1007/s11238-014-9414-7,The effects of uncertainty on the WTA–WTP gap,February 2015,Robert J. Reilly,Douglas D. Davis,,Male,Male,Unknown,Male,"The frequently observed disparity between willingness to pay (WTP) and willingness to accept (WTA) has been a subject of continuing attention among economists. The disparity is of interest for both policy and theoretical reasons. As a policy matter, analysts frequently find themselves in the position of needing to know the value stakeholders place on various alternatives, so determining the appropriate method for eliciting such information is of critical importance. Theoretically, the disparity is a curiosity because willingness to pay (measured as a compensating variation) and willingness to accept, (measured as equivalent variation), should be identical except for a small income effect (Willig 1976).Footnote 1
 An important branch of the literature studying WTA and WTP involves elicitations for goods of uncertain value. In a variety of contexts such as medical insurance, highway safety, and environmental quality enhancements, policymakers are interested in the value consumers place on goods whose usefulness contains a stochastic component. Laboratory implementations of value elicitations for such goods typically involve lotteries. In such experiments, the mean elicited WTA persistently exceeds its WTP counterpart, even under rigorous test conditions involving within-subject comparisons, using financially salient elicitations under an incentive compatible elicitation mechanism, and after practice decisions to ensure familiarity (see, e.g., Harless 1989; Eisenberger and Weber 1995; Schmidt and Traub 2009; Isoni et al. 2011). Most theoretical work on the effects of uncertainty on WTA and WTP has focused on variations of reference dependent utility theory (RDTs), such as loss aversion (Kahneman and Tversky 1979), endowment effects (Thaler 1980), regret (Loomes and Sugden 1982), and status quo bias (Gal 2006). A conclusion of these analyses is that uncertainty creates a “reluctance to trade” by which WTA rises above and WTP falls below expected value.Footnote 2
 Although the magnitude of the gap documented in several empirical studies likely exceeds what we can reasonably expect standard expected utility theory to explain, we observe that it is also possible that a “reluctance to trade” may be a consequence of the way uncertainty affects WTA and WTP under standard expected utility theory (EUT). This possibility has received some limited attention. In separate studies, Isik (2004) and Okada (2010) both conclude that EUT can explain the observed reluctance to trade. Specifically, these authors find that the combination of risk aversion and uncertainty causes symmetric adjustments to WTA and WTP, with WTA rising above and WTP falling below a lottery’s expected value. They both further conclude that increases in uncertainty prompt symmetric adjustments in WTA and WTP that increase the gap’s magnitude. Unfortunately, however, both analyses contain important errors and/or assumptions that economists would find unacceptable.Footnote 3
 Still lacking in the literature, then, is a direct and correct analysis of the effects of uncertainty on the WTA–WTP gap under standard EUT. This paper removes this deficiency. Our development consists of three parts. First, we extend an approach taken by Weber (2003) to the general case of lotteries, in order to develop an exact expression for the size of the WTA–WTP gap under uncertainty. Second, we offer a pair of corollaries that reinterpret and extend an analysis of uncertainty on risk premia by Gabillon (2012) to the case of WTA, WTP and the sign of the WTA–WTP gap.Footnote 4 Third, we derive a pair of theorems that assess the limit behavior of the WTA–WTP gap’s size as income or risk increases. By way of pre-summary, we find the following. First, given risk aversion, uncertainty causes both
WTA and WTP to fall below a lottery’s expected value. Second, the algebraic sign of the WTA–WTP gap is determined by the behavior of the agent’s coefficient of absolute risk aversion. Given decreasing absolute risk aversion (DARA)
\(WTA>WTP\). Given increasing absolute risk aversion (IARA) the reverse is true, and \(WTA<WTP\). Third, we show that in the limit the magnitude of the WTA–WTP gap for favorable lotteries necessarily approaches zero as either income or riskiness increases. This convergence need not be monotone, however, and in the case of increasing risk, a common pattern is a gap that at first widens and then falls to zero. We illustrate this pattern with a representative simulation.",1
78.0,2.0,Theory and Decision,15 February 2014,https://link.springer.com/article/10.1007/s11238-014-9420-9,A proportional value for cooperative games with a coalition structure,February 2015,Frank Huettner,,,Male,Unknown,Unknown,Male,"A cooperative game with transferable utility, or simply a TU-game, captures a situation in which players can achieve certain payoffs from cooperation. The model consists of a finite set of players \(N\) and a coalition function \(v \) that specifies a worth \(v\left( S\right) \) for each coalition of players \(S\subseteq N\). If the players are grouped a priori, the model is enriched by a coalition structure, a partition of the player set, giving a cooperative game with a coalition structure, or simply a CS-game. A single-valued solution, shortly CS-value (TU-value), is a function that assigns to every CS-game (TU-game), an \(\left| N\right| \)-dimensional vector the entries of which are interpreted as the individual player’s payoffs. One standard solution concept for CS-games is the Owen value (Owen 1977) which rests on the interpretation of the a priori groups as bargaining blocks; the collection of all players remains as the productive unit, and hence, the worth of the grand coalition is distributed. The Owen value is a generalization of the Shapley value (Shapley 1953) as for the atomistic partition and the one-block partition, i.e., when players cannot be distinguished on the grounds of their group formation, the outcome according to the Owen value of a CS-game coincides with the Shapley value of the underlying TU-game. In the two-player case, there are only such trivial partitions and both values allocate the standard solution payoffs, giving player \(i\) his singleton worth plus half of the surplus generated from cooperation, However, if the surplus from cooperation is not perceived to be produced merely by the “attendance” of the players, but results also from their productivities, there is some appeal to the idea of sharing the surplus proportionally. For two-player games it is focal to share the surplus proportional to the stand-alone outcomes, giving player \(i\) the payoff Proportional sharing has a long tradition in cost sharing problems. For example, Straffin and Heaney (1981) describe that the Tennessee Valley Authority studied proportional sharing of nonseparable costs in the 1930s. On the other hand, van den Brink et al. (2007) argue that it is standard business practice to share profits proportional to investment. Further references are given by Moulin (1987) who characterizes the proportional sharing according to stand-alone worths in a simple framework with multiple players. As a motivating example, consider the sharing of a penalty due to pollution in a certain industrial district. Assume a linear relation between emission \(e\) of the district and punishment \(p\) up to a certain threshold. If emission exceeds the threshold, a severe extra penalty is imposed.Footnote 1 For instance, let The industrial district contains two production sites \(P_{1}\) and \(P_{2}\), with emissions \(e_{1}=28\) and \(e_{2}=2\). This situation can be modeled by a two-player TU-game in which the worth of a coalition is equal to the fine induced by the pollution of its members, \(v\left( \left\{ P_{1}\right\} \right) =28, v\left( \left\{ P_{2}\right\} \right) =2\), and \(v\left( \left\{ P_{1},P_{2}\right\} \right) =90\). According to the standard solution payoffs, \(P_{1}\) pays \(58\) and \(P_{2}\) pays \(32\). The proportional solution suggests that \(P_{1}\) pays \(84\) and that \(P_{2}\) pays \(6\). It is clear that \(P_{1}\) is much more responsible than \(P_{2}\) for exceeding the threshold, at least to a higher extent than suggested by the standard solution. The proportional solution is more appropriate according to the vision of Rawls (1955): “It is morally fitting that a person who does wrong should suffer in proportion to his guilt, and the severity of the appropriate punishment depends on the depravity of his act.” In order to illustrate why a priori unions among players are important, we extend our example. Assume that a third production site \(P_{3}\), which is located in another district rather far away from the other sites, is now—e.g., due to governmental redefinitions of the radius of consideration—part of the game. Despite the fact that the three production sites are jointly punished for their total pollution, it still makes sense to take into account their geographical locations by considering the coalition structure \(\pi =\left\{ \left\{ P_{1},P_{2}\right\} ,\left\{ P_{3}\right\} \right\} \). Further, assume \(e_{3}=e_{1}=28\) such that the game described in Table 1 is obtained. This yields the payoffs given in Table 2, where the payoffs of the Shapley value and the proportional TU-value are taken from the underlying TU-game, thereby neglecting the coalition structure. It appears that similarly to the Shapley value, the Owen value only preserves differences. On the other hand, the TU-values per definition ignore the different locations of \(P_{1}\) and \(P_{3} \). This is counterintuitive since \(P_{1}\)’s district \(\left\{ P_{1} ,P_{2}\right\} \) alone by itself already passes the threshold, while the district containing \(P_{3}\) exerts some effort to keep its own pollution below the threshold and, therefore, should not bear a cost as large as the one attributed to \(P_{1}\). Note that the proportional CS-value suggests a cost sharing that takes into account the aforementioned principle and also reflects the different degrees of responsibility of \(P_{1}\) and \(P_{2}\). Apart from the Owen value, there exist numerous other CS-values studied in the literature (see Calvo and Gutiérrez (2010) and the references therein). To the best of our knowledge, extensions of the proportional rule to \(n\)-player CS-games that account for a priori partitions as bargaining blocks have not been studied so far.Footnote 2
 We introduce a proportional value for cooperative games with a coalition structure. Our CS-value generalizes the proportional TU-value (Ortmann 2000) in a way that parallels the extension of the Shapley value to the Owen value. Our proportional CS-value and the proportional TU-value due to Ortmann (2000) apply the rule of proportionality on surplus perceived within a pairwise perspective. The crucial condition imposed on the payoff scheme states that for all players \(i, j\) (from the same component), the relative change of player \(i\)’s payoff due to the removal of player \(j\) from the game equals the relative change of player \(j\)’s payoff due to the removal of player \(i\) from the game. There are also other ways to extend the proportional rule from two-player games to \(n\)-player games (e.g., Khmelnitskaya and Driessen 2003 or Vorob’ev and Liapounov 1998). They differ in what is understood to be the surplus from cooperation and in what is used as standard of comparison. The remainder of this paper is organized as follows. Section 2 provides basic definitions and notation. In Sect. 3, we introduce our value and give a characterization which employs axioms that reveal the relations to the Owen value and to the proportional TU-value. In Sect. 4, we employ a consistency property introduced by Winter (1992) in order to provide a second characterization of our value. The appendix provides all the proofs.",8
78.0,2.0,Theory and Decision,06 March 2014,https://link.springer.com/article/10.1007/s11238-014-9421-8,General dual measures of riskiness,February 2015,Klaas Schulze,,,Male,Unknown,Unknown,Male,"The phenomenon of risk plays a ubiquitous role in economics as well as in finance and insurance. It is involved in nearly all economic and financial activities. Risk is a key aspect in diverse situations, as the formation of investment decisions and the operation of financial markets. It is the daily occupation of several professions, as rating agents, financial regulators, and portfolio, and fund managers. These practitioners require to quantify the risk of a random payoff and to decide whether or not to enter a risky position. More generally, all individuals take similar decisions when playing a gamble or lottery—and even when buying an insurance, as it is equivalent to rejecting a gamble. Following Diamond and Stiglitz (1974), these decisions depend conceptually on two distinct aspects: the objective attributes of the gamble, in particular how risky it is, and the subjective attitude toward risk of the investor, in particular how risk-averse she is. For both aspects, the literature provides sophisticated concepts. Concerning the first, actuarial and financial literature has proposed a variety of risk measures over the last few decades, confer, e.g., Bühlmann (1970) and Föllmer and Schied (2002). This research in quantifying the risk of a random payoff lead from simple moment considerations, as the Sharpe ratio, to the commonly used value-at-risk and more sophisticated measures, e.g., expected shortfall, utility-based, or spectral risk measures. Since some of these measures expose unfavorable properties (e.g., value-at-risk might penalize diversification), Artzner et al. (1999) introduce axioms of coherence in order to ensure consistent risk measures. Concerning the second aspect, the classic contribution of Arrow (1965) and Pratt (1964) measures the attitude toward risk by defining a coefficient of risk aversion based on the derivatives of the subjective utility function. We aim to connect both concepts. For this purpose, we introduce and analyze the class of risk measures, which respect uniform comparative risk aversion. The connection is established by imposing an axiom on risk measures. This axiom is called duality and introduced in the seminal work of Aumann and Serrano (2008). Duality asserts, roughly speaking, that less risk-averse agents accept riskier gambles. More precisely, a measure is dual, if every agent accepts all gambles, which are less risky (by means of the risk measure in question) than a gamble, which is already accepted by a uniformly as least as risk-averse agent. There are two reasons for this behavior: first, the agent in question is relatively as least as risk-seeking, and second, the gambles in question are relatively less risky. This makes duality a natural assumption, which can be seen as a requirement on risk measures. It connects the two aspects of decision-making under risk in a natural way. Aumann and Serrano (2008) characterize a risk measure, called index of riskiness, uniquely by duality and the axiom of positive homogeneity. This paper aims to derive and analyze the most general set of risk measures, which satisfy the duality axiom, and thus respect uniform comparative risk aversion. Therefore, we extend the setting of Aumann and Serrano (2008) in following three dimensions: We drop the second axiom of positive homogeneity. This axiom is of minor importance, also in the opinion of Aumann and Serrano (2008). By relaxing positive homogeneity we generalize from one index of riskiness to the class of all risk measures satisfying the natural axiom of duality. This relaxation is relevant, since positive homogeneity can be criticized: risk might increase more than proportional with the size of the gamble. Föllmer and Schied (2002) justify this by additional liquidity risk, and Dhaene et al. (2003) relate positive homogeneity to linear utility, whereas for concave utility, risk is expected to increase over scale. Moreover, this relaxation allows for the following two generalizations. While Aumann and Serrano (2008) concentrate on expected utility agents, who are risk-averse, we also allow agents, who are risk-neutral or risk-seeking or any combination of it. Inspired by Schnytzer and Westreich (2013), this extends our results to settings with higher descriptive power, e.g. gambling behavior can be better explained by risk-seeking agents. In particular, an approximation of the value function of prospect theory of Kahneman and Tversky (1979) reflects loss aversion and risk-seeking on losses. We extend the domain of gambles in three major aspects, as we allow (i) for gambles with zero or negative expected value, (ii) for probability distributions of gambles with heavy negative or heavy positive tails or both, and (iii) for distributions with exponentially decaying positive or negative probabilities or both. These three generalizations are relevant as they allow for the inclusion of all distributions of risk, and in particular those with non-trivial solutions, including, e.g., the lognormal, the exponential, the NIG, and the Gamma distribution. They further maximally broaden the scope of our results to the space of integrable gambles. This space is the canonical space for risk measures as shown by Filipović and Svindland (2012). This simultaneous generalization in three dimensions is challenging. It is achieved by a careful analysis of agents with constant absolute risk aversion (CARA). The key is to derive the cutoff between accepting and rejecting CARA-agents in terms of the Arrow–Pratt-coefficient of risk aversion. Here we observe the following four patterns of economic acceptance behavior: Case \(I\): There exists a unique CARA-agent, who indifferent between accepting and refusing the gamble. The cutoff naturally equals the risk aversion of this agent. We further distinguish the following subcases for risk-averse agents whose cutoff is positive, called Case \(I^+\); for risk-neutral agents with a cutoff of zero, called Case \(I^0\); and for risk-seeking agents whose cutoff is negative, called Case \(I^-\). Case \(A\): All CARA-agents accept the gamble. In this case, we interpret the degenerated cutoff by \(\infty \), as all agents accept the gamble, regardless of how risk-averse they are. Case \(R\): All CARA-agents reject the gamble. In this case, we interpret the degenerated cutoff by \(-\infty \), as all agents accept the gamble, regardless of how risk-seeking they are. Case \(N\): There exists an accepting and a rejecting CARA-agent, but no indifferent one. In this case, we show that the cutoff is still unique and consider the corresponding agent lying between the accepting and the rejecting agents. Again, we further distinguish the subcases for risk-averse agents, called Case \(N^+\); for risk-neutral agents, called Case \(N^0\); and for risk-seeking agents, called Case \(N^-\). We characterize the gambles belonging to each of the eight cases of acceptance behavior in terms of properties of their distribution. The original setting of Aumann and Serrano (2008) restricts to a subset of \(I^+\) and Schnytzer and Westreich (2013) consider additionally a subset of \(I^-\). To our best knowledge, the rigorous generalizations to Cases \(A,R,N^+,I^0,N^0,\) and \(N^-\) have not been covered in the literature. These generalizations are economically relevant as they include all patterns of acceptance behavior as well as numerous common distributions, namely distributions with exponentially decaying probabilities, heavy positive tails, or heavy negative tails; such as the lognormal, the \(t\), the exponential, the Gamma, and the Variance Gamma distribution. We further formalize the derivation of the cutoff between accepting and rejecting CARA-agents by introducing the so-called cutoff measure. Its definition is appealingly simple as it incorporates all eight cases without the need to distinguish them. We analyze the cutoff measure by establishing its uniqueness and its positive as well as negative homogeneity. For closed-form solutions of the cutoff measure for Case \(I^+\), we refer to Aumann and Serrano (2008) for the normal distribution, to Homm and Pigorsch (2012a) for the NIG distribution, and to Schulze (2014) for the exponential, the Gamma, the Variance-Gamma and further distributions. These computations transfer easily from Case \(I^+\) to Case \(I^-\), as we show that the cutoff measure is symmetric at the origin. In remaining cases the cutoff measure can be derived trivially by studying the so-called exponential exponents. These computations show that the cutoff measure maps gambles, which are exposed to high risk, to low parameters and vice versa. The convention, that a risk measure associates high-risk gambles with high numbers and low-risk gambles with low numbers, suggests to reverse the cutoff measure in order to receive a conventional risk measure. Our main theorem verifies, that a reversed cutoff measure is indeed a sensible risk measure: it yields that any measure reversing the ordering of the cutoff measure necessarily satisfies the natural duality axiom. This result further provides an easy method to construct dual measures by composing the cutoff measure with an arbitrary decreasing function. We present six concrete reversions, of which one reversion extends the index of Aumann and Serrano (2008) to Cases \(A,N^+,I^0,I^-,N^-,\) and a subset of \(N^0\), while preserving positive homogeneity. Another reversion generalizes the global index of riskiness, recently introduced by Schnytzer and Westreich (2013), to the Cases \(A,N^+,I^0,N^0,\) and \(N^-\). We further present measures which account for a shortcoming of the cutoff measure: whereas it cannot distinguish gambles within Case \(A\), nor within Case \(R\), nor within Cases \(I^0\) and \(N^0\), we present measures, which are more accurate and are able to distinguish these gambles and quantify their risk, while preserving duality. Moreover, we derive that the opposite implication also holds true: any dual measure reverses the ordering induced by the cutoff measure. This implication provides the insight that all dual measures can be characterized via the cutoff measure and that solely the duality axiom determines the ordinality of its risk measures. A consequence is a representation result which decomposes dual measures into the cutoff measure and a decreasing function, which we derive explicitly. Overall, we present a handy equivalent condition for duality. An insight of this paper is the following: Whereas the cutoff measure determines the ordinal nature of all dual risk measures, its specific reversion determines the cardinal nature of a specific dual risk measure. The ordinal nature is of vast importance in theoretic considerations. Here it is appealing that the cutoff measure, derived by CARA-utility solely, ensures duality for general utility functions beyond CARA-utility, i.e. it ensures consistency with comparative risk aversion for generalized utility functions exhibiting risk-aversion and risk-seeking, e.g., a twice-differentiable approximation of the value-function of Kahneman and Tversky (1979). This highlights the outstanding role of the cutoff measure and of CARA-utility among generalized utility functions. Furthermore, the order of the cutoff measure extends the wealth-uniform dominance from its rather restrictive domain to all integrable gambles. Hart (2011) introduces wealth-uniform dominance and achieves to restate duality with referring to only one agent. On the other hand, the cardinal nature is essential for quantification of risk, which is the main purpose in applications. Here we present some concrete dual measures, which overcome the blind spots of the cutoff measure. An application of the index of riskiness to performance measurement is studied by Homm and Pigorsch (2012b), and further applications are mentioned in Schulze (2014). This paper is organized as follows: the following section introduces the cutoff measure and its properties. The primary result characterizing dual measures is derived in Sect. 3. Section 4 provides examples of dual measures. Proof are delegated to the Sect. 5.",1
78.0,2.0,Theory and Decision,17 March 2014,https://link.springer.com/article/10.1007/s11238-014-9423-6,Partial compensation/responsibility,February 2015,Erwin Ooghe,,,Male,Unknown,Unknown,Male,"The standard way in economics to assess, improve or optimize public policy is based on welfarism: welfare in society is measured by an increasing function of subjective individual utilities only. There are different reasons why using subjective utilities can be objectionable. Rawls (1971) criticizes the welfarist approach and argues in favour of equalizing an objective index of primary goods. In the aftermath of Rawls’ influential work, many alternative theories of distributive justice were developed; see, e.g. Kymlicka (2002, chap. 3) for an overview. Such theories often share a selective-egalitarian viewpoint: equality is still desirable, but only for differences in outcomes that are caused by a selection of morally irrelevant factors. The pragmatic theory of responsibility-sensitive egalitarianism proposed by Roemer (1993, 1998) and the compensation/responsibility framework proposed by Bossert (1995) and Bossert and Fleurbaey (1996) initiated a body of empirical work; see e.g. Ramos and Van de gaer (2012) and Roemer and Trannoy (2013) for recent overviews. These pragmatic empirical approaches have two steps in common. The outcome of interest is linked to several observable determinants that are empirically relevant for the outcome under consideration. Afterwards, a ‘sharp’ cut is made: each of the determinants, including the unavoidable residual in empirical work, is classified as either a compensation or a responsibility factor. As recognized by many authors, some of the determinants are hard to classify. We provide some examples. First, the residual is the outcome part that cannot be explained by the empirical model and captures, e.g. omitted compensation and responsibility factors. It is, therefore, by definition difficult to interpret the residual, let alone classify. The residual also turns out to be of considerable size in most applications, and, as a consequence, results can heavily depend on how one classifies it. Second, some determinants like gender can influence outcomes via different channels, e.g. via opportunities and via preferences. Preferences can be considered a legitimate source of outcome differences if individuals identify with their preferences. If one only wants to compensate for the effect of gender on opportunities, one must be able to separate both channels, e.g. by estimating a structural model. In the absence of good instruments, such models are plagued by identification problems. Third, other determinants, like educational attainment, are usually considered to be under partial control, say, a combination of intelligence and diligence. A second-best setting in which taxes induce agents to reveal their private information could be one solution, but the resulting multidimensional screening exercise is rather cumbersome.Footnote 1 Fourth, even if we know that certain variables like genetic defects are inborn and, therefore, beyond control, there could be other considerations, like cost containment, such that full compensation is not desirable or simply not feasible. A pragmatic way to deal with such problems is to introduce a soft cut: some of the determinants can be partly compensation and partly responsibility. From a normative point of view, a soft cut can enrich some of the existing pragmatic theories of fairness. Of course, it cannot answer the question to what extent we should compensate individuals for the different underlying determinants. There is probably no universal answer to this question, e.g. because the degree of compensation for a determinant could depend on the outcome under consideration. In our view, the soft cut allows the researcher to adopt a range of reasonable values that can tackle some of the problems mentioned before in a pragmatic way. Also, from a descriptive point of view, a soft cut can be better suited to capture individual or social preferences for redistribution. Stated and revealed attitudes towards social spending show a stable pattern over countries and time: individuals are most supportive to compensate for age, followed by sickness and disability, less for needy families with children, and less again for unemployment; see Coughlin (1980). Such individual or social preferences cannot be captured by a sharp cut. There is similar evidence in Gaertner and Schokkaert (2012) that a soft cut based on (what these authors call) intermediate compensation is closer to the opinions on distributive justice in different countries. In Sect. 2, we model gross income as a function of different factors, but in our model these factors can be partitioned into ‘compensation’ groups, i.e. subsets of factors with the same degree of compensation. The model of Bossert and Fleurbaey (1996) is a special case with two subsets of factors, one set with full compensation and one with no compensation (or full responsibility). The central axiom of partial solidarity is introduced in Sect. 3. It requires that the part of a gross income shock for which the individual is responsible should be borne by that individual only, while the remaining part should be spread equally over all individuals in society. Section 4 provides the two main results. First, the introduction of a ‘soft’ cut based on the idea of partial solidarity does not escape the Bossert (1995) and Fleurbaey (1995) separability result: it can be satisfied only if the gross income function is additively separable between the different compensation groups. In case additive separability fits the data, a simple partial sharing rule emerges as a natural candidate for partial redistribution. A second result characterizes this partial sharing rule on the basis of two simple properties, equal treatment of equals and partial solidarity. Additive separability is not necessarily satisfied in practice. We discuss in Sect. 5 how to proceed if separability is rejected by the data. A final Sect. 6 concludes.",1
78.0,2.0,Theory and Decision,27 March 2014,https://link.springer.com/article/10.1007/s11238-014-9424-5,Aggregating infinitely many probability measures,February 2015,Frederik Herzberg,,,Male,Unknown,Unknown,Male,"This paper studies the aggregation of infinitely many probability measures. Depending on one’s disciplinary outlook, the question may be either of interest in its own right or need further motivation. The rest of the introduction and the following Sect. 2 is meant to provide such motivation; these initial parts can be ignored by readers who find the problem of sufficient interest in itself. Formal epistemologists are now increasingly following the decision-theoretic paradigm of recent decades by weakening the strict, classical Bayesian assumption of rational agents being endowed with a single subjective probability measure. (Subjective probability measures are generally referred to as priors in the decision-theoretic literature, even when learning by conditioning is very rarely studied, whence there are, strictly speaking, neither prior nor posterior probability measures in the statistical sense; we submit to this terminological convention.) Bradley (2012), for example, entertains the possibility of rational agents having belief systems that are compatible with several subjective probability measures—as opposed to a single unique one. This appears, at first sight, to be a step in the direction of the recent decision-theoretic literature on multiple priors. However, this is not the case—due to the different perspectives of epistemologists on the one hand and decision theorists on the other. There are at least two ways in which the aggregation of probability measures per se—as opposed to their induced preferences—is of great importance from an epistemological point of view and perhaps also of some interest from a decision-theoretic vantage point. The first comes from social epistemology. Consider a group whose members hold different belief systems, each encoded by a subjective probability measure, and face a collective decision, i.e. they need to choose one of several social alternatives. Suppose this group wants to ensure that their decision is rationally defensible given some belief system which in a reasonable sense aggregates their individual belief system. Let us assume, for simplicity, that the individuals differ only with respect to their beliefs while sharing a common utility function over final outcomes. A good solution to their problem would be to look for a way of rationally aggregating the probability measures describing their individual belief systems and afterwards choosing a social alternative which maximises expected utility with respect to the aggregate probability measure. [Cases similar to this, however with additional serious complications, feature in a forthcoming paper by Bradley et al. (2012).] Another area in which the aggregation of probability measures per se becomes important is a new, epistemologically and psychologically informed account of individual decisions. This account is motivated by theories from contemporary psychology according to which the human mind is to be understood as a composition of more elementary mental agents, viz. in terms of a ‘society of mind’ (Minsky 1986), a ‘multimind’ (Ornstein 1986), or an ‘internal family system’ (Schwartz 1997); it has been formalised and introduced into the epistemological discussion by Bradley (2012). Technically speaking, this new account of human decision making attempts to give a precise description of the decision process of an individual whose belief system is compatible with several probability measures: Bradley (2012) suggests that every decision of an agent with multiple subjective probability measures is preceded by an ‘aggregation’ of those priors—which takes her, temporarily, to a new, aggregate prior, i.e. a new probabilistically consistent belief system; in order to make a decision, she will then evaluate the options available to her using the classical expected utility criterion with respect to the temporary, aggregate prior.Footnote 1 After the decision has been made, she returns to her previous epistemic state encoded by a whole set of subjective probability measures. Note that this set of subjective probability measures will in general not be finite—e.g. when it is the set of all probability measures that are consistent with certain conditional probability assignments or that satisfy certain upper or lower bounds when evaluated at certain events (‘interval probabilities’). Therefore, epistemologists who wish to follow Bradley (2012) in his proposal to integrate insights from contemporary psychology into epistemology should seriously consider the problem of aggregating infinite profiles of probability measures. Of course, the classical decision theory for multiple subjective probability measures (multiple priors), as in Gilboa and Schmeidler (1989), is not—at least not without substantial, quite probably philosophically questionable, detours—applicable in these situations: whilst one could associate each probability measure in the set of priors with an individual and thereby view a maxmin expected utility preference ordering as an aggregate preference ordering, the corresponding ‘aggregator’ would aggregate preferences derived from priors, not priors themselves. Therefore, this theory is—notwithstanding its mathematical elegance and also in spite of its manifold practical use, e.g. in mathematical finance [cf. e.g. Riedel (2009)] —not satisfactory for the epistemologically motivated purposes of the present paper. The paper is structured as follows. Section 2 motivates the problem of aggregating subjective probability measures from a revised Bayesian perspective. This section can be skipped by readers who are only interested in the technical aggregation problem itself, which will be studied in Sect. 3. We will see that there is indeed a relatively natural and decision-theoretically defensible way of aggregating probability measures—via a generalised aggregation theory for infinite profiles of probability measures along the lines of McConway (1981) and Arrow (1963). Moreover, this existence question is not trivial, as certain natural candidates like oligarchic aggregation or aggregation via integrals on the electorate are not feasible. The formal details and proofs are given in Sects. 4 and 5, respectively. Section 6 concludes.",2
78.0,3.0,Theory and Decision,03 April 2014,https://link.springer.com/article/10.1007/s11238-014-9428-1,How do subjects view multiple sources of ambiguity?,March 2015,Jürgen Eichberger,Jörg Oechssler,Wendelin Schnedler,Male,Male,Male,Male,"In Ellsberg’s famous two-color experiment (1961), subjects can choose between placing bets on the color of a ball drawn from one of two urns. The first urn (urn H) contains a number of colored balls, half of which are known to be black and half of which are known to be red. The second urn (urn U) contains balls of the same colors but in unknown proportions. Subjects who irrespectively of the color strictly prefer betting on the urn where half of the balls are black are classified as ambiguity averse. In the classic experiment, ambiguity only concerns the composition of the urns. In reality, however, ambiguity is rarely limited to a specific aspect of a situation. In particular, the gains from winning are often not clear. In this paper, we examine experimentally how ambiguity aversion is affected when there is a second source of uncertainty. As we shall see, this has important consequences for modeling ambiguity aversion. We extend Ellsberg’s two-color experiment by systematically varying the information available about the prize. Subjects decide on an urn (H or U) and a color (black or red). If their color matches that of the ball drawn from the respective urn, subjects receive an envelope that is marked with an equal sign (\(=\)). If not, they receive a (different) envelope that is marked with an unequal sign (\(\not =\)). We consider three situations. In situation O (for open envelope), subjects see the contents of the envelopes. There are 3 euro in the envelope with the \(=\) sign and 1 euro in the other envelope. Situation O corresponds to the usual Ellsberg experiment. In addition, we consider the following two variations. In situation S (for sealed envelope), subjects only know that one of the two envelopes contains 3 euro and the other 1 euro but they do not know which amount is in which envelope. In situation R (for random), subjects know that the content of the envelope (3 euro or 1 euro, respectively) will be determined by flipping a fair coin after they have made their choice on which urn to bet. Since situation O describes the standard Ellsberg experiment, ambiguity-averse subjects should strictly prefer to bet on the urn with the known composition of colors. In situation \(R\), one could argue in the spirit of Raiffa (1961) that decision makers face equal odds of winning the 3 euro no matter which urn they choose.Footnote 1
 For situation S, one could argue as follows: “Given that I have no way of knowing what I win if I win, I should not care whether I win.”Footnote 2 Given this line of reasoning (which is actually entertained by at least two of the authors of this paper), a subject should not care whether he bets on the known or the unknown urn. Predicting formally how ambiguity-averse subjects behave in situation \(S\) is more involved. Our starting point is to represent ambiguity aversion using the MaxMin Expected Utility (MEU) approach by Gilboa and Schmeidler (1989). Later, we also discuss alternative approaches and their predictions like Choquet expected utility (CEU) pioneered by Schmeidler (1989) and the smooth ambiguity model by Klibanoff et al. (2005). Suppose now that subjects believe the envelope’s content to be independent from the color of the ball drawn, which can be formalized with the notion of independence advanced by Gilboa and Schmeidler (1989). Given this notion and the MEU representation, we show that decision makers who strictly prefer to bet on urn H in situation O would also do so in situation  S but be indifferent in situation R. Our results are inconsistent with these predictions. As usual in such experiments, about 2/3 of subjects are ambiguity averse in the sense that they prefer to bet on urn H in situation O. However, we find that in situation S, this share drops significantly. Given that the contents of the envelopes are ambiguous, the additional ambiguity about the content of urn \(U\) seems to be of secondary importance. Finally, few subjects seem to be indifferent between urns in situation \(R\), although popular theories like SEU and MEU (derived in the Anscombe–Aumann framework) predict them to be so. Instead, most subjects still strictly prefer to bet on urn H. Apart from very mild assumptions on the set of priors, our theoretical predictions rely decisively on the assumption that this set is a product set of the set of priors for the urn composition and the envelope’s content. Without this assumption, subjects who strictly prefer urn H when the envelope’s content is known, may well cease to do so when it is unknown. The rest of the paper is organized as follows. In the next section, we review the related literature. In Sect. 3, we describe the experimental design and procedures. In Sect. 4, we derive various theoretical hypotheses. Results are analyzed and discussed in Sect. 5. In Sect. 6, we discuss alternative models of ambiguity. Finally, we close with a brief discussion of the implications of our findings in Sect. 7.",9
78.0,3.0,Theory and Decision,28 April 2014,https://link.springer.com/article/10.1007/s11238-014-9432-5,Are individuals more risk and ambiguity averse in a group environment or alone? Results from an experimental study,March 2015,Marielle Brunette,Laure Cabantous,Stéphane Couture,Female,Female,,Mix,,
78.0,3.0,Theory and Decision,09 March 2014,https://link.springer.com/article/10.1007/s11238-014-9425-4,Relative difference contest success function,March 2015,Carmen Beviá,Luis C Corchón,,Female,Male,Unknown,Mix,,
78.0,3.0,Theory and Decision,20 March 2014,https://link.springer.com/article/10.1007/s11238-014-9426-3,Introducing difference into the Condorcet jury theorem,March 2015,Peter Stone,,,Male,Unknown,Unknown,Male,"As originally formulated, the Condorcet jury theorem (CJT) demonstrates that collectives can outperform individuals. Those collectives, however, were fundamentally homogeneous in the CJT’s original formulation; all individuals in the collective made decisions in basically the same way, and all were equally good at it. The CJT therefore fails to capture the idea that decision-making benefits from presence of difference—that is, the involvement of a diversity of types of people, with different backgrounds, lifestyles, belief systems, etc. This idea is currently receiving a good deal of attention (e.g., Surowiecki 2005; Page 2008), but it has not yet been explored in the CJT framework. This paper attempts this task using the notion of bias in decision-making. It applies this notion, in the CJT framework, to the problem of identifying how difference contributes to collective decision-making, and how deliberately selecting for difference can optimize collective performance. I begin by reviewing the extant literature on the CJT, with particular emphasis upon the notion of biased decision-making. I then explore the conditions under which a heterogeneous committee—a committee composed of people with different forms of bias—will outperform a homogeneous committee. I conclude by reviewing the implications for the CJT and for the role of difference in collective decision-making.",8
78.0,3.0,Theory and Decision,13 March 2014,https://link.springer.com/article/10.1007/s11238-014-9427-2,Negative agency costs,March 2015,Jacques Thépot,,,Male,Unknown,Unknown,Male,"The article by Jensen and Meckling (1976) is one of the most cited papers in Finance and is today referred to in all textbooks as the seminal contribution to the issue of ownership and control issue that has become central to governance theory. That article provides a microeconomic analysis of the agency costs generated by the existence of outside equity: In most agency relationships, the principal and the agent will incur positive monitoring and bonding costs, and in addition, there will be some divergence between the agent’s decisions and those decisions which would maximize the welfare of the principal. The dollar equivalent of the reduction of welfare experienced by the principal due to this divergence is also a cost of the agency relationship, and we refer to this as the residual cost. We define agency costs as the sum of (i) the monitoring expenditures by the principal (ii) the bonding expenditures by the agent (iii) the residual loss. (Jensen and Meckling 1976, p.308). Disregarding the monitoring and bonding costs, the agency cost measures the loss in gains from trade of the principal specifically due to the divergence of interest. This loss is computed with respect to a benchmark arrangement where the managerial opportunism of the agent is neutralized. To be conceptually consistent, this arrangement has to take the form of a contract that becomes accessible to the parties if some institutional rules of verifiability are met. Of course, such a benchmark contract can also be refused by the parties who are both entitled to consider outside opportunities. In such circumstances, measuring agency costs amounts to comparing two types of contracts, each of them being associated with outside opportunities and surpluses. These outside opportunities are related to external market conditions that determine the agency costs and, in some cases, may lead to negative values. Examining the circumstances where this occurs is the subject of this paper. According to Shleifer and Vishny (1997), four main sources of managerial opportunism may lead the management not to act in the shareholder’s best interest: (i) insufficient effort. (ii) extravagant investment. (iii) entrenchment strategies and (iv) self-dealing. Private benefits or perks are the most usual form of self-dealing behavior in a company. The Jensen–Meckling equity model deals with private benefits extraction in a deterministic environment. Following Hellwig (2009), the Jensen–Meckling formulation is a particular case of a general outside finance agency model with insufficient effort. It can then be considered the root of the corporate agency literature that deals with asymmetric information, risk aversion, and default in various financial contracting and informational contexts (cf. Tirole 2006 for a comprehensive overview and Gale and Hellwig 1985; Hart and Moore 1998; Myers 2000, as significant contributions). To the best of our knowledge, the Jensen–Meckling model is among the very few agency type models that provide a tractable measure of agency costs: hence the idea of discussing the negativity of agency costs in a closely related framework. To this purpose, we exhibit the game-theoretic base of the Jensen–Meckling equity model with a focus on the outside opportunities available to the parties. We consider an entrepreneur or a manager (she) who wants to undertake a project that cannot be financed by debt or self-financing. To cover the capital requirement, she needs to raise equity from an outside investor (he) who has alternative access to financial resources at a given rate \(r\). Rate \(r\) expresses the lack of competitiveness of the investment banking industry in which the investor operates. The intensity of competition prevailing in the banking industry has been extensively discussed in the empirical literature (Bikker and Haaf 2002; Van Leuvensteijn et al. 2013). Perfect competition holds in industrialized economies while oligopolistic competition seems to prevail in emerging or developing countries, owing to a low level of institutional development (Berck et al. 2004). In addition, it is also claimed that competition in banking may create instability by increasing incentives to take risks (Boyd and De Nicolo 2005; Vives 2010; Berck et al. 2013). Restricting the intensity of competition in banking must also be promoted by the authorities as a way to ensure the stability of the financial system. All this justifies the existence of a strictly positive interest rate used by the outside investor as a member of the banking sector. We thus depart from the assumption of perfect (or Bertrand-type) competition usually made (for notational simplicity) in corporate finance models (e.g., Tirole 2006). This assumption implies a zero-profit condition for outside investors. For high levels of the interest rate \(r\), we will show that the agency costs may be negative. This means that the agency inefficiencies are mitigated when the financial environment lacks competitiveness. Managerial opportunism deals with income verifiability, which means perfect and costless monitoring of the action of the managers in this context. If the expenses and gains that the manager makes in the name of the corporation are verifiable, the agency problem does not arise. If they are not, she can expropriate some part of the benefits. The agency cost is evaluated by the difference in surpluses between the verifiable and the non-verifiable arrangements. Thus, agency cost is a difference of differences. The paper is organized as follows. The first section is devoted to the presentation of the model. Two types of arrangements are considered according to whether the investor (Sect. 3) or the manager (Sect. 4) gets the leadership in the contract setting. The leadership assignment reflects the balance of bargaining power between the outside investor and the company. When the interest rate is zero, the latter case coincides with the standard Jensen–Meckling equity model. In both situations, the agency cost is measured as the variation in the leader’s payoff due to the lack of verifiability. Under both types of arrangement, this variation is negative for high-interest rates. When the investor acts as the leader, verifiability is worthless to him since it implies a higher level of investment and thus a higher capital cost that reduces his gain. When the manager acts as the leader, high values of \(r\) reduce the ex ante inefficiency in the agency relationship. As a result, the manager may fix the perquisites level at her convenience and increase her utility. In Sect. 5, the efficiency of governance policy is discussed in terms of two factors, one internal and the other external. First, we define the attractiveness of the private benefits as the rate of substitution of a separable linear-quadratic utility function of the manager. We prove that managerial opportunism may improve the value of the firm when the attractiveness of the private benefits is high. Second, since the intensity of competition in the banking sector is a key determinant of the interest rate, we argue that industrialized countries have stronger incentives to implement governance rules than less-developed countries. In Sect. 6, we discuss the limitations of our approach in terms of risk factors consideration. Concluding remarks are given in Sect. 7.",1
78.0,3.0,Theory and Decision,16 April 2014,https://link.springer.com/article/10.1007/s11238-014-9429-0,Scoring rules and social choice properties: some characterizations,March 2015,Bonifacio Llamazares,Teresa Peña,,Male,Female,Unknown,Mix,,
78.0,3.0,Theory and Decision,28 April 2014,https://link.springer.com/article/10.1007/s11238-014-9435-2,Prospectism and the weak money pump argument,March 2015,Martin Peterson,,,Male,Unknown,Unknown,Male,"Your preferences are insensitive to mild sweetening if there are items A, A\(^{+}\), and B such that, all things considered, you have no preference between A\(^{+}\) and B, you prefer A\(^{+}\) over A, and you have no preference between A and B (See Fig. 1). Mild Sweetening 
Hare (2010) proposes a view he calls prospectism for making choices in situations in which preferences are insensitive to mild sweetening. In this note, I show that prospectism permits the decision-maker to make a series of choices she knows in advance will lead to a sure loss. I also argue that a theory that permits the decision-maker to make choices she knows in advance will lead to a sure loss should be rejected. Previous so-called money pump arguments (against cyclic preferences) rely on stronger assumptions. Those arguments are based on the idea that a theory should be rejected if the decision-maker is rationally obliged to make a series of choices she knows in advance will lead to a sure loss.Footnote 1 The alternative, weak money pump argument can, therefore, be applied to a wider class of preference structures, including mild sweetening cases. This makes the weak money pump argument more powerful. In Sect. 2, the weak money pump argument is contrasted with strong versions, and we also discuss the difference between the weak money pump argument against prospectism and related ideas discussed by Broome (2000) and Peterson (2007). Section 3 discusses and responds to some objections to the weak money pump argument against prospectism.",5
78.0,3.0,Theory and Decision,05 April 2014,https://link.springer.com/article/10.1007/s11238-014-9430-7,The Tiebout hypothesis under membership property rights,March 2015,Goksel Asan,M. Remzi Sanver,,Unknown,Unknown,Unknown,Unknown,,
78.0,3.0,Theory and Decision,13 April 2014,https://link.springer.com/article/10.1007/s11238-014-9433-4,Auctioning a discrete public good under incomplete information,March 2015,Murat Yılmaz,,,Male,Unknown,Unknown,Male,"Optimal mechanisms, in a setting where provision of a public good is private, involve complicated transfer schemes. These mechanisms seem implausible as a description of private provision of public goods. Ledyard and Palfrey (1999) point out this issue: ‘[\(\ldots \)]There are several directions worth pursuing. One direction is to explore the use of simple mechanisms. The public good mechanisms proposed here involve complicated transfer schemes that can necessitate the use of very large taxes and subsidies’. We respond to this problem by considering simple and plausibly private mechanisms which are expected to perform well globally. More specifically, we consider the use of auctions as a mechanism for private provision of public goods under incomplete information. Auction mechanisms have proven to be very useful when allocating private goods under asymmetric information.Footnote 1 They are also very simple and familiar. Thus, it is natural to consider auction mechanisms as a way of modelling private provision of public goods. We study an open ascending auction mechanism for public goods and prove a strong negative result regarding its efficiency properties. We focus on discrete public goods where the provision decision is only whether or not to produce the good. A street light, public radio fund-raising to finance a certain program and a toll-free bridge are typical examples of a discrete public good. The cost of providing the good is publicly known and each agent has a privately known valuation for the good. We show that there is no provision in a natural adaptation of open ascending auctions. We also provide alternative formats and analyze their probability of provision. In the open ascending auction mechanism we consider, each bidder, observing the ascending price, and whether the other bidder has dropped out or not, drops out at her preferred price, and a bidder’s contribution is her drop-out price. If the contributions add up to a level at least as big as the cost of the public good, then the good is provided; otherwise, there is no provision and no payments are made.Footnote 2 The reason for the probability of provision being zero in any equilibrium in this open ascending auction mechanism is the following: Because of the sequential nature of the contributions, each bidder is eager to be the first one to contribute in order to free ride on the contribution of the other bidder. By committing to a low level of contribution, a bidder can force the other to contribute the rest of the cost. Thus, no bidder is willing to win the auction; that is, no bidder wants to be the last one to drop out. Thus, the bids are too low, and as a result, there is no provision. This is particularly striking, because although the first bidder to drop out is able to free ride on the other bidder, she also faces the risk that the other bidder may not value the public good high enough for it to be provided.Footnote 3
 Private provision of public goods and its efficiency properties have been vastly studied. In a sequential contribution game under complete information where the amount of the public good to be provided is continuous, the ability to the first mover to credibly commit to a certain level of contribution aggravates the free rider problem (Varian 1994). On the other hand, under incomplete information, a sequential contribution mechanism may perform better than the simultaneous contribution game, in terms of total expected contributions (Bag and Roy 2011). In a sequential contribution game where the public good is either provided or not, the outcome is inefficient if the contributions are sunk, and there is no commitment device. For the case where the contributions are borne if and only if they cover the cost of the public good, the outcome is efficient (Admati and Perry 1991). However, if there is imperfect information about individual actions (aggregate contribution is observed but not the individual contributions), and if players can contribute more than once and at any time, then efficiency is achieved under certain conditions (Marx and Matthews 2000).Footnote 4 In a model of private provision of public goods, the set of undominated perfect equilibrium outcomes is identical to the core (Bagnoli and Lipman 1989).Footnote 5 The private provision mechanism for a discrete public good through both contribution game and subscription game (a simultaneous contribution game with full refunds) is studied and some interim inefficiency results have been shown (Barbieri and Malueg 2008a, b). Barbieri and Malueg (2010) show that, restricting attention to piecewise-linear equilibrium, the subscription game achieves the outcome of the optimal mechanism for a profit maximizing seller. To study continuous equilibria in the private provision game, tools familiar from auction theory are used (Alboth et al. 2001) and Menezes et al. (2001). Auction-like mechanisms are also used to allocate excludable public goods (Deb and Razzolini 1999). Mechanisms in the spirit of \(k\)-th lowest bidder auctions are used in a setting where a group of people must determine which of its members should provide an indivisible public good (Kleindorfer and Sertel 1994). Schmitz (1997) considers a profit maximising monopolist who provides a discrete and excludable public good to a group of \(n\) potential consumers whose valuations are private information, and characterizes the optimal contract.",1
78.0,4.0,Theory and Decision,02 May 2014,https://link.springer.com/article/10.1007/s11238-014-9436-1,A classification of weakly acyclic games,April 2015,Krzysztof R. Apt,Sunil Simon,,Male,,Unknown,Mix,,
78.0,4.0,Theory and Decision,18 April 2014,https://link.springer.com/article/10.1007/s11238-014-9434-3,Reconciling cost-effectiveness with the rule of rescue: the institutional division of moral labour,April 2015,Shepley Orr,Jonathan Wolff,,Unknown,Male,Unknown,Male,"How much should we spend to save a life? Even to ask this question appears heartless, and to betray a fundamental lack of humanity. When lives are at stake, how can we even think of money? Yet it is clear that in the context of health systems we can always do more. We can keep people alive for longer, or at least increase their chances of survival. When treatments are cheap, or, perhaps, where the numbers of people needing help are very small, the decision can be easy to make. Often, however, we find ourselves in circumstances where life saving or extending treatments are very expensive. To fund costly treatments on a large scale threatens to be damagingly expensive, driving expenditure from areas where it could do more total good. The promptings of basic humanity can, therefore, come into worrying conflict with mundane, but vitally important considerations of cost-effectiveness. In the literature on health care allocation the ‘humanitarian’ argument has often been couched in terms of a supposed ‘rule of rescue’ (Jonsen 1986; McKie and Richardson 2003; Sheehan 2007). Outside the health care system from time to time, we are faced with ‘rescue situations’: a child who has fallen down a well; miners trapped underground; a submarine languishing at the bottom of the ocean. In all of these cases, society responds as a matter of supreme urgency and apparently without reference to questions of cost. Action is required, and it is attempted. Few economists would dare enter the public debate to raise questions about whether the rescue is cost-effective, diverting resources from a more effective use. In the world of health care allocation, we rarely meet situations that are directly analogous to rescue cases. But nevertheless there are many examples where a life is in immediate peril and steps could be taken that have a chance of making a difference. Emergency surgery, intensive care, antibiotics for severe infections, transplants or artificial organs, highly active anti-retro-viral therapy (HAART) and life-preserving cancer drugs all can make the difference between life and death. In health care, it seems, rescue, in some form, is routine. Should we put cost-effectiveness analysis to one side in such cases, or would it be morally irresponsible to do so? In this paper, we will argue that the standard methodology in health technology assessment already provides a plausible balance between cost-effectiveness and rescue. We do not claim that any standard methodology at present provides exactly the right balance, but rather that something like an institutional division of moral labour is a permissible method for achieving the correct balance between humanity and cost-effectiveness. But to make this case, we need first to review the standard debate, and to consider what we believe to be an unsuccessful attempt to achieve a reconciliation, in order to be in a position to present our own understanding.",13
78.0,4.0,Theory and Decision,01 May 2014,https://link.springer.com/article/10.1007/s11238-014-9439-y,The \(\chi \) value and team games,April 2015,Tobias Hiller,,,Male,Unknown,Unknown,Male,"In their current article, Hernández-Lamoneda and Sánchez-Sánchez (2010) analyse team games. These games can have non-zero worths only on coalitions with a certain cardinality t. For example, they have players in the NBA in mind. For a basketball game, we have \(t=5.\) Only coalitions/teams with five players make sense. Only these coalitions could get a higher worth than zero, depending on how successful such a coalition is. Another example is given by a poker game. Every card of a deck of poker is a player. Hence, we have 52 players. Every coalition with five players is called a hand of poker. Every hand gets a certain worth depending on the possible confrontations with other hands. All other coalitions of cards get the worth zero. 
Hernández-Lamoneda and Sánchez-Sánchez (2010) apply the Shapley value (Shapley 1953) to team games. This value assumes that all players cooperate; the worth generated through joint effort of all the players will be divided among them. If t is smaller then the number of players, this worth is zero. Hence, for these cases it is a plausible conjecture that not all players work together. Rather, groups with cardinality t will be formed. To model this, coalition structures are used. These divide players into disjoint subsets/components. The most popular value for games with a coalition structure (CS games) is introduced by Aumann and Drèze (1974). This value assumes component efficiency, i.e. the worth of the component is divided among the players of the component.Footnote 1 However, differing outside options of players within a component do not bear on the payoff. This is not very realistic. With this idea in mind, Wiese (2007) introduced the Wiese value. This value accounts for outside options of the players. Additionally, Wiese (2007) applies the Wiese value to measure the power of parties in government coalitions. The main idea of this application is strongly related to the basic motivation of our paper. Wiese (2007) assumes that not all players in a parliament work together—some of them form the governing coalition. Hence, a component efficient CS value reflecting the outside options of the parties is needed for the analysis. Inspired by the Wiese value, Casajus (2009) presents the \(\chi \) value. Also, this value is applied to analyse the distribution of power of parties in governing coalitions (Hiller 2013). The main advantage of the \(\chi \) value with respect to the Wiese value is a “nicer” axiomatisation and a more intuitive definition of the player’s payoffs (Casajus 2009). Analyses of situations making use of methodologies from cooperative game theory are usually conducted in three major steps. In a first step, the situation is modeled by the coalitional function that assigns a certain worth to every group, reflecting the abilities of the groups. In a second step, structures on the set of players are formed. This structure can be a certain coalition structure. Other possible structures are graphs (Myerson 1977; Borm et al. 1992) or hierarchies (Gilles et al. 1992; Brink and Gilles 1996; Brink 1997), for example. In the last step, the payoffs of the players follow. In this context, our paper is on the second and third step of the analysis. Two main questions for team games are derived. First, which components will be established and second, how is the worth of a component divided among the members of the component. In this paper, we employ the \(\chi \) value to answer these questions for two special cases—symmetric games and the gloves game. The paper is organised as follows: basic notations of cooperative game theory as well as the definition of the \(\chi \) value are given in the next section. In Sect. 3, the main results are presented. Section 4 concludes the article with a summary of the results and some questions for further research.",3
78.0,4.0,Theory and Decision,30 April 2014,https://link.springer.com/article/10.1007/s11238-014-9437-0,"Temptation, horizontal differentiation and monopoly pricing",April 2015,Joaquín Gómez-Miñambres,,,Male,Unknown,Unknown,Male,"Consumers’ temptation is an important characteristic of the consumer purchasing behavior. Many consumers establish ex-ante that they would like to commit to consuming healthy, low calorie groceries. Nonetheless, ex-post temptation takes place and they modify their choices toward unhealthier alternatives. This behavior reflects the dynamic inconsistency of consumers’ preferences commonly known as temptation. An important implication is that when consumers are aware of their future change in preferences, they are more willing to enter stores which do not carry unhealthy products in order to avoid ex-post choices inconsistent with ex-ante preferences. To capture this idea, Strotz (1955) and Kreps (1979) introduced a class of preferences that incorporates consumers’ temptation. Let M denote the consumers’ choice set and U the utility function—the commitment utility function—that he has when making the shopping list. The consumer anticipates that, once inside the store, his utility function will change to V with a positive probability \(\pi \). Let \(x^{u}\) denote the consumer’s choice with U –his commitment choice–and \(x^{v}\) denote his choice with V –his tempting choice. Then, the consumer’s expected ex-ante utility is given by Following Dekel and Lipman (2007, 2012), we refer to this class of preferences as the “random Strotz representation.”Footnote 1 In this setting, a dynamic consistency problem arises because, while the consumer would like to commit ex-ante to choose \(x^{u}\), with probability \(\pi \), he ends up choosing \(x^{v}\). We can interpret this preference representation as though the consumer had, ex-post, two different possible selves: a tempted and a committed self. Although simplified, this representation is convenient for our purposes as it is a very simple and tractable way to create an ex-ante demand for menus that implement commitment.Footnote 2 Other authors as Eliaz and Spiegler (2006) consider this consumer representation. A novel contribution of this paper is that we analyze a monopolist’s optimal pricing problem when the product is horizontally differentiated. This allows us to capture temptation as a change in the consumer’s ideal product on the Hotelling line. We consider a continuum of consumer types; each consumer type knows that he has two (possibly distinct) ideal products on a Hotelling line, one when committed and another one when tempted. In the basic model, we assume that all consumer types have the same temptation ideal product, located at one extreme of the Hotelling line, but they differ when committed. This assumption captures the intuitive idea that consumers tend to have more similar (and extreme) tastes when tempted. People tend to be tempted by not working, and not saving or consuming unhealthy, high calorie products. However, when committed, individuals are more likely to differ in the effort they are willing to provide, the money they plan to save or the calories they want to consume. Therefore, even though there is horizontal differentiation in commitment preferences, when tempted, consumers are usually more attracted by the same sort of products. Later in the paper, as well as in an online appendix, we shall study several generalizations of this temptation representation. In this paper, we emphasize that, even though the notion of temptation suggests that there is some vertical ordering in the products being consumed under commitment and temptation, respectively, there is also a fundamental horizontal differentiation. In order to clarify this point, let’s consider the following example. Two consumers (subject A and subject B) enter a burger restaurant. Burgers have two important dimensions that consumers take into account: health and taste. By simplicity, we assume that there are only two possible varieties with equal quality and price but differing in the calorie component: beef burger and tuna burger. Both consumers agree that tuna burger is healthier but less tasty than beef burger. When consumers are committed (i.e., they make decisions according to commitment preferences) they both care about health, but subject B is more concerned than subject A. However, when consumers are tempted (i.e., they make decisions according to temptation preferences), they are exclusively focused on the taste dimension of the products (see Fig. 1). Indifference curves of temptation (V) and commitment (U) utilities for each subject In our example, subject A’s “ideal product” is always beef. However, subject B has two ideal products: beef burger when tempted and tuna burger when committed. This example fits well with the theoretical framework of our model. There is a vertical ordering of the products, in the sense that both consumers agree that the tastier alterative is the least healthy and vice versa. However, since preferences vary across the population, consumers’ ideal products will be different when committed (horizontal differentiation) but they will be the same when tempted (see Fig. 2) Ideal products of subject A and subject B under commitment and temptation To capture the idea that ideal products may be different under commitment, we consider a monopolist selling several product varieties that can differ in their location on the Hotelling line and in pricing. Therefore, the firm’s problem is to decide which goods to offer on the Hotelling line and charge a price for each one in such a way that expected profits are maximized. Since consumers are aware of the dynamic inconsistency of their preferences, when designing the optimal selling strategy, the firm has to worry about both: their incentives to enter the store (ex-ante IR), and their incentives to participate once inside (ex-post IR). Moreover, the firm must ensure that once inside the store, each consumer chooses the product designed for himself (ex-post IC). Using this model, we can understand the relationship between consumers’ temptation and the firm’s optimal product design. Does temptation increase or decrease product diversity? Do prices increase with temptation? Does it have welfare implications? In the standard horizontal differentiation model without temptation, the monopolist offers the ideal product of each consumer type. In our model, instead, the firm faces the following trade-off: by positioning products closer to the temptation preferences, it can increase its profits under the temptation state. However, it also decreases the consumers’ ex-ante utility, especially for those with a greater distance between their ideal products with commitment and temptation. The firm’s optimal menu may exclude products that are too close to the temptation preferences since otherwise these consumers will not derive sufficient utility from entering the store. In equilibrium, two types of consumers coexist: consumers with similar preferences in the two states, who always consume the same product; and consumers with most diverging preferences, who consume different products in different states. The size of the two consumer groups, which is given by the degree of product diversity, and the firm’s profits decrease with the probability of temptation. Intuitively, our results imply that when consumers are aware of their dynamic change in preferences, the firm cannot take advantage of consumers’ temptation but instead, in order to attract them into the store, the firm must compensate ex-ante consumers for the possibility of yielding to temptation once inside the store. The firm can only attract committed consumers truncating the set of products offered, not offering the products closest to temptation, and charging lower prices to the products offered. In recent years, several authors have explored the implications of consumer temptation on pricing. A paper most related to our work is Esteban et al. (2006). Using Gul and Pessendorfer (2001) preferences and a vertical differentiation environment, they construct a model in which a monopolist chooses the price and quality of the goods it offers the consumer once inside one store.Footnote 3 There are two main differences between our model and that of Esteban et al. (2006). First, we consider horizontal product differentiation, whereas they consider vertical differentiation. In Sect. 6, we show that temptation may have different effects under horizontal and vertical product differentiation. Second, we use random Strotz preferences, whereas they use Gul and Pessendorfer (2001) preferences. In Sect. 5.3, we show that the optimal menu differs under these two temptation representations. Despite of these differences, we find that, as in Esteban et al. (2006), the optimal menu is bounded as a result of temptation. This is an interesting result indicating that different temptation representations with a preference for commitment will have similar results under different product differentiations. There is also an important methodological contribution of this paper; our Hotelling model allows us to capture temptation problems in a very intuitive and simple way. Moreover, the tools that we use to find the optimal menu can be easily applied to more general frameworks (see online appendix). 
Eliaz and Spiegler (2006) study a model in which dynamically inconsistent agents sign a contract with a firm, using the same random Strotz representation used here. In contrast with our model, they assume that while the firm correctly anticipates the consumer’s inconsistency, consumers incorrectly believe that, with some probability, they are going to take actions in accordance with their “commitment” preferences. The “non-common priors” assumption is the source of the exploitative contracts that arise in equilibrium. Thus, the monopolist offers a menu which would not be accepted by consumers if they had the same priors as the firm. In contrast, our model assumes that consumers are perfectly aware of their dynamic inconsistency, i.e., they are “sophisticated” consumers. However, in Sect. 5, we extend our model to consider “naive” consumers. The main difference with the Eliaz and Spiegler model is that instead of differing in their temptation beliefs, in our model, consumers differ in their commitment preferences. There are other papers that study mechanism design for consumers with time inconsistent preferences; however, they differ from the current paper by focusing on present-biased preferences with hyperbolic discounting. For instance, O’Donoghue and Rabin (1999) study the optimal contract that the firm must offer to a worker who is naive, i.e., he is unaware of his dynamic inconsistency. DellaVigna and Malmendier (2004) study a model with two kinds of goods: investment goods with immediate costs and delayed benefits, and leisure goods with immediate benefits but delayed costs. In equilibrium, firms price investment goods below marginal costs and leisure goods above marginal costs. If consumers are fully aware of their dynamic inconsistency, then the agents achieve a socially efficient solution, but if agents are naive, then the equilibrium is inefficient. Sarafidis (2005) constructs a model in which consumers form expectations not only about their future behavior but also about the firm’s prices. The paper proceeds as follows. Section 2 describes the basic model. In Sect. 3, we present a simple example that emphasizes the key elements of the model. Section 4 characterizes the monopolist’s optimal menu. Section 5 extends the model relaxing some assumptions and studying more general specifications of consumer preferences. Section 6 compares temptation under horizontal and vertical differentiations. Finally, Sect. 7 concludes.",5
78.0,4.0,Theory and Decision,21 May 2014,https://link.springer.com/article/10.1007/s11238-014-9442-3,Minimax and the value of information,April 2015,Evan Sadler,,,Male,Unknown,Unknown,Male,"In recent years, there has been a revival of interest in the application of minimax procedures to statistical and econometric problems. Manski (2004), for instance, has spawned a literature on the application of minimax regret to treatment choice, and Stoye (2011) has carefully examined the axiomatic distinctions between different minimax rules. One reason for this newfound popularity is surely the increase in computing power now available to researchers, rendering such methods feasible in more domains. With renewed interest has come increased scrutiny. In particular, concerns over the use of information by minimax procedures have resurfaced. It has long been known that minimax applied to negative expected utility (hereafter, “negative income”) can entirely ignore available information in some decision problems. Savage (1954, p. 170) presented a simple example demonstrating the phenomenon, asserting that minimax applied to negative income was entirely inadequate as a criterion for statistics. His alternative was applying minimax to regret. While many credit Savage with the invention of the minimax regret criterion, Savage himself gave priority to Wald (1950), believing that Wald could not possibly have intended minimax negative income in his work. Savage clearly believed this critique inapplicable to regret, but more recent work has disputed this. 
Parmigiani (1992) analyzes an example in which minimax applied to negative income assigns positive value to an experiment, while minimax regret does not. He goes on to characterize the circumstances under which minimax rules refrain from “relevant” experimentation, even if the experiment is nearly costless. Parmigiani describes this phenomenon as ultrapessimism. The essence of his result is best understood if we interpret the statistician’s problem as a zero-sum game played against nature. If nature has an equilibrium strategy that is supported on relatively few states, then experiments that fail to distinguish these states will be worthless. If such cases can be expected to occur regularly outside of contrived examples, then this raises serious concerns about the use of minimax regret in applied work. One goal of the present paper is to controvert this criticism of minimax applied to regret. Savage’s example uncovered a more fundamental flaw in minimax negative income than the one Parmigiani discusses. In some decision problems, minimax negative income not only fails to utilize an observation that another procedure finds valuable, but in fact no conceivable experiment provides any value to an agent following a minimax negative income rule. This is in stark contrast to the situation we find with minimax regret, where we can always construct a valuable experiment. Moreover, even when valuable experiments do exist for the minimax negative income rule, there may still be some states for which no amount of evidence in their favor will cause the decision maker to adopt an action better suited to those states. Again, minimax applied to regret avoids such issues. On further examination, the characterization of ultrapessimism Parmigiani gives can apply to Bayesian procedures just as easily—even when a minimax regret agent would pay for the information. Ultrapessimism, so defined, fails to distinguish meaningfully between decision procedures, but Savage’s observation remains a substantive distinction between minimax regret and minimax negative income. A second goal is to critically examine other objections to minimax regret. There are other senses in which we can describe the procedure as “pessimistic,” and these may present challenges to researchers in applications. Many issues stem from prescriptions that go against what we intuitively expect from a “good” estimator. In a sense, regret is criticized for failing to conform to our prior information, but a major motivation for minimax procedures is to avoid subjective priors. An important direction for future work lies in devising ways to utilize such information alongside minimax procedures. In the next section I provide a detailed analysis of the examples given by Savage (1954) and Parmigiani (1992). The following section generalizes previous results that characterize when minimax values information. This section also establishes the key distinction between minimax applied to negative income and minimax applied to regret. Finally, I offer some thoughts on the problem of “pessimism” in minimax estimation and the effective application of minimax regret.",
78.0,4.0,Theory and Decision,17 May 2014,https://link.springer.com/article/10.1007/s11238-014-9440-5,Strategically equivalent contests,April 2015,Subhasish M. Chowdhury,Roman M. Sheremeta,,Unknown,Male,Unknown,Male,"A contest is a game in which players expend costly resources, such as effort, money, or time, in order to win a prize. Since the seminal papers of Tullock (1980) and Lazear and Rosen (1981), many different contests have been introduced to the literature. For example, Skaperdas (1992) studies contests where the final payoff depends on the residual resources and the prize. Chung (1996) and Kaplan et al. (2002) examine contests with effort-dependent prizes. Lee and Kang (1998) and Baye et al. (2005) study contests with rank-order spillovers. Although these contests are intuitively and structurally very different, they often share common links. There are several studies that establish common links between different contests. For example, Che and Gale (2000) provide a link between a rank-order tournament of Lazear and Rosen (1981) and an all-pay auction of Hillman and Riley (1989); Baye et al. (2012) show the connection between the all-pay auction and pricing games (Varian 1980; Rosenthal 1980). Hirshleifer and Riley (1992) show how an R&D race between two players which is modeled as a rank-order tournament is equivalent to a rent-seeking contest.Footnote 1
Baye and Hoppe (2003) identify conditions under which research tournament models (Fullerton and McAfee 1999) and patent race models (Dasgupta and Stiglitz 1980) are strategically equivalent to the rent-seeking contest. These duality results permit one to apply results derived in the rent-seeking contest literature to the innovation, patent race, and rank-order tournament models, and vice versa. In this paper, we show that intuitively and structurally different contests can be strategically and effort equivalent. We consider a two-player Tullock-type contest, where outcome-contingent payoffs are linear functions of prizes, own effort, and the effort of the rival. Under this structure, we identify strategically equivalent contests that generate the same best response functions and, as a result, the same equilibrium efforts. However, the strategically equivalent contests may yield different equilibrium payoffs. It is important to emphasize that the aforementioned studies establish links between different families of contests, such as all-pay auctions, rent-seeking contests, and rank-order tournaments. The main result of this paper is conceptually different from the findings of the previous studies. In particular, we show that even within the same family of Tullock-type contests, different kinds of contests might produce the same best response functions and the same equilibrium efforts (although not necessarily the same payoffs). 
Tullock (1980) introduced a contest model in which resources expended by a player improve the probability of winning the contest, but can never make the winning probability certain. These types of contests are characterized with high volume of noise in the outcome. Rent-seeking, legal disputes, and sports competitions are often modeled using the Tullock contest model. In a simple two-player lottery Tullock contest, the best response curves are inverted U-shaped, implying that best responses are first strategic complements and then strategic substitutes. It is also the case that a unique pure strategy Nash equilibrium exists, in which each player expends a quarter of the prize. Since the seminal work by Tullock, numerous studies modified the original model to incorporate various structural and policy-related issues such as tax/subsidy (Glazer and Konrad 1999), endogenous prize (Chung 1996; Amegashie 1999), depreciation (Alexeev and Leitzel 1996), spillover (Chowdhury and Sheremeta 2011a), reimbursement (Matros and Armanios 2009), externality (Lee and Kang 1998), liability structure (Skaperdas and Gan 1995), and litigation issues (Farmer and Pecorino 1999)—to name a few (see Konrad 2009 for an extensive review). Although these models use the same framework of the Tullock contest, the final outcomes are often very different. To the best of our knowledge, no study tried to examine possible equivalence among the aforementioned models. Introducing equivalence among Tullock-type contests is important for a number of reasons. First, there exists a substantial literature modeling the rules of the contest as an endogenous choice of a contest designer (Dasgupta and Nti 1998; Epstein and Nitzan 2006; Corchón and Dahm 2011; Polishchuk and Tonis 2013). A contest designer can choose the parameters of the model to maximize the total rent dissipation (as in the case of rent-seeking), or maximize the equilibrium highest effort (as in R&D races), or minimize the total equilibrium effort (as in electoral races), or simply to enhance public welfare. Our results demonstrate that it is possible for a contest designer to achieve different goals using strategically equivalent contests. For example, the contest designer seeking Pareto improvement may choose a contest that generates the same equilibrium efforts, incurs the same costs, but results in higher expected payoffs for contestants. We show, in subsequent sections, that indeed such opportunity may exist. Under certain conditions, for example, a contest designer may prefer to implement a limited liability contest (Skaperdas and Gan 1995) than a contest that imposes full liability on contestants. Finally, certain contests may be not feasible (or too costly) to implement in the field due to regulatory restrictions and the possibility of collusion among contestants. However, such restrictions may not necessarily apply to other strategically equivalent contests. We show, for example, that a contest with endogenous valuation (Amegashie 1999) that may be hard to regulate, is equivalent to a contest with a simple taxation rule (Glazer and Konrad 1999). The remainder of the paper is organized as follows. Section 2 describes the basic model; Sect. 3 specifies the definitions of equivalence and provides with the conditions to achieve them. Section 4 describes contests from literature that are strategically equivalent to the original Tullock contest, as well as introduces a set of modified Tullock-type contests that are strategically equivalent to each other. Section 5 concludes.",19
78.0,4.0,Theory and Decision,14 May 2014,https://link.springer.com/article/10.1007/s11238-014-9441-4,Exchangeability and the law of maturity,April 2015,Fernando V. Bonassi,Rafael B. Stern,Sergio Wechsler,Male,Male,Male,Male,"The law of maturity (of chances) is the belief that less-observed events are mature and, therefore, more likely to occur in the future. The law of maturity has been observed in fields as varied as behavioral finance (Rabin and Vayanos 2010), cognitive psychology (Militana et al. 2010), and decision-making (Oppenheimer and Monin 2009). Simple examples of the law of maturity are: The belief that successive increments in a stock price will eventually cause it to decrease. The belief that the chance of obtaining tails in a coin flipping process increases as the proportion of observed heads increases. This paper focuses on finding conditions such that the law of maturity holds under a model of symmetry between observations. A commonly used statistical model of symmetry is that of infinite exchangeability Bernardo and Smith (1994). Hence, a possible initial consideration is whether infinite exchangeability is compatible with the law of maturity. This question is discussed in O’Neill and Puza (2005) [hereafter, OP] and Rodrigues and Wechsler (1993) [hereafter, RW]. Each paper provides a different probabilistic description of the law of maturity and shows that if a model assumes this description, then it is inconsistent with infinite exchangeability. In particular, both papers show that infinite exchangeability implies the law of reverse maturity—less-observed events are less likely to occur in the future. In the following, the aforementioned results are presented in greater detail. This presentation prepares some of the definitions that are used in this paper. In order to do so, consider that the sequence of discrete random variables \(\mathtt X = (X_{1},X_{2},\ldots )\) can be observed sequentially, and let \(\mathtt X _{n} = (X_{1},X_{2},\ldots ,X_{n})\). Let \(X_{i}\) take value on \(\{1,2,\ldots ,k\}\). Consider the following probabilistic descriptions of, respectively, the law of maturity and the law of reverse maturity: (gambler’s belief) The predictive probability \(P_{X_{n+1}|\mathtt X _{n}}\) of the next possible outcome is ordered inversely to the counts in \(\mathtt X _{n}\). 
(reverse gambler’s belief) The predictive probability \(P_{X_{n+1}|\mathtt X _{n}}\) of the next possible outcome is ordered according to the counts in \(\mathtt X _{n}\). If \(\mathtt X \) is infinitely exchangeable, and the prior distribution for the parameter induced by de Finetti’s theorem Finetti (1931) is exchangeable, then the reverse gambler’s belief holds (OP). Hence, the assumption of infinite exchangeability contradicts the law of maturity in this scenario. In another scenario, let \(X_{i}\) take value on \(\{0,1\}\). Consider the following probabilistic descriptions of, respectively, the law of maturity and the law of reverse maturity: 
(belief in maturity) The longer the streak of observed \(0\)’s, the higher the probability that the next observation is a \(1\). That is, one believes that \(P(X_{n+1}=1|x_{n}=0,\ldots ,x_{1}=0)\) increases with \(n\). 
(belief in reverse maturity) The predictive probability for the end of as streak, \(P(X_{n+1}=1|x_{n}=0,\ldots ,x_{1}=0)\) decreases with \(n\). If \(\mathtt X \) is infinitely exchangeable, then the belief in reverse maturity holds (RW). Thus, infinite exchangeability contradicts the law of maturity also in this scenario. The aforementioned results show that the law of maturity contradicts infinite exchangeability, an assumption of symmetry commonly used in Bayesian studies (Lindley and Phillips 1976). This contradiction motivates the search for weaker symmetry assumptions that support the law of maturity. This paper relates the law of maturity and the assumption of finite exchangeability. Section 2 describes the assumption of finite exchangeability. Section 3 considers this assumption and presents sufficient conditions for, respectively, the law of maturity and law of reverse maturity to hold. These conditions are exemplified with commonly used parametric models.",
78.0,4.0,Theory and Decision,21 May 2014,https://link.springer.com/article/10.1007/s11238-014-9445-0,A note on the ordinal equivalence of power indices in games with coalition structure,April 2015,Sébastien Courtin,Bertrand Tchantcho,,Male,Male,Unknown,Male,"Power is an important concept in the study of simples games. Power indices are quantitative measures and indicators that characterize the game. In the literature, we find several power indices. The most famous power indices are those of Shapley–Shubik (Shapley and Shubik 1954) and Banzhaf–Coleman (Banzhaf 1965; Coleman 1971). There are many other indices not discussed in this paper (see Andjiga et al. 2003), Laruelle and Valenciano (2008) for a detailed description of power indices). Because of the multiplicity of notions of power indices in simple games, it seems natural and important that comparisons between them be made. The idea is to base the comparison on their corresponding preorderings. Tomiyama (1987) proved that, for every weighted game, the preorderings induced by the classical Shapley–Shubik (SS) and Banzhaf–Coleman (BC) indices coincide. He calls this property the “ordinal equivalence” of the two indices. Instead of measuring the players’ voting power with an index, the desirability relation (Isbell 1958; Allingham 1975; Taylor 1995) consists in ranking them with respect to how much influential they are. A given voter who is never needed in any minimal winning coalition may be regarded as not being of any influence at all. On the contrary, if that player is indispensable to every minimal winning coalition, we may think that he is very influential. Most often, it is within these two extreme limits that the majority of voters lie. The ranking of voters with respect to their influence is called the desirability relation. It is a preordering on the set of players. 
Diffo and Moulen (2002) characterize simple games for which the preorderings induced by SS, BC, and the desirability preordering coincide. They proved that the three coincide if and only if the simple game is swap-robust (see Sect. 3). Since any weighted game is swap-robust, hence Diffo and Moulen (2002) generalize Tomiyama’s result. The problem with the traditional power indices which are based only on the set of winning coalitions is that they do not take into consideration a priori relations between different players. Indeed in many negotiations, some players prefer to cooperate with each other rather than with other players due to the existence of common interests. For example, consider international diplomatic relations, especially those of France and the United States. France belongs to the European Union, which is an a priori coalition of European countries, whereas the USA belongs to an a priori coalition with Mexico and Canada (NAFTA). Classical values do not take into consideration this kind of relationship. In order to represent these cooperation situations in a realistic way, some authors (see Aumann and Dréze 1974; Owen 1977, 1981, among others) introduced games with a priori coalition structure. A coalition structure is a partition of the player set in disjoint coalitions. In such a game, it is supposed that players organize themselves to defend their interests into a priori disjoint coalitions (structural coalitions). Owen (1977, 1981) proposed and characterized a modification of SS and BC indices with respect to a coalition structure, the well-known Owen–Shapley (OS) and Owen–Banzhaf (OB) indices. In this case, there is a two-level interaction between players. Firstly, coalitions play an external game among themselves, and each one receives a payoff; secondly, in internal games, the payoffs of each coalition are distributed among their members. Both payoffs, in the external game and in the internal game, are given by the Shapley value or the Banzhaf value. In this paper, along the line of Tomiyama (1987) and Diffo and Moulen (2002), we conduct an ordinal comparison of the desirability relation with the preorderings induced in the set of player of a simple game with coalition structure (denoted SGCS) by OS and OB indices, respectively. First of all, we extend the desirability relation to SGCS. We show that our extended desirability relation generalizes the desirability relation for simple games. Moreover, we extend the swap-robustness property and we prove that a necessary and sufficient condition for the extended desirability relation to be complete is that the game (endowed with a coalition structure) be swap-robust. Furthermore, for swap-robust SGCS, the desirability relation is a complete preorder. In such SGCS, players are pairwise comparable under the desirability relation. It is well known that there are two main approaches of power measuring in the literature: the cardinal measures of power and the ordinal measures of power. Many studies compare OS and OB from an axiomatic point of view. However, it seems intuitive to make a comparison based on their corresponding preorderings, as it is already done for simple game. This comes in reaction to the inconsistent evaluation of power by different theories. To the best of our knowledge, our study is the first work linking cardinal and ordinal approaches within the class of SGCS. It appears that unlike in simple game, OS and OB are not ordinally equivalent even if one considers the subclass of weighted games. This is a relevant result since it implies that the choice of power index for SGCS is an important issue from an ordinal point of view. Likewise, the desirability relation is different from OS and OB in general. Note that in Laruelle and Valenciano (2004), it is stated that comparisons among the Owen–Banzhaf index of different players make full and clear sense only when these players belonging to the same structural coalition. They are said to be partners. In this case, we prove that player i is at least as desirable as j if and only if i has as much power as j according to both OS and OB. Furthermore, even if i and j are not partners, and if structural coalitions have equal size in the partition then both OB and the desirability relation coincide. The remainder of this paper is organized as follows. Section 2 introduces basic notations and definitions. The extension of the desirability relation to simple games with coalition structure and some properties is presented in Sect. 3. Section 4 is devoted to the ordinal comparison of power theories, and Sect. 5 concludes the paper.",5
78.0,4.0,Theory and Decision,24 June 2014,https://link.springer.com/article/10.1007/s11238-014-9455-y,Rationalizing epistemic bounded rationality,April 2015,Konrad Grabiszewski,,,Male,Unknown,Unknown,Male,"As stated in Aumann (1976), the classical model of knowledge consists of state space, \(\varOmega \), and possibility correspondence, \(P\). Each state, \(\omega \), is a description of some possible world. Possibility correspondence is a function that assigns a subset of \(\varOmega \) to each \(\omega \). \(P(\omega )\) is interpreted as a collection of all states that the agent perceives to be possible if \(\omega \) is a true state. Dekel and Gul (1997), Fagin et al. (1995), Geanakoplos (1989), and Rubinstein (1998) discuss the model in great detail, including its interpretations and proofs of classical results. Knowledge axioms describe the properties of \(P\) as follows: Truth Axiom (\(\omega \in P(\omega )\)), Positive Introspection Axiom (if \(\tilde{\omega } \in P(\omega )\), then \(P(\tilde{\omega })\subset P(\omega )\)), and Negative Introspection Axiom (if \(\tilde{\omega }\in P(\omega )\), then \(P(\omega ) \subset P(\tilde{\omega })\)). Satisfying all knowledge axioms means that the agent is epistemically rational and her possibility correspondence is partitional. Epistemic bounded rationality (EBR) occurs when the agent violates at least one of knowledge axioms (see Geanakoplos 1989; Rubinstein 1998). In this paper, I consider the researcher who builds a model, \((\varOmega , P)\), that depicts the agent’s knowledge. The model hypothesized by the researcher may incorrectly represent the agent’s knowledge. In such a case, the researcher may falsely conclude the agent is not epistemically rational. If the researcher detects EBR, then he may suspect that the conclusion derives from his incorrect model of the agent’s knowledge. The researcher could investigate whether or not another model, \((\varOmega ^{*}, P^{*})\) exists, which satisfies all knowledge axioms. If such a model does exist, then the researcher rationalizes EBR. In this paper, I consider a specific structure of \((\varOmega ^{*}, P^{*})\). In particular, I impose that \(\varOmega ^{*}\) is defined as \(\varOmega \times X\) and \(P\) in \((\varOmega , P)\) is a projection of \(P^{*}\) to \(\varOmega \). Because of its construction, I call \((\varOmega ^{*}, P^{*})\) an extended model of knowledge. In the literature on unawareness, the agent is unaware of some events, but the researcher is assumed to be omnipotent. In this paper, I allow for the researcher who considers only \(\varOmega \) and is unaware of \(X\). The researcher’s assumed model, \((\varOmega , P)\), is derived from \((\varOmega ^{*}, P^{*})\). In Sect. 2, I discuss the relationship between \((\varOmega ^{*}, P^{*})\) and \((\varOmega , P)\) and show how the latter is obtained from the former. I also provide an interpretation of \(X\), which expands \(\varOmega \) to \(\varOmega ^{*}\). In Sect. 3, I determine when it is possible to rationalize EBR. I do this by fixing a model, \((\varOmega , P)\), that depicts EBR and investigating when that model can be extended to a model, \((\varOmega ^{*}, P^{*})\), which satisfies all knowledge axioms. In Sect. 4, I address a complementary problem. I fix a model, \((\varOmega ^{*}, P^{*})\), which satisfies all knowledge axioms and determine when that model generates a model, \((\varOmega , P)\), which also satisfies all knowledge axioms.",1
78.0,4.0,Theory and Decision,30 April 2014,https://link.springer.com/article/10.1007/s11238-014-9438-z,Is the newcomer more aggressive when the incumbent is granted a Right-of-First-Refusal in a procurement auction? Experimental Evidence,April 2015,Karine Brisset,François Cochard,François Maréchal,Female,Male,Male,Mix,,
79.0,1.0,Theory and Decision,16 July 2014,https://link.springer.com/article/10.1007/s11238-014-9457-9,Conditions on social-preference cycles,July 2015,Susumu Cato,,,Male,Unknown,Unknown,Male,"This paper is concerned with conditions for the cyclicity of social preference generated by a voting mechanism. Acyclicity of social preference is a necessary and sufficient condition for the existence of a maximal element of social choice, and thus, many authors have investigated conditions that yield social-preference cyclesFootnote 1
 In this paper, we aim to answer a question left open by two related works, Ferejohn and Fishburn (1979) and Schwartz (2007). Given some familiar background assumptions, each of these papers offers a condition for social-preference cycles. Ferejohn and Fishburn (1979) establish a general result on acyclic social choice in the Arrovian framework, while Schwartz (2007) presents a direct generalization of the classical voting paradox. The benchmark of Ferejohn and Fishburn (1979) is Arrow’s (1951) impossibility theorem. They impose Arrow’s independence of irrelevant alternatives and unrestricted domain on a collective choice rule (CCR). When social preferences are required to be transitive (or quasi-transitive), the power structure behind such a CCR is very simple: it is captured by decisive sets (Sen’s 1979 field expansion lemma). However, it is complicated under acyclicity of social preference (see Blair and Pollak 1982). The Ferejohn–Fishburn framework is appropriate for capturing the power structure behind acyclic social choice: Ferejohn and Fishburn introduce the concept of “constitution,” which determines a pair of disjoint coalitions \((A,B)\) for each pair of alternatives. Coalition \(A\) is the set of “winners,” while coalition \(B\) is the set of “losers.” Using this framework, Ferejohn and Fishburn provide a necessary and sufficient condition for acyclicity/cyclicity of social preference. Their theorem can be applied to obtain well-known results of acyclic Arrovian social choice, and thus, it is extended by several researchers.Footnote 2
 Schwartz’s argument begins with a careful reconsideration of the classical voting paradox.Footnote 3 The most common version of the voting paradox has three individuals.Footnote 4 We can obtain a similar paradox by assuming a partition of the population into finite “minorities.” To capture minorities, Schwartz (2007) introduces impotent sets: a set of individuals is impotent when its complement is decisive. Schwartz’s aim is to clarify the role of indifferent individuals. For example, consider a society with five individuals. Under the simple majority rule, a set of two individuals is impotent, but it is not impotent whenever some third individual is indifferent. Thus, minority changes with indifferent individuals. Then, Schwartz (2007) relativizes impotence to a set of indifferent individuals or opposed ones: He proposes Impotent Partition, which roughly states that some partition of the population is fine enough that each set therein is impotent (relative to some fixed set) for some pairs of alternatives. Schwartz’s first theorem states that if unrestricted domain, the Pareto principle, and impotent partition are satisfied, then a social-preference cycle is generated. His second theorem states that if a social-preference cycle is generated and individuals are not unconcerned, then a mild independence condition, monotone independence, implies impotent partition. As corollaries of Schwartz’s results, the paradoxes of Ward (1961), Sen (1970), Brown (1975), Nakamura (1979), and Weber (1993) can be obtained. The motivation and surface meaning of Schwartz’s condition and Ferejohn and Fishburn’s condition are different, as are the assumed background conditions. Is there any logical connections between Schwartz’s (2007) and Ferejohn and Fishburn’s (1979) works? Our answer is that conditions provided by the two works are closely connected in a certain way, and the connection sheds light on a significant aspect of acyclic social choice. Since we cannot directly apply Ferejohn and Fishburn’s (1979) study, we provide a modified version of their condition—FF. First, we prove that FF is a sufficient condition for cyclic social preference. Second, we show that under the Pareto principle, Impotence Partition implies FF. Third, we show that under monotone independence, FF implies Impotent Partition. The rest of the paper is organized as follows. Section 2 introduces basic definitions. Section 3 investigates the logical connection between Schwartz’s condition and Ferejohn and Fishburn’s condition. Section 4 concludes this paper. The Appendix includes a review of Ferejohn and Fishburn’s work.",1
79.0,1.0,Theory and Decision,17 June 2014,https://link.springer.com/article/10.1007/s11238-014-9452-1,A belief-based definition of ambiguity aversion,July 2015,Xiangyu Qu,,,Unknown,Unknown,Unknown,Unknown,,
79.0,1.0,Theory and Decision,24 June 2014,https://link.springer.com/article/10.1007/s11238-014-9451-2,The \(q\)-majority efficiency of positional rules,July 2015,Sébastien Courtin,Mathieu Martin,Issofa Moyouwou,Male,Male,Unknown,Male,"In voting theory, the Condorcet winner is any candidate that would be able to defeat each of the other candidates in a series of pairwise majority comparisons. A Condorcet voting rule selects a Condorcet winner whenever one exists. While it is very appealing for a voting rule to select a Condorcet winner when one exists, another famous class of voting rules, the positional rules, lacks to satisfy this requirement (Condorcet 1785). This is certainly a failure of the positional approach. However, this negative point of view can be contrasted. By computing the probability that a voting rule selects the Condorcet winner, one can check whether such a phenomenon is sufficiently rare to be ignored or not. In the context of the majority rule, this approach has been particularly studied by Gehrlein and Lepelley (2011).Footnote 1 Under the majority rule, a candidate \(a\) is preferred to a candidate \(b\) if the total number of voters who prefer \(a\) to \(b\) is greater than the total number of voters who prefer \(b\) to \(a\). In this paper, we aim to extend Gehrlein and Lepelley analysis to qualified majorities. Although a simple majority is the rule most often used, qualified majorities are also common in actual parliaments on important constitutional issues. Note that in this paper, we consider a weaker notion of a Condorcet winner. According to a given quota \(q\), a candidate \(a\) is beaten by a candidate \(b\) if at least a proportion of \(q\) individuals prefer \(b\) to \(a\). We say that this candidate is “\(q\)-majority beaten.” For a set of positional rules, Baharad and Nitzan (2003) and Courtin et al. (2012) provide a necessary and sufficient condition on the quota for the winner of the election not to be “\(q\)-majority beaten.” For each voting rule considered,Footnote 2
Courtin et al. (2012) provide the lower bound of the quota \((q^{*})\) which guarantees that the winner of the election is not \(q\)-majority beaten. Indeed, the threshold \(q^{*}\) is such that: (a) for any quota \(q\) less than or equal to \(q^{*}\), there always exists a profile of individual preferences for which the winner of the positional rule is beaten under the \(q\)-majority; and (b) for any quota \(q\) greater than \(q^{*}\), the winner of the positional rule is never beaten under the \(q\)-majority. In this paper, along the line of Gehrlein and Lepelley (2011), we evaluate the propensity of positional rules to select a winner which is not \(q\)-majority beaten, when the quota is not achieved. This entails computing the probability (hereafter called “\(q\)-majority efficiency”) that a voting rule selects a winner which is not \(q\)-majority beaten, given that a candidate exits. We know from Courtin et al. (2012) that given a positional rule, the \(q\)-majority efficiency is equal to one for any quota \(q\) greater than \(q^{*}\), whereas the pioneering work of Gehrlein and Lepelley (2011) summarizes the result for simple majority. Our purpose is then to fill the gap for \(\frac{1}{2}<q<q^{*}\). For three-candidate elections, the \(q\)-majority efficiency is evaluated for a wide class of positional rules (simple and sequential). We observe that the \(q\)-majority efficiency is sometimes very low for some simple positional rules. This efficiency is significantly greater for sequential rules than for simple positional rules. Moreover, this efficiency is very close to the \(q\)-Condorcet efficiency, the conditional probability that a positional rule elects the candidate who beats all others under \(q\)-majority, when one exists (see Sect. 5). The remainder of the paper is organized as follows: Sect. 2 is a presentation of the general framework with notation and definitions. Sections 3 and 4 provide computations for the simple positional rules (PR) and the sequential positional rules (SPR), respectively. Section 5 is a discussion about the relevance of our results, and technicalities are relegated into the “Appendix.”",9
79.0,1.0,Theory and Decision,02 July 2014,https://link.springer.com/article/10.1007/s11238-014-9450-3,Stubborn learning,July 2015,Jean-François Laslier,Bernard Walliser,,Unknown,Male,Unknown,Male,"In Evolutionary Game Theory and in Computer Science, various dynamic learning rules have been suggested. The asymptotic properties of the resulting processes are studied along different perspectives, according to the discipline. In Evolutionary Game Theory (Weibull 1995; Fudenberg and Levine 1998), players are assumed to have bounded rationality and incomplete knowledge of their environment. They play the game with heuristic rules and the modeler wonders which outcomes of the game are reached, especially with reference to strong rationality and equilibrium notions (Harstad and Selten 2013; Crawford 2013). In Computer Science (Sutton and Barto 1998), the modeler considers some outcomes of the game as specially desirable (concepts of efficiency or optimality) and treats the learning process as a possible algorithm in order to reach the selected issues designed to “solve the game.” The present paper favors the first perspective and introduces an original rule which is easily implemented by the players. The rule under study, called stubborn learning (SL), satisfies three principles of bounded rationality: (i) players only use information they can gather themselves during the play, namely results of their own actions (ii) information gathered by each player is kept in his memory only for a limited number of periods. (iii) determination of the intended decision by each player is computationally easy. However, the rule is only defined for situations where each player has a unidimensional action set, bounded or not. To be more precise, each player is able to shift his action by an incremental quantity only, in one direction or the other. He compares the utility he got in the last period to that in the penultimate period. If he experienced an increase (decrease) in utility, he keeps on moving his strategy in the same direction (he reverses direction). The player is “stubborn” since he keeps moving in the same direction as long as he gets better. He nevertheless changes immediately when he gets worse. The SL rule can be compared with other classical rules considered in the literature, classified first by their amount of required information, then by the way they combine that information. A first class of rules, called “uncoupled learning” (Hart and Mas-Collel 2003) assumes that a player does not know the others’ utility function, but knows his own utility function, observes the others’ actions and the utility they get when interacting with them. Among them are the “belief-based rules” where each player has some belief about the others that he revises through time, the standard example being fictitious play, but model-free rules are also considered (Fudenberg and Levine 1998). In Computer Science, the prototype is the descent gradient rule, in discrete or continuous time (Singh et al. 2000). It assumes that a player knows his utility function locally and observes the other’s past action with memory one. It states that a player adjusts his action proportionally to the maximal gradient from the present position. Hence, he computes his own payoff variation if the other player’s action would remain constant. Obviously, SL cannot be reduced to a simplified form of the gradient descent rule, since it is only based on past observed utility and, moreover, is defined with a two-period memory. In Game theory, some rules known as “directional learning rules” (Selten and Stoecker 1986; Selten and Buchta 1998) were developed in an experimental setting. They assume that a player tends to modify his action in the direction of a better response to recent others’ actions (Anderson et al. 2004). More precisely, a player searches ex post if he could do better with another action (Grosskopf 2003). Stubborn learning differs from these directional learning rules by the fact that, under SL, a player knows the payoff variation he obtained with his implemented action change, but he does not know the payoff variation for the alternative action change. A second more restrictive class of rules, called “completely uncoupled learning” assumes that a player no more observes the others’ actions, but only the utility he gets when interacting with them. They are sometimes called “reinforcement rules” since a player uses his experience to implement more efficient actions (Börgers and Sarin 1997; Erev and Roth 1998). Especially, a whole family of “regret-based rules” (Foster and Young 2006) considers that the player computes the regret of not having played some other actions, then maintains this action (if maximal regret is lower than some threshold) or chooses another action (among the ones with highest regret). In Computer Science, a rule called Pavlov or Win Stay Loose Shift (Nowack and Sigmund 1993; Posh 1999; Liu et al. 2011) appears as a gradual adaptation process with an aspiration level. It states that a player stays where he is if the past utility stands below some aspiration level, and moves in some other direction if not. The aspiration level is generally exogenously related to the payoff structure of the game and the memory is limited to one period. The rule has been essentially used in instances of pure strategy \(2\times 2\) games (essentially the prisoner’s dilemma) where the player has only two discrete actions available, the aspiration level being the cooperative payoff. By comparison, the SL rule has a two-period memory and no explicit aspiration level, even if one could say that the role of the aspiration level is played by the penultimate utility. In Game Theory, the CPR rule (Cumulative Proportional Rule) was proposed in different contexts (Bush and Mosteller 1955; Roth and Erev 1995; Laslier et al. 2001). It states that a player plays some action with a probability which increases with an index reflecting the past performance of each action. The function relating the action’s probability to its index can be linear or exponential. The index of an action is generally the sum (rather than the average) of all payoffs a player obtains when using this action. In fact, these rules achieve a nice trade-off between exploration of new actions and exploitation of efficient actions. This rule differs heavily from SL since SL uses no action index and only rests on a one period memory. Finally, two more precise rules of the completely uncoupled learning family apply only to unidimensional action spaces and are close to the SL rule. The first is the GLAD rule (Gains and Losses Adjust Directions). It is identical to the SL rule as long as the action space is not constrained. GLAD was first applied to the Cournot duopoly independently by Huck et al. (2004) and by Trégouet (2004). In this game, each firm chooses the price of its product (a real number), with regard to a global demand depending on both prices. It was further applied to a continuous version of the Prisoner’s Dilemma by Huck et al. (2003, 2005). However, to be able to study more games, the SL rule has to complete the GLAD rule in order to handle two cases: when the strategy space is bounded, and when the player’s utility is identical during two successive periods. The second is the TREND rule recently suggested (Nax et al. 2013). It is based on the fact that an increase in action leads to a further increase (decrease) when the last payoff variation was positive (negative) and symmetric. The SL rule appears as a specification of the TREND rule when the (positive or negative) action variation is held constant in each period. Nax et al. 2012 uses the same rule for studying decentralized markets. In the present paper, the SL rule is applied to the complete family of mixed strategy \(2\times 2\) games. For each player, the action space is the closed interval \([0,1]\) relative to the probability of using some pure action. With this interpretation, the payoff is an expected payoff, which we assume to be observed by a player (rather than a concrete realization). In mathematical terms, we study in fact a deterministic game on the square with doubly linear payoff functions. The behavior of the system is studied in all possible configurations and asymptotic results are given for symmetric, zero-sum, and twin games. These results are compared to results obtained in the literature with other rules (Bowling and Veloso 2001; Bowling 2005; Banerjee and Sen 2007). Moreover, more informal comments are given on the robustness of the results with respect to small random shocks. Section 2 provides a formal definition of the Stubborn Learning rule. Section 3 studies the behavior of the system successively at interior points, on the borders of the action space, and at the corners of the action space, according to the matrix of the \(2\times 2\) game. Section 4 is devoted to symmetric games, Sect. 5 to zero-sum games, and Sect. 6 to twin games. A conclusion summarizes the asymptotic results. Proofs are in the Appendix.",1
79.0,1.0,Theory and Decision,17 June 2015,https://link.springer.com/article/10.1007/s11238-013-9369-0,Avoiding both the Garbage-In/Garbage-Out and the Borel Paradox in updating probabilities given experimental information,July 2015,Robert F. Bordley,,,Male,Unknown,Unknown,Male,"Consider an investigator running K trials of an experiment (which might be a physical or a simulation experiment). The ith trial of the experiment involves prespecifying a vector of observable values \(x_i\) describing the initial conditions of the system. Given these initial conditions, the experiment is run and a vector of observable values \(y_i\) is measured. Let \(x=(x_1, \ldots ,\, x_K)\) and \(y=(y_1, \ldots ,\, y_K)\) denote the inputs and outputs of the K trials of the experiment. Define event E as the information from the experiment, i.e., as the event in which experimental trial i (for \(i=1, \ldots K\)) generates values \(y_i\) when the initial conditions are specified equal to \(x_i\). A decision maker is interested in making decisions in an applied real environment different from the environment in which the experiment is conducted. For the sake of generality, we assume that the decision maker believes the actual value of the system’s initial conditions in this real environment is stochastic with a probability distribution \(p(x|\mu )\) where \(\mu \) is a vector of input parameters describing the underlying characteristics of the system. (This paper will make no assumption about whether there is any relationship between the values of x used in the experimental situation and the values of x that are realistic in the applied situation.) Thus in a queuing system, \(x_i\) might be the time between when the ith person enters the system and the \(i-1\)st person enters the system while \(\mu \) might be the average interarrival time of people entering the system. The decision maker also believes that in the real environment, the actual value of the system outputs is stochastic with a probability distribution \(p(y|\nu )\) where \(\nu \) is a vector of output parameters describing other characteristics of the system. Thus if \(y_i\) is the length of time which individual i spends waiting in the system, \(\nu \) might be the average waiting time. This paper focuses on the deceptively simple question of how to update the decision maker’s beliefs about \(\nu \) based on the experimental information. This question is relevant both to the Bayesian analysis of simulation experiments as well as the Bayesian analysis of physical experiments. If \(\mu \) were known, then the decision maker would revise the pre-experimental probability, \(f(\nu )=f(\nu |\mu )\), to \(f(\nu |E)=f(\nu |E,\mu )\). When \(\mu \) is unknown [as is the case of structural uncertainty (Glynn 1986)], investigators frequently have experts assess a probability density over the input parameters, \(\mu \), and then integrate \(f(\nu |E,\nu )\) over this newly assessed probability density for \(\mu \) to get an estimate of \(f(\nu |E)\). There are two ways of making this assessment of \(\mu \): the experts can make their assessment of \(\mu \) based on their prior beliefs about \(\mu \) and knowledge of the results of the experiment. Thus they assess \(f(\mu |E)\). Integrating \(f(\nu |E,\mu )f(\mu |E)\) over \(\mu \) will, in fact, give a consistent estimate of \(f(\nu |E)\). But in many cases, directly assessing \(f(\mu |E)\) is not easier than directly assessing \(f(\nu |E)\). For this reason, it is much more common to have the experts assess \(\mu \) in the absence of knowledge of E. Thus they assess \(f(\mu )\). Integrating \(f(\nu |E,\mu ) f(\mu |E)\) will not, of course, give a consistent estimate of \(f(\nu |E)\). These problems will not be of any practical significance if \(f(\mu |E) \approx f(\mu )\). But as Andradottir and Bier (2000) noted, there is no logical reason why this assumption should be true. In fact, there are well-known practical instances where the assumption is violated. Specifically suppose the decision maker’s beliefs about \(\nu \) are based on considerable information while the assessment of \(f(\mu )\) is based on minimal information. Then upon learning the information E, the Garbage-In Garbage-Out (GIGO) maxim requires that the decision maker avoid modifying beliefs about \(\nu \) because of the garbage-like quality of the inputs, \(\mu \). (In other words, the decision maker must resolve any observed discrepancies between the information E and prior beliefs about \(\nu \) by modifying beliefs about the inputs \(\mu \).) In this case, the impact of the analysis, if measured by the degree of change in the decision maker’s beliefs about \(\nu \), is minimal. Thus, as expected, the Bayesian approach to using experimental information—while disagreeing with non-Bayesian methods in allowing the investigator to use input information of very low quality—still discounts the impact of an analysis for using low-quality information. The difficulty in this previous use of Bayes Rule arose because the decision-maker’s beliefs about both \(\mu \) and \(\nu \) are being modified by the information E. Hence to compute \(f(\nu |E)\), the investigator needs to compute However, this approach also has pitfalls. Specifically consider an asymptotic case where the experimental information is sufficient to deterministically specify \(\nu \) as a function of \(\mu \) (i.e., \(\nu _{\mu }\)). Then assuming that \(f(E|\mu ,\nu )=1\) when \(\nu =\nu _{\mu }\) (and equals zero otherwise) gives the Bayesian synthesis formula, \(f(\nu |E) \propto \int _{\mu } f(\nu _{\mu },\mu )\). But this specification of \(f(\nu |E)\) is questionable because the decision maker’s prior probability for the deterministic relationship specified by E is typically zero. In this case, the Borel-Kolmogorov Paradox (Singpurwalla and Swift 2001) indicates that simple reparameterizations of the model that, in principle, should not matter (like transforming \(\mu \) from length to area) lead to different results. Furthermore Proschan and Presnell (1998), Schweder and Hjort (1996), and Wolpert (1995) demonstrated that this paradox does rule out the possibility of computing \(f(\nu |E)\) from \(\int _{\mu } f(\nu _{\mu },\mu )\). This has led to the abandonment of Bayesian synthesis in favor of a supra-Bayesian heuristic known as Bayesian welding (Poole and Raftery 2000; Roback and Givens 2001; Redtke et al. 2002). To avoid using supra-Bayesian heuristics, this paper focuses on using Eq. (1) to develop a solution for the non-paradoxical case in which the relationship between \(\nu \) and \(\mu \) is probabilistic. Consistent with the Borel Paradox and inconsistent with Bayesian synthesis, the Bayesian solution becomes indeterminate in the asymptotic case where the probabilistic relationship becomes deterministic. Since Lindley’s rule discourages postulating deterministic relationships, this use of Bayesian updating leads to determinate results in most realistic cases. We also show that the Bayesian welding heuristic can be derived from Bayesian updating in the Gaussian case.",
79.0,1.0,Theory and Decision,20 June 2014,https://link.springer.com/article/10.1007/s11238-014-9453-0,A theoretical foundation of portfolio resampling,July 2015,Gabriel Frahm,,,Male,Unknown,Unknown,Male,"This paper provides a theoretical foundation of portfolio resampling. Based on the classic theory of rational behavior (Neumann and Morgenstern 1944), I derive simple conditions under which portfolio resampling can be justified. The given results do not require any specific assumption on the investment strategy, probability distribution of asset returns, or on the utility function of the investor. I clarify the impact of uncertainty on capital allocation in a very general setting. The methodological framework allows me to obtain universal and reliable statements on this subject. This work is inspired by a famous experiment conducted by Harry Markowitz and Nilufer Usmen (2003). They compared a Bayesian portfolio optimization method with a portfolio-resampling technique invented by Richard Michaud and Robert Michaud (1998).Footnote 1 Markowitz and Usmen expected the Bayesian strategy to do better than the resampling approach but, surprisingly, the Michaud strategy won the battle. 
Markowitz and Usmen (2003) frankly state that, the results represent something of a crisis for the theoretical foundations of portfolio theory. At the end of their paper, Markowitz and Usmen raise some basic questions, which can be stated as follows:  How does Michaud’s procedure relate to the classic theory of rational behavior according to Neumann and Morgenstern (1944)? Why does portfolio resampling performs so good in the experiment? How much does the Michaud procedure contribute to the risk-adjusted expected return in practice? This means after taking the true distribution of asset returns, transaction costs, etc., into account. Are Savage’s axioms of subjective probability (1954) violated by the Michaud player and if so, do we have to blame the Michaud player or the axioms? Does the Bayesian strategy perform so bad, because the chosen priors are incorrect or because of a poor numerical approximation of the posteriors? First of all, these questions are highly relevant from an academic point of view, but nevertheless, Markowitz and Usmen (2003) emphasize that their experiment has also practical implications for mean–variance analysis. It is still unclear why Bayesian analysis, which is a sophisticated method, apparently does not succeed over such a relatively simple method like portfolio resampling. Michaud’s portfolio resampling is a patented procedure. This means nobody can use or distribute the algorithm without license or permission. According to Kovaleski (2001), Michaud’s firm New Frontier Advisors “has licensed the optimizer to about 10 financial institutions, including money managers, financial services companies and consultants.” Hence, some practitioners might wish to know why, and under which circumstances, portfolio resampling could be useful in real-life applications. The portfolio-resampling approach has been published in Michaud (1998), and only a few years later, it has become the subject of a highly controversial discussion and big scientific dispute. In the following, I will give an overview on the literature. It is easy to divide the list of authors into the proponents and opponents of the Michaud approach. 
Fletcher and Hillier (2001) compare the out-of-sample performance of the Michaud procedure with traditional mean–variance analysis based on empirical data of equity indices between 1983 and 2000. They consider several estimation approaches for the mean excess returns: The historical sample mean, the James–Stein shrinkage estimator, a single-factor estimator, and a conditional model. Their study reveals that portfolio resampling does slightly better than traditional mean–variance analysis, although the results are not significant. With regard to the Fletcher–Hillier study, Michaud and Michaud (2003) warn that “A casual reader might conclude that resampled efficiency improvements are not of investment significance.” Their opinion is that a backtest cannot provide reliable information, since a “good” strategy may perform poorly, and a “bad” strategy may perform well in any given period. Moreover, Michaud and Michaud (2003) point out that, “This is why I used simulation methods to prove the superiority of resampled efficiency.” 
Scherer (2002) tries to explain the potential benefits and pitfalls of the Michaud procedure. He concludes that portfolio resampling can be useful to develop statistical tests for the difference between two portfolios, but at the end of his paper, he writes, “What is not clear, however, is why averaging over resampled portfolio weights should represent an optimal portfolio construction solution to deal with estimation error.” Further, he argues that portfolio resampling should not be compared with the naive Markowitz approach, where sample means and sample (co-)variances are treated like true moments, and thus, estimation risk is completely neglected. He recommends to use Bayesian portfolio optimization strategies instead. In fact, Markowitz and Usmen (2003) apply a Bayesian strategy in their experiment but, in contrast to Fletcher and Hillier (2001), they conduct a Monte Carlo simulation and not an empirical study. As already mentioned, they come to the conclusion that the Michaud procedure dominates the Bayesian procedure. At the same time, Scherer (2004) propagates Bayesian strategies as a better alternative to portfolio resampling and asserts that this is the “correct way to deal with estimation error.” He concentrates on the drawbacks of portfolio resampling and concludes that the Michaud approach has “serious statistical and decision theoretic limitations.” By contrast, Kohli (2005) conducts an empirical study based on stock market data from 2011 to 2013 and find “no conclusive advantage or disadvantage of using resampling as a technique to obtain better returns,” but “resampled portfolios do seem to offer higher stability and lower transaction costs.” Scherer (2006) runs a new Monte Carlo simulation, which reveals that the James–Stein shrinkage estimator outperforms portfolio resampling. Nevertheless, the Michaud procedure is able to outperform the naive Markowitz approach, but “the exact mechanics remain unclear (and unformulated).” Similarly, Scherer (2007) states that portfolio resampling is a “practitioner-based heuristics with no rooting in decision theory.” Finally, Scherer (2006) notes that it is impossible to compare portfolio resampling with Bayesian procedures: “For every distribution, a prior will be found that will outperform resampling (and vice versa). Equally, for every prior, a distribution will be found where resampling outperforms.” 
Herold and Maurer (2006) compare Bayesian strategies with heuristic approaches including portfolio resampling. In their study, they distinguish between “conditional” and “unconditional” strategies. Unconditional strategies assume that asset returns are serially independent and identically distributed (i.i.d.). By contrast, the conditional strategies are based on the assumption that asset returns are predictable. Their study incorporates a long history of US stock market data and a shorter history of European stock prices. It turns out that the Michaud procedure does not significantly improve the Sharpe ratio. 
Wolf (2007) compares the Michaud approach with an active investment strategy based on a shrinkage estimator for the covariance matrix of asset returns. This is done on the basis of US stock market data from 1983 to 2002. He finds that the shrinkage estimator systematically outperforms portfolio resampling. 
Harvey et al. (2008) repeat the Markowitz–Usmen experiment after changing the priors and the numerical algorithm for calculating the posteriors. Their results are in direct contrast to Markowitz and Usmen (2003), i.e., Bayes beats Michaud. They state that “the Bayes player was handicapped because the algorithm that was used to evaluate the predictive distribution of the portfolio provided only a rough approximation.” Michaud and Michaud (2008) reply that Harvey et al. (2008) use an inefficient method for portfolio resampling, and moreover, the two inventors argue that the numerical implementation of the Michaud procedure was less accurate than the implementation of the Bayesian estimates. 
Fernandes and Barros (2012) assert that “Several out-of-sample evaluations have shown results in favor of resampling methodology, using different sets of data.” Becker et al. (2013) perform an empirical study and a Monte Carlo experiment to compare portfolio resampling with several optimization methods and find that Markowitz outperforms Michaud on average. The empirical study is based on US and European stock market data from 1988 to 2007. However, the resampling method implemented by Becker et al. (2013) suffers from the same drawback as in Harvey et al. (2008). Summing up, there is still no clear answer to the questions raised by Markowitz and Usmen (2003). Most firms and professional investors would never implement a rule-based investment strategy without a backtest. Hence, it is necessary to evaluate every investment rule ex post on the basis of historical price data, at least for demonstrating its potential benefit. Nevertheless, a serious empirical study that aims at comparing different strategies must control for non-normality, serial dependence, selection bias, multiple testing, etc. After taking all that issues properly into account, in general, it is hard to find significant results in favor of any strategy (Frahm et al. 2012). This means the question whether portfolio resampling is better or worse than other investment strategies cannot be solved by statistical inference. Unfortunately, the same holds for Monte Carlo simulation. As is pointed out by Scherer (2006), one can always create situations where a Bayesian strategy performs better than any other strategy and vice versa. Every strategy works good if the underlying assumptions are satisfied, and so it is relatively easy to construct a Monte Carlo experiment where Strategy A outperforms Strategy B and to find another setting where the opposite is true. For this reason, Monte Carlo simulation can only be used to compare strategies under a hypothetical situation, but does not explain how the strategies perform in real life. In my opinion, the only way to provide satisfactory answers to the basic questions raised by Markowitz and Usmen is to derive precise theoretical arguments for or against portfolio resampling. This goal can be only achieved by analytical investigation and not by observation or simulation. I hope that the results presented in this paper provide new insights and a better understanding of the mechanisms governing individual investment decisions in a world of uncertainty.",10
79.0,1.0,Theory and Decision,26 June 2014,https://link.springer.com/article/10.1007/s11238-014-9454-z,"Politicians, governed versus non-governed interest groups and rent dissipation",July 2015,Gil S. Epstein,Yosef Mealem,,Male,Male,Unknown,Male,"When the government proposes a new policy or project, it may have differential effects on various groups in society. This can give rise to contests between interest groups and as a result, the government is often a leading player in such contests. Knowing this, the government can play the role of contest designer by determining the number of players in a contest, the type of contest to be played and the stakes of the contest.Footnote 1
 One of the main concerns of the contest designer is to choose the optimal type of contest. Two main types of contests, as represented by their contest success functions (CSF), are considered in the literature: the lottery proposed by Tullock (1980) and the all-pay auction contest (hereafter: APA) which is a special case of the Tullock CSF (see Konrad 2009, and references within). One reason for the popularity of these CSF functions is their appealing axiomatization, as discussed in Skaperdas (1996), Blavatskyy (2010), Corchón and Dahm (2010) and Jia (2008, 2010).Footnote 2 For a discussion of the simple lottery, see Hillman and Riley (1989), Fullerton and Preston McAfee (1999), Baye and Hoppe (2003) and Franke et al. (2013). Most of the literature on optimal contest design has focused on the choice of the contest prize (Glazer and Hassin 1988; Runkel 2006; Singh and Wittman 1998; Epstein and Nitzan 2002, 2006a, b, 2007); the set of contestants (Baye et al. 1993; Amegashie 2000; Moldovanu and Sela 2006; Taylor 1995); the set of contestants and the prize system (Che and Gale 2003); the structure of multi-stage contests (Gradstein 1998; Gradstein and Konrad 1999; Amegashie 2000); caps on political lobbying (Che and Gale 1998); and the CSF, which relates the contestants’ efforts to their probabilities of winning (Che and Gale 1997; Nti 1997, 2004). In this paper, we consider two interest groups: one that is governed by a central planner and one that is not. For example, consider the case of a firm (a governed interest group) that is defending its market power over consumers (a non-governed interest group) who are challenging that power (see, for example, Baik 1999; Ellingsen 1991; Epstein and Nitzan 2003, 2007 and Schmidt 1992). The firm is a single entity, while the consumers are a diffuse group. In another example (see Epstein and Nitzan 2006a, b, 2007), employers are defending the minimum wage level against a union that is seeking to increase it. 
Riaz et al. (1995) present a general model of rent-seeking for a public good by expanding the rent seeker’s consumption bundle to include preferences over a public good and a private good. Their results suggest that collective rent-seeking is positively related to group size. Although free riding exists within a group, the tradeoff is not one-for-one. Furthermore, rent-seeking is found to increase with wealth. In determining the type of contest, the contest designer (a politician, legislator, regulator, etc.) influences the scope of rent dissipation. Rent dissipation can be viewed as a benefit to the contest designer, in that he receives the resources invested by the interest groups (see for example, Persson and Tabellini 2000; Grossman and Helpman 2001 and Epstein and Nitzan 2007). Alternatively, rent dissipation is often perceived as involving the wastage of resources invested in winning the contest. Reducing the amount of wasted resources, i.e. rent dissipation, is thus welfare enhancing.Footnote 3
 A great deal of effort has been made in the literature to determine maximal rent dissipation using various types of discrimination between the contestants. Discrimination can be multiplicative, as in Epstein et al. (2011, 2013); Franke (2012); Franke et al. (2013); Konrad (2002) and Lien (1990), additive, as in Li and Yu (2012), or direct via differential taxation of the contested prize, which affects the contestants’ actual prize valuations, as in Mealem and Nitzan (2012, 2014). As mentioned, in this paper we consider two interest groups: one that is governed by a central planner and one that is not. While the objective of the contest designer is similar to that in Franke et al. (2013); Franke et al. (2014) and others, in that it attempts to achieve maximal rent dissipation, our approach differs from the literature in two aspects: First, Franke et al. (2013) determines the optimal level of discrimination in the contest such that it maximizes rent dissipation, and as a result, the planner ends up encouraging the weaker player. Second, Franke et al. (2014) also showed that rent dissipation under APA is higher than under the simple lottery. In contrast to these models and others that achieve maximal rent dissipation through the types of discrimination mentioned above, we emphasize a different types of discrimination, in which the contest designer limits the size of the non-governed interest group and thus determines the level of rent dissipation. The first question, we wish to answer is how the size of the non-governed interest group affects the rent dissipation that will be obtained by the contest designer in the two types of contest: APA versus logit. In addition, the contest designer can also limit the size of the non-governed interest group in order to maximize the resources invested in the contest (for the benefit of the contest designer) or to minimize the wasted resources invested in the contest. Therefore, the second question to be answered is what is the optimal group sizeFootnote 4 from the point of view of the contest designer. The following examples will help to demonstrate the relevance of the model: The government, via a central land planning authority (the central planner), holds an auction of land on which a new neighborhood is to be built. The future neighborhood is located next to a nature reserve, and therefore, there is opposition from a green group that wishes to keep the area natural. The contest then is between the green group and the winners of the land auction. In this case, the land planning authority, as the central planner, has the power to determine the number of those who will receive land in the auction. The Ministry of Communication decides to issue a tender in order to choose a number of new providers of cellular phone service. In order to provide this service, a cellular provider will have to place antennas in a location that belongs to a private non-profit organization (or a private individual). The contest is then between the non-profit organization and the cellular providers who win the tender. In this case, the Ministry of Communication, as the central planner, can determine the number of cellular providers that can compete in the tender. The government outsources the provision of office supplies to the schools to a single supplier. The government is indirectly responsible for the supplier’s workers, in that it wishes to determine a minimum wage for them. The contest is between the management of the company and its workers who receive the minimum wage. In this case, the government’s determination of the minimum wage to be discussed by the two sides indirectly influences the number of workers who will receive it, and therefore, the number of “contestants” among the workers. The local government can determine the location of a new supermarket which will operate as a monopoly in the area. The contest is between the supermarket chain, which would like to charge high prices, and consumers, who would like to see lower prices. The farther (closer) is the supermarket from the residential area, the lower (higher) the number of consumers that will shop there. In determining the location of the supermarket, the local government has an indirect influence on the number of consumers, and therefore, on the number of “contestants” among them. Understanding how a change in the contest success function will affect the rent dissipation of the contest may help to understand why in some contests the contest designer will choose APA and in others a logit-type contest. With regard to the type of lottery preferred by the planner, given the size of the group, we provide conditions under which the APA contest success function can create higher or lower rent dissipation than the generalized logit contest success function. Our results shed light on the objectives of the contest designer via the choice of the contest success function and the size of the non-governed interest group. Our results depend, to some extent, on the size of the non-governed interest group. However, if the stakes are similar in size for both groups, then rent dissipation under the APA will be greater regardless of the size of the non-governed interest group. With regard to the optimal size of the group from the perspective of the contest designer, it is shown that in the generalized logit contest, if the contest designer wishes to maximize rent dissipation, then he will limit the size of the non-governed interest group. If, on the other hand, he wishes to reduce the amount of resources wasted in the contest, it may be optimal (especially in large societies) not to limit the size of the non-governed interest group. These results may indicate the extent to which a contest designer perceives rent dissipation in a positive light, i.e., the weight he assigns to rent dissipation in his objective function. A society that limits the size of the non-governed interest group is governed by a contest designer who wishes to maximize rent dissipation and is concerned less with social welfare. The remainder of the paper is organized as follows: Sect. 2 presents the model. Section 2.1 describes equilibrium and rent dissipation for the generalized logit contest success function and for APA and at the end of the section, we compare the rent dissipation in the two situations (Proposition 1). Section 2.2 presents the optimal group size that either maximizes or minimizes the rent dissipation of the contest (Proposition 2). Concluding remarks are presented in Sect. 3. The proofs of the two propositions are presented in the Appendix.",
79.0,1.0,Theory and Decision,14 August 2014,https://link.springer.com/article/10.1007/s11238-014-9462-z,Extended present bias: a direct experimental test,July 2015,Robin Chark,Soo Hong Chew,Songfa Zhong,,,Unknown,Mix,,
79.0,2.0,Theory and Decision,14 October 2014,https://link.springer.com/article/10.1007/s11238-014-9472-x,A preference model for choice subject to surprise,September 2015,Simon Grant,John Quiggin,,Male,Male,Unknown,Male,"Von Neumann and Morgenstern Neumann and Morgenstern (1947) and Savage (1954) provided the foundations for the formal theory of decision under uncertainty. Von Neumann and Morgenstern presented a simple set of axioms under which preferences over risky prospects (lotteries allocating prizes with known objective probabilities) can be represented by an expected utility functional. Savage addressed the more general setting of representing preferences over acts, that is, mappings from a set of possible states of the nature, without known objective probabilities, to a set of conceivable consequences. From the 1980s onwards, a variety of alternatives to, and generalizations of, expected utility theory were proposed, including rank-dependent utility theory, weighted utility theory, cumulative prospect theory, and maxmin expected utility theory. These theories were motivated by the observation of violations of the predictions of expected utility theory, such as the Allais problem (Allais 1953) and the Ellsberg problem (Ellsberg 1961). From the normative perspective of expected utility (EU) theory, the EU axioms are implications of rationality, and non-EU representations of choice involve violations of rationality. Nevertheless, in common with EU, all the generalized EU models discussed above require unbounded rationality, or at least unbounded reasoning capacity. Individuals are presumed to consider all possible states of nature, and arbitrarily large sets of acts. In the Twenty-first century, research in decision theory has focused more and more on the case of bounded rationality. Among the most important examples of bounded rationality is the case where agents have only partial awareness of the set of states of nature. A large literature has developed concerning the representation of unawareness. Notable contributions include Halpern and Rêgo (2009) and Heifetz et al. (2006). Schipper (2014) provides an extensive bibliography. By contrast with the focus on epistemological problems, relatively little attention has been paid to problems of decision under bounded awareness, as in Grant and Quiggin (2013a).Footnote 1 For some representations of unawareness, the problem is trivial. If agents are naively unaware of their own unawareness, their preferences may be represented by expected utility theory or one of its generalizations, applied to the restricted representation of the world available to them. More difficult problems arise when individuals are aware (for example, on the basis of inductive reasoning) of their own unawareness. In this case, the naive approach of maximizing expected utility with respect to a restricted representation of the world is unappealing, and vulnerable to manipulation by other, more aware, agents. On the other hand, any attempt to take unconsidered possibilities into account appears to run into an insoluble paradox—if possibilities are taken into account in decision making, they cannot be said to be unconsidered. 
Grant and Quiggin (2013a, b) suggest that agents employ heuristics to constrain the set of acts under consideration before applying standard decision theory, based on their restricted model of the world to the remaining acts. An example, developed in Grant and Quiggin (2013b) is the precautionary principle, commonly advocated as a requirement for decision making in relation to environmental hazards. The aim of this paper is to provide an axiomatic foundation, and an associated representation theorem, for the preference model proposed by Grant and Quiggin. The unawareness of the agent is encoded both in the specification of the states and in an elaboration of the set of consequences. The state space includes both a standard Savage state space and a set of ‘surprise’ states. The ‘standard’ set of consequences (taken to be a closed bounded interval of ‘monetary’ consequences) is augmented by two unforeseen consequences, one unfavorable and ranked below the worst possible monetary consequence and the other favorable and ranked above the best possible monetary consequence. Unforeseen consequences can arise only in surprise states. The formal specification of the above framework appears in Sect.  2. We then posit in Sect. 3 that the preferences restricted to those acts involving no unforeseen consequences admit a Gul and Pesendorfer (2014) expected uncertain utility (EUU) representation.Footnote 2 We extend this to the entire set of acts by means of a three-cell partition of the set of acts. The first cell of the partition comprises those acts the decision-maker perceives to be subject to an unforeseen unfavorable surprise. The second cell comprises those acts the decision-maker perceives to be subject to an unforeseen favorable surprise but not subject to an unforeseen unfavorable surprise. The third cell consists of acts for which the possibility of an unforeseen consequence may be neglected. Our axioms ensure that all acts in the first (respectively, second) cell are ranked below (respectively, above) those in the third. To allow for the representation to represent the preferences of pairs of acts that reside within the same cell of our three-cell partition of the acts, we define an operation that maps any act into the set of acts with no unforeseen consequences by replacing any occurrence of the unforeseen unfavorable (respectively, favorable) consequence with the worst (respectively, best) possible monetary consequence. The resulting act is thus one that is a member of the subset of acts involving no unforeseen consequences, which is the domain of the EUU representation. We derive an associated representation and illustrate how the representation can be applied to rationalize two notions of the precautionary principle that were proposed and discussed in Grant and Quiggin (2013b). Section 4 offers some concluding comments.",13
79.0,2.0,Theory and Decision,24 July 2014,https://link.springer.com/article/10.1007/s11238-014-9458-8,Social choice with approximate interpersonal comparison of welfare gains,September 2015,Marcus Pivato,,,Male,Unknown,Unknown,Male,"It is relatively uncontroversial that a bowl of rice will benefit a starving man more than a well-fed man. Likewise, it seems likely that a tax refund of $10 will benefit a poor labourer more than a typical billionaire, ceteris paribus. But if two people are roughly equally wealthy and healthy, then it is not at all clear who will benefit more from a free meal or a $10 refund, even if we suppose that they are identical in terms of education, family status, and all other observable features. Each of these three cases involved interpersonal comparisons. Furthermore, these were interpersonal comparisons not of welfare levels, but rather, of welfare gains—the welfare gained from an increment of food, or from an increment of money. As these examples show, some interpersonal comparisons of welfare gains and losses are obvious and uncontroversial; Alice’s death after falling into an open sewer is clearly a greater tragedy than Bob’s paper cut. But other interpersonal comparisons are less clear; we may lack sufficient information to make them in practice, or they may be impossible to make even in principle, because they implicate subtleties of human psychology that are impossible to observe or evaluate. In this paper, we will represent this system of partial and approximate interpersonal comparisons with a mathematical structure called a difference preorder: an incomplete preorder on the space of personal state transitions. The central problem of this paper is as follows: given the approximate interpersonal comparisons encoded in an (incomplete) difference preorder, how can we compare the aggregate welfare gains or losses for society of different policies?
 Formally, let \({\mathcal { X}}\) be a space of ‘personal states’. Any person, at any moment in time, resides at some point in \({\mathcal { X}}\). We suppose that an element \(x\in {\mathcal { X}}\) encodes all the factors which influence this person’s welfare; this may include both psychological factors (e.g. beliefs, values, desires, personality, memories, etc.) and physical factors (e.g. health, physical location, consumption bundle, etc.). Exactly what information we encode in \({\mathcal { X}}\) will depend on the nature of the problem we are modelling (see Example 3.1 and Sect. 5 below for several possibilities). This paper makes no assumptions about the precise structure of \({\mathcal { X}}\). We will just assume that we can (sometimes) make sense of the statement: We can represent this with an (incomplete) preorder \((\succeq )\) on the Cartesian product \({\mathcal { X}}\times {\mathcal { X}}\). We will write an ordered pair \((x_1,x_2)\in {\mathcal { X}}\times {\mathcal { X}}\) as “\(x_1\leadsto x_2\)” to emphasize that it represents a change from \(x_1\) to \(x_2\). Then the statement (1) is represented by the formula “\((x_1\leadsto x_2)\,{\mathop {}\limits ^{\displaystyle \succ }}\,(y_1\leadsto y_2)\)”. This formal model is totally neutral about the substantive nature of “well-being”; it could be hedonist, preferencist, eudaemonist, etc. The preorder \((\succeq )\) might represent the subjective judgement or “extended preferences” of Harsanyi’s impartial observer,Footnote 1 or it might represent objective facts about human nature, like Kolm’s “fundamental preferences”;Footnote 2 we do not need to commit to one interpretation or the other. Some of our examples will flesh out this general framework with more specific assumptions, but these are only intended to provide concrete illustrations, not to advance any particular theory of human well-being. The main goal of the paper is to demonstrate that, even in such a minimal framework, it is possible to develop a substantive and broadly “utilitarian” theory of social welfare. This paper has two major themes. The first is the use of difference preorders (rather than ordinary preorders) to compare the welfare gain of state changes (rather than the welfare of the state themselves), at both the individual level and the social level. This is a way to have a roughly “cardinal” notion of welfare, without assuming the existence of a cardinal utility function. This idea has a long history; Alt (1936, 1971), Suppes and Winet (1955), Scott and Suppes (1958), Debreu (1958), Pfanzagl (1968), Krantz et al. (1971), Wakker (1989), and Köbberling (2006) have used (complete) difference preorders to construct cardinal utility representations for individual preferences.Footnote 3 Meanwhile, Dyer and Sarin (1978, 1979a, b), Harvey (1999) and Harvey and Østerdal (2010) studied the utilitarian aggregation of such individual difference preorders into a (complete) social difference preorder. (Theorem 4.2 of this paper is roughly comparable to these earlier results.) However, these papers all work with complete preorders, and they do not grapple with the issue of interpersonal comparisons. The second theme of this paper is the use of incomplete preorders to represent “imperfect” or “approximate” interpersonal comparisons. This represents a compromise between (i) the ‘perfect’ interpersonal comparability assumed by some models [e.g. d’Aspremont and Gevers (2002)] and (ii) the total impossibility of interpersonal comparisons asserted by other writers [e.g. Robbins (1938)]. Position (i) is open to serious epistemological (or even metaphysical) objections.Footnote 4 But position (ii) contradicts common sense, and reduces welfare economics to the ethically vacuous criterion of Pareto optimality. However, the conflict between (i) and (ii) is a false dichotomy. Sen (1970a, b, 1972), Fine (1975), Blackorby (1975), Basu (1980), Baucells and Shapley (2006, 2008), Adler (2012) and Pivato (2013b, d) have all approached social choice and welfare from the perspective of approximate or incomplete interpersonal comparisons. These papers all involved interpersonal comparison of welfare levels, and aside from the last two papers, they all worked with (approximately comparable) cardinal utility functions.Footnote 5 In contrast, the present paper does not assume the existence of a utility function, and it concerns interpersonal comparison of welfare changes. The rest of this paper is organized as follows. Section 2 introduces notation and terminology. Section 3 introduces difference preorders: preorders on \({\mathcal { X}}\times {\mathcal { X}}\) which encode statements like (1). Section 4 then defines a social difference preorder (SDP) to be a preorder on the space of social state changes that satisfies weak versions of the Pareto and Anonymity axioms. Section 4.1 introduces the family of quasiutilitarian SDPs, which rank two state changes by comparing their utilitarian sums with respect to some list of utility functions compatible with \((\succeq )\). Our first main result (Theorem 4.2) says that these are the only SDPs which can be represented by social welfare functions. Section 4.2 introduces the minimal SDP, which is a subrelation of every other SDP (Proposition 4.3). Three of our other major results (Theorems 4.4, 4.5 and 4.7) say that, under certain conditions, the minimal SDP is the approximate utilitarian SDP —the quasiutilitarian SDP defined by the list of all utility functions compatible with \((\succeq )\). Section 4.3 extends the model of Sect. 4 to infinite populations; this is important for intertemporal or risky social choice. Finally, Sect. 5 applies the SDP concept to a simple model of redistributive wealth transfers. All proofs are in the Appendices. Appendix A contains background material, Appendix B contains the proofs of results from Sect. 4.1, Appendix C contains the proofs of results from Sect. 4.2, and so on.",2
79.0,2.0,Theory and Decision,06 November 2014,https://link.springer.com/article/10.1007/s11238-014-9470-z,Consistent enlargements of the core in roommate problems,September 2015,Duygu Nizamogullari,İpek Özkal-Sanver,,,Female,Unknown,Mix,,
79.0,2.0,Theory and Decision,04 September 2014,https://link.springer.com/article/10.1007/s11238-014-9467-7,Stable partitions in many division problems: the proportional and the sequential dictator solutions,September 2015,Gustavo Bergantiños,Jordi Massó,Alejandro Neme,Male,Male,Male,Male,"Consider the division problem faced by a set of agents who have to share a unit of an homogeneous and perfectly divisible good. For instance, a group of agents participate in an activity that requires a fixed amount of labor measured in units of time. Given a wage, classical monotonic and quasi-concave preferences on the set of bundles of money and leisure generate single-peaked preferences on the set of potential shares, where the best share is the amount of working time associated to the optimal bundle and in both sides of the best share the preference is strictly monotonic, decreasing at its right and increasing at its left. Similarly, a group of agents join a partnership to invest in a project (an indivisible bond with a face value, for example) that requires a fixed amount of money, neither more nor less. Their risk attitudes and wealth induce single-peaked preferences on the amount to be invested. As in the previous examples, there are other social choice settings for which the division problem appears as its reduced problem (see for example, Barberà and Jackson 1995). A solution is a family of mappings that select for each instance of division problem (a set of agents and their single-peaked preferences) a vector of shares, one for each agent. But for most single-peaked preference profiles, the sum of the best shares will be either larger or smaller than the total amount to be allocated. A positive or negative rationing problem emerges depending on whether the sum of the best shares exceeds or falls short of the fixed amount. Sprumont (1991) started a large literature characterizing solutions in terms of alternative sets of properties. These solutions differ on the underlying principles guiding how the rationing problem has to be solved.Footnote 1
 In this paper, we study the division problem when the good to be allocated also comes with fixed amounts but now agents may share several units, whose number is endogenous because it may depend on agents’ preferences. Consider for example a group of entrepreneurs examining several business opportunities. Each entrepreneur is willing to devote himself to at most one of those business opportunities and as before, their risk attitudes and wealth induce single-peaked preferences on the amount to be invested. We let agents partition themselves into coalitions in such a way that agents in each coalition will have to share one and only one unit of the good. We want to emphasize that the class of real-life examples that we want to model have to have the feature that every coalition of the partition, no matter what its composition is, gets a full unit to allocate among its members; for instance, when a set of agents who want to invest some of their savings in a particular indivisible bond, no coalition is able to buy strictly less than the face value of the bond. An allocation is a pair consisting of a partition of the set of agents and a vector of allotments specifying for each coalition in the partition a vector of shares, one for each agent in the coalition, whose components add up to one unit. A rule is a mapping that selects for each profile of single-peaked preferences an allocation. Thus, a rule can be decomposed into two procedures. For each profile of single-peaked preferences, the first procedure is a function that selects a partition of the set of agents while the second procedure is a solution to be applied to the subprofile of single-peaked preferences of the agents in each coalition of the partition. We restrict ourselves to second procedures that select the allotment by means of a unique solution applied to each rationing problem faced by each coalition in the partition. This restriction implies that the same principles are used across coalitions and it can be interpreted as a consistency requirement. Thus, a rule can be identified with a partition function (mapping single-peaked preference profiles into partitions of the set of agents) and a solution (to be applied to each coalition of the selected partition). Our main concern in this paper is the stability of rules.Footnote 2 Specifically, fix a solution. We want to know whether there exists a partition function that, together with the fixed solution, constitute a stable rule. Our notion of stability is based on the principle that the allocations proposed by the rule have to be voluntarily accepted by the agents in the following sense. Consider a rule and a profile of single-peaked preferences. Apply the rule to the profile, thereby obtaining a partition and a vector of allotments. Take an agent in a coalition and another coalition (which may be empty), and suppose that (1) the agent wants to leave his original coalition to join the other one because the share assigned to him by the solution applied to the subprofile of preferences of the agents in the new coalition is strictly preferred to his former share, and (2) all agents of the receiving coalition want to admit the agent because the shares assigned to them by the solution applied to the subprofile of preferences of the agents in the new coalition are weakly preferred to their respective former shares. In this case, the original chosen allocation would be unstable at the profile to which the rule has been applied. A rule is stable if it chooses stable allocations at each profile of single-peaked preferences. Three remarks are in order. First, to generate an instability we are requiring that the moving agent has to obtain a strictly preferred share while the agents of the receiving coalition have to obtain a weakly preferred share. This captures the idea that to move from one coalition to another (the origin of the instability) requires a bit more than just to admit a new member in the coalition. Second, instabilities are generated only by one agent moving to a new coalition. In this case, the needed coordination among agents to fulfill the instability is minimal compared with the coordination needed if non-singleton subcoalitions would be allowed to change coalitions. Third, the receiving coalition may be empty, in which case the instability would be produced only by the agent that by leaving his current coalition could be strictly better off; i.e., the agent would strictly prefer the full share of one unit of the good to the share he had been assigned in his original coalition. In a similar setting Gensemer et al. (1996, 1998) study another concept of stability that they call “migration equilibrium”. Agents with single-peaked preferences are partitioned into several local economies, each of which has an endowment that is allocated among its participants following a given solution. A migration equilibrium requires that no agent will be better off by leaving his economy to join another. They show that when the solution applied to each local economy is well behaved there might not exist a migration equilibria.Footnote 3 Note that the receiving economy cannot ban the arrival of a new agent and hence the migration equilibrium is a stronger stability condition than the one studied in this paper. Which stability concept to apply depends on the applications. In an environment with a small number of agents with decision power such as in joint ventures, our concept is more appealing whereas for movements across countries or big societies the migration equilibrium is the one to be considered.Footnote 4
 We found that in general, finding partition functions for well-known and simple solutions, to constitute together stable rules, is not an easy task. Indeed, it may become extremely complex in the general setting of the division problem. Thus, we have simplified the problem by assuming that agents’ single-peaked preferences are in addition symmetric.Footnote 5 A single-peaked preference is symmetric if the following additional condition holds: a share is strictly preferred to another one if and only if the former is strictly closer to the best share. Observe that in many applications the linear order structure on the set of potential shares, relative to which single-peakedness is defined, conveys to agents’ preferences more than just an ordinal content. Often, an agent’s preference on the set of shares is responsive also to the notion of distance, embedding to the preference its corresponding property of symmetry (see Massó and Moreno de Barreda 2011 for the use of symmetric single-peaked preferences in the context of selecting a public good, as in Moulin 1980). The use of symmetric single-peaked preferences has the additional advantage that, without loss of generality, the domain of the rule is the set of vectors of best shares, instead of the set of profiles of full preferences. Our main results establish that, provided that agents’ preferences are symmetric single-peaked, the proportional solution (Proposition 1) and all sequential dictator solutions (Proposition 2) have the property that for each one of them there exists a partition function that, together with the corresponding solution, constitute a stable rule.Footnote 6 The proportional solution of the division problem assigns to each agent, given a vector of agents’ best shares, a share that is equal to his best share divided by the sum of all the best shares. Remember that the solution is applied to each coalition in the partition selected by the partition function at the vector of agents’ best shares. Given an ordering on the set of agents, the sequential dictator solution associated with this ordering, and applied to a vector of agents’ best shares, let each agent, except the last one, choose sequentially (following the ordering) his share of what is left of the good (if anything) by his predecessors. The last agent in the ordering gets the remainder. Observe that (i) each ordering on the set of agents define a different solution of the division problem, and (ii) the order is fixed and used in each of the coalitions selected by the partition function at the same vector of agents’ best shares. The proofs of the two results are constructive and proceed by induction on the number of agents. In addition, we exhibit examples showing that for both rules stability is a strong requirement incompatible with many other desirable properties like efficiency, strategy-proofness, anonymity, and non-envyness. We also show that there are simple solutions for which there do not exist partition functions that together constitute stable rules. In particular, we exhibit an example of a vector of weights (one for each agent) and a profile of preferences with the property that there is no partition function that, together with the corresponding weighted proportional solution, constitute a stable rule. 
Amorós (2002), Adachi (2010), and Morimoto et al. (2013) study also multi-dimensional extensions of Sprumont (1991) division problem. They extend the uniform solution of a division problem to many division problems (Amorós 2002 does it for problems with only two agents). Their approach is different to ours because they consider problems where the goods to be allocated may be different and each agent has preferences on vectors of his potential shares (one for each different good). Their main contribution is to extend and axiomatically characterize the uniform solution to the multiple goods setting. The paper is organized as follows. In Sect. 2 we introduce the model. In Sect. 3 we state and prove our main results. In Sect. 4 we present some final comments.",1
79.0,2.0,Theory and Decision,14 August 2014,https://link.springer.com/article/10.1007/s11238-014-9463-y,The sequential equal surplus division for rooted forest games and an application to sharing a river with bifurcations,September 2015,Sylvain Béal,Amandine Ghintran,Philippe Solal,Male,Female,Male,Mix,,
79.0,2.0,Theory and Decision,10 August 2014,https://link.springer.com/article/10.1007/s11238-014-9460-1,Eyes on social norms: A field study on an honor system for newspaper sale,September 2015,Thomas Brudermann,Gregory Bartel,Sebastian Seebauer,Male,Male,Male,Male,"An honor system is a form of sales concept which is mainly used for marketing low-price goods. In honor systems, consumers typically acquire a certain product on a self-service basis and pay for it without any payment control, e.g., by inserting the required amount of money into a cash box available at the point of sale. Honor systems are a cheap and simple method of marketing goods, especially where the expected turnover is relatively low and does not justify employing a cashier, or using a sales machine. Frequent applications of this honesty-based sales concept include the sale of candy or coffee in firm canteens, the self-picking of flowers, small-scale farmer-to-consumer direct marketing, or the sale of newspapers. Another related example is self-checkout cash desks at retail stores where consumers scan the barcodes of items themselves and then pay with their bank or credit card. As the term ‘honor system’ suggests, all these sales systems are dependent on the honesty of the customers. Usually, a certain share of free-riders can be tolerated, but too large a number will cause any honor system to collapse. Thus, the central challenge is to ensure high payment morale and to limit free-riding as much as possible despite the anonymous interaction between vendor and customer. At the same time, honor systems represent a nice opportunity for studying honest and dishonest behavior in the field. The question of how to reduce free-riding and how to increase honesty has been addressed by previous studies in a variety of settings, applying moral appeals, or ‘eye images,’ i.e., photos of a pair of human eyes, as cues for social norms. One straightforward approach is to withdraw the service, as soon as the payment levels fall under a certain threshold. This was done, for example, in a candy bar case studied by Haan and Kooreman (2002). In their study, involving medium and small-scale companies provided with candy bars, the consumers (i.e., employees of the respective companies) were informed that the service would be stopped if payments fell below a certain threshold. The threshold was set at 65 %, but this was not revealed to the consumers. Honesty levels under these circumstances were rather high, and only in 29 out of 166 companies were the service discontinued due to shortfalls in payments. Another approach is to use moral appeals. For example, in one case study on an honor system for newspaper sale, it was found that while moral reminders increased the amount of money given by those who actually paid, there was no increase in the number of people paying (Pruckner and Sausgruber 2013). In a study on honesty-based flower picking, the presence of internal moral norms was suggested as being an important predictor of payment levels, thus, making it plausible that activating these norms would improve payment morale (Schlüter and Vollan 2011). Appeals based on social norms seem to be a promising approach for increasing payments in honor systems as well. In experimental settings, the compliance of most people with social norms is conditional upon the compliance of others (Fehr et al. 2002; Fehr and Gächter 2002; Fischbacher et al. 2001). For the case of honor systems, this finding implies that most consumers would be willing to pay correctly, if they knew that others did so as well. Hence the challenge is to make norms of desired behavior more salient, for example, by providing information on how other people behave in certain situations. Such attempts succeeded in a study on towel re-use conducted in US hotels (Goldstein et al. 2008), and in a study on household energy conservationFootnote 1 (Schultz et al. 2007). Participants in these studies were simply informed of other people’s good conduct, and this was sufficient for them to align their own behavior to the norm. Environmental cues also have the potential of communicating a certain normative standard. Keizer et al. (2008) for instance showed that the presence of graffiti next to a no-graffiti sign increases the likelihood of littering at such a location. They suggested that the presence of graffiti indicated the violation of the no-graffiti-norm and led people to believe that it was normal or acceptable to disregard rules, and consequently other norms were more likely to be violated as well. More recently, a completely different approach has emerged: Haley and Fessler (2005) initially suggested that the presence of pictures of human eyes positively affects pro-social behavior in experimental settings. This observation was confirmed in a quasi-field study conducted by Bateson et al. (2006). They found that university staff paid more for their coffee in a self-service coffee-bar, when such ‘eye images’ were present. Subsequent studies with eye images have since been extended to cover applications beyond honor systems. The aim is to improve honesty and pro-social behavior or to reduce delinquent behavior in anonymous settings. Ernest-Jones et al. (2011) successfully used eye images in order to reduce littering in a university canteen. In addition, combining the eye images with a written appeal not to litter was more effective than combining eye images with an unrelated message. In a field study at a bus stop, people waiting were more often willing to remove litter (placed there secretly by the experimenters), when eye images were present (Francey and Bergmüller 2012). Apart from that, eye images might also aid crime prevention. The presence of a large eye image at bicycle racks, in combination with a moral reminder (‘cycle thieves we are watching you’), dramatically reduced thefts in those locations where it was used (Nettle et al. 2012). However, since control locations experienced a dramatic increase of thefts, it seems that overall crime levels were not reduced, but merely that crime shifted to different locations. However, recent experimental results call for a very careful interpretation of the effects observed. Fehr and Schneider (2010) found zero effect when investigating the impact of ‘eye cues’ on cooperativeness in a trust game, and Raihani and Bshary (2012) similarly found no effect of eye images on cooperative behavior in a dictator game carried out under truly anonymous conditions. On the other hand, a recent replication study by Nettle et al. (2013) confirmed the initial findings by Haley and Fessler (2005), and clarified that while ‘watching eyes’ may increase the probability of donating something, they do not increase the mean donation in the dictator game setting. To sum up, the presence of eye images seems to increase cooperation levels and norm-compliance in a variety of settings—but not in all settings. It is not yet clear from the literature in which a kind of settings cues of being watched have the potential to increase honesty or pro-social behavior and in which not. An explanation for the efficacy of eye images as a subtle cue to being observed may be the essential role that eyes and eyebrows play, as has been pointed out by several studies investigating eye movement and gaze control during face learning and face recognition (Grüter et al. 2008; Henderson et al. 2005; Janik et al. 1978; Walker-Smith et al. 1977). The aim of this paper is to investigate payment morale in a specific honor system, namely the ‘silent sale’ of newspapers in an urban region. The chosen study region represents a ‘difficult case’ in that relatively high levels of dishonesty prevail, i.e., payment morale here is particularly low when compared to other regions. Individual behavior in this setting is completely anonymous, so that customers need not fear retribution and cannot infer anything about norms by observing the actions of others. Under such circumstances, increasing the level of payment is not likely to be an easy task. We aim to address this task and state our research question as follows: Which cues are effective in increasing honesty in such a system—norm-based appeals, the presence of eye images, or both? The combination of descriptive norms and cues about being watched has, to the best of our knowledge, only been addressed marginally, although such a combination seems promising: Social norms may lose power when people feel unobserved. This can be counteracted to some extent by introducing specific ‘cues’ which increase the sense of being watched. Furthermore, we are also interested in whether such cues are subject to a saturation/exhaustion effect. Does their effectiveness decreases over time? Can cues be ‘turned on and off’, so that the ‘moral effect’ vanishes when the cue is canceled, but reappears as soon as the cue is introduced again? We have to keep in mind that this real-world setting involves behavior which is repeated on a regular basis for a long period, and which is habitual to a certain degree. It is rather difficult to isolate this decision-making process, i.e., whether to pay correctly or not for the newspaper, from the familiar context it usually is embedded in. We therefore consider a field study as the best approach for addressing our research questions. Previous field studies with eye cues have to date mainly been conducted in university settings (i.e., institute kitchen, university canteen, campus) and have involved academic staff and students. Thus, studies of the general population tend to be underrepresented in this domain. This means that the overall validity of the results from such studies if often open to challenge (Levitt and List 2007; Thøgersen 2008). In contrast, our study helps rectify the situation somewhat by dealing with a population outside the world of students and academic staff. As a result, our approach offers a higher degree of external validity, but also less rigor regarding experimental conditions. The remainder of the paper is structured as follows: In Sect. 2, our study design and the chosen methods are presented. Section 3 summarizes the results of statistical analysis. In Sect. 4, we discuss the limitations to and implications of our study. A concluding section completes the paper.",13
79.0,2.0,Theory and Decision,12 September 2014,https://link.springer.com/article/10.1007/s11238-014-9466-8,Inattentive consumers in markets for services,September 2015,Stefania Sitzia,Jiwei Zheng,Daniel John Zizzo,Female,Unknown,Male,Mix,,
79.0,2.0,Theory and Decision,14 August 2014,https://link.springer.com/article/10.1007/s11238-014-9461-0,Tractable consumer choice,September 2015,Daniel Friedman,József Sákovics,,Male,Male,Unknown,Male,"
John has just realized that it is his wedding anniversary. He enters the only liquor store in town and looks for champagne. There is only one bottle, which costs $100. How does he decide whether to buy it? 
 In this paper, we argue that a consumer like John should recall his utility value for money, call it \(\lambda \), and use it to estimate the opportunity cost of spending $100. He should purchase the champagne if and only if the utility it brings covers that opportunity cost. Our recommendation might sound like common sense, but it differs sharply from what economics textbooks propose. General equilibrium theory would require John to consider the impact on all possible future consumption plans. As is widely recognized, for actual people, or even Homo Economicus facing moderate computation costs or other frictions, this is too complex a problem to solve. Consequently, micro textbooks (and recent behavioral models of mental accounting) propose an alternative, partial equilibrium solution: John should have a pre-established budget to spend on items including last minute anniversary presents, and should purchase the champagne if and only if that budget exceeds $100 and, on the margin, the other claims on the budget are less pressing. We shall argue that this alternative solution, although tractable, is twice hobbled. First, it is silent on how to specify the budget set. When and how should John come up with the amount to spend and the range of goods to consider? Second, the budget constraint rules out substitution of purchasing power between present and future, thus preventing John from properly responding to prices that turn out to be higher or lower than expected. We propose to use \(\lambda \), the marginal utility of money, rather than a budget constraint, to link the present problem to the rest-of-life problem. The numerical value of \(\lambda \) captures the trade-off between current and rest-of-life expenditure, and thus allows the consumer to make optimal saving/borrowing decisions. This approach resonates with the findings of consumer research, and \(\lambda \) itself is a meaningful concept that can be approximated, learned, and adjusted in intuitive ways. Our approach relates to several strands of consumer choice theory, some historical and some more recent. These include inter alia Marshall’s theory of demand; partial equilibrium analysis; the use of cardinal and quasi-linear utility functions; Frisch demand functions; and recursive life-cycle models. A distinctive aspect of our approach is how it connects present and future choices. Our consumer occasionally adjusts her estimated marginal utility of money looking forward, but when faced with a specific consumption choice she just uses the current value of \(\lambda \). Existing approaches either ignore the future entirely (partial equilibrium) or else refuse to consider the future as qualitatively distinct from the present (general equilibrium). The simple connection we impose naturally leads to a better understanding of where \(\lambda \) comes from and what makes it useful. Our free standing theory of consumer choice includes the following innovations: Neoclassical foundations for (cardinal) quasi-linear utility and for the use of \(\lambda \) as a rule of thumb. Proof of near-optimality of a \(\lambda \)-based response to price surprises. Extensions to choice among baskets of indivisible goods, and choice given liquidity constraints. An adaptive learning model for \(\lambda \), complementing the forward-looking analysis. Examples showing how \(\lambda \) can embody behavioral biases. Section 2 begins the exposition with a review of the lifetime consumption problem and its textbook solution. We then define separable subproblems and use the indirect utility function to obtain a quasi-recursive solution to the subproblem and its continuation problem. The first-order condition defines \(\lambda \) as the opportunity cost of subproblem expenditure. Section 3 analyzes the subproblem solution given an exogenous value of \(\lambda \). It shows that the consumer moves out along the subproblem’s Income Expansion Path—the locus of points where the ratio of marginal utility to price is the same for each good—until that ratio equals \(\lambda \). The solution yields the “moneysworth” demand function, whose price effects and \( \lambda \) effects are shown to be quite natural. In particular, demand is always decreasing in own price (hence there are no Giffen goods), while the cross-price effects straightforwardly reflect preference-based substitutability or complementarity. Quasi-linear preferences emerge as a natural special case. Section 4 discusses how the consumer can calibrate \(\lambda .\) A Taylor expansion of indirect continuation utility reveals that \(\lambda \) is approximately constant when subproblem expenditures are small relative to lifetime income. An implication is that \(\lambda \) is reusable: the consumer typically need not adjust it between consecutive small purchases. Eventually, however, she does need to update. We posit two complementary methods: one is forward looking and uses relevant news in a life-cycle setting, while the other is experiential and uses previously observed prices to update \(\lambda \). Section 5 recalls the textbook partial equilibrium approach, which holds the subproblem budget constant, and yields the Marshallian demand function. Proposition 3 shows that the “true” lifetime-optimal demand elasticities are typically well approximated by those of the moneysworth demand function, but only under quite special conditions are they close to the Marshallian elasticities. Section 6 extends the model to include liquidity constraints which are shown to correspond to the situation where \(\lambda \) is a step function of available liquidity. Marshallian demand reappears as an extreme case, where \(\lambda =0\) below the budget and \(\lambda =\infty \) over the budget. We also show that consumers can handle indivisibilities by comparing \( \lambda \) to an appropriately defined quality-price ratio. Section 7 reiterates that, compared to textbook approaches, the \( \lambda \) (or moneysworth) approach offers (a) more robust prescriptions for how consumers should react to surprises, (b) a better way to connect partial equilibrium to general equilibrium analysis, and also (c) more plausible descriptions of actual human behavior. That section also notes fruitful avenues for future research. Appendix concisely reviews connections to the most relevant strands of historical and recent economic literature, and Appendix collects mathematical details.",5
79.0,2.0,Theory and Decision,09 September 2014,https://link.springer.com/article/10.1007/s11238-014-9468-6,On \(\mathcal {S}\)-independence and Hansson’s external independence,September 2015,Dan Qin,,,Male,Unknown,Unknown,Male,"Among all Arrovian axioms, independence of irrelevant alternatives (IIA) is arguably the most controversial. A tremendous amount of literatures are devoted to analyze its property and the consequence of relaxing it. The relaxing of IIA has been examined in at least two ways. IIA requires that the social preference over a pair of distinct alternatives should depend only on the individual preferences over the same pair. In other words, every pair of distinct alternatives is self-dependent. The first route is to retain the self-dependent spirit and apply it to certain bigger sets than doubletons. Blau (1971) introduces m-ary independence: a social welfare function satisfies m-ary independence if ranking of a subset of the set of alternatives with cardinality \(m\) does not depend on preferences on other alternatives. Blau (1971) shows that m-ary independence is equivalent to Arrow independence with \(m\) smaller than the cardinality of the alternative set. Cato (2013) generalizes this result by introducing \(\mathcal {S}\)-independence: given a collection \(\mathcal {S}\) of subsets of the set of alternatives, a social welfare function satisfies \(\mathcal {S}\)-independence if social ranking of any element \(S\) of \(\mathcal {S}\) only depends on individual preferences on \(S\) itself. Cato shows that \(\mathcal {S}\)-independence is equivalent to Arrow independence when \(\mathcal {S}\) satisfies certain connectedness condition. In similar spirit, Hansson (1973) provides a unique way of analyzing independence conditions through notions called external similarity and external similarity independence. For any \(\{x,y\}\), two profiles are said to be external similar on \(\{x,y\}\) if they agree on \(\{ \{w,z\}:\{w,z\}\cap \{x,y\} \ne \emptyset \}\). External independence says that if two profiles are externally similar with respect to some set \(S\) then the social ranking generated from these profiles is also externally similar on \(S\). Equivalently, external independence requires that the external set of any subset of the set of alternatives is self-dependent. The second route is to abandon the self-dependent spirit all together. Campbell and Kelly (2000) propose information function method to analyze the requirements of preference information to rank a pair of distinct alternatives. This line of research can also be traced back to Hansson (1973). Hansson’s (1973) weak external independence requires that the social ranking over a subset of the set of alternatives depends only on the individual preferences over the external set of that subset. The current article is in the spirit of Blau (1971) and Cato (2013), but generalizes the results in light of the following observation. The external sets Hansson (1973) discussed are structurally different from m-ary independence and \(\mathcal {S}\)-independence. External independence describes self-dependent subsets of the set of pairs of distinct alternatives. Literatures in economic environment also mainly discuss information on pairs of alternatives (See, Fleurbaey et al. 2005a, b).Footnote 1 The independence conditions in allocation theory are closely related to Hansson’s (1973) partially relevant information, which can only be captured by pairs of alternatives. On the other hand, both m-ary independence and \(\mathcal {S}\)-independence describe self-dependent subsets of the set of alternatives. Because both social and individual preferences are binary relations, there exist certain types of self-dependency which cannot be described by m-ary independence and \(\mathcal {S}\)-independence. Consider a social welfare function and a set of unordered pairs of alternatives \(\{\{x,y\},\{x,z\}\}\) such that its social ranking only depends on individual preferences on itself. In this case, there exists no obvious elements in \(\mathcal {S}\), while self-dependent subset actually exists : \(\{\{x,y\},\{x,z\}\}\). The reason is the following. Independence conditions describe preference information which should have no impact on certain part of the social preference. Both individual and social preferences are binary relations on the set of alternatives \(X\), i.e., subsets of \(X\times X\). To define independence conditions, one needs to employ the concept of restriction of preferences on certain subsets of \(X\times X\). For \(\hat{S}\subseteq X\times X\), restriction is naturally defined as \(R\cap \hat{S}\). If one only considers \(S\subseteq X\), restriction is then defined as \(R\cap S\times S\). However, there exist subsets of \(X\times X\) which cannot be expressed in the form of \(S \times S\). In other words, only part of the self-dependency can be identified if one focuses on subsets of alternatives. This deficiency can be addressed if we consider subsets of the set of pairs of distinct alternatives. We refine \(\mathcal {S}\)-independence by proposing \(\mathcal {\hat{S}}\)-independence: given a collection \(\mathcal {\hat{S}}\) of subsets of the set of all pairs of distinct alternatives, a social welfare function satisfies \(\mathcal {\hat{S}}\)-independence if social ranking of any element \(S\) of \(\mathcal {\hat{S}}\) only depends on individual preferences on \(S\) itself. In the previous example, \(\{\{x,y\},\{x,z\}\}\) is an element of \(\mathcal {\hat{S}}\). We show that with similar connectedness condition, \(\mathcal {\hat{S}}\)-independence is equivalent to Arrow independence. In addition to theoretical interests, this refinement also provides a tool to analyze special kinds of independence conditions, e.g., Hannson’s (1973) external independence conditions. In light of the observation that external independence actually describes a special kind of \(\mathcal {\hat{S}}\)-independence, we complement Hansson’s (1973) results by showing the equivalence of external independence and Arrow independence. In contrast to analysis in economic environment, studies of independence condition in Arrovian framework provide little discussion about information on pairs of alternatives. This work is also one attempt to fill this blank. The rest of this article is organized as follows. Section 2 introduces notation and definitions. Section 3 strengthens Cato ’s (2013) \(\mathcal {S}\)-independence theory. Section 4 examines external similarity independence conditions. Section 5 discusses how our analysis can be applied to the second route in relaxing IIA.",
79.0,2.0,Theory and Decision,24 July 2015,https://link.springer.com/article/10.1007/s11238-015-9508-x,Erratum to: My experimental meanderings,September 2015,John Hey,,,Male,Unknown,Unknown,Male,"I must apologise to Noemi Pace for omitting her from my list of co-authors printed at the end of this paper. This final paragraph should read: “I am fortunate to have many co-authors, most of whom have inspired and encouraged me. Those who have worked with me on experiments include (and I hope that I have not omitted any) Louise Allsopp, David Ansic, Marie-Edith Bissey, John Bone, Roberto Burlando, David Butler, Enrica Carbone, Anna Conte, Valentino Dardanoni, Daniela Di Cagno, Xueqi Dong, Mariateresa Fiocca, Konstantinos Georgalos, Ricardo Goncalves, Jinkwon Lee, Gianna Lotito, Julia Knoll, Anna Maffioletti, Konstantina Mari, Peter Moffat, Andrea Morone, Tibor Neugebauer, Chris Orme, Stefania Ottone, Noemi Pace, Luca Pannacione, Carmen Pasca, Massimo Paradiso, Cristina Pitassi, Martin Reynolds, Karim Sadrieh, Patrizia Sbriglia, Ulli Schmidt, Elisabetta Strazzera, John Suckling and Wenting Zhou. My thanks to all of them. And, once again, my thanks to the organisers of this special issue.”",
79.0,3.0,Theory and Decision,17 October 2014,https://link.springer.com/article/10.1007/s11238-014-9476-6,Almost expectation and excess dependence notions,November 2015,Michel M. Denuit,Rachel J. Huang,Larry Y. Tzeng,Male,Female,Male,Mix,,
79.0,3.0,Theory and Decision,24 October 2014,https://link.springer.com/article/10.1007/s11238-014-9475-7,The value of risk reduction: new tools for an old problem,November 2015,David Crainich,Louis R. Eeckhoudt,James K. Hammitt,Male,Male,Male,Male,"Let \(x\) be a continuous variable, representing wealth or health, and consider the binary lottery with outcomes \(x_\mathrm{{{1}}} = x\) and \(x_\mathrm{{{0}}}= x-L_{x}\), with \(L_{x} > 0\). The probability of the worse outcome \(x_\mathrm{{{0}}}\) is \(p\) and the probability of the better outcome \(x_\mathrm{{{1}}}\) is (1\(-p\)). Expected utility is given by where \(u(\cdot )\) is a continuously differentiable utility function with \({u}^{{\prime }}> 0\) (primes denote derivatives). The marginal rate of substitution between \(x\) and \(p\) is If \(x\) represents wealth, d\(x\)/d\(p\) represents the marginal WTP to reduce the chance of the low-wealth outcome and increase the chance of the high-wealth outcome. For example, one might rent a safe-deposit box to protect valuables from theft. Alternatively, if \(x\) represents health, d\(x\)/d\(p\) represents the marginal willingness to compromise health to reduce the chance of a bad health outcome. For example, one might choose radiation or other treatments with adverse side effects to reduce the chance that a cancer proves fatal. Define \(\bar{{x}}=E\left( {\tilde{x}} \right) =px_0 +\left( {1-p} \right) x_1 \) and \(\sigma _x^2 =\hbox {var}\left( {\tilde{x}} \right) =p\left( {1-p} \right) L_x^2 \). Approximate the terms in the numerator of Eq. (1.2) using a second-order Taylor series expansion around \(\bar{{x}}\) to obtain: The denominator of Eq. (1.2) may be approximated by where \(\psi \) is the prudence premium, which may itself be approximated by (Kimball 1990). Substituting these approximations into Eq. (1.2) yields Dividing the numerator and denominator by \({u}^{\prime }\left( {\bar{{x}}} \right) \) yields Equation (1.8) shows that the marginal rate of substitution between \(x\) and \(p\) is the product of two terms: the marginal rate of substitution if \(u\) is linear (the potential loss \(L_{x})\) and an adjustment factor. The numerator of the adjustment factor depends on the probability of the adverse outcome \(p\), the possible loss \(L_{x}\), and the coefficient of absolute risk aversion \(\left( {-\frac{{u}^{{\prime }{\prime }}}{{u}^{\prime }}} \right) \). Interestingly, the effect of risk aversion depends on the difference between the initial probability of loss \(p\) and the critical value 1/2. Note that if \(p = 1/2\), the numerator equals one, regardless of the potential loss and the degree of risk aversion. In contrast, if \(p < 1/2,\) the numerator is increasing in both \(L_{x}\) and the measure of risk aversion and is larger than one if \(u\) is risk averse. This result is intuitive: for \(p < 1/2\), a decrease in \(p\) reduces the variance of final wealth, which is appreciated by a risk-averse decision maker, so he values the decrease by more.Footnote 2 The opposite effect occurs for \(p > 1/2\), for which a decrease in \(p\) increases the variance of final wealth.Footnote 3
 Notice that the true value of d\(x\)/d\(p\) must be positive since a decrease in \(p\) induces a first-order stochastically dominant improvement in the decision maker’s situation. This shift must be compensated by a decrease in \(x\) (a first-order stochastically dominant deterioration) to maintain welfare constant. To be useful, approximations such as the one in Eq. (1.8) must at least have the same sign as the true value. For a risk-averse decision maker, this is definitely the case when \(p \le 1/2\).Footnote 4 From this point forward, we assume the initial probability of loss does not exceed 1/2, which is realistic for most applications. The denominator of the adjustment ratio depends on the riskiness of the lottery (measured by its variance) and a measure of downside risk aversion, \(\frac{{u}^{{\prime }{\prime }{\prime }}}{{u}^{\prime }}\) (Modica and Scarsini 2005; Crainich and Eeckhoudt 2008). This coefficient can be interpreted as follows: the effect of a zero-mean risk \(\tilde{\varepsilon }\) on the marginal utility of wealth is measured by which is the difference between the expected marginal utility at \(x\) with and without \(\tilde{\varepsilon }\). Of course \({u}^{{\prime }{\prime }{\prime }}\left( x \right) > 0\) implies \(v(x) > 0\); hence for a prudent/downside risk-averse decision maker, a zero-mean risk increases expected marginal utility. A second-order approximation of \(v(x)\) yields which, as we have seen, measures a gain in marginal utility (\(\sigma _\varepsilon ^2 \) is the variance of \(\tilde{\varepsilon })\). To transform this gain in marginal utility into its monetary equivalent, one divides, as usual, by the marginal utility of wealth \({u}^{\prime }\left( x \right) \) so that \(\frac{{u}^{{\prime }{\prime }{\prime }}}{{u}^{\prime }}\) is a measure of the intensity of downside risk aversion.Footnote 5
 Downside risk aversion (equivalently, prudence) is characterized by a positive third derivative (i.e., convex marginal utility). The denominator of Eq. (1.8) is increasing in both the variance of the lottery and downside risk aversion; hence greater downside risk aversion yields a smaller willingness to pay to reduce the probability of the adverse outcome. Intuitively, greater downside risk aversion means that marginal utility rises more as \(x\) declines, hence the utility cost of sacrificing \(x\) if the adverse outcome occurs is larger. This suppresses the willingness to sacrifice \(x\) to reduce \(p\). For the case where \(x\) represents wealth, it is conventional to assume that both risk aversion and downside risk aversion are positive. Risk aversion increases the marginal willingness to pay to reduce \(p\) (for small probabilities of loss, i.e., \(p < 1/2\)), but downside risk aversion decreases it. As a result, contrary to a widespread belief, the total effect of an increase in risk aversion on WTP is ambiguous since it may increase both the numerator and denominator of (1.8). At this stage, it should be noted that risk aversion and downside risk aversion are not independent concepts. As shown in Appendix 1, an increase in risk aversion typically yields an increase in downside risk aversion. Exceptions are possible, however. For a quadratic utility function, downside risk aversion is constant (and equal to 0) whatever the degree of risk aversion. For the case where \(x\) represents health, the curvature of the utility function depends on how health is measured. One possibility is that \(x\) measures longevity. Empirical evidence suggests that some individuals are risk neutral, some are risk averse, and some are risk seeking with respect to longevity (Pliskin et al. 1980; Nielsen et al. 2010). Moreover, the sign of risk aversion with respect to longevity may vary with age and longevity; e.g., it seems plausible that young adults might be risk seeking for longevities ranging over middle ages but risk averse over greater longevities. In this case, the adjustment factor may depend on age and the values of \(x_\mathrm{{{0}}}\) and \(x_\mathrm{{{1}}}\). Alternatively, \(x\) may represent quality of health, which is often conceptualized as ‘health-related quality of life’ (HRQL) and combined with duration to calculate ‘quality-adjusted life years’ (QALYs), which assume risk neutrality with respect to HRQL (Pliskin et al. 1980; Hammitt 2002). HRQL is measured by various forms of hypothetical questions, including standard gambles between full health and death. The standard gamble format also assumes risk neutrality with respect to HRQL. If utility is risk neutral with respect to \(x\), then the adjustment factor equals one.",2
79.0,3.0,Theory and Decision,11 December 2014,https://link.springer.com/article/10.1007/s11238-014-9471-y,Weighted sets of probabilities and minimax weighted expected regret: a new approach for representing uncertainty and making decisions,November 2015,Joseph Y. Halpern,Samantha Leung,,Male,Female,Unknown,Mix,,
79.0,3.0,Theory and Decision,04 November 2014,https://link.springer.com/article/10.1007/s11238-014-9474-8,Cooperative games with homogeneous groups of participants,November 2015,L. Hernández-Lamoneda,Francisco Sánchez-Sánchez,,Unknown,Male,Unknown,Male,"One of the principal practical problems in cooperative game theory arises when the number of agents is large. As the number of players grows, the number of all possible coalitions grows exponentially, making it impractical (or even impossible) to compute typical solutions, such as Shapley’s value. Nevertheless, in some instances, large populations of agents may be grouped in somewhat homogeneous classes. In this article, we propose a method to deal with this situation. In particular, we give easily computable versions of the principal values in cooperative game theory adapted to, what we call, groups’ games. In general, though we consider players individually, for groups’ games the number of evaluations needed to compute Shapley’s (or any other solution) is a polynomial function in the number of players. There have been different approaches to deal with large numbers of agents. Perhaps, most notable, is the idea of considering a partition of the set of players and then look at each partition class as a (weighted) player in a new game. This idea may be the only sensible thing to do in some applications, but we believe that in certain situations (that we exemplify ahead) it might be preferable to still consider players individually, while acknowledging that they can be grouped into homogenous classes. As the examples show, solutions under these two different approaches will vary significantly, and we argue that the axiomatics of Shapley’s value (for example) support our choice. The article contains the following. The first two sections introduce groups’ games and compute explicit formulas for linear, symmetric and efficient solutions on them (in particular, the one for Shapley’s value) first and also for semivalues (an explicit formula for Banzhaff’s is given here). Precise statements are given, though the proofs are delayed to an appendix. Whereas computation of these solutions for arbitrary games requires \(2^n\) evaluations, for game groups’, partitioned into \(m\) classes, the number of evaluations is of the order of \(n^m\). Section 4 consists of several classes of examples to which the notion of groups’ games may be applied. It is important to clarify that some of these problems (and their solutions) are well known in the literature, yet they illustrate some advantage of our setup. Examples 1 and 3 show how to obtain the “classical” solution in a much faster and efficient way. Example 2 shows how one obtains a “fairer” solution than with the usual partition methods. Other examples point to possible applications.",1
79.0,3.0,Theory and Decision,23 November 2014,https://link.springer.com/article/10.1007/s11238-014-9477-5,The Nash solution is more utilitarian than egalitarian,November 2015,Shiran Rachmilevitch,,,Female,Unknown,Unknown,Female,"Nash’s (1950) bargaining problem is described in the utility space: two players face a compact, convex, and comprehensive set \(S\subset {\mathbb {R}}_+^2\) of available utility allocations, from which they need to choose one; if they agree on \(x\in S\), then each player \(i\) receives the utility payoff \(x_i\), but if they fail to reach an agreement both get zero.Footnote 1
\(^,\)
Footnote 2 The Nash solution (1950) to the problem \(S\), denoted as \(N(S)\), is the maximizer of \(x_1\cdot x_2\) over \(x\in S\). In general, a bargaining solution is a selection—a function that assigns a unique point of \(S\) for every such \(S\). Two other known bargaining solutions are the egalitarian solution, \(E\), and the utilitarian solution, \(U\). Like the Nash solution, they are related to certain maximizations. The utilitarian solution selects an agreement under which the sum of the players’ utilities is maximized. The egalitarian solution selects the weakly efficient agreement under which both players receive identical payoffs; this agreement maximizes \(\text {min}\{x_1,x_2\}\) over \(x\in S\).Footnote 3
\(^,\)
Footnote 4 In this paper, I study various connections between the Nash solution and the aforementioned ones. As we will see, there are several non-trivial connections. It is known that the Nash solution exhibits attractive fairness properties. For example, Brock (1979) showed that this solution is actually egalitarian, but not in the sense of recommending equal payoffs, but in the sense of recommending payoff ratios that are equal to the marginal rate of utility substitution between the players. An additional fairness property of the Nash solution is that it guarantees for each player at least one half of his maximum possible utility. This midpoint domination property was originally formulated by Sobel (1981); when combined with other standard conditions, it characterizes the Nash solution (Moulin 1983; de Clippel 2007). Another fairness-related result is by Mariotti (1999), who characterized the Nash solution on the basis of Suppes-Sen dominance,
Footnote 5 a criterion that requires that if \(x\) is the selected agreement and \((a,b)\) is some other feasible agreement, then both \((a,b)>x\) and \((b,a)>x\) are false. This requirement captures both fairness and efficiency restrictions, since it stems from a combination of impartiality and the Pareto principle: if, for example, \((b,a)>x\) for some feasible utility pair \((a,b)\), then since \(x\) is Pareto inferior to \((b,a)\) and impartiality implies that \((b,a)\) and \((a,b)\) are indistinguishable from an ethical point of view, \(x\) should not be selected.Footnote 6 Further works on the Nash solution in the context of fairness and distributive justice include Binmore (1989, 1991, 2005), Sacconi (2010) and Trockel (2005). In addition to exhibiting these fairness properties, the Nash solution also creates the following compromise between egalitarianism and utilitarianism (Fig. 1): Illustration of Proposition 1
 For any bargaining problem \(S\), the point \(N(S)\) lies on \(S\)’s Pareto boundary in between \(E(S)\) and \(U(S)\).Footnote 7
 Hereafter, I will refer to the bounds that are described in Proposition 1 as the \(E\)-\(U\)
bounds. Proposition 1 brings about the following questions: (I) Which side of the compromise between egalitarianism and utilitarianism, if any, is “favored” by the Nash solution? (II) What is the meaning of “being between \(E\) and \(U\)”? In light of the extensive work emphasizing the fairness aspects of the Nash solution, one might suspect that, in the abovementioned compromise, it is egalitarianism that gets the upper hand. I suggest otherwise—I will highlight the utilitarian aspects of the Nash solution and show that, in a certain sense, it is “more utilitarian than egalitarian.” This sense takes on the following form. First, Proposition 2 shows that whenever \(N\) coincides with \(E\), this common point is also utilitarian. Hence, the Nash solution is utilitarian “more frequently” than it is egalitarian. Proposition 3 gives sufficient conditions for the converse implication. Next is Proposition 4, which shows that for a normalized problem \(S\),Footnote 8 the Nash solution point for \(S\) is not only between \(U(S)\) and \(E(S)\), but, moreover, is between \(U(S)\) and \(A(S)\), where \(A(S)\) is the maximizer of \(\frac{1}{2}(x_1+x_2)+\frac{1}{2}\text {min}\{x_1,x_2\}\) over \(x\in S\)—that is, the maximizer of the average of the utilitarian and egalitarian objectives. Proposition 5 is a generalization of Proposition 4: it shows that for any problem \(S\), normalized or not, the Nash solution point for \(S\) lies between \(RU(S)\) and \(NA(S)\), where \(RU\) is the relative utilitarian solution (Dhilon and Mertens 1999, Pivato 2009, Segal 2000 and Sobel 2001) and \(NA\), the “normalized average” solution, is defined as follows: for every \(S\), the point \(NA(S)\) maximizes \(\frac{1}{2} \left( \frac{x_1}{a_1(S)}+\frac{x_2}{a_2(S)} \right) +\frac{1}{2}\text {min} \left\{ \frac{x_1}{a_1(S)},\frac{x_2}{a_2(S)}\right\} \) over \(x\in S\), where \(a_i(S)\equiv \text {max}\{x_i: x\in S\}\). More than thirty years ago, Cao (1982) presented a result that is similar to Proposition 1: he proved that for every problem \(S\), the point \(N(S)\) lies between the points that are selected by the Kalai-Smorodinsky solution (Kalai and Smorodinsky 1975) and the relative utilitarian solution.Footnote 9 On the class of normalized problems, his result and Proposition 1 amount to the same statement.Footnote 10 In addition, Proposition 5 tightens Cao’s (1982) bounds. Next is Proposition 6 which shows that for any \(\rho \in [-\infty ,1]\), the corresponding constant elasticity solution (CES) for \(S\)—the maximizer of \([x_1^\rho +x_2^\rho ]^{1/\rho }\) over \(x\in S\)—lies between \(U(S)\) and \(E(S)\); the fact that the Nash solution adheres to these bounds is a particular instance of this more general result, since the Nash solution corresponds to the limit \(\rho \rightarrow 0\). Therefore, Proposition 1 is a corollary of Proposition 6.Footnote 11
 An additional generalization of Proposition 1 is obtained by considering the weighted versions of \(N\), \(U\), and \(E\). I prove, in Proposition 7, that an appropriately weighted Nash solution lies “between” the weighted utilitarian and egalitarian solutions. Proposition 1 is a corollary of Proposition 7. In Section 5, I turn to question (II) from above: namely, to the meaning of “falling between \(E\) and \(U\).” I prove that it is equivalent to egalitarianism if utility can be transferred between the players at some cost, which is convex in the amount of the transfer. The models of transferable and non-transferable utility are particular instances of this more general model—they correspond to the cases where the cost function is the identity function and where it is identically infinity, respectively. The remainder of the paper is organized as follows. In Sect. 2, I prove results that formalize the idea that the Nash solution is “at least as utilitarian as it is egalitarian.” They are formal versions of some well-known facts about the geometry of the Nash solution, but, to the best of my knowledge, they have not been previously published. Section 3 is dedicated to the various bounds. In Sect. 4, I consider the non-symmetric generalization of Proposition 1. In Sect. 5, I prove that “falling between \(E\) and \(U\)” is equivalent to egalitarianism if utility can be transferred between the players at some (convex in the amount of the transfer) cost. Section 6 contains a discussion about the interpretation of the results, and Sect. 7 provides a conclusion. The analysis in this paper is only for 2-person bargaining. The reconciliation of utilitarianism and egalitarianism with more than two players is beyond the scope of the present paper.",17
79.0,3.0,Theory and Decision,31 January 2015,https://link.springer.com/article/10.1007/s11238-015-9481-4,Can priming cooperation increase public good contributions?,November 2015,Michalis Drouvelis,Robert Metcalfe,Nattavudh Powdthavee,Male,Male,Unknown,Male,"In psychology, priming is an important instrument that has often been used in a laboratory setting to bring about changes in human behaviour (Dolan et al. 2012; Kahneman 2011). It is formally defined as “the procedural feature that some previously activated information impacts on the processing of subsequent information” (Hertel and Fiedler 1998). Although psychological research has demonstrated that priming generally leads to a change in individuals’ psychological processes (Bargh 2006), there are some gaps in the literature with respect to incentive–compatible behaviours. Although we know from studies in psychology that people’s attitudes and behaviours can be altered via priming, so far little is understood about whether priming a concept of cooperation can effectively alter an individual’s decisions when economic payoffs from certain behaviours are involved. We aim to contribute to the literature by answering the following question: does priming a concept of cooperativeness make people more pro-social in an environment where personal and collective interests are at odds and where there are clear financial incentives to free ride? Priming can take a variety of forms. For example, individuals can be primed through perceptual/attention priming, motor/action priming, or semantic priming (LaBerge and Buchsbaum 1990; Strack and Deutsch 2004). The most popular method of priming is perhaps semantic priming, i.e. word primes. For example, interesting research on semantic priming has found that individuals can be influenced to walk more slowly and to have a poorer memory of a room if they are exposed to words relating to the elderly (e.g. “wrinkles”) at the start of the experiment (Dijksterhuis and Bargh 2001). In their meta-analysis, Bargh and Ferguson (2000) show that social behaviour can be carried out without the interaction of conscious acts of will and guidance and that priming will influence this unconscious behaviour (see also Bargh 1989, for a review). Priming is not the same as framing or anchoring. The concept of framing refers to the re-description of a logically equivalent decision problem in a positive or negative light (see, e.g. Ross and Ward 1996; Liberman et al. 2004) and to how descriptive valence influences information processing,Footnote 1 whereas, anchoring (see, e.g. Ariely et al. 2003) involves eliciting participants to focus on one trait or piece of information when making decisions. Priming, on the other hand, involves exposing a subject’s mind to a stimulus, concept, or memory—without requiring the re-description of the decision problem—so that his or her pathways to that particular stimulus, concept, or memory are reinforced and would also later be reflected on the processing of subsequent information (Kolb and Wishaw 2009). Whereas the issue of priming has been extensively explored over the last three decades in psychology, evidence on priming in economics remains scarce. If there is an experimental technique that can affect many types of behaviours, then economists should be using it, or at least exploring it. If priming affects people’s decisions independently of information and financial incentives, then standard economic models appear to be missing an important aspect of human behaviour. It also appears to be important for economists and social scientists to know whether priming may have a significant impact on cooperative behaviours in situations where the individual’s dominant strategy is to free ride. The frequent occurrence of social dilemmas in economic and social life makes them important for empirical investigation, in particular because experimental behaviour in this simple game has inspired the development of models of other-regarding preferences. Thus far, a few notable studies have examined how priming on religious identities, optimistic social outlook, and social identity affects economic outcomes, including risk preferences, behaviour in trust and gift exchange games and dictator giving (Benjamin et al. 2010a, b; McKay et al. 2010; Atlas and Putterman 2011; Cappelen and Drange Hole 2011). In our paper, we focus on cooperative priming in a standard linear public goods game in which randomly selected participants make decisions in situations where personal and group interests are misaligned. Much of the research on public goods experiments tends to focus on uncovering explicit mechanisms that foster cooperation, which include the introduction of sanctions and reward systems (e.g. Fehr and Gächter 2000, 2002; Sefton et al. 2007; Sutter et al. 2010), the establishment of a leader (e.g. Güth et al. 2007; Levati and Sutter 2007) and the option for individuals to communicate prior to playing the game (e.g. Isaac and Walker 1988; Bochet et al. 2006). Such explicit mechanisms have been found to increase significantly the provision of public goods and to help to reduce the extent to which people free ride (see Gächter and Herrmann 2009; Chaudhuri 2011, for recent reviews). By contrast, we explore the effect of a significantly more subtle intervention on the incentive to free ride in social dilemmas. We test whether activating the concept of cooperativeness through priming leads to people behaving significantly more pro-socially.Footnote 2 Our main hypothesis is that cooperative priming will enable individuals to raise their voluntary contributions towards their common resources. Our findings indicate that priming a concept of cooperation reduces free riding and leads to an 11-percentage-point increase (from approximately 25–36 %) in public good giving compared with a non-primed group. These results were obtained when real money was at stake for the participants. Together, these findings demonstrate that subtle priming cues may have a significant impact on cooperative behaviour and that research should be directed towards better understanding whether other modes of priming may change cooperative behaviours both in the laboratory and in the field. The remainder of this paper is organised as follows. Section 2 describes the design and the hypotheses of our experiment. Our experimental findings are presented in Sect. 3. Section 4 concludes.",19
79.0,3.0,Theory and Decision,08 October 2014,https://link.springer.com/article/10.1007/s11238-014-9469-5,"Honestly, why are you donating money to charity? An experimental study about self-awareness in status-seeking behavior",November 2015,Mitesh Kataria,Tobias Regner,,Unknown,Male,Unknown,Male,"Status-seeking is a prevalent behavior in everyday life.Footnote 1 Yet, it rarely happens that someone proudly claims having bought something in order to signal his or her status, or having donated to charity for this very reason. Presumably, striving for status or positional goods (not only houses, cars, and mobile phones but also education and fame) in order to impress friends is in fact less impressive.Footnote 2 Of course, it is fairly easy to hide one’s true motives to others. The social reputation of explicitly seeking status can be manipulated, but what about concerns for the self-image? Do people have a tendency to believe that every action taken for status is actually pursued for other, nobler reasons? Take a musician, for instance, who might be more driven by the desire to earn a lot of money than by artistic ambitions. Nevertheless, he or she may be eager to convince others as well as the self that the nobler motivation dominates the chosen track of career. In similar fashion, a donor might tend to give more when the donation is visible but unaware about this particular behavior. Consequently, while high-status individuals gain favorable treatment (see, e.g., Ball et al. 2001) status-seeking behavior could be perceived as a negative character trait and people may have a tendency to downplay its role in their decision making. This is what Johansson-Stenman and Martinsson (2006) find in a transportation-related survey. People who are asked which attributes in a car are most important to them stated environmental performance near the top and social status near the bottom. However, when asked about their expectations about the preferences of their neighbors or average compatriots, they give reversed rankings. There are at least two explanations to these observations. One is that people underestimate their own status-seeking behavior, while correctly assessing others’ propensity to engage in status-seeking behavior. The other is that people overestimate others’ status-seeking behavior, while correctly assessing own behavior. Our study picks up this open question and aims to test the relative merit of these explanations. We experimentally investigate (i) whether people engage in social status- seeking behavior, (ii) whether or not people are aware of their status-seeking behavior, and (iii) to what extent they expect others to behave in a status-seeking way.Footnote 3 For this purpose we conduct a real-effort experiment where subjects’ performance translates into donations to a charity.Footnote 4
 Our experimental setup is designed to analyze subjects’ behavior in two subsequent rounds that differ in the visibility of subjects’ performance (private/public feedback within subjects). We also vary the incentivization of the belief elicitation scheme (low/high monetary reward for accurate guesses of performance in the public setting between subjects). Insights from social psychology and economics on cognitive dissonance and self-image concerns (see Festinger 1957; Konow 2000; Bénabou and Tirole 2011) guide our predictions for behavior in this situation. While performing well in comparison to others may be attractive as it provides high status, such status-seeking behavior may not necessarily coincide with one’s behavioral standard or self-image. Cognitive dissonance would result as a consequence of actual behavior deviating from one’s standard of behavior. One way such cognitive dissonance can be resolved/reduced is by forming beliefs about relevant aspects in a self-serving manner. In the context of a dictator game experiment, for instance, the dictator can nurture self-serving beliefs about what is fair. Based on the literature on self-image and self-deception (see Konow 2005, for an overview) we expect people, on average, to systematically underestimate their own propensity to engage in status-seeking behavior. While they benefit from higher status (in expectations, that is, they expect to gain status) by increasing effort, they do not suffer from a conflict between action and self-image if self-deception takes place. Note that while holding such a self-deceiving belief is a motivated act the individual is actually not aware of it (Gur and Sackeim 1979). Following Johansson-Stenman and Martinsson (2006) we also expect subjects to believe that their own concern for status is minor in comparison to the believed status concerns of others. As a secondary element of our experimental design we vary subjects’ monetary reward for accurate estimates of performance in the public setting. While a negligibly low compensation for accuracy of beliefs can be seen as the relevant level of real life status-seeking behavior, our high belief compensation condition introduces a higher cost for deceiving one self. This allows us to test, whether self-deception prevails (in order to maintain a positive self-image) even at a higher cost, or whether subjects are less prone to deceive themselves due to the monetary incentives. Hence, we expect subjects to have a higher self-awareness of their status-seeking behavior when acknowledging it pays off. Our results confirm status-seeking behavior—previously established in various between-subjects studies—in a within-subjects design: subjects’ average performance is significantly higher in the public than in the private feedback round. When asked about their performance in the public round (given feedback about private round performance), subjects tend to underestimate their performance. This belief-behavior gap indicates a lack of self-awareness about status-seeking behavior which could point to self-image concerns (specifically self-deception) as a motivation for subjects’ behavior. High compensation for accurate beliefs about own behavior does not decrease the belief-behavior gap at a statistically significant level. Finally, in contrast to Johansson-Stenman and Martinsson (2006), we found that subjects expected others to be as status-seeking as they are themselves (when compensation for accurate estimations was high they even expected others to be less status-seeking than themselves). The structure of the paper is as follows. In the next section we discuss the relevant theoretical literature and present our hypotheses. In Sect. 3 we describe the experimental design. Results are presented and discussed in Sect. 4. Section 5 concludes.",6
79.0,3.0,Theory and Decision,12 October 2014,https://link.springer.com/article/10.1007/s11238-014-9473-9,Strategic behavior in regressions: an experimental study,November 2015,Javier Perote,Juan Perote-Peña,Marc Vorsatz,,Male,Male,Mix,,
79.0,4.0,Theory and Decision,11 February 2015,https://link.springer.com/article/10.1007/s11238-015-9489-9,Purely subjective extended Bayesian models with Knightian unambiguity,December 2015,Xiangyu Qu,,,Unknown,Unknown,Unknown,Unknown,,
79.0,4.0,Theory and Decision,01 February 2015,https://link.springer.com/article/10.1007/s11238-015-9484-1,Detecting heterogeneous risk attitudes with mixed gambles,December 2015,Luís Santos-Pinto,Adrian Bruhin,Thomas Åstebro,Male,Male,Male,Male,"Decisions made when agents confront risky alternatives are conventionally explained by expected utility (EU) theory. However, evidence from the field and the lab shows that, on average, subjects’ behavior deviates from EU predictions (see Schoemaker 1982; Starmer 2000 for reviews). Furthermore, studies looking at individual decisions under risk like Lattimore et al. (1992) and Hey and Orme (1994) found heterogeneity in behavior. These studies typically found that some subjects behave in line with EU, but most do not. Recent studies that use finite mixture models to take the presence of EU and non-EU types into account report similar results (Bruhin et al. 2010; Conte et al. 2011).Footnote 1
 Ignoring this behavioral heterogeneity may lead to biased conclusions. For instance, studies that attempt to discriminate between alternative theories of non-EU behavior assuming a representative decision maker may suffer from such biased conclusions. Failure to acknowledge that a significant proportion of subjects may behave according to EU leads to confounded parameter estimates that do not correctly reflect the non-EU subjects’ behavior. Furthermore, acknowledging behavioral heterogeneity may also be important for field and lab experiments that are increasingly being used in a diversity of fields (Levitt and List 2007; Falk and Heckman 2009). Rather than assuming a representative decision maker—who is typically postulated to act according to EU— experimental researchers would benefit from being able to identify in their samples who acts according to EU and who does not. Accounting for this heterogeneity may prove to be important to explain what may seem to be odd patterns of choices. We propose a method that allows the experimenter to economically elicit information about risk preferences, while accounting for the presence of both EU and non-EU subjects. Due to its simplicity, our procedure can be easily used for characterizing EU and non-EU subjects in field or lab experiments, even when elicitation of risk preferences is not the main concern. The proposed method consists of a lottery choice task in which subjects accept or reject a series of simple two-outcome mixed gambles, i.e., gambles involving a gain and a loss. The task has a clear-cut reference point, uses 30 decisions per subject, and applies monetary incentives in a symmetric way across the gain and loss domains. The use of mixed gambles yields data that are rich enough to control for heterogeneity in risk attitudes and to discriminate between reference-dependent and non-reference-dependent models of choice under risk.Footnote 2
 We use finite mixture models to simultaneously segregate EU from non-EU types and compare the descriptive power of the different models that depart from EU maximization. Finite mixture models endogenously classify subjects making noisy decisions into a pre-defined number of distinct preference types. They require substantially less parameters than estimations at the individual level but are still able to account for the most important aspect of heterogeneity, namely, the existence of such distinct preference types. Therefore, mixture models represent a neat compromise between parsimony and flexibility. These models are relatively new in decision theory but have previously been used to uncover different types of behavior in complex decision situations (El-Gamal and Grether 1995; Stahl and Wilson 1995; Houser et al. 2004; Houser and Winter 2004). To model non-EU behavior, we focus on four theories: rank-dependent utility (RDU), weighted utility (WU), prospect theory (PT), and salience theory (ST). In RDU, people evaluate utility over final wealth levels, and the only deviation from EU is probability weighting (Quiggin 1982). In WU, people also evaluate utility over final wealth levels, and the only deviation from EU is outcome weighting (Chew 1983). In PT  there are multiple deviations from EU: people evaluate utility relative to a reference point, they can exhibit diminishing sensitivity to monetary gains and losses, they may apply different probability weights in the gain and loss domains, and they may be averse to losses (Kahneman and Tversky 1979; Tversky and Kahneman 1992). In ST, people deviate from EU in similar ways as in PT but overweigh the probabilities of salient payoffs (Bordalo et al. 2012).Footnote 3
 Our main results are as follows. First, individual behavior is heterogeneous with roughly 50 % of the subjects behaving as expected utility (EU) maximizers and 50 % as non-EU maximizers. There is a clean segregation into types, i.e., almost all subjects are unambiguously classified either as EU or non-EU. Second, models where the non-EU types have reference-dependent preferences, i.e., they derive utility from gains and losses relative to a reference point, perform better than those where non-EU types derive utility from final outcomes. Third, the individual classification into EU and non-EU types remains the same regardless of whether the non-EU types are specified by PT or ST. Fourth, models that allow for domain-specific decision weights outperform models which constrain the decision weights’ pattern to be the same for gains and losses. Fifth, there is no evidence for loss aversion. The paper proceeds as follows. Section 2 describes the experimental design. Section 3 introduces the different theories for non-EU behavior. Section 4 explains our estimation strategy. Section 5 presents the results. Finally, Sect. 6 concludes the paper.",10
79.0,4.0,Theory and Decision,06 February 2015,https://link.springer.com/article/10.1007/s11238-015-9485-0,Sub-models for interactive unawareness,December 2015,Simon Grant,J. Jude Kline,John Quiggin,Male,Unknown,Male,Male,"An innovative approach that allows for unawareness in interactive settings has been proposed by Heifetz et al. (2006), hereafter HMS. They follow the event-based approach of Aumann (1976), Aumann (1999).Footnote 1 A logic-based approach to capture awareness and unawareness was developed earlier by Fagin and Halpern (1988). Subsequently, both Heifetz et al. (2008) and Halpern and Rego (2008) have shown that logic-based approaches to unawareness correspond to the HMS event-based approach. While the event-based approach of HMS builds on that of Aumann, an important difference is that Aumann presumed in his set-up that the model is “ commonly known” by all the agents. The “ commonly known” appears here in (scare) quotes to emphasize that it refers to a claim about knowledge of the model as distinct from knowledge of an event which is described within the model. A natural question is whether or not an HMS model can be “ commonly known,” or even “ fully known” by one agent. If it is “ fully known” by an agent who is modeled as being unaware of some aspects, then the agent might arguably, by inspecting his model, become aware of those aspects. A related difficulty arises from the structure of the agent’s representation of the model. If this representation lacks structural features of a full model, then the agent can infer that she is unaware of aspects of the model of which other agents might be aware, even without any understanding of what those aspects might be. This creates some tension between the unawareness the model intends to capture and the possible discovery by an agent of her unawareness via her reasoning within the model. In this paper, we resolve this tension by proposing a notion of a sub-model for each agent at each state. We argue that we can interpret each agent as being fully cognizant of her sub-model without compromising any agent’s awareness level. That is, even if an agent fully explores her sub-model, she will never become aware of any aspect outside the awareness level she is prescribed within the (grand) model. Moreover, the agent’s sub-model inherits all structural features of the full model that might be used to make inferences about the agent’s own awareness. HMS imposed a set of conditions on the possibility correspondences of the agents. We strip down the model to its bare essentials for an analysis of sub-models. We then use the sub-models to motivate the HMS conditions on the possibility correspondences. The paper proceeds as follows. In Section 2, we present a generalized version of the HMS framework with minimal conditions on possibility correspondences and define the concept of a sub-model. We show that each sub-model is itself a model. In Section 3, we give necessary and sufficient conditions for HMS knowledge of an event to be an event. In Section 4, we use sub-models to motivate the full set of HMS conditions on possibility correspondences. In Section 5, we re-visit the Holmes and Watson Example of Galanis (2013) to explore our results on sub-models. We conclude in Section 6. Proofs appear in the appendix.",1
79.0,4.0,Theory and Decision,10 February 2015,https://link.springer.com/article/10.1007/s11238-015-9487-y,Two preference metrics provide settings for the study of properties of binary relations,December 2015,Vicki Knoblauch,,,Female,Unknown,Unknown,Female,"How likely is it that an individual’s or a group’s preferences are transitive? Questions like this about properties of preferences often have context-dependent answers. For the sample question stated, when a consumer forms preferences over three new cars that differ only in color, transitivity seems likely. On the other hand, when a committee forms preferences over a slate of candidates via head-to-head voting, violations of transitivity can occur even when each voter’s preferences are transitive, complete, and antisymmetric. 
Klaska (1997) provided a context-free answer to the sample question by proving that the number of transitive binary relations on a set of n alternatives is \(2^{f(n)}\) where \(f(n)\) is asymptotically \(n^{2}/4\). Since the number of binary relations on a set of n alternatives is \(2^{(n^{2})}\), we can conclude that transitivity is rare. Klaska’s approach and ours in what follows are context free in that they are free of social context; neither is concerned with whether preferences are determined by an individual, by head-to-head majority voting, by unanimous consent of a committee, etc. However, the conclusion drawn using Klaska’s result does assume implicitly that all ordered pairs of alternatives are equally important. Some starting point is always needed. In what follows, we will provide two settings for the context-free study of properties of binary relations on infinite  sets. First, starting with a set of alternatives \(X\) and a finite measure Footnote 1 on a \(\sigma \)-algebra over \(X\times X\) (for example, Lebesgue measure on \([0,1]\times [0,1])\) we define the symmetric difference metric on the collection of measurable binary relations on X (actually, on equivalence classes of measurable binary relations). This allows us to give topological answers to questions like our sample question concerning transitivity. In a short introductory study (Knoblauch 2014) we applied this construction in the simplest case, \(X\) countably infinite with a simple measure on \(X\times X\), a case that avoids the chief difficulties, one of which is the necessity to work with equivalence classes of binary relations. Second, starting with a set of alternatives \(X\) and a metric on \(X\) (for example, Euclidean distance on \([0,1]\)), we define the Hausdorff metric on the collection of binary relations on \(X\) (again, actually on equivalence classes of binary relations). This provides a second collection of topological answers to questions about properties of preferences, which we can use to test the robustness of the answers provided by the first construction. Footnote 2
 It will be seen in Sect. 2 that, roughly speaking, under the symmetric difference metric, the distance between two binary relations \(R\) and \(S\) is the number of disagreements (more accurately, the measure of the disagreement set \((R\backslash S)\cup (S\backslash R))\), while under the Hausdorff metric, the distance between two binary relations is the magnitude of the greatest disagreement, that is, the greatest distance from an ordered pair in one binary relation to the other binary relation. We will also see that when we explore the question of whether certain properties of binary relations—completeness, transitivity, symmetry, asymmetry, antisymmetry, and linear orderability—occur commonly or rarely, there is some agreement and some disagreement between conclusions drawn using the symmetric difference metric and those reached using the Hausdorff metric. 
Kemeny and Snell (1962) defined the symmetric difference metric on the collection of binary relations on a finite  set. The so-called Kemeny–Snell distance has many applications, including image analysis (Luo et al. 2002) and information retrieval systems (Yao 1995). Our is not the first study to define a metric on a collection of preference profiles on an infinite set . Chichilnisky (1980) defined a metric on smooth  preference profiles on a manifold and applied the resulting metric space to the study of social choice (see Mehta 1997, for an overview of topological methods in social choice theory). We take a more general approach by not assuming smoothness, which allows us to address more basic questions. A study previous to ours concerned itself with the rarity of properties of binary relations. Balasko and Crès (1997) showed that Condorcet cycles are rare in preferences over finite  sets aggregated by a super majority rule (53 % or greater). The paper is organized as follows. Section 2 contains definitions. Section 3 compares continuity and measurability of preferences. In Sects. 4 and 5 properties of binary relations are studied using the symmetric difference metric first in a general setting and then for binary relations on \([0,1]\). It is shown that certain collections of binary relations have countable dense complements and are contractible. In Sect. 6 the analysis of the previous two sections is repeated using the Hausdorff metric in place of the symmetric difference metric. In Sect. 7, the results of the two approaches are compared.",1
79.0,4.0,Theory and Decision,17 February 2015,https://link.springer.com/article/10.1007/s11238-015-9488-x,An experimental test of a search model under ambiguity,December 2015,Takao Asano,Hiroko Okudaira,Masaru Sasaki,Male,Female,Male,Mix,,
79.0,4.0,Theory and Decision,17 March 2015,https://link.springer.com/article/10.1007/s11238-015-9492-1,An experiment on case-based decision making,December 2015,Brit Grosskopf,Rajiv Sarin,Elizabeth Watson,Female,Male,Female,Mix,,
79.0,4.0,Theory and Decision,04 February 2015,https://link.springer.com/article/10.1007/s11238-015-9483-2,An experimental study on the effect of ambiguity in a coordination game,December 2015,David Kelsey,Sara le Roux,,Male,Female,Unknown,Mix,,
79.0,4.0,Theory and Decision,12 February 2015,https://link.springer.com/article/10.1007/s11238-015-9486-z,Statistical inference for measures of predictive success,December 2015,Thomas Demuynck,,,Male,Unknown,Unknown,Male,"Given a behavioural model and an outcome space of possible observations, Selten (1991) distinguishes between three types of theories. A point theory gives a single element of the outcome space and predicts this point as the central tendency of the observations. A distribution theory gives a probability distribution over the outcome space and predicts that observations are independently drawn according to this distribution. Finally, an area theory only predicts that the observed outcomes should lie in a certain subset of the outcome space. For example, a distribution theory could predict that some variable of interest is uniformly distributed on the unit interval. A point theory, on the other hand, would predict that the mean (or median) of the observations is equal to 0.5. Finally, an area theory would predict that the observations lie in the interval \([0,1]\). Given this classification, a distribution theory is more informative than either a point theory or an area theory in the sense that if we know the observations to be uniformly distributed, we also know their central tendency (mean or median) and their area (support). Many applications in experimental and revealed preference settings fall into the class of area theories. With respect to these theories, models are often evaluated on the basis of two metrics: the hit rate and the area. The hit rate gives the percentage of all observations that fall within the predicted subset of the outcome space. A high hit rate implies that many subjects have made choices that are consistent with the model’s predictions. The hit rate, however, only captures one dimension of the model’s performance. In general, the hit rate of a model will be higher if the model becomes less permissive (i.e. the model imposes weaker restrictions on the observed behaviour). Therefore, for an area theory to be meaningful it is desirable that the empirical test is sufficiently strong. The permissiveness can be measured by the ‘area’ of the test, which gives the relative size of the predicted subset compared to the set of all possible outcomes.Footnote 1
 Generally, a favourable hit rate, for a specific behavioural model, provides convincing support for the model only if the associated area is sufficiently small. In practice, however, the two measures are almost always positively correlated, which in fact makes it interesting to define a summarizing measure that combines the two measures of empirical performance into a single metric, a so called measure of predictive success. Selten (1991) argues in favour of the functional specification that determines the predictive success as the difference between the hit rate and the area: This measure of predictive success is frequently used experimental studiesFootnote 2 and has recently been advocated for use with revealed preference tests by Beatty and Crawford (2011).Footnote 3 In revealed preference studies, the area is usually quantified as one minus the Bronars (1987) power, which gives the probability that a randomly generated datasets (obtained from a uniform distribution on the budget hyperplanes) will fail the revealed preference test. Different area theories (and revealed preference models) can be evaluated on the basis of their predictive success, and models with higher predictive success can be seen as having a better empirical fit. However, when comparing the predictive success between two models, it is not at all obvious how big the difference in predictive success needs to be in order to be ‘significant’. The literature dealing with predictive success measures is silent on this point. The main reason for this is that the theory underlying the predictive success measure is not a stochastic theory: the observations are either inside or outside the predicted set (see Hey (1998) for a discussion). However, by considering the space of all possible observed behaviour as the relevant population, we show that it is nevertheless possible to conduct valid statistical inference. Our paper uses elementary large sample theory to construct asymptotically valid confidence intervals for various predictive success measures. In this way it becomes possible to construct asymptotic valid hypothesis tests to verify whether the predictive success of a model is larger than some benchmark threshold (e.g. zero) or to compare the predictive success between different opposing models. In the next section, we set out the framework and derive the statistical results. Section 3 contains an empirical illustration of our findings that compares the predictive success of different revealed preference tests for models of intertemporal decision making.",11
80.0,1.0,Theory and Decision,19 April 2015,https://link.springer.com/article/10.1007/s11238-015-9495-y,Predicting human cooperation in the Prisoner’s Dilemma using case-based decision theory,January 2016,Todd Guilfoos,Andreas Duus Pape,,Male,Male,Unknown,Male,"In this paper, we use Case-based decision theory (Gilboa and Schmeidler 1995) to explain experimental data of human behavior in the repeated Prisoner’s Dilemma game. We find that the aggregate dynamics of cooperation are predicted by this theory. We fit the parameters of the model to data and establish that all parameters are statistically significant. We establish this fact by comparing experimental data collected by Camera and Casari (2009) against simulated data generated by a computer program called the Case-based Software Agent (CBSA). CBSA was introduced in Pape and Kurtz (2013), who show that CBSA (and therefore Case-based decision theory) explains individual human behavior in a series of classification learning experiments from Psychology starting with Shepard et al. (1961). Here we show that CBSA can explain human group behavior in a setting that is dynamic and strategic. Case-based decision theory is a mathematical model of choice under uncertainty which has the following primitives: A set of problems or circumstances that the agent faces; a set of actions that the agent can choose in response to these problems; and a set of results which occur when an action is applied to a problem. Together, a problem, action, and result triplet is called a case, and can be thought of as one complete learning experience. The agent has a finite set of cases, called a memory, which it consults when making new decisions. The Case-based Software Agent is a software agent, i.e., “an encapsulated piece of software that includes data together with behavioral methods that act on these data (Tesfatsion 2006).” CBSA computes choice data consistent with an instance of CBDT for an arbitrary choice problem or game, provided that the problem is well defined and sufficiently bounded. We analyze data from an experiment by Camera and Casari (2009), in which study subjects are grouped into small ‘economies’ to play the repeated Prisoner’s Dilemma. The purpose of their experiment is to vary the level of information available to players about each other and measure the effect on cooperation. For example, in one treatment, the players are supplied unique identifiers for their opponents, so they know when they encounter the same opponent again. Because CBDT encodes the agent’s information about the current choice directly (in the aforementioned “problem” variable), this is a particularly appropriate experiment to test with CBSA. We compare simulated data to real data by measuring the mean squared difference in probability of cooperation over time. Like regression analysis, we then search the space of free parameter values for those that provide the best fit (minimizing mean squared error).Footnote 1 Moreover, we establish the precision by which we are able to estimate these parameters by bootstrapping standard errors, which is a first for agent-based models. We are able to establish four key facts about CBDT and its relationship with human choice behavior in Camera and Casari’s Prisoner’s Dilemma experiment. We find The choice behavior of this software agent (and therefore Case-based decision theory) correctly predicts the empirically observed trajectory of average cooperation rates over time across three different treatments. This shows that CBDT can predict human behavior in a strategic and dynamic setting. The choice behavior implied by CBSA is a closer fit to the empirical data than the best-fitting Probit model (from Camera and Casari’s paper), and CBSA has only a fifth as many parameters to fit to data. This is a vote in favor of CBSA as a useful empirical description of human behavior in the repeated Prisoner’s Dilemma and is a novel result in the literature. The best-fitting CBSA parameters suggest humans aspire to a payoff value above the mutual defection payoff but below the mutual cooperation payoff, which suggests they hope, but are not confident, that cooperation can be achieved. In principle, the best-fitting aspiration values could have fallen into the ‘unreasonable’ range: namely greater than the best or lower than the worst possible payoff. The fact that this did not happen serves as an specification test of CBSA.Footnote 2
 Circumstances with more details are easier to recall. The evidence is that our best-fitting level of recall probability increases as the experimental treatment varies as to share more information with the agents. These findings are useful in understanding the behavior of human subjects as well as developing a framework in which we can predict human behavior. For example, the infinitely iterated Prisoner’s Dilemma can sustain cooperation when sufficiently patient agents employ the ‘grim’ strategy, defecting forever if their partner defects, but this strategy does not seem to be played by human subjects. This paper can be thought of as part of an effort to find alternative, empirically valid explanations of decision-making in this strategic context. Below, we first review the relevant literature in decision theory, game theory, and the empirical study of the Prisoner’s Dilemma (Sect. 2). Second, we describe the experiment of Camera and Casari (2009) and explain how we simulate this experiment it in the Case-based Software Agent framework, which implements Case-based decision theory (Sect. 3). Third, we describe our statistical method of finding the best-fitting parameters of CBSA to match the human data, including how we bootstrap standard errors of our parameter estimates (Sect. 4). Fourth, we present and discuss our empirical results (Sect. 5), and, in Sect. 6, we discuss the implications of these results for case-based decision theory. In Sect. 7, we conclude.",10
80.0,1.0,Theory and Decision,17 July 2015,https://link.springer.com/article/10.1007/s11238-014-9422-7,Computational complexity in the design of voting rules,January 2016,Koji Takamiya,Akira Tanaka,,Male,,Unknown,Mix,,
80.0,1.0,Theory and Decision,25 July 2015,https://link.springer.com/article/10.1007/s11238-014-9478-4,Circulant games,January 2016,Ɖura-Georg Granić,Johannes Kern,,Unknown,Male,Unknown,Male,"Games with cyclical structures are ubiquitous in game theory. Simple examples like Matching Pennies and Rock-Paper-Scissors are routinely used to illustrate the concepts of mixed strategies and mixed strategy Nash equilibria in any introductory class to game theory. Beyond their pedagogical value, these simple examples have a wide range of application in game theory. Evolutionary game theory is one prominent example and, e.g., the mating strategies of the common side-blotched lizard have been shown to follow a Rock-Paper-Scissors pattern (Sinervo and Lively 1996). A cyclical game structure can be captured by circulant payoff matrices, in which each row vector is rotated by one element relative to the preceding row vector (Hofbauer et al. 1980; Diekmann and van Gils 2009). Games with circulant payoff matrices have been studied extensively in the literature on evolutionary game theory (Hofbauer and Sigmund 1998) and population dynamics (Hofbauer et al. 1980; Diekmann and van Gils 2009). Circulant payoff matrices also underly certain classes of coordination games, starting with matching games, that have been studied in the literature on symmetries and focal points (Casajus 2000; Janssen 2001).Footnote 1
 The class of games we study here is important for at least two fields of applications. First, the analysis of the convergence properties of various evolutionary dynamics for cyclical game structures has often focused on uniformly mixed strategies. Games in which this strategy profile is the unique equilibrium constitute important examples of convergence failure (see, e.g., Sandholm 2010, Chap. 9.2.1, pp. 327–330). Still, many games with a cyclical structure have more than one equilibrium, and the non-convergence to one particular equilibrium may not be conclusive for the convergence properties of the whole system. Second, matching games and more general coordination games constitute an archetypal framework to analyze features external to the games’ formal structure. The cyclical game structure provides a framework where strategies cannot be differentiated according to differences in payoffs. Yet, matching games are just one particular representation of such symmetric frameworks and many different, equally appropriate cyclical game structures may exist (see, e.g., Alós-Ferrer and Kuzmics 2013). A rigorous characterization of the set of Nash equilibria of cyclical game structures in general is still missing. The aim of this paper is to bridge these gaps and provides a more general analysis of games with a cyclical structure. More precisely, we investigate a class of finite two-player normal-form \(n \times n\) games we coin circulant games, in which the players’ payoff matrices are circulant. We also require that the first row of each matrix is ordered. This approach allows us to integrate classical examples from Game Theory into one single class of games. Well-known games such as the ones mentioned above, as well as subclasses of common interest and coordination games (including matching games) belong to the class of circulant games. Our results shed new light on the common features shared by these games. Our main results identify the exact number of (pure or mixed) Nash equilibria in circulant games. We also obtain necessary and sufficient conditions for the existence of pure strategy Nash equilibria and, in case of non-existence, for the uniqueness of the uniformly mixed Nash equilibrium (a profile which we show to be a Nash equilibrium for all circulant games). As a consequence of our main results, we obtain that the maximal number of Nash equilibria in these games is exactly \(2^n-1\). The number of pure strategy Nash equilibria is either 0, 1, 2, or n. Further, we are also able to characterize the structure of the set of mixed Nash equilibria. The best response correspondences induce an equivalence relation on each player’s set of pure strategies. In any Nash equilibrium, all strategies within one equivalence class are either played with strictly positive or with zero probability. We show how to derive the equivalence classes, allowing for a characterization of the support of all Nash equilibrium strategies. It is worth noting that all our results are invariant to strictly monotone transformations of each player’s payoff function. Hence, they constitute ordinal properties (see Durieu et al. 2008, and the references therein for a discussion on the importance of ordinal properties in game-theoretic analysis). Our results also contribute to the literature on the number of Nash equilibria in finite two-player normal-form \(n\times n\) games. Provided that such a game is non-degenerate the number of Nash equilibria is finite and odd (see, e.g., Shapley 1974). Quint and Shubik (1997) show that for any odd integer number y between 1 and \(2^n-1\), there exists a game with exactly y Nash equilibria. However, as shown in von Stengel (1997), \(2^n-1\) is not an upper bound on the number of Nash equilibria in such games. New upper bounds on the number of distinct Nash equilibria are established in Keiding (1998) and von Stengel (1999). These results notwithstanding, the initial conjecture in Quint and Shubik (1997) motivated many scholars to investigate the conditions under which \(2^n - 1\) is the tight upper bound on the number of Nash equilibria. For instance, Keiding (1997) and McLennan and Park (1999) prove that generic \(4 \times 4\) two-person games have at most \(2^4-1=15\) Nash equilibria. For the class of coordination games, \(2^n-1\) is also the (tight) upper bound on the number of equilibria (Quint and Shubik 2002). Our results show that this is also true for the class of circulant games. Recently, several other articles have analyzed subclasses of games with a special focus on different notions of cyclicity. Duersch et al. (2012) consider symmetric two-player zero-sum normal-form games and define generalized Rock-Paper-Scissors matrices (gRPS) in terms of best response cycles. In their setting, a game has a pure strategy Nash equilibrium if and only if it is not a gRPS. Bahel (2012) and Bahel and Haller (2013) examine zero-sum games that are based on cyclic preference relations on the set of actions and characterize the set of Nash equilibria. In the former paper, actions are distinguishable, i.e., one specific action is the beginning of the cyclic relation, and there exists a unique Nash equilibrium. In the latter, actions are anonymous, i.e., each action can be seen as the beginning of the cycle without affecting the relation, and depending on the number of actions, the Nash equilibrium is unique or there exists an infinite number of Nash equilibria. The remainder of this paper is structured as follows. Section 2 introduces the class of circulant games. Section 3 states the main results and presents a recipe to characterize the support of all Nash equilibrium strategies for a given circulant game. Section 4 presents generalizations of circulant games, Sect. 5 analyzes the evolutionary stability properties of circulant games, and Sect. 6 concludes. All proofs are relegated to the appendix.",2
80.0,1.0,Theory and Decision,21 April 2015,https://link.springer.com/article/10.1007/s11238-015-9490-3,Comparing attitudes toward time and toward money in experience-based decisions,January 2016,Emmanuel Kemel,Muriel Travers,,Male,Female,Unknown,Mix,,
80.0,1.0,Theory and Decision,03 September 2015,https://link.springer.com/article/10.1007/s11238-014-9480-x,Incremental willingness to pay: a theoretical and empirical exposition,January 2016,Karine Lamiraud,Robert Oxoby,Cam Donaldson,Female,Male,,Mix,,
80.0,1.0,Theory and Decision,01 April 2015,https://link.springer.com/article/10.1007/s11238-015-9493-0,Repeated interactions and endogenous contractual incompleteness,January 2016,Jean Beuve,Claudine Desrieux,,Male,Female,Unknown,Mix,,
80.0,1.0,Theory and Decision,21 March 2015,https://link.springer.com/article/10.1007/s11238-015-9491-2,A note on cancellation axioms for comparative probability,January 2016,Matthew Harrison-Trainor,Wesley H. Holliday,Thomas F. Icard III,Male,Male,Male,Male,,6
80.0,2.0,Theory and Decision,29 May 2015,https://link.springer.com/article/10.1007/s11238-015-9496-x,The value of information and the value of awareness,February 2016,John Quiggin,,,Male,Unknown,Unknown,Male,,17
80.0,2.0,Theory and Decision,24 June 2015,https://link.springer.com/article/10.1007/s11238-015-9504-1,Choice under aggregate uncertainty,February 2016,Nabil I. Al-Najjar,Luciano Pomatto,,Male,Male,Unknown,Male,"An important question facing individuals and economic institutions is how to react to correlation. A recent example is the debate over the role of systemic risk in financial markets. Although the financial positions of individual institutions may appear sound, recent crises make a compelling case for heightened regulatory scrutiny of banks’ exposure to the correlation in these positions.Footnote 1 The role of correlation appears, of course, in other even more basic contexts. In consumer theory, for example, a consumer will likely treat uncorrelated changes in relative prices differently from aggregate shifts in consumption levels.Footnote 2
 A common modeling practice is to assume an additively separable utility: Here, the agent faces \(n\) coordinates, \(s=(s_1, \ldots , s_n)\) is a generic profile, \(v_i\) is the utility derived from the \(i\)th coordinate, and \({1\over n}\) is an innocuous normalization. A lottery \(P\) over profiles is evaluated based on its expected utility \(E_P \, V\). Although widely used for their tractability and appealing foundations, additively separable utilities suffer from a major limitation, namely their insensitivity to correlation.Footnote 3 As noted above, concern about aggregate uncertainty is natural in many settings. It is entirely reasonable for a public authority to treat correlated pandemic risk (such as the reaction to the recent Ebola scare) differently from uncorrelated health incidents. The fact that additively separable utilities cannot distinguish the two is potentially an important limitation. There is, of course, an easy way out: simply replace \(V\) by a general von Neumann–Morgenstern utility function \(U(s_1, \ldots , s_n)\) that is not separable in its \(n\) coordinates. While this approach can succeed in introducing sensitivity to correlation, it is intractable without further structure on \(U\). For example, one would like to answer questions like: What is the impact of increasing \(n\)? Is randomization beneficial? or What are good quantitative measures of the attitude towards correlation? These questions are central to understanding the impact of aggregate risks, yet they can be difficult, if not impossible, to answer with a general utility function. This paper provides a very simply way to identify and measure the sensitivity of economic decisions to aggregate uncertainty. Our starting point is an agent who ranks lotteries based on their expected utilities with respect to a von Neumann–Morgenstern utility \(U\). A tractable model is obtained by requiring, further, that the ranking of deterministic profiles satisfies the conditions in Debreu ’s (1960) classic characterization of separable preferences. His theorem then implies that this ranking has a cardinal representation V, in the sense of (1) above.Footnote 4 We are therefore given two cardinal utilities, U and V, with identical ordinal ranking of profiles. It follows that there must be a (cardinally unique) strictly increasing function \(u\) such that \(U\) has the aggregative utility form \(U(s) = u(V(s)) = u(n^{-1}\sum _i v_i(s_i))\). We interpret \(u\) as reflecting the attitude towards aggregate uncertainty. To justify this claim, note that the function \(U\) captures two conceptually distinct aspects of the problem: (1) uncertainty about which profile will obtain; and (2) the aggregation of \(n\) coordinates into a utility of a profile. These two components are intertwined in a general \(U\). Combining the von Neumann–Morgenstern and Debreu’s cardinal theories is a natural way to disentangle the two. This approach is very simple, making it all the more surprising that, to our knowledge, it has not been taken before. Indifference to correlation obtains if and only if \(u\) is affine, while aversion to aggregate uncertainty corresponds to the strict concavity of \(u\). More generally, with obvious caveats to be discussed below, the standard machinery of utility theory can be used to quantify the attitude towards correlation. We provide a detailed example in the case of the Dixit–Stiglitz CES aggregator. Aggregate uncertainty plays a central role in evaluating public policies, especially as they relate to catastrophic risks. Catastrophes represent, almost by definition, risks that are correlated across individuals (firms, assets, or investments). The increasing interconnectedness of modern economies sharpened the impact of old sources of correlation, such as systemic risk in financial markets, medical treatment uncertainty, and product recalls. Political and technological changes also created new sources of correlation, including climate change risk and global terrorism. Many authors noted the fact that American public opinion reacts differently to these risks compared to more familiar ones like car accidents and house fires.Footnote 5 A large debate on the proper public policy attitude to catastrophic risk centers around the status of the Precautionary Principle, a policy position that has been widely adopted in many laws, international treaties, and government regulations.Footnote 6 This and other related principles that focus on worst-case scenarios have been criticized by Sunstein (2005) and others as incoherent. Aggregative utility can also provide a foundation for randomized decision rules. We consider three instances where randomization arises in the literature. First, Manski (2004, 2011) makes a normative case for such rules when the effectiveness of a treatment is uncertain. He derives the optimality of randomized rules by assuming a utilitarian social planner who uses a non-Bayesian minimax regret criterion. We show, consistently with Manski and Tetenov (2007), that a Bayesian planner with an aggregative utility may strictly prefer to randomize as a way to hedge against aggregate uncertainty. Second, sensitivity to aggregate uncertainty relates to an insight of Schmeidler (1989) and Gilboa and Schmeidler (1989) that individuals may randomize to hedge against unknown probabilities. Our work is related to Halevy and Feltkamp ’s (2005) finding that strict preference for randomization arises when utility depends on the outcomes of multiple correlated urns. We note that the decision maker in Halevy and Feltkamp (2005) has aggregative utility in our sense. Finally, a surprising context where randomization appears is evolutionary dynamics. In Bergstrom (1997), for example, Nature may introduce heterogeneity in preferences when there is aggregate uncertainty. Section 5.4 discusses how randomization by introducing heterogeneity in a population may be interpreted within our analysis. The plan of the paper is as follows: Sect. 2 introduces notation and a motivating example, while Sect.  3 states the main theorems. Section 4 considers asymptotics as \(n\) increases. Finally, Sect. 5 considers public choice questions, fractional allocations, and connects our model to some of the findings in the literatures.",5
80.0,2.0,Theory and Decision,01 May 2015,https://link.springer.com/article/10.1007/s11238-015-9498-8,The Anscombe–Aumann representation and the independence axiom: a reconsideration,February 2016,Abhinash Borah,Christopher Kops,,Unknown,Male,Unknown,Male,"The Anscombe and Aumann (1963) model of subjective expected utility (SEU), following on Savage (1954), is a popular theoretical framework underlying the Bayesian approach in economics and related disciplines. Indeed, the framework within which this model has been presented in Fishburn (1970) has become a workhorse for theoretical models of decision-making under uncertainty.Footnote 1 In this paper, we revisit this model, within Fishburn’s framework, with the goal of providing an alternative behavioral foundation for an SEU representation. The most demanding restriction on behavior in the original axiomatization of SEU within this framework comes in the form of the independence axiom. In particular, independence is required to hold over the domain of all acts, where an act refers to a mapping from an underlying set of uncertain states to objective lotteries over an outcome space. The independence axiom requires that the ranking of any two acts should not change when they are probabilistically mixed with the same third act using identical probability weights. The crucial analytical step of the theory is in laying down how such probabilistic mixtures of acts are defined. In this framework, such mixtures are defined state wise, and the probabilistic mixture of two acts defines an act that, in any state, offers the lottery formed by taking a convex combination of the lotteries under these two acts using the corresponding probabilities of the mixture. Therefore, a mixture here may be viewed as an ex post randomization in relation to the underlying subjective uncertainty. That is, the objective probabilistic mixing occurs after the subjective uncertainty has resolved itself. To understand better why independence is a demanding restriction, observe that the above notion of an ex post randomization may be contrasted with an ex ante view of randomization where the objective mixing takes place before the resolution of the subjective uncertainty, so that a probabilistic mixture of two acts in this alternative formulation is a lottery that gives these two acts with the corresponding probabilities of the mixture. It is in this latter sense of probabilistic mixtures that the independence axiom is more naturally understood, with the interpretation that if two such lotteries offer the same act as prize with some probability, then replacing that act with any other act will not change the ranking of these lotteries. Adopting such an interpretation within the Anscombe–Aumann framework necessitates maintaining a rather strong assumption, referred to as reversal of order in compound lotteries, under which the decision maker is required to be indifferent as to whether the objective randomization takes place before or after the resolution of the subjective uncertainty. It is this property of reversal of order that makes independence a fairly demanding restriction on behavior in the context of the subjective uncertainty in the Anscombe–Aumann model. In this paper, the key innovation in our behavioral foundation is to establish that, for an SEU representation, we need not explicitly assume that preferences satisfy the independence axiom over the domain of all acts. Rather, the substantive implications of independence for an SEU representation may equivalently be derived from less demanding conditions over certain smaller classes of acts that we refer to as uncertainty-free comparable because of their transparent and easy to compare structure with respect to the underlying uncertainty. Specifically, we think of acts as being uncertainty-free comparable (UFC) if either (i) they are constantFootnote 2 or (ii) they are almost identical in the sense that they differ from one another in at most one state. Uncertainty-free comparable (UFC) acts should be relatively simple for a decision maker to compare, precisely because she can rank such acts based on her risk preferences over objective lotteries alone, without having to consider what her perception of and attitude toward subjective uncertainty is. To see this better, note, first, that a constant act is essentially an objective lottery. So, ranking two or more constant acts is equivalent to ranking the corresponding objective lotteries. Second, in ranking acts that are almost identical, the comparison, presumably, comes down to ranking the respective objective lotteries under these acts in the state in which they vary. Therefore, a decision maker should be able to rank UFC acts based on her risk preferences over objective lotteries alone. With reference to only such UFC acts, we introduce two relatively weak axioms that together, in terms of their implications for an SEU representation, are equivalent to independence. The first of these axioms is a weakening of the certainty independence axiom of Gilboa and Schmeidler (1989) and thus a particularly weak version of independence. We refer to it as uncertainty-free certainty independence. It states that if two acts are both uncertainty-free comparable to the same constant act, then mixing these two acts with that constant act using identical probability weights should not change their ranking. Note that for an act to be uncertainty-free comparable to a constant act, it must either itself be constant or differ from that constant act in at most one state. Mixtures of such acts have a very transparent structure, and reversal of order, presumably, is not as demanding a restriction for such mixtures as it is for mixtures involving arbitrary acts. This makes uncertainty-free certainty independence a fairly weak restriction on behavior. Our other new axiom applies to the second class of UFC acts mentioned above, referred to as almost identical acts because they differ from one another in at most one state. This axiom, which we call uncertainty-free translation invariance, formalizes the idea that a decision maker’s assessment of the utility difference between two almost identical acts should be independent of what these acts offer in the states where they are identical and depend only on what they offer in the state where they differ. This axiom is in the nature of a separability axiom and retains the spirit of other such axioms in the literature (most famously, Savage’s sure thing principle) with the message that uncertain prospects should be compared based on what they offer in states where they differ, ignoring what they offer in states where they are the same. However, in keeping with the spirit of our axiomatization, this axiom is less demanding than most such axioms in the sense that it applies only to acts that differ from one another in at most one state. In our representation result, we show that uncertainty-free certainty independence and uncertainty-free translation invariance, along with the standard axioms of weak order, Archimedian–continuity, monotonicity, and non-triviality, suffice to characterize an SEU representation a la Anscombe and Aumann (1963). Given that these last four axioms along with independence characterizes an SEU representation in the standard approach, what our result essentially establishes is that, in the context of the Anscombe–Aumann model, the effective contribution of the independence axiom toward an SEU representation may be decomposed in terms of our two new axioms. The significance of this decomposition comes from the fact that these two axioms operate completely within the domain of UFC acts, thus establishing that, rather than employing the entire set of acts, the substantive implications of independence for an SEU representation may be derived, via our two axioms, from the much smaller classes of UFC acts. This, in turn, facilitates the discussion about the normative grounds for Bayesian decision-making. To elaborate on the last observation, consider Bob who, in the context of the Anscombe–Aumann representation, is trying to convince his student David about the merits of assessing any uncertain prospect based on a (unique) probabilistic assessment of uncertain events and an expected utility evaluation with respect to this probabilistic assessment, in other words, to behave like a Bayesian. Given that the independence axiom over the domain of all acts is the most demanding restriction on behavior that Bob’s argument involves, convincing David of its plausibility by substituting it with our two new axioms on the smaller domain of UFC acts, arguably, simplifies Bob’s pedagogical task. This is particularly so since UFC acts have a transparent and easy to compare structure with respect to the underlying uncertainty. At the same time, our formulation makes it easier for David to figure out whether he wants to behave like a Bayesian or not. In particular, if he is convinced that he satisfies our two new axioms over the smaller domain of UFC acts, then based on our result, he can rest assured that he will indeed want to satisfy independence over the domain of all acts and behave like a Bayesian. Lest we be misunderstood, we want to clarify that we are not making the claim here that because of our alternative foundation for SEU, it is easier, on normative grounds, to adhere to Bayesianism. Rather, the claim simply is that our foundation helps streamline the discussion about whether a decision maker would want to behave like a Bayesian or not in a given environment. We emphasize this point by referring to Ellsberg’s famous urn. Consider Ellsberg’s urn with \(30\) blue (\(B\)) and \(60\) green (\(G\)) or red (\(R\)) balls and bets offered on the color of a randomly drawn ball. Let \(f\) be the act that gives \(100\) if the drawn ball is \(B\) (and nothing otherwise); \(g\) be the act that gives \(100\) if it is \(G\); \(f'\) be the act that gives \(100\) if it is \(B\) or \(R\); and \(g'\) be the act that gives \(100\) if it is \(G\) or \(R\) (Table 1). Observe that the acts \(f'\) and \(f\) are almost identical as they are the same on \(B\) and \(G\) and differ only on \(R\). Uncertainty-free translation invariance requires that a decision maker’s assessment of the utility difference between \(f'\) and \(f\) should, therefore, be independent of what they offer on \(B\) and \(G\) and depend only on what they offer on \(R\). A similar observation applies to her assessment of the utility difference between \(g'\) and \(g\) as these two acts also are the same on \(B\) and \(G\) and differ only on \(R\). Further, on \(R\), \(f'\) and \(g'\) are the same as are \(f\) and \(g\). Accordingly, this axiom demands that the utility difference between \(f'\) and \(f\) should be the same as that between \(g'\) and \(g\). However, for decision makers who are sensitive to ambiguity, it is possible that their assessment of the utility difference between \(g'\) and \(g\) is strictly greater than that between \(f'\) and \(f\). To see this, note that moving from \(f\) to \(f'\) involves a similar “risk advantage” as moving from \(g\) to \(g'\). But whereas the former move increases the DM’s exposure to ambiguity the latter move reduces it. So, overall, the latter move might be preferable, which is also evident from the standard Ellsberg preferences: \(g' \succ f' \succ f \succ g\). Hence, Ellsberg-type preferences, under our axiomatization, are reflected by a violation of uncertainty-free translation invariance. On the other hand, uncertainty-free certainty independence does not rule out such preferences.Footnote 3
 The fact that of our two new axioms, uncertainty-free translation invariance is the one with a greater bite for SEU is also borne out by the fact that, as we will show, it is this axiom again, and not uncertainty-free certainty independence, that is violated in the 50:51 and reflection examples of Machina (2009). These are two other examples of ambiguity-sensitive decisions that have been prominently cited in the recent literature. In that sense, our axiomatization provides a transparent characterization of the exact condition that makes the Anscombe–Aumann model incompatible with some of the leading examples of ambiguity-sensitive decisions provided in the literature. This, we feel, is a strength of our axiomatization as the independence axiom may not always lend itself to such a transparent analysis in this regard.
 We should point out here that the question of providing a behavioral foundation for an SEU representation, within the Anscombe–Aumann setup, with less restrictive assumptions than the full-blown independence axiom has also been addressed in a recent paper by Lehrer and Teper (2015). Their approach, which is not just restricted to SEU but extends more generally to non-SEU assessments of uncertain prospects, is based on the idea of “similarity classes” of acts. To understand their alternative axiomatization of SEU a bit better, restrict the set of acts to random variables. Let \(b_E\) be an act (a bet) that gives \(b\) in event \(E\) and nothing otherwise, \(f\) be any other act, and consider the set of acts \([b_E,f] := \{\alpha b_E + (1-\alpha )f: \alpha \in [0,1]\}\). A set of the type \([b_E,f]\) may be thought of as a similarity class, as acts within this set have a similar structure in terms of how the payments under them vary across states.Footnote 4 The key content of their axiomatization is to show that for an SEU representation, we do not need to assume independence over the domain of all acts but only over such similarity classes of acts. Viewed from this perspective, our axiomatization provides a stark characterization of this idea of deriving the substantive implications of independence for an SEU representation from acts that have a similar structure. After all, UFC acts have a very obvious similar structure. The next section introduces the setup. The subsequent one provides our new behavioral foundation for SEU. A final section relates our two new axioms to some prominent examples of ambiguity averse preferences that are naturally incompatible with SEU.",2
80.0,2.0,Theory and Decision,21 May 2015,https://link.springer.com/article/10.1007/s11238-015-9503-2,"Risk aversion, prudence, and asset allocation: a review and some new developments",February 2016,Michel M. Denuit,Louis Eeckhoudt,,Male,Male,Unknown,Male,"Consider a risk-averse decision maker who has to invest a given initial wealth in two risky assets with possibly correlated performances over a given reference period. The agent is assumed to act in order to maximize the expected utility of terminal wealth. In this paper, we investigate two closely related problems. On the one hand, we further study the conditions on the joint probability distribution of the assets such that all risk-averse decision makers agree to diversify their position, i.e., to invest a positive fraction of their initial wealth in each of the two assets. This first question is referred to as the positive demand problem in the literature. On the other hand, we consider a risk-averse investor holding a given portfolio made of these two risky assets and we derive criteria expressed in terms of the joint probability distribution of the assets and of the underlying portfolio to ensure that the share of an asset is increased at the expense of the other in the portfolio. This second question is referred to as the minimum demand problem in the literature. Let us now briefly summarize the results available in the literature about these two problems. Negative expectation dependence introduced by Wright (1987) is the key concept to solve the positive demand problem. The expectation dependence concept has been extended to higher orders by Li (2011). Denuit et al. (2015) introduced a dual version of this dependence concept, called excess dependence, and provided an application to the positive demand problem, generalizing the results of Wright (1987) to investors’ preferences exhibiting higher-order risk apportionment in the sense of Eeckhoudt and Schlesinger (2006). The positive demand condition for risk-averse investors obtained by Wright (1987) has been extended by Hadar and Seo (1988) to ensure that the proportion of a given asset in the optimal portfolio exceeds a given threshold. This minimum demand condition is closely related to the concept of marginal conditional stochastic dominance (MCSD) introduced by Yitzhaki and Olkin (1991) and Shalit and Yitzhaki (1994) as a condition under which all risk-averse investors agree to increase the share of one risky asset over the other. In this paper, we revisit the minimum demand problem by considering risk-averse investors exhibiting prudence. In the expected utility setting, prudence is defined by the non-negativity of the third derivative of the utility function. This risk attitude was initially justified by reference to the decision of building up precautionary savings in order to better face future income risk (Kimball 1990
Footnote 1). The role of prudence has also been illustrated in other contexts, including self-protection activities (Chiu 2005), optimal audits (Fagart and Sinclair-Desgagné 2007), or decreasing sensitivity to an increase in correlation when the initial wealth increases (Denuit and Rey 2010). Quite surprisingly, although prudence originates in savings problems, this concept does not seem to have been applied to optimal portfolio selection so far. The present paper investigates the role of this additional assumption of prudence in asset allocation. By restricting the class of risk-averse decision makers to the subset of those investors exhibiting prudence, we can weaken the condition imposed by Hadar and Seo (1988) on the joint probability distribution of the assets ensuring that the optimal portfolio comprises at least a given percentage of one of them. In particular, imposing prudence in addition to risk aversion also weakens the positive demand condition in Wright (1987) and the MCSD criterion. This produces new interpretations of the results, extending Denuit et al. (2015) who only considered positive demand problem and provided sufficient conditions (whereas our conditions are both necessary and sufficient). The results obtained both by Wright (1987) and with the MCSD criterion for risk-averse decision makers can be interpreted in terms of covariances between asset returns and payoffs of digital options written on the performances of the reference portfolio comprising the desired proportion of the assets when the expected returns of the risky assets are equal. When one assumes prudence beyond risk aversion, digital options are replaced by European put options written on the reference portfolio. As such puts can theoretically be replicated by means of digital options protecting against weak performances of the reference portfolio, we thus get a weaker condition. As is now well known, prudence is one of the risk attitudes beyond risk aversion and this is related to the notion of higher-order risk apportionment, as defined by Eeckhoudt and Schlesinger (2006). The notion of risk apportionment is a preference for a particular class of lotteries combining sure reductions in wealth and zero-mean risks. These higher-order risk attitudes entail a preference for combining relatively good outcomes with bad ones and can be interpreted as a desire to disaggregate the harms of unavoidable risks and losses. Risk apportionments of orders 2 and 3 correspond to risk aversion and prudence, respectively. Increasing the order of risk apportionment further restricts the class of investors and thus gives weaker conditions on the joint probability distribution of the assets to ensure that the optimal portfolio comprises at least a given percentage of one of them. The remainder of this paper is organized as follows. Section 2 describes the problem investigated in the present paper and gives the solution for risk-averse decision makers, summarizing in a unified way the results available in the literature. The derivation of these existing results from common grounds allows for an extension in Sect. 3 to risk-averse investors exhibiting prudence and in Sect. 4 to investors whose preferences exhibit higher-order risk attitudes. Specifically, an extension of the results obtained for risk-averse and prudent decision makers to investors exhibiting risk apportionment of orders 1 to \(4, 5,\ldots \) is proposed in Sect. 4. The final Sect. 5 discusses the results obtained in this paper. Some technical results are gathered in the appendix to this paper, which contains the derivation of a new expansion formula for expected utility.",2
80.0,2.0,Theory and Decision,28 May 2015,https://link.springer.com/article/10.1007/s11238-015-9502-3,Solution concepts for games with ambiguous payoffs,February 2016,Dorian Beauchêne,,,Male,Unknown,Unknown,Male,"The outcome of games can depend on some random state of the world, such as the result of a coin flip. Such payoff uncertainty may not be objectively measurable by a single probability distribution. In this case, such payoffs are said to be ambiguous, as opposed to risky. In this paper, I investigate the interactions between players’ preferences regarding ambiguity and the equilibrium properties of games. Specifically, I show that allowing ambiguity aversion or ambiguity seeking preferences leads to equilibrium behavior that is qualitatively different than what one could get with Expected Utility players. Notably, as proven by Azrieli and Teper (2011), standard Nash equilibrium may fail to exist when players exhibit ambiguity seeking behavior. Given the properties of Nash equilibrium differ when allowing for non-Expected Utility preferences over the environment, I consider here three different equilibrium solutions present in the literature: Nash equilibrium, Crawford equilibrium in beliefs and pure equilibrium in beliefs. These three solution concepts are conceptually close, and often equivalent as shown in this paper, so that it makes sense to consider them in conjunction. Indeed, they are similar in that equilibrium objects are standard mixed strategies with the usual restrictions that players play best responses and hold accurate beliefs over their opponents’ play. They differ however on how the equilibrium mixed strategies are derived from best responses. Nash equilibrium requires such mixed strategies to be best responses. A Crawford equilibrium in beliefs requires that mixed strategies be a mixture of best responses and not necessarily best responses themselves. A pure equilibrium in beliefs is like a Crawford equilibrium in belief but restricts best responses to pure actions. The principle of indifference states that if a player randomizes between two actions, he must be indifferent between these actions. The reduction principle states that players can disregard uncertainty and base their equilibrium actions on certainty-equivalents. These two principles are verified by Nash equilibrium when players are Expected Utility maximizers. These are central to the computation of Nash equilibrium in standard games and as such, their violation would entail the potential for non-Nash behavior. Furthermore, both have descriptive and normative implications. Failure of the principle of indifference is related to the failure to reduce compound lotteries and can be likened to dynamic inconsistency. Assuming a player randomizes in equilibrium between two actions, it is unclear why he should agree to playing an action he dislikes if it were given by the randomization process. On the other hand, if a player exhibits a strict preference for randomization and possess some kind of commitment device, or plays some version of the game sufficiently many timesFootnote 1, it would seem more appropriate to let the player choose some mixture over his actions, even at the cost of sometimes playing one he does not like. Note that I do not take a side on the desirability of the principle of indifference as an equilibrium property. However, I do contend that, whichever side one is on, the principle of indifference is a central property of equilibrium and that knowing whether it is satisfied or not sheds light on the implications of the various solution concepts. The reduction principle is also related to the failure to reduce compound lotteries. This is related to the order of resolution of uncertainty as perceived by the playersFootnote 2. By applying the various equilibrium concepts, players act as if contextual ambiguity were resolved before strategic uncertainty (consistent with an Anscombe and Aumann (1963) framework). By computing the equilibrium for a single-state game where payoffs are given by the certainty-equivalents, players would act as if contextual ambiguity is resolved after the strategic uncertainty. I show in this paper that the principle of indifference holds for all games and all solution concepts when the players are all ambiguity seeking. When this is not the case, only the pure equilibrium in beliefs satisfies the principle of indifference for all games. The reduction principle on the other hand holds for all solution concepts if and only if the players are all Expected Utility maximizers. I further show that both Crawford and pure equilibria in beliefs exist regardless of the players ambiguity preferences whereas Nash equilibrium exists only if the players are all ambiguity averse. Finally, I show that Crawford and pure equilibria in beliefs coincide if and only if all the players are ambiguity seeking while Nash equilibrium and Crawford equilibrium in beliefs coincide if and only if all the players are ambiguity averse. Together, these results suggest that, when considering non ambiguity averse players, the more relevant solution concept is either equilibrium in beliefs. On the other hand, considering ambiguity averse players, the choice of concept should depend on one’s position regarding the desirability of the principle of indifference. The necessary violation of the reduction principle, for any solution concept when players are not Expected Utility maximizers, suggests that incorporating ambiguity preferences in economic applications can potentially lead to results that are not explainable with the Expected Utility paradigm. Note that this paper introduces ambiguity directly on the states of the world but solves the strategic uncertainty inherent to games using fairly standard game theoretic tools. Thus, this paper’s approach is distinct from that of Dow and Werlang (1994), Eichberger and Kelsey (2000), Jungbauer and Ritzberger (2011) or Bade (2011) which all introduce uncertainty in the players strategies/beliefs. Dow and Werlang (1994) considers an equilibrium in beliefs type of equilibrium in which ambiguity averse players possess non-additive beliefs over other players actions. Eichberger and Kelsey (2000) extends Dow and Werlang (1994) analysis to ambiguity-seeking players. Bade (2011) focuses on games where players would actively want to use ambiguous strategies. In these papers, and their descendants, the equilibrium object is not a mixed strategy. This paper does not consider these definitions of equilibrium and sticks to standard mixed strategies as equilibrium object. Finally, Jungbauer and Ritzberger (2011) does consider ambiguity in the environment in which games are played, in addition to the strategic component of games. This paper is therefore more conservative and takes the view that lotteries that emerge from equilibrium play (from whichever definition is considered more suitable) can be considered as objectively known. Closer in spirit, Crawford (1990) showed, albeit without referring to ambiguous payoffs, that Nash equilibrium could fail to exist when the players’ functionals were quasi-convex and put forward the Crawford equilibrium in beliefs as a fix, showing its existence in the more general case. I restate these results in a framework that gives an explicit meaning and economic interpretation to the non-linearity of functionals. Kajii and Ui (2005), focusing on players with MaxMin Expected Utility functionals à la Gilboa and Schmeidler (1989), introduced the notion of pure equilibrium in beliefs and showed its existence in games with incomplete information. I extend this result for general functionals. Azrieli and Teper (2011) show that quasi-concavity of the functionals is both a sufficient and necessary condition for Nash equilibrium to exist for all games. Dekel et al. (1991) focused on the failure of the principle of indifference of Nash equilibrium for quasi-concave functionals. To the best of my knowledge, there has been no study of the reduction principle for games with ambiguous payoffs. The failure argument is however close in spirit to the “immunization against strategic uncertainty” of Riedel and Sass (2014). Note that several of the results of this paper have close counterparts in the existing literature. However, this paper generalizes these results and “fills the gaps” left. Finally, Beauchêne (2014) presents a survey of the literature on games and ambiguity. Section 2 focuses on the decision theoretic aspect of the paper, defining the ambiguous environment and formulating the assumptions made on the players’ functionals. Section 3 delves into the game theoretic side and defines the three equilibrium notions as well as the principle of indifference and reduction principle which are studied. The following two sections bridge the gap between both aspects of the paper, Sect. 4 states the general results with examples for each result. Section 5 concludes.",6
80.0,2.0,Theory and Decision,12 May 2015,https://link.springer.com/article/10.1007/s11238-015-9500-5,A strategic approach for the discounted Shapley values,February 2016,Emilio Calvo,Esther Gutiérrez-López,,Male,Female,Unknown,Mix,,
80.0,2.0,Theory and Decision,29 May 2015,https://link.springer.com/article/10.1007/s11238-015-9499-7,"Asymmetric dominance, deferral, and status quo bias in a behavioral model of choice",February 2016,Georgios Gerasimou,,,Male,Unknown,Unknown,Male,"Decision problems may be of two types, depending on the nature of their status quo. The first type includes problems where the status quo enters as an option that is qualitatively similar to all others. A consumer would be facing a decision problem of this type if he was endowed with insurance policy A and was confronted with the choice between keeping A or giving it up for one of policies B or C. The second type includes decision problems where the status quo is not of the same nature as the other feasible alternatives. In a modification of the previous example, the consumer would be facing a problem of this type if he was uninsured and policies A, B, and C were available to him. Maintaining the status quo in problems of the former type translates into choosing a physical alternative. By contrast, status quo maintenance in problems of the latter type translates into choosing no physical alternative. In this paper, the former type of problems will be referred to as being associated with a specific status quo and the latter as ones with a non-specific status quo. 
Choice deferral refers to the observation that, when faced with decision problems with a non-specific status quo, people sometimes choose none of the options available to them, or equivalently choose their non-specific status quo. Some reasons for such behavior include undesirability of the feasible alternatives, difficulties in making comparisons and finding a dominant option that are caused by decision conflict, and choice overload (Zakay 1984; Tversky and Shafir 1992; Greenleaf and Lehmann 1995; Dhar 1997; Luce 1998; Iyengar and Lepper 2000; Dhar and Simonson 2003). Importantly, when decision conflict is the reason why the agent defers, it is understood that he does so despite the fact that all alternatives are desirable in the sense that each of them would be chosen had it been the only feasible option. On the other hand, status quo bias or default bias refers to the observation that, when faced with decision problems with a specific status quo, individuals more commonly choose a feasible option when this is actually their status quo than when it is not (Knetsch and Sinden 1984; Knetsch 1989; Samuelson and Zeckhauser 1988). Behavior that exhibits choice deferral or status quo bias is sometimes called avoidant behavior (Anderson 2003). This paper uses the expanded choice domain introduced in Masatlioglu and Ok (2005) and extends the model of partially dominant choice proposed in Gerasimou (2015) in the direction of allowing for both types of avoidant behavior. This model is the first in the literature to feature a decision maker whose choices are guided by an acyclic and incomplete preference relation that is independent of the decision problem’s type and menu of feasible alternatives. The way this preference relation is used by the agent is similar across the two types of decision problems. Specifically, in problems with a non-specific status quo, the decision maker chooses an option if and only if it dominates (i.e., is preferred to) some feasible alternative and is not dominated by any such alternative. The agent defers choice when and only when such a feasible option cannot be found. In problems with a specific status quo, on the other hand, he chooses an option other than the status quo if and only if the option is undominated and dominates the status quo, while the latter is chosen if the above is not the case for any alternative. The model explains choice deferral that is due to incomparability/decision conflict. Moreover, in contrast to existing models, it also formalizes directly, by means of a single preference relation, the explanation of status quo bias that is based on preference incompleteness (Bewley 2002; Mandler 2004; Masatlioglu and Ok 2005). As such, it relies on neither the use of multi-functions (Masatlioglu and Ok 2005) nor on multiple, reference-dependent preference relations (Tversky and Kahneman 1991; Bleichrodt 2007; Apesteguia and Ballester 2009). In addition, the partial dominance structure of the model allows for a simple explanation of findings reported in Dhar and Simonson (2003) suggesting a strengthening of the attraction effect (Huber et al. 1982) and a weakening of the compromise effect (Simonson 1989) when deferral is permissible. Finally, this model appears to be the first in the literature to make a clear theoretical distinction between choice deferral and status quo bias, and also to suggest a theoretical connection between the potential drivers of the attraction effect and of status quo bias or avoidant behavior more generally. The next section introduces the model, which is referred to as the extended partial dominance procedure. The section following it introduces and discusses the axioms that characterize this choice procedure and identifies the way in which utility maximization is included as a special case by the model whenever preferences are complete. Section 4 discusses the model’s predictions in relation to the experimental evidence, provides a review of the related literature, and concludes with some methodological remarks about two approaches in the modeling of choice deferral.",18
80.0,2.0,Theory and Decision,19 April 2015,https://link.springer.com/article/10.1007/s11238-015-9494-z,Do we agree? Measuring the cohesiveness of preferences,February 2016,Jorge Alcalde-Unzu,Marc Vorsatz,,Male,Male,Unknown,Male,"Group cohesiveness can be defined as the degree to which the members of the group stick together, both emotional and task related. If we think, for example, of an organizational unit at the workplace, group cohesiveness plays a central role not only because it leads to a better performance (i.e., there is a higher likelihood that a goal is met and/or the quality of the output is higher), but also because it decreases the probability of internal conflicts. 
Eisenberg (2007) reviewed various aspects that affect group cohesiveness; the most important ones are the closeness of the members of the group (a higher degree of closeness leads to more trust), the group size (social norms are easier to maintain in smaller groups), the entry difficulty (forming part of an exclusive group has a high emotional value), the group success, and the existence of external competition and threats (more competition makes the individuals focus more on the joint goals). Among these factors, the closeness of the group members—one distinguishes here between external variables such as age, gender, or religion and internal variables such as values and attitudes—is often regarded a key variable. In group decision problems (e.g., the neighbors of a community have to decide on where to build a new club facility, the members of a scientific society have to elect a new board of representatives, or the members of the human resources section of firms have to decide which person to hire as a new specialist), the internal variables are often summarized by the preferences of the group members on the set of objects. Most of the literature on social choice has focused on how to aggregate preferences in these type of situations, and only little effort has been made to evaluate the potential conflict inherent in the underlying preference structure. To address this question, we study in this paper the closeness of the preferences of the individuals that belong to a group or society. For simplicity, we will call this closeness of preferences in a preference profile the cohesiveness. 
Bosch (2006) was the first to define a cohesiveness measure in group decision situations as a function that assigns to each profile of individual strict preferences a number from the unit interval (he uses the term consensus measure). His study also provides axiomatizations of some simple measures. Then, García-Lapresta and Pérez-Román (2010) introduced several properties and studied whether some more elaborated measures satisfy these conditions, and Alcalde-Unzu and Vorsatz (2013) provided closed axiomatic characterizations of a new family of cohesiveness measures. Recently, García-Lapresta and Pérez-Román (2011) and Alcantud et al. (2013) have extended some of the existing results to models with weak orders (complete preorders) or approval profiles (individuals decide whether or not to approve an object). One intuitive way to construct a cohesiveness measure is to determine first the cohesiveness for each pair of individuals (or, in dual terms, the distance between their preferences) and then aggregate these numbers by taking, for example, the mean. In fact, this is the approach followed by, for example, Hays (1960) and García-Lapresta and Pérez-Román (2011), García-Lapresta and Pérez-Román (2010). When considering this approach, the following two issues have to be considered: (i) Which is the appropriate cohesiveness measure for pairs of individuals? And (ii) is all relevant information present in the preference profile aggregated correctly? With respect to the first question, it is important to note that, for the case in which individual preferences can be represented by means of ordinal rankings, many distance metrics for pairs of individuals have been studied. The most important one is probably the Kemeny (1959) distance that calculates the proportion of pairwise comparisons the two rankings do not coincide upon. This idea has been followed up by Kendall (1962) who suggests to measure the cohesiveness between any two rankings by one minus the Kemeny distance; that is, the proportion of pairwise comparisons the two individuals agree upon. This measure is usually referred to in the literature as Kendall’s \(\tau \). Alternative distance measures for the case of two individuals have been proposed, for example, by Cook and Seiford (1978), Cook and Seiford (1982) and, more recently, by Klamler (2008) and Can (2014). The former authors suggest to sum the differences in the Borda score of each object in the two rankings and divide the resulting number by the maximal possible value of this sum. Klamler (2008), on the other hand, considers all possible subsets (with a size of at least two) generated by the set of objects and calculates the proportion of different maximal objects between the two rankings. Hence, this study puts equal weight on all possible subsets, an assumption that has recently been relaxed by Baldiga and Green (2013) by requiring that only subsets of the same size receive the same weight. Finally, Can (2014) evaluates the distance between two preferences by focusing on a path of pairwise changes that connects them, where the weight of each pairwise change depends on its position.Footnote 1 It can be observed that the three last-mentioned measures take into account that higher positions in the rankings are more important to calculate the cohesiveness. In fact, Klamler (2008) and Baldiga and Baldiga and Green (2013) focus on the choices that the preferences generate over sets, while Can (2014) directly allows for higher weights for pairwise comparisons situated higher up in the rankings. With respect to point (ii) mentioned above, it is indeed true that the usual approach (and, probably, the most intuitive one at first sight) consists in selecting a cohesiveness measure between two rankings of the ones cited above together with an aggregator (usually the mean). For example, the average tau (\(\bar{\tau }\)) introduced by Hays (1960) calculates first Kendall’s \(\tau \) for all possible pairs of individuals. The cohesiveness is then determined by taking the average of all those numbers. However, we will show with the help of an instructive example that selecting a measure defined over two individuals derived from a neutral distance metric together with the mean aggregator seems too restrictive as an approach as it leaves important information unconsidered. One way to overcome this problem is by constructing a cohesiveness measure by focusing on the preference profile as a whole (and not on pairs of individuals alone) and taking into account that a higher weight should be given to objects that are chosen more often when the group is presented a (random) menu of objects. The set of axioms we present in this paper is motivated by these two points. As a result, we characterize a family of measures that evaluate the cohesiveness in a preference profile as follows: (i) If there are only two objects, the cohesiveness measure equals the proportion margin by which one object is preferred to the other in the society; and (ii) if there are more than two objects, the values of the proportion margins for each subset of two objects are aggregated by a weighted mean. Thereby, the weights indicate how important objects are at the social level. In particular, the weights are a function that depends on the number of objects each of the two objects under consideration wins and ties against (with a tie counting half a win) using pairwise comparisons. The measures of the family differ in the degree of this positive dependence. It is important to note that all measures belonging to the characterized family account for the issues raised in the paper even though they are simply based on pairwise comparisons (as Hays’ \(\bar{\tau }\)). Obviously, one can think of extending the analysis by proposing measures that use the positions the objects occupy in the rankings or the maximal objects in each possible subset as the basic ingredient and that focus, at the same time, on the whole preference profile. We have opted here for the simpler approach to highlight the advantages of considering the whole preference profile, even though we only use the information of pairwise comparisons. Finally, we would also like to mention that the present study builds upon Alcalde-Unzu and Vorsatz (2013), where we characterize a family \(\Gamma \) of cohesiveness measures. The measures belonging to this family depend on a parameter that evaluates the effect on the cohesiveness if one individual changes a pairwise comparison as a function of how many of the remaining individuals in the group prefer the first object over the second and vice versa. So these measures do not take into account how important objects are at the social level. In fact, the set of axioms that we propose in the present study corresponds to the set of axioms that characterizes \(\Gamma \) with two exceptions: we propose here two new axioms (Ranking and Consistency) that substitute two of the properties that characterize \(\Gamma \) (Independence and Concordance), so that the insights that arise from the instructive example are taken into account by the newly characterized measures. We proceed as follows. First, we introduce notation and definitions. Section 4 presents an example showing the difficulty of constructing an appropriate cohesiveness measure derived from a distance metric and the mean aggregator. Section 5 discusses the set of axioms paying special attention to the explanation of the new axioms. Section 6 introduces the new family of cohesiveness measures and presents the characterization result. Finally, we conclude. The main proofs are relegated to the Appendix.",6
80.0,3.0,Theory and Decision,22 May 2015,https://link.springer.com/article/10.1007/s11238-015-9505-0,A minimal extension of Bayesian decision theory,March 2016,Ken Binmore,,,Male,Unknown,Unknown,Male,"Bayesian decision theory was created by Savage (1954) in his ground-breaking Foundations of Statistics. He emphasizes that the theory is not intended for universal application, observing that it would be “preposterous” and “utterly ridiculous” to apply the theory outside what he calls a small world (Savage 1954, p. 16). It is clear that Savage would have regarded the worlds of macroeconomics and finance as large, but it is not so clear just how complex or surprising a world needs to be before he would have ceased to count it as small. Nor does he offer anything definitive on how rational decisions are to be made in a large world.Footnote 1
 In recent years, a substantial literature has developed that offers various proposals on how to extend Bayesian decision theory to at least some kinds of large worlds. This paper offers another theory of the same kind that deviates more from the Bayesian orthodoxy than my book Rational Decisions (Binmore [2009, Chapter 9]) but remains only a minimal extension of the standard theory. It should be emphasized that the paper only discusses rational behavior, which may or may not reflect how people behave in practice. A decision problem can be modeled as a function where \(A\) is a space of feasible actions, \(B\) is a space of possible states of the world whose subsets are called events, and \(C\) is a space of consequences that we assume contains a best outcome \(\mathcal{W}\) and a worst outcome \(\mathcal{L}\). Each action is assumed to determine a finite gamble G of the form in which \(\mathcal{E}=\{E_1, E_2, \ldots , E_m\}\) is a partition of the belief space \(B\), and the prize \({\mathcal{P}}_i\) is understood to result when the event \(E_i\) occurs. Different symbols \({\mathcal{P}}_i\) for a prize need not represent different outcomes of a gamble. It is taken for granted that the order in which the columns of (1) appear is irrelevant, and that it does not matter whether columns with an empty \(E_i\) are inserted or deleted. We follow the standard practice of assuming that the decision maker has a preference relation \(\preceq \) defined on whatever set \(G\) of gambles need to be considered. If \(\preceq \) satisfies sufficient rationality (or consistency) postulates, it can be described by a Von Neumann and Morgenstern (VN & M) utility function \({u:\,G\rightarrow \mathbb {R}}\). It is usually assumed that the gamble G whose prizes are all \({\mathcal{P}}\) can be identified with \({\mathcal{P}}\). The standard VN&M utility function \({U:\,C\rightarrow \mathbb {R}}\) of the orthodox theory can then be identified with the restriction of \(u\) to \(C\). The assumptions of Bayesian decision theory then allow \(u(\mathbf{G})\) to be expressed as the expected utility: where \(p\) is a (subjective) probability measure. As in other generalizations of Bayesian decision theory, we replace the subjective probability \(p(E)\) by a less restrictive notion that we denote by \(\pi (E)\). As with \(p(E)\) in the orthodox theory, \(\pi (E)\) is defined as the utility \(u(\mathbf{S})\) of the simple gamble S that yields \(\mathcal{W}\) if the event \(E\) occurs and otherwise yields \(\mathcal{L}\). The surrogate probability \(\pi (E)\) need not be additive, but we do not call it a non-additive probability to avoid confusion with Schmeidler (1989, (2004) well-known theory. The utility \(u(\mathbf{G})\) similarly fails to reduce to an expected utility of the form (2), but we make no use of the Choquet integral. The version of Savage’s sure-thing principle offered as Postulate 5 generalizes the linear expression (2) to a multivariate polynomial in the quantities \(x_i=U({\mathcal{P}}_i)\)
\((i=1,2,\ldots m)\). We refer to this extension of expected utility as expectant utility for the reasons given in Sect. 4.4. Because we confine attention to minimal extensions of the Bayesian orthodoxy, it is easy to make assumptions that tip the theory back into Bayesianism. Postulate 8 is such an assumption. If it is imposed on all gambles in \(G\), then \(\pi \) must be a probability and we recover the expected utility formula (2). However, Sect. 6 argues against imposing Postulate 8 in a large-world context. Instead, a class \(\mathcal{M}\) of measurable events is defined using the criterion that gambles constructed only from events in \(\mathcal{M}\) satisfy Postulate 8. The restriction of \(\pi \) to \(\mathcal{M}\) then satisfies the requirements of a probability, and so we denote it by \(p\). We then say that the events in \(\mathcal{M}\) are not only measurable, but have been measured. Requiring that expected utility should be maximized for gambles constructed only from events in \(\mathcal{M}\) imposes restrictions on the coefficients of the multivariate polynomial representation of expectant utility (Sect. 6.1). 
Ambiguity versus uncertainty There is more than one way of interpreting unmeasured events in this model. The more orthodox assumes that the decision maker would be able to assign a subjective probability to all unmeasured events if better informed, but her ignorance prevents her settling on a particular value of this probability. This ambiguity interpretation needs to be compared with the wider uncertainty interpretation, which allows that it may be intrinsically meaningless to assign probabilities to some unmeasured sets. The model of this paper is intended to be applicable even with the uncertainty interpretation. The work outlined in Sect. 1.2 is prefixed by asking how a surrogate probability \(\pi (E)\) might be constructed from \(p\) on the assumption that all the decision-maker’s information has already been packaged in the subjective probabilities she has assigned to the measured events in \(\mathcal{M}\). All that she can then say about an event \(E\) is that it has outer measure \(\overline{p}(E)\) and inner measure \(\underline{p}(E)\) (Sect. 2.2). Following Good (1983), Halpern and Fagin (1992), and Suppes (1974) and others, we identify \(\overline{p}(E)\) with the upper probability of \(E\) and \(\underline{p}(E)\) with its lower probability. The Hurwicz criterion (Hurwicz 1951; Chernoff 1954; Milnor 1954; Arrow and Hurwicz 1972) expresses \(\pi (E)\) as a weighted arithmetic mean of \(\overline{p}(E)\) and \(\underline{p}(E)\): The ambiguity aversion reported in experiments on the Ellsberg paradox is sometimes explained by taking \(\alpha <{1 \over 2}\) in Eq. (3). Ambiguity-loving behavior then corresponds to \(\alpha >{1 \over 2}\) and ambiguity neutrality to \(\alpha ={1 \over 2}\). However, Theorem 3 suggests that, when upper and lower probabilities are identified with outer and inner measures, only \(\alpha ={1 \over 2}\) is viable. 
Alpha-maximin Equation (3) assigns a utility to any simple gamble. With the ambiguity interpretation, there is a natural extension called \(\alpha \)-maximin to the case of a general gamble G. One first computes the expected utility of G for all probability distributions that are not excluded by the decision-maker’s information. The utility of G is then taken to be a weighted arithmetic mean of the infimum and supremum of this set of expected utilities. Arguments for this conclusion are given by Ghirardato et al. (2002) and Klibanoff et al. (2005). My earlier work (Binmore 2009) assumes the same extension of the simple Hurwicz criterion to the general case, but the theory offered in Sect. 5 of the current paper is not consistent with \(\alpha \)-maximin.",4
80.0,3.0,Theory and Decision,10 July 2015,https://link.springer.com/article/10.1007/s11238-015-9509-9,In search of good probability assessors: an experimental comparison of elicitation rules for confidence judgments,March 2016,Guillaume Hollard,Sébastien Massoni,Jean-Christophe Vergnaud,Male,Male,Unknown,Male,"In many situations, it is interesting to elicit an accurate measure of probabilistic beliefs, for instance to obtain individual opinions or information about unknown events, to predict better individual behavior under uncertainty, to obtain information about an expert’s knowledge, etc. In lab experiments or surveys, the practice of belief elicitation has grown significantly. However, different elicitation methods are employed and determining the best rule is open to debate (Schotter and Trevino 2014). This paper is a contribution to this debate. A good belief elicitation rule is a rule that permits truthful and informative beliefs to be revealed (Winkler and Murphy 1968). “Truthfulness” requires that elicited beliefs be as close as possible to the subjective beliefs individuals hold. To study the accuracy of an elicitation rule in revealing truthful beliefs, elicited beliefs need to be compared to a benchmark. Recently, some papers in economics have attempted to evaluate the goodness of different rules in terms of their ability to reveal objective probabilities (Armantier and Treich 2013; Hao and Houser 2012; Hossain and Okui 2013; Kothiyal et al. 2011). In a pure subjective framework where there are no objective probabilities available, the absence of any benchmark limits the possibility of assessing how good elicitation rules are in terms of truthfulness. “Informativeness” means that a subject is a “good” probability assessor, that is, her beliefs are statistically reliable. The information content of the beliefs can be evaluated through two dimensions: calibration refers to the quality of the match between stated and empirical probabilities, while discrimination (also called resolution) focuses on the extent to which elicited beliefs permit the occurrence of different events to be distinguished. Our aim is to compare different elicitation rules according to their respective performances in terms of calibration and discrimination. In this study, we focus on a specific kind of subjective probability, which is confidence in one’s own judgment. We ask subjects to reveal how confident they are in the accuracy of a past decision. The advantage of this setup is threefold. First, individuals are used to make confidence judgments in their everyday lives. It also permits a rich set of individual data to be gathered and since the design of the experiment comprises two tasks, we can make comparisons across domains. The tasks are the following: a cognitive task (questions of general knowledge and logic) which is often used to assess subjects’ overconfidence (Lichtenstein and Fischhoff 1977), and a perceptual task which is new in experimental economics as it is mainly used in cognitive psychology (Baranski and Petrusic 1994; Harvey 1997). Finally, for the perceptual task, we use a robust model of decision making based on signal detection theory with which predictions of confidence judgments can be made. Using the same dataset, Massoni et al. (2014) compare elicitation rules according to their goodness of fit with predictions issued from this model. These results complement our present findings on informativeness. A set of scores measure calibration or discrimination. We restrict our attention to four indexes. We use the decomposition of the Brier score (Brier 1950) in two components [a calibration index (CI) and a discrimination index (DI)] proposed by Murphy (1998). Overconfidence is another measure of calibration that focuses on how close beliefs are to subjects’ actual average performances (see the literature on overconfidence—Lichtenstein et al. 1982 or Wallsten and Budescu 1983). As a second measure of discrimination, we consider the area under the receiver operating characteristic (ROC) which is often used to study metacognition in neuroscience (see Fleming and Dolan 2012 for a review). Three elicitation rules are considered. We use the quadratic scoring rule (QSR) since it is the most widely used elicitation rule (QSR) (Nyarko and Schotter 2002; Offerman et al. 2009, or Palfrey and Wang 2009). The second rule, which we hereafter call the “Matching Probability” method (MP), uses the Becker–DeGroot–Marschak method to elicit an objective probability equivalent to the subjective belief. The principle of this procedure has been known for a while (Arrow 1951; Raiffa 1968; Winkler 1972; LaValle 1978 among others), but has rarely been used until recently (Grether 1992; Abdellaoui et al. 2005; Holt 2006; Holt and Smith 2009 are some exceptions).Footnote 1 The third one is a simple ordinal scale without incentives that we call the “Free Rule” (FR). It is common in psychology to elicit confidence judgments without incentives. It should be noted that all these rules have major drawbacks. The QSR is known to yield biased estimates of subjective beliefs if the subjects are risk averse. The MP may appear cognitively demanding for subjects because it involves a complex scheme of payment mechanisms and difficult instructions. The FR provides no incentives to make efforts, but since reporting confidence may be a cognitive low-cost task, the absence of incentives may not matter. These weaknesses provide motivations for a study comparing their performances. Beside the comparison of the three elicitation rules according to the calibration and discrimination measures in two domains, our set of data allows specific issues concerning confidence judgments to be addressed. Are calibration and discrimination stable measures across domains? Are calibration and discrimination separate components of the accuracy of confidence? Do we observe the same pattern of results across rules? Our results show that the MP is the better probability assessor than the two other rules. Even if we observe that QSR gives the best results in terms of overconfidence in the cognitive task, this performance may be due to its risk aversion distortion. The MP has a better performance in terms of discrimination. Since in Massoni et al. (2014), results clearly show that elicited beliefs through MP fit better the predictions of a computational model of confidence, MP seems to have an overall advantage. The free rule, despite its lack of incentives, also performs well. The results also provide evidence that overconfidence and discrimination are two distinct components of the accuracy of confidence, and argue in favor of using the two measures instead of focusing on only one of them. The inter-task correlations show that individual ability to elicit informative beliefs is stable across domains. This pattern of results appears more clearly in the MP and the FR data than in the QSR data. This article is organized as follows: in Sect. 2, we describe the three elicitation rules used and in Sect. 3 we present the criteria comparing these rules. Section 4 presents the experimental design and Sect. 5 compares the rules using the data. Section 6 concludes.",27
80.0,3.0,Theory and Decision,01 August 2015,https://link.springer.com/article/10.1007/s11238-015-9512-1,The impact of ambiguity and prudence on prevention decisions,March 2016,Loïc Berger,,,Male,Unknown,Unknown,Male,"Prevention is defined by the Handbook of Insurance as a “risk-reducing activity that takes place ex-ante, i.e. before the loss occurs” (Courbage et al. 2013). As risk is defined through the size and probability of the potential loss, this activity can either impact the size of the potential loss, its probability or both. Ehrlich and Becker (1972) were the first to study these risk management tools, referred to as self-insurance and self-protection, and used to deal with the risk of facing a monetary loss when market insurance is not available. In both situations, a decision maker (DM) has the opportunity to undertake an effort to modify the distribution of a given risk. In particular, the self-insurance effort (also called loss reduction) corresponds to the amount of money invested to reduce the size of the loss occurring in the bad state of the world, while the self-protection effort (also called loss prevention) is the amount invested to reduce the probability of suffering from the loss. Examples of such efforts may be found in many every day life situations as well as in many different economic fields. From the installation of an airbag system in a car, a sprinkler system in a house, or investments in adaptation efforts to fight global climate changeFootnote 1 in the case of self-insurance, to the attendance of driving safety lessons, the installation of a burglar system, or investments in mitigation effortsFootnote 2 in the case of self-protection. Though these models have received a great deal of attention in the recent literature, it is worth noting that they have, until now, generally been studied only in simple one-period, two-state settings remaining in the expected utility framework (see Courbage et al. 2013 for a recent overview). Although these relatively simple monoperiodic models were well adapted to understand the key properties of the self-insurance/self-protection tools in situations of risk, they appear too restrictive to describe a large number of important issues in at least three aspects. First, there are many situations requiring self-insurance or self-protection in real life, in which the decision to make an effort and the realization of uncertainty do not take place at the same time (consider for instance the examples above). A long period of time may pass between these two events, leading to the necessity of taking intertemporal considerations into account and building multi-period models. The second limitation is that most of the models studied in the literature remain in the expected utility framework, and are, therefore, unable to deal with other kinds of uncertainty besides risk.Footnote 3 In many real-life problems, however, the nature of the uncertainty considered cannot be limited to risk since the probabilities associated with the realization of uncertain events cannot always be objectively known. In these kinds of situation, ambiguity plays a central role, and the attitude that agents generally manifest towards it (ambiguity aversion) needs to be taken into account.Footnote 4 This is not the case of the subjective expected utility theory, that assumes ambiguity neutrality. Therefore, it is important to consider alternative models when considering problems in the presence of deeper uncertainty. Finally, the third limitation usually shown by the models proposed in the literature is the restriction to the case of Bernoulli distribution of outcomes (notable exceptions are the models proposed by Meyer and Meyer (2011) and Alary et al. (2013)). In reality, however, there are many situations in which the good and bad states are not necessarily unique. A DM may be willing to self-insure or self-protect a loss even if he does not know exactly what this loss will be. A generalization to a multi-state model is, therefore, necessary to study these cases. To illustrate these three points, consider the case of a young man, who faces the risk of developing heart disease when he is older, but who can choose in his youth whether or not to practice sport as a preventive measure. Sport is costly, but can either reduce the probability of heart disease with which a potentially important but uncertain loss is associated (self-protection), or can reduce the severity of a disease that develops with a fixed probability (self-insurance).Footnote 5 While it is clear that many years may separate the moment at which the effort decision is taken and the moment at which the uncertainty is realized, additional difficulties in such a situation, is that at the time the decision is taken of doing sport on a regular basis or not, both the probability of developing a heart disease at old age, and the probability of suffering from a bigger or smaller loss in case of heart disease are unknown.Footnote 6
 In this paper, I present models of self-insurance and self-protection that are able to overcome the above-mentioned limitations. Each model takes the form of a simple two-period model where preferences are represented by the model Klibanoff et al. (2005, 2009) (KMM) developed to deal with ambiguity.Footnote 7 The timing of the decision process is simple: in the first period, a DM chooses the level of effort he wants to exert to affect either the probability of being in a set of states that are considered as bad in the second period, or to affect the level of wealth in these states. Using this setting, I derive the conditions under which ambiguity aversion raises the demand for insurance, self-insurance and self-protection. In particular, I show that when the effort is undertaken during the first period, ambiguity aversion tends to have a positive impact on the demand for (self-)insurance and self-protection. However, as for the study of risk attitude in which risk aversion alone is not sufficient to guarantee a higher level of prevention [since risk prudence is also needed (Menegatti 2009)], I show that the extra condition of ambiguity prudence attitude is also needed to observe this positive impact. Contrary to the conflicting results obtained in the one-period settings (Eeckhoudt and Gollier 2005), the close relationship that is achieved between prudence and prevention in the two-period setting is then re-established in the presence of ambiguity. This paper, hence, constitutes both an extension of the research on self-insurance and self-protection under ambiguity initiated by Snow (2011) and Alary et al. (2013)Footnote 8 in the sense that it goes from the study of the one- to the two-period problem, but also of Menegatti (2009) as it allows for non-neutral ambiguity attitudes. Overall, it also constitutes an extension of the existing literature on self-insurance and self-protection in the sense that it considers multiple states of the world, and does not restrict ambiguity to be concentrated on only one state of nature. Except that the results concerning self-insurance under ambiguity are shown to be easily extended to the two-period case, the particular interest of this approach is that it enables to treat the situations in which the effort of self-protection goes together with a decrease in the degree of ambiguity. Indeed, it appears more natural in many situations that self-protection would reduce both risk and ambiguity (think for example the security, climate change and health examples in which the effort not only decreases the average probability of loss, but also its range of possible values). In that sense and contrary to the results obtained in Alary et al. (2013), this paper enables to give, for most usual situations, a clear answer to the question: Does ambiguity aversion raise the optimal level of effort? The remainder of this paper is organized as follows. In Sect. 2, I introduce the simple two-period model under ambiguity by studying the problem of full insurance. Then in succession, I analyze the willingness to pay (Sect. 3) and the optimal effort (Sect. 4) for self-insurance and self-protection. Section 5 concludes the paper.",16
80.0,3.0,Theory and Decision,30 July 2015,https://link.springer.com/article/10.1007/s11238-015-9511-2,Risk preferences of Australian academics: where retirement funds are invested tells the story,March 2016,Pavlo R. Blavatskyy,,,Male,Unknown,Unknown,Male,"We elicit risk preferences of Australian academics by analyzing the distribution of their retirement funds across available investment options. Current and former employees of Australia’s higher education and research sector (as well as their family members) often invest their retirement funds through the sector investment fund UniSuper. The fund is one of the largest superannuation funds in Australia with more than 450,000 member accounts as per 31 December 2013. The fund offers its customers 15 investment options where they can invest their retirement funds. Figure 1 shows the aggregate distribution of retirement funds across 15 available options. As per 31 January 2014, “Balanced” was, by far, the most popular investment option with AUD 9 832.2 million (or 48.07 %) invested in it. The second most popular option was “Growth” with AUD 3 564.8 million (or 17.43 %) invested in it. The third most popular option was “High Growth” with AUD 2 003.2 million (or 9.79 %) invested in it. Altogether these three options attracted just over three quarters of all retirement funds. Another four options (“Conservative Balanced”, “Cash”, “Capital Stable” and “Socially Responsible High Growth”) attracted AUD 3 699.1 million (or 18.08 %) of retirement funds. The remaining eight options attracted less than 6.63 % of retirement funds. These investment options were relatively recently introduced to the customers and no long-term data are yet available for assessing their performance. We do not analyze these eight options in this paper. The aggregate distribution of retirement funds across 15 available investment options Table 1 shows quarterly returns on seven most popular investment options over the last ten years (2004–2013). We shall assume that decision makers take a discrete probability distribution presented in Table 1 as an accurate approximation of the true (unknown) joint distribution of assets’ returns. This information is publicly available for customers on the website of UniSuper. Figure 2 plots the expected quarterly return and its standard deviation for seven investment options. If Australian academics cared only about the expected return and its standard deviation (or variance) then they would have never invested into “Socially Responsible High Growth”. This investment option is dominated by three other available investment opportunities (offering a higher return and a lower risk). Yet, as per 31 January 2014, “Socially Responsible High Growth” attracted AUD 615.6 million (or 3.01 %) of retirement funds. Similarly, if Australian academics cared only about the expected return and its standard deviation (or variance) then they would have never invested into “Conservative Balanced”. For example, this investment option is dominated by a 50–50 % mixture of investment opportunities “Capital Stable” and “Balanced”. Yet, as per 31 January 2014, “Conservative Balanced” attracted AUD 1 177.2 million (or 5.76 %) of retirement funds. In fact, holding money in “Socially Responsible High Growth” and “Capital Stable” is never optimal under all preference specifications considered in this paper. The remaining five investment options offer a classical trade-off between risk and return. In terms of an increasing risk (and an increasing expected return) the sequence of investment options can be arranged as: “Cash”, “Capital Stable”, “Balanced”, “Growth” and “High Growth”. Expected return and its standard deviation for seven investment options Figure 3 plots cumulative distribution functions of investment options. The cumulative distribution function of “Socially Responsible High Growth” lies nearly always above that of “High Growth”, i.e., the former option is nearly first-order stochastically dominated by the latter option. In general, however, the cumulative distribution functions exhibit a single-crossing property: an investment option with a larger probability of high returns also yields a larger probability of low returns. Cumulative distribution functions for seven investment options",
80.0,3.0,Theory and Decision,11 July 2015,https://link.springer.com/article/10.1007/s11238-015-9510-3,Egalitarian–utilitarian bounds in Nash’s bargaining problem,March 2016,Shiran Rachmilevitch,,,Female,Unknown,Unknown,Female,"A (2-person) bargaining problem is a compact and convex set \(S\subset \mathbb {R}^2_+\) that contains \(\mathbf{0 }\equiv (0,0)\) and satisfies \(S\cap \mathbb {R}_{++}^2\ne \emptyset \). The set S is interpreted as a menu of available utility pairs, out of which the two players (bargainers) need to chose a single point. If they agree on x then player i ends up with the utility payoff \(x_i\), and failing to reach agreement leads to the implementation of the status quo payoffs—\(\mathbf{0 }\); since \(S\cap \mathbb {R}_{++}^2\ne \emptyset \), there are strict incentives to avoid disagreement. Let \(\mathcal {B}\) denote the collection of all bargaining problems. A bargaining domain is a non-empty subset \(\mathcal {D}\subset \mathcal {B}\), and a solution (on \(\mathcal {D}\)) is any function \(\sigma :\mathcal {D}\rightarrow \mathbb {R}^2\) that satisfies \(\sigma (S)\in S\) for all \(S\in \mathcal {D}\). The best-known solution in the literature is the Nash solution, N (due to Nash (1950)), which assigns to each problem S the point \(x\in S\) that maximizes the payoffs-product, \(x_1\cdot x_2\). Other solutions that will be referred to in the sequel are the egalitarian solution and the utilitarian solution. The egalitarian solution, E, which was introduced into the bargaining literature and axiomatized by Kalai (1977a), assigns to each problem S the intersection point of its Pareto frontier and the \(45^\circ \) line. Denote this point by \(E(S)\equiv (e(S),e(S))\); that is, e(S) is the maximum payoff that can be provided to both players simultaneously in the problem S. For a comprehensive problem S, the point E(S) maximizes \({\text {min}}\{x_1,x_2\}\) over \(x\in S\).Footnote 1
 A utilitarian solution, by contrast, maximizes the utility sum over S. In general, this sum may be maximized at multiple points of S, hence there are multiple utilitarian solutions, but they only differ in their tie-breaking among utility-sum maximizers; a generic such solution will be denoted by U. Utilitarianism and egalitarianism are basic principles of distributive justice (see, among many others, Fleurbaey et al. (2008)). The utilitarian philosophy takes the sum of the individual utilities to be society’s utility; by contrast, the egalitarian philosophy, the most prominent representative of which in the twentieth century is John Rawls, puts the emphasis on advancing the welfare of society’s worst-off members. Thus, U and E are the formal expressions of these competing principles in the context of the bargaining model. Both utilitarianism and egalitarianism are based on interpersonal utility comparisons (Elster and Roemer 1991); for instance, if utilities are not comparable, then their summation bears no ethical significance. Unfortunately, however, even if utilities are comparable in principle, they may not be comparable in practice. To be specific, the bargaining model does not contain any information about the nature of the utility numbers described by any point \(x\in S\). Therefore, without out-of-the-model information about the meaning of utilities, the solutions U and E do not necessarily implement their respective underlying philosophies. A natural reaction to this difficulty is to adopt suitable normalizations. In particular, a relative utilitarian solution (Dhilon and Mertens 1999; Pivato 2009; Segal 2000; Sobel 2001) assigns to each S a maximizer of \(\frac{x_1}{a_1(S)}+\frac{x_2}{a_2(S)}\) over \(x\in S\), where \(a_i(S)\equiv {\text {max}}\{s_i: s\in S\}\) (a(S) is called the ideal point of S); the aforementioned sum may be maximized at multiple points—each such point is called a relative utilitarian point—hence there are multiple relative utilitarian solutions; a generic such solution will be denoted by RU. Relative egalitarianism is expressed by the Kalai–Smorodinsky solution, KS, due to Kalai and Smorodinsky (1975), which assigns to each S the point \(\lambda a(S)\), where \(\lambda \) is the maximum possible. The solutions KS and RU are the normalized versions of E and U, respectively. Mathematically, this normalization finds its expression in the fact that, as opposed to E and U, KS and RU are scale invariant solutions, meaning that for every S and every pair of positive linear transformations \(l=(l_1,l_2)\) the solution \(\sigma \) satisfies \(\sigma (l\circ S)=l\circ \sigma (S)\) (the solution N is also scale invariant).Footnote 2
 In broad terms, the subject of this paper is the relationships between the solutions in \(\{E,U,KS,RU\}\) on the one hand, and N on the other hand. To the best of my knowledge, the first result in the literature about a non-trivial such relationship is the following, due to Cao (1982). Cao proved that for every comprehensive problem S for which the relative utilitarian point is unique, the point N(S) is “between” RU(S) and KS(S). That is, Say that a solution, \(\sigma \), satisfies Cao’s bounds, if it satisfies the counterpart of (1); that is, \(\sigma \) satisfies Cao’s bounds if the following is true for every problem S whose relative utilitarian point is unique: Adhering to Cao’s bounds can be interpreted as respecting a minimal degree of both (relative) utilitarianism and egalitarianism. In a recent paper (Rachmilevitch 2014), I showed that the fact that N satisfies Cao’s bounds can be strengthened: it was proved in that paper that the following is true for any comprehensive problem S whose utilitarian point, U(S), is unique: Inequality (2) implies (1) in the following sense: for every problem S such that \(U(l\circ S)\) is unique for every rescaling l, (2) implies (1). To see why, consider such an S. Apply to it a transformation l such that \(a_1(S')=a_2(S')\), where \(S'=l\circ S\). By assumption (2) holds for \(S'\), and therefore (1) holds for it as well, because on normalized problems—problems the ideal point of which is on the \(45^\circ \)-line—there is no difference between the normalized and non-normalized versions of egalitarianism and utilitarianism. Since (1) holds for \(S'\) it also holds for S, because (1) involves only scale-invariant solutions. Say that a solution, \(\sigma \), satisfies the EU bounds, if it satisfies the counterpart of (2); that is, \(\sigma \) satisfies the EU bounds if the following is true for every problem S whose utilitarian point is unique: Cao’s bounds and the EU bounds bring about two questions: Can N be axiomatized on the basis of these bounds? Can the bounds be improved for N? In Rachmilevitch (2014), I derived an axiomatization of N on the basis of the EU bounds (on the domain of comprehensive problems, N is the unique scale-invariant solution that respects them), and I showed that for any S for which \(RU(S)\ne KS(S)\), the part of S’s Pareto frontier that ranges between RU(S) and KS(S) can be narrowed to a smaller range in which N(S) is guaranteed to lie. In particular, it was shown that, in certain senses, N(S) is closer to RU(S) than it is to KS(S). In the present paper, I derive analogous results: I characterize N on the basis of Cao’s bounds and I show that the EU bounds can be tightened in a way that lends formal meaning to “N is closer to U than it is to E.” The characterization of N involves an axiom which is new to the literature, the sandwich axiom. This axiom requires that for every triple of nested problems, \(S\subset V\subset T\), if \(\sigma (S)=\sigma (T)=x\), then \(\sigma (V)=x\). This axiom is a weakening of Nash’s (1950) independence of irrelevant alternatives (IIA), which requires \(\sigma (S)=\sigma (T)\) whenever \(\sigma (T)\in S\subset T\).Footnote 3 The typical justification for IIA is that if the agreement \(\sigma (T)\) is “revealed by the solution” to be superior to any alternative in \((T{\setminus } \{\sigma (T)\})\), then it is obviously superior to any alternative in \((S{\setminus } \{\sigma (T)\})\); in particular, whether the alternatives in \((T{\setminus } S)\) are available for choice is irrelevant as long as \(\sigma (T)\) is available. Along similar lines, the sandwich axiom, when applied to a triple \(S\subset V\subset T\), can be interpreted as follows: if the alternatives in \((T{\setminus } S)\) were proved to be irrelevant, then those in \((T{\setminus } V)\) are surely irrelevant. Namely, the sandwich axiom requires that a subset of irrelevant alternatives be a set of irrelevant alternatives. The Nash solution has a well-known generalization: \(\sigma \) is an asymmetric Nash solution if there is a \(\beta \in (0,1)\) such that \(\sigma (S)\) maximizes \(x_1^\beta \cdot x_2^{1-\beta }\) over \(x\in S\), for every S. This solution was first axiomatized by Kalai (1977b), on the basis of an axiom list that contains IIA. I show that in this axiomatization, IIA can be weakened to the sandwich axiom. The rest of the paper is organized as follows. Section 2 formally defines several important bargaining domains; Sect. 3 contains the axiomatization of the Nash solution and other results that are related to the sandwich axiom (among them is the just-mentioned axiomatization of the asymmetric Nash solution); Sect. 4 is dedicated to the improvement of the EU bounds—an improvement which is based on a certain “more-utilitarian-than” ordering of EU bounds respecting bargaining solutions; in Sect. 5, alternative “more-egalitarian/utilitarian-than” orderings of solutions are studied.",7
80.0,3.0,Theory and Decision,29 May 2015,https://link.springer.com/article/10.1007/s11238-015-9506-z,Richter–Peleg multi-utility representations of preorders,March 2016,José Carlos R. Alcantud,Gianni Bosi,Magalì Zuanon,Male,Male,Unknown,Male,"The multi-utility representation of a not necessarily total preorder or quasi-ordering \(\precsim \) on a decision space \(\mathbf{X}\) characterizes the preorder by means of a family \(\mathbf{V}\) of real-valued (isotonic) functions, in the sense that, for all elements \(x,y \in \mathbf{X}\), \(x \precsim y\) is required to be equivalent to \(v(x) \le v(y)\) for all functions \(v \in \mathbf{V}\). On the other hand, a function v on \(\mathbf{X}\) is said to be a Richter–Peleg utility representation or an order-preserving function for a preorder \(\precsim \) on \(\mathbf{X}\) if it is increasing (i.e. \(x \precsim y \) implies that \(v(x) \le v(y)\)) and in addition \(x \prec y \) implies that \(v(x) < v(y)\), where \(\prec \) stands for the strict part of the preorder \(\precsim \). While a Richter–Peleg utility v does not characterize the preorder \(\precsim \), if we are interested in finding a maximal element for \(\precsim \), then such an element can be determined by maximizing v. A Richter–Peleg multi-utility representation
\(\mathbf{V}\) for a preorder \(\precsim \) on \(\mathbf{X}\) is a multi-utility representation for \(\precsim \) such that every function \(v \in \mathbf{V}\) is a Richter–Peleg utility for \(\precsim \). This representation notion is a synthesis of the two aforementioned approaches that preserves the advantages of both. In this paper, we prove that the existence of a single Richter–Peleg utility is necessary and sufficient for the existence of a Richter–Peleg multi-utility representation. A perfectly analogous result holds true when we require upper (or lower) semicontinuity of all the functions involved. We also show that the problem of obtaining a continuous Richter–Peleg multi-utility representation can be transformed to the problem of obtaining a continuous Richter–Peleg utility plus a continuous multi-utility representation. These results can also be combined with the earlier findings on the existence of Richter–Peleg and multi-utility representations. For example, as a corollary of our main result, it follows that on a second countable topological space, the existence of a continuous multi-utility representation implies the existence of a continuous Richter–Peleg multi-utility representation. Another notable corollary is that every preorder on a countable set has a (countable) Richter–Peleg multi-utility representation. Both of these observations follow from the fact that the existence of a countable multi-utility representation implies the existence of a Richter–Peleg utility. As a disadvantage of our approach, we prove that it is impossible to represent a nontotal preorder on a connected topological space by means of finitely many, continuous Richter–Peleg utilities. Seminal contributions to the literature on multi-utility representations include Levin (2001) and Evren and Ok (2011). In particular, Evren and Ok develop the ordinal theory of multi-utility representations [see also the more recent paper by Bosi and Herden (2012)]. The case of a finite representing family was studied by Ok (2002) and more recently by Kaminski (2007). The notion of a Richter–Peleg multi-utility representation was first introduced and studied by Minguzzi (2013), whose focus is different than ours. The paper is organized as follows. Section 2 contains the definitions. Section 3 presents the main results, whose applications are developed in Sect. 4. Section 5 focuses on the case of connected topological spaces, while Sect. 6 concludes.",15
80.0,3.0,Theory and Decision,02 June 2015,https://link.springer.com/article/10.1007/s11238-015-9507-y,Optimal stealing time,March 2016,Andrea Gallice,,,Female,Unknown,Unknown,Female,"In this paper, we introduce and study what we call the stealing game. A stealing game is a dynamic game in which a number of agents steal portions of a homogeneous and perfectly divisible pie from each other. The portion of the pie that a player can steal is stochastic. However, the expected value of this random variable is increasing in the agent’s current holdings such that larger players are able on average to steal larger portions. Within such a framework, agents must decide when and whom to rob, with the goal of finishing the game as the leader, i.e., the player who holds the largest share of the pie.Footnote 1
 Our primary goal is to solve for the optimal timing strategies of the agents. We want to find the best moment for a player to behave aggressively and steal part of the pie owned by his rivals. Such a decision is affected by an intuitive trade-off between preempting or postponing one’s move. A player who moves as soon as possible eliminates the possibility of being preempted, but he is then forced to passively suffer the potential retaliation of those who waited. On the other hand, a player who postpones his move can observe the new state of the world and react optimally. However, the agent faces the risk of being preempted and robbed by a rival, in which case his market share goes down as does the expected effectiveness of his stealing attempt. We characterize the pure strategy equilibria of the stealing game under different specifications for the number of players, the duration of the game, and the number of stealing possibilities players are endowed with. We start by explicitly solving a two-period stealing game in which players have a single stealing opportunity. Despite its simplicity, this setting highlights the strategic peculiarities of the game and shows how the above-mentioned trade-off has different solutions depending on the number of participants. No player postpones his move in a two-player game. A three-player game displays multiple equilibria and, in some of them, all agents postpone their moves. Finally, when the number of players is larger than three, we show that the number of preempting equilibria is strictly larger than the number of postponing equilibria and that asymmetric equilibria may also occur. We then generalize some of these results to a setting in which n players have K stealing opportunities in a stealing game that lasts for \(T>K\) periods. The paper is organized as follows: Sect. 2 reviews the relevant literature. Section 3 formally introduces the stealing game. Section 4 defines the equilibria of the game when players have a single stealing opportunity and there are only two periods. Section 5 generalizes the results, and Sect.  6 concludes.",
80.0,3.0,Theory and Decision,24 April 2015,https://link.springer.com/article/10.1007/s11238-015-9497-9,Make-up and suspicion in bargaining with cheap talk: An experiment controlling for gender and gender constellation,March 2016,D. Di Cagno,A. Galliera,L. Panaccione,Unknown,Unknown,Unknown,Unknown,,
80.0,3.0,Theory and Decision,16 May 2015,https://link.springer.com/article/10.1007/s11238-015-9501-4,Consistent collective decisions under majorities based on difference of votes,March 2016,Mostapha Diss,Patrizia Pérez-Asurmendi,,Unknown,Female,Unknown,Female,"Since Condorcet (1785) introduced the voting paradox, it is well known that the aggregation of transitive individual preferences under simple majority rule could lead to inconsistent collective preferences. Recalling the classical example, consider a three-alternative election with alternatives \(x_1, x_2, x_3\) and three individuals endowed with the following rankings \(x_1 x_2 x_3\), \(x_2 x_3 x_1\) and \(x_3 x_1 x_2\), where, for instance, \(x_1 x_2 x_3\) means that \(x_1\) is preferred to \(x_2\), \(x_2\) is preferred to \(x_3\) and \(x_1\) is preferred to \(x_3\). For each pair of alternatives, each individual casts a vote for her/his preferred alternative following just assumed orderings. Adding up these votes, alternatives \(x_1, x_2\) and \(x_3\) defeat \(x_2, x_3\) and \(x_1\) respectively, by two votes to one. In that voting situation, there is a cycle on the ordering induced by the strict collective preference. In such a case, that preference fails on transitivity and on triple-acyclicity given the requirements of such conditions. To illustrate, assume that alternative \(x_1\) defeats \(x_2\) and \(x_2\) defeats \(x_3\); \(x_1\) defeats \(x_3\) whenever the strict collective preference is transitive whereas \(x_3\) does not defeat \(x_1\) whenever the strict collective preference is triple-acyclic. Consider now the following voting process’ outcome: \(x_1\) defeats \(x_2\) and it is indifferent to \(x_3\), and \(x_2\) is also indifferent to \(x_3\). In this case, the weak collective preference fails on consistency. Notice that the strict preference associated with that weak preference behaves right but the indifference relation associated with the weak preference fails on transitivity. The idea that the Voting Paradox ‘should rarely be observed in any real three-candidate elections with large electorates’ stated by Gehrlein (2009) promotes the probabilistic study of the occurrence of that paradox and of their consequences under different aggregation rules. In several studies, it is assumed an a priori probability model to estimate the likelihood of different voting situations, derived the conditions under which the paradox or the effects of that appear and reached probabilities through combinatoric calculus. In this context, stand out the studies about simple majority rule (Gehrlein and Fishburn 1976; Fishburn and Gehrlein 1980; Gehrlein 1983), supermajority rules (Balasko and Crès 1997; Tovey 1997) or scoring rules (Gehrlein and Fishburn 1980, 1981, 1983; Cervone et al. 2005), among others. This paper is devoted to analyze and compare the probabilities of consistent collective decisions over three alternatives for a class of majority rules: majorities based on difference of votes (García-Lapresta and Llamazares 2001; Llamazares 2006; Houy 2007). Given two alternatives, these majorities based on differences focus on requiring to an alternative, to be declared the winner, to reach a number of votes that exceeds the number of votes for the other alternative in a quantity fixed before the voting process. In such majorities, individual preferences are understood as crisp preferences, i.e., given a pair of alternatives individuals declare if they prefer an alternative to another one or if they are indifferent between them. Here, we distinguish between the case where individuals are endowed with weak preferences and the case where individuals are endowed with linear orderings. Coming back to the consistent collective decisions analyzed here, we specifically calculate the probabilities of transitive and triple-acyclic strict collective preferences and the corresponding ones of transitive weak collective preferences for the majorities based on difference of votes, as the proportion of collective voting situations giving rise to consistent collective decisions over the total number of possible voting situations. To calculate the probabilities of consistent outcomes under majorities based on difference of votes taking into account weak and linear individual preferences respectively, we consider the impartial anonymous culture (IAC) condition (Gehrlein and Fishburn 1976) to describe the likelihood of the possible individual orderings. On the one hand, assuming weak or linear individual orderings jointly with the IAC condition allows to know the total number of possible collective preferences (again, Gehrlein and Fishburn 1976). On the other hand, the number of consistent profiles is calculated by means of Ehrhart polynomials, a method recently introduced in the social choice literature by Wilson and Pritchard (2007) and Lepelley et al. (2008) in order to estimate the probabilities of some voting paradoxes under the IAC condition. The methodology proposed here allows us to find the needed thresholds which guarantee that the probability of consistent outcomes is close to 1. In addition, we are able to hypothesize about the relationship between the type of individual preferences assumed and the likelihood of consistent collective decisions. Moreover, we set forth our results for majorities based on difference of votes with previous ones on simple majority rule (Gehrlein 1997; Lepelley and Martin 2001) and supermajority rules (Ferejohn and Grether 1974). The paper is organized as follows. Section 2 describes the theoretical framework followed in this paper and introduces majorities based on difference of votes. Sections 3 and 4 provide the results about the probability of consistent collective decisions under majorities based on difference of votes with linear preferences and with weak preferences, respectively. Section 5 concludes.",3
80.0,3.0,Theory and Decision,12 December 2014,https://link.springer.com/article/10.1007/s11238-014-9479-3,What independent random utility representations are equivalent to the IIA assumption?,March 2016,John K. Dagsvik,,,Male,Unknown,Unknown,Male,"The representation of probabilistic models of choice behavior by random utility functions has a long history. One of the early pioneers was Thurstone (1927) who proposed a Probit type model based on normally distributed utilities. In contrast, the theory of Luce (1959) was derived from his Choice Axiom (equivalent to IIA) without reference to an underlying random utility interpretation. Subsequently, Holman and Marley (see Luce and Suppes 1965, p. 338, footnote 7), showed that the Luce model can also be interpreted as a random utility model derived from extreme value distributed random utilities. McFadden (1973), Yellott (1977), and Strauss (1979) have investigated the following identification problem related to the Luce model, namely if there are distributions of the utilities other than the extreme value distributions that yield the Luce model. It turns out that under the assumptions of additively (or multiplicatively) separable utility functions in a deterministic part and a random part, the answer negative, provided the utilities are independent. To this end, the most general results have been obtained by Yellott (1977) and Strauss (1979). Strauss (1979) has also obtained some results for the case where the random parts of the utility function are not necessarily independent across alternatives. Related works are Falmagne (1978), Strauss (1979), Colonius (1984), Monderer (1992), Barberà and Pattanaik (1986), and Fiorini (2004) who have discussed necessary and sufficient conditions on systems of choice probabilities so as to be consistent with a random utility representation. Dagsvik (1994, 1995) showed that any random utility model can be approximated arbitrarily closely by Generalized Extreme Value models. In this paper, we consider another extension: we maintain the assumptions of the utilities being independent across alternatives but abandon the assumption of separability. Under these assumptions, and with infinite universal set of alternatives, it turns out that the most general random utility representation of the Luce model is a utility function that is an arbitrary strictly increasing transformation of a separable utility function (additive of multiplicative) with extreme value random component.",2
80.0,4.0,Theory and Decision,18 August 2015,https://link.springer.com/article/10.1007/s11238-015-9514-z,The emergence of reciprocally beneficial cooperation,April 2016,Sergio Beraldo,Robert Sugden,,Male,Male,Unknown,Male,"Studies of animal behaviour have found many practices which create collective benefits at some apparent cost or risk to individual participants. Examples include alarm calls, food-sharing, grooming, and participation in inter-group warfare. One of the most fundamental problems in evolutionary biology since Darwin (1859) has been to explain how such forms of cooperation evolve by natural selection. An analogous problem in economics has been to look for explanations of cooperative human practices, such as the fulfilment of market obligations, the provision of public goods through voluntary contributions, and the management of common property resources, that are consistent with the traditional assumption of individual self-interest.Footnote 1 Many different theories have been proposed by biologists and economists as possible solutions. Among the mechanisms that have been modelled are direct and indirect reciprocity, kin selection, group selection, and the ‘green beard’ mechanism. (For an overview of these mechanisms, see Nowak (2006). Tomasello (2014) provides a comprehensive account of why and how cooperation may have evolved among early humans. His hypothesis is that early humans were forced by ecological circumstances into more cooperative modes of life, and this led to the evolution of ways of thinking that were directed towards coordination with others to achieve joint goals.) Some economists have combined biological and economic modes of explanation, hypothesising that human cooperation in the modern world is a product of genetically hard-wired traits that evolved by natural selection to equip Homo sapiens for life in hunter-gatherer societies. In some versions of this hypothesis, those traits act as equilibrium selection devices in the modern ‘game of life’ (e.g. Binmore 1994, 1998); in others, they can generate non-selfish behaviour in modern societies (e.g. Boyd et al. 2005; Bowles and Gintis 2011). However, a recent trend in biology has been to question whether such sophisticated explanations are always necessary. Many forms of apparently cooperative behaviour have been found to be forms of mutualism: the ‘cooperating’ individual derives sufficient direct fitness benefit to make the behaviour worthwhile, and any effect on the fitness of others is incidental (e.g. Clutton-Brock 2002, 2009; Sachs et al. 2004). The Snowdrift game (Sugden 1986), in which equilibrium involves cooperation by one player and free-riding by the other, is increasingly used in biology as a model of such behaviour. In this paper, we present a new model of the evolution of cooperation which fits with this trend of thought. Our methodological approach treats the biological and economic problems of cooperation as isomorphic to one another. That is, we hypothesise that the emergence and reproduction of human cooperative practices are governed by evolutionary mechanisms that are distinct from, but structurally similar to, those of natural selection. Candidate mechanisms include trial-and-error learning by individuals, imitation of successful neighbours, and cultural selection through inter-group competition. Analyses which use this approach may be both informed by and informative to theoretical biology. For example, Sugden’s (1986) analysis of the emergence of social norms was inspired by the earlier work of theoretical biologists, but it developed new models (in particular, the Snowdrift and Mutual Aid games) which have since been widely used in biology (e.g. Leimar and Hammerstein 2001; Nowak and Sigmund 2005). The model that we present in this paper can be interpreted as a representation either of natural selection or of trial-and-error human learning. Our modelling strategy is distinctive in that it uses three assumptions which in combination rule out most of the mechanisms that feature in existing theories of cooperation. Specifically, we assume that interactions are anonymous, that evolution takes place in a large, well-mixed population, and that the evolutionary process selects strategies according to their material payoffs. The assumption of anonymity excludes mechanisms based on reputation, reciprocity or third-party punishment. The assumption of well mixedness excludes mechanisms of group or kin selection. The assumption that selection is for material payoffs excludes mechanisms which postulate non-selfish preferences as an explanatory primitive. Working within the constraints imposed by these assumptions, we are able to generate a simple and robust model of cooperation. Our model adapts the familiar framework of a Prisoner’s Dilemma that is played recurrently in a large population. We introduce two additional features, which we suggest can be found in many real-world cases of potentially cooperative interaction, both for humans and for other animals. The first additional feature is that participation in the game is voluntary. One of the restrictive properties of the Prisoner’s Dilemma is that, in any given interaction, an individual must act either pro-socially (the strategy of cooperation) or anti-socially (the strategy of defection or cheating, which allows a cheater to benefit at the expense of a cooperator). There is no opportunity to be simply asocial. We add an asocial strategy, that of opting out of the interaction altogether. Of course, if the only difference between anti-social and asocial behaviour was that asocial individuals did not benefit when their co-players chose to cooperate, asociality would be a dominated strategy. It is an essential part of our model that if both players cheat, both are worse off than if they had opted out of the interaction.Footnote 2
 The second additional feature is that the payoff that each player receives if they both cooperate is subject to random variation. Before choosing his (or her, or its) strategy, each player knows his own cooperative payoff, but not the other player’s. With non-zero probability, the payoff from mutual cooperation is greater than that from cheating against a cooperator. Thus, there are circumstances in which it would be profitable for a player to cooperate if he were sufficiently confident that the other player would cooperate too. Crucially, however, it is never common knowledge that the payoffs are such that mutual cooperation is a Nash equilibrium. In our model, players receive no information at all about the realisation of the random component of their co-players’ cooperative payoff. This is obviously an extreme assumption; we use it only as a modelling simplification. In real interactions, players often have some such information. [For example, explanations of animal behaviour in asymmetric contests often depend on the assumption that both contestants recognise some feature of the game which signals which of them is more likely to attach the higher value to the disputed resource (Maynard Smith and Parker 1976).] But the main qualitative results of our model require only that each player has some private information about his own payoffs such that, with non-zero probability, a player may know that cooperation is a non-dominated strategy for him without knowing whether the same is true for the other player.Footnote 3
 As an illustration of the kind of interaction that our model represents, we offer the following variant of Rousseau’s (1755/1998, p. 36) story of hunting in a state of nature. Two individuals jointly have the opportunity to invest time and energy to hunt a deer. The hunters can succeed only by acting on a concerted plan out of sight of one another. A hunt begins only if both individuals agree to take part. Each can then cheat by unilaterally pursuing a smaller prey, which the other’s deer hunting tends to flush out and make easier to catch. The anticipated benefit of deer hunting to an individual, conditional on the other’s not cheating, can be different for different individuals and on different occasions. Sometimes, but not always, this benefit is sufficiently low that unilateral cheating pays off. As a more modern illustration, consider two individuals who make contact through the internet. One of them is offering to sell some good which has to be customised to meet the specific requirements of the buyer; the other is looking to buy such a good. If they agree to trade, each individual invests resources in the transaction (exchanging information, producing and dispatching the good, sending payment). Each may have opportunities to gain by deviating from the terms of the agreement. Sometimes, but not always, the benefit of completing the transaction is sufficiently low that unilateral cheating pays off. We will show how the interaction of voluntary participation and stochastic payoffs can induce cooperation. Of course, it is well known that voluntary participation can facilitate cooperation when players can distinguish between more and less cooperative opponents. If such distinctions are possible, voluntary participation can allow cooperators to avoid interacting with cheats. This can sustain cooperation without the need for informationally and cognitively more demanding strategies of reciprocity or punishment—an idea that can be traced back to Smith’s (1763/1978, pp. 538–539) analysis of trustworthiness among traders in commercial societies. But such mechanisms are ruled out by our anonymity assumption. In our model, voluntary participation facilitates cooperation by a different route. Because would-be cheats have the alternative option of non-participation, and because non-participation is the best response to cheating, the equilibrium frequency of cheating is subject to an upper limit. If cheating occurs at all, the expected payoff from cheating cannot be less than that from non-participation. Thus, for any given frequency of cooperation, the frequency of cheating is self-limiting. The underlying mechanism is similar to that of the Lotka–Volterra model of interaction between predators and prey: the size of the predator population (the frequency of cheating) is limited by the size of the prey population (the frequency of cooperation). Clearly, however, this mechanism can support cooperation only if, when the frequency of cheating is sufficiently low, some players choose to cooperate. This could not be the case if, as in the Prisoner’s Dilemma, cooperation was always a weakly dominated strategy. In our model, random variation in the payoff from mutual cooperation ensures that players sometimes find it worthwhile to cooperate, despite the risk of meeting a cheat. The players who cooperate are those for whom the benefit of mutual cooperation is sufficient to compensate for this risk. Because cooperators are self-selecting in this way, the average payoff in the game is greater than the payoff to non-participation. In other words, despite the presence of cheats, beneficial cooperation occurs. As a first step in developing an evolutionary model, we begin (in Sect. 2) by presenting our variant of the Prisoner’s Dilemma as a one-shot game and identifying its symmetric Bayesian Nash equilibria. We show that, provided the upper bound of the distribution of cooperative benefit is not too low, the game has at least one such equilibrium in which beneficial cooperation occurs. In Sect. 3, we investigate some comparative-static properties of equilibria in this game. We show that as the distribution of cooperative benefit becomes more favourable, the maximum frequency of cooperation that is sustainable in equilibrium increases. In Sect. 4, we examine the dynamics of the model, using simple analytical methods. In Sect. 5, we supplement this analysis by computer simulations based on replicator dynamics. Our analysis shows that, in the neighbourhood of ‘interior’ equilibria in which some but not all players choose non-participation, the dynamics are similar to those of predator–prey models. Depending on the payoffs of the game, interior equilibria may be locally stable (with evolutionary paths spiralling in from a large zone of attraction) or unstable (with evolutionary paths spiralling out and ending at an equilibrium of non-participation). In Sect. 6, we discuss the contribution that our model can make to the explanation of cooperative behaviour. We show that, despite sharing some features of existing biological models of mutualism and voluntary participation, our model isolates a distinct causal mechanism.",1
80.0,4.0,Theory and Decision,11 August 2015,https://link.springer.com/article/10.1007/s11238-015-9513-0,Exploiting the guilt aversion of others: do agents do it and is it effective?,April 2016,Eric Cardella,,,Male,Unknown,Unknown,Male,"The results from a growing body of experimental literature suggest that economic agents may not be solely motivated to maximize their own material payoffs. One example of such a “non-selfish” behavioral motivation is guilt aversion. The general idea of guilt aversion is that an agent would suffer disutility, in the form of guilt, from hurting or letting down another agent, relative to that other agent’s expectations;Footnote 1 thus, a guilt averse agent may be motivated to avoid hurting or letting down that other agent, even at the expense of his/her own material payoff, to assuage the guilt feeling. Behavior consistent with agents exhibiting guilt aversion has been documented in several experimental studies. For example, Dufwenberg and Gneezy (2000), Charness and Dufwenberg (2006, 2010), Bacharach et al. (2007), Reuben et al. (2009), and Attanasi et al. (2013) find evidence of guilt aversion using variations of two-player experimental “trust” games (Berg et al. 1995).Footnote 2 Specifically, these papers show that the amount of money the trustee (second mover) returns to the trustor (first mover) is positively correlated with how much the trustor expects to get back (or how much the trustee thinks the trustor expects back). The idea being that guilt averse trustees give back more (forgo their own material payoff) the more their trustor expects back to avoid the guilt that would result from failing to meet the trustor’s expectations. In light of the experimental evidence of exhibited guilt aversion, it is important to consider the richer set of interpersonal strategic implications that can arise when agents are motivated by guilt aversion. In particular, the guilt aversion of one agent can influence the behavior of other agents in important ways. In certain social interactions, the possibility may arise for agents to behave opportunistically and exploit the guilt aversion of others. Charness and Dufwenberg (2006), in their concluding remarks, point to such a possibility by raising the question, “do people manipulate the guilt aversion of others in self-serving ways?” (p. 1595). Given the opportunity and incentive, agents could attempt to influence the behavior of guilt averse others by strategically inducing guilt upon these agents, i.e., increasing the amount of guilt they would feel. Consequently, guilt averse agents may be more motivated to respond in kind to avoid hurting or letting down those agents that had induced guilt upon them. These interpersonal implications of guilt aversion can impact strategic decision making and, consequently, economic outcomes in important ways that have yet to be explored. The goal of this study is to explore some of these interpersonal implications of guilt aversion. Specifically, this paper introduces an experimental design aimed at shedding light on the following three questions: First, do economic agents attempt to exploit the guilt aversion of other agents in self-serving ways by strategically inducing guilt upon those other agents? Second, is strategic guilt induction effective at influencing the behavior of other agents, i.e., are agents susceptible to this exploitation? Third, does having the opportunity to induce guilt upon other agents impact strategic behavior? Although previously unexplored in the economics literature, the interpersonal implications of guilt aversion have been studied and documented in the psychology literature (Vangelisti et al. 1991; Baumeister et al. (BSH henceforth) 1994, 1995; Tangney and Fischer 1995; De Hooge et al. 2011). In particular, BSH (1994) argue that one of the primary functions of guilt is to motivate others to behave in a more desirable way. In their study, BSH (1994) note that “we observed ample evidence of the hypothesized function of guilt as an interpersonal influence technique: People induced guilt to get another person to comply with their wishes.” (p. 249) Similarly, Vangelisti et al. (1991) argue that people induce guilt “primarily to achieve their own end-to persuade their listeners to do or not to do something.” (p. 33) These psychology studies provide foundational insights regarding the interpersonal functions of guilt in social relationships by drawing conclusions from non-incentivized personal narratives and surveys. However, these functions of guilt may not be restricted to social interactions; guilt may also function as an “interpersonal influence technique” in strategic economic interactions. An incentivized experimental game provides a suitable platform for investigating these interpersonal implications of guilt aversion in economic settings. Before I proceed, I pause to highlight some of the economic settings where strategic guilt induction could be relevant in terms of influencing the behavior of others and, consequently, impact outcomes. In contracting environments, guilt induction may allow a disadvantaged party to influence the behavior of an advantaged counterparty. For instance, a contracted firm that had made relationship-specific investments could possibly thwart opportunist re-contracting and hold-up by conveying to the counterparty firm the loss in profits associated with such a hold-up. In the workplace, managers could induce guilt upon employees to mitigate shirking by conveying to employees how their sub-standard effort adversely affects other employees.Footnote 3 In academia, assistant professors could possibly induce guilt upon journal editors to get a more timely review decision on a submitted paper by gently informing the editor, at the time of submission, that his/her tenure review is rapidly approaching and a lengthy review period could hinder his/her tenure prospects.Footnote 4 Many economic settings, like these mentioned, permit the possibility to induce guilt upon others. Hence, a deeper understanding of the strategic interpersonal implications of guilt aversion is required to better ascertain how the guilt aversion of agents will impact outcomes in such settings; the insights gleaned from this paper are intended to help with this understanding. To investigate whether agents strategically induce guilt upon others and its potential effectiveness, it is crucial to first identify how agents can attempt to induce guilt upon others. For this, I draw foundational insights from BSH (1994), who posit the following method for how people induce guilt in others: “If Person A wants Person B to do something, A may induce [more] guilt in B by conveying how A suffers [how much A is let down] over B’s failure to act in the desired fashion” (p. 247). In regards to strategic economic settings, this proposed method by BSH (1994) would correspond to Person A conveying how low his/her payoff would be, i.e., how he/she suffers, as a result of Player B choosing an undesirable action toward Player A.Footnote 5 Note that this method for inducing guilt implicitly requires that (i) Person A has private information about his/her own payoff and the degree to which he/she may suffer from Player B’s action, and (ii) Person A has the possibility to convey such private information to Person B.Footnote 6 Previous studies that have investigated guilt aversion in strategic settings mostly consider variations of 2-player “trust” games that do not feature either of these properties.Footnote 7 Hence, a new game is warranted that provides a rich enough strategic structure to allow agents the opportunity to induce guilt upon others. In this paper, I employ a novel experimental design that uses a 2-player, binary choice trust game featuring both private payoff information and an opportunity to convey this private information. In the game, the privately informed first mover (Player A) is effectively given an opportunity to convey to the second mover (Player B) how low his/her payoff would be if Player B fails to act in the desired fashion.Footnote 8 This game allows me to then derive testable hypotheses regarding whether Player As attempt to strategically induce guilt upon Player Bs, and whether Player Bs are susceptible to strategic guilt induction. Additionally, I show that an important artifact of this game is the ability to identify behavior consistent with guilt aversion without having to elicit beliefs. The experimental design also includes a second, related trust game that does not feature an opportunity for Player As to induce guilt upon Player Bs. This second game provides a baseline trust measure for Player As, which then allows me to derive a testable hypothesis regarding whether Player As are more trusting of Player Bs when Player As have an opportunity to induce guilt upon Player Bs. The experimental data seem to suggest that Player Bs are susceptible to the guilt induction of Player As. However, the data reveal little evidence that Player As are attempting to induce guilt upon Player Bs, à la BSH (1994). Interpreted differently, the data reveal that Player As are not attempting to fully exploit the guilt aversion of Player Bs by inducing guilt upon them; although, had they done so, it would have been effective at increasing the likelihood of Player B choosing the desired action. Furthermore, I find evidence of marginally significantly higher trust rates by Player As in the trust game where Player As have the opportunity to induce guilt upon Player B, compared to a trust game with no such opportunity. Furthermore, I show that the derived hypotheses are consistent with predictions of the formal model of “simple” guilt developed by Battigalli and Dufwenberg (2007) (B&D henceforth).Footnote 9 I also show that in the game considered, effective guilt induction can be supported in equilibrium under the B&D framework. The paper proceeds with the experimental design and hypothesis development in Sect. 2. I outline the experimental procedure in Sect. 3. I present and discuss the results in Sect. 4, and Sect. 5 concludes.",5
80.0,4.0,Theory and Decision,19 September 2015,https://link.springer.com/article/10.1007/s11238-015-9515-y,Divergent platforms,April 2016,Sophie Bade,,,Female,Unknown,Unknown,Female,"Two parties never run on the exact same platform in an electoral campaign. In contrast, two-party models following Downs (1957) and Hotelling (1929) predict that parties choose identical platforms in equilibrium. Downs–Hotelling type models assume that parties strive to win as many votes as possible and that voters have single-peaked preferences. I show that some games of electoral competition, that maintain these two standard assumptions, have equilibria in which parties announce substantially different platforms. To obtain such divergent equilibria I relax two features of the Downs–Hotelling model: I neither assume uni-dimensional issue spaces nor that parties are expected utility maximizers. Instead there are multiple issues and parties hold multiple beliefs on the distribution of voter preferences. A party changes its platform only if such a change achieves a higher vote share according to all its beliefs. Two forces drive parties to adopt identical platforms in the tradition of Downs and Hotelling: convex preferences and the fact that any party can get half the vote by adopting the platform of the opponent. Assume that Parties 1 and 2 run on two different platforms \(\mathbf x \) and \(\mathbf y \). In the Downs–Hotelling model Party 1 cannot loose by moving its platform from \(\mathbf x \) to some intermediate platform \(\lambda \mathbf x +(1-\lambda \mathbf y )\) (for \(\lambda \in (0,1)\)) since any voter with single-peaked preferences who prefers \(\mathbf x \) to \(\mathbf y \) also prefers \(\lambda \mathbf x +(1-\lambda \mathbf y )\) to \(\mathbf y \). The crux of the argument is that single-peaked preferences over a uni-dimensional issue space are convex. However, single-peaked preferences over a multi-dimensional issue space need not be convex, and a voter might well prefer both platforms \(\mathbf x \) and \(\mathbf y \) to the intermediate \(\lambda \mathbf x +(1-\lambda \mathbf y )\). With a multi-dimensional issue space, a party might lose votes by moving its platform closer to that of the opponent.Footnote 1
 For a party that obtains less than half the vote in electoral competition à la Downs–Hotelling there is yet another incentive to emulate: the party can increase its vote share by adopting the opponent’s platform. Whenever the vote is not evenly split, then one party has an incentive to copy the other. To see that the presence of multiple priors in my model dilutes this incentive, consider a model in which each party uses two different beliefs to evaluate electoral outcomes. Say that at some platform profile, Party 1 obtains 41 % according to the first belief and 55 % according to the second. According to the first belief Party 1 can increase its vote share from 41 to 50 % by adopting of Party 2’s platform, according to the second belief the same change decreases Party 1’s vote share from 55 to 50 %. In sum, there are platform profiles and sets of party beliefs, such that neither party has an incentive to copy the other although the vote is unevenly split according to each belief in the set. In Theorem 1, I show that games of electoral competition with multi-dimensional issue spaces, single-peaked preferences and two parties with multiple beliefs on the electorate can have divergent equilibria. Both forces that drive parties to adopt identical equilibrium platforms in the Downs–Hotelling model are defused in my model. Under the assumption of a multi-dimensional issue space the voters’ single-peaked preferences need not be convex. Without convex preferences, the first of the two gravitational forces described above looses its strength. The assumption of multiple priors weakens the second force. In my model vote shares may be split unevenly in equilibrium. Theorem 1 then shows that these two changes of the Downs–Hotelling model are sufficient to obtain divergent equilibria. I give an example of a game of electoral competition with office-motivated parties and single-peaked voter preferences that has a divergent equilibrium. Theorem 1 furthermore shows that divergent equilibria need not be flukes. The game of electoral competition I define to prove Theorem 1 not only has a divergent equilibrium, this equilibrium is unique up to a role-reversal of parties. So divergent equilibria need not be part of large equilibrium sets. Moreover the equilibrium I find is robust to small changes in the parties’ beliefs. Theorem 2 shows that the assumptions of a multi-dimensional issue space and multiple beliefs are necessary for the existence of divergent equilibria: if we only introduce one of these two features into a Downs–Hotelling type model, the classic conclusion that equilibrium platforms converge persists. Let me give two motivations for the multiple beliefs model of party preferences and behavior. Parties might not know enough about the voters’ preferences to assign objective probabilities to vote shares for all possible constellations of platforms: electoral outcomes are generally hard to predict as elections are held in changing environments characterized by new and surprising issues, turnover of party elites, and an evolving electorate. It is well documented that individuals violate expected utility maximization when faced with such subjective uncertainty.Footnote 2 In Bewley ’s (2002) multiple prior model, a cornerstone to the literature on uncertainty aversion,Footnote 3 agents calculate expected utilities for a set of priors \(\varPsi \) on a subjectively uncertain environment. An agent prefers some action f over some other action g if f yields a higher expected utility according to all priors \(\psi \in \varPsi \). The resulting preferences are incomplete: if some action f is associated with strictly higher expected utility than g according to some prior \(\psi ^f\in \varPsi \) while the reverse holds true according to a different \(\psi ^g\in \varPsi \), the agent cannot rank f and g. To determine behavior in the face of incompleteness, Bewley’s framework contains an inertia assumption: a new course of action has to be strictly preferred to the status quo to be adopted. The multiple prior model of party preferences and behavior can also be derived from a model of group decision making. Parties generally consist of many members or factions who have to come to some form of compromise to decide on a platform. We could assume that all these leaders share a common goal: they all hope to maximize their party’s vote share. However, the leaders might have different expectations with respect to the preferences of the electorate. The model of party preferences proposed here obtains if the change of any plank of a party’s platform requires the agreement of all factions. If no compromise can be reached, the status quo rules. Roemer (1999) and Levy (2004) tell similar stories of parties as non-unitary actors. Both posit that a party prefers platform profile \(( \mathbf x ,\mathbf y )\) over another profile \(( \mathbf x ',\mathbf y ')\) if it is preferred from the vantage point of every faction or member of that party. However, while different leaders are set apart by different beliefs in the present model, they are set apart by different goals in the models of Roemer (1999) and Levy (2004). Formal studies of electoral competition typically simplify by assuming a uni-dimensional issue space. The Downs–Hotelling model thereby ensures that an equilibrium always exists. If one replaces the uni- with a multi-dimensional issue space, the voters’ preferences have to meet stringent conditions for a Downs–Hotelling type model to have an equilibrium (in pure strategies). Davis et al. (1972), Grandmont (1978), and Plott (1967) all provide negative results on the existence of equilibria in multi-dimensional Downs–Hotelling games. Thanks to the assumption of multiple beliefs, the games under review in this study are not plagued by the same non-existence problems.Footnote 4 With the problem of equilibrium existence resolved by the assumption of multiple priors, the usually troublesome assumption of a multi-dimensional issue space turns into a an advantage. The voters’ single-peaked preferences over a multi-dimensional issue space may exhibit non-convexities and thereby allow for equilibria with substantially different platforms. Non-convexities play a major role in my arguments. To see the plausibility of non-convex preferences over multi-dimensional issue spaces take the example of a mayor who can spend 10 to improve the opera or the museum. Consider a voter V who is indifferent between either spending 10 on the opera or on the museum. Convexity would demand that V weakly prefers spending 5 on each of the institutions. This preference does not hold if V prefers one outstanding cultural institution to two mediocre ones. Besides this—standard—argument that preferences might not be convex there is another argument specific to preferences over issue spaces that lack a natural scale of measurement. While it might be easy to say which one of two foreign policies is more dovish and which one of two sets of abortion rights is more liberal, it is harder to say whether one policy is twice as dovish or liberal as another. But such statements are required in the definition of convex preferences. Since agents may disagree about the natural scale there may not be a common scale with respect to which all voters’ preferences are convex (even assuming that there is such a scale for each agent). The present model is not the first model of electoral competition with divergent equilibrium platforms. However, as far as I am aware, the emergence of different platforms is usually derived from an assumption of an exogenous ideological allegiance of parties and/or politicians, see Wittman (1973), Osborne and Slivinsky (1996), Besley and Coate (1997), and Roemer (1999). Similarly the candidates in Krasa and Polborn (2012) have some immutable characteristics that exogenously differentiate them. Another set of models goes a different route by prefacing Downsian competition among two parties by a stage in which these two competitors are selected (or threatened by the entry of a third), see Palfrey (1984) and Brusco et al. (2012). A more complex picture arises when the assumption that parties are uncertain about voter preferences is combined with the assumption that voters need not be certain about party positions. An influential paper, Glazer (1990), shows that both parties may prefer to state ambiguous policies that leave voters uncertain about their positions if the parties are uncertain about the median voter’s preferred policy position. Meirowitz (2005) and Aragones and Neeman (2000) add the candidates’ desire to remain flexible as an explanation of ambiguous platforms. If one allows for both voter and party uncertainty in a classical Downs–Hotelling model, equilibrium platforms might neither converge nor diverge: parties might refuse to declare precise platforms.",2
80.0,4.0,Theory and Decision,26 September 2015,https://link.springer.com/article/10.1007/s11238-015-9516-x,Maxmin weighted expected utility: a simpler characterization,April 2016,Joseph Y. Halpern,Samantha Leung,,Male,Female,Unknown,Mix,,
80.0,4.0,Theory and Decision,31 October 2015,https://link.springer.com/article/10.1007/s11238-015-9517-9,A characterization of the generalized optimal choice set through the optimization of generalized weak utilities,April 2016,Athanasios Andrikopoulos,,,Male,Unknown,Unknown,Male,"The theory of optimal choice sets is a solution theory that has a long and well-established tradition in social-choice and game theories. A choice process is presented by a choice function that associates with each feasible set of alternatives a certain subset of it, which consists of the optimal elementsFootnote 1 according to the viewpoint of a binary relation. A special type of optimal element is the Condorcet winner, which is an alternative that dominates every other alternative and is dominated by none. A Condorcet winner is unique whenever it exists, which unfortunately is not always the case. For this reason, a variety of general solution concepts, which generalize the notion of Condorcet winner, have been proposed. Smith (1973) introduces a generalization of the Condorcet Criterion which uses the notion of dominant set. However, Smith does not discuss the idea of the smallest dominant set. Fishburn (1977) narrows Smith’s generalization of the Condorcet criterion to the smallest dominant set and calls it Smith’s Condorcet Principle. Schwartz (1986) discusses Smith’s Condorcet Principle as a possible standard for optimal choices, and defines the generalized optimal choice set by using the notion of undominated set. The formulation of the problem of finding the optimal choice set turns out to be much easier to handle, when we use utility functions instead of binary relations. A utility function is a way of assigning a number to every alternative such that more-preferred alternatives get assigned bigger numbers than less-preferred ones. In economic and game theories, it is natural to assume the existence, for each binary relation R, of a continuous utility function u with certain properties that make it easy to maximize with usual mathematical tools. In this case, we say that u is a representation of R. The existence of a utility representation u for a binary relation R is important because every element that maximizes u is a maximal element for R. This means that the existence of maxima of u ensures the existence of non-empty choice sets or equilibrium points of mathematical economies and generalized games in different underline spaces. Therefore, we are often interested in establishing conditions under which a binary relation can be represented by a (weak) utility function. Peleg (1970) and Richter (1980) were the first to give for topological spaces sufficient conditions under which a strict partial ordering R on a set X has a weak utility function, i.e., a continuous real valued function f on the space, having the property xRy implies \(f(x)>f(y)\). Peris and Subiza (1995) give conditions under which an acyclic binary relation has a numerical representation function which guarantees the existence of maxima of such a function and, therefore, the existence of maximal elements of the binary relation. Another result of this kind is that of Rader (1963) and Campbell and Walker (1970). More precisely, Rader uses upper semicontinuous utility functions to represent preorders and thus the existence of the maximum is guaranteed. Campbell and Walker introduce the notion of weakly continuous relations and obtain a numerical representation for preorders satisfying such a condition. Traditionally, the problem of finding the optimal choice set of an arbitrary binary relation R is modeled by an R-space (Bosi and Herden 2006, p. 275), i.e., a topological space \((X,\tau )\) that is, in addition, endowed with the binary relation R. A natural requirement for R is to be acyclic. This is because (a): the existence of a weak utility that represents R implies that R is acyclic, and (b): acyclicity and existence of weak utilities are equivalent in finite or countable spaces (see Bridges 1983). On the other hand, a central problem that arises in R-spaces is the compatibility between the topology \(\tau \) and the binary relation R. In the literature, Nachbin’s compatibility is used, which is defined in (Nachbin 1965, p. 25) as follows: Nachbin’s compatibility is quite restrictive, however, in that, by definition, it leads to symmetrical topologies. That is, for an alternative \(x\in X\), the set of the elements that dominate x and the set of the elements that are dominated by x are open for the same topology. In many problems encountered in economic theory, the transition from a state x to a state y is not symmetrical to the transition from y to x. Thus, a generalization of Nachbin’s compatibility is needed. We need a different compatibility, where one topology can be used to move to the better choices and a different topology to move to the worse choices. In other words, we need two topologies, i.e., a bitopological space. This is consistent with the fact that every binary relation defines two topologies.Footnote 2 In general, a consideration of the bitopological spaces is natural for an understanding of the duality of relations (if R is a binary relation then \(R^{-1}\) is also a binary relation). There is also a natural identification of a topological space \((X,\tau )\) with the self dual (i.e, symmetric) bitopological space \((X,\tau ,\tau )\). Thus, bitopological results involving this duality look like topological theorems in the symmetric space. A more natural compatibility is described in (Andrikopoulos 2011a, Lemma 1.5) as follows: 
Let
\((X,\tau _{_1},\tau _{_2})\)
be a bitopological space endowed with an arbitrary binary relation
R
in
X. Then, for each
\(x\in X\), the set
\(\mathcal {L}_{_x}=\{ y\in X\vert xRy\}\)
is open for the topology
\(\tau _{_1}\)
and the set
\(\mathcal {U}_{_x}=\{ y\in X\vert yRx\}\)  is open for the topology
\(\tau _{_2}\). To see the importance of the new compatibility, let us consider the following example (see Peleg 1970, Example 2.1): Let Q be a binary relation in the plane \(\mathbb {R}^2\) endowed with the Eucledean topology \(\tau \), which is defined by Then, R is a partial order that does not have a continuous utility representation (see Peleg 1970, p. 94). But, by adding the discrete topology \(\tau ^{*}\) in the space \((\mathbb {R}^2,\tau )\) there emerges a bitopological space \((\mathbb {R}^2,\tau ,\tau ^{*})\), which satisfies the conditions of Theorem 1 (see Remark 1 below). This ensures the existence of an upper semicontinuous utility representation in the space \((\mathbb {R}^2,\tau ,\tau ^{*})\) that characterizes the maximal elements as maxima of the utility function. In this paper, a utility function is defined on bitopological spaces for arbitrary binary relations satisfying a condition that is weaker than that of weak utility, called generalized weak utility. The main result is the characterization of the Generalized Optimal Choice set in any subset of a bitopological space in terms of the maxima of a generalized weak utility.",
80.0,4.0,Theory and Decision,30 October 2015,https://link.springer.com/article/10.1007/s11238-015-9518-8,Consistent inconsistencies? Evidence from decision under risk,April 2016,Guillaume Hollard,Hela Maafi,Jean-Christophe Vergnaud,Male,Female,Unknown,Mix,,
80.0,4.0,Theory and Decision,27 October 2015,https://link.springer.com/article/10.1007/s11238-015-9519-7,Characterizations of weighted and equal division values,April 2016,Sylvain Béal,André Casajus,Philippe Solal,Male,Male,Male,Male,"The axioms employed to design values in cooperative game theory with transferable utilities can be divided up into punctual and relational axioms (see Thomson 2012). A punctual axiom applies to each game separately and a relational axiom relates payoff vectors of games that are related in a certain way. This article introduces one new punctual axiom and one new relational axiom. The well-established null player axiom and nullifying player axiom are punctual axioms. The former axiom recommends to assign a zero payoff to a null player, i.e., a player with zero contribution to coalitions. The latter axiom enforces a zero payoff to a nullifying player, i.e., a player belonging to coalitions with zero worth only. These two axioms play important roles since they enable distinguishing the Shapley value (Shapley 1953) from the equal division value, two values built on opposite equity principles (see van den Brink 2007). There exist few more axioms that rest on the null and nullifying players, or on variants of these types of players. Three examples are the null player out axiom (Derks and Haller 1999), the null player in a productive environment axiom (Casajus and Huettner 2013) and the nullified solidarity axiom (Béal et al. 2014). The first one is a relational axiom stating that removing a null player from a game does not affect the payoff of the remaining players. The second one is a punctual axiom that specifies to assign a non-negative payoff to a null player if the grand coalition has a non-negative worth. The third one compares a game before and after a specified player becomes null, i.e., the worth of a coalition in this new game is the worth of the coalition without the specified player in the original game. Nullified solidarity is a relational axiom requiring that if the specified player loses from such a change, then every other player should lose too, albeit the magnitudes of these losses can vary. In this article, we call upon the null player in a productive environment axiom and nullified solidarity, and introduce a variant of the nullifying player axiom called the non-negative player axiom. This axiom is a punctual axiom that requires to assign a non-negative payoff to a non-negative player, i.e., a player belonging to coalitions with non-negative worth only. Any nullifying player is also a non-negative player, but the nullifying player and non-negative player axioms are not related to each other, i.e., neither axiom implies the other. The vast category of relational axioms includes as a subclass the axioms of invariance. Such axioms specify either the same payoff vector or the same payoff for some specific players across games that are related in certain ways. Besides the null player out axiom, a well-known example of axiom of invariance is the axiom of marginality (Young 1985), which requires to attribute the same payoff to a player in two games where his marginal contributions to coalitions are identical. Further axioms of invariance are discussed by Béal et al. (2015a, b, c, d). We introduce a new axiom of invariance relying on the idea of bi-partitions, which dates back to von Neumann and Morgenstern (1953).Footnote 1 More specifically, our axiom of addition invariance on bi-partitions states that the chosen payoff vector should not be affected by an identical change in worth of both a coalition and the complementary coalition. We show that this axiom is equivalent to self-duality if one restricts to the domain of additive values, which means that it is satisfied by a lot of well-known values. We study the consequence of imposing some of the aforementioned axioms in addition to some classical axioms such as efficiency, additivity, linearity, or the equal treatment axiom. It turns out that the resulting values or classes of values have all in common to split efficiently the worth achieved by the grand coalition according to an exogenously given weight vector summing up to unity. We refer to the weighted division values when the weight vector can contain negative coordinates, and to the positively weighted division values for the subclass of weighted division values with non-negative weights. Naturally, the equal division value is the unique positively weighted division value with identical weights. All in all, the article contains ten characterizations of such values or classes of values. To the best of our knowledge, the only similar articles in cooperative game theory are due to van den Brink (2009) who obtains a characterization of the class of all weighted division values by imposing the axiom of collusion neutrality (see Haller 1994) in addition to linearity and efficiency, and Béal et al. (2015c), who characterize positively weighted division values (resp. positively weighted surplus division values) by means of efficiency, linearity and the axiom of invariance from player deletion in presence of a nullifying (resp. dummifying) player.Footnote 2
 The weighted division values constitute an interesting class of values for at least two reasons. First, although the requirement to treat substitute players equally appears to be natural in many situations, it is desirable to have the option of treating substitute players differently to reflect exogenous characteristics, such as income or health status. This can be achieved by incorporating exogenous weights into the construction of a value. Weighted values have been popularized by Kalai and Samet (1987) who study the weighted Shapley values. In a sense, the positively weighted division value generalize the equal division value as the weighted Shapley values generalize the Shapley value. Second, proportional division methods are very often employed in a lot of applications such as claim problems, cost allocation problems, insurance, law and so on. We refer to Tijs and Driessen (1986), Lemaire (1991), Balinski and Young (2001), and Thomson (2003) for rich surveys, and to Chun (1988), Moulin (1987), and Thomson (2013) for proportional division methods that rest on exogenously given weights. Our study exhibits further interesting aspects. From a theoretical point of view, the axiomatic characterizations of the equal division value always rest on at least one of the classical axioms of efficiency, the equal treatment axiom, or linearity/additivity. Some of our results avoid to use some of or all these axioms. As an example, Theorem 2 proves that the equal division value is characterized by addition invariance on bi-partitions, the nullifying player axiom, and weak covariance, where this last axiom is a weak version of covariance in the sense that the added additive game is symmetric. Moreover, two of our characterizations of the positively weighted division values give insight into the role of the equal treatment axiom in the characterizations of the equal division value. While the role of the equal treatment axiom is obvious in these two characterizations of the equal division value, it is more difficult to grasp in the characterization provided by van den Brink (2007). The rest of the article is organized as follows. Section 2 presents the basic material about cooperative games with transferable utilities. Section 3 introduces the axiom of addition invariance on bi-partitions, and contains all the results in which this axiom is invoked. Section 4 defines the non-negative player and nullified solidarity axioms, and offers the results mobilizing these axioms. A comparison with the main result in van den Brink (2007) is provided in Sect. 5. Section 6 concludes. Finally, the logical independence of the axioms used in each of our characterizations is demonstrated in the appendix.",14
81.0,1.0,Theory and Decision,02 November 2015,https://link.springer.com/article/10.1007/s11238-015-9521-0,A Condorcet jury theorem for couples,June 2016,Ingo Althöfer,Raphael Thiele,,Male,Male,Unknown,Male,"Should only one agent or a jury make an important decision? The Condorcet jury theorem (CJT) provides an answer for this question. This theorem is named after the French philosopher and mathematician Nicolas Caritat Marquis de Condorcet. His classical formulation can be found for instance in Grofman (1975), Miller (1986), and Boland et al. (1989). It includes the following assumptions. The agents of a jury have to decide between a good and a bad option through simple majority voting. Every agent chooses the better option with the same probability (homogeneity assumption). This probability is higher than one half, so the agents are assumed to be “experts”. Furthermore, their decisions do not influence each other (independence assumption). If a jury consists of an odd number of experts and satisfies the assumptions above, then the following two statements are valid. For one thing the jury is always more competent than each of its experts. And the jury competence converges to 1, if the number of experts goes to infinity. The first statement is also known as the non-asymptotic part of the CJT, whereas the second statement is called the asymptotic part. However, these assumptions are usually too restrictive compared to the real world. Various approaches exist, which generalize the classical CJT. Most of these extensions relax the assumption of homogeneity or independence. We give a review of such extensions in Sect. 2 and refer to Grofman et al. (1983), Berend and Sapir (2007) for other approaches. We present a modification of the classical model which relaxes both the homogeneity and independence assumption. For this, we use Bahadur (1961) representation of correlated Bernoulli random variables. However, we allow only second-order correlations (pairwise correlation). This approach is comparable to Kaniovski (2010) and Zaigraev and Kaniovski (2013). However, Kaniovski only investigated the homogeneous case with equal correlation coefficients between all pairs of experts, while Zaigraev and Kaniovski (2013) investigated the heterogeneous case with only positive correlation. In our model every expert is correlated with exactly one other expert (couple model). As a consequence most of the correlation coefficients are fixed to zero. Thus, we do not need the homogeneity assumption for the remaining correlation coefficients. Furthermore, we allow different individual competences; however, they must always be equal for the experts within a couple. We investigate the impact of the correlation coefficients onto the jury competence for given individual competences. We show that couples with positively correlated experts are harmful for the jury competence, whereas couples with negatively correlated experts are beneficial for them. These findings are comparable to results by Kaniovski (2010). Based on these results we establish the correlation coefficients which minimize and maximize the jury competence, respectively. Finally, tight lower and upper bounds for the jury competence result from these correlation coefficients. The paper is organized as follows. Section 2 contains a review of some generalizations of the classical CJT. In addition, we present the formulation of our new N-couple model. The main results of this model and some numerical examples are presented in Sect. 3. Section 4 contains two applications of the N-couple model. We conclude with a summary and discussion in Sect. 5.",2
81.0,1.0,Theory and Decision,30 October 2015,https://link.springer.com/article/10.1007/s11238-015-9522-z,‘Divide-and-choose’ in list-based decision problems,June 2016,Dinko Dimitrov,Saptarshi Mukherjee,Nozomu Muto,Male,Unknown,Male,Male,"Classical choice theory discusses the act of choosing one or more alternatives from a given set of alternatives. Empirical research on regular choice problems corroborates that often the choice behavior is affected by the structure with which the alternatives appear to the decision maker. In general, additional information contained in the decision making problem apart from the set of feasible alternatives makes an impact on the choice. Salant and Rubinstein (2008) refer this as a “framing effect”. In particular, there are many situations where the decision problem takes the form of choice from a list. Common examples would be buying decisions from an e-commerce website where the products are displayed sequentially, or selecting a combo-menu from a list of menus in some fast-food center. For such kind of problems, Rubinstein and Salant (2006) introduce a very intuitive axiom on choice functions from lists as to show that it is exactly the structure of the list that helps the decision maker select an alternative from a given set of alternatives. It is important to note here that the crucial assumption in the mentioned work is that the decision maker observes all alternatives in any given finite list. However, in many circumstances an individual might not want to continue the search till the end of the sequence. This, for instance, might not be feasible due to the specific way in which the alternatives are presented to her (cf. Diehl and Zauberman 2005). In such a case, the individual faces two different decision problems: she has to determine when to stop her sequential check, and which alternative to choose from those observed so far. Rubinstein and Salant (2006) fully characterize the  selection rule from the lists, while they do not allow the decision maker to stop before the list finishes. Our aim in this paper is to consider the problems of choice from lists when the decision maker has an option to terminate her search after observing any alternative in a list. In other words, we try to understand how the framing (in this case it is the “list”) influences both the aspects of decision-making: when to stop and what to choose. We provide an axiomatic treatment of the decision problem mentioned above and characterize a rule which we call a “divide-and-choose” rule. This rule outputs the chosen element for each list as well as the position at which the agent stops. More specifically, there are two main components of the procedure: a partition of the entire set of alternatives X into \(X_{1}\) (a set of “satisficing” elements) and \(X_{2}\), and a complete binary relation R on X with asymmetric and symmetric parts denoted by P and I , respectively. When an agent is going through a list, her stopping and choosing behavior is determined as follows: She stores the first element (say, \(\ell _{1}\)) of the list in a register and if \(\ell _{1}\in X_{1}\), then she immediately stops and chooses it. However, if \(\ell _{1}\in X_{2}\), then she moves on to the next element (say, \(\ell _{2}\)). Again, if \(\ell _{2}\in X_{1}\), then the agent stops and chooses it. If \(\ell _{2}\in X_{2}\) and \(\ell _{2}P\ell _{1}\), then she replaces the element in the register with the new list element; however, if \(\ell _{2}\in X_{2}\) and \(\ell _{2}I\ell _{1}\), then an indicator function (as defined in Sect. 2) guides the replacement of \(\ell _{1}\) by \(\ell _{2}\). The decision maker proceeds through the list in this way, taking into account the relation R, the mentioned partition of X, and the indicator function at each step. When she reaches the end of the list without observing a member of \(X_{1}\), then she chooses the alternative in the register. We note that this rule encompasses both the aspects of decision-making, i.e., when to stop (if the decision maker observes any element from \(X_{1}\), else she goes on till the end of the list) and what to choose (the binary relation R and the indicator function intensively account for the “order” in which the elements appear in the list). A special feature of this rule is that it can be seen as a unification of successive choice and satisficing choice as introduced in Salant (2003) and Simon (1955), respectivelyFootnote 1—the rule uses successive choice (applying the binary relation R and the indicator function) up to the point a satisficing alternative is observed. In that sense, the generation of satisficing alternatives (those in the set \(X_{1}\)) is axiomatically founded and derived from a decision maker’s observed choice and stopping behavior.Footnote 2
 We propose five behavioral axioms to prove the characterization result—choice data satisfy these axioms if and only if the decision rule is a divide-and-choose rule. The axioms restrict “stopping” and “choice” decisions in some particular circumstances when choosing from a list. With regards to the axioms, we also make an important assumption on the observability of data. We assume that not only the choice but also the point in the list when the decision maker stops is observable. This assumption on observability is a major one and completely in line with the approach taken by Eliaz and Rubinstein (2014) and also by Salant and Rubinstein (2008). In Eliaz and Rubinstein (2014), the choice procedure generates (apart from the chosen alternative) a “neuro-evidence”, that is, some additional observable information about the process that led the decision maker to those choices.Footnote 3 In our model this information is on the stopping point, i.e., where the decision maker stops her search in any list. This assumption on observable data paves the way for our main characterization result which encompasses rules dictating “when to stop” and “what to choose” from any list. Recently eye-tracking techniques are common in studying decision-making behavior, particularly in experimental set-up. Such techniques allow to study how consumers select products from various types of displays and thus, they help us to more precisely identify a stopping decision. Eye-tracking exercises primarily detect dynamics of eye movement of a consumer observing the products and provide a “scanpath” of these dynamics when the consumer observes the products in a sequence (in a supermarket or in online shopping portals). In general, as Knoepfle et al. (2009), Shi et al. (2013), or Wang et al. (2010) point out, an eye-tracking exercise helps eliciting unobservable private information of decision making. It is therefore possible to analyze the stopping behavior of consumers through eye-tracking exercises (see Stüttgen et al. 2012) which performs an eye-tracking exercise to record consumers’ search paths). Recently, influential papers like Caplin and Dean (2011) and Caplin et al. (2011) also acknowledge the usability of eye-tracking experiments in analyzing decision making behavioral patterns. As for our case, suppose for instance that the alternatives in a list are shops in a shopping mall and the decision maker visits the shops in a sequence. Then, it should not be difficult to extract (through an appropriate survey) the fact whether the decision maker would prefer to visit all shops or stop after visiting a few shops. The similar data can be extracted if the decision maker visits all items (displayed as a list) of an online shop or stops clicking “next” after visiting some items. We also note that the procedure of choosing by the decision maker is not observable. But by verifying the behavioral axioms proposed in this paper, we can identify the structure of decision-making or the choice procedures. For instance, for a decision maker whose choice data satisfies the axioms, by observing the stopping points in various lists one can identify the set of satisficing elements for the decision maker. Similarly, it is also possible to identify the underlying binary relation that drives the choice in any list. Thus, the axiomatization brings out the observable content of the divide-and-choose rule. We would like to stress at this point that our framework is purely ordinal. This is in sharp contrast for instance to the seminal work of Weitzman (1979) and the more recent works of Lim et al. (2006) and Smith et al. (2007). Weitzman (1979) considers a situation in which the decision maker has several boxes to open and each box has a reward that is a random variable. The decision maker can then choose to open the boxes in any  order and if she stops, the pay-off would be the maximum value found so far. Weitzman (1979) characterizes then the optimal decision strategy in this model. Lim et al. (2006) and Smith et al. (2007) consider a multi-attribute option search problem, in which options are characterized by value functions and the decision maker can choose to select an option, purchase information about an attribute value, reject the current option and continue the search, or terminate the search and accept a status quo outcome. Apart from being cardinal, this way of modeling exhibits the following crucial differences when compared to our framework: in the multi-attribute search problem, each option must be selected or rejected before continuing on to search for the next option; in contrast, in our model the decision maker is allowed to select any option from the observed elements up to her stopping point. More importantly, we built up on the properties of the revealed choice and stopping behavior as to derive the rule the decision maker is using in the problem. This in particular means that we do not need to make any assumption on the sort of beliefs the decision maker holds about the unobserved items (ex-post choice) in the list. Finally, at a more general level, the optimal threshold policy derived in these works can be seen as being of a similar spirit with the fact that our axioms lead to a partition of the set of options into satisficing and non-satisficing for the decision maker. However, these results have been derived in completely different ways—as a solution to a value maximization problem in the mentioned search models and as part of an axiomatic characterization in our framework, respectively.Footnote 4
 Our paper fits into the strand of literature on sequential choice procedures or choices with frames (cf. Rubinstein and Salant 2006; Salant and Rubinstein 2008; Eliaz and Rubinstein 2014; Mandler et al. 2012; Salant 2011; Yildiz 2015, among others), and it is related also to works dealing with the analysis of search problems in such a context. We provide an overview of these papers to indicate how our contribution is placed in the literature. 
Rubinstein and Salant (2006) introduce the idea of choice from lists and characterize a class of rules satisfying a simple axiom. However, as discussed earlier, their paper does not consider the “stopping behavior” while searching through a list of alternatives. But the basic framework or the structure of the set of alternatives in our paper is as same as theirs. Salant (2003) is the first to discuss a successive choice rule from lists, but the binary relation used in this rule is asymmetric and not necessarily complete unlike the one used in the characterization of our rule. This leaves our rule richer. Apesteguia and Ballester (2013) consider sequential choice procedures guided by routes that are generated endogenously, while the list structure is exogenously given in our model. On the other hand, Yildiz (2015) discusses “list rationalizable” choice functions: there is a fixed list such that for each choice set, the alternatives according to the list is compared successively and finally retrieves the chosen alternative. Again, in that paper the list structure is derived endogenously and used to compare the alternatives sequentially. Mandler et al. (2012) consider sequential shortlisting of alternatives through a checklist of desirable properties. The outcome of this eliminative process would result then in the final choice. Also in that paper the structure of the set of alternatives is not exogenously given to the decision maker. Masatlioglu and Nakajima (2013) focus on choice problems when the search path depends on an initial and externally observable reference point. Thus, in any two lists with the same initial alternative, the choice remains the same with their procedure. Additionally, Caplin and Dean (2011) characterize two types of search behavior (alternative-based and reservation-based), and consider a rich data set as to test their models. In both these papers, the set of alternatives is unordered. Let us however stress the fact that the mentioned works do not address the issue of how the given sequence of alternatives the decision maker is facing affects her stopping decision. The latter is also a major difference between our work and a recent relevant paper by Guney (2014). This paper considers choice from lists and characterizes a successive choice procedure from lists. But there is a very important distinction of the model in Guney (2014) from ours: Guney (2014) assumes that the decision maker observes all alternatives in any list, i.e. she does not stop before she comes to observe the last alternative of any list. Thus, Guney (2014) does not consider the decision maker’s stopping decision. In our model, the decision maker has an option to stop anywhere in the list, i.e. after observing any k number of alternatives in a list of length n, \(n\ge k\). Notice that one prime objective in our paper is to characterize the stopping decision (i.e. where to stop in any list) as well apart from the choice decision (i.e. what to choose from any list). Finally, Horan (2010) is also a relevant paper which considers choice from lists. In that work a choice function from lists (in the model with recall) is rationalized by a linear order unlike the binary relation used in our model. Thus, the order effects used in our paper are effective in a richer manner to determine the stopping point as well as the choice following a successive choice procedure. We believe that the paper contributes to the literature on choice theory and bounded rationality in several ways. To the best of our knowledge, our work is the first to provide axiomatic characterization of a stopping as well as choice rule from lists. As explained with the examples, this provides a foundation to cognitive decision-making in various business models where consumers may not want to search till the end of sequentially listed options. Moreover, the “observability” components in the axioms we introduce provide a wide scope for novel experimental design in investigating testable implications of stopping and choice behavior.",3
81.0,1.0,Theory and Decision,30 October 2015,https://link.springer.com/article/10.1007/s11238-015-9520-1,Veto players and equilibrium uniqueness in the Baron–Ferejohn model,June 2016,Levent Celik,Bilgehan Karabay,,Male,,Unknown,Mix,,
81.0,1.0,Theory and Decision,28 November 2015,https://link.springer.com/article/10.1007/s11238-015-9524-x,Transfers and exchange-stability in two-sided matching problems,June 2016,Emiliya Lazarova,Peter Borm,Arantza Estévez-Fernández,Female,Male,Female,Mix,,
81.0,1.0,Theory and Decision,23 December 2015,https://link.springer.com/article/10.1007/s11238-015-9527-7,Good manners: signaling social preferences,June 2016,Russell Golman,,,Male,Unknown,Unknown,Male,"People are social creatures. We care about each other, and we care about how others feel about us. To understand economic behavior, such as public goods contributions, employee relations, consumption of socially responsible products, and more, we must account for the role of social preferences in the choices people make. Economists have long recognized other-regarding preferences, including altruism (Becker 1976; Andreoni 1989), spitefulness, and reciprocity (Levine 1998; Fehr and Gächter 2000b; Charness and Rabin 2002; Sobel 2005; Falk and Fischbacher 2006; Ackermann et al. 2014). They have also recognized social image concerns (Holländer 1990; Bernheim 1994; Tadelis 2008; Andreoni and Bernheim 2009; Grossman 2015). It is tempting to try to attribute social behavior (such as giving freely to others) to one of these motives as opposed to the others, but this is a false choice—there is evidence that both kinds of social preferences are at play together (Bowles and Gintis 2005; DellaVigna et al. 2012).Footnote 1
 Certainly there is value in modeling either other-regarding preferences or social image concerns in isolation so that we may understand the specific implications of each. The seminal work of Benabou and Tirole (2006) shows the value of modeling them together because having image concerns along with other-regarding preferences opens up the possibility that extrinsic rewards can crowd out prosocial behavior by spoiling its reputational value.Footnote 2 This article contends that we get an additional insight from modeling both forms of social preferences together, i.e., from considering the interaction of other-regarding preferences with social image concerns. Together, these social preferences create a new incentive to communicate about one’s preferences. Fundamentally, this is because people’s beliefs directly affect their utilities (see, e.g., Akerlof and Dickens 1982; Köszegi 2006; Golman and Loewenstein 2015). People care about their beliefs—and about others’ beliefs—and communicate accordingly.Footnote 3
 Communication about one’s preferences can take many forms. Information can be conveyed through one’s actions, i.e., via costly signaling, which is generally acknowledged to be credible. Information can also be conveyed through one’s words, which are not directly payoff relevant. While such messages are sometimes dismissed as cheap talk, in certain cases they may actually be credible. Cheap talk messages can be “self-signaling,” i.e., naturally demonstrating their credibility by the fact that they were sent (see Farrell and Rabin 1996). Crawford and Sobel (1982) show that cheap talk is capable of revealing some truthful information, and Farrell and Rabin (1996) argue that it commonly does. We argue here that messages about how one feels about others (which are not directly payoff relevant and thus seem to be cheap talk) are often naturally credible. We characterize the signaling about social preferences that can emerge through cheap talk in a (partially) separating equilibrium. As in Crawford and Sobel (1982) and Farrell and Rabin (1996), cheap talk achieves credibility despite the opportunity to freely imitate others (i.e., to lie) because it is in the speakers’ interests in equilibrium to (partially) reveal their actual types (i.e., to tell the truth). Notably, whereas Crawford and Sobel (1982) and Farrell and Rabin (1996) consider situations in which people want to tell the truth because of the actions the listeners will take in response, in our setting people want to tell the truth because they care intrinsically about the listeners’ beliefs. Andreoni (1989, 1990) distinguishes warm-glow altruism from pure altruism, recognizing that people typically are not motivated just by concern for the welfare of others but also by the act of contributing to their welfare. Often people are more sensitive to their own role in helping others than to the need for that help. Our model captures warm-glow altruism as the product of pure altruism and the signal sent through the act of giving or helping. A desire to signal altruism or fairness has been recognized as one possible basis for warm-glow motivation (see Andreoni and Bernheim 2009). This signal might be sent to inspire reciprocity (Levine 1998) or simply to earn social approval (Andreoni and Bernheim 2009), or, as we additionally suggest here, to express social approval out of pure altruistic regard for another person. Our schema thus contributes to developing microfoundations for warm-glow motivation. This account of the warm glow can provide insight about the situations in which warm-glow motivation will play a role over and above pure altruism. Our perspective suggests that many social and economic phenomena should be seen as forms of communication about people’s social preferences (see also Fehr and Fischbacher 2002; Sliwka 2007). An illustrative example of this is good manners. Saying “I’m sorry” when one has hurt (or even failed to help) another person conveys that any harm done was not the object of one’s actions, but merely incidental to some other purpose. In many (though certainly not all) contexts, a person says “I’m sorry” expressly to make the victim feel better, and doing so credibly signals that the person bears no ill will, or else the person would not want to make the victim feel better. Similarly, saying “thank you” after receiving a favor conveys appreciation for the other person. Positive regard is generally valued for its own sake, and an expression of positive regard can be credible simply because it is given as a form of non-monetary reward. Of course, proper etiquette is a social norm. Most people are not consciously determining the precise message they would like to convey with their polite remarks. They may think of their behavior as simply conforming to established social norms. But social norms concerning manners arise as equilibria of games of communication. (Other social norms, e.g., concerning fairness, might arise from deep-seated distributional preferences or from a need to reduce conflict among members of a society who frequently interact, but etiquette is inherently about what others will think. See Kahneman et al. 1986; Camerer and Thaler 1995.) These norms make the meaning of arbitrary behaviors easily understood as conveying consideration or disregard for others, as a matter of common knowledge. It requires little or no deliberation to interpret ordinary good manners as polite and violations of norms of etiquette as rude. Etiquette may seem to be just a cute example of behavior that serves to signal people’s social preferences, but its economic relevance should not be dismissed. An organization’s efficiency and worker productivity often depend on the teamwork of employees. Organizational citizenship behaviors are essential for highly functioning teams (Organ et al. 2006), and many forms of organizational citizenship should be seen as expressions of proper etiquette. For example, an experienced worker may show a new hire the ropes, offer assistance with a challenging task, or simply introduce herself as part of the team, often as a gesture even if this help is not actually needed. Of course, it is undeniable that organizational citizenship behaviors (and conformity to norms of etiquette more generally) are sometimes strategically motivated, with one’s reputation for a repeated game in mind—e.g., covering for a co-worker in anticipation of future reciprocation. But, it would stretch credulity to believe that people always have such strategic motives in mind (see also Organ 1988). We do not generally observe workers on the day they retire leaving a mess for their colleagues to clean up, nor do we see all tenured professors resigning from their administrative committees. We could just say that it would feel wrong to do so, but the reason it feels wrong is the signal of disrespect it communicates.Footnote 4 We further discuss implications for organizational efficiency and worker productivity in Sect. 4, touching on the robustness of cooperation and gift exchange. We proceed by presenting a model of people with social preferences, entailing other-regarding preferences and social image concerns, in Sect. 2. Section 3 analyzes credible communication with cheap talk about social preferences, characterizing a (partially) separating equilibrium in the simplest setting. Section 4 discusses signaling of altruism and spite in more general settings and explores broader implications for our understanding of economic phenomena, before concluding.",3
81.0,1.0,Theory and Decision,12 January 2016,https://link.springer.com/article/10.1007/s11238-015-9530-z,"A note on monotonic power indices, smaller coalitions, and new members",June 2016,Dominik Karos,,,Male,Unknown,Unknown,Male,"The problem of measuring power in simple games appeared to be solved after the discoveries of the Shapley–Shubik index (Shapley 1953; Shapley and Shubik 1954) and the Banzhaf–Coleman index (Banzhaf 1965; Coleman 1971). Of course, these are not the only power measures (see for instance Fara et al 2014, for more on power measures), but they can be considered as the most important ones. However, both indices, when applied to games with different player sets, have properties that are not intuitive. These issues were first described by Brams (1975) and Brams and Affuso (1976) who showed, using the extension of the EU Council in 1973, that adding new players to a game might increase the power of some old players. Brams (1975) called this discovery the paradox of new members. 
Shenoy (1979) derived a similar result when applying power indices not only to the grand coalition but to each of its subcoalitions. Intuitively, players should have more power in smaller coalitions. However, it turned out that in some games there are smaller coalitions in which some players are worse off than in the original larger coalition. In this case, a power index would exhibit the paradox of smaller coalitions. The latter paradox played a crucial role in the theory of coalition formation in simple games. Shenoy (1979) showed that if a power index does not exhibit the paradox of smaller coalitions on a simple game, then there is a minimal winning coalition which is core stable—that is, which will not be left by any group of players. Dimitrov and Haake (2008) relaxed this condition and showed that it is sufficient that any coalition has at least one subcoalition in which each player is not worse off. This seems like a fair generalization of Shenoy’s result; however, I will show that in the case of monotonic power indices, it is still quite restricting. The initial work of Brams and Affuso (1976) derived results on how likely the paradox is to appear. I follow a different approach and investigate the connection between the paradox and the monotonicity of a power index. The relevant definitions can be found in Sect. 2; I also show there that the paradoxes of Shenoy and Brams are, in a sense, equivalent. Two versions of the paradox of new members are investigated in Sect. 3. In particular, it is shown that they are exhibited by any monotonic power index on (almost) every simple game that can be extended. The two versions of the paradox of smaller coalitions are under consideration in Sect. 4, where I show that they are exhibited if players are not almost symmetrical. The condition derived there is both sufficient and necessary for the Shapley–Shubik index. I close the article by briefly discussing the consequences of the incompatibility of monotonicity and the paradoxes’ not being exhibited.",
81.0,1.0,Theory and Decision,09 December 2015,https://link.springer.com/article/10.1007/s11238-015-9525-9,Cognitive ability and the effect of strategic uncertainty,June 2016,Nobuyuki Hanaki,Nicolas Jacquemet,Adam Zylbersztejn,Male,Male,Male,Male,"Coordination games provide a useful game-theoretical paradigm for analyzing a wide range of economic phenomena, such as macroeconomic fluctuations (Cooper and John 1988), bank runs and speculative currency attacks on financial markets (Morris et al. 2003; Heinemann 2012), and commercial production processes (Brandts et al. 2014). Because of the multiplicity and the Pareto-rankability of the Nash equilibria in these games, and because decisions are usually made in a state of strategic uncertainty regarding others’ behavior, the resulting outcomes can be driven away from the Pareto–Nash equilibrium—a phenomenon known as coordination failure. Coordination failure has been shown to be a persistent pattern in numerous lab implementations (Camerer 2003, Ch.7). The present paper contributes to a large body of experimental studies exploring this welfare-reducing phenomenon. A vast part of this literature is based on the core idea that coordination failures arise from strategic uncertainty, and various institutional designs are put forth as a remedy against it: introducing repeated encounters, varying the stability and the size of groups, providing information feedback, allowing for observation of others’ past behavior, or introducing pre-play communication between players (see also Devetag and Ortmann 2007, for an extensive survey of this literature.) However, although these mechanisms are usually found to improve efficient coordination, they fall short of completely solving the problem of coordination failure. In this paper, we take a further step to deepen our understanding of the nature of coordination failure. Our experimental results confirm that strategic uncertainty is an important determinant of the efficiency of strategic decision-making. Even more importantly, our experiment shows that individual cognitive ability has a strong link with the way strategic uncertainty influences the decisions of subjects in our experimental coordination games. Our investigation involves a classic \(2 \times 2\) coordination game, based on Selten (1975) and Rosenthal (1981), and is presented in Table 1. With \(L<S<H\), \(m<h\), and \(s < h\), the game is one-step dominance solvable: the elimination of Player B’s weakly dominated strategy l immediately leads to the Pareto–Nash equilibrium (R, r). Moreover, from the standard theory perspective, (R, r) is a natural candidate for a focal point, since it is also risk dominant.Footnote 1
 Notwithstanding these predictions, various studies have found a frequent failure to achieve the efficient equilibrium (see, e.g., Beard et al. 1994, 2001; Goeree and Holt 2001; Cooper and Van Huyck 2003; Jacquemet and Zylbersztejn 2014) both in sequential and simultaneous implementations of this game. Depending on the exact experimental setup, between 20 and 84 % of observed outcomes are not Pareto efficient. While the literature has long focused on the strategic uncertainty faced by Player A as the source of coordination failures in these experiments, recent evidence provides different clues for explaining this behavior. For example, Polonio et al. (2015) use eye-tracking data gathered from simple \(2\times 2\) games to demonstrate that some subjects do not pay attention to the payoffs of their opponent, and thus do not realize that the opponent has a dominant strategy. Thus, some Player As in our coordination game may choose L without taking Player Bs’ behavior into consideration at all—which precludes any meaningful role of strategic uncertainty on the decision-making of the former. Therefore, the first aim of this paper is to understand the extent to which deviations from strategy R by Player As is due to strategic uncertainty, which constitutes an important step towards designing more efficient mechanisms aimed at eliminating coordination failure. To address this issue we conduct a set of experiments based on four variations of a dominance solvable coordination game shown in Table 1, in which human subjects (acting as Player As) interact with Player Bs represented by either (a) other human subjects, or (b) a computer program. Computerized Player Bs are programmed to always choose r, and this fact is clearly explained to the subjects. Therefore, subjects acting as Player As interacting with computers do not face any strategic uncertainty, which provides an empirical benchmark for assessing the effect of strategic uncertainty on Player As’ behavior in human–human interactions. In this sense, our experiment is related to a recent and growing body of experimental studies that seek to separate and evaluate the behavioral effect of strategic uncertainty in collective decision-making. For example, in an alternating bargaining game, Johnson et al. (2002) investigated the effects of two potential causes for failure in backward induction: confusion or other-regarding social preferences. They found evidence that confusion was an important cause of deviations from the equilibrium outcome. Houser and Kurzban (2002) and Ferraro and Vossler (2010) do the same in public good contribution experiments, and estimate that confusion explained up to around one half of contribution levels. Fehr and Tyran (2001) focused on the strategic aspects of “nominal illusion”. They considered four-player repeated price setting games, and introduced a negative nominal shock in the middle of the experiment. They found that roughly half of non-immediate adjustment to the new equilibrium after the shock was due to individual bounded rationality (or confusion) and the other half was due to strategic uncertainty. Finally, Akiyama et al. (2015) investigated the magnitude of the effect of strategic uncertainty in explaining the observed deviation of price forecasts from the fundamental values in an experimental asset market à laSmith et al. (1988). They found significant effects of both confusion and strategic uncertainty.Footnote 2
 Our second objective is to shed new light on the relationship between cognitive ability and strategic thinking. In particular, we investigate whether the failure to seek efficiency by choosing R is more widespread among Player As with low cognitive ability than for those with high cognitive ability. We address this question by conducting a cognitive ability test in several experimental sessions (involving both human–human and human–robot interactions), and correlate the efficiency of observed behavior and subjects’ test scores, while controlling for the presence of strategic uncertainty. From this perspective, our study contributes to recent literature that investigates the relationship between subjects’ cognitive ability and their degree of strategic sophistication. For example, Brañas-Garza et al. (2012) reported that subjects with higher scores on the Cognitive Reflection Test (CRT, Frederick 2005) choose, on average, numbers closer to the Nash equilibrium in the beauty contest games. In the same vein, Akiyama et al. (2015) reported that the magnitude of the effect of strategic uncertainty is positively correlated with subjects’ scores on the CRT test, while the effect of confusion is negatively correlated with the score. Burks et al. (2009) reported that subjects (trainee truckers) with higher scores in Raven’s progressive matrix testFootnote 3 are more patient and more willing to take calculated risks.Footnote 4 In addition, they reported that subjects with higher Raven’s test scores more accurately predict others’ behavior in a sequential prisoners’ dilemma game, and better adapt their behavior to others’ behavior. Carpenter et al. (2013) showed that subjects with higher scores in Raven’s test more frequently win in “Race to 5, 10, or 15” gamesFootnote 5 and guessed others’ choices better in a 20-player beauty contest game. Finally, Gill and Prowse (2015) also reported that subjects with higher scores in Raven’s test not only choose numbers closer to the equilibrium in a repeated 3-player beauty contest game, but also respond to the average score of other subjects in the group by choosing number close to the equilibrium when facing with others with higher scores than when facing with others with lower scores. Fehr and Huck (2015) reported similar results from a beauty contest game. They found a critical threshold of cognitive ability (measured by CRT) below which subjects choose random numbers and do not respond to their beliefs about others’ cognitive ability. Subjects with cognitive ability above this threshold, however, tend to act much more strategically: they systematically choose lower numbers and respond to their beliefs about the cognitive ability of other players. Finally, recent evidence from psychological research reveals the relationship between fluid intelligence and the theory of mind (Ibanez et al. 2013).Footnote 6
 To sum up, these empirical studies suggest that people with high cognitive ability respond more aptly to strategic conditions they face than those with low cognitive ability. The present study extends this investigation to a new and important economic environment—the coordination game. As will be seen, we find that Player As’ failure to choose R can be only partially explained by uncertainty about their partners’ intentions: in many cases, the former act in this manner even when interacting with a computer program that is known to always act reliably by choosing r. We also report that Player As with high cognitive ability (measured in terms of Raven’s test scores) tend to be more sensitive to strategic uncertainty than those with low cognitive ability.",20
81.0,1.0,Theory and Decision,26 November 2015,https://link.springer.com/article/10.1007/s11238-015-9526-8,Minimizing regret in dynamic decision problems,June 2016,Joseph Y. Halpern,Samantha Leung,,Male,Female,Unknown,Mix,,
81.0,2.0,Theory and Decision,22 January 2016,https://link.springer.com/article/10.1007/s11238-016-9534-3,Peter Fishburn’s analysis of ambiguity,August 2016,Mark Shattuck,Carl Wagner,,Male,Male,Unknown,Male,"In ordinary discourse the term ambiguity typically refers to vagueness or imprecision in a word or phrase of some natural language. Since its introduction in the now classic paper by Ellsberg (1961), however, this term has been used by most decision theorists to refer to imprecision in an individual’s probabilistic judgments, in the sense that the available evidence is consistent with more than one probability distribution over the possible states of the world. Abstaining from a prior commitment to either of these interpretations, Fishburn (1991) has explored ambiguity as a primitive concept, through an analysis of mappings \(a:2^{\Omega }\rightarrow [0,\infty )\), where \(\Omega \) is a finite set of possible states of the world, and a(A) denotes what he calls the ambiguity measure of A, characterized by the following axioms : These axioms arise from a subtle measurement-theoretic analysis of the binary relation \(\gg \) on \(2^{\Omega }\), where \(A\gg B\) asserts that the event A is at least as ambiguous as the event B. Fishburn states and justifies six axioms for this relation and proves a representation theorem to the effect that \(\gg \) satisfies these axioms if and only if there exists a mapping \(a:2^{\Omega }\rightarrow [0,\infty )\) such that \((i.) \, A\gg B \, \Leftrightarrow a(A)\ge a(B)\) and (ii.) axioms (1.1)–(1.5) hold. In this paper, we investigate the extent to which axioms (1.1)–(1.5) are consistent with the notion of ambiguity as linguistic vagueness, as contrasted with probabilistic imprecision. The paper is structured as follows: In Sect. 2 we outline pertinent results from the theory of lower and upper probabilities, conceived, following Smith (1961) and Walley (1991), as threshold buying and selling prices for certain bets. We explain why it is desirable that an individual’s lower probability \(\lambda \) and upper probability \(\upsilon \) should coincide, respectively, with the lower envelope and upper envelope of some nonempty set P of probability measuresFootnote 1 on \(2^{\Omega }\), in the sense that If \(\lambda \) and \(\upsilon \) are an individual’s lower and upper probabilities on \(2^{\Omega }\), the mapping \(a=\upsilon -\lambda \), which Walley calls the degree of imprecision of the pair \((\lambda ,\upsilon )\), and which is in essence a bid-ask spread, is a natural candidate for an ambiguity measure. We prove, however, that if \(\lambda \) and \(\upsilon \) are lower and upper envelopes, the mapping a, so defined, may fail to satisfy axiom (1.3), although it always satisfies axioms (1.1) and (1.2) . We further show that axiom (1.4) may fail to hold even when \(\lambda \) is a highly structured type of lower envelope known as a belief function and \(\upsilon \) is its corresponding upper envelope, known as a plausibility function (Shafer 1976), and that the same is true for axiom (1.5). We conclude by describing the special class of consonant belief functions, which reappear in a different guise in Sect. 3, where we consider the numerical representation of linguistic vagueness by means of fuzzy membership functions and their associated necessity and possibility measures (Dubois et al. 2000). We show that the class of necessity measures is identical to the class of consonant belief functions, and we argue that if \(\lambda \) and \(\upsilon \) are necessity and possibility measures on \(2^{\Omega }\), then \(a=\upsilon -\lambda \) is a natural candidate for a measure of linguistic vagueness. In Sect. 4, we prove that \(\upsilon -\lambda \) satisfies the complete set of axioms (1.1)–(1.5) when \(\lambda \) is a necessity measure and \(\upsilon \) is its corresponding possibility measure. In Sect. 5 we offer a brief summary and conclusion.",2
81.0,2.0,Theory and Decision,05 November 2015,https://link.springer.com/article/10.1007/s11238-015-9523-y,Lexicographic expected utility without completeness,August 2016,D. Borie,,,Unknown,Unknown,Unknown,Unknown,,
81.0,2.0,Theory and Decision,22 December 2015,https://link.springer.com/article/10.1007/s11238-015-9529-5,Choquet expected utility with affine capacities,August 2016,Pascal Toquebeuf,,,Male,Unknown,Unknown,Male,"In the psychological literature, several experimental evidences (see, recently, Madan et al. 2014a, b) suggest that individuals tend to obey an “extreme outcome rule”: they overestimate the likelihoods of largest gains and largest losses. An economic example of this behavioral bias may be a market entry decision in which you have to consider whether or not to start a business. As you weigh up the pros and cons, you tend to place too much weight on extremely positive outcomes (you become a monopoly) or extremely negative ones (you exit the market after having made sunk expenditures), whereas intermediate options could be much more likely. Decision theory distinguishes risk and ambiguity. Whereas the former concerns those situations where probability distributions on outcomes are perfectly known, the latter relates to situations where the decision maker (DM) is not sure about the true probability law. Several decision models rationalizing the extreme outcome rule have been proposed in both settings. For decisions under risk, Cohen (1992) proposes an axiomatic model in which the DM attaches special importance to the security level (worst outcome) and the potential level (best outcome). For decision making under ambiguity, a popular model is the Choquet Expected Utility (CEU), in which individual beliefs are represented by a Choquet capacity. Chateauneuf et al. (2007) have axiomatically characterized a refinement of Choquet capacities, named neo(non-extreme outcomes) capacities. Here, the preference representation is a convex combination of the expected utility of the lottery and its maximal and minimal outcomes. In this paper, we axiomatically characterize a specific class of Choquet capacities that is essentially an affinity of a subjective probability. Our main axiom, Extreme Outcome Sensitivity (EOS), restricts Savage’s Sure Thing Principle to acts yielding the same extreme utilities. CEU with respect to an affine capacity allows one to account for the extreme outcome rule, since it consists in an affine combination of the highest utility level, the lowest utility level and the expected utility of the valued act. To see this, let f be a real-valued random variable taking finite values. When extreme outcomes do not occur on events which are judged impossible by the DM, the criterion we propose values the “act” f as: where \(a\in \mathbb {R}\) and \(b>0\) are parameters, and \(E(f\vert \pi )\) denotes the mathematical expectation of f with respect to the subjective probability \(\pi \). The Choquet expectation with respect to an affine capacity is not additive on extreme values \(-\max f\) and \(\min f\), whereas intermediate results are weighted by the probability measure \(\pi \). Parameters a and b reflect how the DM reacts to extreme outcomes as opposed to non-extreme outcomes. Such a decision model slightly generalizes the approach of Chateauneuf et al. (2007). Affine capacities are a sub-class of generalized neo-additive capacities (GNACs)Footnote 1 proposed by Eichberger et al. (2012). These authors show that, for sequential decisions, the axioms of consequentialism, state-independence, null-event consistency and conditional certainty equivalent consistency characterize GNACs updated by the full Bayesian update rule. Hence, the question of the axiomatic characterization of GNACs and affine capacities within a static setup, similar to the one used by Chateauneuf et al. (2007), is left open. Such capacities would be then consistent with any updating rule for Choquet capacities. The remainder of the paper is organized as follows. Section 2 presents our decision framework. In Sect. 3, we axiomatically characterize CEU with respect to an affine capacity. Section 4 links our result to the existing literature. Section 5 concludes.",
81.0,2.0,Theory and Decision,23 December 2015,https://link.springer.com/article/10.1007/s11238-015-9528-6,Rational beliefs in rationalizability,August 2016,Xiao Luo,,,,Unknown,Unknown,Mix,,
81.0,2.0,Theory and Decision,01 February 2016,https://link.springer.com/article/10.1007/s11238-016-9535-2,Approval elections with a variable number of winners,August 2016,D. Marc Kilgour,,,Unknown,Unknown,Unknown,Unknown,,
81.0,2.0,Theory and Decision,09 February 2016,https://link.springer.com/article/10.1007/s11238-015-9532-x,Counting votes in coupled decisions,August 2016,Andreas Wendemuth,Italo Simonelli,,Male,Male,Unknown,Male,"Decision processes have attained increased attention recently. As an example, almost all democracies have bicameral legislative bodies where decisions must be passed by two chambers. Another issue are majority voting rules with quorums, a characterization of which can be found in (Houy 2009). A more complicated example are the regulations of the Lisbon Treaty which defines so-called “qualified majorities” in European Union decisions, starting from 2014 (Lisbon Treaty 2007). Where most decision theories deal with independent choices of the subjects in the electorate (“voting games”), there is also the case that the same group of persons is assigned to multiple decision bodies in personal union, as it is common, for example, in supervisory boards of stock corporations. In the past, personal union had been so much over-used that legislation had to be adopted to bound it [e.g., in Germany the Lex Abs (1965)]. These decision processes make it hard to assess which fraction of possible votes of the electorate (“constellations”) will lead to an adoption of a given cause. In one deciding body, the necessary quorum can be simply fixed, making it transparent how many constellations will lead to a 50 % or 2/3 majority. In coupled decision processes, this cannot be seen as easily. The formal issue of this paper is derived from these decision processes as follows. We are interested in the adoption rate, i.e., the ratio of those constellations which satisfy a given quorum, relative to all possible constellations. In a wider sense, this can be mapped to the notation in cooperative game theory [see, e.g., Brandenburger (2007)]. There, a set of N players would be assigned. One addresses all possible subsets S (“coalitions” or “constellations”) of the N players. A characteristic function v maps every constellation in this power set of N to a “value” which in our case is a constant \(2^N/Q\) whenever a given “rule of the game” is satisfied by the constellation, and 0 otherwise. The number Q is the total number of constellations (out of \(2^N\) many) which do satisfy the rule. In many questions in game theory, one is interested in more complex value functions and in corresponding optimal constellations, respectively, coalition sets S. Particular coalitions can then be found which maximize social welfare, respectively, exhibits great voting power. From the large body of literature on this topic, we mention exemplarily Penrose (1946), Shapley and Shubik (1954), Banzhaf (1965), Coleman (1971), Deegan and Packel (1978), Holler and Packel (1983), Hodge and Klima (2005) and Brams (2013). In the current paper, every constellation which obeys the game’s rule has the same value and hence is equally favorable. The question is rather how favorable such constellations are, in the sense that if there are only few constellations which obey the rule, a higher value should be assigned to each of them. This is achieved by the normalization inherent in the assignment of Q, namely \(\sum _{\{S\}} v(S) = 2^N\), which ensures that the total value assigned in the game equals the number of possible coalitions and hence provides the correct scaling for arbitrary sizes of N. The magnitude of the value is only clear with knowledge of the number Q, which is far from trivial. Hence, the direction of research in this paper is the calculation of Q. The complexity of this calculation stems from the complexity of the underlying rule of the game. As we shall see, in the case of “games” with a multi-chamber/ multi-majority “rule”, the calculation of Q can become very complex even for seemingly simple rules. Since Q determines the value of a coalition, the desired result is that this value can become very high if the rule can be satisfied only for very few coalitions, an effect which cannot be read off from the rule by simple inspection. Clearly then, the inverse of the value is the adoption rate given above. Obviously, one can simply give the adoption rate by direct counting. For a multi-chamber and/or multi-majority quorum, coupled with personal union, however, these countings will get very involved, comprising many sums over multiple binomial or multinomial factors (multi-chamber), where the summation indices are restricted by multiple inequalities (multi-majority quorum). Clearly, in almost all—but very few special—cases these summations cannot be performed in closed form. Therefore, the aim of this paper is to introduce a general method which allows to give the adoption rate while: reducing the multiple sums to fewer ones, preferably to one sum; providing unrestricted summations over the full index range, removing the index restrictions generated by multi-majority inequalities; giving a general construction method for arbitrary problems of this nature. Since multiple inequalities have to be observed, we have to evaluate partial sums of the entry of a row of a Pascal triangle, which is a special case of lacunary sums for which no closed form exists [see (Graham et al. 1989) and (Petkov s̆ ek and Wilf 1997)]. Hence, the following results are maximum contractions of the original problem in the sense that no further closure is possible. The presented problem class also cannot be treated with automatized solutions of the hypergeometric series family (Petkov s̆ ek et al. 1996). Our described method belongs to the class of double counting methods (Aigner et al. 1998). As we shall see, the essence of the transformed counting is to separate unrestricted counting in independent decision bodies (“chambers”) and independent counting over each (in)equality restriction. The paper is organized as follows: For better understanding, the method can be accessed inductively. First, five dedicated problems are introduced. Theorems for these problems are proven in the appendix. In Sect. 5, the general formulation is given, which can clearly be deduced from the individual problems. A conclusion ends the paper. Leading paragraph numbers in square brackets, like [1], refer to the number of the individual steps in the general proof of theorems, Sect. 5. This is a guidance for the reader who wishes to regard the proofs of the theorems 3–7 as special cases and wants to follow the general outline of Sect. 5. Having done so will result not only in a theoretical, but also in a practical benefit: With C independent decision bodies (“chambers”), naive counting will result in \(\sum ... \sum \)(multinomial expression in independent decision variables) with C such sums, each sum comprising multiple, in general \(e_c\), indices which, for every chamber c, sum up to \(N_c\), the number of members of these chambers. The sums’ index ranges will be restrained by multiple inequalities. A rough estimate for the number \(E_C\) of evaluations of different binomials (a multinomial will be counted as \(e_c\) binomials) is then where this number is reduced, albeit only by an algebraical factor, by exploiting symmetries, by nesting sums, and by observing the cumbersome multiple inequalities. Clearly, for rising numbers \(\left\{ N_c\right\} \), \(E_C\), and the associated computation time, will become prohibitively large. In contrast, after conversion, D independent decision bodies remain, where for each body a—generally more involved—expression with \(f_d\) different binomials will have to be computed, where generally \(f_d > e_d\). We obtain many evaluations of different binomials. The leading sum in Eq. (1.2) will generally be larger than the one in Eq. (1.1), however if \(D < C\), \(E_D << E_C\) will result for reasonable \(\left\{ N_c\right\} \). For all five dedicated problems evaluated in this paper, a conversion to \(D=1\) with \(e_1 = 1\) could be obtained, hence \(E_{D=1} = f_1 N_1 \). For these five problems, values for \(E_C\) and \(E_D\) will be given in the following. Further, after conversion the sums are not restricted by index inequalities any more, hence no case evaluation has to be performed. This, and the major reduction to \(E_D\), shows the power of the method.",
81.0,2.0,Theory and Decision,31 December 2015,https://link.springer.com/article/10.1007/s11238-015-9533-9,The promise of pick-the-winners contests for producing crowd probability forecasts,August 2016,Phillip E. Pfeifer,,,Male,Unknown,Unknown,Male,"The notion that a crowd of forecasters usually does better than a single expert was popularized in the wisdom of crowds (Surowiecki 2006). This idea encouraged some organizations to find clever ways to harness that wisdom and helps explain why prediction markets have received so much attention (Wolfers and Zitzewitz 2004). Prediction markets are markets set up for the purpose of constructing a probability forecast for some quantity of interest to the organization. When that quantity is 0/1 (the outcome of some future binary event), the goal of the prediction market is to produce a crowd probability forecast. The focus of this paper is to explore the potential of a much simpler mechanism for producing crowd probability forecasts—pick-the-winners contests (PWCs). Our inspiration for this exploration came from the LaxPower\(^{\mathrm{TM}}\) website, which for several years has conducted a PWC in which players register (for free) and over the course of several weeks pick the winners of selected U.S. men’s college lacrosse games. The overall winner was the player with the most correct picks, and weekly winners were also announced. No prizes were awarded. Players entered and competed for the reward of potentially seeing their username published as the winner. Table 1 is a summary of the first week of picks from the 2011 contest. Notice first that the crowd proportions (the fraction of players picking the home team) behave like probabilities in that they are positive and between 0 and 1. Notice next that even though Syracuse, Virginia, and Cornell were judged by most to be the best teams, their opponents (the underdogs) always managed to receive some picks. This suggested to us that these crowd proportions might be better calibrated than the usually overconfident probabilities received from untrained experts (Lichtenstein et al. 1982). This paper has three purposes: (1) to examine how well these LaxPower crowd proportions perform if used as probability forecasts (to that end, we compare their performance to that of a fairly sophisticated statistical model. The crowd proportions outperformed the statistical model); (2) to provide theory that helps explains why the crowd proportions did so well; and (3) to explain why some players pick the underdog even though doing so shifts the distribution of their number of correct picks to the left. The paper is organized as follows. Section 2 defines pick-the-winner contests and describes the notation and terminology used throughout the paper. Section 3 examines the theoretical properties of PWCs when the probabilities of contest events are known. It is in this section that we explain why it is in a player’s best interest to sometimes pick the underdog. Section 4 considers contests wherein probabilities are unknown using a simple model of overlapping information in which players have both public and private information about each contest game. We also present a closed-from expression for the mean squared error of crowd proportions used as probability forecasts under a reasonable assumption about player behavior. This result shows the promise of PWCs in that a large crowd of experienced players acting in their own best interests produces crowd proportions that do quite well when used as probabilities. Section 5 presents the empirical analysis of 6 years of LaxPower picking contests where we find that the crowd proportions do better than probabilities from a sophisticated statistical model. The paper concludes with a summary.",7
81.0,2.0,Theory and Decision,24 December 2015,https://link.springer.com/article/10.1007/s11238-015-9531-y,"Satisficing, preferences, and social interaction: a new perspective",August 2016,Wynn C. Stirling,Teppo Felin,,,Male,Unknown,Mix,,
81.0,3.0,Theory and Decision,03 March 2016,https://link.springer.com/article/10.1007/s11238-016-9542-3,Nash was a first to axiomatize expected utility,September 2016,Han Bleichrodt,Chen Li,Peter P. Wakker,,,Male,Mix,,
81.0,3.0,Theory and Decision,30 March 2016,https://link.springer.com/article/10.1007/s11238-016-9544-1,"Ambiguity attitudes, framing, and consistency",September 2016,Alex Voorhoeve,Ken Binmore,Lisa Stewart,Male,Male,Female,Mix,,
81.0,3.0,Theory and Decision,04 March 2016,https://link.springer.com/article/10.1007/s11238-016-9543-2,A robust resolution of Newcomb’s paradox,September 2016,Thomas A. Weber,,,Male,Unknown,Unknown,Male,"Suppose a “superior being” claims to be able to predict an agent’s action and, conditional on this prediction, influence his payoffs in a simple game of choice. In this game, there are two boxes, labelled I and II. Box I is transparent and contains a positive monetary reward r. Box II is opaque and, unbeknownst to the agent, the being puts in it either a monetary reward R (where \(R>r\)) or no reward at all. The reward in box I remains untouched. The agent faces the choice of either taking both boxes or taking only box II; his payoff consists of the rewards in the boxes he takes. The being claims that box II will contain R if and only if the agent takes only box II. What should a rational agent do? This problem, devised by William A. Newcomb in 1960 (Gardner 1973) and published by Nozick (1969), is now referred to as Newcomb’s paradox because for both of the agent’s feasible actions a priori reasonable justifications have been advanced (see also Gardner 1974). Taking both boxes (the “two-box strategy”) seems plausible because of the following dominance argument: no matter what box II contains, adding the reward of box I increases the agent’s payoff. Taking only box II (the “one-box strategy”) may be plausible because a superior being by simulating the agent’s decision process could anticipate the two-box strategy, which would therefore lead to no reward in box II and thus a payoff of r, which in turn is less than the reward R the agent could earn by only taking box II. Lastly, to discourage randomization on the part of the agent, the being claims to put no reward in box II whenever the agent’s choice emanates from a mixed strategy, i.e., a randomization over his two pure strategies (Nozick 1969, pp. 115/143, endnote 1). In this paper, we resolve Newcomb’s paradox using game theory based on a belief the agent should reasonably form about the structure of the game, specifically about the probability p that the being is informed about the agent’s action before placing the reward. We first consider a risk-neutral agent. Depending on whether the belief probability exceeds a certain threshold or not, the agent will prefer either the one-box strategy or the two-box strategy. If the being can observe this belief, then by anticipating the agent’s choice the being can ensure perfect prediction on the equilibrium path, where, perhaps somewhat unexpectedly, an agent who thinks ex-post intervention of the being unlikely, ends up with the low reward r in the unique Nash equilibrium of the game. If the belief p belongs to the agent’s private information, then the being can devise a mechanism that leads to full information revelation, guarantees perfect prediction, and implements the (first-best) one-box strategy, given the two additional assumptions that the agent’s belief is always strictly positive and that the agent believes the being may be able to inflict a negative payoff to discourage deviations which therefore never occur in equilibrium. If the agent is not risk-neutral, his optimal decisions are generally described by two different thresholds, corresponding to whether, in equilibrium, R ends up in box II or not, should the being move first. Still, the being can solve Newcomb’s problem with zero prediction error in equilibrium unless the agent is risk-seeking. Faced with a risk-seeking agent of intermediate belief, the being cannot maintain a zero prediction error in equilibrium without relying on noncredible threats. 
Nozick (1969) formulates Newcomb’s paradox and links it to decision theory without offering a resolution; he leans towards the two-box strategy on account of the “dominance principle” which he also defends in later discussions (Nozick 1993, 1997). In the first account of the problem, rewards are set to \(r=\$1{,}000\) and \(R=\$1\) million, and these amounts have been largely maintained in the subsequent literature. Bar-Hillel and Margalit (1972) argue for the one-box strategy, stating that the “dominance principle looses its appeal when applied to situations where the states of the world (...) are affected by the decision maker’s actions” (p. 297). Their solution invokes game theory where the being acts in order to maximize the probability of being correct. However, they assume this probability is given exogenously, which makes it generally inconsistent with the proposed equilibrium of the game. Schlesinger (1974) points out that the one-box strategy might be interpreted as backward causation, questioning the possibility of free will, which he considers in turn inherently unpredictable.Footnote 1 He then attempts to rationalize the two-box strategy based on the agent’s following a well-meaning observer’s imaginary advice. Benditt and Ross (1976), Locke (1978) and Horgan (1981) give largely intuitive counterarguments. Sorenson (1983) and, more recently, Burgess (2004) defend the two-box strategy by separating in time the agent’s conclusion about the contents of box II and the eventual possibility of revising his decision to also take box I. Horwich (1985) argues for the “evidential principle” and thus the one-box strategy because “the choiceworthiness of an act depends solely upon its likelihood of being associated with desirable events” (p. 432), so the agent should maximize expected utility rather than following “causal decision theory” which relies on the additional assumption that his “act might be a cause of the desired outcome” (ibid.). The counterargument by Sobel (1988) invokes the possible endogeneity of the probability that the being predicts the agent’s action correctly, for the agent a priori cannot “be sure that he cannot falsfify the prediction whatever it is” (p. 20). In the early discussions of Newcomb’s paradox, the being’s accuracy in predicting the agent’s choice is justified by past observations of this predictive performance. Schlesinger (1974) argues that no amount of inductive evidence is able to increase the agent’s confidence in the being’s prediction performance. Brams (1975) discusses some analogies to the prisoner’s dilemma game,Footnote 2 which are again predicated upon assuming an exogenous probability of the being’s correctly predicting the agent’s choice which generally turns out to be inconsistent with the pure-strategy equilibrium.Footnote 3 The viewpoint in this paper is different, in that the being can in the end perfectly forecast the agent’s choice, simply based on a robust assumption of common knowledge about the game that is being played (Lewis (1969); Aumann 1976; Brandenburger 1992). No performance data on repeated versions of the game or alternative justification of the superior being’s predictive abilities are required other than the ability to determine the equilibrium of a game, which is then realized based on rational expectations (Muth 1961) on the part of the agent and the being. 
Frydman et al. (1982) and Broome (1989) point out that the problem of a government committing to a policy which tries to implement measures contingent on agents’ actions taken under the policy may be akin to Newcomb’s problem. Indeed, when the agents anticipate the implemented policy measures, their actions may tend to undo the effect of these measures. Broome (1989) explains this using the example of monetary policy, where expanding the money supply leads to increased employment (ranked best) as long as this measure is unexpected, and otherwise to inflation (ranked third-best). If, on the other hand, the government does not expand the money supply, then provided an expansion is expected, it leads to a recession (ranked worst) and otherwise to no change at all (ranked second-best). As a result, assuming rational expectations on the part of the agents, the best the government can do is to obtain inflation, corresponding to the third-best outcome. As Frydman et al. (1982) point out, this dilemma relates to a problem discussed in a seminal paper by Kydland and Prescott (1977) where a government seeks to find an optimal dynamic policy. Because it cannot commit to its future actions, any time-consistent policy [which it has an incentive to adhere to at any future time; see Weber (2011), p. 191] is suboptimal compared to what could be achieved with full commitment, when agents believe that the initially announced policy remains unchanged. 
Sugden (1991) points out that in dynamic games actions can be time-inconsistent, even though commitment may at the outset be in a player’s best interest. He compares Newcomb’s paradox to the “toxin” puzzle, where a player is promised a large reward based on his current intention to drink a nonlethal toxin after having received the reward. More importantly, Sugden (1991) also notes that in games, other players’ actions are generally endogenous and cannot, in principle, be viewed as mere lotteries. Hence, a player’s strategic choice is not reduced to a mere decision problem in the tradition of Savage (1954). For Newcomb’s problem, we show that because of rational expectations the probability of the being’s making correct predictions is endogenously determined in equilibrium and must be 100 % (except for one special case), provided the being, in the absence of other pursuits, has just the slightest interest in making correct predictions.",2
81.0,3.0,Theory and Decision,15 February 2016,https://link.springer.com/article/10.1007/s11238-016-9536-1,Hart–Mas-Colell implementation of the discounted Shapley value,September 2016,Tomohiko Kawamori,,,Male,Unknown,Unknown,Male,"Marginalism and egalitarianism are two reasonable principles regarding division of coalitional worth. Marginalism entails dividing the coalitional worth according to players’ marginal contributions to coalitional worth, while egalitarianism entails equally dividing the coalitional worth independently of players’ contributions. The Shapley value by Shapley (1953) is based on marginalism, while the egalitarian valueFootnote 1 is based on egalitarianism. 
Joosten (1996) introduced the \(\alpha \)-discounted Shapley value as a value that makes a compromise between marginalism and egalitarianism. Each of marginalism and egalitarianism focuses on merely one aspect of the division of worth, and thus, it is more realistic to take into account both. Formally, the \(\alpha \)-discounted Shapley value of the coalitional game \(\left( N,v\right) \) is defined as the Shapley value of the coalitonal game \(\left( N,w\right) \) such that for each \(S \in 2^N\), \(w\left( S\right) = \alpha ^{\left| N\right| - \left| S\right| } v\left( S\right) \), where \(\alpha \in \left[ 0,1\right] \). The \(\alpha \)-discounted Shapley value includes the Shapley value and the egalitarian value as two extreme special cases: if \(\alpha = 1\) (\(\alpha = 0\)), the \(\alpha \)-discounted Shapley value is the Shapley value (egalitarian value). As \(\alpha \) becomes smaller, the worths of subcoalitions under w are smaller, but the worth of the grand coalition is constant, and thus, the \(\alpha \)-discounted Shapley value is more egalitarian. The discounted game
\(\left( N,w\right) \) is obtained by more heavily discounting the worth of a smaller coalition in the original game \(\left( N,v\right) \). It might represent the following situation: a coalition is formed by (sequentially) removing the players outside the coalition; hence, as the coalition is smaller, more players must be removed, and thus, the cost of coalition formation is higher. It might also represent the following alternative situation: when a coalition is formed, the outside players, i.e., the players excluded from the coalition feel hostile toward the coalition members and make trouble to the coalition; hence, as the coalition is smaller, more players make trouble, and thus, the worth of the coalition is more heavily damaged. In such situations, the effective worth of a coalition, \(w\left( S\right) \), is obtained by discounting the nominal worth, \(v\left( S\right) \), according to the number of outside members. While Hart and Mas-Colell (1989) axiomatized the Shapley value by consistency and standardness, Joosten (1996) axiomatized the \(\alpha \)-discounted Shapley value by consistency and \(\alpha \)-standardness. A value \(\phi \) is \(\alpha \)-standard if for each two-player coalitional game \(\left( N,v\right) \) and each \(\left( i,j\right) \in N^2\) such that \(i \ne j\), \(\phi _i\left( N,v\right) = \frac{v\left( N\right) + \alpha v\left( \left\{ i\right\} \right) - \alpha v\left( \left\{ j\right\} \right) }{2}\). Similar to how Hart and Mas-Colell (1989) derived the Shapley value from the potential function, Driessen and Radzik (2002) derived a generalization of the Shapley value from the weighted pseudo-potential function, which is a generalization of the potential function. This generalized Shapley value includes the \(\alpha \)-discounted Shapley value. van den Brink and Funaki (2010) axiomatized the \(\alpha \)-discounted Shapley value in line with Shapley (1953), by generalizing the null player property. 
Hart and Mas-Colell (1996) gave the Shapley value a noncooperative foundation. Hart and Mas-Colell (1996) presented a noncooperative coalitional bargaining model with nontransferable utilities, wherein if a proposal is rejected, the player that offers the proposal becomes inactive with probability \(1 - \rho \). Hart and Mas-Colell (1996) showed that the Shapley value is implemented as the expected payoff tuple of each equilibrium in the transferable utility case. We give the \(\alpha \)-discounted Shapley value a noncooperative foundation. We incorporate time discounting into the model of Hart and Mas-Colell (1996), where each player is supposed to not discount future payoffs, in the transferable utility case. The incorporation of time discounting unifies the models of Hart and Mas-Colell (1996) and Okada (1996), where the equilibrium payoff tuple is the egalitarian value. We show that given continuation probability \(\rho \) and discount factor \(\delta \), the \(\frac{\delta \left( 1 - \rho \right) }{1 - \rho \delta }\)-discounted Shapley value is implemented as the expected payoff tuple of each subgame-efficient stationary subgame perfect equilibrium (SSPE), which is an SSPE where the full coalition (the coalition that consists of all active players) forms without delay in every subgame. We also show that the \(\alpha \)-discounted Shapley value approximately coincides with the ex post payoff tupleFootnote 2 by each subgame-efficient SSPE when \(\rho \) and \(\delta \) go to unity with \(\frac{\delta \left( 1 - \rho \right) }{1 - \rho \delta }\) converging to \(\alpha \). 
van den Brink and Funaki (2010) also gave the \(\alpha \)-discounted Shapley value a noncooperative foundation. van den Brink and Funaki (2010) incorporated time discounting into the model of Pérez-Castrillo and Wettstein (2001) (the bidding mechanism). van den Brink and Funaki (2010) showed that under discount factor \(\delta \), the \(\delta \)-discounted Shapley value is implemented as the equilibrium payoff tuple. Since \(\frac{\delta \left( 1 - \rho \right) }{1 - \rho \delta } < \delta \), the value obtained using the Hart–Mas-Colell implementation in the present paper is closer to the egalitarian value than that obtained using the Pérez-Castrillo–Wettstein inplementation in van den Brink and Funaki (2010). This difference might be because the proposer whose proposal is rejected becomes inactive with a certain probability in Hart and Mas-Colell (1996) and with certainty in Pérez-Castrillo and Wettstein (2001). Thus, if the proposer whose proposal is rejected probabilistically becomes inactive in Pérez-Castrillo and Wettstein (2001), both approaches may yield the same value. 
Joosten (1996) introduced the \(\alpha \)-egalitarian Shapley value, which also takes into account both marginalism and egalitarianism. The \(\alpha \)-egalitarian Shapley value is a convex combination of the Shapley value and the egalitarian value. van den Brink et al. (2013) incorporated the breakdown of negotiation into the first round of the bidding mechanism in Pérez-Castrillo and Wettstein (2001) and showed that the \(\alpha \)-egalitarian Shapley value is implemented as the equilibrium payoff tuple. While each proposer proposes only an allocation for the full coalition in Hart and Mas-Colell (1996), each proposer proposes both a coalition and an allocation for the coalition in the present paper. Thus, in the present paper, a coalition structure is endogenously decided and a subcoalition might form, which leads to inefficiency under superadditivity. We then provide a necessary and sufficient condition for a subgame-efficient SSPE to exist. Using this condition, we also provide the conditions for a subgame-efficient SSPE to exist when \(\rho \) and \(\delta \) go to unity with \(\frac{\delta \left( 1 - \rho \right) }{1 - \rho \delta }\) converging to \(\alpha \): a necessary condition is that for each subgame of the underlying coalitional game, the \(\alpha \)-discounted Shapley value of the subgame is in the core of the subgame; a sufficient condition is that for each subgame, the \(\alpha \)-discounted Shapley value of the subgame is in the interior of the core of the subgame. 
Calvo and Gutiérrez-López (2016) and the present paper have studied the Hart–Mas-Colell implementation of the discounted Shapley value independently of each other. Calvo and Gutiérrez-López (2016) introduced time discounting into the model of Hart and Mas-Colell (1996) and showed coincidence of the discounted Shapley value and the equilibrium payoff tuple of the model. The main difference between Calvo and Gutiérrez-López (2016) and the present paper is as follows. In Calvo and Gutiérrez-López (2016), each proposer proposes only an allocation for the full coalition, i.e., full-coalition formation is assumed; in the present paper, each proposer proposes a coalition as well as an allocation for the coalition, and a condition for full-coalition formation is derived. In Calvo and Gutiérrez-López (2016), even the nontransferable utility case is considered; in the present paper, only the transferable utility case is considered. The remainder of this paper is organized as follows. Section 2 describes a noncooperative coalitional bargaining model; Sect. 3 shows that the \(\alpha \)-discounted Shapley value is supported by each subgame-efficient SSPE; Sect. 4 gives the conditions for a subgame-efficient SSPE to exist; Sect. 5 concludes this paper. The appendix contains the proofs of all propositions.",4
81.0,3.0,Theory and Decision,05 February 2016,https://link.springer.com/article/10.1007/s11238-016-9537-0,On the existence of altruistic value and utility functions,September 2016,Jay Simon,,,Male,Unknown,Unknown,Male,"It is widely recognized that humans often display altruistic behavior. That is, we make decisions that are inconsistent with some narrowly defined sense of self-interest. These decisions may involve charitable giving, volunteering for a non-profit organization, helping a friend move, fixing a flat tire for a stranger, or any of a number of other generous actions. All of these activities reflect an aspect of preferences that cannot be captured adequately by the traditional economic idea of value or utility achieved through consumption of goods and services. However, the prevalence of altruistic behavior has been acknowledged widely in both economics and psychology literature, and a great deal of discussion exists exploring the reasons behind it. Arrow (1975) presents various types of preferences under which it may be desirable for an individual to donate blood. Andreoni (1990) uses the term “impurely altruistic” to express the idea that individuals receive some benefit not only from improvement in a public good, but also from the act of giving itself. McCardle et al. (2009) use a model of preference that incorporates the recognition or acclaim received by individuals for charitable acts. There are also many evolutionary models that justify widespread altruistic behavior in humans (Hamilton 1963; Trivers 1971; Alexander 1987; Simon 1990; Bergstrom and Stark 1993; Nowak and Sigmund 2005; Montero 2008). Intergenerational altruistic models are also common; see, for instance, Phelps and Pollak (1968), Ray (1987), Hori and Kanaya (1989), and Saez-Marti and Weibull (2005). Alger and Weibull (2010) examine altruistic models between siblings rather than across generations. Given that decision makers are likely to be altruistic, developing value and utility functions that capture altruistic preferences is desirable to help these decisions makers achieve more desirable outcomes. However, it is not clear that these value and utility functions are guaranteed to exist, as the conditions typically used to ensure existence of value and utility functions are substantially stronger when multiple individuals’ preferences are being aggregated. In addition, even if such functions exist, there are differing views in the previously cited literature on the form that they should take. For example, paternalistic altruistic value or utility functions take another individual’s outcome as an argument, while nonpaternalistic altruistic value or utility functions take the value or utility achieved by another individual as an argument. In this paper, we provide representation theorems for paternalistic altruistic functions, and then expand them to additive nonpaternalistic functions by introducing one further condition. There is some disagreement in the literature regarding the names used to describe preference functions, which we will not attempt to resolve here. In this paper, a “value function” represents preferences over a set of outcomes in an environment of certainty, and may be either ordinal or cardinal, while a “utility function” represents preferences for risky decisions, i.e., preferences over gambles defined on a set of outcomes. The goal of this paper is to define altruism precisely using preference relations, and to establish the existence of an altruistic value or utility function representing these preference relations based on relatively simple conditions. In particular, we do not assume that the altruistic preference relations are complete, transitive, or continuous; the existence of altruistic value and utility functions can be established using weaker preference conditions. The value functions may be either ordinal or cardinal, i.e., they may only preserve the ordering of elements, or they may preserve relative differences between pairs of elements. This paper also explores some of the relevant properties of such preference relations and value functions; of particular interest are the conditions required for the functions to have additive forms. The benefit of weakening the preference conditions is greater in a context of multiple individuals, because completeness, transitivity, and continuity become substantially stronger conditions when the preference relation aggregates the preferences of different people. A weaker set of preference conditions will make the existence of an altruistic value or utility function more plausible. The results need not apply only in the specific altruistic framework described in the paper. If the relevant properties and conditions are satisfied in other contexts involving multiple individuals (e.g., a parent considering outcomes affecting his/her multiple children), then the analogous representation theorems will apply, and a value or utility function will exist. The structure of the altruistic value and utility functions is similar to that of multiattribute value and utility functions for individual decision makers. Indeed, if the analogous properties of the preference relations are satisfied in that context, weaker conditions than those typically used can guarantee the existence of a multiattribute value or utility function. However, it is generally considered a reasonable assumption that rational individuals will have complete, transitive, and continuous preferences. There is some debate in the psychology literature regarding the underlying factors driving altruistic behavior. Batson et al. (1989), for instance, claim that anticipated improvement in the decision maker’s mood is not necessary for helping others under conditions of empathy. Cialdini et al. (1987), on the other hand, argue that an egotistical explanation is more appropriate. This issue is discussed in greater depth by Reyniers and Bhalla (2013). This debate serves as an interesting background for the material presented in this paper, but the concepts described here are applicable regardless of whether the factors underlying altruistic behavior are selfish or selfless. The remainder of the paper is structured as follows. Section 2 introduces the concept of altruistic preference relations. Section 3 explores the representation of altruistic preferences with ordinal value functions. Section 4 adds preference difference relations, which can allow for the use of cardinal altruistic value functions. Section 5 considers preference relations over gambles, and presents conditions leading to the existence of altruistic utility functions. Section 6 concludes the paper.",6
81.0,3.0,Theory and Decision,17 February 2016,https://link.springer.com/article/10.1007/s11238-016-9539-y,Order of limits in reputations,September 2016,Nuh Aygün Dalkıran,,,Male,Unknown,Unknown,Male,"One of the most prominent results in the reputations literature is due to Fudenberg and Levine (1989, 1992), who studied infinitely repeated, reputation games where a long-run player faces an infinite sequence of short-run players. They showed that an arbitrarily patient strategic long-run player can guarantee herself a payoff close to her Stackelberg payoff when there is a small ex ante probability that the long-run player is a commitment type who always plays the Stackelberg action. Their result implies that quite small perturbations of the complete information model might have large effects on the set of limit equilibrium payoffs. This paper studies the set of equilibrium payoffs in repeated games with incomplete information as in Fudenberg and Levine (1992), when the long-lived player’s discount factor is fixed. We show that even when the discount factor of the long-run player is very high, arbitrarily small perturbations cannot open the possibility of equilibrium payoffs far from the complete information equilibrium payoff set—as long as the discount factor of the long-run player is fixed. Our main result might seem in stark contrast with the opening quotation of this paper, yet, it is indeed complementary to Fudenberg and Levine’s (1992) result. Our main result highlights that, as anticipated, Fudenberg and Levine’s (1992) result holds true due to a specific order of limits. That is, if the discount factor of the long-run player tends to 1 while holding the commitment type’s ex ante probability fixed, then the aforementioned reputation result à la Fudenberg and Levine (1992) holds true; however, if the commitment type’s ex ante probability tends to 0 while holding the discount factor of the long-run player fixed, then the incomplete information equilibrium payoffs cannot be far from the complete information equilibrium payoff set. As far as we know, this is the first paper that explicitly points out the importance of the order of limits issue in these results. From a technical point of view, our main result is an upper-hemi continuity result. We show that in reputation games of this type, the equilibrium payoff set is, for a fixed discount factor, upper-hemi continuous in the prior probability that the long-run player is a commitment type at zero when there is full-support imperfect public monitoring. We are aware that upper-hemi continuity results in the game theory literature are plenty. Yet, our result is the first result that explicitly provides a proof for the current upper-hemi continuity property, which highlights the order of limits issue in the reputations literature. Furthermore, given Bayesian updating, sequential rationality, and the dynamic structure of reputation games, our result is not a straightforward generalization of any such result in the literature. Other techniques might, of course, be used to prove similar results, yet our method of proof is relatively novel, employing the Abreu et al. (1990) techniques. We believe that this is another technical contribution of this paper because using such techniques to study repeated games with incomplete information is rare.Footnote 1 It is our hope that our proof will inspire other researchers to use similar techniques to tackle similar problems in the literature. While Cripps and Thomas (2003) established both upper-hemi continuity and lower-hemi continuity of the equilibrium payoff set of repeated games with two long-lived players with equal discount factors when one-sided incomplete information vanishes, their results do not extend to our setting. Unfortunately, we fail to provide a proof (or a counter-example) for the lower-hemi continuity counterpart of our result, thus this stays as a hard open problem. An affirmative conjecture for a necessary condition of the lower-hemi continuity counterpart of our result was done by Cripps et al. (2004), but they also failed to provide a proof or a counter-example for this necessary condition.Footnote 2
Footnote 3
 The first papers that introduced the adverse selection approach to study reputations are Kreps et al. (1982), Kreps and Wilson (1982), and Milgrom and Roberts (1982). They show that the intuitive expectation of cooperation in early rounds of the finitely repeated prisoners’ dilemma and entry deterrence in early rounds of the chain store game can be rationalized due to “reputation effects”. As mentioned above, Fudenberg and Levine (1989, 1992) extended this idea to infinitely repeated games and showed that a patient long-run player facing infinitely many short-run players can guarantee herself a payoff close to her Stackelberg payoff when there is a slight probability that the long-run player is a commitment type who always plays the Stackelberg action. When compared to the folk theorem (see Fudenberg and Maskin 1986; Fudenberg et al. 1994), their results imply another intuitive expectation: the equilibria with relatively high payoffs are more likely to arise due to reputation effects. 
Fudenberg et al. (1990) provided an upper bound for the equilibrium payoffs of a long-run player facing infinitely many short-run players under imperfect public monitoring, which is independent of the discount factor of the long-run player and might be strictly less than the Stackelberg payoff. Hence, Fudenberg and Levine (1992)’s results imply that under imperfect public monitoring new equilibrium payoffs may arise with incomplete information when the discount factor of the long-run player is sufficiently high. Even though the results of Fudenberg and Levine (1989, 1992) hold for both perfect and imperfect public monitoring, Cripps et al. (2004) showed that reputation effects are not sustainable in the long-run when there is imperfect public monitoring. In other words, it is impossible to maintain a permanent reputation for playing a strategy that does not play an equilibrium of the complete information game under imperfect public monitoring. Since Cripps, Mailath, and Samuelson’s (2004) work, there has been a large literature which studies the possibility / impossibility of maintaining permanent reputations: Ekmekci (2011) showed that reputation can be sustained permanently in the steady state by using rating systems. Ekmekci et al. (2012) showed that impermanent types would lead to permanent reputations, as well. Atakan and Ekmekci (2012, 2013, 2015) provided positive and negative results on permanent reputations with long-lived players on both sides. Liu (2011) provided dynamics that explain accumulation, consumption, and restoration of reputation when the discovery of the past is costly. Liu and Skrzypacz (2014) provided similar dynamics for reputations when there is limited record-keeping. To sum up, the adverse selection approach to study reputations in repeated games has been quite fruitful. This approach teaches us that reputational concerns can explain the emergence of intuitive equilibria in both finitely and infinitely repeated games. There has been considerable amount of work in the literature which focus on whether or not it is possible to maintain a permanent reputation and how reputation is accumulated, consumed, and restored. The next section describes our model. Section 3 provides a motivating example, Sect. 4 presents our main result, and Sect. 5 concludes the paper.",1
81.0,3.0,Theory and Decision,16 March 2016,https://link.springer.com/article/10.1007/s11238-016-9541-4,The Shapley–Shubik power index for dichotomous multi-type games,September 2016,Sébastien Courtin,Zéphirin Nganmeni,Bertrand Tchantcho,Male,Male,Male,Male,"This paper is devoted to the study of multi-type games, which are a generalization of the classical cooperative game. This framework models games for which the following conditions are met: (i) Several, say r non-ordered, types (not levels) of support are allowed in the input. Each player chooses one of the r types of support. Note that a player can also make no choice. The players’ choices then lead to a choice configuration. (ii) The characteristic function maps each choice configuration to a real number, the value of the configuration. By their nature, classical cooperative games allow two levels of inputs. For example, in a transferable utilities game, each player chooses either “to participate” or “not”, and a characteristic function maps each choice configuration to a real value. In a simple voting game, when a single alternative, such as a bill or an amendment, is pitted against the status quo, the alternative is approved only on the basis of the votes cast by those who are in favor. In other words, voting “yes” and “no” are the only feasible alternatives. It has been pointed out by many scholars that a number of interesting questions of economics, politics and more generally social sciences cannot be described by a classical cooperative game. For example, in some real voting systems such as the United Nations Security Council, the United States federal system and the Council of Ministers of the European Union, “abstention” plays a key role. However, these voting systems cannot be modeled by such games. This goes some way towards explaining the introduction of many different models of such games in recent years. 
Felsenthal and Machover (1997, 1998, 2001) introduced ternary voting games where “abstention” is permitted as a distinct third option of a voter, intermediate between a “yes” and a “no” vote. Thus, each individual voter expresses one of the three possible levels of input support, and the output consists of either collective acceptance or collective rejection. A similar model is the bicooperative game of Bilbao et al. (2000) in which the third alternative is “no participation”. Clearly, these three input supports are totally ordered in the sense that a “yes” vote is more favorable to the collective acceptance than an “abstention” vote (or a “not participating” vote), which in turn is more favorable than a “no” vote. Ternary voting games or (3, 2) games have been directly extended to (j, k) games by Freixas and Zwicker (2003). In the (j, k) simple games, each player expresses one of j ordered possible levels of input support, and the output consists not of a real value but of one of k possible levels of collective support. Another model of game with ordered inputs is the so-called multichoice game introduced by Hsiao and Raghavan (1993). In these games, each player is allowed to have a given number of effort levels, each of which is assigned a nonnegative weight. The weight assigned to an effort level leads to an ordering on the set of effort levels. Any choice configuration is then associated with a real value. An example of a model in which alternatives in the inputs are not totally ordered is the one formalized by Laruelle and Valenciano (2012). They study quaternary voting rules in which the four possible alternatives are “yes”, “no”, “abstention” and “non-participation”. The collective decision is dichotomous, i.e. either the proposal is accepted or rejected. Levels of support here are not totally ordered, since the “non-participation” and the “abstention” options are not ranked. Indeed, in some situations, the “abstention” option may be more favorable to the rejection of the proposal than the “non-participation” option, while in other situations the converse is observed. However, the “yes” alternative is always more favorable than any other alternative, while the “no” alternative is less favorable than any alternatives. This model and the other models above are particular cases of the more general framework of games on lattices developed by Grabisch and Lange (2007). In this model, each player i has a set \(L_i\) of possible actions and this set is endowed with a lattice, i.e. a partial order such that any pair of actions possesses at least the upper bound and a greatest lower bound. A model of a game that does not feature among the class of games on lattices is that developed by Bolger (1986, 1993, 2000, 2002), called games with n players and r alternatives, or simply r-games. In such games, there are r possible input alternatives that are not ordered. Each alternative j attracts its own coalition of supporting voters. A configuration, which is a partition of the set of players into r subsets (some of which might be empty), is then associated with an r-tuple of cardinal values. The component j represents the value of the coalition of the configuration that has chosen the input j. This model is related to ours in the sense that the set of inputs is not ordered. No alternative is a priori more favorable than another. However, the models differ in their outputs. Our output consists of a single value. It is worth noting that our model is less general than that of Monroy and Fernandez (2009), who introduced multi-criteria simple games. Indeed, in their framework, no ordering is considered in the input or in the output, and furthermore more than one alternative can be chosen by each player. As usual, a central concern in game theory is to define a value or solution concept for a game. The prominent value is the well-known Shapley value. This value for multichoice cooperative games is due to Hsiao and Raghavan (1993). More generally, Grabisch and Lange (2007) extend the Shapley value to all games on lattices. The corresponding value for r-games was developed by Bolger (1993) and later extended by Amer et al. (1998) and Magaña (1996). In the particular context of simple games, different theories of power have been proposed. The most famous is the Shapley–Shubik (1954) voting power index. This index has been extended to the context of multiple alternatives in various games. It was defined for ternary voting games by Felsenthal and Machover (1997). For (j, k) games the extension is due to Freixas (2005). Our main contribution is to extend and fully characterize the Shapley–Shubik index, when “dichotomous” multi-type games (DMG) are considered. DMG are particular cases of multi-type games where the output is dichotomous. This work is structured as follows. Section 2 introduces the general framework of multi-type games and some examples. Section 3 defines and characterizes the Shapley–Shubik and Banzhaf power indices for DMG. Finally, Sect. 4 concludes the paper.",9
81.0,3.0,Theory and Decision,01 March 2016,https://link.springer.com/article/10.1007/s11238-016-9538-z,"Consistency, population solidarity, and egalitarian solutions for TU-games",September 2016,René van den Brink,Youngsub Chun,Boram Park,Male,Unknown,Unknown,Male,"A situation in which a finite set of players can obtain certain payoffs by cooperation can be described by a cooperative game with transferable utility, shortly TU-game. A (point-valued) solution on a class of TU-games assigns a payoff vector to every game in the class. Recently, egalitarian or equal surplus sharing solutions gained attention in the literature. Three well-known equal surplus sharing solutions are the equal division solution (axiomatized in van den Brink 2007), the center-of-gravity of the imputation-set value, and the egalitarian non-separable contribution value. The equal division solution allocates the worth of the ‘grand coalition’ (being the coalition consisting of all players) equally among all players. The center-of-gravity of the imputation-set value, shortly denoted by the CIS-value (see Driessen and Funaki 1991) first gives every agent its own singleton worth and distributes the remainder equally among all players. The egalitarian non-separable contribution value (also known as equal allocation of non-separable costs), shortly denoted by the ENSC-value, is the dual of the CIS-value. In van den Brink and Funaki (2009) the class of all convex combinations of these three solutions is studied. 
Chun and Park (2012) characterize the CIS-value by efficiency, covariance, and population solidarity, the last property requiring that upon the arrival of a new player all the original players should be affected in the same direction, all weakly gain or all weakly lose. We show that all convex combinations of the equal division solution and the CIS-value satisfy population solidarity and extend the characterization of the CIS-value by Chun and Park (2012) to this class of solutions. A main goal of axiomatizing solutions is to compare different solutions based on their characterizing properties. In particular, in this paper we consider classes of different equal surplus sharing solutions. On the one hand, by characterizing such a class of solutions we can compare this class with other solutions. On the other hand, to compare solutions within this class, we will consider axioms that depend on the specific parameter determining a specific solution in this class. Besides axiomatizing all convex combinations of the equal division solution and the CIS-value using population solidarity, we reconsider the axiomatizations using consistency provided by van den Brink and Funaki (2009). They axiomatized the class of all convex combinations of the equal division solution, the CIS-value, and the ENSC-value using a parameterized standardness for two-player games and a parameterized consistency. The convex combinations of the equal division solution and the CIS-value have a nonparameterized consistency in common which we use in an axiomatization together with \(\alpha \)-standardness for two-player games. For a fraction \(\alpha \in [0,1]\), \(\alpha \)-standardness for two-player games states that for two-player games each player first receives a fraction \(\alpha \) of its singleton worth, and what remains of the worth of the ‘grand coalition’ is split equally among the two players. In van den Brink and Funaki (2009) it is shown that any solution that satisfies efficiency, symmetry, and linearity on the class of two-player games satisfies \(\alpha \)-standardness for two-player games for some \(\alpha \in [0,1]\). Since linearity is only used for two-player games, and in many (economic) applications it is a technical axiom, we prefer to have a characterization of \(\alpha \)-standardness without linearity. On the other hand, in their characterization of the CIS-value, Chun and Park (2012) use covariance. However, the CIS-value is the only covariant solution in the class considered here. Therefore, we consider a weak covariance which requires that the payoffs of all players change by the same amount if to each coalition we add a constant times the number of players in the coalition. This property can also be seen as a weakening of the fairness axiom in van den Brink (2001). In this paper, we also use another weakening of fairness requiring that the payoffs of all players change by the same amount when we only change the worth of the ‘grand coalition’. We show that any solution on the class of two-player games that satisfies these two properties together with efficiency, homogeneity, local monotonicity, and nonnegativity, satisfies \(\alpha \)-standardness for some \(\alpha \in [0,1]\). To select a particular solution from the convex combinations of the equal division solution and the CIS-value, we use \(\alpha \)-individual rationality requiring that for appropriate games a player always earns at least a fraction \(\alpha \in [0,1]\) from its singleton worth. For specific values of \(\alpha \) (1, respectively 0), this axiom yields the usual individual rationality or nonnegativity. Finally, we introduce a dual version of population solidarity that is satisfied by the ENSC-value, the equal division solution, and all their convex combinations. The paper is organized as follows: Sect. 2 discusses some preliminaries on TU-games and solutions. In Sect. 3, we consider two-player games and characterize \(\alpha \)-standardness for two-player games. In Sect. 4, we extend these definitions to n-player games using consistency. In Sect. 5, we give an axiomatic characterization using population solidarity. In Sect. 6 we consider the dual class consisting of all convex combinations of the equal division solution and the ENSC-value. Finally, Sect. 7 contains some concluding remarks.",25
81.0,3.0,Theory and Decision,25 February 2016,https://link.springer.com/article/10.1007/s11238-016-9540-5,Compromising in bifocal distribution games: the average value,September 2016,Pedro Gadea-Blanco,José-Manuel Giménez-Gómez,M. Carmen Marco-Gil,Male,Unknown,Unknown,Male,"In the summer of 2009, during an International Meeting on Game Theory, we had a very interesting conversation about the cost allocation that a major company undertook after receiving a detailed report carried out by an economics research group. As a conclusion, the report provided two possible cost distributions and, surprisingly enough, the company’s final decision was to distribute the cost according to the average of them. This paper aims to provide some new theoretical support for the popular proverb ‘Virtue lies in the middle ground’ which gathers the previous behavior, so common in so many and different situations. Particularly, we consider the normative approach to sharing problems which, as superbly expressed by Young (1994), does not boil down to a single formula, but represents a balance between different competing principles. In this context, we introduce bifocal distribution problems by adding, to generic distribution problems, two prominent proposals for solving them. Then, we model these kinds of problems as transferable utility cooperative games (TU-games, hereinafter) by associating to each coalition the smallest quantity of the ‘good’ to be distributed that it would receive according to the two proposed allocations. These games, that we call bifocal distribution games , are the minimum of two additive games and we show that this specific structure leads to ‘solid’ grounds of intermediate compromises. Specifically, our main result states that, although these games are not convex in general, the Shapley value is a Core selection that coincides with the Nucleolus and recommends the Average value, that is, the average of the two focal distributions. These technical arguments reinforce the support that the Average value has, as it is, from the point of view of equity. On the one hand, it represents an equidistant choice from the starting point of the two proposals since it could be achieved by performing equivalent concessions. On the other hand, all agents receive an equitable treatment in the sense that each of them both gets and loses the same percentage of the potential gains and losses, regarding their worse and best allocations, respectively. Finally, we interpret the Average value by means of two different axiomatic characterizations. The first one is based on an adaptation to our context of Additivity, the well-known property introduced by Shapley (1953b) to propose his value for TU-games. The main property of our second axiomatic result, which was first studied in the context of bankruptcy problems by O’Neill (1982), is No Advantageous Merging or Splitting, and demands a solution immunity to manipulations of regrouping or division of agents. The paper is organized as follows: Section 2 introduces the main concepts and definitions. Section 3 provides game-theoretic grounds of intermediate compromises. Section 4 presents two axiomatic characterizations of the Average value. Section 5 summarizes our conclusions. Section “Appendices” contains the technical proofs.",
81.0,4.0,Theory and Decision,16 March 2016,https://link.springer.com/article/10.1007/s11238-016-9545-0,An expected utility theory for state-dependent preferences,November 2016,Edi Karni,David Schmeidler,,Male,Male,Unknown,Male,"This note is a generalization and improved interpretation of the main result of Karni and Schmeidler (1980). We never attempted to publish our original paper because, subsequently, we wrote another paper jointly with Karl Vind that, we thought, superseded our result (see Karni et al. 1983). Invoking the same analytical framework, the two papers give necessary and sufficient conditions for the existence of subjective expected utility representations of state-dependent preferences. The difference is that in Karni et al. (1983) we invoked only a subset of the state-prize lotteries introduced in Karni and Schmeidler (1980). At the time, our main interest was the characterization, in terms of preference relations, of the existence of unique subjective probabilities on the state space. Because the two models yielded the sought-after result and the result with Karl Vind required weaker hypothesis, we shelved our original result. What we failed to fully appreciate at the time is the significance of the fact that the subjective probabilities and state-dependent utility function obtained in Karni et al. (1983) depend on an arbitrary choice of the subset of state-prize lotteries. Specifically, subsets of state-prize lotteries with different marginal probabilities on the states yield distinct subjective probabilities and utilities. Consequently, there is no guarantee that the subjective probabilities thus obtained represent the decision-maker’s beliefs regarding the likely realization of the states, and there is no reason to suppose that the utility function represents his evaluation of the outcomes in the different states. This failure became apparent in Karni and Mongin (2000) who observed that the subjective probabilities in Karni and Schmeidler (1980) are the unique subjective probabilities that are independent of the marginal probabilities on the states. Furthermore, even if the underlying preference relation displays state independence, if there is a discrepancy between the subjective expected utility representation of Anscombe and Aumann (1963) and that of Karni and Schmeidler (1980), the latter probabilities and utilities are the correct representations of the decision-maker’s beliefs and his evaluation of the consequences in the different states.Footnote 1
 For given marginal probabilities on the states, the utility function in Karni et al. (1983) is unique up to cardinal unit-comparable transformations. Hence it does not allow a meaningful way of comparing the utility of a prize in distinct states. By contrast, in Karni and Schmeidler (1980) the subjective probability is unique and the utility function is unique up to positive linear transformation, which permits the aforementioned comparisons. In retrospect, the main result of our 1980 paper, summarized here, turns out to be more useful for a certain strain of literature, despite its more restrictive assumptions. This result is discussed in Nau (2001), Drèze and Rustichini (2004), Karni (2009), and Lu (2015). It was invoked by and played a crucial role in recent works by Riedener (2015), Baccelli (2016), and Karni and Safra (2016). In the standard formulation of subjective expected utility theory, the preferences on alternative courses of actions are assumed to be state independent. The representation of these preferences consists of a subjective probability measure on the set of states, supposedly representing the decision-maker’s beliefs regarding the likely realization of the different events (that is, subsets of the set of states of nature) and a utility index representing his evaluation of the consequences, independent of the underlying events. The imposition of state-independent utility is irreconcilable with some applications including the choice of life insurance, certain aspects of health and disability insurance, and insurance of family heirlooms. In these instances, the decision-maker’s evaluation of the pecuniary outcome is not independent of the underlying state of nature. Hence the interest in extending the subjective expected utility model to allow for state-dependent preferences and a utility index that is assigned to each prize–state of nature pair. The utilities of two such pairs may differ, even if the prize is the same in both. One implication of this situation is that even if a risk-averse decision-maker is offered fair insurance terms, he may not choose full insurance (see Cook and Graham 1977; Hirshleifer and Riley 1979; Karni 1985). Clearly, the extension of the subjective expected utility model to include state-dependent preferences requires more information (observations) than the state-independent theory. Attempts to construct an expected utility theory for state-dependent preferences were made by Fishburn (1973) and Drèze (1987). Fishburn assumed the existence of a preference relation over all acts conditioned on events. He required that, for every two disjoint events, not all the consequences conditioned upon one event are preferred to all the consequences conditioned upon the other. This restriction is irreconcilable with some applications (e.g., life insurance problems) that motivated our research [see Fishburn’s own criticism (Fishburn 1974)]. Dreze combined state-dependent preferences with moral hazard. Following Karni and Schmeidler (1980), we assume that there are finitely many states of nature and finitely many prizes, and it is not required that every prize be available in every state. Acts assign extraneous lotteries to each state of nature. Since the evaluation of a prize in our model depends upon the state, it is possible to think of a prize–state of nature pair as the ultimate outcome and to consider extraneous lotteries over these outcomes (state-prize lotteries for short). For a detailed discussion of the various types of state-dependent preferences and utility functions and the approach of Karni and Schmeidler (1980), see Baccelli (2016). A decision-maker is supposed to possess a preference relation on acts and another preference relation on state-prize lotteries. The existence of a preference relation over acts is standard and requires no further elaboration. The preference relation over the state-prize lotteries requires explanation. For example, a person has to choose between two ClubMed resorts in February: ski in the Alps or Mauritius. In the event his leg is broken he prefers the latter, and otherwise he prefers ski. Here we assume that the person can rank (among others), staying the week in February in the ski resort with a broken leg, same in a good health, staying in the ClubMed Mauritius resort with a broken leg, and the same in a good health. Moreover, he can rank lotteries with these state-prize outcomes. Savage’s P3, and P4 exclude the basic example’s preferences. But Savage was aware of the possibility of such preferences and suggested how to deal with them in his framework: redefine outcomes to include the state-prize outcomes of our model, redefine acts accordingly, etc. Thus the same comparisons we used appear in Savage’s redefined model.Footnote 2 The term ‘principle’ in the expression ‘observable in principle’ is very misleading. The preference relation on acts and the preference relation on state-prize lotteries are assumed to satisfy the usual von Neumann–Morgenstern axioms. In addition, we impose a natural consistency axiom connecting the two preference relations. The consistency axiom requires that the preference relation on acts restricted to a state of nature agrees with the preference relation on state-prize lotteries restricted to the same state of nature. (A detailed discussion of this axiom in relation to null states appears in the last section.) Applying the von Neumann–Morgenstern expected utility theorem to the preference relation on the state-prize lotteries, we obtain a utility index for each state-prize pair. This utility index is unique up to positive linear transformations. Applying the same theorem to the preference relation over acts yields an evaluation function on the state-prize pairs. This function is unique up to positive linear transformations, one for each state of nature, but with identical multiplicative coefficient across states. Using our consistency axiom we show that, for each state of nature, the utility function and the evaluation function are proportional. Properly normalized, the coefficients of proportionality constitute a subjective probability measure on the set of states of nature. The subjective probability measure is unique, except for the case in which all the prizes are equally desired for some states of nature. In the process of revising our 1980 paper, we discovered that part of the consistency axiom we used is redundant. We deleted it.Footnote 3
 Section 2.1 describes the basic structure of the model and states our main result. The proof appears in the second subsection. In Sect. 3, we discuss the meaning of null states.",19
81.0,4.0,Theory and Decision,22 April 2016,https://link.springer.com/article/10.1007/s11238-016-9549-9,Quantum-like models cannot account for the conjunction fallacy,November 2016,Thomas Boyer-Kassem,Sébastien Duchêne,Eric Guerci,Male,Male,Male,Male,"Conjunction fallacy was first empirically documented by Tversky and Kahneman (1982, 1983) through a now renowned experiment in which subjects are presented with a description of someone called “Linda”: Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Then, subjects are shown a list of eight possible outcomes describing her present employment and activities, and are asked to rank the propositions by representativeness or probability. Two items were specifically tested: “Linda is a bank teller”, “Linda is a bank teller and is active in the feminist movement”. Empirical results show that most people judge (2) more probable than (1). In the framework of classical probabilities, this is a fallacy—the conjunction fallacy—since a conjunction cannot be more probable than one of its components. If Linda being active in the feminist movement is denoted by F and Linda being a bank teller by B, then \(p(F \cap B) \leqslant p(B)\) should classically prevail. The conjunction fallacy has been shown to be particularly robust under various variations of the initial experimental protocol (cf. Tversky and Kahneman 1982, 1983; Gigerenzer 1996; Kahneman and Tversky 1996; Hertwig 1997; Hertwig and Chase 1998; Hertwig and Gigerenzer 1999; Mellers et al. 2001; Stolarz-Fantino et al. 2003; Bonini et al. 2004; Tentori et al. 2004; Hertwig et al. 2008; Moro 2009; Kahneman 2011; Erceg and Galic 2014; for a review, cf. Moro 2009). It has been observed in other cases than the Linda story, about topics like sports, politics, or natural events, and in scenarios in which the propositions to be ranked are not preceded with a description. The fallacy also persists when the experimental setting is changed, e.g. in “between subjects” experiments in which (1) and (2) are presented to different subjects only. Semantic and syntactic aspects have also been discussed, in relation with possible misunderstandings, like the implicit meaning of the words “probability” and “and”. Careful experiments show that the conjunction fallacy persists. The conjunction fallacy questions the fact that classical probability theory can be used to describe human judgment and decision making, and it can also be viewed as a challenge to the definition of what a rational judgment is. Thus, it is no surprise that the conjunction fallacy has been the subject of a big amount of research (Tentori and Crupi 2012 give the number of a 100 papers devoted to it). It has interested psychologists, economists and philosophers alike. For instance, behavioral economists have looked at the consequences of the fallacy for understanding real life economic behavior, measuring the robustness of this bias in an economic context with incentives or in betting situations (e.g. Charness et al. 2010; Nilsson and Andersson 2010; Erceg and Galic 2014). They have also investigated whether the cognitive abilities of subjects are related to behavioral biases in general (and to the conjunction fallacy in particular, cf. Oechssler et al. 2009), and this has led to stimulating research with applications in finance. Epistemologists have made confirmation and Bayesianism enter the debate (e.g. Tentori and Crupi 2008, 2012, Hartmann and Meijs 2012; Schupbach 2012; Shogenji 2012). Given that a conjunction fallacy occurs under robust experimental conditions, a natural question arises: how can this fallacy be explained? Several accounts have been argued for, but no one has reached an uncontroversial status today (as noted by Fisk 2004; Nilsson et al. 2009; Jarvstad and Hahn 2011; Tentori et al. 2013). First, Tversky and Kahneman originally suggested that a representativeness heuristic (i.e. the probability that Linda is a feminist is evaluated from the degree with which the instance of Linda corresponds to the general category of feminists) could account for some conjunction fallacy cases. But it has been argued that the representativeness concept involved is informal and ill-specified (Gigerenzer 1996; Birnbaum et al. 1990), and suggestions to specify it in the technical sense of a likelihood value (Shafir et al. 1990; Massaro 1994) account for limited cases only (Crupi et al. 2008). According to another suggestion, agents actually evaluate the probability of the conjunction from some combination of the probabilities of the components, like averaging or adding (Fantino et al. 1997; Nilsson et al. 2009). However, such explanations do not resist empirical tests, as Tentori et al. (2013) have argued. The latter propose an account of the conjunction fallacy based on the notion of inductive confirmation as defined in Bayesian theory, and give experimental grounds for it—it is one of the currently promising accounts. Others have argued, also within a Bayesian framework, that there are cases in which the conjunction fallacy is actually not a fallacy and can be accounted for rationally (Hintikka 2004; VonSydow 2011; Hartmann and Meijs 2012). Finally, another prominent proposal to account for the conjunction fallacy, on which we focus here, makes uses of so-called “quantum-like” models, which rely on the mathematics of a major contemporary physical theory, quantum mechanics (Franco 2009; Busemeyer et al. 2011; Yukalov and Sornette 2011; Pothos and Busemeyer 2013)—note that only mathematical tools of quantum mechanics are exploited, and that the models are not justified by an application of quantum physics to the brain. The quantum-like account of the conjunction fallacy is particularly promising as it belongs to a more general theoretical framework of quantum-like modeling in cognition and decision making, which has been applied to many fallacies or human behavior considered as irrational (for reviews, see Pothos and Busemeyer 2013; Ashtiani and Azgomi 2015, or Bruza et al. 2015; textbooks include Busemeyer and Bruza 2012; Haven and Khrennikov 2013). For instance, quantum-like models of judgments have been proposed to account for order effect, i. e. when the answers given to two questions depend on the order of presentation of these questions (Atmanspacher and Römer 2012; Busemeyer and Bruza 2012; Wang and Busemeyer 2013; Wang et al. 2014); for the violation of the sure thing principle, which states that if an agent prefers choosing action A–B under a specific state of the world and also prefers choosing A–B in the complementary state, then she should choose A over B regardless of the state of the world (Busemeyer et al. 2006a, b; Busemeyer and Wang 2007; Khrennikov and Haven 2009; for Ellsberg’s paradox (Ellsberg 1961) more specifically, cf. Aerts et al. 2011, 2014; Aerts and Sozzo 2013; for Allais’ paradox (Allais 1953), cf. Khrennikov and Haven 2009; Yukalov and Sornette 2010; Aerts et al. 2011); for asymmetry judgments in similarity, i.e. that “A is like B” is not equivalent to “B is like A” (Pothos and Busemeyer 2011); for paradoxical strategies in game theory such as in the prisoner’s dilemma (Piotrowski and Stadkowski 2003; Landsburg 2004; Pothos and Busemeyer 2009; Brandenburger 2010). More generally, new theoretical frameworks with quantum-like models have been offered in decision theory and bounded rationality (Danilov and Lambert-Mogiliansky 2008, 2010; Lambert-Mogiliansky et al. 2009; Yukalov and Sornette 2011). As the quantum-like account of the conjunction fallacy is one of the few promising accounts of the conjunction fallacy that are discussed today, we choose to focus on it in this paper. More specifically, we focus on the class of quantum-like models which are presented or defended in Franco (2009), Busemeyer et al. (2011, 2015), Busemeyer and Bruza (2012) and Pothos and Busemeyer (2013).Footnote 1 In these models, an agent’s belief is represented by a quantum state—and not for instance by a measurement context. Our aim is to assess the empirical adequacy of these quantum-like models that are used to account for the conjunction fallacy. We think that two points deserve particular scrutiny. First, it is not always clear which version of the models is supposed to account for particular cases of conjunction fallacies—are the simplest ones, called non-degenerate, sufficient? Or are the more general ones, called degenerate, needed? More recent works tend to favor degenerate models over non-degenerate ones, and non-degenerate models have received some recent criticisms (cf. Tentori and Crupi 2013; Pothos and Busemeyer 2013, pp. 315–316), but a clear and definitive argument on the matter would be welcome. Second, the models have not yet been much tested on other predictions than the ones they were intended to account for. It should be checked that they are not ad hoc by testing their empirical adequacy in general. It is understandable that these two points have not been tested beforehand, as a new general pattern of explanation for the conjunction fallacy is hard to come up with. But since the models have come to be seen as one of the most promising accounts, it becomes urgent to assess them empirically more thoroughly—this is our goal in this paper. As for the first point—discriminate between non-degenerate and degenerate models—we follow a suggestion made by Boyer-Kassem et al. (2016) to test so-called “GR equations”, that are empirical predictions made by non-degenerate models.Footnote 2 Such a GR test requires a new kind of experiment: not the original Linda experiment, in which agents have to rank propositions, but an order effect experiment, in which two yes–no questions are asked in one order or in the other, to different agents. Existing data cannot answer the question of whether the GR equations are verified, as was already noted in 2009 by Franco: There are no experimental data on order effects in conjunction fallacy experiments, when the judgments are performed in different orders. Such an experiment could be helpful to better understand the possible judgment strategies (Franco 2009, 421). We fill this gap here by running several order effect experiments that collect the needed data. As for the second point—test new empirical predictions of the models—we consider two tests that apply to any version of the quantum-like models, whether degenerate or not, that are used in the account of the conjunction fallacy. It is well known in the literature that quantum-like models that account for the conjunction fallacy predict an order effect for the two questions associated with the conjunction (“Is Linda a bank teller?” and “Is Linda a feminist?”). Actually, this predicted order effect is not a side effect of the quantum-like models, but a core feature of them: they cannot account for the conjunction fallacy without it. This enables a direct test of the quantum-like account of the conjunction fallacy, that we apply to our collected experimental data. In addition, it has been shown that any quantum-like model of the kind involved in the account of the conjunction fallacy must make an empirical prediction called the “QQ equality” (Wang and Busemeyer 2013; Wang et al. 2014). We thus test whether the QQ equality is verified. The failure of any of these last two tests will be enough to refute the current quantum-like account of the conjunction fallacy. Here also, the needed data are not available in the literature, but can be conveniently obtained from the same above-mentioned new experimental configuration, with two yes–no questions in both orders. Note that our methodology is novel: we are not testing the quantum-like models against data produced by traditional conjunction fallacy experiments that the model were designed to explain, but we are testing them against other data, in a new experimental framework on which the models actually make some predictions, and it is why the experimental situation we shall consider is different from the usual Linda experiment. Our experiment instantiates the mechanism that the quantum-like account claims agents follow: to evaluate a conjunction like “feminist and bank teller”, agents are supposed to evaluate one characteristic after another, answering for themselves to two yes–no questions (“Is Linda a feminist?”, “Is Linda a bank teller?”). In other words, the experiment we run somehow forces agents to follow the purported quantum-like mechanism. To have more powerful tests, we have conducted several experiments, with variations of the scenario (Linda, but also others known as Bill, Mr. F. and K.), of the protocol (questionnaires or computer-assisted experiment) and with or without monetary incentives. The results we obtain show that current quantum-like models are not able to account for the conjunction fallacy. The outline of the paper is the following. In Sect. 2, a general quantum-like model is introduced. Section 3 presents the three empirical tests that will be performed: the GR equations, order effect, and the QQ equality. The experimental protocol is presented in Sect. 4, and the results in Sect. 5. Section 6 presents the statistical analysis, and Sect. 7 discusses the scope of the results and the future of the research on the conjunction fallacy account.",26
81.0,4.0,Theory and Decision,16 April 2016,https://link.springer.com/article/10.1007/s11238-016-9546-z,Bid pooling in reverse multi-unit Dutch auctions: an experimental investigation,November 2016,Philippe Gillen,Alexander Rasch,Peter Werner,Male,Male,Male,Male,"Multi-unit Dutch auctions and their procurement counterparts are implemented in a variety of real-world markets. In these settings, bid pooling or bidding frenzies (i.e., many bidders submit bids at the same time/clock price) and crashes (i.e., situations where bidders withhold bids) are frequently observed phenomena (see Bulow and Klemperer 1994). One example for the use of multi-unit Dutch auctions in practice is the sale of new securities by US underwriters. There, an initial price is maintained or supported as long as either an issue is sold out or demand turns out to be so low such that a significant price decrease is necessary. Moreover, multi-unit Dutch auctions are also used in commodity markets such as fish markets or markets for fresh produce (see Cassady 1967; Romeu 2000). Furthermore, tickets for concerts, shows, etc., are typically sold on a first-come-first-serve basis which can be interpreted as a multi-unit descending auction.Footnote 1
 Despite the practical importance of multi-unit Dutch procurement auctions in reality and the (potentially) detrimental effects of bid pooling for buyers resulting from an allocative inefficiency, as products are not necessarily supplied by the most efficient sellers, related empirical and experimental evidence on the basic multi-unit Dutch auction is scarce.Footnote 2 Our study contributes to the literature by showing that this auction format is prone to higher prices than predicted by standard theory and is characterized by bid pooling. Furthermore, we set up a theoretical framework to show that these experimental results can be organized by boundedly rational bidding strategies.Footnote 3
 In the auction that we analyze, each subject can sell at most one unit and faces the same commonly known costs to produce it. In many environments where inputs are procured through reverse auctions, transparent information about costs seems to be a plausible assumption. For example, cost structures are transparent for industry sectors such as for raw material as well as standardized and upstream products. These products are typically characterized by a relatively small value-added or sunk R&D costs. More generally, as Haruvy and Katok (2013) point out, bidder-specific attributes may be well known in markets in which a relative small number of bidders repeatedly interact with each other. In every round of our game, a buyer starts the price clock at some low price and the selling price is increased continuously. At any price, four bidders can decide to sell their product or remain in the auction and wait for a higher price. As soon as the buyer has obtained the desired number of objects, the auction ends. In this setup, standard economic theory predicts that all bidders accept to sell the good either at a price equal to costs or at the start price when the start price is above costs. Therefore, our design shares important features of Bertrand-style competition. At the same time, bidders prefer higher bids to increase revenues. Indeed, we find that bids in our experiment are substantially above the Nash equilibrium price with rational bidders and only gradually approach it over the periods. Bid pooling is a predominant pattern—the majority of bids within an auction occur immediately after the first supplier submitted a bid. In addition, bidders seem to focus on reference prices equal to the highest successful bids in the previous auction when they decide about accepting the clock price. We propose a framework that integrates bounded rationality into the derivation of bidding functions by assuming that bids are heterogeneous with respect to their strategic sophistication. In this framework, we distinguish between myopic bids consisting of a simple backward-looking heuristic and sophisticated bids, which means that agents anticipate the behavior of others and choose their optimal bids according to their expectations, but may make mistakes. This approach can organize our experimental observations on the aggregate, suggesting that on the individual level, about half of the bids are sophisticated, whereas the other half are myopic. Bidding behavior in descending multi-unit auctions has been theoretically analyzed in Bulow and Klemperer (1994), Martínez-Pardina and Romeu (2011), as well as Gretschko et al. (2014). These articles show that any symmetric equilibrium in this auction format is inefficient, as bid pooling occurs under standard assumptions with bidder heterogeneity. In the present setting, however, bidders are homogeneous with respect to their costs, such that simultaneous bidding should only be observed at the start price. Yet, under the assumption that subjects are boundedly rational, bid pooling is predicted at prices substantially above the start price. Moreover, in our framework, sophisticated bidders maximize their profit when they just preempt bid pooling, i.e., accept the price clock just before all others do. As these subjects will not enter the auction immediately when the highest price in the previous period is sufficiently far away from the minimum price, but aim at undercutting it gradually, prices will decline over time and eventually converge to the starting price. Our experimental design can be linked to other classes of experiments. With its equilibrium of placing a bid equal to the lowest possible price, our design is related to investigations of Bertrand competition with homogenous products in which participants have to decide about the price they charge for the goods. Experimental studies in this area have found that the realized prices typically range above the equilibrium prices of rational profit-maximizing players—at least when the number of competitors is sufficiently low (see, for example, Dufwenberg and Gneezy 2000; Muren and Pyddoke 2006; Hinloopen and Soetevent 2008; Fonseca and Normann 2012). Moreover, several studies on Bertrand competition with capacity constraints have found evidence for the importance of myopic pricing strategies in repeated interactions, for example, for the occurrence of Edgeworth price cycles (see, for instance, Kruse et al. 1994; Fonseca and Normann 2013, Heymann et al. 2014; Jacobs and Requate 2016). Related to this, a number of Bertrand game experiments investigate the emergence of Edgeworth cycles based on the framework of Maskin and Tirole (1988) in which competitors decide sequentially. In an Edgeworth cycle equilibrium competitors mutually undercut each other before the price is increased again (see Leufkens and Peeters 2011; Healy et al. 2013 and the references cited therein). At the same time, there are important design aspects that distinguish our setting from studies on Bertrand competition. First, the clock structure of the auction format in our game makes bidding a dynamic decision problem from the subjects’ perspective. Due to the continuous price increase, bidders in our experiment are able to react to the bids of others in the course of each auction and the timing of bids becomes endogenous–bids can be placed either in sequences or simultaneously. Second and related to this, the demand for the objects in the auctions can shrink during a period and bidders may immediately adapt their strategies to changes in the demand. Third, competitors in Bertrand experiments typically pick one price from an interval of possible prices, whereas bidders in our setting face a repeated binary decision. This is due to the fact that for each price step, they have to choose whether to accept the price or to wait. In addition, our design shares important features with centipede games due to its complete information structure and the sequential decision-making of subjects (McKelvey and Palfrey 1992). In the standard version of the centipede game, two players repeatedly choose whether to exit the game or to pass the decision to the other player. Whereas total payoffs increase with the number of times the decision is passed on, each player has the incentive to exit at every stage. Under standard assumptions, rational players immediately exit. Yet, most experimental variations of this game find strong deviations from the Nash equilibrium predictions, with a substantial probability that players pass the decision to others even in later stages of the game (see, for example, Rapoport et al. 2003; Murphy et al. 2006; Palacios-Huerta and Volij 2009; Levitt et al. 2011 and the references cited therein).Footnote 4 Our setup differs from “classic” centipede games in several important aspects. First, in the present setting, more than two players interact with each other and decide simultaneously at each stage whether or not to sell their items. Second, as our game does not end once a single player (of a total of n players) has moved, the relation of bidders and goods in our design does not produce one winner and \(n-1\) losers (as in centipede games), but \(n-1\) winners and one loser. Third, depending on the bids placed in the auctions, winners in our auction may obtain substantially different payoffs. Finally, our experiment is related to clock games as introduced by Brunnermeier and Morgan (2010). In a clock game, several players have to decide when to sell an asset whose value increases exponentially over time. If a player does not sell the asset, he receives an “end of game” payoff that is stochastically determined and relevant for all players. At some point in time, each player receives a private signal that the value of the asset exceeds the “end of game” payoff. If the decisions to sell the asset are observable, the model predicts that players initially wait but that there is “herding” after the first asset has been sold. This pattern is confirmed in experimental tests. Similar to the theoretical papers on descending multi-unit auctions, the pooling of bids is explained by strictly rational behavior. The article proceeds as follows. First, we describe the experimental design and derive the theoretical prediction given standard assumptions (Sect. 2). We then proceed with reporting our experimental results (Sect. 3) and suggest a behavioral model of the interaction between sophisticated and myopic bidding strategies to organize the observed patterns in our data (Sect. 4). The final section discusses our findings and concludes.",3
81.0,4.0,Theory and Decision,25 April 2016,https://link.springer.com/article/10.1007/s11238-016-9547-y,The possibility of Arrovian social choice with the process of nomination,November 2016,Yukinori Iwata,,,Male,Unknown,Unknown,Male,"In this paper, we propose a social choice framework to capture the process of nomination, in which some alternatives become eligible for collective decision making. In the real world, the members of a community often make a collective choice from a list of properly nominated alternatives rather than from all potentially feasible alternatives. For example, the winner of a prize in economics is chosen not from the set of all living economists but from a list of economists nominated in advance. In addition, a political agenda (e.g., which public goods deserve to be provided or where they should be located) depends on the needs of citizens, the interests of politicians and bureaucrats, and the evaluations of specialists. The purpose of this paper is to introduce the process of nomination into Arrovian social choice theory (Arrow 1963). In Arrovian social choice theory, the society makes a collective choice, from a given set of feasible alternatives, by aggregating the preferences of its members. Thus, Arrovian social choice theory, by definition, has a difficulty with regard to analyzing the nomination process because the set of feasible alternatives is specified from outside.Footnote 1 To deal with the process of nomination endogenously, we interpret it as a part of the collective decision making. That is, we consider a two-stage social choice procedure in which some alternatives are first nominated from potentially feasible alternatives, and then the society makes a collective choice from the nominated alternatives. A question posed here is how to describe the process of nomination. We suppose that a subset of the members of the society, called nominators, have their opinions about which alternative should be eligible for collective decision making. In the two examples above, such opinions can be interpreted as the nominators’ recommendations regarding candidates eligible for the prize and as the citizens’ needs, politicians’ and bureaucrats’ interests, and specialists’ evaluations of public goods. We assume that these opinions are expressed in one of three ways, that is, each nominator expresses his positive, negative, or neutral opinion about which alternative deserves to be eligible for collective decision making. The opinions of nominators are aggregated to specify the set of nominated alternatives. We will define the nomination process explained here as a nomination rule. The original idea of a nomination rule can be traced back to Kasher and Rubinstein (1997) and Samet and Schmeidler (2003). They examine the group identification problem, in which the society must identify the individuals eligible for group membership by aggregating the individuals’ dichotomous opinions that are expressed either positively or negatively. In a recent paper, Ju (2010) generalizes the concept of the rules and distinguishes the set of the alternatives from the set of the individuals who are qualified.Footnote 2 We adopt Ju’s (2010) concept of the rules as the definition of the nomination rules and, thus, do not always assume that the set of alternatives is the same as the set of individuals. After the process of nomination, the second stage of social choice is based on a standard Arrovian social choice framework. Voters express their preferences over all the alternatives and they are aggregated to make a collective choice from the alternatives nominated at the first stage of social choice. We describe the second stage of social choice as a social choice correspondence with the set of nominated alternatives. Technically, it has a similar structure to the framework proposed by Denicolò (1985, 1993). One analytical problem of the framework is how to relate the opinions of nominators to the preferences of voters. If an individual expresses both opinion and preference (i.e., if the individual is a nominating voter), it is natural to suppose that his opinion is correlated with his preference. In the public good example above, it is unreasonable to consider that, although a citizen needs one public good more than another public good, the former is expressed as the worst alternative according to his preference. Therefore, we assume that every nominating voter’s opinion restricts the set of admissible preferences expressed at the second stage of social choice and that his preference is consistent with his opinion expressed at the first stage of social choice. More precisely, we assume that he always prefers positive alternatives to neutral ones and neutral alternatives to negative ones according to his opinion. Under this assumption, one may say that an opinion is a less informative preference, that is, a preference that has at most three indifferent classes. In fact, that is true in many cases. Nevertheless, a nominating voter’s opinion may provide much more information than his preference when all alternatives are indifferent for him. This is because he has only one indifference class in terms of his preference, but he can still give one of the three interpretations in terms of his opinion to the indifference class. That is, all alternatives may be positive, negative, or neutral in terms of his opinion, which implies that an opinion is not always a special case of a preference. The two-stage social choice procedure proposed in this paper is descriptively appealing, because it can be applied to many social and political situations in the sense that when a large number of alternatives exist, the society often narrows them down in advance. Moreover, this procedure is normatively appealing, because it has advantages in treating information to make a collective decision in the sense that when a large number of alternatives exist, it is easier to express and aggregate opinions than preferences from a practical point of view. As a result, a nomination rule may drastically narrow the alternatives down to meaningful ones before voters express their preferences. The above discussion emphasizes what is the fundamental information required to make a collective decision in the framework. Unlike standard Arrovian social choice theory, in our framework nominators’ opinions are more fundamental than voters’ preferences. This is because nominators’ opinions have priority over voters’ preferences in that nominators’ opinions determine the set of nominated alternatives before voters express their preferences and each nominating voter’s set of admissible preferences is restricted by his previously expressed opinion. The contributions of this paper are as follows. First, we characterize Arrow-consistent preference domains on which a social choice correspondence with the set of nominated alternatives is defined, which is a similar result to that of Sakai and Shimoji (2006) in terms of standard Arrovian social welfare functions and gives the insight that a diversity of voters’ attitudes to the alternatives is essential to overcoming an impossibility result by Denicolò (1985, 1993). A preference domain is Arrow-consistent if Arrow’s impossibility theorem does not hold on the preference domain. Second, we find a resolution of Arrow’s impossibility theorem in the framework when at least one nominating voter exists. Furthermore, we find a two-stage social choice procedure satisfying some further desirable properties when there exist at least two nominating voters. The remaining sections are as follows: Section 2 presents the model. Section 3 characterizes Arrow-consistent preference domains at the second stage of the social choice framework. Section 4 provides a resolution of Arrow’s impossibility theorem in the framework. Section 5 concludes the paper. Proofs of some of the results are presented in the Appendix.",2
81.0,4.0,Theory and Decision,24 May 2016,https://link.springer.com/article/10.1007/s11238-016-9548-x,Cournot and Stackelberg equilibrium under strategic delegation: an equivalence result,November 2016,Giorgos Stamatopoulos,,,Unknown,Unknown,Unknown,Unknown,,
81.0,4.0,Theory and Decision,24 May 2016,https://link.springer.com/article/10.1007/s11238-016-9551-2,Multiple rounds in a chain store game,November 2016,Michael Melles,Rainer Nitsche,,Male,Male,Unknown,Male,"Multimarket firms, like a chain store, may have an incentive to prey on small entrants even though it is costly in the short run. In complete and perfect information models such predatory behaviour does not arise—the famous chain store paradox (Selten 1978). Milgrom and Roberts (1982, hereafter MR) and Kreps and Wilson (1982, hereafter KW) resolved this paradox by introducing uncertainty about the type of the multimarket firm (hereafter mm-firm), which could, with a small probability, be a tough type that fights no matter what. They showed that by preying, normal firms can imitate the tough type and preserve a reputation of being tough, which deters entry in some future markets. As the game proceeds, entry begins to occur stochastically and will eventually be accommodated. In this note, we show that the stochastic entry of “late” entrants depends on the assumption that single-market firms (hereafter sm-firms) that have decided to stay out “earlier” cannot reconsider their decision to stay out when entry, and the mm-incumbent’s response to it, can be observed elsewhere. In the setup we propose, the game ends only if all remaining sm-firms have decided to stay out. It follows that to the large incumbent, every small entrant looks like the first in a series of M. This multiple round game (“mrg” ) approach preserves the sequential logic in the analysis but eliminates one restrictive timing implication. As a result we find that no sm-firm would like to “test the water” . If the prior that the incumbent is tough is sufficiently high, all firms stay out.",
81.0,4.0,Theory and Decision,11 May 2016,https://link.springer.com/article/10.1007/s11238-016-9550-3,Who are the voluntary leaders? Experimental evidence from a sequential contribution game,November 2016,Raphaële Préget,Phu Nguyen-Van,Marc Willinger,Female,,Male,Mix,,
81.0,4.0,Theory and Decision,18 May 2016,https://link.springer.com/article/10.1007/s11238-016-9552-1,Optimal group composition for efficient division of labor,November 2016,Takuya Sekiguchi,,,Male,Unknown,Unknown,Male,"The problems of collective decision making can be classified into two areas (Nitzan 2009). One is determining the type of social rule desirable in aggregating individuals’ different preferences, as represented by Arrow’s general possibility theorem and its subsequent development in the field of social choice theory. The other is examining group decision accuracy under the assumption that all group members have the same preference. Condorcet’s jury theorem has been a conventional benchmark in the latter research field. This theorem states that if the probability that a group member makes a correct decision for a dichotomous choice situation is greater than one half, the majority vote accuracy is the increasing function of group size. Condorcet’s jury theorem has been extended from various perspectives: dependency among votes (Boland et al. 1989; Ladha 1992), heterogeneous competence (Nitzan and Paroush 1982; Owen et al. 1989), logically interconnected agenda (List 2005), supermajority rule (Fey 2003), and game theoretic situations (Austen-Smith and Banks 1996; Ben-Yashar and Nitzan 2001). The present study considers the maximization of collective benefit within the framework of Condorcet’s jury theorem. However, as Tomiyama (1991) notes, almost all of the extensions of Condorcet’s jury theorem dealt with the entire group’s decision making on a single issue. Nevertheless, in reality, a group facing multiple tasks often assigns the tasks to subgroups to increase the collective benefit. This study’s purpose is to determine what we should do to divide labor efficiently, or more specifically, how to divide an entire group into subgroups to maximize the collective benefit. Tomiyama (1991) has addressed this problem in the case where a group facing two types of tasks is divided into two subgroups. The present study uses approximations whereas Tomiyama (1991) maintained exactness. Thus, the present study can examine more general cases at the expense of the exactness. Consequently, we demonstrate that our result is consistent with Tomiyama’s (1991) one in more general cases once the approximation is accepted. After the demonstration, we also discuss how our and Tomiyama’s (1991) results are robust in the cases where the approximation is not applicable. The paper proceeds as follows. Section 2 describes the framework of the focal problem. Section 3 provides the main results and their proofs, and it considers the situation containing decision-making costs. Section 4 discusses our results and describes the limitations of them. Section 5 concludes, suggesting the future direction.",4
82.0,1.0,Theory and Decision,28 June 2016,https://link.springer.com/article/10.1007/s11238-016-9556-x,Regular preorders and behavioral indifference,January 2017,Mauricio Ribeiro,Gil Riella,,Male,Male,Unknown,Male,"Incomplete preferences is one of the main fields in contemporary decision theory. One of its landmarks is Eliaz and Ok’s (2006) essay “Indifference or indecisiveness? Choice theoretic foundations of incomplete preferences”. It aims to show that we can expand the range of applicability of the “maximizing paradigm” by a suitable relaxation of the weak axiom of revealed preference (WARP). At the same time, the authors propose a framework that allows one to distinguish between indifference and indecisiveness based on observable choice behavior. In this article, we try to clarify, and to a certain extent improve, some aspects of Eliaz and Ok’s analysis. We begin by outlining the basic features of their framework. We then argue that, instead of taking their proposed notion of observable incomparability (which they denote by c-incomparability) as primitive, we should rather start from a notion of behavioral indifference. Although in Eliaz and Ok’s setup both approaches are entirely complementary, in the sense that as soon as one notion is defined the other is also automatically characterized, we believe that the notion of behavioral indifference is more fundamental and has a wider range of applicability. We substantiate this claim by showing some of the implications of such notion to the class of decision models known as models of behavior. We then proceed to analyze a property of preorders introduced by Eliaz and Ok (2006), namely regularity. This property is essential if we want the notion of behavioral indifference to translate to indifference according to the preorder which rationalizes a given choice correspondence. In particular, we show that, as long as we are only interested in rationalizability of a choice correspondence, regularity imposes no further restriction at all: it amounts to a special kind of maximality of the preorder used in the representation of the choice correspondence. However, if in addition one is also interested in capturing other observables aspects of choice behavior, like the notion of behavioral indifference, then regularity is an essential requirement. Finally, we demonstrate that if our aim is to obtain a characterization of the rationalizability of a choice correspondence by a (possibly incomplete) regular preorder, Eliaz and Ok’s weak axiom of revealed non-inferiority (WARNI) is too strong a property and may be weakened. We then propose a suitable relaxation of that postulate that gives us a tight characterization of the class of choice correspondences that can be rationalized by a (possibly incomplete) preorder.",6
82.0,1.0,Theory and Decision,16 June 2016,https://link.springer.com/article/10.1007/s11238-016-9555-y,Risk-induced discounting,January 2017,Marc St-Pierre,,,Male,Unknown,Unknown,Male,"“Time,” said George, “why I can give you a definition of time. It’s what keeps everything from happening at once.” - Ray CummingsFootnote 1
 Time breeds uncertainty. In his pioneering work on capital, Rae (1834) emphasized the importance of the “uncertainty of human life” to intertemporal choice. The connection between time and uncertainty is also well captured by modern definitions of time, such as this one from the oxford dictionary: “the indefinite continued progress of existence and events in the past, present, and future.” This has important ramifications for intertemporal choice models, as time intervals and delays can be loaded with uncertainty relating to other utility-relevant variables, e.g., health, unemployment, recession, and climate change. The connections between time preferenceFootnote 2 and risk are important to understand as they potentially modify the form of the objective function. In this paper, we study the role of risk as the essential motive behind time preference, as assessed by the discount function. We construct a model in which individuals have multi-attribute preferences for lifetime streams of consumption and health statuses as in Bleichrodt and Quiggin (1999) and, more recently, by Lichtendahl et al. (2012). Preferences are represented by a weighted utility framework, where individuals are—to use the terminology of Bleichrodt and Quiggin (1999)—timing neutral, i.e., they have no intrinsic need to discount the future. This allows for a full interpretation of time preference as a risk motive. The key to our approach is to assume that consumption is an intertemporal decision but that the health statuses are random. From this interpretation emerges a risk-induced discount function which existence is entirely due to the presence of health risk. More specifically, it corresponds to a normalized expected utility of health statuses. Our approach is tractable and delivers novel and insightful interpretations by exposing the intertwined role of beliefs and health statuses on the shaping of a discount function. We work in a discrete setting and are able characterize various forms of discounting encountered in the literature, such as exponential discounting and quasi-hyperbolic discounting, in addition to conditions leading to time-consistent discounting in general. The role of risk on time preference has been examined before for single attribute preferences, i.e., the instantaneous utility function is a univariate function of consumption. Recently, Sobel (2013) argues that the discounting axioms imply risk neutrality, i.e., the instantaneous utility function is affine. In the words of the author: “The weakest known discounting axioms are not orthogonal to axioms which yield a von Neumann–Morgenstern utility function.” One can interpret this to mean that time preference and risk aversion are deeply connected in this model. This idea is reflected by the approach of Baucells and Heukamp (2012) who propose a risk/time tradeoff axiom. Ermolieva et al. (2010) provide a catastrophic risk-management perspective. They interpret the (deterministic) discount function as a sequence of probabilities—about the realization of a catastrophe—in a stopping time process. This allows to rewrite the discounted sum as the expectation of a stopping time process (random horizon), which is more suitable to analyze rare events. This interpretation is similar to a commonly held view of discounting as a probability of death, which root causes are often uncertain (Yaari 1965). Other approaches add exogenous risk considerations to an existing framework. In that vein, both Azfar (1999) and Sozou (1998) have independently showed that the constant presence of risk about discount rates can generate hyperbolic discounting (HD), within a standard expected utility framework. Bommier (2006) showed that risk aversion over the length of a life shapes the discount function. Halevy (2008), relying on non-expected utility theories, has generated time-inconsistent preferences also in the presence of an uncertain lifetime. Finally, others assume a discount rate that is function of choices, noting that the presence of risk at the time of making these choices will be reflected the discount rate. In this vein, Epstein (1983) provides a growth model in which the discount rate of the Von Neumann-Mogernstern utility function depends on (past) consumption. In a different context, Becker and Mulligan (1997) propose a model in which the rate of exponential discounting is endogenous to an individual’s investment into the formation of perceptions about the future, e.g. schooling. In contrast to the cited literature, our approach is applicable to a wide range of models, essentially any model that fulfills the four conditionsFootnote 3 of either Theorems 1 or 2 of Bleichrodt and Quiggin (1999). In this model, the multi-attribute preferences are represented by an instantaneous utility function that is multiplicatively separable in consumption and health statuses. The model is a natural extension of the quality-adjusted life-years model (QALY) to life-cycle preferences over consumption and health. The addition of exogenous uncertainty about health statuses at every date, i.e. health risk, necessarily affects how individuals value current consumption over future consumption. It is this discounting motive that the risk-induced discount function captures. However, this function might vary according to the current health status. This poses a difficulty, since such a function can be ill-defined, leaving us with no clear concept of discounting. We address this issue in two separate ways: (1) using full approximation and (2) using partial approximation. With full approximation, conditions on the distribution of the risk are provided ensuring that the risk-induced discount function is independent of health statuses at each date. Otherwise, a partial approximation is considered, consisting of the risk-induced discount function in a specific, default health status—interpreted as a state of normalcy—within each date. For tractability, our results are obtained assuming that health risk is modeled as a discrete Markov process. Starting with a stationary Markov process, we show that full approximation yields the discounted utility (DU) model. The technical result makes use of linear algebra equating the risk-induced discount function to the eigenvalues of the transition matrix, which are necessarily positive and less than unity. This exercise reveals the specific nature of the risk behind exponential discounting, which is described in a few propositions. Of special importance is that the DU model requires the presence of mortality risk, i.e., a state from which it is impossible to transit out of once reached and for which the associated utility is zero. This supports the common view that exponential discounting (Yaari 1965) represents the probability of death, although, as an example shows, this interpretation is misleading if more than two health statuses exist. The rest of the paper is dedicated to full approximation under two non-stationary Markov processes and then to partial approximation under a stationary Markov process. Relaxing only the non-stationarity of the Markov process can approximate non-exponential discounting functions. Time-consistency, however, is still occurring under full approximation when changes in beliefs are anticipated. This is a case, for instance, when anticipated age-specific factors play a role by modifying transition probabilities in a predictable fashion. In contrast, when beliefs are suddenly updated at various points in time due to the reception of new information about the likelihood of future health statuses, the Markov process is delay-dependent and full approximation yields a risk-induced discounting function that displays time-inconsistency. We further investigate the importance of belief updating to the shape of the risk-induced discount function and show that, in particular, the quasi-hyperbolic discounting (QHD) of Laibson (1997) is a full approximation when current and future beliefs are updated in the same way at every date. Other forms of time-inconsistent discounting can emerge depending on the specific flow of information. Finally, we return to the analysis of stationary Markov processes when full approximation is not possible. This occurs if a given risk-induced discount factor takes a different form depending on the current health status. To circumvent this ill-definition, we consider the minimal concept of partial approximation that focusses on the risk-induced discount function for some given recurrent health status, i.e. a default status or a state of normalcy. The main result of this section highlights the fragility of full approximation and time-consistency. We show that some small perturbation of the beliefs yields a partial approximation with a time-inconsistent risk-induced discount function. We also provide a result showing that the QHD model is a partial approximation when individuals’ beliefs are state independent, which incidentally rules out mortality risk. The structure of the paper is as follows. Section 2 provides some brief preliminaries and the following section introduces the model and its assumptions. The main results are provided in Sect. 4, along with several applications. Concluding remarks are provided in Sect. 5, and mathematical proofs, if not in text, can be found in the Appendix.",
82.0,1.0,Theory and Decision,05 August 2016,https://link.springer.com/article/10.1007/s11238-016-9563-y,Resolute majority rules,January 2017,Hyewon Jeong,Biung-Ghi Ju,,Unknown,Unknown,Unknown,Unknown,,
82.0,1.0,Theory and Decision,25 June 2016,https://link.springer.com/article/10.1007/s11238-016-9559-7,"Sleepiness, choice consistency, and risk preferences",January 2017,Marco Castillo,David L. Dickinson,Ragan Petrie,Male,Male,Unknown,Male,"There is growing evidence that individual risk attitudes, as measured by economic experiments, vary across people and circumstances. These include life-cycle changes, traumatic personal or family experiences (Voors et al. 2012; Malmendier and Nagel 2011; Callen et al. 2014), physical conditions (Garbarino et al. 2011; Wozniak et al. 2014), priming and framing (Benjamin et al. 2010), cognitive ability (Dohmen et al. 2010; Burks et al. 2009; Benjamin et al. 2013), the different way in which some may bracket choices (Read et al. 1999) and by ones’ genetic makeup (Cesarini et al. 2009). In this paper, we examine if temporary challenges to cognitive functioning yield choices that are consistent with rationality or whether differences in decisions are due to lapses in rational behavior broadly defined (e.g., choosing randomly). In particular, we investigate whether a sleepiness manipulation through circadian mismatch, which is shown to be associated with impairment of cognitive abilities (Bodenhausen 1990; Kruglanski and Pierro 2008; Dickinson and McElroy 2012), produces changes in preferences while maintaining consistency of behavior. A common assumption in standard economic models (e.g., Arrow-Debreu state-dependent preferences model, Mas-Collel et al. 1995), as well as behavioral models (Tversky and Kahneman 1992; Koszegi and Rabin 2007; Becker and Murphy 1988), is that changes in preferences can occur without the loss of rationality. Our experimental design provides empirical evidence to investigate this assumption. Circadian timing of decisions is a natural environment in which to test the stability and consistency of preferences. First, sleepiness has been widely studied in the sciences, and its effects on performance in many domains are well documented and understood.Footnote 1 Second, it is a physical condition commonly experienced by most people at some, or many, period(s) of their lives. Because of this, circadian mismatch, compared to other ways to temporarily deplete cognitive resources, is a manipulation that is less likely to generate inconsistencies in behavior due to learning or adaptation to the circadian mismatch. This is important because such learning would confound an examination of preference consistency across states. The optimal time-of-day for alertness varies across individuals because of differences in morning or evening diurnal preferences. While time-of-day preference may tend towards morning time as individuals age, it is considered largely independent of gender (see Paine et al. 2006). We, therefore, recruit only young men and women who are validated morning- and evening-type individuals.Footnote 2 Morning- versus evening-type preference individuals are known to have daily alertness/sleepiness cycles that peak at different times-of-day (see Smith et al. 2002), independent of the sleep inertia that builds from cumulative hours awake.Footnote 3 Importantly, results from this type of environment should be relevant to policy. Understanding whether risky choice decisions while sleepy are rational or not and if preferences change could help inform the design of institutions and policies. Our research protocol is designed to reduce the issue of selection, and allow interpretation of our results due to temporary cognitive resource depletion. Specifically, once we identify morning- and evening-type subjects they are randomly assigned to a session (i.e., morning or evening session) and not allowed to select their own session time. This minimizes the problem of subjects selecting into session by time of day preference. To identify validated morning- and evening-type individuals, we invite by email all students at two large universities to complete an online survey (see Appendix 2A for full set of pre-screen survey questions). Within the survey, there is a validated instrument to measure diurnal preference (Adan and Almiral 1991). The data are then used to identify two classic diurnal preference groups: those who are naturally most alert in the morning and those who are naturally most alert in the evening. These morning types and evening types were then randomly assigned to one of two session times: early morning or late night. This produced two treatments, participants who were “matched” in terms of their circadian rhythm (e.g., morning type in a morning session and evening type in an evening session) and circadian “mismatched” (e.g., morning type in an evening session and evening type in a morning session). Participants assigned to one session time were not allowed to switch to the other. Compliance with session assignment was voluntary and, importantly, we find no significant differences in compliance across treatment conditions. Participants were allowed to take all the time they needed to make their decisions, and this was done to allow participants the opportunity to express their preferences unconstrained by time. Our results show a significant treatment effect on risk decisions, and as mentioned above, this is not due to selection or compliance across treatments. Circadian mismatched participants have higher certainty equivalents for different risky asset bundles, indicating they are less risk-averse. Also, the variance in risky asset investment is larger for these subjects, showing a tolerance for more variability in payments. While the manipulation clearly worked as designed and affected preferences, it did not alter the likelihood a subject behaved rationally. Adherence to the generalized axiom of revealed preference (GARP) is identical between mismatched and matched participants.Footnote 4 As a result, the estimated behavioral differences result cannot be attributed to an increase in noisiness of the data following circadian mismatch because an increase in noise would manifest in increased violations of choice consistency of one sort or another. All of this suggests that preferences can be altered without altering adherence to rational behavior. Our paper contributes to the literature by showing that a slight manipulation of physical conditions, which has been shown to produce a temporary challenge to cognition, produces changes in risk attitudes without producing a breakdown of rationality. The reader should note that our results, whereby we present a temporary cognitive resource challenge, are not meant to be directly compared to studies examining the impact of permanent cognitive abilities on risk taking (e.g., Dohmen et al. 2010; Burks et al. 2009; Frederick 2005).Footnote 5 Our manipulation leads to higher certainty equivalents for mismatched subjects, indicating an increased preference for monetary risk when sleepy. Our experiment was not designed to identify the mechanism causing these effects. However, our results do show that the relationship between sleepiness and preferences is causal. To our knowledge, this is the first paper to show that changes in preferences can occur without loss of rationality. The closest paper to ours is that of Burghart et al. (2013), who examine behavioral effects of alcohol intake but conclude that it does not impair rational decision-making. In our data, we do identify a change in risk preference in spite of no difference in rationality.Footnote 6
 In Sect. 2, we describe the experimental design and the cognitive resource manipulation. Section 3 reports our results, first by confirming that our manipulation worked, and then examining rationality and choice behavior in the risk task. Finally, Sect. 4 concludes.",23
82.0,1.0,Theory and Decision,03 June 2016,https://link.springer.com/article/10.1007/s11238-016-9557-9,Modelling curiosity in decision-making,January 2017,Kusha Baharlou,,,Unknown,Unknown,Unknown,Unknown,,
82.0,1.0,Theory and Decision,07 June 2016,https://link.springer.com/article/10.1007/s11238-016-9558-8,How does socio-economic environment influence the distribution of altruism?,January 2017,Hideaki Goto,,,Male,Unknown,Unknown,Male,"Most people are altruistic, at least to some extent. Many human behaviors, from those in laboratory experiments to charitable donations, intergenerational transfers, and the voluntary provision of public goods, exhibit altruism (Camerer 2003; Fehr and Schmidt 2006). This raises the important question of how altruism is formed and what are its implications. Two strands of economics literature answer this question. A strand of literature focuses on evolutionary selection mechanisms and investigates conditions under which altruistic individuals can have a payoff advantage (Alger 2010; Bester and Guth 1998; Bolle 2000; Heifetz et al. 2007; Possajennikov 2000). In this strand, individuals are assumed either to inherit their degrees of altruism from their parents, or to imitate other individuals’ altruism based on those individuals’ material success. In either case, the number of individuals with any level of altruism is increasing in material payoffs. Another strand of the literature sheds light on other important aspect, namely, cultural transmission (Bisin and Verdier 2001; Boyd and Richerson 1985; Cavalli-Sforza and Feldman 1981; Saez-Marti and Sjogren 2008).Footnote 1 For example, in their seminal paper, Bisin and Verdier (2001) analyze a model in which children acquire the preferences of either their parent or a role model in the population with some probability that can be controlled by parents. This paper aims to complement the existing literature by focusing on a still other aspect, namely socio-economic environment and analyzes its effect on individual altruism and the distribution of altruism within a society. The paper is motivated by the following findings in economic, psychological and behavioral genetic research. First, using methods of behavior genetics, Rushton et al. (1986) study 573 adult twin pairs and reveal that genetic effects explain approximately \(50~\%\) of the variance in individual altruism; the remaining \(50~\%\) can be explained by environmental experiences specific to each twin. Furthermore, Cesarini et al. (2009) have recently found that genetic influences explain only approximately \(20~\%\) of variation in altruism, with the remaining nearly \(80~\%\) being explained by experiences idiosyncratic to each twin.Footnote 2
 Then, the important questions is: what kind of experience influences individual altruism? Psychological studies have found that peers affect prosocial development because of their roles as models (Eisenberg et al. 2006; Eisenberg and Mussen 1989). For example, adolescents whose friends involve in community and volunteer work are more likely to volunteer themselves (Zaff et al. 2003), whereas those who belong to a crowd that assigns a high value to having fun are unlikely to volunteer (Youniss et al. 2001). Economists have also found that people exert high effort in the gift exchange game (Gachter et al. 2012) and contribute more to public goods (Bardsley and Sausgruber 2005; Shang and Croson 2009) and to charitable giving (Frey and Meier 2004) if others are doing so. Moreover, people contribute more (respectively, less) to public goods when exposed to others contributing more (respectively, less) than themselves (Alpizar et al. 2008; Croson and Shang 2008; Martin and Randal 2008). In particular, Chen et al. (2010) find that, after being informed of the median contribution, public goods provision changes in the direction to the median level. These studies indicate not only that peers affect people’s behavior but that there are occasions in which people change their behavior regardless of material incentives.Footnote 3
 Importantly, as referred to in Eisenberg et al. (2006), Richard A. Fabes and his coauthors found that influences from prosocial peers were still observed one year later (see also Rice and Grusec 1975; Rushton 1975). This implies that peers’ prosocial behavior is internalized as prosocial preferences. Similar results have been obtained for the effects of strangers (Eisenberg et al. 2006). It is worth noting that the subjects in these studies were affected by others’ behaviors without knowing the true extents of other people’s social preferences (Bryan 1971; Bryan and Walbek 1970; Presbie and Coiteux 1971). Furthermore, there exist substantial cross-cultural differences in the extent of social preferences (Bowles and Gintis 2011; Camerer 2003; Henrich et al. 2004, 2005). For example, Henrich et al. (2005) performed experiments in 15 small-scale societies around the world and demonstrated that mean offers in ultimatum games varied from 26 to \(58~\%\). By statistically analyzing the results, they found that two variables predict cultural differences in offers: the extent to which economic life depends on cooperative institutions (or economies of scale in production), and the degree of market integration. As Bowles and Gintis (2011) put it:  More recent experimental results are consistent with the view that the social preferences that become salient in a population depend critically on the manner in which a people’s institutions and livelihood frame social interactions and shape the process of social learning. Given the above evidence, this paper analyzes how the socio-economic institutions that structure the actions and interactions of people influence the formation of individual altruism and its distribution within a society. It first develops a realistic framework with which the above question can be investigated in a rigorous fashion, and then identifies socio-economic determinants of people’s altruism and its distribution. This paper thereby aims to contribute to a deeper understanding of human altruism and its cultural differences. Moreover, as Thoni and Gachter (2014) point out, “Both social preferences and social influence effects are firmly established empirically, but little is known about how social preferences, which carefully control for material incentives, are influenced by, and related to, peer effects.” This paper may be regarded as an attempt to answer the research question as well. The paper is organized as follows. Section 2 describes the model. Section 3 considers the existence and uniqueness of an equilibrium of the games we analyze in this paper. Section 4 presents the main results, and Sect. 5 concludes the paper. All proofs are included in Appendix 1. Finally, Appendix 2 briefly describes how to extend the model with two-player games to the one with N-player games.",
82.0,1.0,Theory and Decision,06 June 2016,https://link.springer.com/article/10.1007/s11238-016-9554-z,Leadership and the effective choice of information regime,January 2017,Mana Komai,Philip J. Grossman,Evelyne Benie,Female,Male,Female,Mix,,
82.0,1.0,Theory and Decision,13 May 2016,https://link.springer.com/article/10.1007/s11238-016-9553-0,A model of scholarly publishing with hybrid academic journals,January 2017,Damien Besancenot,Radu Vranceanu,,Male,Male,Unknown,Male,"Scholarly publishing of articles in peer-reviewed journals is by far the most important research communication channel. Depending on the underlying economic model, there are two well-established types of peer-reviewed journals: traditional and open access. Using an economic model established three centuries ago, a traditional journal restricts its access to subscribers. With the advent of the internet and the ubiquitous portable document format (pdf), traditional journals have created their own web sites and, gradually, users have shifted from reading the printed edition to downloading and reading electronic versions of the journal provided that they are subscribers. Traditional journals will grant free access to a very small number of high-impact papers as a teaser for marketing purposes. Several journals in Science allow for self-archiving, making the papers available free of charge. At the same time, the internet has supported the emergence of an alternative economic model for scholarly publishing in so-called open access (OA) journals. OA journals maintain editorial boards; some of them run a demanding paper selection process. They allow web users to freely download papers; in turn, many open access journals will charge a publication fee to authors to cover the administrative costs and make a profit. In December 2015, the Directory of Open Access journals recorded 11,000 titles and this number is increasing steadily.Footnote 1
 Until very recently, a significant majority of leading journals relied on the traditional model. In the last twenty years, the academic publishing business has undergone a dramatic process of consolidation, driven by successive mergers between large players and waves of acquisition of smaller presses (Bowen et al. 2013). As a result, most academic journals are now owned by large, for-profit-publishing companies. In 2006, Reed Elsevier, Springer and Taylor and Francis—accounted for 60 % of all journal titles indexed in the ISI Web of Science. If we add the collections by Sage and John Wiley and Sons (which acquired Blackwell in 2007), it is clear that these five major publishing houses (MPHs) have a tight grip on the traditional journal market. With their increased market power, MPHs have the ability to implement aggressive marketing strategies, such as journal bundling, which allows them to charge higher prices and reap higher profits (Edlin and Rubinfeld 2005; Nevo et al. 2005; Dewatripont et al. 2007).Footnote 2 In the last two decades, subscription fees to traditional journals have risen dramatically, with a much steeper increase recorded by for-profit journals compared to non-profit ones. As a consequence, the affordability of these journals has gradually declined. Limiting access to knowledge to only those who can afford to pay a high price is a dubious ethical practice and can harm long-term economic growth by setting a cap on human capital accumulation.Footnote 3
 To avoid these charges while protecting their high returns, in April 2013, the aforementioned big-five MPHs operated a coordinated move to replace the traditional model with a “hybrid” or “two-track” academic journal model. All authors of accepted papers now have the choice between the “traditional” track and an “open access” track (of the same journal). The traditional track is free, but only readers who paid the journal subscription fee have access to the paper, either in the journal’s online or printed version. If the author chooses the open access track, he or she must pay a substantial publication fee, ranging between USD 500 and 5000, often approximately USD 3000 (Müller-Langer and Watt 2015). In turn, the paper becomes freely available to all internet users. The composition of traditional journals might, therefore, gradually evolve toward a hybrid structure, with a subset of papers published under the traditional, restricted access track, and a subset of papers published under the OA track. Because of the increased revenue flow from authors, the MPHs have already announced they will reduce the 2015 subscription fees for journals with a large number of OA papers. For instance, Taylor and Françis clearly indicate that if “paid OA content makes up 95 % of a journal’s content in a volume year, we will convert that title to pure OA in the following year”.Footnote 4
 While hybrid journals existed before 2013, they were seen as a scant curiosity. As noted by Walker (2004), the Entomological Society of America, which publishes four important academic journals, was the first to sell Open Access by article in 2000, followed by the American Society of Limnology and Oceanography, which adopted the hybrid model in 2003. The early experience of these small presses was positive, with a substantial increase in net revenues, and the proportion of authors that chose the OA track rose during the first years of adoption. This paper aims to analyze the prospective evolution of scholarly publishing given the rise of hybrid academic journals. As an original contribution to the literature on scholarly publishing, it investigates whether authors can use the new publication options to signal the quality of their papers, taking into account readers’ beliefs and the profit-maximization behavior of a publishing house. This research question is motivated by existing empirical analyses of the relationship between the number of citations, as a proxy for academic impact, and the recourse to OA facilities as provided by traditional journals. Early empirical analyses have shown that OA papers tend to be cited 3–6 times more frequently than restricted access papers (e.g., Lawrence 2001; Harnad and Brody 2004). However, Kurtz et al. (2006), Gaulé and Maystre (2011), McCabe and Snyder (2013, (2015) and Müller-Langer and Watt (2015) found that the high academic impact of OA papers is explained by their better intrinsic quality, and not the higher accessibility of the OA facilities.Footnote 5 They conclude that the self-selection bias can explain most if not all of the positive impact of OA on citations as observed in the early studies. In other words, so far authors of good quality papers tend to submit them to the good OA journals.Footnote 6
 To analyze how the emergence of the new publication track (OA-per-article) can be used by authors as a signaling device, we introduce the problem as an imperfect information game between authors of accepted papers, readers, and one profit-maximizing MPH that manages one representative hybrid journal. The MPH total revenue comprises publication fees collected on papers registered for the OA track, plus the subscription fee. Only authors know the intrinsic quality of their paper, which can be high or low. The utility of such a normal scholar depends on the potential readership and on the true and perceived quality of the paper. Because the OA track provides better visibility for their work, authors of high-quality papers have an incentive to choose this track even if it means that they have to pay the publication fee. However, in this case authors of low-quality papers might try to benefit from the positive signal and follow the same publication strategy. As all signaling games, this game presents several equilibria in pure and mixed strategies; it will be shown that the type of equilibrium depends on the OA publication fee.Footnote 7 Hence, the MPH can use the fee as an equilibrium selection device; it will choose the fee that maximizes its total revenue. This adds a screening level to the traditional signaling model used to analyze the behavior of authors and readers. The model builds on several restrictive assumptions. It has been assumed that there is only one MPH, running one journal. Our approach is justified because so far the competition among MPHs appears to be relatively weak. Should competition in the publication market increase in the future, the strategic interaction between MPHs must be taken into account. Second, the analysis focuses on the decisions of the myriad of “normal” scholars, and rules out reputation effects associated with the “stars” of the profession. Third, an implicit assumption is that at the end of the transition to the hybrid model, MPHs will choose OA publication fees that authors can afford to pay. Today, the “regular” USD 3,000 fee seems to be prohibitive (Björk 2012), but some journals have already pushed it down to more reasonable levels. The degree of complexity of the problem would increase without fundamentally changing its conclusions if we assume that some (but not all) of the authors of the good papers cannot afford to pay the fee.Footnote 8 Furthermore, should authors’ employers decide to cover the OA publication fee in part or in full, a more sophisticated analysis would be required to account for the strategic behavior of the employer. By focusing on accepted papers, our model cannot account for the certification function of academic journals. Because OA journals collect large fees from authors, they might be tempted to be more lenient in accepting papers. Should this be the case, our finding that authors of good papers might use the OA track as a signal for quality might be disproven. Finally, we will assume that authors know the quality of their paper, which might be challenged if at least some authors have fallen victim to the sin of overconfidence. Given that the generalized adoption of the hybrid model by the MPHs is relatively recent, there are few theoretical papers to which we can directly relate our analysis. One notable set of studies includes analyses by McCabe and Snyder (2005, (2007) and Jeon and Rochet (2010) who modelled the academic publication market as a two-side platform, with journals playing the mediation role between authors and readers. Each side of the platform benefits from the positive externalities of the other side: authors enjoy a larger readership and readers like journals publishing many papers of a high quality. The quality of the papers depends on the resources spent for paper selection. Editors, who aim to maximize profits, must arbitrate between a higher subscription fee (reducing the number of readers) and a higher OA publication fee (reducing the number of authors). Depending on the parameters of the problem, journals would opt for the traditional model, for the OA model, or for the hybrid structure. While these studies explicitly model the process of selecting top quality papers, they simplify the information structure of the problem, either by considering that authors do not know the quality of their paper or that the journal can detect this quality without error. Thus, they cannot study the signaling behavior by informed authors. In contrast to them, our paper simplifies the paper selection process by focusing only on accepted papers, but it builds on a more sophisticated, asymmetric information structure that allows us to address the signaling/screening problem.Footnote 9
 The rest of the paper is organized as follows. The first section introduces the main assumptions. Section 3 presents the equilibria of the game. In Sect. 4, we determine the MPH’s optimal pricing and preferred equilibrium. The last section concludes the paper.",6
82.0,2.0,Theory and Decision,30 June 2016,https://link.springer.com/article/10.1007/s11238-016-9560-1,Explaining robust additive utility models by sequences of preference swaps,February 2017,K. Belahcene,C. Labreuche,W. Ouerdane,Unknown,Unknown,Unknown,Unknown,,
82.0,2.0,Theory and Decision,25 July 2016,https://link.springer.com/article/10.1007/s11238-016-9566-8,A simple framework for the axiomatization of exponential and quasi-hyperbolic discounting,February 2017,Nina Anchugina,,,Female,Unknown,Unknown,Female,"The axiomatic foundation of intertemporal decisions is a fundamental question in economics and generates considerable research interest. Despite the fact that a number of possible ways of discounting have appeared in the literature so far, two types have been predominantly used: exponential discounting, first introduced by Samuelson (1937), and quasi-hyperbolic discounting (Phelps and Pollak 1968; Laibson 1997). The important question to be answered is which axioms allow us to say that the preferences of a decision-maker can be represented using the discounted utility model with exponential or quasi-hyperbolic discount functions? Existing axiom systems for intertemporal decisions address this question. These systems can be roughly divided into two main groups: those with preferences over deterministic consumption streams and those with preferences over stochastic consumption streams. The first group has been the leading approach in the area, both for exponential and quasi-hyperbolic functions. In this framework, a consumption set is endowed with topological structure, and Debreu’s (1960) theorem on additive representation is a key mathematical tool. Koopmans’ result for exponential discounting with deterministic consumption streams (Koopmans 1960, 1972; Koopmans et al. 1964) remains the most well known. A revised formulation of Koopmans’ result was proposed by Bleichrodt et al. (2008), using alternative conditions on preferences. A similar approach was also suggested by Harvey (1986). The axiomatic foundation of exponential discounting for the special case of a single dated outcome was presented by Fishburn and Rubinstein (1982). In a non-stochastic framework with a discrete time space, quasi-hyperbolic discounting has been axiomatized by Olea and Strzalecki (2014).Footnote 1 Building on Bleichrodt et al. (2008) they provide three alternative sets of axioms. Olea and Strzalecki’s axiomatization will be discussed in more detail in Sect. 6. All the axiomatization systems mentioned above are formulated for infinite consumption streams. The finite horizon case has rarely been discussed. For exponential discounting, however, it can be found in Fishburn (1970). The second group of axiomatic systems considers stochastic consumption streams. To obtain an additive form, the fundamental representation theorem of von Neumann and Morgenstern (vNM) (1947) is used. The application of this approach to exponential discounting was given by Epstein (1983). A consumption stream is considered to be an outcome of a lottery. The axiomatization of quasi-hyperbolic discounting by Hayashi (2003) builds on Epstein’s (1983) axiom system. Both Hayashi and Epstein axiomatize preferences over infinite stochastic consumption streams. In this paper, we work with preferences over streams of consumption lotteries, i.e., a setting in which there is a lottery in each period of time. In other words, we restrict Epstein and Hayashi’s framework to product measures. This framework allows us to apply Anscombe and Aumann’s (1963) result from Subjective Expected Utility Theory. The main advantage of this method is that it gives an opportunity to construct the discussed functional forms of discounting in a simpler way. Importantly, the present work establishes a unified treatment of exponential and quasi-hyperbolic discounting in both finite and infinite settings. With Fishburn (1982) and Harvey (1986) as the key sources of technical inspiration, our approach requires relatively simple axioms and facilitates proofs that are relatively straightforward.",3
82.0,2.0,Theory and Decision,26 August 2016,https://link.springer.com/article/10.1007/s11238-016-9565-9,Gender differences in ambiguity aversion under different outcome correlation structures,February 2017,Andreas Friedl,Patrick Ring,Ulrich Schmidt,Male,Male,Male,Male,"Ambiguity aversion describes the tendency that individuals prefer known probability distributions over unknown probability distributions. This behavior was reported by Ellsberg (1961). Since then, it became an important explanation for a variety of observed phenomena, among others are the equity premium puzzle (Collard et al. 2011; Rieger and Wang 2012) and the stock market participation puzzle (Dow and Werlang 1992; Easley and O’Hara 2009). Although a general aversion towards ambiguity is reported in many studiesFootnote 1, empirical findings about gender differences are mixed. While some studies do not observe gender differences (Banerjee 2014; Binmore et al. 2012; Dimmock et al. 2016), others do (Schubert et al. 2000; Borghans et al. 2009; Pulford and Gill 2014). Studies reporting differences in ambiguity aversion between women and men also do not show a clear direction of the effect. Schubert et al. (2000) and Pulford and Gill (2014), on the one hand, find that women are more ambiguity averse than men in the gain domain. Borghans et al. (2009), on the other hand, report the opposite for low levels of ambiguity. From a biological point of view, gender differences in human behavior are to be expected due to different requirements for women and men during the evolutionary past (Buss 1989; Daly and Wilson 2001). Females produce relatively few gametes compared to the number of male gametes. For the production of an offspring, one gamete of each type is necessary. Thus, there is an excessive supply of male gametes. Due to this imbalance, female gametes are the limiting factor of human reproduction and males will compete over it (Bateman 1948; Trivers 1972). Because men face a higher sexual selection than women, they should be more concerned about relative outcomes, i.e. their relative position in society. Women, by contrast, have a higher level of parental investment and less sexual selection. Thus, they should be more concerned about absolute outcomes, i.e. about their access to resources for themselves and their children (Buss 1989; Ermer et al. 2008). Schmidt et al. (2015) show empirically that the above outlined theoretical framework can help to explain gender differences in decision-making under risk. The authors identify the outcome correlation structure as one variable that affects women and men differently and therefore contributes to gender differences in decision-making under risk. The motivation of this paper is to analyse whether there also exists an effect of different outcome correlation structures on ambiguity aversion. As mentioned above, risk-taking behavior is influenced by this experimental feature and we test whether this also holds for ambiguity attitudes. To extend this field to ambiguity appears relevant, because most decisions in the real world are characterized by ambiguity rather than risk (Heath and Tversky 1991). To our knowledge, the effect of different outcome correlation structures on gender differences in ambiguity aversion has not been studied before. In this study, we measure participants’ willingness to pay (WTP) for an investment game. The participants faced a risky lottery first and an ambiguous lottery afterwards. This feature of the experimental design is motivated by the comparative ignorance hypothesis by Fox and Tversky (1995), which states that ambiguity aversion results from a direct comparison of ambiguous and risky alternatives. The difference between the WTP for the risky lottery and the ambiguous lottery is used as an indicator for participants’ ambiguity attitudes. Positive values indicate ambiguity aversion, negative values indicate ambiguity seeking, and zero indicates ambiguity neutrality. In two separate treatments, we study the effect of different outcome correlation structures on gender differences in ambiguity aversion. In the uncorrelated treatment, the outcomes of the investment game were determined individually. In the correlated treatment, the outcomes of the investment game were determined collectively within a reference group. Based on the above outlined theory, we expect that the type of outcome correlation structure has a larger effect on men than on women. In particular, we hypothesize that men are less ambiguity averse under uncorrelated outcomes than under correlated outcomes for the following reason. Under risk, with \(p = 0.5, 50\,\,\%\) of the group should win and 50  % of the group should loose, i.e. inequality is maximal. Under ambiguity, by contrast, probabilities are unknown. Hence, it is possible and should be taken into account that the probabilities of winning and losing are not distributed equally, e.g. 10  % winning and 90  % loosing or vice versa. This alternative reduces inequality and therefore should be more attractive for inequalityFootnote 2 averse individuals (in our theoretical framework: men). In the correlated treatment, this effect is obsolete, because the group wins and looses together. Women caring more about absolute outcomes should be less affected by different outcome correlation structures. Thus, we have the following two hypotheses: 
H1: Men are less ambiguity averse in the uncorrelated treatment than in the correlated treatment. 
H2: Women’s choices are not significantly affected by different outcome correlation structures. In line with previous research, we observe significant levels of ambiguity aversion in the aggregate data set and also in both treatments individually. Analysing the effects for men and women separately, we find that men show significant levels of ambiguity aversion in the correlated treatment, but not in the uncorrelated treatment. In line with our predictions, men’s behavior is significantly affected by the type of outcome correlation structure such that men are less ambiguity averse in the uncorrelated treatment. Women’s choices are not significantly different between the two treatments.",3
82.0,2.0,Theory and Decision,29 June 2016,https://link.springer.com/article/10.1007/s11238-016-9562-z,Social comparison and risk taking behavior,February 2017,Astrid Gamba,Elena Manzoni,Luca Stanca,Female,Female,Male,Mix,,
82.0,2.0,Theory and Decision,29 June 2016,https://link.springer.com/article/10.1007/s11238-016-9561-0,The efficiency of crackdowns: a lab-in-the-field experiment in public transportations,February 2017,Zhixin Dai,Fabio Galeotti,Marie Claire Villeval,Unknown,Male,Female,Mix,,
82.0,2.0,Theory and Decision,29 September 2016,https://link.springer.com/article/10.1007/s11238-016-9573-9,Pool size and the sustainability of optimal risk-sharing agreements,February 2017,Francesca Barigozzi,Renaud Bourlès,Giuseppe Pignataro,Female,Male,Male,Mix,,
82.0,2.0,Theory and Decision,12 August 2016,https://link.springer.com/article/10.1007/s11238-016-9564-x,A note on a recent paper by Dagsvik on IIA and random utilities,February 2017,P. O. Lindberg,Tony E. Smith,,Unknown,Male,Unknown,Male,"In a recent paper in this journal, Dagsvik (2016) addresses the question of what independent random utility (RU) representations are equivalentFootnote 1 to the famous independence-from-irrelevant-alternatives (IIA) assumption of Luce (1959). There are clear connections between Dagsvik’s paper and the paper Lindberg (2012b), in Lindberg’s thesis (Lindberg 2012a).Footnote 2 As we shall clarify below, the key contribution of Dagsvik’s paper is to establish a simpler proof of the main result in Lindberg (2012b) (though under stronger assumptions).",
82.0,3.0,Theory and Decision,06 September 2016,https://link.springer.com/article/10.1007/s11238-016-9571-y,Catastrophic risk: social influences on insurance decisions,March 2017,Michal W. Krawczyk,Stefan T. Trautmann,Gijs van de Kuilen,Male,Male,Male,Male,"The economic consequences of catastrophic events have become more severe in recent years (IPCC 2012; Michel-Kerjan and Kunreuther 2011). On the one hand, it is believed that climate change leads to more variability and more extremity of weather events. On the other hand, migration to and capital accumulation in vulnerable areas have increased over time. To limit the negative consequences of catastrophic events, policies are required that reduce the vulnerability to catastrophic losses and redistribute or shift the exposure to risks to those who are willing and able to bear them. Government-sponsored protection and insurance programs have already been installed in many countries, and will increasingly become important if weather events become more severe in the near future. While the supply of affordable insurance and protection products is crucial, it has been observed that there may also be important problems on the demand side for these products. As Michel-Kerjan and Kunreuther (2011) discuss, take-up of catastrophic insurance, for example, flooding insurance, is surprisingly low. Inhabitants of vulnerable areas might be very hesitant to take up even subsidized insurance products. For example, only 40 percent of residents of the New Orleans parish had flood insurance when hurricane Katrina struck, despite support from the National Flood Insurance Program (Insurance Information Institute 2005). There is little agreement yet on why people do not have strong preferences for insuring catastrophic losses. Assuming that they are typically risk averse with respect to their financial outcomes, fair or even subsidized insurance products should be very attractive. Some important observations have been made in the literature. Risk attitudes and risk perception for small probability events may be subject to significant biases and basic deviations from rationality principles (Botzen and Bergh 2009; Gallagher 2014; Schade et al. 2012; Viscusi and Zeckhauser 2006). The leading alternative theory of decision making under risk, cumulative prospect theory (Tversky and Kahneman 1992) proposes that individuals attach an overly large weight to unlikely but extreme events. In this case, the loss will be overweighted, which again strengthens willingness to insure. However, empirical regularities that gave rise to prospect theory were observed mostly in the case of decisions from description, i.e., when probabilities were explicitly provided in a numerical format. By contrast, when probabilities are only learned by observation or experience, individuals may be prone to the underestimation of low probabilities (Hertwig et al. 2004). In particular, a significant group of people may simply neglect small risks (Botzen and Bergh 2012), but at the same time, over-insure against high-probability low-cost risks (Browne et al. 2015). An important dimension of risk perception and insurance choice concerns the social effects caused by the observation of other decision makers (Kunreuther et al. 2009). Using field data, Gallagher (2014) shows when a county is hit by a flood, residents in neighboring counties increase their insurance take-up. Friedl et al. (2014) argue that the simultaneous over-insurance for high-probability low-cost risk and under-insurance for low-probability high-cost risks is due to social comparison and correlated losses: typical low-probability natural risks are highly correlated across people in a region or neighborhood (e.g., flood insurance as in Browne et al. 2015) and typical high-probability risks are uncorrelated across people (e.g., bike theft as in Browne et al. 2015). In the former case, social comparison does, therefore, not lead to the feelings of loss, because peers also lose, while in the latter case, the loss is felt more strongly. In contrast to these findings, Viscusi and Zeckhauser (2015) have shown that people may in fact not learn much from informative experiences of other people in their social environment. Using field data on tap water contamination, they show that people’s beliefs are not influenced by negative observations of their peers. In the context of low-probability loss events, such a neglect of others’ information significantly delays the learning process about the risk. The current paper builds on these findings. We observe that the aforementioned results derive from a set of studies that each used specific empirical and experimental setups and that it is not clear in how far these findings generalize across different situations. Studies based on field data as in Gallagher (2014), Viscusi and Zeckhauser (2015) or Browne et al. (2015) have high external validity for the type of catastrophic risk under investigation. However, they typically have less control over the underlying mechanisms. For example, it is not clear whether higher take up in neighboring counties after a flood is caused by higher demand by homeowners, or by stronger or more successful marketing effort from the side of the insurance companies. The strong influence of peers on insurance decisions found in some studies seems at odds with the neglect of peer information in the formation of expectations observed in others.Footnote 1 In the context of small probability losses, we, therefore, investigate the robustness of these empirical patterns in one uniform catastrophic loss insurance setting: how do decision makers process probabilistic information in low-probability loss settings? What is the effect of peer outcomes on beliefs and insurance take-up? How are these effects moderated by the correlation of potential losses among people? Low-probability events are especially difficult to study in the field. Moreover, it is not easy to identify causal effects of peers’ behavior and experiences in non-experimental settings (see the discussion in Viscusi and Zeckhauser 2015). Because people may self-select into vulnerable areas (Page et al. 2014), and because this self-selection may interact with insurance choices, few conclusions regarding the effect of exogenous policy changes can be drawn. We, therefore, conduct a controlled laboratory experiment to identify the causal effects of social information and risk correlation (across people) on risk perception and insurance take-up. Our findings can be summarized as follows. In line with the decision-from-experience literature and with the previous field studies on natural disaster risk perception, we find that people underestimate the likelihood of low-probability outcomes. At the same time, they behave risk averse, conditional on these probability estimates, and the resulting subjective expected losses. Moreover, we confirm non-experimental results by Viscusi and Zeckhauser (2015) showing that people seem to significantly discount relevant information available through other people’s experiences. This is an important result, because in contrast to Viscusi and Zeckhauser’s study, in our setup, the other person’s outcomes are unambiguously relevant to the decision maker’s beliefs. However, we do not find any evidence that insurance take-up is affected by the correlation of an individual’s risk of losses with those faced by other participants (Friedl et al. 2014); neither do we find evidence for direct imitation of insurance behavior. We discuss possible reasons for this result. Our findings show that behavioral effects in individual decision making are robust and replicate across vastly different empirical setups, and thus form a good basis for economic policy intervention (World Bank 2015). Social effects seem less robust. More research is needed to establish how policy can benefit from considering these effects.",9
82.0,3.0,Theory and Decision,24 September 2016,https://link.springer.com/article/10.1007/s11238-016-9575-7,Hard evidence and ambiguity aversion,March 2017,Mehdi Ayouni,Frédéric Koessler,,Male,Male,Unknown,Male,"Consider a situation in which a decision-maker wants to implement different actions in different states of the world, but does not observe the true state while an informed agent does. Because the agent’s interest might not be aligned with the decision-maker’s preferences, he can be asked to certify–either partially or totally–the information he reports. When certain constraints (like time, complexity, cost, or technological constraints) preclude the option of unlimited information certification, the decision-maker has to choose which information has to be certified by the agent. An easy way to illustrate this framework is to consider a situation with multidimensional states (representing, for example, all transactions and activities of a firm) where the decision-maker (for example, a financial auditor) has to choose one dimension of the state to be certified by the firm. There are several ways to choose which activities to investigate. The first and most basic is to define rules that associate an activity to investigate to each possible state, for example, investigate the largest activity. The problem with such deterministic rules is that they let the agent know with certainty which dimension is going to be investigated, which implies that he could easily manipulate the decision; hence, the set of implementable allocations will usually be rather small. The second way is for the decision-maker to use mixed strategies, for example, choose randomly between all activities with equal probability. Using such a rule, it could be more difficult for the agent to manipulate the decision, thereby enlarging the set of implementable allocations. Although requesting information certification with mixed strategies could improve the situation for the decision-maker, it does not eliminate the effect of the limitation on information certification. The original idea of this article is to show that if the agent is ambiguity averse in the sense of maxmin expected utility (Gilboa and Schmeidler 1989), then requesting information certification with an ambiguous (instead of a pure or mixed) strategy completely eliminates the effect of limited certification. The reason behind this is that an agent who is ambiguity averse in the sense of maxmin expected utility always anticipates the worst case scenario. In the audit example, if the auditor chooses an activity using an ambiguous strategy, an agent who lies about one or more activities gets investigated on one of these activities in the worst case scenario. Therefore, he would act exactly as if all activities were going to be investigated. In this sense, ambiguity saves time on information investigation. We prove this result in a general model: any allocation rule that is implementable with unlimited certification (with or without ambiguity) is also implementable with limited certification and ambiguity if agents are averse to ambiguity in the sense of maxmin expected utility. We give examples where the converse assertion does not hold, but show that the equivalence holds if there is a single agent and a worst outcome. The topic of mechanisms with certifiable information has been actively investigated in the literature, such as in Ben-Porath and Lipman (2012), Bull (2008), Bull and Watson (2007), Deneckere and Severinov (2008), Glazer and Rubinstein (2001, (2004), Green and Laffont (1986), Forges and Koessler (2005), Kartik and Tercieux (2012), Koessler and Perez-Richet (2014), Sher (2011), Sher and Vohra (2015), Singh and Wittman (2001), Strausz (2016). These papers, among others, give rise to important results about implementable allocation rules and some of them establish a revelation principle for settings with certifiable information. Ambiguity has recently been incorporated into mechanism design mainly in two different ways: either through incomplete preferences as in Lopomo et al. (2013) or through the strategic use of ambiguity as in Bose and Renou (2014) and Di Tillio et al. (2012). In a model of insurance under moral hazard, Lang and Wambach (2013) have shown how uncertainty about an insurer’s cost of an audit leads to ambiguity about the probability of an audit, which in turn induces ambiguity averse agents to undertake less fraud. In this paper, we allow the designer to use ambiguous communication devices, but unlike the cited papers we consider mechanism design problems with certifiable information. In Sect. 2, we present the model. In Sect. 3, we give a characterization of the implementable allocation rules under unlimited information certification. In Sect. 4, we show that ambiguity allows to implement all those allocation rules in settings of limited information certification. We also show that if there exists a worst outcome, then any allocation rule that is implementable with ambiguity is also implementable under unlimited information certification. In Sect, 5, we discuss the implications of our results for communication games with certifiable information, and their limits in settings with multiple agents or other types of ambiguity averse preferences.",4
82.0,3.0,Theory and Decision,20 August 2016,https://link.springer.com/article/10.1007/s11238-016-9568-6,Punishing greediness in divide-the-dollar games,March 2017,Shiran Rachmilevitch,,,Female,Unknown,Unknown,Female,"Dividing a limited resource among a group of agents is one of the most basic economic problems there are. The Divide-the-dollar game, henceforth DD, is one of the simplest models of this problem. The rules of DD are as follows: each of n players submits a demand (or bid) \(x_i\in [0,1]\); if \(\sum _{i=1}^nx_i\le 1\), then each player obtains what he asked for; otherwise—i.e., if the sum of demands exceeds the available resource—no one gets anything. Each player’s utility equals the amount of money he receives.Footnote 1
 DD has two major drawbacks: first, any split of the dollar is supported as a Nash equilibrium outcome; second, even the slightest degree of infeasibility—when the sum of demands equals \(1+\epsilon \), with \(\epsilon >0\) being arbitrarily small—leads to a complete waste of the social resource. These shortcomings led researchers to consider modifications of DD to overcome them. In particular, Brams and Taylor (1994, henceforth BT) considered several variants of DD, one of which—a game they call DD1—will be of special interest in this paper. The rules of DD1 are as follows. Given the vector of demands, the players are partitioned into equivalence classes, according to the amount they demand. Each player in the set of the lowest bidders—the first equivalence class—receives what he asks for if there is enough money to do so; otherwise, the dollar is divided evenly among these players. If there is money left after the just-mentioned step has been applied, the second equivalence class is considered, to which the same rule is applied. This process continues in the same fashion until (i) the dollar is exhausted, or (ii) each player received his demand. DD1 overcomes the drawbacks that were mentioned in the previous paragraph: the entire dollar is distributed to the players if the sum of demands is at least one, and the equilibrium-multiplicity problem is significantly mitigated.Footnote 2 Unfortunately, DD1 suffers from a new drawback: when each player demands approximately the entire dollar, then if the least greedy player is unique, then this player obtains approximately the entire dollar even if he is only slightly less greedy than the other players. For instance, if there are two players and the demands are 0.98 and 0.99, the payoff are (0.98, 0.02). The main contribution of this paper is a parameterized family of 2-person DD games that generalizes DD1. The parameter value, which is associated with each family member, \(\lambda \in [0,1]\), captures the degree to which the greediness-related problem can occur. The higher is \(\lambda \), the smaller is the scope of the problem. When \(\lambda =1\), the problem is completely solved. On the other hand, small values of \(\lambda \) allow for severe punishment of the most greedy player.Footnote 3
 Another issue I address is the fact that the analysis in BT is carried out for the case of discrete bids. Namely, there is a minimal money-unit—a “cent”—and all demands must be multiples of it. There are two reasons to relax this assumption. First, the “cent” takes the form \(\delta =\frac{1}{n K}\), where n is the number of players and K is some positive integer. Therefore, the model contains an unbreakable link between the number of players and the grid of currency—two conceptually unrelated quantities. Second, one would like to set up the environment in a way that gives some flexibility in altering the rules of the game; namely, an environment, in which a variety of DD games can be defined. The discreteness assumption hurts this flexibility. To illustrate the point with a concrete example, consider the following DD game, \(\hbox {DD}^*\), due to Anbarci (2001). In its 2-person version, when the demands are infeasible, namely when \(x_1+x_2>1\), each player i receives the payoff \(f_i x_i\), where \(f_i\equiv \frac{1-x_i}{x_j}\). The economic rationale behind this rule is as follows. When player 1 makes his demand, he offers player 2 the amount \(1-x_1\). Since player 2 demands \(x_2\), it is as if player 1 asks player 2 to settle for a fraction \(f_1\) of what player 2 asks for himself, where \(f_1\equiv \frac{1-x_1}{x_2}\). Similarly, player 2 asks player 1, effectively, to settle for a fraction \(f_2\) of what player 1 asks for himself, where \(f_2\equiv \frac{1-x_2}{x_1}\). Now, if it is legit to expect your fellow bargainer to settle for a fraction f of his demand, then, one may argue, it is perfectly fine to require that you, the proposer, would also settle for this fraction out of what you wish for yourself. This is the logic of Anbarci’s game. Since the \(f_i\)’s are, by definition, fractions, \(\hbox {DD}^*\) is not well defined under discrete demands. For example, consider the most basic case, where the minimal money-unit is 0.01; here, if player 1 demands \(x_1=0.6\) and player 2 demands \(x_2=0.51\), then player 1’s payoff is \(f_1 x_1=\frac{0.24}{0.51}\), which is not a multiple of cents. In reality, money is discrete, and hence, one may argue that this discreteness should be reflected in the model. My point is that even though discreteness is realistic, it expresses a constraint which should not be at the center stage of the analysis. The money-continuity assumption, which is typical to bidding games, is, therefore, in place. The rest of the paper is organized as follows. In Sect. 2, I present a list of conditions for DD games, due to BT, called reasonableness conditions. In Sect. 3, I revisit DD1, but, as opposed to BT, under the assumption that money is continuous (an assumption I will maintain throughout the paper). I denote the DD1 game under continuous demands by cDD1. Both DD1 and cDD1 are reasonable. The choice between continuity and discreteness of money has far-reaching implications. In DD1 there are multiple (payoff-equivalent) Nash equilibria and an equilibrium can be arrived by iteratively removing weakly dominated strategies; in addition, in the 2-person DD1, as in any 2-person reasonable DD game with discrete demands, the egalitarian demand level is weakly dominated.Footnote 4 By contrast, in cDD1, the vector of egalitarian demands is the unique Nash equilibrium, and the egalitarian demand level is not a weakly dominated strategy. In Sect. 4, I present the family of 2-person DD games \(\{\text {DD}^\lambda \}_{\lambda \in [0,1]}\). The generalization of \(\{\text {DD}^\lambda \}_{\lambda \in [0,1]}\) to n bidders is in Sect. 5. In Sect. 6, I conclude.",6
82.0,3.0,Theory and Decision,13 August 2016,https://link.springer.com/article/10.1007/s11238-016-9567-7,The valuation “by-tranche” of composite investment instruments,March 2017,Doron Sonsino,Mosi Rosenboim,Tal Shavit,Male,Unknown,,Mix,,
82.0,3.0,Theory and Decision,25 August 2016,https://link.springer.com/article/10.1007/s11238-016-9570-z,Is diversity in capabilities desirable when adding decision makers?,March 2017,Ruth Ben-Yashar,Shmuel Nitzan,,Female,Male,Unknown,Mix,,
82.0,3.0,Theory and Decision,26 August 2016,https://link.springer.com/article/10.1007/s11238-016-9569-5,Piecewise linear rank-dependent utility,March 2017,Craig S. Webb,,,Male,Unknown,Unknown,Male,"When considering choice under risk, evidence suggests that most decision-makers are simultaneously pessimistic and optimistic—they are ambivalent. It has been argued before that these departures from expected utility can be explained by taking into account the particular salience of the best and worst outcomes of decisions (Lopes 1987; Cohen 1992). An additional focus on the worst outcome is akin to pessimism, and on the best outcome is akin to optimism. In this way, the NEO-expected utility model (Chateauneuf et al. 2007) elegantly extends expected utility to incorporate ambivalence. NEO-expected utility successfully organises several robust empirical findings of choice under risk. It allows for optimism and pessimism, in the sense of Wakker (1994), but also retains expected utility “inside the probability triangle”, where violations are less frequently observed (Abdellaoui and Munier 1998). Due in most part to its tractable form, NEO-expected utility has been applied extensively.Footnote 1 Departures from expected utility are captured using discontinuities in the evaluation formula. Because of these discontinuities, however, the axiomatic foundations of NEO-expected utility are much more complicated than expected utility (see Webb and Zank 2011). 
Piecewise linear rank-dependent utility (\(\text {RDU}_{\text {PL}}\)), the model considered in this paper, is a continuous version of NEO-expected utility. \(\text {RDU}_{\text {PL}}\) is the special case of rank-dependent utility with a piecewise linear probability weighting function. The well-known “inverse-S shaped” probability weighting scheme associated with ambivalence is approximated under \(\text {RDU}_{\text {PL}}\) with “stretched-N” shaped probability weighting. \(\text {RDU}_{\text {PL}}\) could be called an empirical generalisation of NEO-expected utility. In terms of observable choices, NEO-expected utility cannot be distinguished from \(\text {RDU}_{\text {PL}}\). In this sense, the foundational difficulties of NEO-expected utility are resolved with very little cost. Furthermore, \(\text {RDU}_{\text {PL}}\) allows for some additional realism. For example, optimism and pessimism have been observed for non-extreme outcomes (see, for example, Wu and Gonzalez 1996), which is captured to some extent by \(\text {RDU}_{\text {PL}}\) in a way that is ruled out by NEO-expected utility. 
Webb (2015) gave an axiomatisation of the analogue of the \(\text {RDU}_{\text {PL}}\) model under purely subjective uncertainty—the Savage framework. In this paper, \(\text {RDU}_{\text {PL}}\) is axiomatised using objective probabilities—the von Neumann–Morgenstern framework. Only the richness of the probability interval is used, the outcome set can be arbitrary. Hence, the theory may be applied to monetary outcomes, health outcomes, indivisible goods, and so on. The key axiom under risk, called complementary additivity, is more intuitive, easier to test empirically, and the proof is shorter and more direct. The formal definitions are outlined in Sect. 2. The piecewise linear rank-dependent utility model is presented in Sect. 3. A simple tradeoff axiom, necessary for rank-dependent utility, is presented in Sect. 4. The axiomatic foundation of piecewise linear rank-dependent utility is presented in Sect. 5. All proofs are contained in the Appendix.",2
82.0,3.0,Theory and Decision,19 September 2016,https://link.springer.com/article/10.1007/s11238-016-9572-x,Information transparency and equilibrium selection in coordination games: an experimental study,March 2017,Jia Liu,Yohanes E. Riyanto,,,Unknown,Unknown,Mix,,
82.0,3.0,Theory and Decision,13 October 2016,https://link.springer.com/article/10.1007/s11238-016-9574-8,"Overlapping coalitions, bargaining and networks",March 2017,Messan Agbaglah,,,Unknown,Unknown,Unknown,Unknown,,
82.0,4.0,Theory and Decision,24 September 2016,https://link.springer.com/article/10.1007/s11238-016-9576-6,Linear symmetric rankings for TU-games,April 2017,L. Hernández-Lamoneda,F. Sánchez-Sánchez,,Unknown,Unknown,Unknown,Unknown,,
82.0,4.0,Theory and Decision,30 September 2016,https://link.springer.com/article/10.1007/s11238-016-9577-5,Subgame perfect equilibrium in a bargaining model with deterministic procedures,April 2017,Liang Mao,,,,Unknown,Unknown,Mix,,
82.0,4.0,Theory and Decision,10 December 2016,https://link.springer.com/article/10.1007/s11238-016-9583-7,Clan information market games,April 2017,Saadia El Obadi,Silvia Miquel,,Female,Female,Unknown,Female,"
Muto et al. (1989) introduced information market games. They modelled the trading of information between one informed firm and some other initially non-informed firms. Later, Potters and Tijs (1990) allowed more than one initially informed firm to exist. In both cases, in information market games with one informed player (Muto et al. 1989) and with more than one informed player (Potters and Tijs 1990), the information is unique. In the first case, only one firm has the information, and in the second more than one player has that same information. Like Potters and Tijs (1990), we consider that more than one player has information. However, the information that each player has is not the same, but complementary. Similarly to Muto et al. (1989), the aim of this paper is to analyze the cooperative behaviour of firms, faced with the introduction of some new technology, which is essential for manufacturing a new product. However, now we ask what happens if the new product needs more than one new technology to manufacture it. In this paper, this situation, which implies that more than one informed player is needed to produce the good, is considered. Let us consider a simple example:Footnote 1 the production of waterproof books. Two technologies are needed to produce these: one for the paper and another for the suitable ink. Thus, we need two players, two patent holders, to be informed and be able to develop the new product. Furthermore, we assume that the market for waterproof books is the whole of Europe, which is divided into submarkets (each corresponding, for instance, to a set of European regions). The firms or group of firms have the right and possibility to enter one or another submarket. For each submarket, the maximal profit which can be achieved by producing and selling waterproof books is known. In this new situation, the information is divided into several parts (or technologies) and each patent holder initially possesses just one of the parts. Let us call clan the finite set of patent holders. As we assume perfect patent protection and also that the production of the new commodity needs all those patents, all the members in the clan together may monopolize every submarket the clan has access to. Nevertheless, no single informed player has the technology required to produce the new commodity. Each firm or group of firms has the possibility to manufacture the new product if they have all the technological information. They can obtain maximum benefit in the submarket they have access to only if they know the new technologies. By sharing these technologies with other firms (licensing), the clan may access and make a profit indirectly in other submarkets where the clan has no access by itself. Therefore, faced with this situation, cooperation is beneficial. Formally, such an information market where complementary pieces of information are distributed among more than one player is defined by where \(N=\{ 1,2,\ldots ,n\}\) is the set of firms, and \(C\subseteq N\) is the clan, i.e., the set of patent holders. Further, Fig. 1
Footnote 2 shows the partition of the consumers’ market into submarkets. For each submarket, there is a set of firms \(T\subseteq N\) which are the only ones able to access this submarket. Muto et al. (1989) named \(M_T\) the submarket where only firms in \(T\subseteq N\) have access to. Since this notation becomes superfluous here, we do not use it. The maximal profit obtainable in a submarket controlled by \(T\subseteq N\) (whenever the information is available) is \(r_T\in {\mathbb {R}}_+\). Market partition (example with \(N=\{1,2,3,4\}\)). For each submarket in the partition, only agents of a given coalition \(T\subseteq N\) have access to and \(r_T\in {\mathbb {R}}_+\) stands for the maximal attainable profit in that submarket The convenience of cooperation among firms seems clear. The question is to which firms the informed players will sell their license rights and at what price. Cooperation among players in information markets was also studied by Slikker et al. (2003) and Brânzei et al. (2001). They followed Aumann (1999) in assuming that players do not have perfect information on the true state of the world. They consider that the outcome of the decision that any player makes depends on the true state of the world. In the first case, different players have to make decisions and sharing their information might increase joint profits. In the second case, only one action taker can improve its action choices by gathering information from some players who are more informed about the situation. This paper is organized as follows. The next section presents the concepts on cooperative game theory that will be referred to throughout the paper. In Sect. 3, the cooperation in information markets with more than one player owning part of the information is considered. The corresponding cooperative game with side payments is defined. It is named clan information market game (CIG). Muto et al. (1988) showed that the class of information market games is generalized by the class of big boss games, where the presence of the big boss is necessary for a coalition to attain any profit. When more than one agent is indispensable, Potters et al. (1989) introduced the class of clan games. In a short note, Tijs (1990) introduced clan information market games as an example of clan games with no further development of the model. Clearly, every clan information market game is a clan game, although the opposite is no longer true. The existence of population monotonic allocation schemes (PMAS) is studied in Sect. 4. Section 5 shows that the \(\tau \)-value of a clan information market game also has some population monotonicity property: it yields a bi-mas. Finally, Sect. 6 characterizes the class of clan information market games and provides conditions on the market, under which the Shapley value belongs to the core.",1
82.0,4.0,Theory and Decision,28 October 2016,https://link.springer.com/article/10.1007/s11238-016-9578-4,For whom does social comparison induce risk-taking?,April 2017,Oege Dijk,,,Male,Unknown,Unknown,Male,"In the popular press, the excessive risk-taking by bankers and traders in the run-up to the latest financial crisis is often blamed on a ‘bonus culture’ among financial professionals. In a survey performed after the financial crisis among risk and compliance officers at financial services firms, 72 % of respondents agreed that the bonus culture in the City of London had led to “uncontrollable risk-taking”.Footnote 1 Indeed in a recent experiment with financial professionals, these professionals proved to respond to the mere display of their rank stronger than to actual bonus incentives (Kirchler et al. 2016). And it is not just financial professionals for whom risk appetite may depend on their social surroundings. Also, households tend to invest more in the stock market when they socialize more with their neighbors (Hong et al. 2004), and when those neighbors invest more themselves (Brown et al. 2008). This suggests that social context can influence people’s risk-taking decisions, and thus it is important to investigate this channel to understand how and for whom social comparison can lead to increased or even excessive risk-taking. In the same way that choices over risky gambles reflect the shape and curvature of a utility function over monetary outcomes, choices over risky gambles in a social context would reflect the shape and curvature of the social comparison function. The social comparison function entails two important parts: the utility one experiences when having a better outcome than a peer (referred to in this paper as social gain seeking), and the utility one experiences when having a worse outcome than a peer (referred to in this paper as social loss aversion).Footnote 2 In an extension of earlier work on the shape of the social comparison function and emulating behavior (Clark and Oswald 1998; Maccheroni et al. 2012), I show that when social gain seeking is stronger than social loss aversion, an agent would (i) prefer gambles that are negatively correlated with the outcomes of their peers and (ii) take more risk in a situation where social comparison is possible. By contrast agents for whom social loss aversion is stronger than social gain seeking, would prefer positively correlated outcomes and could even reduce risk-taking when social comparison is possible. In a simple between-subjects experimental risky investment game with both a social and an individual treatment, I find a strong correlation between choices for negatively correlated outcomes and risk-taking in the social COMPARISON treatment. Overall, about one third of subjects reveal a preference for negatively correlated outcomes, and these subjects invest almost 50 % more of their endowment in a risky investment than comparable subjects in the baseline INDIVIDUAL treatment. Subjects that reveal a preference for positively correlated outcomes in the social COMPARISON treatment do not invest significantly more than the individual control group. Thus this paper provides evidence that social comparison could lead to more risk-taking as long as there are sufficiently many individuals for whom social gain seeking is stronger than their social loss aversion. Furthermore, this paper shows that there is significant and important individual heterogeneity when it comes to risk-taking and social preferences, and provides a novel tool (preference for correlated outcomes) to investigate such heterogeneity. These findings corroborate recent results on the physiological and neurological responses to social gains and social losses, where subjects showed bigger physiological and neurological responses to social gains than to losses (Bault et al. 2008, 2011). The results presented in this paper provide further evidence for an important implication of such preferences, namely the increased and possibly excessive amount of risk-taking that a social context can induce for part of the population (see Hong et al. 2004; Brown et al. 2008). Combined with the findings of Linde and Sonnemans (2012) that found that when it comes to social comparison subjects behave less risk-averse in social gains than in social loss situations (contra the predictions of traditional Prospect Theory, Kahneman and Tversky 1979), this paper further adds to the evidence that social reference points may have different effects on risky choice than traditional private reference points. This paper is related to a few different strands of literature. In the first place there is the strand of literature that focuses on the effect of social comparison on economic decision-making in general, starting with Veblen’s Theory of the Leisure Class (1899). Since then, social comparison has been implicated in saving behavior (Duesenberry 1949; Bertrand and Morse 2013; Frank et al. 2014), the demand for positional and non-positional goods (Hirsch 1976; Frank 1985), wage compression within firms (Frank 1984) and excessive consumption of status goods (Ireland 1998; Hopkins and Kornienko 2004). It has also been shown to have an effect on happiness (Luttmer 2005) and wage satisfaction (Clark et al. 2009), and could explain the Easterlin paradox (Clark et al. 2008). Furthermore, the existence of relative preferences would have implications for public good provision and taxation (Aronsson and Johansson-Stenman 2008; Ireland 2001), economic growth (Corneo and Jeanne 2001; Cooper et al. 2001), environmental policy (Wendner 2005) and even monetary stabilization policy (Ljungqvist and Uhlig 2000). The second strand of literature is related to the shape of the social comparison function itself. Clark and Oswald (1998) theoretically show that comparison-concave preferences lead to emulation and herding behavior, whereas comparison-convex preferences give rise to diversity and deviance. With a comparison-convex utility function the marginal utility from social comparison is (strictly) increasing the further one is ahead, whereas the marginal disutility from comparison is (strictly) declining the further you are behind. Thus for any comparison-convex utility function, social gain seeking is necessarily stronger than social loss aversion. When utility is comparison-concave, social loss aversion is stronger than social gain seeking. Maccheroni et al. (2012) show that the results of Clark and Oswald (1998) hold broadly and only depend on the convexity or concavity of the kink around the reference point. I show that the distinction also affects preferences over positively or negatively correlated outcomes in gambles. Most work on social preferences that follow in the vain of Fehr and Schmidt (1999) and Bolton and Ockenfels (2008) assume highly comparison-concave preferences where individuals not only get disutility from disadvantageous inequality but also from advantageous inequality. Also, most papers that model relative preferences as a function of the difference between own consumption and average consumption in the population (e.g. Rauscher 1992; Mui 1995; Cooper et al. 2001, etc) assume that this social comparison function is indeed concave (social loss aversion being stronger than social gain seeking), although they do not posit disutility from disadvantageous inequality. This comparison concavity then leads to overconsumption of positional goods, which to some extent can be remedied by progressive taxation. A different strand of literature that looks at preference for social rank (e.g. Robson 1992; Frank 1985; Hopkins and Kornienko 2004, etc) presumes that individuals have preferences over the rank ordering of income or consumption, and that the difference between first place and second place is bigger than the difference between second place and third place. These kind of preference specifications could be seen as comparison-convex (social gain seeking stronger than social loss aversion). Thus the theoretical literature is still divided on how best to specify relative preferences. This paper provides evidence that relative preferences may vary significantly across the population. Finally, this paper touches upon the small but growing literature on the intersection of social preferences and risky choice. A nice recent overview of work on social influences and risk is provided by the handbook chapter of Trautmann and Vieider (2011). Bault et al. (2008) find that when subjects can observe both the outcome of their own lottery choice and the outcome of another subject, they react more strongly to social gains than to social losses, both in their subjective appraisal and their physiological reactions. Bolton and Ockenfels (2008) find that subjects act more risk-averse when the outcome of a lottery extends to another subject, and are more likely to choose a risky option when the safe option implies unfavorable inequality. Linde and Sonnemans (2012) let subjects choose between risky lotteries when a reference subject either has a high fixed payoff or a low fixed payoff. They find less risk-aversion when outcomes are contextualized as social gains than as social losses. Dijk et al. (2014) find that fund managers in an experimental investment game tend to increase their investments in positively skewed lottery-type assets when lagging behind in a tournament setting, but also find the same type of behavior when only the relative rank in performance is displayed, even without the corresponding tournament incentives. Similarly, Fafchamps et al. (2013) find evidence that subjects increase their risk-taking after observing previous subjects winning similar lotteries. Schoenberg and Haruvy (2012) show that larger asset bubbles occur when subjects learn about the wealth of the leading trader than when they learn about the wealth of the laggard. On the other hand, Rohde and Rohde (2011) do not find a significant effect of the risk that other subjects are exposed to on the risk attitudes of a decision-maker. Delgado et al. (2008) find that social loss aversion (fear of losing a bidding war with others) could explain overbidding in auctions. Linde and Sonnemans (2015) study a decision between two options of three potential outcomes each with the same expected value. They find no difference when subjects make this decision individually, or when they are matched with two other subjects that made the same decision and each subject gets one of the three potential outcomes.",3
82.0,4.0,Theory and Decision,14 November 2016,https://link.springer.com/article/10.1007/s11238-016-9581-9,Prospect theory and tax evasion: a reconsideration of the Yitzhaki puzzle,April 2017,Amedeo Piolatto,Matthew D. Rablen,,Male,Male,Unknown,Male,"The expected utility theory (EUT) model of tax evasion predicts a negative relationship between tax rates and evasion whenever fines are imposed on the evaded tax and taxpayers exhibit decreasing absolute risk aversion (Yitzhaki 1974). Although empirical evidence on this question is mixed, and depends to an extent on the econometric methodology used (Bernasconi et al. 2014), a substantial body of empirical and experimental evidence finds a positive relationship between evasion and the tax rate (see, e.g., Ali et al. 2001; Alm et al. 1995; Clotfelter 1983; Crane and Nourzad 1986; Friedland et al. 1978; Pommerehne and Weck-Hannemann 1996; Poterba 1987).Footnote 1 Owing to the weight of contradicting empirical evidence, and its counter-intuitive nature, the negative relationship between tax rates and evasion predicted by the EUT model has sometimes been termed the “Yitzhaki paradox” or “Yitzhaki puzzle” . Prospect Theory (PT) has become a centrepiece of behavioural economics, for it is able to resolve many puzzles associated with EUT and it provides a better fit to much empirical data (Bruhin et al. 2010).Footnote 2 It remains disputed, however, whether the application of PT to tax evasion resolves the Yitzhaki puzzle: Bernasconi and Zanardi (2004), Dhami and al-Nowaihi (2007) and Yaniv (1999), among others, each illustrate a specification of PT that resolves the Yitzhaki puzzle, but Hashimzade et al. (2013, p. 16) show alternative specifications that do not. Unpicking these divergent results is far from straightforward, however, as the elements of PT are specified differently across studies and as some studies invoke auxiliary assumptions (in addition to those of PT). Hashimzade et al. (2013) do consider some general specifications of PT, but do not directly compare to other preference models, while Dhami and al-Nowaihi (2007) undertake a comparison of PT and EUT, but not in a unified framework that nests EUT and PT, and for only one specification of PT. In this paper, therefore, we seek to evaluate, in a general sense, the marginal contribution of the elements of PT—individually and collectively—towards resolving the Yitzhaki puzzle. By disentangling the separate driving forces, we are able to reconcile seemingly contradictory results in the literature and to clarify which of the elements of PT, if any, contribute to solving the Puzzle. As these elements are now widely applied in the broader literature on behavioural decision making, by isolating the different components, our results can be readily extended to many further behavioural models besides PT. We perform our evaluation in a general environment—which we shall term the Taxpayer behavioural model (TBM)—in which it is possible to vary (i) the specification of reference income; (ii) the elements of PT that are assumed to hold—to separate out the distinct effects of reference dependence, diminishing sensitivity, loss aversion, and probability weighting; (iii) the auxiliary assumptions assumed to hold vis-à-vis those of the standard portfolio model of tax compliance. The TBM is sufficiently general to encompass much of the existing literature, but sufficiently specific to yield conditions with clear economic and psychological interpretation.Footnote 3
 Our first contribution is to show that several seemingly disparate approaches to the specification of reference income in the existing literature are variants of a simple, yet general, formulation. Within this general formulation, our two main results are as follows: first, matching Yitzhaki’s original demonstration of the Puzzle under EUT, we give apparently plausible conditions under which the Puzzle still holds under PT (and stripped-down variants). Second, we find that, although some specifications of PT do reverse the Puzzle, such reversals often rely on the psychologically questionable implication that a tax rise makes taxpayers feel subjectively richer (relative to reference income) in the not-caught state, and in expectation. In particular, for PT to resolve the Puzzle, this condition must hold when preferences are homogeneous, a common assumption in applications of PT. Thus, while our results do not necessarily endorse the descriptive validity of EUT, we find nonetheless that a set of specifications of PT—which includes many specifications proposed in the literature—is either psychologically questionable, or share similar descriptive deficiencies in respect of the Puzzle. We examine the implications for both EUT and PT (among other variants) of allowing for two auxiliary assumptions: social stigma costs and a variable audit probability. Allowing for sufficient social stigma always resolves the Puzzle under EUT, but not always under PT. In contrast, allowing for a variable audit probability does not clearly improve the ability of either EUT or PT to resolve the Puzzle. In general, the conditions under which these auxiliary assumptions improve the predictions of PT with respect to the Puzzle are the same as those which also improve the predictions of EUT. By allowing for stripped-down variants of PT, we observe the marginal contribution of each of its elements. Under PT preferences, reference dependence is necessary to overturn the Yitzhaki puzzle, but the remaining PT elements—diminishing sensitivity, loss aversion and probability weighting—are neither necessary nor sufficient. These findings seem consistent with the nascent literature on the relative economic importance of the PT elements—reference dependence being the most widely accepted (see, e.g., Barberis 2013; Santos-Pinto et al. 2015). The results of this study contribute to the literature on the use of non-expected utility preferences (and PT in particular) to explain tax evasion, and to the wider literature on the descriptive usefulness of non-expected utility preferences (Kim 2005; Harrison and Rutström 2009; Bruhin et al. 2010; Isoni 2011; Rees-Jones 2014; Masatlioglu and Raymond 2016). We do not claim that EUT is descriptively superior or inferior to PT over the full gamut of empirical regularities on tax-related behaviour, and other evidence relating to behaviour in risky settings more generally. Our results do, though, lead us to question the claim that PT is inherently better able to reconcile the Yitzhaki puzzle than is EUT. The rest of the paper is organised as follows. Section 2 introduces a general model of the tax evasion decision that nests both PT and EUT preferences. In Sect. 3, we analyse the model for a fixed audit probability, and then for a variable audit probability. Section 4 concludes with a discussion of our findings and some wider thoughts on the choice of reference income and of preferences in applications of PT to tax evasion. All proofs are given in the Appendix.",16
82.0,4.0,Theory and Decision,17 November 2016,https://link.springer.com/article/10.1007/s11238-016-9582-8,Estimating cumulative prospect theory parameters from an international survey,April 2017,Marc Oliver Rieger,Mei Wang,Thorsten Hens,Male,,Male,Mix,,
82.0,4.0,Theory and Decision,28 October 2016,https://link.springer.com/article/10.1007/s11238-016-9579-3,Outsourcing with identical suppliers and shortest-first policy: a laboratory experiment,April 2017,Flip Klijn,Marc Vorsatz,,Male,Male,Unknown,Male,"In the last decade, outsourcing has become increasingly important as advanced products are typically no longer completely built in-house. Hence, apart from managing their own production facilities, companies have an increasing need to tightly control outsourced operations. In this paper, we report on a laboratory experiment that deals with a stylized model of decentralized decision-making situations in which companies outsource production orders or jobs to multiple identical suppliers. We assume that each company can only freely decide to which supplier it outsources each of its jobs. In particular, outsourcing companies cannot decide on the order in which each supplier processes the received jobs. This assumption reflects the fact that most transactions in supply chains are governed by price-only contracts. A drawback of these contracts is that they do not coordinate the supply chain. That is, locally optimal decisions may lead to a joint outcome that can be improved upon (see, e.g., Perakis and Roels 2007). We consider the well-known shortest-first policy (i.e., Smith 1956 rule) where each supplier processes jobs optimally by placing them in order of increasing processing time. We assume that each outsourcing company is selfishly interested in minimizing the sum of completion times of its jobs. This may reflect for instance the fact that different jobs of the same outsourcing company are part of different final products, all of which the company aims to produce and sell as soon as possible. Thus, individual agents interact to make decisions that affect them collectively. The results of Hamers et al. (2015) imply that this strategic interaction needs not lead to socially optimal schedules. In fact, there is no centralized mechanism that implements socially optimal schedules in Nash equilibrium. In a related study, Braat et al. (2015) provided a tight bound for the price of anarchy, i.e., the ratio between the social costs in the worst Nash equilibrium and the optimal social costs. Their theoretical results and simulations suggest that it is hard to coordinate outsourcing decisions in such a way that social costs are relatively small. 
Objective
 Our laboratory experiment aims to complement the above results by determining to which extent and under which conditions coordination failures occur or are less likely to occur. Most of the literature that studies coordination mechanisms for supply chains and competitive scheduling environments considers one-shot games (for a review see, e.g., Li and Wang 2007). In practice, however, firms usually compete repeatedly in market situations facing the same or possibly different competitors. In particular, this raises the question whether and to which extent there is a difference (in terms of costs, individual behavior, etc.) between outsourcing problems in which firms face the same competitors and outsourcing problems in which firms face possibly different competitors in each period. 
Laboratory experiment in a nutshell
 We carry out a laboratory experiment that mimics a stylized outsourcing problem with two players and two identical suppliers (the latter will be referred to as “machines”). There are four jobs denoted by A, B, C,  and D which only differ in their processing times: \(p_{A}<p_{B}<p_{C}<p_{D}\). Each player owns exactly two jobs. We study the two arguably most interesting stage games. In one stage game (“AC”) some player owns jobs A and C, and in the other stage game (“AD”) some player owns jobs A and D. In each session, each of the two stage games is repeatedly played during twenty rounds. We use two different matching mechanisms. In the so-called Partner treatment each subject has a fixed opponent during all twenty rounds, while in the so-called Random treatment in each round each subject is randomly matched to some (possibly distinct) opponent.Footnote 1
 From a game-theoretical perspective the stage games are such that it is never worthwhile for players to send both jobs to the same machine. This leaves the players with two undominated strategies. Then, it is easy to see that both games have strong coordination effects. For instance, if one player sends her shortest job to one machine, the other player has incentives to send her shortest job to the other machine. The two games differ because in game AC there is a unique Nash equilibrium in mixed strategies, while in game AD there are two pure strategy Nash equilibria that sustain the optimal social costs and a continuum of mixed strategy Nash equilibria with higher social costs. Finally, in both games the optimal social costs can also be implemented via strategy profiles that consist of dominated strategies. 
Main findings
 Our findings are as follows. First, even though our setting is based on possibly the simplest non-trivial outsourcing situation, we observe a non-negligible play of dominated strategies in both treatments which slightly diminishes over time. This behavior can partly be justified for game AD in treatment Partner, as some groups manage to reach repeatedly the optimal social costs via these decisions. Second, subjects coordinate more often—that is, the optimal social costs are obtained with a higher frequency—if they are matched in fixed pairs than under a random matching only in game AD. But this is not because subjects play in treatment Partner a socially optimal pure strategy equilibrium and in treatment Random a socially non-optimal mixed strategy equilibrium (as one might guess): surprisingly the aforementioned coordination on a socially optimal dominated strategy profile is the driving factor of this finding. In game AC, on the other hand, the degree of coordination in treatment Partner is the same as in treatment Random. Third, for both games, the two treatments present the same average social costs. Finally, since our results stand in contrast with expected payoff maximization, we study theoretically the effect of “social preferences” in games AC and AD. We show that preferences for efficiency cannot explain our experimental results as these preferences induce the same equilibrium structure in the two games. Inequality aversion, on the other hand, constitutes a possible explanation since (efficient) dominated strategy profiles can be sustained in equilibrium in game AD but not in game AC. 
Related experimental literature
 Apart from the theoretical literature mentioned earlier, there is a large number of related laboratory experiments on coordination games. For example, Cooper et al. (1989) show that in the “Battle of the Sexes Game” (where subjects have opposing preferences over the set of pure strategy equilibria) coordination cannot be established in the long run. However, if one player communicates the strategy she is going to follow in a non-binding way, then the degree of coordination improves significantly. If the game has multiple Pareto-rankable equilibria in pure strategies, i.e., the “Stag-Hunt Game,” Cooper et al. (1990, 1992) find that subjects end up in the inefficient equilibrium and that the presence of dominated strategies is not inconsequential (as we do). Clark and Sefton (2001) complement these works by showing that playing the game in fixed pairs helps subjects coordinating on the efficient outcome. Also, according to Brandts and Cooper (2006) and Brandts et al. (2007) it is possible to move away from the bad equilibrium if one player sticks to her part of the good equilibrium, hoping that the other player changes beliefs. Along this line, Terracol and Vaksmann (2009), Danz et al. (2012), and Hyndman et al. (2012) identify players who willingly forego immediate payoffs to be able to coordinate later on a better equilibrium. More recently, Hyndman et al. (2014) find that in a game with strategic complementarities a fixed matching leads to higher payoff variability. Our study is different in that the pure strategy equilibria in game AD are not Pareto rankable and that game AC has a unique equilibrium in mixed strategies. Therefore, we cannot expect such a teaching effect. Finally, experiments on coordination games typically do not involve dominated strategies. Our study thus allows to analyze if and how the presence of dominated strategies and the presence of pure strategy equilibria that sustain the optimal social costs (i.e., a comparison between game AC and game AD) affect play and the degree of coordination.",
83.0,1.0,Theory and Decision,20 December 2016,https://link.springer.com/article/10.1007/s11238-016-9584-6,Comparing uncertainty aversion towards different sources,June 2017,Aurélien Baillon,Ning Liu,Dennie van Dolder,Male,,Male,Mix,,
83.0,1.0,Theory and Decision,05 January 2017,https://link.springer.com/article/10.1007/s11238-016-9585-5,Stake effects on ambiguity attitudes for gains and losses,June 2017,Ranoua Bouchouicha,Peter Martinsson,Ferdinand M. Vieider,Unknown,Male,Male,Male,"Some form of uncertainty is involved in most important decision-making processes, and uncertainty about the future may, indeed, be considered inherent to the human condition. It is thus not surprising that how people react to such uncertainties has received considerable attention in decision theory. Most empirical investigations have, however, concentrated on the special case of risk, where objective probabilities are known. While this case may be a good representation of some decision situations, such as roulette wheels or many medical decision problems (for which precise probabilities can be derived from a large number of past observations), the case of uncertainty proper, for which no objectively derivable probabilities are available, is likely to be much more important in praxis. Notwithstanding its importance in the real world, such uncertainty has until recently mostly been studied in opposition to behavior under risk, and even so mostly for a restricted class of 50-50 gambles over gains (see Trautmann and van de Kuilen (2015), for an overview). The phenomenon by which people tend to prefer outcome generating processes characterized by known probabilities (risk) over outcome generating processes with vague or unknown probabilities (uncertainty) which are normatively equivalent under subjective expected utility theory (Savage 1954) is known as ambiguity aversion (Ellsberg 1961). In addition to the challenges it creates for subjective expected utility, ambiguity aversion has been deemed to be an important determinant of a variety of real-world decisions, including the home bias (Kilka and Weber 2000), reluctance to vaccinate (Ritov and Baron 1990), and the preference for established brands over new ones (Muthukrishnan et al. 2009), to name but a few. Only recently has a more systematic way of measuring ambiguity attitudes been developed, which allows for measurements over the whole probability spectrum and for less artificial types of uncertainty (Abdellaoui et al. 2011, 2005; Fox and Tversky 1998; Kilka and Weber 2000; Maafi 2011; van de Kuilen and Wakker 2011). Given the recency of these developments, many issues remain to be explored. This paper specifically investigates whether ambiguity attitudes are affected by the magnitude of the monetary outcomes at stake, a problem on which no evidence exists to date. Ambiguity attitudes are thereby estimated for different stake levels over the whole probability space. In addition, we can trace any stake effects we find for ambiguity back to its constituent parts—stake effects for risk and stake effect for uncertainty—and thus determine what underlying patterns drive stake effects for ambiguity. Contrary to most of the literature on stake effects under risk, we use a between-subjects design which avoids potential contrast effects resulting from the direct contraposition of high and low stake decisions (Greenwald 1978; Read 2005). The issue of stake effects on ambiguity and uncertainty attitudes is important to the extent that real-world decisions involving more or less well-known probabilities may concern a diverse spectrum of outcomes, ranging from relatively trivial stakes to extremely high ones. Any differences in ambiguity attitudes dependent on stakes can thus be informative about this type of behavior. Furthermore, possible stake effects under uncertainty provide information on the extent to which results obtained with low stakes in the laboratory can be generalized to decisions with non-trivial stakes in the field, or on how any such generalizations might need to be adjusted for differences in stakes. Since we elicit certainty equivalents (CEs) for a number of prospects with different probabilities, we can analyze how such a stake effect manifests itself over the whole probability space. Furthermore, we present evidence for both gains and losses. This may be interesting for analyzing decisions, e.g., on the stock market, which can be characterized as decisions under true uncertainty, and which have been linked to ambiguity aversion In contrast to uncertainty, the effect of different stake sizes has been studied extensively for the case of risk, especially for gains. In an experiment conducted in rural India, Binswanger (1980) showed that risk aversion increases in stake levels. Kachelmeier and Shehata (1992) ran high stakes experiments in China, finding that risk seeking for small probabilities was reduced relative to smaller stakes, a result that was replicated by Lefebvre et al. (2010) in France using a between subjects design. Holt and Laury (2002) supplied further evidence on how subjects become more risk averse as stakes increase, and Fehr-Duda et al. (2010) showed with Chinese subjects how, in a prospect theory framework, such stake effects will affect the probability weighting function as well as the utility function, thus violating the separability precept by which attitudes towards money are supposed to be reflected purely in utility (although this may, to some extent, depend on the functional form of utility—see Scholten and Read (2014), and Bouchouicha and Vieider (2016)). Stake effects are less well understood for risky losses. Hogarth and Einhorn (1990) found no effect of such stake variations on losses. Etchart-Vincent (2004) tested different hypothetical stake levels against each other, and found only slight evidence for increased risk aversion under high stakes. Bosch-Domènech and Silvestre (2006), on the other hand, found large increases in risk aversion for both small and large probabilities of losses as stakes increased. Fehr-Duda et al. (2010) found inconclusive effects in the loss domain. Moreover, there exist several papers investigating the issue of whether hypothetical choice differs from choices for real money under losses (Etchart-Vincent and L’Haridon 2011; Laury et al. 2009; Schoemaker 1990; Vieider 2011), which is, however, quite a different issue. We provide some additional evidence on the effect of stake sizes for decisions under risk in the loss domain. We conducted an experiment in Ethiopia where we compare a baseline condition, providing already relatively high stakes, to a high stake condition in which all monetary amounts are doubled in a clean between subjects design. For gains, high stakes cause increases in ambiguity seeking for small probabilities and increases in ambiguity aversion for large probabilities relative to the baseline. This means that under high stakes, we observe more extreme jumps at the endpoints of the probability scale, resulting in reduced sensitivity to probabilistic change in the intermediate ranges. We next trace this effect back to stake effects for risk and uncertainty. For known probabilities, we replicate the well-known finding of risk aversion increasing in stake size—an effect that is uniform over the probability space. For unknown probabilities, we find uncertainty aversion to increase in stake sizes just as for risk. This effect, however, happens entirely through increased uncertainty aversion for large probabilities with uncertainty attitudes for small probabilities remaining unchanged, thus resulting in reduced probabilistic sensitivity for intermediate probabilities. The effect found under ambiguity is thus driven mainly by stake effects in decisions under uncertainty. This pattern is mirrored for losses. In the baseline condition, we find a pattern of ambiguity aversion for low probability levels and ambiguity seeking for high probability levels. Under high stakes, we again find an accentuation of these patterns at the endpoints of the probability scales, with subjects being more ambiguity averse for small probabilities and more ambiguity seeking for large probabilities. Once again, this pattern is driven by more extreme uncertainty attitudes at the endpoints of the probability scale, while we find no effect of stakes on risk attitudes.",9
83.0,1.0,Theory and Decision,30 January 2017,https://link.springer.com/article/10.1007/s11238-017-9590-3,Does inducing choice procedures make individuals better off? An experimental study,June 2017,Luigi Mittone,Mauro Papi,,Male,Male,Unknown,Male,"Over the past years innovative online choice platforms have been developed to facilitate the consumer’s purchasing decisions. For example, a consumer interested in purchasing a high-performance sports car can visit the Ferrari’s web-site and use the car configurator, which allows the consumer to design his ideal car by shaping its characteristics, such as colour, seat type and, wheel cap.Footnote 1 Another example is given by web-sites that through dedicated web-search engines allow the consumer to shortlist the set of available products by specifying the characteristics that the desired product should possess. Rightmove.co.uk, for instance, is a British website specialized in flat renting and gives the consumer the opportunity to shortlist on the basis of attributes, such as location, number of bedrooms, and rental price.Footnote 2
 Such choice platforms induce the consumer to follow certain choice procedures. In the Ferrari’s car configurator case, the consumer is asked to construct his most preferred product by combining a set of available attributes. In the Rightmove.co.uk’s case, on the other hand, the consumer has to shortlist the set of available products by specifying what properties his most desirable product should satisfy. In general the nature of the choice procedure utilized by a decision-maker might have an effect on the outcome of the decision and, as a result, on the welfare of the decision-maker himself. So far the psychological and economic literature on multi-attribute individual decision-making has focused its attention on examining what choice procedure better describes the subjects’ behaviour.Footnote 3 In contrast, motivated by the continuing expansion of online choice platforms, we investigate whether inducing subjects to use holistic vs. characteristic-based search (CBS) procedures makes them better off. Holistic procedures are procedures according to which the decision-maker examines the attributes within alternatives. Utility maximisation is an example of a holistic procedure, as a rational decision-maker first examines the attributes within an alternative (e.g. the prizes and the corresponding probabilities of a lottery), ‘attaches’ a utility value to it (e.g. expected utility), and then examines the next alternative. Another example of a holistic procedure is the satisficing heuristic (Simon 1955). CBS procedures, on the other hand, are procedures according to which the decision-maker examines the attributes across alternatives (Payne et al. 1993). The lexicographic and elimination-by-aspects (Tversky 1972) procedures are CBS examples. A decision-maker following a CBS procedure focuses his attention on one dimension (or various dimensions) only and discards all alternatives that are dominated on that dimension or do not meet a certain pre-determined threshold. Unlike traditional supermarkets, online choice platforms, such as the ones described above, typically induce procedures that encompass CBS elements.Footnote 4
 Our research question is relevant for both psychologists and economists for various reasons. First, understanding whether inducing individuals to use different cognitive-comparative processes of the alternative’s characteristics has an effect on the quality of their decision is a key problem. Second, it is interesting to examine whether or not subjects perform better by using a class of heuristics (i.e., CBS) that are generally inconsistent with the application of the utility maximisation procedure. Third, given the growing interest within economic theory about rational and boundedly rational choice procedures, our experiment ‘searches for facts’ within the domain of multi-attribute decision problems.Footnote 5
 In this paper we propose a between-subject design in which subjects are asked to perform the same choice task by inducing them to use different procedures. Our experiment consists of an innovative visual choice task, whereby subjects are shown a target alternative—an abstract figure—and asked to select or construct the alternative that most closely looks like the target. The baseline treatment induces a holistic procedure by asking subjects to select the figure that most closely looks like the target among those available. In contrast, two other treatments, which we call build and destroy, induce—in different ways—subjects to use characteristic-based search (CBS) procedures by asking them to construct the figure the most closely looks like the target figure by combining the blocs available. Across all treatments we vary both the time pressure level—interpretable as the search cost—and the complexity of the choice task. It is worth emphasizing that in this experiment we fully control subjects’ preferences, which we induce via monetary incentives. This is not because we rule out the possibility that inducing individuals to use certain choice procedures does not have an effect on their preferences. On the contrary, we believe that the preference-formation issue is relevant within the broader context of our research question. However, we think that our methodology is appropriate for at least two reasons. First, given that our aim is to understand whether inducing choice procedures has an effect on individuals’ welfare, we need to know their preferences to be able to make welfare judgments. Inducing preferences via monetary incentives is a standard technique in experimental economics to achieve this goal (Smith 1976; Camerer and Hogarth 1999). Second, since our study is—to the best of our knowledge—the first to investigate the effects of inducing individuals to use certain choice procedures, it is natural to start with an experimental design over which we have as much control as possible to be able to isolate the different effects.Footnote 6 The natural following step of this research, which we are already working on, is to extend the current experimental design with the objective of examining whether (and possibly how) preferences are affected by the inducement to use certain choice procedures. The results of our experiment are threefold. First, inducing subjects to certain choice procedures has an effect on their welfare. Specifically, subjects’ performance is distinctly better in the build and destroy treatments than in the baseline treatment, indicating that inducing subjects to use CBS heuristics (as opposed to holistic ones) increases their welfare. To the best of our knowledge, our paper is the first one to show that CBS procedures may be welfare increasing. If we interpret subjects’ behaviour as being the implementation of some payoff-maximizing objective function subject to cognitive constraints, then our results suggest that inducing subjects to use CBS procedures relaxes their cognitive constraints and, as a result, improves their performance. We also detect a slight difference in performance between build and destroy in favour of the destroy treatment. We attribute such difference to the fact that, by the nature of the treatments themselves, it takes more time to construct an alternative in the build than in the destroy treatment. As a result, especially at relatively high time pressure levels, subjects tend to do better in the destroy treatment. Second, by looking at disaggregated data by complexity, we find that the divergence in performance between baseline and build/destroy is maximised at intermediate complexity levels. That is, at simple problems the assignment to the treatment does not affect subjects’ performance. As complexity increases the performance in the baseline treatment worsens at a higher rate than in the build/destroy treatment leading to the maximum degree of divergence at intermediate complexity levels. At relatively high complexity levels the performance across treatments tends to converge. This second finding suggests that the ‘ecological rationality’ of CBS heuristics holds within a certain range of complexity. At very simple and very complex problems subjects’ performance is not affected by the nature of the heuristic induced. On the contrary, at moderately complex problems inducing CBS procedures, as opposed to holistic ones, pays off. Third, we compare random choice (which we construct by running simulations) and with subjects’ choice and find that subjects behaviour is distinctly different from random in both the build and destroy treatment. On the contrary, in the baseline treatment we find that subjects’ choice is different from random choice at relatively simple problems only. At relatively high complexity levels, we cannot rule random choice out. This result is consistent with the other findings. The remainder of the paper is structured as follows: Section 2 presents the theoretical framework behind the experiment; Sect. 3 discusses the experimental design; Sect. 4 illustrates the results of the simulations; Sect. 5 presents the results of the experiment; Sect. 6 discusses the related literature and concludes. The supplementary material contains additional figures (including examples of screenshots), a detailed comparison of build and destroy treatments, and the instructions.",
83.0,1.0,Theory and Decision,03 February 2017,https://link.springer.com/article/10.1007/s11238-017-9586-z,Axiomatization and implementation of a class of solidarity values for TU-games,June 2017,Sylvain Béal,Eric Rémila,Philippe Solal,Male,Male,Male,Male,"One of the main issues in economic allocation problems is the trade-off between marginalism and egalitarianism, which can be tackled by cooperative games with transferable utility. A cooperative game with transferable utility (TU-game) on a given player set specifies, for each coalition of players, a worth measuring the best possible result for the coalition should its members cooperate without the help of any other player. A value assigns a payoff vector to each such game, which can be interpreted as the payoffs given by a regulator to the players for participating in the game. The Shapley value (Shapley 1953) and the Egalitarian Division value are two well-known values, but each only incorporates one of the two above-mentioned principles. Assuming that the grand coalition has formed by a succession of one-by-one arrivals, the Shapley value of a player is equal to his expected contribution to the coalition of players he joins upon arriving. Therefore, the Shapley value is exclusively based on a marginalistic principle. As a consequence, unproductive players get zero payoff, which means that the Shapley value rules out every kind of solidarity between the players. By contrast, the Egalitarian Division value, which divides the worth achieved by the grand coalition equally among all players, does not depend on the players’ contributions at all, and as such can be seen as too solidaristic. Although these two values seem to be rather opposite, they both satisfy basic axioms such as Efficiency, Anonymity and Linearity. The class of values satisfying these three axioms has been studied and characterized by Ruiz et al. (1998) and Radzik and Driessen (2013), among others. In the latter article, it is proved that any efficient, anonymous and linear value can be formulated as the Shapley value of an appropriately modified game. A growing literature in which less extreme visions of the solidarity principle are invoked to design values has emerged within the class of efficient, anonymous and linear values. Such values incorporate some modes of solidarity: the most productive players should obtain a better treatment, but a solidarity principle should ensure a reduction of the payoffs inequalities with the less productive players. The Equal Surplus Division value (Driessen and Funaki 1991) first assigns to each player his stand-alone worth and then splits equally what remains of the worth of the grand coalition. The Solidarity value (Sprumont 1990; Nowak and Radzik 1994) is similar to the Shapley value, except that the contribution of a player to a coalition is replaced by the average contribution over the coalition’s members. The Least Square Prenucleolus (Ruiz et al. 1996) first assigns to each player his Banzhaf value (Banzhaf 1965) and then splits equally what remains of the worth of the grand coalition. The generalized Consensus values (Ju et al. 2007) are the class of all convex combinations between the Shapley value and the Equal Surplus Division value. The Egalitarian Shapley values (Joosten 1996; Brink et al. 2013) are the class of all convex combinations between the Shapley value and the Egalitarian Division value. The discounted Shapley values (Joosten 1996; Brink and Funaki 2015) also contain the Shapley value and the Egalitarian Division value as extreme points. The values belonging to the class introduced in Casajus and Huettner (2014a) are distinguished by the type of player whose removal from a game does not affect the remaining players’ payoffs. This class contains the Shapley value and the Egalitarian Division value as extreme points, and the Solidarity value is its center. The Procedural values (Malawski 2013) is a class of values similar in spirit to the Shapley value except that the contribution of the arriving player can be arbitrarily shared among him and the players arrived before him. The Solidarity value, the Egalitarian Division value, the Shapley value and the Egalitarian Shapley values are instances of the Procedural values. These relationships have allowed for comparable axiomatic characterizations of these values (see Brink 2007; Kamijo and Kongo 2012; Chameni Nembua 2012; Casajus and Huettner 2013, 2014a, b in addition to the aforementioned articles). The Equal Surplus Division value, the generalized Consensus values and the Least Square Prenucleolus are not Procedural values; the class studied in Casajus and Huettner (2014a) is not related to the class of Procedural values by set inclusion. In this article, we introduce a new class of values which combines marginalistic and egalitarian principles and which is included in the class of efficient, anonymous and linear values. As for the Solidarity value and the Procedural values, we keep Shapley’s idea that the one-by-one formation of the grand coalition is modeled by permutations of the players. Nonetheless, instead of rewarding every player with his (individual) contribution to the coalition he joins upon entering, we also rely on the notion of collective contribution to the grand coalition so as to reflect some aspects of solidarity. More specifically, for a coalition S, the collective contribution of S to the grand coalition N is measured by the difference between the worth of N and the worth of the coalition of players in N but not in S. In a sense, if S was considered as a single entity, then the collective contribution of S to N would reduce to the individual contribution of player S to N. The computation of the Shapley value only involves individual contributions. On the contrary, the Egalitarian Division value only rests on the collective contribution of the grand coalition to itself, which then is split evenly. In the building blocks of our class of values, the collective contribution to the grand coalition replaces the individual contributions as soon as the currently formed coalition has reached some size p. Thus, there are two distinct steps. Before attaining the critical size p, each entering player gets his individual contribution to the coalition he joins. When the critical size p is reached, the remaining players keep on entering one by one, but instead of rewarding each of them when entering, they cumulate their contributions until the grand coalition is formed. Then, the collective contribution of the coalition of remaining players to the grand coalition is evenly distributed among them. This procedure can be interpreted as the creation of a mutual fund by these remaining players, which is used for promoting equality among them, creating de facto some solidarity. Our construction procedure can be justified by two phenomena. On the one hand, the fact that the mutual fund is established when some size p is attained seems consistent with both empirical and theoretical findings as emphasized by García and Vanden (2009, pp. 1980). On the other hand, the appeal to a regulator to ensure some solidarity among the players is sometimes necessary. For instance, in the context of health insurances, Stone (1993) points out that a mutual insurance can hardly be implemented without the coercive authority of a state. The author underlines that the competitive insurance industry in the US often leads to fragmentation of the society into ever-smaller, more homogeneous groups, which in turn implies the destruction of mutual aids. We call Sol\(^p\) the value defined by averaging the payoff vector described in the previous paragraph over all permutations of the players. The class of solidarity values that we study, denoted by Sol
\(_N\), is the convex combination of all Sol\(^p\) values. The class Sol
\(_N\) and its elements are investigated through the following three types of results. First, we relate our class to the previously mentioned values and classes of values. Proposition 6 shows that Sol
\(_N\) is (strictly) included in the class of Procedural values. Nevertheless, although the construction of Sol
\(_N\) and the Procedural values are different, Proposition 6 also provides an alternative formulation of each extreme point Sol\(^p\) of Sol
\(_N\) in terms of Procedural values. More specifically, Sol\(^p\) coincides with the Procedural value in which the contribution of the entering player is assigned to himself if the size of the current coalition is not larger than \(p+1\), and to the player entered in position \(p+1\) otherwise. Since Sol\(^0\) and Sol\(^{n-1}\) coincide with the Egalitarian Division value and the Shapley value, respectively, Sol
\(_N\) also contains the class of all Egalitarian Shapley values. Furthermore, Sol
\(_N\) includes the Solidarity value. This result follows from Proposition 8, which characterizes Sol
\(_N\) by means of the modified game studied in Radzik and Driessen (2013). Based on this result, we derive closed form expression of each Sol\(^p\). Second, we offer a strategic non-cooperative implementation of any value in Sol
\(_N\), which highlights an alternative interpretation of these values. Our mechanism belongs to the so-called class of bidding mechanisms initiated by Demange (1984) and Moulin (1984) and developed by Moulin and Jackson (1992) in economic environments, and then adapted by Pérez-Castrillo and Wettstein (2001) and Ju and Wettstein (2009) to implement solution concepts for cooperative TU-games. The bargaining procedure has three stages. The first two stages are bidding stages in which the players bid to select an ordering of the players and a position for the future proposer. The two selected parameters then induce a sequential bargaining stage in which the player at the chosen position in the ordering makes an offer. This offer is sequentially accepted or rejected by the other players. If the offer is unanimously accepted, the game ends and the proposer pays the player according to his/her offer, and receives what remains of the worth of the grand coalition. If any player rejects the offer, then the proposer and the players following him in the ordering leaves the game and obtain a null payoff. Furthermore, the remaining players engage again in a bargaining stage in which the player in the position preceding the originally chosen position is the new proposer, and so on. The chosen position, therefore, discriminates the players according to the two main bargaining procedures: the players preceding the proposer are engaged in an alternating-offer procedure, while the players succeeding the proposer play in a take-it-or-leave-it procedure. Proposition 10 shows that the payoffs of the subgame perfect Nash equilibria of this mechanism coincide with the values in Sol
\(_N\). Proposition 10 relies on a result that we prove for a more general class of abstract mechanisms (Proposition 9). Our mechanism differs substantially from those in the above-mentioned articles. Contrary to Pérez-Castrillo and Wettstein (2001), where the proposer is always located in last position of the ordering, we include a bidding stage to determine the position of the proposer. So, in a sense, the procedure in Pérez-Castrillo and Wettstein (2001) can be considered as a particular case of our mechanism, even though the bidding procedure is not exactly the same. Moreover, instead of the second chance offered to the proposer in case of a rejection as in Ju and Wettstein (2009), the flexibility of our mechanism is materialized by the simultaneous presence of the alternating-offer and take-it-or-leave-it procedures in various proportions. The class of values implemented by Ju and Wettstein (2009) and our mechanism have a non-empty intersection since they both include the Shapley value, but they are not related with respect to set inclusion. Ju and Wettstein (2009) implement the Equal Surplus Division and the generalized Consensus value, which do not belong to Sol
\(_N\), while we implement the Egalitarian Division value and the Solidarity value. We also implement the Egalitarian Shapley values by a different mechanism from the one proposed by Brink et al. (2013), which differs from the classical mechanism in Pérez-Castrillo and Wettstein (2001) only in an additional possibility of breakdown of the negotiations after a rejection. Similarly, Brink and Funaki (2015) only modify the mechanism in Pérez-Castrillo and Wettstein (2001) by introducing a discount parameter to implement the discounted Shapley values, which are not in Sol
\(_N\) except for the Shapley value and Egalitarian Division value. Third, we provide an axiomatic characterization of Sol
\(_N\) and of each of its extreme points Sol\(^p\). Proposition 11 characterizes each Sol\(^p\) value by the standard axioms of Efficiency, Equal treatment of equals and Additivity together with the new p-null player axiom. The latter axiom falls in line with other parametrized alterations of the null player axiom (see Ju et al. 2007; Kamijo and Kongo 2012; Chameni Nembua 2012; Casajus and Huettner 2014a; Béal et al. 2015; Radzik and Driessen 2016).Footnote 1 It assigns a null payoff to a player who has null contribution to coalitions of size less than p and such that every coalition of size p without this player has the same worth as the grand coalition. To characterize the class Sol
\(_N\), we invoke in Proposition 12 the four axioms Efficiency, Additivity, Desirability and Monotonicity used by Malawski (2013) to characterize the larger class of Procedural values, and add the new axiom of Null player in a null environment for positive games. The latter axiom imposes that a null player does not obtain a positive payoff if both the worth of all coalitions are non-negative and if the grand coalition achieves a zero worth. This axiom aims at emphasizing the limits of the solidarity when the resources available to the society are not sufficient to redistribute monetary payoffs to unproductive players. Our characterization is also comparable to Theorem 2 in Casajus and Huettner (2013), which characterizes the Egalitarian Shapley values by Efficiency, Additivity, Desirability and Null player in a productive environment. The latter axiom points out situations in which solidarity is possible by requiring that null players obtain non-negative payoff if the grand coalition has a non-negative worth. One can move from our Proposition 12 to Theorem 2 in Casajus and Huettner (2013) by dropping Monotonicity and by replacing Null player in a null environment for positive games by Null player in a productive environment. Finally, Proposition 12 implies that Sol
\(_N\) neither includes nor is included in the class of values studied in Casajus and Huettner (2014a). The rest of the article is organized as follows. Section 2 gives the basic definitions and contextualizes our study by stating the closest results. The class Sol
\(_N\) and its extreme points are constructed in section 3. Their properties are studied in Sect. 4. Section 5 presents the strategic implementation. Section 6 contains the axiomatic characterizations. Section 7 concludes. All proofs are relegated to the Appendix.",16
83.0,1.0,Theory and Decision,07 February 2017,https://link.springer.com/article/10.1007/s11238-017-9587-y,A note on budget constraints and outside options in common agency,June 2017,Chiu Yu Ko,,,,Unknown,Unknown,Mix,,
83.0,1.0,Theory and Decision,07 February 2017,https://link.springer.com/article/10.1007/s11238-017-9589-9,Coordination when there are restricted and unrestricted options,June 2017,Shaun P. Hargreaves Heap,David Rojo Arjona,Robert Sugden,Male,Male,Male,Male,"Subjects manage to coordinate their decisions with a surprisingly high frequency in laboratory experiments on pure coordination games (e.g. see Schelling 1960; Mehta et al. 1994; Bardsley et al. 2010). This is both impressive and important. It is impressive because the games are symmetric in pay-offs. Strategies have different labels and the player’s objective is to match the choice of the other, defined in terms of its label. The pay-off from matching is always the same, whichever label it occurs around, and it is zero otherwise. There is nothing in terms of pay-offs to distinguish the strategy labels and so it seems that subjects are able to exploit some asymmetry in the labels themselves when they manage to match to a surprising degree.Footnote 1 It is important because many real-world interactions appear to have multiple equilibria. The selection of one involves solving a coordination game and it seems we have a surprising capacity to do this in the laboratory, even in the difficult cases of pure coordination. This paper is concerned with this apparent capacity to solve pure coordination games. First, we consider whether the capacity to coordinate in the laboratory depends on restricting the number of strategy options. This is potentially important because laboratory experiments often restrict the strategy options, whereas choices outside the laboratory are more typically unconstrained. As a result, the observed capacity to coordinate in the laboratory could to some degree be an artifact of the laboratory setting and its constraint on choice. Towards this end, our first experiment tests for differences in coordination when otherwise identical coordination problems have either restricted or unrestricted options. Second, we examine with the aid of another experiment how the observed levels of coordination in the first experiment might be explained by appealing to the idea that people follow rules that are based on the labels of the strategies in such games. The difference between restricted and unrestricted options seems likely to be important because the scope for dis-coordination appears to grow as the possible number of objects in the choice set increases. For instance, when there is one option, the only outcome is a coordinated one; whereas, when there are two options, only half of the possible outcomes are coordinated, when there are three options, the number of possible outcomes that are coordinated drops to one third, and so on. Our first Hypothesis follows from this reasoning. H1: Coordination is higher when the choice set is restricted. We test H1 in our first experiment by asking some subjects to play 16 pure coordination games with a restricted number of strategy options and others to play 16 corresponding games with no restrictions. In each coordination game, the options belong to a particular category (i.e. a class of objects). For example, one category is car manufacturers. In the restricted version for this category, we ask subjects to choose one of the following options: {HONDA, MERCEDES, FORD, FERRARI, BMW}. The identified car manufacturers are the strategy labels. The analogous unrestricted version of this coordination game asks subjects to choose a car manufacturer. In both cases, the subjects are told that they will be randomly matched with another subject and ‘if you give the same answer as the other person you will win some MONEY. If not, you will win NOTHING’. In fact, we reject H1. Contrary to expectation, coordination is typically higher when options are unrestricted than when they are restricted. It seems in this respect that laboratory experiments with restricted sets may understate the likely coordination when there are unrestricted options outside the laboratory. The result sets a problem that we take up in our second experiment: why is coordination higher when options are unrestricted? We address this problem from a particular perspective. We follow Schelling (1960) and Sugden (1995) in assuming that individuals use a selection rule based on the labels attached to the strategies in such games. In particular, subjects are randomly matched and we elicit their application of seven rules in a ‘naïve’ and in a ‘strategic’ way. Thus, suppose the rule is ‘favourites’ and is applied to the category of car manufacturers. In the ‘naïve’ elicitation, we ask the subject to choose their favourite: ‘Circle/write down your favourite car manufacturer’. When applied ‘strategically’, we ask the subject to choose the other person’s favourite: ‘Circle/write down the other person’s favourite car manufacturer’. The seven rules are drawn from the psychology literature. We use these elicitations to assess whether any of three conceptually distinct reasons, within this framework, explains why one observes more frequent coordination in unrestricted than restricted sets of the first experiment. One possible reason is that individuals are using the same rule and coordination improves because the individual interpretations of that rule become closer to each other when there are unrestricted options. For example, with an ‘odd one out’ rule it may help such convergence in interpretation when selecting a number from the unrestricted set of all positive integers as compared with a restricted set like {1, 2, 3, 7, 9}. This is because, in the restricted set, both ‘1’ and ‘9’ might stand out as, respectively, the lowest and highest while ‘2’ could do the same as the only even number. However, in the unrestricted set there is no highest number and there are lots of even numbers, so only the lowest number, ‘1’ stands out. This is the basis of our second Hypothesis. We define the average own rule concordance rate for rule i as the average coordination rate that would be achieved across all categories when all subjects use rule i in the manner suggested by the elicitations of rule i in the second experiment. H2: The average own rule concordance rate is higher with unrestricted than with restricted options. Another possible reason is that each individual could be using a constant rule across the categories but the constant rule is not the same for all individuals and greater coordination is achieved because the application of these different rules yields greater concordance on the unrestricted than the restricted options. For example, people who select the object with the most features associated with that category of objects (i.e. the ‘most similar’ rule) could plausibly choose ‘1’ on both the unrestricted and restricted positive integer sets because in both cases every number is divisible by ‘1’ (but not by any other). As a result, if, as suggested above, people using the ‘odd one out’ rule are more likely to select ‘1’ on the unrestricted than the restricted set, then they will achieve more coordination with those using the ‘most similar’ rule on the unrestricted than on the restricted set. This is our third Hypothesis. We define the average cross-rule concordance rate between rule i and j as the average coordination rate that would be achieved across the categories when one individual uses rule i and the other individual uses rule j (in the manner suggested by the rule elicitations in the second experiment). H3 is distinct from H2, but they are not mutually exclusive. H3: The average cross-rule concordance rate is higher with unrestricted rather than restricted options. The final possible explanation builds on the second. Individual subjects use different rules and the frequency of their use of each rule changes between the unrestricted and restricted options sets. For example, the ‘odd one out rule’ might seem obviously applicable in the restricted set of car manufacturers like {HONDA, MERCEDES, FORD, FERRARI, BMW} when subjects come from the US because FORD is the only US manufacturer while all others originate from foreign countries. But, this rule may not be as attractive to use on an unrestricted domain of car manufacturers because being a domestic manufacturer no longer singles out one option and there are no obvious alternative bases for an ‘odd one out rule’ that would work on car labels. H4 concerns whether there is evidence of different frequency of rule use on restricted and unrestricted options. H4: Rules are used with different frequencies by individuals when options are unrestricted as compared with when they are restricted. We find no evidence for H2. Indeed for each of our rules when used by all, the concordance rate is higher with the restricted options than the unrestricted ones. Nor do we find evidence in favour of H3: the cross-rule concordance between different rules in our experiment is very similar and almost always slightly higher when options are restricted rather than unrestricted. Finally in relation to H4, we find evidence that subjects use rules with different frequencies on the restricted as compared with the unrestricted sets. Using the different estimated frequencies of use, we are able to generate the predicted coordination rates when subjects are given restricted and unrestricted options. These predictions are below, but close to what was observed in the first experiment and the predicted coordination rate is higher, albeit only marginally so, when the options are unrestricted. In short, there is evidence that different rules are triggered when options are restricted as compared with when they are unrestricted and this contributes to the explanation of why we observe higher coordination when options are unrestricted. The first experiment on restricted and unrestricted coordination games and its results are described in Sects. 2 and 3. Sections 4 and 5 do the same for the second experiment on the possible explanation of the differences in coordination observed in the first experiment. Section 6 discusses these results and concludes.",7
83.0,1.0,Theory and Decision,02 February 2017,https://link.springer.com/article/10.1007/s11238-017-9588-x,Meaningful learning in weighted voting games: an experiment,June 2017,Eric Guerci,Nobuyuki Hanaki,Naoki Watanabe,Male,Male,Male,Male,"For many years, experimental studies on learning in games have focused on describing the manner in which subjects learn to play strategic games.Footnote 1 Various models of learning have been proposed to replicate and to understand the forces behind the observed dynamics of subjects’ behavior, such as reinforcement learning (e.g., Erev and Roth 1998), belief-based learning (e.g., Cheung and Friedman 1997), and experience weighted attraction learning or EWA (Camerer and Ho 1999). However, Arifovic et al. (2006) show that these models of learning fail to replicate human behavior in some of the games they have considered, especially, in a repeated Battle of the Sexes game. Erev et al. (2010) also report that standard learning models based on the evolution of attraction do not perform well in predicting how people behave in market entry games. Researchers continue to propose new models that capture observed human behavior. For example, Marchiori and Warglien (2008) incorporate “regret” in their neural network-based learning model, and show that it better replicates observed human behavior than either the EWA or neural network-based learning models without regret; Hanaki et al. (2005) and Ioannou and Romero (2014) extend the reinforcement and the EWA learning models, respectively, to allow players to learn which repeated-game strategies to use in repeated games; Arifovic and Ledyard (2012) report that their “individual evolutionary learning” model captures most of the stylized results in Public Goods game experiments; Spiliopoulos (2012, 2013) embed abilities to recognize an opponent’s behavioral patterns in a belief-learning model to better capture the elicited subjective beliefs about the opponent’s strategy in games with a unique mixed strategy Nash equilibrium. In contrast to a vast existing literature on learning how to play a game, investigation on learning across games have only recently gained attention by scholars (see, among others, Cooper and Kagel 2003, 2008; Haruvy and Stahl 2009; Rick and Weber 2010; Dufwenberg et al. 2010). The main research question is whether subjects learn the underlying properties of the games they play and whether they generalize what they have learned in one situation and apply it to similar but different situations. In terms of the depth of learning, this higher order concept of learning should be distinguished from learning to make choices that generate better outcomes in a given situation; Rick and Weber (2010) call it “meaningful learning” while Dufwenberg et al. (2010) call it “epiphany”.Footnote 2
 These studies mentioned above do not account for no-feedback learning with the exception of Weber (2003); Rick and Weber (2010), who study p-Beauty Contest games (Ho et al. 1998). These studies show that not only learning takes place without any feedback, but also Rick and Weber (2010) find that withholding feedback promotes meaningful learning in the sense that subjects learn to perform iterated dominance.Footnote 3 On the other hand, Neugebauer et al. (2009) report that subjects do not learn to play the dominant strategy in a Voluntary Provision of Public Goods game if they do not receive feedback information. One of our aims in this paper is to study the effect that withholding immediate payoff-related feedback has on this deeper learning in weighted voting games.Footnote 4
 Weighted voting, which gives a different number of votes to different voters, is a popular collective decision-making system in many institutions such as stockholder voting in corporations or voting blocs in multi-party legislatures. However, the relationship between nominal voting weight and real voting power is often complex. In a study of the Council of Ministers in the European Economic Community, Felsenthal and Machover (Felsenthal and Machover 1998, pp. 164–165) suggest that it must have been difficult even for the policy makers and officials who designed and re-designed the system to see through the underlying relationship between the nominal voting weights and the actual voting power. To better understand the complex relationship inherent to weighted voting, researchers have begun to conduct experimental studies to complement empirical analyses because many features that are unobservable in actual practices can be controlled in experiments. Montero et al. (2008), Aleskerov et al. (2009), Esposito et al. (2012), Guerci et al. (2014), and Watanabe (2014) conducted experiments involving subjects deciding the allocation of a fixed amount of resources among themselves via weighted voting. These experiments are all conducted in a cooperative game environment where extensive forms are not specified for negotiations among the subjects.Footnote 5 These studies find that an experimental measure of “a posteriori” voting power, which is defined as the average payoff a voter obtains during the experiment, differs dramatically from theoretical measures of “a priori” voting power, as can be seen in the studies by Banzhaf (1965), Shapley and Shubik (1954), and Deegan and Packel (1978). These remarkable discrepancies between theoretical predictions and experimental observations call for a better behavioral theory of weighted voting games including how and what subjects learn while playing these games. Of particular interest is the question of how subjects learn about the underlying relationship between their nominal voting weights and their actual expected payoffs. This question is of particular interest because of the inference made by Felsenthal and Machover (1998) noted above; it seems to be extremely difficult to inductively learn about this underlying relationship. In this paper, we take a step toward answering this question by focusing on subjects learning about the relationship between their nominal voting weights and their expected payoffs. In particular, by employing binary committee choice problems, we investigate how varying the feedback information about resulting payoffs affects: (1) subjects learn the underlying relationship between their nominal voting weights and their actual expected payoffs; (2) subjects transfer what they have learned from their limited experience in one committee choice problem to another committee choice problem. Each session of the experiment is comprised of a series of periods where each subject must choose to join one of two committees (weighted voting games). Both committees consist of four voters: the subject and three fictitious voters. The payoffs that a subject receives after having chosen a committee are based on a theory of voting power, DPI, proposed by Deegan and Packel (1978). Subjects are not informed that payoffs are generated based on this precise theory, but instead are told that a theory of decision making in committees determines their payoffs. After experiencing a committee choice problem, subjects then face another one. Although we do not apply other well-known theories of voting power, e.g., the BzI proposed by Banzhaf (1965) and the SSI proposed by Shapley and Shubik (1966) to the payoff determination, we examine binary committee choice problems in which the better options for the subjects are the same regardless of whether we employ DPI, BzI, or SSI, so that subjects can unequivocally recognize, if any, the voting powers of committee members. Why do we employ binary choice problems instead of letting subjects play weighted voting games? The main reason is that, as Esposito et al. (2012) show, it is too difficult to detect what inexperienced subjects learn while actually playing a weighted voting game with other inexperienced subjects. Because the resulting outcomes of negotiations vary greatly from one round of game play to another (see, e.g., Montero et al. 2008; Guerci et al. 2014), we consider such an experiment to be too complex for inexperienced subjects to make deep inferences about the underlying relationships between nominal voting weights and expected payoffs. Therefore, we drastically simplify the experimental design by removing the issue of simultaneous learning among subjects. Because the method for generating payoffs in our binary committee choice problems remains the same throughout the experiment, our subjects have a better chance to learn about the hidden payoff mechanism, i.e., the underlying relationship between their nominal voting weights and their actual expected payoffs. We believe this is the most favorable environment for subjects to learn. Because of our focus on the effect of varying payoff-related feedback on learning, we use three treatments with three different levels of feedback to subjects in the experiment: no-feedback, partial-feedback, and full-feedback. In the no-feedback treatment, subjects are given no information about the payoffs they obtain after each choice. The only information available to them in making their choices is the description of how the votes are divided among the four players in the two committees. In the partial-feedback treatment, after each choice subjects are given information about their own payoffs but not the payoffs of the other fictitious members of the committee they chose. In the full-feedback treatment, after each choice, subjects are given information about their own payoffs and the payoffs for all the fictitious members of the committee they chose. Note that in both the partial- and the full-feedback treatments, subjects do not receive any information about payoffs for the committee they did not choose. This matches the feedback information given in a standard Two-Armed Bandit problem.Footnote 6
 Our main findings are as follows: (1) the percentage of subjects who chose the weighted voting game that generated the higher expected payoff increased even without any feedback information on what payoffs they received; and (2) there was statistically significant evidence of meaningful learning (or transfer of learning) only for the treatment with no payoff-related feedback. Standard models of learning based on the evolution of attraction (e.g., reinforcement learning and EWA learning models) cannot be used to make any predictions about how subjects learn to choose the option with a higher expected payoff in a no-feedback treatment. In fact, we are not aware of any learning models that allow us to study learning with no explicit feedback information. Nevertheless, we observed that the percentage of subjects who chose the option with the higher expected payoff increased even with no feedback information. Further, we found statistically significant evidence of meaningful learning between the two binary committee choice problems in the no-feedback treatment, but not in the partial-feedback or full-feedback treatments. This finding calls for re-thinking existing models of learning to incorporate some type of introspection where it would be valuable. Of course, our finding about learning under no-feedback condition would not be replicated in situations where different introspections lead to different “correct” choices, or where introspection is not useful at all in reaching the “correct” choice, such as in the standard two-armed bandit problem. The rest of the paper is organized as follows. Section 2 describes the experimental design. We explain some more features on the power indices there in detail. Section 3 presents a detailed discussion of the experimental results. Section 4 concludes the paper.",3
83.0,2.0,Theory and Decision,08 March 2017,https://link.springer.com/article/10.1007/s11238-017-9592-1,"Optimize, satisfice, or choose without deliberation? A simple minimax-regret assessment",August 2017,Charles F. Manski,,,Male,Unknown,Unknown,Male,"
Simon (1955) introduced the concept of satisficing, but he did not provide a precise definition. Neither here nor later did he analyze the circumstances in which a person might reasonably choose to satisfice rather than use another decision strategy. Other researchers have subsequently interpreted satisficing in various ways and have performed analyses thereof. However, a consensus perspective still has not emerged. This paper interprets satisficing as a class of decision strategies that a person might use when deliberation is costly. Costly deliberation lies at the heart of Simon’s motivation of satisficing, but he did not formalize the idea. Neither, as far as I am aware, have other researchers analyzed how the magnitude of deliberation costs may affect choice of a decision strategy. I do so here and report simple specific findings that hold if the person uses the minimax-regret criterion. I consider a person who must choose an action from a finite choice set. The person wants to maximize a specified welfare function, but he has limited capacity to compute the welfare of actions. I study decision making as a problem of planning with reducible ambiguity. The person faces ambiguity because he does not initially have sufficient knowledge to determine an optimal action. Ambiguity is reducible because the person can learn about his welfare function through costly deliberation. I suppose that the available deliberation strategies include optimization, satisficing, and the null option of no deliberation. Optimization has positive deliberation cost and reveals an optimal action. The null option has no cost and yields no information about welfare. Satisficing comprises a class of strategies with positive costs that reveal whether actions yield welfare values that are at least as large as a specified sequence of aspiration levels. Members of the class vary in the maximum number of rounds of deliberation performed and the aspiration levels used each round. In this setting, I study ex ante minimax-regret (MMR) decision making with commitment. The findings are sensible and simple, showing how the MMR decision depends on deliberation costs. When satisficing, the aspiration level used in each round is midway between the lower and upper bounds on optimal welfare learned from previous rounds. One continues to deliberate as long as the maximum benefit of refined knowledge of the welfare function exceeds the deliberation cost. This sequential decision rule is intuitive and easy to use. Hence, I think it reasonable to conjecture that persons may actually use it in practice. Given that Simon’s ideas were appealing but vague, it should not be surprising that alternative formal interpretations of satisficing have been proposed and studied over the years. I cite some of them in Sect. 2. As far as I am aware, the literature has not generated research similar to the simple minimax-regret analysis developed here. I do not assert that my interpretation of satisficing supersedes that of other researchers or is closer to what Simon really intended. I do, nevertheless, think that the present paper makes a distinctive new contribution to our understanding of an intriguing concept. The analysis in this paper is abstract, so the reader may benefit from mention of potential applications. Simon did not pose a biological or psychological argument explaining why humans might find it easier to distinguish satisfactory and unsatisfactory outcomes than to compute precise welfare values for alternative actions. Not being a cognitive scientist, I will not advance such an argument here. I feel more comfortable contemplating applications in which the person may incur deliberation costs to learn about his external environment. For example, consider the medical problem of choice among alternative diagnostic or screening tests that vary in their effectiveness in detecting the presence of a pathogen. Let welfare increase with detection ability, a test being better if it can detect a lower concentration of the pathogen. It may be costly to determine the optimal test, which detects the smallest concentration of the pathogen. It may be less costly to perform experiments in which the concentration is set at some level and one evaluates whether each feasible test can detect the presence of the pathogen at this concentration. A satisficing strategy would be to perform a sequence of such experiments with concentration set at different levels, followed by choice of a test. Section 2 describes Simon’s ideas and some of their interpretation by others. Section 3 sets forth the problem of planning with reducible ambiguity studied in this paper. Section 4 presents the analysis of MMR decision making. Section 5 adds remarks about empirical considerations.",19
83.0,2.0,Theory and Decision,29 March 2017,https://link.springer.com/article/10.1007/s11238-017-9597-9,Conditional expected utility,August 2017,Massimiliano Amarante,,,Male,Unknown,Unknown,Male,"In an interesting paper, Fishburn (1973) studied decision makers who are characterized by the property that their conditional preferences obey the axioms of Subjective Expected Utility (SEU) theory. That is, once they are informed that the true state lies in a certain subset of the state space, these decision makers evaluate acts by means of an expected utility criterion. In his study, Fishburn is motivated by the observation that Savage’s derivation of SEU theory rests on the assumption that a sufficiently “rich” set of acts be available to the decision maker, but the assumption seems hardly met in many actual situations. Thus Fishburn, and Luce and Krantz (1971) before him, proposed the model of conditional EU preferences as a way to remedy this situation. Once the assumption of conditional preferences is made, the problem becomes that of understanding how these decision makers would evaluate their options ex-ante, that is before they receive their information. Decision makers that display conditional EU preferences with respect to a certain class of events need not obey the SEU axioms ex-ante. An easy example that shows this point is as follows. Let S denote the state space, and assume that S is the interval [0, 1] endowed with the usual Borel \(\sigma \)-algebra. Consider a Maxmin Expected Utility decision maker who is described by a set of priors \({\mathcal {C}}=co\left\{ \mu ,\lambda \right\} \), the convex hull of two probabilities \(\mu \) and \(\lambda \). Assume further that \(\mu \) has a density with respect to \(\lambda \) given by and that \(\lambda \) is the Lebesgue measure on [0, 1] (i.e., \(\lambda \) has density \(g\equiv 1\) on [0, 1]). It is clear that this decision maker satisfies conditional expected utility with respect to the family of events \({\mathcal {E}}=\left\{ [0,1/3],[1/3,1]\right\} \) while, by assumption, he is Maxmin Expected Utility ex-ante. In his paper, Fishburn provides axioms guaranteeing not only that the ex-ante preference be SEU, but also that the conditional preferences be obtained from the unconditional one by means of Bayes rule. In this paper, we are concerned with the study of conditional EU decision makers but from a different angle. Precisely, we are interested in characterizing all ex-ante (\(=\)unconditional) preferences that are compatible with conditional EU, and that satisfy an additional assumption, which we will discuss momentarily. Our interest is motivated not only by examples like the one above, but also by the examples in Fishburn (1973, pp. 19–23), which seem to suggest that a variety of non-EU unconditional preferences might be compatible with conditional EU. The assumption we were referring to is that of dynamic consistency. It is well known (see Ghirardato 2002) that in a SEU framework, the assumption of dynamic consistency is equivalent to that of Bayesian updating. Thus, our extension retains the spirit of Fishburn’s paper by replacing Bayesian updating with one of its possible generalizations. By extending Fishburn’s work along these lines, we obtain the following generalization of his result. Fishburn’s theorem can be thought of as saying that, under certain conditions, the unconditional preference is a “weighted average” of the conditional preferences. The operation of weighing is described by a probability measure. Here, we show that under our milder conditions, the unconditional preference is still a “weighted average” of the conditional preferences, but we allow for a more permissive notion of “weighing”: Fishburn’s probability is replaced by a capacity. The resulting unconditional preferences are a subclass of the Invariant Bi-separable Preferences of Ghirardato et al. (2004) but display, of course, a very special structure due to the combined action of the hypothesis of conditional expected utility and dynamic consistency. By using a result of Dominiak and Lefort (2011), we show that this structure is incompatible with that of Choquet Expected Utility preferences. We show, however, that there are many preferences of the \(\alpha \)-Maxmin Expected Utility type that satisfy our assumptions, and give several concrete examples of settings where it is reasonable to hypothesize that preferences would be of this type. Finally, we derive Fishburn’s theorem as a special case of our result. The paper unfolds as follows. The formal setting is described in Sect. 2. In Sect. 3, we face the conceptual problem of extending the meaning of conditional expected utility outside the realm of SEU theory and Bayesian updating. Definition 1 of Sect. 3 gives our solution. We conclude that section by giving an example of a preference relation of Bewley’s type (thus not SEU) which satisfies the criteria of our definition. In Sect. 4, we fully characterize those Conditional EU preferences which are, in addition, C-independent and dynamically consistent. In Sect. 5, we study more closely the structure of these preferences. In Sect. 6, we identify several concrete settings where our results could be applied. In Sect. 7, we present a set of results which parallel those of Fishburn (1973, pp. 19–23) and that, in fact, have a broader range of applicability. A detailed comparison of our assumptions with those of Fishburn (1973) can be found in the Appendix.",1
83.0,2.0,Theory and Decision,11 March 2017,https://link.springer.com/article/10.1007/s11238-017-9591-2,(Sub) Optimality and (non) optimal satisficing in risky decision experiments,August 2017,Daniela Di Cagno,Arianna Galliera,Noemi Pace,Female,Female,Female,Female,"The rational choice approach, albeit dominating (micro)economics, should be considered with caution in the real world because optimizing is often difficult: limited cognitive abilities, information overload and complexity will regularly lead to suboptimal decision making.Footnote 1 Such regular suboptimality renders also “rationality with errors” questionable.Footnote 2 Moreover, most choice situations involve multiple incompatible goals that must somehow be combined to reach a decision (as in multi-objective optimization).Footnote 3
 Many scholars now focus on alternative modelsFootnote 4 and, more generally, on bounded rationalityFootnote 5, such as satisficing behavior. To compare these two different strands of the literature (i.e., optimizing versus satisficing), we implement a choice class allowing not only for unique and set-valued optimality in the sense of expected utility maximization but also experimentally elicit and incentivize aspiration formation. Although the experimental setup is framed as a financial investment task this is only done to discourage other regarding concerns. Our main aim in designing it has been to control the usual tool used to align rational choice predictions with choice behavior. How this can be achieved will be described when presenting the class of experimental choice task. What can be said already now is that we do not claim external validity for the choice tasks. Rather they were signed to allow clearcut tests of optimality as well as optimal satisficing in consequentialistic decision making.Footnote 6 Our main message is to illustrate that, even when rationality relies on rather mild assumptions both optimality and satisficing might require experience.Footnote 7 In particular, in our setting each participant confronts two random sequences of 18 different (continuous) choice tasks whose expected success depends on a binary random event. Optimality requires only to prefer more money to less and to find the focal (corner) maximum of the expected success with several trials. Thus optimality can be assessed based on how choices deviate from the (corner) optimum and how costly this is. To avoid criticizing without providing alternatives, we also consider bounded rationality based on consequentialist choice deliberations and satisficing rather than optimizing. Instead of reacting to given and well-behaved preferences and beliefs about circumstances beyond their control, participants are asked to form goal aspirations and then successively test behavioral options to determine whether they are satisficing these aspirations before making a choice. From a methodological perspective this means that we do not rely on the revealed aspiration approach for empirical economics. Whereas “revealed motives” presuppose satisficing (respectively optimizing) eliciting aspirations in addition to choice data allows to test satisficing.Footnote 8
 In our setup, the realization of a binary chance event is beyond our control either “boom” (good outcome) or “doom” (bad outcome) circumstances result. Abstaining from imposing intrapersonal payoff aggregation as in expected utility and prospect theory (by aggregating the probability-weighted choice implications in boom and doom), means that participants form goal aspirations for boom and doom scenarios. When satisficing, one chooses a portfolio whose returns in boom, respectively doom, satisfy both aspirations. This does not rule out optimality as a border case: set-valued optimal satisficing requires that it is impossible to increase the aspiration for boom or doom without reducing the other. Such set optimality does not require probabilities of boom and doom, which are partly not experimentally induced.Footnote 9 Even when probabilities are experimentally induced, they may not be used for intrapersonal payoff aggregation but in forming and adapting aspiration levels, for instance, by forming more ambitious (moderate) aspirations for the more (less) likely event. A portfolio choice may be either satisficing (but not set optimal) or non-satisficing. In an experiment, one can confirm a portfolio choice even when it is not satisficing. According to our interpretation, satisficing is based on a forward-looking decision-making process involving several successive steps for the task at hand, aspiration formation and searching for satisficing options in the action space, with possible feedback loops in light of new information. We experimentally compared participants who are forced to reason according to this structure before deciding to participants who are allowed to decide freely, that is, without having to form aspirations. We neither predicted optimality nor optimal satisficing and we expected even considerable non-satisficing choices that will become less frequent when participants become more experienced that would illustrate how experience can improve behavior. Section 2 introduces the 18 choice tasks, or cases, and derives their optimal choices or choice sets. We then discuss the hypotheses in Sect. 3. The treatments and other details of the experimental protocol are described in Sect. 4. Sections 5 presents the results on (sub)optimality as pairwise comparisons of treatments; Sect. 6 focuses on satisficing and its statistical analysis, while Sect. 7 refers to special cases in the data. Section 8 concludes. The appendices include additional data analysis and the translated instructions of the experiment.",4
83.0,2.0,Theory and Decision,03 April 2017,https://link.springer.com/article/10.1007/s11238-017-9594-z,Does exposure to unawareness affect risk preferences? A preliminary result,August 2017,Wenjun Ma,Burkhard C. Schipper,,Unknown,Male,Unknown,Male,"The economics literature assumes that risk preferences do not change when perceiving new information. While this assumption may be justified when receiving standard information that is perfectly anticipated and foreseen, it is less compelling when conceiving novel facts of which the decision maker was previously unaware. Unawareness refers more fundamentally to the lack of conception rather than the lack of information (see Schipper 2015, for a survey). Nevertheless, the literature on unawareness also assumes that risk preferences are not affected by changes of awareness. For instance, all approaches to extensive-form games with unawareness in the literature (Feinberg 2012; Grant and Quiggin 2013; Halpern and Rêgo 2014; Heifetz et al. 2013b; see Schipper 2014, for a nontechnical review) assume implicitly that utilities associated with terminal histories are unaffected by awareness.Footnote 1 In work on updating under changes of awareness by Karni and Vierø (2013, 2017), the assumption of risk preferences being invariant to changes of awareness is the key axiom for linking preferences before and after becoming aware. Schipper (2013) characterizes awareness-dependent subjective expected utility theory with and without the assumption of invariant risk preference. All applications of unawareness in the literature (e.g., Auster 2013; Li et al. 2014; Maskin and Tirole 1999; Schipper and Woo 2015; von Thadden and Zhao 2012) implicitly assume that risk preferences are invariant to awareness. In this paper, we report on a preliminary test on this assumption. We conduct an experiment in which we first measure participants’ certainty equivalents for a lottery in phase 1, then expose participants to lotteries with various degrees of unawareness of outcomes (depending on the treatment) in phase 2, and then in phase 3 we remeasure participants’ certainty equivalents for the first lottery. This design allows us to test the null-hypotheses that certainty equivalents do not differ between treatments (of various forms of unawareness) and within subjects (after being exposed to various forms of unawareness). Somewhat to our surprise, we cannot reject the null-hypotheses in our data. One challenge of exposing participants to lotteries with unawareness of outcomes is how to describe the information about likelihood of outcomes to participants since probabilities must add up to one. We circumvent this difficulty by specifying the relative likelihood of outcomes of which participants are aware of rather than probabilities. That is, instead saying that this or that outcome has such and such probability, we say that this outcome is x-times as likely as that outcome. Our design allows us to study various notions of unawareness. Besides unawareness (i.e., Fagin and Halpern 1988; Heifetz et al. 2006), we also consider versions of awareness of unawareness. This is relevant because in many circumstances a decision maker may realize that she is unaware of something. Awareness of unawareness has been epistemologically studied by Ågotnes and Alechina (2007), Board and Chung (2011), Halpern and Rêgo (2009, 2013), Sillari (2008) and Walker (2014) (see Schipper 2015, for a survey). We consider two versions of awareness of unawareness. In the first version, the decision maker realizes that she is unaware of some outcomes and does not know how many outcomes there could be. In the second version of awareness of unawareness, the decision maker realizes that she is unaware of some outcomes and knows exactly how many outcomes she is unaware of. The latter version of awareness of unawareness has been implicitly used in the literature on incomplete contracting by Maskin and Tirole (1999) who assume that agents may not foresee physical contingencies but are able to foresee exactly the number of contingencies (see Ma and Schipper 2015, for an extension to asymmetric information). Karni and Vierø (2017) study subjective expected utility under awareness of unawareness and changes thereof. Again, the assumption that risk preferences are invariant to changes of awareness is their key axiom for linking preferences before and after changes of awareness. We also add a treatment with just ambiguity, i.e., in which the probability distribution over outcomes is not completely specified, in order to find out whether the effect of exposure to ambiguity on risk preferences is different from the effect of exposure to various notions of unawareness on risk preferences. We are not the first ones who attack experimentally the question of whether risk preferences are invariant to exposure of varying degrees of ignorance. We were inspired by Mengel et al. (2016). They present an experiment in which participants first choose between a fixed lottery and varying sure outcomes. There are three treatments that differ in the amount of information available about the lottery. In a second phase, which is identical in all treatments, participants choose between different lotteries and sure outcomes with all information available. The main finding is that participants who have been exposed to imperfect information become more risk averse in the second task. We discuss the similarities and differences between their and our experiment in Sect. 5. The paper is organized as follows. The next section presents the hypotheses, which is followed by the exposition of the experimental design in Sect. 3. Our results are reported in Sect. 4 and subsequently discussed in Sect. 5. An Appendix collects further details on the experiment such as instructions, screenshots and some additional analysis.",8
83.0,2.0,Theory and Decision,07 March 2017,https://link.springer.com/article/10.1007/s11238-017-9593-0,Who should cast the casting vote? Using sequential voting to amalgamate information,August 2017,Steve Alpern,Bo Chen,,Male,Male,Unknown,Male,"Beginning with the celebrated Jury Theorem of Condorcet (1785), the reliability of majority verdicts in secret (or simultaneous) ballots between two alternative states of Nature has been extensively analyzed. The states of Nature might be “Innocent” and “Guilty” in a trial context, or “In” and “Out” for a tennis refereeing team. Condorcet (1785) showed that the reliability of the verdict, that is the probability that the verdict is correct, approaches one as the number of voters (called jurors) goes to infinity. With the notable exceptions of Dekel and Piccione (2000) and Ottaviani and Sørensen (2001), the case of sequential voting has not received similar attention. We consider the notion of sequential voting quite generally, as covering voting schemes where numbered jurors vote in order, with each voter aware of the votes of a given subset of the earlier voters. For example, in so-called roll-call voting, the given subset is simply all the earlier voters; while in secret or simultaneous ballots, the given subset of each voter is always the empty set. In the casting-vote scheme, extensively studied here, all but one juror announce their votes simultaneously and then the “casting voter” has the deciding vote in a tie. Here, the given subsets for the early voters are empty, whereas that of the casting voter consists of all earlier voters. The casting-vote scheme in action can be observed, for example, in the academic review process for a journal or conference, where often an Associate Editor has the casting vote if recommendations of the two independent referees are in opposite directions. When the abilities of the jurors (to discern the true state of Nature) are heterogeneous, the order of voting in terms of juror ability may well affect the reliability of the verdict. Here we assume that the abilities of the jurors are common knowledge, though the problem of determining these abilities has been considered by Baharad et al. (2012). As discussed by Ottaviani and Sørensen (2001), different approaches to this problem have been made over time. Taking the term seniority to denote higher ability, they contrast the anti-seniority rule (increasing ability order) of the ancient Sanhedrin court to the seniority rule (decreasing ability order) for debating order in the US Supreme court. Alpern and Chen (2017), using the same private information model as here, show numerically that for roll-call voting of three jurors, reliability is maximized for neither of these rules: it is always best for the juror of median ability to vote first. To determine the optimal voting order, we need a strong model of private information and of juror ability: the former is determined by independent signals of real numbers in an interval which are correlated with the state of Nature; the latter is quantified by a number between 0 (no ability) and 1 (maximum ability) that determines the probability densities of these signals. Binary signals, which are common in the literature, are not sufficient to obtain our results. In this paper, we introduce general sequential voting schemes with our main results centering on the casting-vote scheme of three voters. We show that for honest voting (for the alternative that appears more likely at the time of voting) reliability is maximized when the median-ability juror has the casting vote. This contradicts common practice and accepted wisdom that gives the casting vote to the senior judge in a panel of three or gives the tennis umpiring post (with overrule or casting vote) to more senior people than to the linesmen. We also give some results for larger juries.",2
83.0,2.0,Theory and Decision,21 March 2017,https://link.springer.com/article/10.1007/s11238-017-9596-x,A note on identification in discrete choice models with partial observability,August 2017,Mogens Fosgerau,Abhishek Ranjan,,Male,Unknown,Unknown,Male,"This note establishes a new identification result for additive random utility discrete choice models, showing that the complete system of choice probabilities is identified from observation of the joint probability for a subset of the alternatives as a function of a vector of location shifts. A random utility model (RUM) associates a vector \(\mathbf {U}=\left( U_{1},\ldots ,U_{J}\right) \) of random utilities with a choice set consisting of J alternatives. A decision-maker receives a realization of the random utility vector and chooses the alternative with the maximum utility. If the joint distribution of utility is absolutely continuous, then this induces a unique multinomial choice probability vector, which can be computed by the analyst given knowledge of the joint distribution of utility. An additive random utility model (ARUM) is a random utility model where the random utility vector is perturbed by a deterministic vector \(\mathbf {m}\) such that the decision-maker maximizes \(\mathbf {m}+\mathbf {U}\) and the choice probability vector becomes a function of \(\mathbf {m}\), i.e. the choice probability system In applications, the perturbed random utility vector is parametrized to depend on observable variables. 
Matzkin (2007) showed that an ARUM is identified from the probability of a single alternative. That means that it is possible to determine the whole choice probability system (1.1) from the observation of a function \(\mathbf {m\rightarrow }\Pr \left( j|\mathbf {m} \right) \) that relates the probability of a single alternative j to the perturbation vector \(\mathbf {m}\). In this paper, we extend this result to the case where one observes the probability that the choice is in a set of alternatives that is any proper subset of the choice set. An example may be where the researcher observes prices and characteristics for all the different car models on a market, but where he/she only observes demand , e.g., at the level of brands. In such situations it is important to know what is identified from the data and what identification relies on parametric model specification. The result in this paper is a nonparametric identification result. Such results are useful to establish, on the one hand, the limits of what can be learnt from data and, on the other hand, to develop nonparametric estimators. Random utility models, and most often additive random utility models, have been extensively used in economics and other social science fields since the pioneering work of McFadden (1974). Amemiya (1981) and Maddala (1983) discuss an extensive list of applications of this model. These include the choice of mode of transportation, choice of occupation, and choice of residence. Section 2 first presents some preliminaries. Section 3 establishes identification from observation of the probability of a single alternative. This is expanded in Sect. 4 to the case where the probability of a set of alternatives is observed. Section 5 establishes precisely that the choice probability vector only identifies the distribution of random utility up to a univariate random variable added to all components of the random utility vector. Section 6 shows a way to construct ARUM with a nested structure corresponding to a partitioning of choice alternatives into groups. This construction relies on the function that relates expected maximum utility to the perturbation \(\mathbf {m}\). Section 7 concludes. For the exposition of the theory of ARUM we have drawn on an unpublished lecture note written by Dan McFadden (2014).",
83.0,2.0,Theory and Decision,25 March 2017,https://link.springer.com/article/10.1007/s11238-017-9598-8,Fair student placement,August 2017,José Alcalde,Antonio Romero-Medina,,Male,Male,Unknown,Male,"This paper explores the trade-off between efficiency and equity in the Student Placement problem and provides a new ‘compromise’ solution for this family of models that always select, at least, one allocation. Student Placement mechanisms were modeled in Balinski and Sönmez (1999), inspired by the two-sided problems introduced by Gale and Shapley (1962). These authors explore how school seats should be distributed among the students. A specific, and thus differential, characteristic of this problem is that schools are modeled so as to capture social conventions on commonly accepted primitives of equity. These social agreements are captured in the description of how schools prioritize the different students.Footnote 1 The connection between Student Placement and two-sided matching problems has inspired some authors to propose allocation systems to improve upon the ones that were established in some geographical areas (Abdulkadiroğlu and Sönmez 2003) and/or to identify the problems that current Student Placement systems might exhibit, and thus propose how to avoid them. One of the main problems faced by the Student Placement systems is the existence of an equity–efficiency trade-off (see, e.g., Example 1 in Abdulkadiroğlu and Sönmez 2003). The persistence of this conflict is reflected in the proposal of two (incentive compatible) procedures to distribute the available seats among the newcomers. The first one, named the Top Trading Cycles mechanism, TTC hereafter, ensures allocative efficiency at the expense of equity. The second procedure, known as the Student Optimal Stable mechanism, seeks to ensure equity at the cost of reducing the efficiency of the allocation. Since Abdulkadiroğlu and Sönmez (2003) the literature has proposed a few ways to reduce the relevance of such a conflict. In parallel with this normative approach, it has become commonly known that any attempt to reduce the efficiency–equity collision conflicts with the design of incentive compatible mechanisms (see, e.g., Kesten 2010, Proposition 1). In this matter, Kesten (2010) resorts to the idea of ‘consent’ by some students to avoid the potential presence of some inefficiency. The interpretation of Kesten’s consent is very related to the algorithm he describes to reduce the relevance of this trade-off between the two properties above, namely efficiency and equity. Under the Efficiency-Adjusted Deferred Acceptance algorithm, EADAM hereafter, the students may consent to waive her priority over some schools. A waiver by some student holds whenever two conditions are fulfilled. The first one is that, by waiving the priority, she does not hurt herself; the second one is that by waiving, the assignment of other students improves. This idea of consent has been also employed by Tang and Yu (2014) to introduce an algorithm that is computationally simpler than the EADAM. As Kesten (2010) and Tang and Yu (2014) show, their algorithms select efficient allocations that minimize the presence of inequity when all the students consent to waive their priorities. The lack of a rationale behind the consenting process prevents EADAM to substitute the role of a fairness concept overcoming the efficiency–equity trade-off. Recently, Morrill (2015) has concentrated on procedural equity, rather than allocative equity. His equity notion can be described as follows: imagine that we are employing a specific mechanism. As a consequence, Abel obtains a seat at School 1. Then, Beth argues that it is unfair because she prefers to be allocated at School 1 rather than being at her actual school. Beth’s objection is disregarded whenever there is another student who might obtain the seat Abel was assigned to by misreporting her preferences. Morrill (2015) proves that the TTC is the only mechanism that is strategy-proof, efficient and fulfills his equity notion (see Morrill 2015, Definitions 1 and 2). In Morrill (2015), the conditions that allow an objection—to an allocation—to be admissible is very related to the procedure used to select the allocation. This induces that some of the placements sanctioned as just are hardly identifiable as equitable (see Example 2). In a framework close to ours, Abdulkadiroğlu and Sönmez (1998) propose an alternative way to circumvent the efficiency–equity trade-off. They exploit the fact that some students declare preferences for schools that are not in their area of residential priority. They propose the following procedure to combine efficiency and equity: first, to ensure efficiency, they allow students to sequentially select their preferred school among those with vacant positions. Then, since this procedure is very dependent on the ordering in which the students make their choices—and thus very inequitable—select the ordering in which the selections are made at random, by drawing a fair raffle among all the possible orderings. This mechanism, known as the Random Serial Dictatorship, combines ex-post efficiency and ex-ante equity. Nevertheless, as pointed out by Bogomolnaia and Moulin (2001), the Random Serial Dictatorship mechanism fails to be ex-ante efficient. Furthermore, Bogomolnaia and Moulin (2001) establish the incompatibility of the two appealing normative properties from an ex-ante perspective. Our approach in the present paper departs from the above-mentioned analysis. Our aim is not to propose a systematic way to select efficient allocations that are not questionable in terms of equity.Footnote 2 Our objective is to find a weakening of equity that turns out to be compatible with efficiency. To describe how we reach our target it might be relevant to go back to the origins of this literature. As we pointed out, the growth of the literature on Student Placement is related to the analysis of two-sided matching mechanisms. The idea of justified envy, as defined by Balinski and Sönmez (1999), is borrowed from the notion of pairwise stability introduced by Gale and Shapley (1962). This connection invites to explore how some classical ideas of stability, weaker than that of the core, might be re-interpreted in terms of weak equity in the Student Placement framework to elude the disconnection between efficiency and (weak) equity. Therefore, we discuss how to state whether an individual’s objection is admissible.Footnote 3
 In this paper, we propose a notion of weak equity that follows the idea of absence of envy introduced by Foley (1967).Footnote 4 To illustrate whether an allocation is weakly equitable or not, let us assume that this allocation has been proposed. A student can object to this allocation by proposing an alternative allocation assigning her to a school where she has priority (with respect to some of the students previously assigned to that school). Then, the society should evaluate whether the objection is accepted or not. The proposal is defeated, and thus the objection is not accepted, whenever some student can present a new objection to the previous one formulated in the same terms. Otherwise, the objection is admissible. We consider that an allocation is almost-equitable, or \(\upalpha \)-equitable henceforth, whenever no student is able to object to it, by proposing an alternative allocation which cannot be objected in the same terms. We prove, Theorem 1, the existence of efficient, \(\upalpha \)-equitable allocations. Recall that under equity, as defined by Balinski and Sönmez (1999), such an existence cannot be granted. A further question that we deal with is how strong our equity property is. In Theorem 2 we show that \(\upalpha \)-equity is a weaker condition of equity, which overcomes the trade-off with efficiency: whenever we restrict to efficient allocations, \(\upalpha \)-equity and equity coincide unless no equitable allocation exists. The remaining of the paper is organized as follows: Sect. 2 introduces the basic model. \(\upalpha \)-equity is defined in Sect. 3, which also contains our main results. Finally, Sect. 4 concludes. The proofs are gathered in the Appendix.",14
83.0,3.0,Theory and Decision,02 February 2017,https://link.springer.com/article/10.1007/s11238-016-9580-x,Dynamic awareness and zero probability beliefs,October 2017,John Quiggin,,,Male,Unknown,Unknown,Male,"Decisions must be often be made in circumstances where decision-makers are unaware of all the relevant possibilities. The problem of unawareness has been the subject of a rapidly growing literature. A variety of approaches have been proposed including those of Grant and Quiggin (2013), Halpern and Rego (2014), Heifetz et al. (2006) and Karni and Viero (2013). Schipper (2016) provides a Bibliography. The problem of unawareness has assumed increasing importance in the light of the global financial crisis, which was unforeseen by most policy-makers and market participants. The popular treatment and well-timed treatment of the topic by Taleb (2007) refers to such events as ’black swans’. The global financial crisis also raises the need to model the dynamics of changing awareness, and in particular the process by which decision-makers become aware of previously unconsidered possibilities. There is no general agreement on the appropriate way to represent awareness and unawareness even in a static context. Heifetz et al. (2006) present a model in which the state space considered by a decision-maker is the codomain of a surjection with the full state space, available to an unboundedly rational decision-maker, as the domain. This representation of unawareness is referred to by Grant and Quiggin (2013) as ‘coarsening’. Grant and Quiggin propose an alternative approach, referred to as ‘restriction’ in which the state space considered by decision-maker is a subset of the full space. Only limited attention has been paid to the dynamics of awareness. The most notable contribution is the ‘reverse Bayesianism’ of Karni and Viero (2013). The central idea is that, when decision-makers become aware of a new possible state of nature, and assign it a positive probability, the probabilities for all elements of the pre-existing state space are reduced proportionally. An important question in this context is that of whether restriction unawareness may be managed within a standard Bayesian setting by treating events of which decision-makers are unaware as having zero, or vanishingly small, prior probability, then being updated in the light of new information. This question has been addressed, in passing, by authors including Grant and Quiggin (2013), Li (2008), Modica (2008) and Meier and Schipper (2013). All the authors cited have drawn distinctions between unawareness and zero probability beliefs. However, none has addressed the problem systematically. In this note, it is shown that in a Bayesian model with unawareness of impossible (probability zero), or vanishingly improbable, events, awareness can only change after such an improbable event has actually been observed.",
83.0,3.0,Theory and Decision,16 May 2017,https://link.springer.com/article/10.1007/s11238-017-9603-2,Uncertain discount and hyperbolic preferences,October 2017,Daniele Pennesi,,,Female,Unknown,Unknown,Female,"Individuals are impatient. The utility of a reward decreases as the time before its experience increases. Impatience is typically modelled using a constant discount factor (Koopmans 1960). Experimental evidence,Footnote 1 however, suggests that actual discount factors are not constant, but they decline over time: the further away is a payoff, the higher is individual’s patience. Such phenomenon is called Diminishing Impatience (DI), it may be a source of dynamic inconsistency and it is not compatible with the standard exponential discounting. In this work, we propose a possible explanation for DI stemming from the interaction of savagean uncertainty and intertemporal choice. Intuitively, uncertainty about the true state of the world may affect the time preferences of the individual, in particular, the discount factor used to evaluate future utility attached to the state. For example, the value of a sure payment at time t may depend on the result of a diagnostic test conducted before t, even though the utility of the payment remains the same. What affects the present value of the payment is the possibility of being more or less impatient, conditional on the result of the test.Footnote 2 Before uncertainty is resolved, the individual is unsure about the discount factor she will use, even when evaluating risk-free future payoffs. In turn, linear aggregation of state-dependent discount factors generates diminishing impatience. When uncertainty is resolved, the individual discounts the future utility exponentially, according to the discount factor attached to the realized state. Formally, the Uncertain Discount Expected Utility (UDEU) model evaluates an uncertain consumption plan \(c:\varOmega \rightarrow X^ \infty \) (a map from states of nature \(\varOmega \) to consumption streams \((c_0(\omega ),c_1(\omega ),c_2(\omega ),\ldots )\) and \(X\subset {\mathbb {R}}\)) by where \(\omega \in \varOmega \) is a Savage state of the world and p is a subjective probability. The model can be interpreted in the following way: the individual fixes a state \(\omega \), she calculates the present value of the consumption stream in \(\omega \), \((c_0(\omega ),c_1(\omega ),\ldots )\) according to the utility u over payoffs and the discount factor \(\delta (\omega )\). She repeats the same procedure for all states and she aggregates the present values using the subjective probability p. When evaluating consumption plans that do not depend on the state of the world, the actual discount factor is the subjective expected value of the discount factors \(E_p[\delta (\omega )^t]\). Diminishing impatience occurs since, as the delay increases, the more patient factors decrease more slowly than the less patient ones, lowering the overall impatience (Theorem 1). Therefore, diminishing impatience arises naturally as the result of state-dependent time preferences. Violations of stationarity (diminishing impatience) are often identified with time inconsistency, i.e., dynamic preference reversal. However, the two properties are logically independent: stationarity restricts preferences at a given point in time, whereas dynamic consistency restricts preferences at different points in time. Halevy (2015) clarified the relation between the two, showing that non-stationary preferences are time inconsistent only if they are stable over time, a property called time invariance. We generalize the result to infinite consumption streams and we give a sufficient condition to observe a UDEU utility that is non-stationary but time consistent. The condition restricts the time evolution of subjective probabilities. This extends to the Savage’s setting the approaches of Sozou (1998), Azfar (1999), Halevy (2005), who proposed non-stationary but time consistent models of choice assuming objective uncertainty (risk). Differently from these, in our model, Bayesian updating of subjective probabilities may be at odds with time consistency (see Example 1). The contribution of the paper is twofold: first, we show how diminishing impatience arises naturally in a setting where time preferences may depend on the realization of the state of nature. Our approach is less artificial than alternative approaches assuming uncertainty about the discount rates from the outset. Indeed, our model provides a foundation for the Implicit Risk Approach.Footnote 3 According to such theory, a subjective risk is attached to any future outcome: for example, the mortality risk or the possibility that a promise may be breached. In our model, uncertainty about the future is due to uncertainty about time preferences and it is completely subjective. Our model extends those assuming uncertainty coming from mortality rates. Indeed, the UDEU can be interpreted as a model of uncertain lifetime, if \(\delta (\omega )^t\) is interpreted as the probability of “reaching” time t. But it is not restricted to such interpretation, since state-dependent time preferences may follow from alternative explanations. For example, uncertainty about future wealth, education, self-control. Fisher (1930) proposed six “personal factors” that may influence impatience: foresight, self-control, habit, expectation of life, concerns for the lives of other persons, fashion. If uncertainty affects one of the factors, impatience is random. Second, we show that also in the subjective uncertainty setting, we can explain a variety of behavioral patterns related to non-stationarity and time consistency. The UDEU may exhibit non-stationary and time inconsistent choices (if we retain time invariance). Alternatively, the UDEU can accommodate non-stationary but time consistent choices (if we drop time invariance). Lastly, the UDEU can also reproduce a discounting behavior that resembles the quasi-hyperbolic discounting model of Laibson (1997), although the nature is different. It corresponds to the UDEU with only two distinct discount factors, one of which is extremely impatient (see Sect. 3.1); for example, evaluating future payments before a dangerous surgery. In this case, the discount functions are equivalent to the \(\beta \)–\(\delta \) model, where \(\beta \) is the probability of the extremely impatient rate (die during the surgery). The interaction between time and uncertainty as a possible explanation for diminishing impatience has been proposed, under various forms, in the literature. Halevy (2008), Epper and Fehr-Duda (2015), Baucells and Heukamp (2012), and Saito (2015) identify delayed payments with objective lotteries, since any future payoff is intrinsically uncertain, for example, due to mortality risk. In the objective risk domain, this identification establishes a one-to-one relation between violations of expected utility (non-linear weighting of probabilities) and violations of stationarity.Footnote 4 Such identification is supported experimentally (see Keren and Roelofsma 1995; Baucells and Heukamp 2010). Our approach differs from this, since in the Savage’s setting there is no objective risk and the relation between violations of expected utility and non-stationarity breaks down. We can have non-stationary discount with subjective expected utility (see Theorem 1) and stationary discount with non-linear subjective expected utility (see Sect. 7). Alternative models such as Azfar (1999), Farmer and Geanakoplos (2009), explained DI assuming uncertainty in the discount factor and retaining expected utility. However, the assumption that subjective discount factors are uncertain per se is difficult to support outside the realm of finance. Our approach offers a possible rationale for uncertainty in the discount factors: state-dependent time preferences. All the results in our setting depend on the effect of uncertainty, therefore, our explanation of diminishing impatience does not substitute those proposed in deterministic settings (for example lack of self-control). What we want to highlight is that with Savage’s uncertainty, diminishing impatience may be natural and not related to lack of self-control or uncertainty in the experience of future payoffs. While state-dependent preferences are natural, the exponential discounting form of conditional preferences \(\delta (\omega )^tu(c_t(\omega ))\) may be questioned. Firstly, the current work contributes to a literature who explains diminishing impatience exploiting the interaction between time preferences and uncertainty. Since we are assuming that uncertainty is the only driving force behind diminishing impatience, it is natural to posit that absence of uncertainty or the knowledge of the true state of the world should make preferences stationary. This is true in the UDEU, due to exponential discounting of conditional preferences. If the individual knows the true state of the world \(\hat{\omega }\), she will evaluate future consumption using \(\delta (\hat{\omega })^tu(c_t(\hat{\omega }))\), a stationary preference. In other words, if the state of the world affects the individual’s tastes about future consumption, when knowing the true state she can anticipate her future tastes and behave rationally. This prediction is peculiar to our model since, it is the unique model studying the interaction between discount and subjective uncertainty. Indeed, if we introduce a precise form of subjective uncertainty in the immediate payoffs (see Fact 1), diminishing impatience should disappear. This is a testable restriction that can be used to validate or falsify the model. Lastly, in Sect. 7, we relax the expected utility assumption allowing for a different attitude toward the uncertainty coming from the states of the world and the “future”, in the spirit of Nau (2006), Ergin and Gul (2009). Relaxing linearity in probability, we can observe stationary preferences also in the presence of state-dependent time preference. Therefore, the result linking diminishing impatience and uncertain discount factors is peculiar to the (state-dependent) expected utility form of the UDEU.",5
83.0,3.0,Theory and Decision,27 April 2017,https://link.springer.com/article/10.1007/s11238-017-9600-5,When and how to satisfice: an experimental investigation,October 2017,John D. Hey,Yudistira Permana,Nuttaporn Rochanahastin,Male,Unknown,Unknown,Male,"This paper is about satisficing behaviour. Way back in 1955 Herbert Simon made a call for a new kind of economics stating that the task is to replace the global rationality of economic man with a kind of rational behavior that is compatible with the access to information and the computational capacities that are actually possessed by organisms, including man, in the kinds of environment in which such organisms exist. (p 99) There is a fundamental conflict here provoked by the use of the word ‘rational’, and economists’ obsession with it. The problem is that the expression ‘rational behaviour’ covers virtually all forms of behaviour, as long as it is motivated by some ‘rational’ objective function, and the decision-maker has all relevant information available to him or to her, and the decision-maker (henceforth, DM) can perform all the necessary calculations costlessly. If calculations are costly, then we are led into the infinite regression problem, first pointed out by Conlisk (1996), and rational behaviour, as defined by economists, cannot exist. We are, therefore, constrained to operate with rational models, defined as above. The way forward, within the economics paradigm, is therefore to weaken our ideas of what we mean by rational behaviour. This is the way that economics has been moving. Prominent amongst these latter weaker theories are theories of incomplete preferences (Ok et al. 2012; Nau 2006; Mandler 2005; Dubra et al. 2004); theories of behaviour under ambiguity (Etner et al. 2012; Gajdos et al. 2008; Ghirardato et al. 2004; Hayashi and Wada 2010; Klibanoff et al. 2005; Schmeidler 1989; Siniscalchi 2009); theories of rational inattention (Sims 2003; Manzini and Mariotti 2014; Matejka and McKay 2015; Caplin and Dean 2015); and search theories (Masatlioglu and Nakajima 2013; McCall 1970; Morgan and Manning 1985; Stigler 1961). A useful survey of satisficing choice procedures can be found in Papi (2012). Almost definitionally, models of incomplete preferences have to be concerned with satisficing: if the DM does not know his or her preferences, it is clearly impossible to find the best action. These models effectively impose satisficing as the only possible strategy. The problem here is that complete predictions of behaviour must also be impossible. Prediction is possible in models of behaviour under ambiguity. But here again satisficing behaviour ‘must’ occur, if only because not all the relevant information is available to the DM. Unless the DM’s information is objectively correct, there is presumably always some action that is better than the one chosen by the DM. But here the DM does not choose to satisfice; nor does he or she choose how to satisfice. Models of rational inattention also capture the idea of ‘satisficing’ behaviour—in that choice is made from a subset of the set of possible actions—those which capture the attention of the DM, that is, those which are in the consideration set of the DM. However, these theories are silent on the reasons for the formation of a consideration set, and, in some of them, on how the consideration set is formed. We examine a new theory—that of Manski (2017)—which might be classified as an extended search model. Search models seem to be closest to the scenario in which Manski’s paper is set. Standard search models assume that the DM is searching for the highest number in some distribution and that there is a cost of obtaining a drawing from that distribution. Because of this cost, the DM does not keep on searching until he or she finds the highest number: generally he or she should keep on searching until a ‘sufficiently’ high number is found. This could be termed the DM’s aspiration level. One interpretation of Manski’s paper is that he generalises the story: in addition to being able to search for numbers greater than some (or several) aspiration level(s), the DM can pay a higher search cost and be able to find the highest number, and also the DM can choose not to indulge in any search and simply receive a lower number. Manski not only considers choice between these three strategies, but also the choice of the aspiration level(s). This is the ‘how’ of Manski’s theory: he explains how many times satisficing should be implemented, how aspiration levels should be formed and how they should be changed in the light of the information received.Footnote 1
 We experimentally test this new theory. Some of the other models that we have discussed have also been tested experimentally; for incomplete preferences we refer the reader to Cettolin and Riedl (2016), Costa-Gomes et al. (2014) and Danan and Ziegelmeyer (2006); for behaviour under ambiguity to Abdellaoui et al. (2011), Ahn et al. (2010), Halevy (2007), Hey and Pace (2014) and Hey et al. (2010); for rational inattention to Chetty et al. (2009), De Los Santos et al. (2012); and for search theories to Caplin et al. (2011), De Los Santos et al. (2012), Hayashi and Wada (2010) and Reutskaja et al. (2011). Our experimental test has some similarities in common with some of these and some differences. In some ways our test is closest to that of Hayashi and Wada (2010), though they test minimax, \(\alpha \)-maximin and the (linear) contraction model (Gajdos et al. 2008). We test Manski’s model and have a different way of generating imprecise information/ambiguity. In the next section we describe the Manski model, while in Sect. 3 we discuss the experimental design. Our results are in Sect. 4, and Sect. 5 concludes.",15
83.0,3.0,Theory and Decision,11 May 2017,https://link.springer.com/article/10.1007/s11238-017-9602-3,A note on the Condorcet jury theorem for couples,October 2017,Raphael Thiele,,,Male,Unknown,Unknown,Male,"Should the UK stay in the European Union or leave? This question was answered by a referendum in June 2016. Each adult of the UK had the choice to vote for one of these two options. The option which got the majority of the votes represented the opinion of the crowd. The ballot was secret, but it is quite likely that the votings were not independent of each other. An explanation for this proposition is that the decision-making processes happen in the weeks and days before the referendum. In this time period people discussed with each other about the referendum or were influenced by newscasts. We transfer the framework of this referendum into a simplified model. Hence, the complexity has to be reduced compared to the real world. Each person of the crowd is represented by his or her individual competence, which is the probability that he or she votes for the better option. This approach is well known in Social Choice Theory (see Grofman 1975; Berg 1993). For simplification of the dependences, we use the approach of Althöfer and Thiele (2016). They argue that the biggest influence exists between a wife and her husband. The influence of other people is relatively small thus it can be neglected. Abstractly, this results in a crowd where each person is correlated with exactly one other person and votes independently of the remaining persons. However, the authors demanded the same individual competences for the agents who form a couple. In this note we generalize this approach by allowing different individual competences within the couples. The note is organized as follows. Section 2 contains the generalized N-couple model and Sect. 3, the corresponding theoretical results. We conclude with a summary and discussion in Sect. 4.",
83.0,3.0,Theory and Decision,27 March 2017,https://link.springer.com/article/10.1007/s11238-017-9595-y,Equilibria with vector-valued utilities and preference information. The analysis of a mixed duopoly,October 2017,Amparo M. Mármol,Luisa Monroy,Asunción Zapata,Female,Female,Female,Female,"The theory of games with vector payoffs is concerned with situations in which a number of players must take into account several objectives, each of which depends on the decision of all players. This type of game was introduced by Blackwell (1956). Subsequently, Shapley (1959) presented the natural extensions of the concept of Nash equilibrium, (Nash 1951), for two-person zero-sum finite games with vector payoffs: strong equilibria and weak equilibria, which are the basic concepts in multi-objective games. Among the authors who have studied the existence of equilibria for this kind of game are Zeleny (1975), Corley (1985), and Borm et al. (1988). For general n-person multi-objective games, the first results on the existence of equilibria were established in Zhao (1991). Interesting work on the topic includes Voorneveld et al. (1999), Bade (2005), and Patriche (2014). The present paper is also devoted to the analysis of equilibria of n-person non-cooperative games where the payoffs of the agents are multi-dimensional. We call them games with vector-valued utilities. These games represent situations in which the preferences of the agents on the results of the interactions are incomplete. In the existing literature on models with incomplete preferences, the two classic references are Aumann (1962) and Bewley (1986). More recently, this decisional framework has been studied from various viewpoints. In particular, certain authors have established a formal connection between incomplete preferences and multi-objective decision-making under certainty and risk (Ok 2002; Dubra et al. 2004; Sagi 2006). The fundamental difficulty is the impossibility of representing incomplete preferences in terms of utility functions, and hence the application range remains limited. This is the main cause of the scarcity of results in this line of research. However, as shown in Ok (2002), under certain not particularly restrictive hypotheses, incomplete preferences can be represented by means of vector-valued utility functions. This approach causes no loss of information and enables these situations to be studied from an analytical standpoint by using the well-developed theory of vector optimization in the operations research literature. The literature on incomplete preferences mainly deals with issues of individual choice, and only a few papers address non-cooperative models of interaction between agents with incomplete preferences. Some exceptions of note are found in Shafer and Sonnenschein (1975), Bade (2005), and Park (2015). As mentioned in Bade (2005), one argument for studying games with vector-valued utilities is the possibility of addressing cases in which agents do have weighted utilities over multiple criteria but the modeler, as an outside observer, remains unaware of agents’ weights. In these situations, the set of equilibria of the game modeled as a vector-valued game will always include the equilibria of the actual game. An alternative interpretation of the equilibria in this model is that the agents are uncertain about their own weights and they will only deviate when the deviation gain is positive for every possible weight vector. Nevertheless, in either interpretation, it is also possible that the agents’ weights are known to belong to certain subsets, in which case the incorporation of this information into the model should lead to predictions that are better adjusted. In the present paper, we initially adopt the formal framework in Bade (2005) to model n-person games with incomplete preferences that can be represented by a vector of utility functions. In Bade’s paper, the equilibria for these games are characterized in terms of the equilibria of weighted games under reasonable concavity assumptions. Her results are then applied to identify the sets of equilibria in several examples from oligopoly theory. However, the set of equilibria of games with incomplete preferences may contain a large number of strategy profiles, some of which may not represent realistic predictions. For this reason, the first goal of our research is to present a procedure to obtain a number of refinements of the set of equilibria based on partial preference information. Thus, we can study which kind of predictions are derived from the information available without requiring additional assumptions about the utility functions of the agents. To deal with partial information, we need to rely on the weak extension of the standard concept of the Nash equilibrium (see Shapley 1959; Wang 1993; Voorneveld et al. 1999). The adoption of this extension is not, in general, a drawback, since the sets of strong equilibria and weak equilibria often coincide. In fact, under certain concavity assumptions, the two concepts of equilibrium coincide. When they do not, the difference usually lies on boundary points. A first interesting result is the characterization of equilibria in terms of the reaction functions of the components of the utility. This result is relevant from an operational point of view, and will recursively be applied throughout the paper for the identification of equilibria. We assume that the underlying incomplete preferences of the agents can be represented by weighted additive value functions where weights may be interpreted as the relative importance that the agents assign to the components of their vector utility functions. The equilibria of the game are then identified with the equilibria of the corresponding weighted games. In this framework, information about preferences is formalized by means of sets of weights which, in general, are different for each agent. A central result is established which identifies the set of equilibria of the game with partial preference information with the set of equilibria of a transformed vector-valued game. The inclusion of agents’ preferences into the model through admissible weights provides the equilibria in accordance with these preferences. However, it is often the case that the agents, even when they admit that a certain set of weights are possible, might also exhibit a conservative attitude with respect to the results they will eventually achieve. Empirical evidence that agents are more uncertainty-averse than uncertainty-loving can be found, for instance, in Wakker (2001). More recently, Kozhan and Salmon (2009) found significant evidence of uncertainty aversion in the foreign exchange market. Thus, we apply a worst-case analysis based on the well-known max–min criterion proposed by Wald (1950), and adapted to partial preference information (see also Gilboa and Schmeidler 1989). We propose a refinement based on a rule by which the agents select their decisions by maximizing the worst evaluation from among those provided by the feasible weights in their information sets. A second goal of this paper is to show the potential of this analysis for the study of non-cooperative economic models. The theoretical results herein developed have been applied to a relevant type of decision-making process with interacting agents. We have extended the analysis of standard oligopoly models from firms that only consider the maximization of their own profits to a more realistic situation where firms incorporate additional goals. We have focused on mixed oligopoly models. Traditionally, these models are those that consider private firms that are profit maximizers and public firms with social goals. Since the seminal paper of Merrill and Schneider (1966) appeared, there has been a growing literature concerning several aspects and implications of mixed duopolies in the markets (see De Fraja and Del Bono 1989, 1990) and the references therein). Nevertheless, the literature on the topic usually excludes profit maximization from the goals of the social firms. Our approach is more flexible and permits the analysis of firms with social objectives which do not completely abandon the pursuit of maximum profits that ensures their permanence in the market. We specifically analyze the case of mixed duopoly under various assumptions about the firms’ objectives and present the results and interpretations regarding the sets of equilibria. The following notation will be used. Let \(\mathbb {R}(\mathbb {R}_+)\) denote the set of all (non-negative) real numbers and let \(\mathbb {R}^k (\mathbb {R}_+^k)\) be the k-fold Cartesian product of \(\mathbb {R}(\mathbb {R}_+)\). The origin of \(\mathbb {R}^k\) is \(0^k\) and \(1^k\) is a k-dimensional vector with components equal to one. We use conventional notation for the comparison of vectors: \(x \geqq y\) means that \(x_i \ge y_i\) for all \(i=1, \ldots , k\); \(x \ge y\) indicates that \(x \ge y\) and \(x \ne y\); and \(x >y\) indicates that \(x_i >y_i\) for all \(i=1,\ldots ,k\). By \(x \cdot y\) we denote the scalar product of vectors \(x, y \in \mathbb {R}^k\), that is, \(x \cdot y = \sum _{i=1}^{k} x_iy_i\). The rest of the paper is organized as follows. In Sect. 2 the concepts of equilibria and weak equilibria for n-person games with vector-valued utility functions are stated and their relationship with the equilibria of weighted games is investigated. In Sect. 3, we present refinements of the equilibria by incorporating information about the preferences of the agents into the model. Section 4 is devoted to the analysis of a mixed oligopoly in which the firms pursue additional goals to those of maximizing profits. Section 5 is devoted to setting out the conclusions. Proofs are contained in an Appendix.",12
83.0,3.0,Theory and Decision,11 May 2017,https://link.springer.com/article/10.1007/s11238-017-9604-1,Nullified equal loss property and equal division values,October 2017,Sylvain Ferrières,,,Male,Unknown,Unknown,Male,"Reconciling individual and social interests is a common theme in many economics fields. For instance, solutions for bankruptcy problems often possess an egalitarian flavor (see Thomson 2015, for a recent survey). Similarly, egalitarian considerations are also central in fair division problems as pointed out by Thomson (2011). Cooperative games with transferable utility (TU-games henceforth) are often used to model analogous allocation situations. A solution for a class of TU-games is called a value and assigns to each TU-game in the class and to each player a payoff for her participation. This article deals with egalitarian solutions by introducing a new axiom for TU-games called the nullified equal loss property. This axiom rests on the nullification operation, introduced in Neyman (1989), used in Gómez-Rúa and Vidal-Puga (2010) and Béal et al. (2014), and studied in Béal et al. (2016). A player is nullified if the worth of any coalition to which she belongs becomes equal to the worth of the same coalition without the player, i.e. the player is null in the resulting new game. The nullified equal loss property requires that if a player is nullified, then all other players experience the same payoff variation. Our results detailed in the next paragraph suggest that this axiom captures an essential feature of egalitarian values such as the equal division and equal surplus division values, as opposed to marginalistic values such as the Shapley value (Shapley 1953). These results are in line with a recent and growing literature on the axiomatic characterizations of classes of egalitarian values (van den Brink and Funaki 2009; van den Brink et al. 2016), their axiomatic comparisons with the Shapley value (van den Brink 2007; Béal et al. 2015), and axiomatic characterizations of combination of both types of values (Ju et al. 2007; Casajus and Hüttner 2014b). The main results are as follows: first, if two values satisfy the nullified equal loss property and efficiency, and furthermore coincide on the class of additive TU-games, then they are equal for all TU-games (Proposition 1). This result provides the principle of a unique extension from additive TU-games to all TU-games. Second, Proposition 2 extends this principle for values that are linear, symmetric and efficient, and proves that the extended value must be a linear combination of the equal division value and the equal surplus division value. As a corollary, the latter class of values is characterized by linearity, symmetry, efficiency and the nullified equal loss property. Third, the more natural class of convex combinations of the equal division value and equal surplus division value is singled out by invoking efficiency, additivity, the nullified equal loss property together with desirability and superadditive monotonicity (Theorem 1). Desirability (Maschler and Peleg 1966) requires that if a first player contributes not less than a second player to coalitions, then the first player should not obtain a smaller payoff than the second player. Superadditive monotonicity is new and states that the players’ payoff are nonnegative in a TU-game that is both superadditive and monotone. The axiom is implied by both monotonicity (Weber 1988), which does not require the superadditivity of the monotone TU-game, and the axiom of nonnegativity in van den Brink et al. (2016) which imposes nonnegative payoff for nonnegative TU-games in which the grand coalition achieves a worth not less than the sum of the singletons’ worth. This class emerges naturally in auction games as a mean for the player who obtains the indivisible good to compensate the other players (see van den Brink 2007). Interestingly, all axioms in Theorem 1 except the nullified equal loss property are also satisfied by the Shapley value. This enables comparisons: replacing the nullified equal loss property by the classical null player axiom yields a characterization of the Shapley value, and replacing the nullified equal loss property by the null player in a productive environment (Casajus and Hüttner 2013) characterizes the egalitarian Shapley values, even if some axioms may be redundant. Fourth, thanks to Proposition 1, an elegant characterization of the equal surplus division value is obtained by adding the well-known inessential game property to efficiency and the nullified equal loss property. Although there are few applications of egalitarian solutions for TU-games to economic models, the last part of this article presents two such applications. The first one considers the nullification of a player as a random event in a context of bargaining under risk. It shows that the nullified equal loss property is compatible with non-linear values that incorporate the risk aversion of the players. The second one endogenizes a choice of a value in a non-cooperative model of common-pool management. It is shown that the unique value which maximizes the social welfare at equilibrium is a specific convex combination of equal division value and equal surplus division value. We recover here an old result shown in Sen (1966) in a related context. The rest of the article is organized as follows: Section 2 provides necessary notation and definitions. Section 3 contains the results. The two applications are presented in Sect. 4. The final Sect. 5 discusses some nice parallelisms with the related literature.",14
83.0,3.0,Theory and Decision,26 April 2017,https://link.springer.com/article/10.1007/s11238-017-9601-4,A multiattribute decision time theory,October 2017,Nobuo Koida,,,Male,Unknown,Unknown,Male,"A decision maker (DM) often experiences a tradeoff when choosing between alternatives. One example is the choice between the attribute values of goods, such as cars. For instance, car x is a luxury model but expensive, whereas car y provides average performance but is reasonably priced. Another example is the tradeoff between current and future profits: investment project x yields a substantial current profit but a trivial future profit, whereas investment project y yields an enormous future profit but a negligible current profit. As a third example, a tradeoff may exist between self-interest and emotion (Loewenstein and O’Donoghue 2004): option x is more self-serving, whereas option y is more emotionally appealing. Finally, this type of tradeoff is also likely to occur in collective decision-making, wherein DMs are individuals who have different objectives. If alternatives x and y in these examples appear almost equally attractive to the DM but some difference is still discernible, the tradeoff is intensified and referred to as a conflict. Because the DM finds it difficult to resolve such a conflict, he or she may carefully examine each alternative before reaching a decision, which presumably involves a longer decision time.Footnote 1 Such a relationship between conflict and decision time is well known in psychology (Berlyne 1960; Dhar 1997; Diederich 2003; Festinger 1964; Tversky and Shafir 1992; Tyebjee 1979; Weber et al. 2000), and has been confirmed by recent empirical studies on economic topics, such as time preference (Chabris et al. 2009), the ultimatum game (Knoch et al. 2006), search (Gabaix and Laibson 2005; Gabaix et al. 2006), and lottery choice (Wilcox 1993).Footnote 2
 Accordingly, the objectives of this paper are threefold. First, to formalize conflict, decision time, and their relationships within a binary choice framework, we axiomatize a multiattribute decision time (MDT) representation, which is a dynamic extension of the classic multiattribute expected utility theory (e.g., Debreu 1959; Fishburn 1965; Pollak 1967) that allows for potentially incomplete preferences. Consider the preferences \(\succsim ^\tau \) for periods \(\tau = 0,1,\ldots ,\) and regard the ranking between alternatives (i.e., multiattribute lotteries) at \(\tau \) as decisive if one is preferred to the other, and as indecisive if neither is preferred. Our main theorem, Theorem 1, indicates that MDT renders the ranking between alternatives decisive, that is, alternative p is preferred to q at period \(\tau \), if and only if the weighted sum of the attribute-dependent expected utility induced by p is greater than that induced by q for all attribute weights in some closed and convex set \(\varLambda ^\tau \). Conversely, the ranking between p and q is indecisive if there is a disagreement in the evaluation, that is, the weighted sum induced by p is larger for some attribute weights, whereas that induced by q is larger for other weights. This latter scenario captures the conflict described above. An advantage of the MDT approach over the aforementioned existing studies is that our axiomatization can characterize conflict not only descriptively but also normatively. Our first key axiom, single-attribute regularity, requires that a ranking between two alternatives that differ with respect to only one attribute is decisive, and independent of values in other attributes because no conflict exists between such alternatives. This implies that conflict only occurs in our model if the alternatives’ values differ in two or more attributes. Second, our consistency axiom requires that the ranking between alternatives is never indecisive once it has been decisive in an earlier period. Corollary 1 indicates that this axiom uniquely determines the decision time \(\tau ^*(p, q)\) between alternatives p and q as the earliest period at which the ranking between p and q became decisive. Second, we conduct comparative statics with respect to the decision time, that is, a strictly positive decision time in MDT not only indicates whether conflict exists between alternatives, but also derives information on how difficult it is to reach a decision, which also sets it apart from atemporal incomplete preference approaches, such as those discussed by Bewley (1986) and Dubra et al. (2004). First, we conduct an interpersonal comparison and find that the set \(\varLambda ^\tau \) of attribute weights serves as an index of the DM’s susceptibility to conflict, that is, the preference becomes less decisive (i.e., the decision time between any given alternative pair becomes longer, while preserving the ranking over the alternative pair) if and only if \(\varLambda ^\tau \) expands for all \(\tau \) (Theorem 2). Second, we provide two conflict intensity indices relevant to the decision time. One may assume that a longer decision time between two alternatives is associated with the alternatives being closer to indifference. Thus, we define our first conflict intensity index by the angle formed between the long-run indifference curve (i.e., an indifference curve induced by an attribute weight \(\lambda ^\infty \) included in all \(\varLambda ^\tau \)) and the difference in the expected utility vectors induced by the alternatives. A similar reasoning is used to derive our second conflict intensity index, the difference of weighted expected utility induced by alternatives, given the long-run attribute weight \(\lambda ^\infty \). Theorems 3 and 4 indicate that these indices are indeed the predictors of decision time, and the decision time is also the predictor of these indices. This result agrees with those in the empirical literature and shows that the decision time provides useful information for specifying indifference curves for potentially incomplete preferences. Finally, we demonstrate that MDT can explain many empirical findings relevant to the decision time in the literature, such as choice under conflict (Tversky and Shafir 1992), time preference (Chabris et al. 2009), and the ultimatum game (Knoch et al. 2006). Our model typically predicts a zero decision time when one alternative dominates the other. This is derived from our monotonicity axiom, and carries intuitive and appealing implications. The remainder of this paper is organized as follows. In Sect. 2, we present our basic framework, state the main representation theorem, and formalize the decision time. In Sect. 3, we provide an interpersonal comparison result and define two conflict intensity indices relevant to the decision time. In Sect. 4, we illustrate some applications of MDT that accommodate empirical results. In Sect. 5, we review the related literature, and provide concluding remarks in Sect. 6.",3
83.0,3.0,Theory and Decision,06 April 2017,https://link.springer.com/article/10.1007/s11238-017-9599-7,Optimal inequality behind the veil of ignorance,October 2017,Che-Yuan Liang,,,Unknown,Unknown,Unknown,Unknown,,
83.0,4.0,Theory and Decision,21 June 2017,https://link.springer.com/article/10.1007/s11238-017-9609-9,Correlated equilibrium in a nutshell,December 2017,Rabah Amir,Sergei Belkov,Igor V. Evstigneev,Unknown,Male,Male,Male,"Correlated equilibrium, introduced by Aumann (1974), is a classical concept in game theory. A distinctive literature has emerged to address various facets of this concept, including a multi-stage extension (Forges 1986), epistemic foundations (Aumann 1987), a direct approach to existence (Hart and Schmeidler 1989), communication mechanisms (Forges 1990), perfection-type refinements (Dhillon and Mertens 1996), emergence as limit of a class of learning dynamics (Hart and Mas-Colell 2000), and computational methods (Papadimitriou and Roughgarden 2008). Introductory material on correlated equilibrium can be found practically in every modern text on game theory or microeconomics. Typically, the presentation of this subject is structured as follows. First a definition of this concept is given in a relatively general setting, and then the conceptual analysis proceeds nearly exclusively via examples. To the best of our knowledge, we cannot point to a source where the exposition deviates from this scheme and where a complete theory is provided, at least in a sufficiently narrow framework. The purpose of this note is to fill this gap. As a vehicle for our analysis, we use the framework of two-player two-strategy games. It is well known that such games possess a fairly rich structure. They represent a convenient and simple model that makes it possible to clearly demonstrate a number of fundamental ideas, results and paradoxes of game theory, as reflected in the fact that some of the most widely invoked games in the social sciences belong to this class. We begin with the definition of correlated equilibrium in a two-player two-strategy game. This notion is explained and interpreted by using Harsanyi’s (1967–1968) “mediator”. A system of linear inequalities is derived whose solutions are correlated equilibrium strategies. Then this system is transformed and represented in a compact and tractable form. Based on this representation, we develop an intuitive and easily memorizable algorithm allowing one to check whether a given correlated strategy forms a correlated equilibrium. An outline of the algorithm is as follows. First the original game is reduced to an equivalent (having the same correlated equilibria) simple game. A game is called simple if the non-diagonal payoffs for both players are zero. After that the question boils down to the characterization of correlated equilibria for the given simple game. To analyze this question we construct an auxiliary “test game”, in which Players 1 and 2 have strategies \(a_{1},a_{2}\) and \(b_{1},b_{2}\), respectively. It turns out that the given correlated strategy represents a correlated equilibrium if and only if each of the strategy profiles \((a_{1},b_{1})\) and \((a_{2},b_{2})\) forms a Nash equilibrium in the test game. We apply the above algorithm to describe all symmetric correlated equilibria in a symmetric simple two-strategy game. In this narrow, but still interesting, framework a correlated strategy can be specified by two numbers and thus can be represented by a point on the plane. Several cases are considered in which the set of correlated equilibria corresponds to a quadrangle, triangle, or segment in the plane. This leads to a complete theory of symmetric correlated equilibria in the simplest model we are dealing with. In the authors’ mind, the main motivation behind this novel treatment of correlated equilibrium is twofold. First, it may pave the way for a more extensive use of correlated equilibrium in applied settings in economics and other social sciences. Second, it may facilitate the teaching of this concept at a level beyond illustrative examples. The remainder of the paper is organized as follows. In Sect. 2, we introduce the concept of correlated equilibrium and derive the system of linear inequalities characterizing all correlated equilibria. In Sect. 3, we develop an algorithm based on the “test game” making it possible to verify that a given correlated strategy forms a correlated equilibrium. In Sect. 4, we provide a complete characterization of all symmetric correlated equilibria for a symmetric simple two-strategy game. An example in Sect. 5 concludes the paper.",1
83.0,4.0,Theory and Decision,31 May 2017,https://link.springer.com/article/10.1007/s11238-017-9606-z,On the characterization of weighted simple games,December 2017,Josep Freixas,Marc Freixas,Sascha Kurz,Male,Male,,Mix,,
83.0,4.0,Theory and Decision,21 July 2017,https://link.springer.com/article/10.1007/s11238-017-9626-8,On Hobbes’s state of nature and game theory,December 2017,Bertrand Crettez,,,Male,Unknown,Unknown,Male,"In Leviathan (1981) [1651] (p. 185), Thomas Hobbes described the state of nature as the war of every one against everyone. As Hobbes (ibid, p. 186) famously puts it In such condition, there is no place for industry, because the fruit thereof is uncertain: and consequently, no culture of the earth, no navigation, nor use of the commodities that may be imported by sea; no commodious building, no instruments of moving, and removing such things as require much force; no knowledge of the face of the earth, no account of time; no arts, no letters; no society; and what is more, continual fear, and danger of violent death; and the life of man, solitary, poor, nasty, brutish, and short. There is a large body of literature that relies on two-player two-action (henceforth \(2 \times 2\) games) games to analyze Hobbes’s state of nature. In these games, the state of nature is considered as being the strategy profile where no agent cooperates. A common feature of the \(2\times 2\) games literature is that the state of nature is also a Nash equilibrium. The intellectual challenge of this literature is then to design institutional mechanisms allowing to escape from this depressing fate.Footnote 1
 By definition, the \(2 \times 2\) literature only focuses on duels. Yet, if we consider general games, i.e., with more than two agents, analysis of Hobbes’s state of nature in terms of duel is not completely satisfying, since it is a very specific interpretation of the war of all against all. To the best of our knowledge, there is no definition of the state of nature for general games with an arbitrary number of players. The aim of this paper is twofold. First, we propose a definition of the state of nature in games where the number of agents and/or the number of their strategies can be greater than two (we restrict ourselves, however, to strategic form games with complete information). We define a state of nature as a strategy profile which is such that for any agent, all the other players choose their strategies to minimize his utility. This definition captures Hobbes’s idea that the state of nature is a war of everyone against everyone. We show that this definition corresponds to the strategy profile generally considered as Hobbes’s state of nature in \(2 \times 2\) games. Second, we consider states of nature which are also Nash equilibria. We call them rational. For these states of nature, we show that the utility level of any agent is equal to his maximin payoff. Furthermore, we show that rational states of nature always exist in inessential games.Footnote 2 We also provide an example of a game which is not inessential and whose unique state of nature may or may not be rational. Finally, we report an existence result for state of nature in a class of (not necessarily inessential) symmetric games. The paper unfolds as follows. In the next section, we propose and study a definition of the state of nature for general strategic form games with complete information. Section 3 concentrates on states of nature that are also Nash equilibria. Section 4 considers the existence of a state of nature in a class of (not necessarily inessential) symmetric games. All the proofs are relegated to an appendix.",
83.0,4.0,Theory and Decision,18 July 2017,https://link.springer.com/article/10.1007/s11238-017-9625-9,A triple test for behavioral economics models and public health policy,December 2017,Ryota Nakamura,Marc Suhrcke,Daniel John Zizzo,Male,Male,Male,Male,"Behavioral economics (BE) has been seen as holding great promise in a range of policy applications, including that of improving health outcomes (Frank 2004; Zimmerman 2009; Loewenstein et al. 2007, 2012; Barberis 2013). This promise has been recognized by policy- makers across a range of countries, including France (Ouillier and Sauneron 2010), the United States (Lott 2013) and the United Kingdom (Dolan et al. 2010). It has broadly matched the rise of the behavioral ‘nudge’ agenda: the possibility of obtaining quick wins in terms of policy outcomes by altering the decision environment of the individual in a way that does not forbid any option or change any economic incentive (see Thaler and Sunstein 2008). The original classic example by Thaler and Sunstein concerned the case of a cafeteria where, by changing the placement of healthy and unhealthy food, it would be possible to affect the extent to which agents chose each. The alleged policy advantages, particularly to policy makers in an age of economic recession, were clear: the potential of better health outcomes without restricting the choice set of the rational consumer and, significantly, at little or no cost for the policy maker.Footnote 1
 That said, a disconnection between the excitement of the promise of BE and the evidence base has been noted (Marteau et al. 2011). Early proposers of BE have put this in terms of policy getting ahead of science (Loewenstein et al. 2012), and of hard shoves (in terms of regulation) being needed as much as soft nudges. A recent report of the U.K. House of Lords has reached qualified conclusions on the potential of using only behavioral interventions in affecting outcomes (House of Lords 2011). A recent scoping review of choice architecture interventions has reached the conclusion that the jury is still out on effect sizes for such interventions, both singly and in combination (Hollands et al. 2013). The key question of this paper is the degree to which BE is actually adding to the public health policy debate. For the purpose of this paper, we define BE as comprising economic models that relax the standard assumptions of rationality, pure self-interest or both. We label ‘standard economic models’ as models that keep both of these assumptions. BE mostly encapsulates and incorporates concepts and findings from psychology or cognate disciplines, which are combined with economic modelling to produce potentially new insights hopefully of interest outside economics, including to policy makers (for examples, see Camerer et al. 2004; Skořepa 2011; Cartwright 2011). We propose a triple test for whether a behavioral economic model is relevant for public health policy: 
Test 1: the model has to yield specific predictions relative to standard economic models or established psychological theories in terms of individual behaviors or reaction to incentives; 
Test 2: the model has to provide specific predictions regarding specific public health policies; 
Test 3: the model has to be appropriately validated by empirical evidence. This paper considers example BE models and shows how these tests can be usefully employed.Footnote 2 We consider three areas where one can, with some legitimacy, claim that the first test is passed: social interactions; self-control devices; and prospect theory. We find that, with the partial possible exception of the area of self-control, in all three areas there needs to be further progress on the connection between insights from BE models, policy application and corroboration. We suggest that the proposed triple test can be employed to verify the policy relevance of BE insights. Section 2 provides the conceptual background to our triple test. Sections 3, 4 and 5 consider our three areas of application. The supplementary material provides relevant lists of empirical references and key findings. Section 6 concludes. Identifying the value added of behavioral economics",4
83.0,4.0,Theory and Decision,27 June 2017,https://link.springer.com/article/10.1007/s11238-017-9622-z,Ensemble prospectism,December 2017,Kim Kaivanto,,,,Unknown,Unknown,Mix,,
83.0,4.0,Theory and Decision,05 October 2017,https://link.springer.com/article/10.1007/s11238-017-9635-7,On the consistency of choice,December 2017,Ola Mahmoud,,,Male,Unknown,Unknown,Male,"Consistency of choice is a fundamental and recurring theme in decision theory, social choice theory, behavioral economics, and psychological sciences. The purpose of this paper is to formalize the notion of consistency of choice at both a theoretical and a behavioral level, independent of the particular economic theory or choice model at hand. The realm of definitions of consistency has traditionally been restricted to either an identification with some form of rationality or to an imposition of properties that choice functions and preference relations must satisfy. Such restrictions have been categorized as being rationality or consistency conditions, two terms that are often used synonymously and have become loaded with connotations in choice theoretic writings. Any such conditions, however, strictly relate to properties that a given preference relation or a choice function needs to satisfy for it to be considered consistent, and not to the inherently logical meaning of consistency. What one considers to be consistent in models of choice thus essentially amounts to a subjective model-dependent evaluation of properties of choice functions and preference relations. There is, however, an intrinsically objective and unambiguous view of consistency that is linked to the logical quality of deductive reasoning. The purpose of this paper is hence to provide a concise and objective formalization of the notion of consistency in choice theoretic models. It is argued that such an objective evaluation of consistency of an arbitrary theory of choice reduces to an analysis of the choice system as a whole as opposed to a subjective analysis of properties of preference relations or choice functions. More specifically, for an economic theorist who comes equipped with a choice model, an evaluation of consistency can be categorized as taking one of two forms. On one hand, one hopes that the given theoretical choice system is in itself consistent in the sense that it does not generate contradictory statements. This is referred to as internal consistency. On the other hand, consistency of an abstract choice model is often evaluated with respect to some real world observations. This shall be called semantic consistency. It refers to the idea that a given choice theory’s preferences are valid with respect to some observed choice data. These two pillars of consistency are arguably used implicitly in any evaluation of a choice theoretic model and their formalization is the subject of this paper. The crux of this foundational view of consistency is threefold. First, internal consistency of a choice model is defined in terms of the impossibility of deriving a contradiction from the model. Such a contradiction consists of the logical incompatibility of two or more choices made. To be able to objectively formulate what constitutes contradictory choices, a context-dependent viewpoint of choices and preferences takes central stage throughout, and it is argued that context dependence enables a precise definition of contradictory choice in terms of the negation of context-dependent preference relations. Second, internal consistency is used to define an objective notion of rationality of an economic agent that is linked to his quality of reasoning. More precisely, consistent axiomatizability of a decision maker’s choices may fail, and if his behavior contains a logically contradictory choice, then a choice theory representing his observed behavior must be internally inconsistent. Third, semantic consistency represents an inherent analysis of the relationship between theory and observation, and a substantial part of this paper is dedicated to examining this relationship more closely. In particular, the passage between the universe of axiomatic choice theories and the universe of observed choice structures is formalized via transformational mappings, which are referred to as the duality mappings. They are essential ingredients in the analysis of the semantic consistency of choice and may in fact already be implicitly used, whether one is studying a proposed axiomatic model of choice or trying to model observed data. Beyond being a formalization of this relationship, the duality mappings provide insights into the process of evaluating choice consistency in practice. The remainder of the paper is structured as follows. The foundational view of internal and semantic consistency as opposed to traditional consistency conditions is motivated and discussed in Sect. 2. Section 3 sets up the choice theoretic framework and provides the context-dependent definition, which is a fundamental concept in the development of internal consistency. In Sect. 4, the relationship between theory and observation is analyzed in terms of the second pillar of consistency, namely semantic consistency. The passage between the universe of axiomatic choice theories and the universe of observed choice structures is then formalized via transformational mappings, which are referred to as duality mappings. In Sect. 5, the process of evaluating semantic consistency is examined more closely in terms of the duality mappings, and the relationship between theory and reality of choice is revisited. In particular, the duality mappings are used to show that (1) an observed choice is semantically consistent with a given theory if and only if it is a consequence of the theory’s axioms; (2) a consistent axiomatization, even though based on observed data, implicitly also axiomatizes what is unobserved; (3) the universes of theory and reality are not equivalent; (4) one strand of rationality capturing logical quality of reasoning can be defined in terms of internal consistency. Section 6 concludes by discussing the implications of the proposed framework and how it relates to classical revealed preference theory and other formalizations of the relationship between the theory and reality of choice.",2
83.0,4.0,Theory and Decision,07 June 2017,https://link.springer.com/article/10.1007/s11238-017-9605-0,Multidimensional Pigou–Dalton transfers and social evaluation functions,December 2017,Marcello Basili,Paulo Casaca,Maurizio Franzini,Male,Male,Male,Male,"There has been a resurgence of interest in multidimensional social evaluation functions mainly due to new techniques that extend in the multidimensional setting the pioneering works by Atkinson (1970, 1987), Kolm (1976a, b, 1977) and Sen (1976). In particular, Tsui (1995, 1999) and Gajdos and Weymark (2005) have offered axiomatic approaches to designing income inequality measures in a multiattribute context. Although Tsui mainly used the additive approach, Gajdos and Weymark built upon the generalized Gini social function. These two different approaches are not at all innocuous. The former aggregates the attributes of each individual and then additively aggregates the resulting values; the latter evaluates the different attributes through a specific aggregation and then simply aggregates the values. Note also that although Tsui (1999) approach is a ‘traditional’ additive evaluation, Gajdos and Weymark (2005) adopted a non-additive approach, which was introduced by Weymark in his seminal paper in 1981. In this paper, we follow the traditional additive approach, but instead of imposing the majorization theory of the m-dimensional case as in Tsui (1995), we confine ourselves to accommodating a particular Pigou–Dalton transfer, which we believe is relevant. Furthermore, our approach is consistent with the meaningful property of Correlation Increasing Majorization (e.g., Tsui 1999). In actual fact, we investigate a class of utilitarian social evaluation functions (that consist in aggregating the individual utilities), where the utility function, the same for all the individuals and which depends on several attributes, will prove to be inframodular (a natural multivariate generalization of the notion of concavity). More precisely, the paper aims at characterizing a class of inframodular functions initially proposed in the literature on decision under risk, by Müller and Scarsini (2012). Letting \(x=(x_{1}, \ldots , x_{m})\) be the list of endowments of an individual in each of the m attributes, the paper investigates the class of inframodular utility functions that can be written as \(u(x)=\psi \left( \sum ^{m}_{i=1}\alpha _{i}x_{i}\right) \) where \(\psi \) is increasing and concave, \(\alpha _{i} > 0\) and \(\sum _{i}\alpha _{i}=1\). Notice that such a utility function requires the commensurability of the different dimensions. Additionally, from Theorem 1 in Sect. 3, the fact that u is defined up to a positive affine transformation suggests that the underlying attributes are cardinally measurable. Although the same criticisms can be raised against the standard Human Development Index (HDI) approach, we have the feeling that such limitations allow, however, to treat most of the interesting situations. Moreover, we show that inframodular functions are intimately linked with a natural generalization to the multidimensional case of the classical unidimensional Pigou–Dalton transfers principle, which allows one to consider situations where some of the welfare attributes are not transferable (to be compared with Bosmans et al. 2009). This point will lead us to a key simple “transfers principle” axiom. Likewise, since inframodular functions are submodular, it turns out that our evaluation function will also accommodate the property of Correlation Increasing Majorization. Finally, we specify our social evaluation function to compare it with the famous HDI using actual data. The paper is organized as follows. Section 2 presents the notation and also introduces the paper’s motivation and the axioms. Section 3 offers our main theorem, specifically Theorem 2 which characterizes our social evaluation function. Section 4 aims at reducing the number of parameters, namely specifying some fundamental \(\psi \) function, for our purpose. Thus, we deliver a tractable relative inequality index in the corollary of Theorem 3. Section 5 shows that our evaluation function can accommodate Correlation Increasing Majorization. Section 6 illustrates using Brazilian data in what way the current methodology differs from the popular HDI. Finally, Sect. 7 concludes and some proofs can be found in “Appendix A” and “Appendix B”.",5
83.0,4.0,Theory and Decision,09 October 2017,https://link.springer.com/article/10.1007/s11238-017-9632-x,Asymmetric Choquet random walks and ambiguity aversion or seeking,December 2017,Rossella Agliardi,,,Female,Unknown,Unknown,Female,"In several real-life situations, the decision-maker is confronted with the elusive notion of ambiguity, that is, a form of uncertainty that is not captured by the classical probabilistic methods and is, in some sense, an ’incalculable’ form of uncertainty (Knight 1921). In particular, we refer to situations in which individuals cannot easily attach meaningful probabilities to the relevant outcomes, but they have a subjective uncertainty about probabilities over states (“perceived ambiguity”) and their behavior is determined by their “attitude toward ambiguity”, i.e., on how averse or attracted they are to this induced subjective uncertainty. While the foundations of ambiguity theory have been studied extensively, and an abundant literature presents applications to many static problems in economics and finance, a little progress has been done in developing a dynamic framework to model ambiguity and to study the counterpart of the classical stochastic processes that are commonly used in finance, corporate finance, economics, etc. Some exceptions are Chateauneuf et al. (2001); Chen and Epstein (2002); Nishimura and Ozaki (2007); Kast and Lapied (2010a, b), where a dynamic approach is provided. In Kast and Lapied (2010b) (see also Kast et al. 2014) a dynamically consistent Choquet random walk is constructed that allows to model the ambiguity aversion embedded in several economic and financial problems in a simple way. The construction is performed by starting with symmetric binomial trees—with capacities replacing the exact probabilities on the branches—and letting them converge to Wiener processes. In contrast with the previous frameworks, the resulting ’deformed’ Brownian motion under ambiguity aversion exhibits a lower drift and volatility than the classical ’probabilistic’ case, which makes the applications to finance, corporate finance, economics, etc., not trivial to obtain. In this note, we extend the setting by allowing for a more general random walk and address the issue of the robustness of the main findings when the starting stochastic process is changed. For example, what happens if we start with an asymmetric binomial tree? Does the comparison between the ambiguous and the classical ’probabilistic’ case still preserve a lower drift and volatility in the case of ambiguity aversion? Are the theoretical findings consistent with the experimental evidence pointing to an analysis of higher order risk taking? This paper aims at revisiting the approach taken in Kast and Lapied (2010b), by presenting it in a slightly more general setting. Our asymmetric Choquet capacities framework allows to model those situations where agents have differential expectations towards good and bad prospects, which is often the case in many practical situations. We present some applications to financial markets, although our model is usable in a wider range of problems where ambiguity plays a role. By relaxing the symmetry restriction of Kast and Lapied (2010b) random walks and by merging the basic model in a more general setting, we better highlight the role of its relevant parameter, representing the conditional capacity along the binomial tree, and provide additional flexibility to the approach. Asymmetric binomial random walks are defined in Sect. 3 and some properties are proved, in particular, the effect of ambiguity on the moments of the distribution is studied. Section 4 presents some financial applications.",2
84.0,1.0,Theory and Decision,16 August 2017,https://link.springer.com/article/10.1007/s11238-017-9627-7,Double auctions with no-loss constrained traders,January 2018,Nejat Anbarci,Jaideep Roy,,Male,Unknown,Unknown,Male,"Trade in most important homogeneous-goods markets has been governed primarily by periodic systems in the form of a ‘double auction’ (DA) for more than 150 years. They are used to open many continuous markets such as the New York Stock Exchange (NYSE), the Tokyo Stock Exchange and the Chicago Mercantile Exchange. DAs collect bids and asks/offers from traders, implicitly construct supply and demand curves, announce a market-clearing price, and execute the indicated trades. As such, DA comes closest to operationalising Marshall’s supply–demand diagram among all institutions that mediate trade. These markets are always in flux where investors need to make quick decisions, many of which push them to eventual losses. In finance, in particular, it is well recognised that investors are prone to becoming extra cautious in trying to avoid losses at almost all costs. This can encourage traders to play it safe by never bidding above their private valuations and never asking below their private costs. Hard budget constraints may be another reason for such cautious behaviour as traders may have no means to face any amount of ex-post monetary loss. In this paper, we use the weakest form of such a constraint. In particular, while we allow traders to bid above their valuations or ask below their costs, they are constrained from incurring any loss ex-post. Such traders will be called no-loss-constrained (NLC) traders and will be the focus of this paper. The main issue in the literature on DA is the tension between efficiency, incentive compatibility and balanced-budget. As the famous impossibility result of Myerson and Satterthwaite (1983) suggests, under fairly general conditions, any double auction that is incentive compatible for both the buyers and the sellers must yield a deficit if it wants to achieve ex-post efficiency. It is also well known, for example, that the Vickrey–Clarke–Groves mechanism (Vickrey 1965; Clarke 1971; Groves 1973) achieves efficiency, individual rationality and incentive-compatibility but runs a deficit. A question that becomes central then is: what can we achieve with a balanced budget DA? Chatterjee and Samuelson (1983) proposed a mechanism for bilateral trading where the buyer pays the seller an average of their bids and nothing otherwise. The mechanism is balanced budget and strategyproof but not efficient. McAfee (1992), for instance, devised an ingenious yet simple DA mechanism (MDA hereafter for the McAfee double auction) with single-unit demand and supply that achieves strategyproofness, and is minimally inefficient (in the sense that it leaves out at most the least efficient trading pair). Indeed it also avoids a budget deficit, but fails to have a balanced budget via a uniform price, since MDA may use dual prices, where the difference between prices is channelled as a surplus to the market maker.Footnote 1
 In this delicate context, it is far from clear whether NLC constraints alleviate this problem or not. This is because it is not obvious how incentives to misreport may get affected in the presence of NLC. We find that in a world with NLC traders, there exists a simple variant of MDA that improves the performance of the mechanism: it not only remains strategyproof and equally efficient as MDA but also has a uniform price—via a price lottery—and hence a balanced budget. In a sense, this result highlights the crucial link of NLC to strategyproofness when DAs are concerned. Indeed, MDA fails to be strategyproof if one uses our price lottery to achieve a uniform price in a non-NLC world. As a consequence, since the surplus that would go to the market maker in MDA gets channelled either to the buyers or to the sellers in our scheme (thus benefitting both sides in expected sense without hurting either in ex-post sense), NLC traders would strictly prefer our scheme to MDA.Footnote 2 It is important to note that, instead of assuming NLC, if we assume stricter restrictions where buyers never bid above their valuations or sellers never ask below their costs, then our results continue to hold.Footnote 3 Finally, as we discuss further in the paper, randomness of the price does not necessarily have to be a mechanism-design instrument. Instead, one may think of a situation where the market maker is endowed with a private type that depicts whether she is “pro-buyer” or “pro-seller”. The randomised price can then be determined by the realisation of this type, giving rise to sunspots. The rest of the paper is structured as follows. In Sect. 2, we describe the DA market and in Sect. 3 we define our DA mechanism. In Sect. 4, we state and prove our main result, followed by a discussion. The paper provides its concluding remarks in Sect. 5.",
84.0,1.0,Theory and Decision,06 June 2017,https://link.springer.com/article/10.1007/s11238-017-9607-y,Empirical evaluation of third-generation prospect theory,January 2018,Michael H. Birnbaum,,,Male,Unknown,Unknown,Male,"
Schmidt et al. (2008) proposed third-generation prospect theory (TGPT) as a unified theory to account for judgments of value of risky prospects as well as choices between such prospects. This theory was intended to account for the discrepancy between willingness to pay (WTP) and willingness to accept (WTA) and for preference reversals between choices and judgments of value. This paper shows that third-generation prospect theory implies three properties that are empirically violated by data but which are consistent with an older theory known as configural weight theory. Before presenting the properties and the empirical evidence testing them, it is useful to review the history of the models and their relationships. Original prospect theory (Kahneman and Tversky 1979) made use of a subjectively weighted utility formulation similar to that of Edwards (1954). Cumulative prospect theory (CPT) by Tversky and Kahneman (1992) is a variant of rank- and sign-dependent utility (RSDU), by Luce and Fishburn (1991), with particular functions specified. Schmidt et al. (2008) refer to CPT as “second generation” prospect theory. Schmidt et al. retained CPT for choices between risky prospects, but added new assumptions to account for judgments of value-maximal buying prices (WTP) and minimal selling prices (WTA). Thus, TGPT is an extension of CPT, rather than a revision of it. 
Birnbaum and Stegner (1979) tested a configural weight model that predicted specific relationships between judgments of highest buying and lowest selling prices. Birnbaum and Stegner referred to the empirical effects as effects of the “judge’s point of view”. They fit a configural weight averaging model in which the configural weights of lower or higher values are affected by instructions to identify with the buyer, seller, or an independent. They assumed that buyers would place greater configural weight on lower estimates, attributes, or consequences of an option than would sellers. The data showed strong effects consistent with this interpretation. 
Thaler (1980), who did not cite the earlier configural weight theory or data, proposed the term, “endowment effect” and suggested that such phenomena might relate to “loss aversion,” postulated by Kahneman and Tversky (1979) as a property of the utility function—“losses loom larger than gains”. The phenomenon is sometimes described as a special case of a “status quo bias” (Samuelson and Zeckhauser 1988). Tversky and Kahneman (1991) elaborated the idea that the discrepancy between WTP and WTA for riskless goods might be explained by a utility function in which a loss of x has greater negative utility than that of a gain of comparable absolute value. Loss aversion was also incorporated in CPT (Tversky and Kahneman 1992). To represent risky gambles, Schmidt et al. (2008) proposed TGPT, which used CPT, combined with the assumption that prices paid or accepted are integrated into the consequences of a prospect. In TGPT, buying or selling prices of risky prospects are theorized as decisions among mixed gambles that are affected by loss aversion, even when all consequences are strictly positive. These main ideas of TGPT had already been proposed and evaluated by Birnbaum and Zimmermann (1998, Appendix B, Model 2), along with certain other “loss aversion” theories of the so-called endowment effect. They rejected TGPT model as a descriptive account of judgments of highest buying and lowest selling prices, as well as the theory of Tversky and Kahneman (1991) and an anchoring and adjustment model. In response to unpublished findings by Birnbaum and Yeary (1998) testing implications of configural weighting models, Luce (2000) developed a more elaborate theory in which prices and consequences are integrated via a joint receipt operation; this theory was further developed and evaluated by Birnbaum et al. (2016). The main difference between TGPT and Luce’s (2000) approach is that in Luce’s approach prices are integrated with consequences via a joint receipt operation rather than by simple addition or subtraction as in Birnbaum and Zimmermann (1998) and in Schmidt et al. (2008). However, in Luce’s (2000) approach, like that of Birnbaum and Stegner (1979), the utility of negative consequences (“loss aversion”) plays no role in the theory. The present article presents new analyses of previously published data to evaluate the empirical status of TGPT. These results show that TGPT is not an accurate empirical description of either judgments of value (WTP and WTA) or of choices between prospects. The rest of this paper is organized as follows: Sect. 2 presents the key ideas of TGPT and presents theorems of TGPT (testable properties) that can be evaluated empirically; Sect. 3 presents evidence that these implications of TGPT are violated systematically by empirical findings. Section 4 presents a configural weight model (Birnbaum and Stegner 1979) and shows that it explains these phenomena that refute TGPT and provides a better fit to data. Section 5 discusses the implications of those empirical results and related findings for theories of choice and judgment.",15
84.0,1.0,Theory and Decision,07 September 2017,https://link.springer.com/article/10.1007/s11238-017-9629-5,A second-generation disappointment aversion theory of decision making under risk,January 2018,Pavlo Blavatskyy,,,Male,Unknown,Unknown,Male,"There is a nonempty abstract topological set X. The elements of set X are called outcomes (consequences). For example, an outcome \(x\in X\) can be a monetary amount, a consumption bundle, a financial portfolio, a health state, a marital status, etc. Choice alternatives are lotteries—probability distributions over set X. A (simple) lottery is denoted by \(L:X\rightarrow [ {0,1} ]\), \(\sum _{x\in X} L(x)=1\). A decision maker has a real-valued utility function \(u:X\rightarrow {\mathbb {R}}\) that is unique up to a positive affine transformation (i.e., without the loss of generality, utility function u(.) can be normalized for two arbitrary outcomes). Expected utility of lottery L is defined by Eq. (1). Expected utility of a degenerate lottery that yields one outcome \(x\in X\) for sure (i.e., with probability one) is simply u(x). Expected utility \(\hbox {EU}(L)\) can be interpreted as a reasonable ex ante utility level that a decision maker may expect from the ex post outcome of lottery L. Some decision makers may experience disappointment when the utility of lottery L’s ex post outcome falls short of its ex ante expectation \(\hbox {EU}(L)\). Let \(X_L^-\) denote the set of all such disappointing outcomes, i.e. \(X_L^- \equiv \{ {x\in X|u(x)\le \hbox {EU}( L)}\}\). On the other hand, some decision makers may experience elation when the utility of lottery L’s outcome exceeds its ex ante expectation \(\hbox {EU}(L)\). Let \(X_L^+ \) denote the set of all such elating outcomes, i.e. \(X_L^+ \equiv \{ {x\in X| \mathrm{u(x)}>\mathrm{EU(L)}} \}\). The extent of possible disappointment is measured by expected utility deviation below the expected utility of a lottery. It is defined by Eq. (2). The extent of possible elation is measured by expected utility deviation above the expected utility of a lottery and it is also given by (2). Expected utility deviation of a degenerate lottery is zero. Expected utility deviation of a non-degenerate lottery is strictly positive unless all possible outcomes of this lottery yield the same utility (i.e., this lottery is de facto degenerate on the utility scale). The second part of Eq. (2) shows that expected utility deviation is nothing but one half of the mean absolute deviation of utility of lottery’s outcomes from the expected utility of the lottery. The mean absolute deviation is a well-known statistical measure of dispersion that is sometimes used in finance to capture financial risk (cf. Blavatskyy 2010b). The third part of Eq. (2) shows that expected utility deviation is similar to Gini (1912) mean difference statistic (on the utility scale). For binary lotteries (when sets \(X_L^+ \) and \(X_L^- \) are both singleton), expected utility deviation is nothing but one half of Gini (1912) mean difference statistic (which is also known as the second L-moment, or L-scale, cf. Hosking 1990). For lotteries with more than two outcomes, however, expected utility deviation is less than one half of Gini (1912) mean difference statistic. The latter aggregates utility differences over all possible pairs of outcomes, whereas the former aggregates utility differences only over pairs consisting of one disappointing and one elating outcome. Thus, one half of Gini (1912) mean difference statistic is generally greater than expected utility deviation. Expected utility deviation \(\hbox {EUD}(L)\) can be interpreted as a reasonable ex ante bound on how much a decision maker may expect the utility of lottery L’s ex post outcome to deviate from its ex ante expectation \(\hbox {EU}(L)\). In other words, it may be reasonable for a decision maker to form an ex ante expectation that the utility of L‘s ex post outcome falls within \(\hbox {EU}(L)\pm \hbox {EUD}(L)\). From this point of view, a decision maker may experience euphoria when the ex post L’s outcome yields utility higher than \(\hbox {EU}(L)+\hbox {EUD}(L)\). The extent of such euphoria can be measured by expected utility deviation above \(\hbox {EU}(L)+\hbox {EUD}(L)\). On the other hand, a decision maker may experience misfortune (bitter disappointment) when the ex post lottery L’s outcome yields utility lower than \(\hbox {EU}(L)- \hbox {EUD}(L)\). The extent of such misfortune or bad luck can be measured by expected utility deviation below \(\hbox {EU}(L)-\hbox {EUD}(L)\). For lotteries yielding outcomes that are symmetrically distributed on the utility scale, these two measures of euphoria and misfortune are exactly equal. For lotteries yielding outcomes that are positively (negatively) skewed on the utility scale, the measure of euphoria is greater (smaller) than the measure of misfortune. Thus, the difference between the measures of euphoria and misfortune can be interpreted as expected utility skewness. It is defined in Eq. (3) below. Alternatively, we can think of measure (3) as utility of gambling since it measures “excessive” happiness that a decision maker can experience from betting on the long shots. Expected utility skewness of a degenerate lottery that yields one outcome for sure is zero. In expected utility theory, a decision maker behaves as if he maximizes only expected utility (1) and does not care about expected utility deviation/skewness (von Neumann and Morgenstern 1947). In the mean-variance approach to optimal portfolio investment, pioneered by Markowitz (1952), an investor behaves as if trading off the expected return on an asset vs. the dispersion of asset’s returns. In the financial literature, dispersion is usually measured through variance or standard error, which leads to normatively unappealing violations of the first-order stochastic dominance, cf. Borch (1969). These violations are avoidable if dispersion is measured through the mean absolute (semi-)deviation, cf. Blavatskyy (2010b). Generalizing such a model to allow for a non-linear utility function, which is rarely done in finance, we obtain a decision theory in which a decision maker cares not only about expected utility (1) but also about expected utility deviation (2). In this paper, we go one step further. We assume that a decision maker may care not only about expected utility (1) and expected utility deviation (2) but also about expected utility skewness (3). The simplest model of multiattribute choice is a model that aggregates various attributes into one (weighted) index. In a decision theory proposed in this paper, preferences over lotteries are represented by utility function (4) that is a weighted sum of expected utility (1), expected utility deviation (2) and expected utility skewness (3): where \(\rho \in [-1,1]\) and \(\tau \in [-1,8/9]\) are the two subjective parameters. Expected utility theory is a special case of model (4) when \(\rho =\tau =0\). Parameter \(\rho \) captures a decision maker’s attitude to expected utility deviation. Parameter \(\tau \) captures a decision maker’s attitude to expected utility skewness. We expect a typical decision maker to be averse to expected utility deviation (the sentiment of disappointment to outweigh the sentiment of elation) but attracted to expected utility skewness. Ebert and Wiesen (2011, figure 6, p. 1343), Ebert (2015, Section 5.5.1, p. 94) and Astebro et al. (2015, Section 3.1, p. 198) present experimental evidence of such skewness preferences. Thus, parameter \(\rho \) enters into (4) with a negative sign, whereas parameter \(\tau \) with a positive sign. Ceteris paribus, a decision maker with a positive (negative) parameter \(\rho \), is averse (attracted) to lotteries yielding outcomes that are widely dispersed on the utility scale. Ceteris paribus, a decision maker with a positive (negative) parameter \(\tau \), is attracted (averse) to lotteries yielding outcomes that are positively skewed on the utility scale and averse (attracted) to negatively skewed lotteries. A lottery L with a cumulative distribution function F(.) first-order stochastically dominates lottery \(L^{\prime }\) with a cumulative distribution function G(.) if \(G(x)\ge F(x)\) for all \(x\in X\). A decision maker does not violate first-order stochastic dominance if \(U(L)\ge U(L')\) for all such pairs of lotteries L and \(L^{\prime }\). This imposes a restriction on subjective parameters \(\rho \) and \(\tau \) of utility function (4). Specifically, for \(\tau \in [-1,0]\) parameter \(\rho \) must not exceed \(1+\tau \) in the absolute value, i.e. \(|\rho |\le 1+\tau \); for \(\tau \in [0,1/3]\) parameter \(\rho \) must not exceed one in the absolute value, i.e. \(|\rho |\le 1\); and, finally, for \(\tau \in [1/3,8/9]\) inequality \(({\rho \pm \tau } )^{2}\le 8{\uptau }({1-\tau })\) must hold. Thus, as long as subjective parameters \(\rho \) and \(\tau \) are not too large, preferences represented by utility function (4) do not violate the first-order stochastic dominance.",4
84.0,1.0,Theory and Decision,24 October 2017,https://link.springer.com/article/10.1007/s11238-017-9636-6,Risk attitudes in axiomatic decision theory: a conceptual perspective,January 2018,Jean Baccelli,,,Male,Unknown,Unknown,Male,"This paper focuses on choice under risk. In decision-theoretic terminology, risk refers to when the decision-maker faces options that constitute random prospects on a given set of possible results, and the prospects follow a known probability distribution. This is exemplified in games of chance such as dice, cards, or roulette. Playing such games, the decision-maker wins or loses money randomly (unlike in choice under certainty). However (unlike in choice under uncertainty), her odds follow known probability distributions. They are determined by the particular chance mechanism which she is confronted with, i.e., the number of faces on the dice, the number and kinds of available cards, or the roulette table layout. It is well known that the attitude towards risk is one of the topics considered in this branch of decision theory. In decision-theoretic terminology, risk attitude refers to technical concepts capturing parts of our intuitive psychology regarding the various temperaments that can be exhibited in situations like the ones above. For instance, some love gambling despite the eventuality of going bankrupt, others prefer to play it safe whenever possible, and still others act as if they were altogether insensitive to any such feature of their choice situation. In the present paper, I assess the significance of the technical concepts echoing those intuitive ideas in decision theory. I offer a new conceptual perspective on pre-existing results, which I select, bring together, and interpret. I also articulate some of the open questions which these results lead to. Whatever their domain of interest (be it certainty, risk, or uncertainty), decision theorists are primarily concerned with analyzing decision models. A decision model can be thought of as an algorithm for evaluating options. In the case of risk, examples include computing the expected value of some utility function on the set of possible results, calculating this expectation with respect to some transformation of the known probabilities, or proposing some way of combining both the expectation and the variance of the utility values. More often than not, decision theorists introduce or even discover models directly from such a numerical perspective. However, their specific task is to characterize each numerical form of evaluation by a few basic properties, namely, those displayed by the preferences of a decision-maker to whom the examined model would apply. This requires, if possible, proving a representation theorem showing how the numerical evaluation reflects structural aspects of the underlying preferences. To this extent, decision theory is essentially a development of representational measurement theory. More generally, it is an application of the axiomatic method.Footnote 1 Axiomatic analysis, as I will henceforth summarize the decision theorists’ task, enables decision models to be compared with one another. Rigorously speaking, the numerical forms of evaluation which they are associated with are not directly comparable. However, once they are translated into the common language of preference, one can identify the true differences between them. Thus, for decision theorists, the most significant properties of preference are those on the basis of which decision models can be axiomatically distinguished from one another. In this paper, I apply the axiomatic criterion of significance introduced above to assess the status of the risk attitude concepts in decision theory. I start by recalling their technical definitions in a preliminary section. Then, following a deliberately naïve baseline analysis, I provide evidence showing that these concepts do not play any axiomatic role in the theory of decision-making under risk. At this juncture, I stress that the risk attitude concepts do not seem able to account for the fundamental divide between expected utility and the non-expected utility models. This fact is recognized in the current literature, but it deserves to be better highlighted and detailed. To this end, I provide an illustrative discussion of the Allais paradoxes (Allais 1953), which I enrich in the subsequent sections of my paper. Next, I show that following a second, more thorough analysis, different and less familiar conclusions prevail. Specifically, I show that in at least two respects, which pertain to what I call, respectively, the conditional variation and the strengthening of risk attitudes, axiomatic analysis can rely on risk attitudes to distinguish decision models from one another. In providing this more thorough analysis of risk attitudes, my paper relies on two strands of literature. The first strand of literature analyzes decision models by means of so-called conditional certainty equivalents (see especially Machina 1982; Chew and Epstein 1989; Chew et al. 1993). Unlike the previous contributors to this literature, I show its conceptual importance for assessing the status of risk attitudes in axiomatic decision theory. The second strand of literature aims at algebraically characterizing each of the risk attitudes, when the numerical framework of a given decision model is taken for granted (for a review of such results, see, e.g., Chateauneuf et al. 1997). More systematically than the previous contributors to this literature, I show that such algebraic characterizations lead to a general axiomatic typology of the existing decision models. In particular, in this context, I offer a new discussion of the recent cautious expected utility model (Cerreia-Vioglio et al. 2015), which proves instrumental in establishing the generality of the typology in question. This discussion also provides new evidence in support of the exceptional flexibility of the rank-dependent utility model (first introduced in Quiggin 1982). These two strands of literature lead to the most accurate assessment of the decision-theoretic status of the risk attitudes concepts. Admittedly, this final assessment amounts to more of a qualification than a rejection of the naïve baseline assessment which I am going to sketch first. However, as I will show, it opens many new theoretical perspectives that, to the best of my knowledge, are yet to be explored.",6
84.0,1.0,Theory and Decision,04 December 2017,https://link.springer.com/article/10.1007/s11238-017-9644-6,"On societies choosing social outcomes, and their memberships: internal stability and consistency",January 2018,Gustavo Bergantiños,Jordi Massó,Alejandro Neme,Male,Male,Male,Male,"Classical social choice studies problems where a fixed set of agents have to choose an outcome from a given set of outcomes, and agents have preferences only over this set. However, there are settings where, depending on the chosen outcome, some agents might want to leave the society; and this, in turn, might be perceived by some agents that were initially willing to remain in the society as negative, and now they might also want to leave. For instance, in the case of an excludable and costly public good, agents’ preferences may depend on the level of the public good and on the size of the set of agents consuming (and contributing to finance) it. Also, when membership is voluntary in a double sense, no agent can be forced to belong to the final society and any agent can be part of it, if the agent wishes to be. A prototypical example of this class of problems is a political party, whose membership may depend on the positions that the party takes on issues like the death penalty, abortion or the possibility of allowing a region of a country to become independent. A professor in a department may start looking for a position elsewhere if he considers that the recruitment of the department has not been satisfactory to his standards; and this, in turn, might trigger further exits. To be able to deal with such situations, the classical social choice model has to be modified to include explicitly the possibility that initial members of the society may leave it as the consequence of the chosen outcome and hence, preferences have to be extended to order pairs formed by the final society and the chosen outcome. There is a large literature that has already considered explicitly the dependence of the final society on the choices made by the initial society.Footnote 1 Barberà et al. (2001), Barberà and Perea (2002), and Berga et al. (2004, 2006, 2007) study alternative models in terms of the voting methods used to choose the outcome and the timing under which members reconsider their membership. Jackson and Nicolò (2004) study the provision of excludable public goods when agents care also about the number of other consumers. In this note (as we also do in the companion paper Bergantiños et al. 2017), we look at the general setting without being specific about the two issues. We do that by considering that the set of alternatives are all pairs formed by a subset of the original society N (an element in \(2^{N},\) the subset of agents that will remain in the society) and an outcome in X. Then, we assume that agents’ preferences are defined over the set of alternatives \(2^{N}\times X\) and satisfy two natural requirements. First, each agent has a strict preference between any two alternatives, provided he belongs to at least one of the two corresponding societies. Second, each agent is indifferent between any two alternatives, provided he is not a member of any of the two corresponding societies; namely, agents that do not belong to the final society do not care about neither its composition nor the chosen outcome. We consider rules that operate on this restricted domain of preference profiles by selecting, for each profile, an alternative (a final society and an outcome). In Bergantiños et al. (2017) we characterize the class of strategy-proof, unanimous and outsider independent rules as the family of all serial dictator rules.Footnote 2
 For applications where the profile is common knowledge (and hence, the strategic revelation of agents’ preferences is not an issue), we focus on internally stable and consistent rules.Footnote 3 Internal stability says that nobody can force an agent to remain in the society if the agent does not want to do so. This is a minimal requirement of individual rationality, and it is a desirable property whenever membership is voluntary. A rule is consistent if the following property holds. Apply the rule to a given profile and consider the new problem where the new society is formed by the subset of agents chosen at the original profile. A consistent rule has to choose, at the subprofile of preferences of the agents that remain in the society, the same alternative. Thus, a consistent rule does not have to be reapplied after an alternative has been chosen. We want to emphasize that, in contrast with the standard notion, our consistency property requires to re-apply the rule only to the (non-empty) set of agents that has been selected at the original profile. We think that this is the relevant consistency notion because the new composition of the society is not just a hypothetical circumstance, it is a fact. Internal stability and consistency are desirable if we want to interpret the alternative chosen by the rule as being the final one, in a double sense. Members of the final society want to stay and if the rule would be applied again to the final society it would choose the same final society and the same outcome, so there is no need to do so. We adapt well-known voting methods to our setting with the goal of making them either internally stable or consistent, or both.Footnote 4 We show that two prominent scoring methods, plurality voting and the Borda rule, do not satisfy consistency. However, approval voting not only satisfies internal stability and consistency but it also satisfies efficiency and neutrality. Finally, we show that the Condorcet winner is internally stable, consistent, efficient, neutral and anonymous at those profiles where an alternative beats all other alternatives by majority voting (namely, whenever it is a well-defined rule). The paper is organized as follows. In Sect. 2 we describe the model. Section 3 contains the definitions of the properties of rules that we are interested in. Section 4 contains the analysis of well-known rules from the point of view of their internal stability and consistency properties. Section 5 has two final remarks.",2
84.0,1.0,Theory and Decision,06 December 2017,https://link.springer.com/article/10.1007/s11238-017-9649-1,Another perspective on Borda’s paradox,January 2018,Mostapha Diss,Abdelmonaim Tlidi,,Unknown,Unknown,Unknown,Unknown,,
84.0,1.0,Theory and Decision,22 November 2017,https://link.springer.com/article/10.1007/s11238-017-9641-9,When irrelevant alternatives do matter. The effect of focusing on loan decisions,January 2018,Barna Bakó,Gábor Neszveda,Linda Dezső,Male,Male,Female,Mix,,
84.0,2.0,Theory and Decision,20 February 2018,https://link.springer.com/article/10.1007/s11238-017-9645-5,Special issue in the honor of Daniel McFadden: introduction,March 2018,André de Palma,Nathalie Picard,Moshe Ben-Akiva,Male,Female,Male,Mix,,
84.0,2.0,Theory and Decision,09 January 2018,https://link.springer.com/article/10.1007/s11238-017-9648-2,Mobility decisions within couples,March 2018,Nathalie Picard,Sophie Dantan,André de Palma,Female,Female,Male,Mix,,
84.0,2.0,Theory and Decision,02 January 2018,https://link.springer.com/article/10.1007/s11238-017-9651-7,Revisiting consistency with random utility maximisation: theory and implications for practical work,March 2018,Stephane Hess,Andrew Daly,Richard Batley,Unknown,Male,Male,Male,"Discrete choice models have established themselves as an important tool for the analysis of individual decision making across numerous fields (see Anderson et al. 1992; Train 2009, for comprehensive overviews). The normativeFootnote 1 paradigm of utility maximisation has served as the basis for the vast majority of discrete choice models reported in the literature and, as we shall discuss, there are good reasons why this should be so.Footnote 2 A historical perspective on this is given by McFadden (2000). Nevertheless, applications of positivistFootnote 3 behavioural paradigms that depart from utility maximisation, or for which consistency with utility may be tenuous, have become more numerous and have been shown to represent aspects of behaviour that cannot be straightforwardly explained by utility (e.g. Chorus 2010; Leong and Hensher 2015; Guevara and Fukushi 2016). In this paper we, therefore, attempt to explore the basis on which utility maximisation is adopted, what behavioural phenomena have been detected that seem to be inconsistent with that paradigm and the issues that result from attempting to represent those phenomena in practical choice models. The next section of the paper describes the random utility modelling (RUM) paradigm and how it has been used to model choice. This is followed by a discussion of behavioural ‘anomalies’ and an overview of efforts to accommodate these in choice models, looking both at extensions to RUM as well as the use of other model frameworks in this context. We highlight how some of these alternative structures actually remain close to RUM, while also questioning whether some of the extensions of RUM lead to violations of utility maximisation.",42
84.0,2.0,Theory and Decision,29 June 2017,https://link.springer.com/article/10.1007/s11238-017-9620-1,More on random utility models with bounded ambiguity,March 2018,Charles F. Manski,,,Male,Unknown,Unknown,Male,"Econometric analysis of discrete choice has made considerable use of random utility models (RUMs) to interpret the observed choice behavior (McFadden 1974, 1981). Much empirical research concerns choice problems in which persons act with partial knowledge of the utilities of the feasible actions. Economists have used random expected utility models to analyze such choice problems. A common practice has been to specify fully the expectations that persons hold, in which case choice analysis reduces to inference on preferences alone. Unfortunately, the expectations assumptions made in empirical research often have little foundation, diminishing the credibility of the findings. Consider, for example, the analysis of travel mode choice for the journey between home and work, one of the earliest applications of random utility models and still an important subject of empirical research (e.g., Warner 1962; Domencich and McFadden 1975). The canonical mode-choice model supposes that, each day, a worker chooses between two alternatives, travel by automobile and public transit. The utility of each mode depends on its travel cost and travel time. Empirical researchers have commonly used models of traffic flow on transportation networks to predict the travel times that particular workers would experience by each mode. Researchers have also assumed that these predicted travel times agree with the travel times that workers perceive when they make their mode choices. The accuracy with which researcher-predicted travel times measure travel time expectations is questionable. Transportation network models cannot precisely emulate the circumstances of individual travelers. Moreover, workers typically are uncertain how long the journey will take by each mode. Travel times may vary from day to day due to unforeseen variation in traffic volume and the possibility of accidents. RUMs with incorrectly predicted expectations of travel times or other attributes of alternatives are misspecified. The generic result for discrete choice analysis is inconsistent parameter estimation, the specifics depending on the case. To enhance the credibility of econometric analysis, I have recommended survey measurement of the expectations that decision makers hold Manski (2004). A small but growing body of empirical research proceeds in this manner, measuring the probabilistic expectations of sampled decision makers and combining choice and expectations data to estimate RUMs. See Lochner (2007), Delavande (2008), van der Klaauw and Wolpin (2008), van der Klaauw (2012), Zafar (2011), Wiswall and Zafar (2015) and Giustinelli (2016). What can one do in the absence of expectations data? In this case, one can still study how inference depends on the expectations assumptions imposed. Manski (2010) considered inference when one specifies a set of expectations that decision makers may plausibly hold. I first posed the idea in abstraction and then specialized to binary response with linear utilities, where the analysis is straightforward. I mainly assumed that decision makers possess unique subjective probability distributions on the states of nature and make choices that maximize expected utility. I briefly considered the possibility that persons place only partial probabilistic structure on the states of nature and make choices in some manner that uses the available structure. I referred to the models of choice behavior developed in Manski (2010) as random utility models with bounded ambiguity (RUMBAs). Ambiguity may be observational, in that the researcher does not observe the expectations that decision makers hold. Ambiguity may be behavioral in the sense of Ellsberg (1961); that is, the persons under study many not possess complete probabilistic expectations. The adjective “bounded” refers to the fact that meaningful inference on the population distribution of preferences is possible only if the researcher possesses sufficient a priori knowledge of their expectations. This paper revises and updates the presentation of Manski (2010). Sections 2 and 3 consider observational and behavioral ambiguity, respectively.",2
84.0,2.0,Theory and Decision,12 December 2017,https://link.springer.com/article/10.1007/s11238-017-9647-3,D-efficient or deficient? A robustness analysis of stated choice experimental designs,March 2018,Joan L. Walker,Yanqiao Wang,Moshe Ben-Akiva,Female,Unknown,Male,Mix,,
84.0,2.0,Theory and Decision,17 November 2017,https://link.springer.com/article/10.1007/s11238-017-9638-4,A new mixed MNP model accommodating a variety of dependent non-normal coefficient distributions,March 2018,Chandra R. Bhat,Patrícia S. Lavieri,,,Female,Unknown,Mix,,
84.0,2.0,Theory and Decision,26 October 2017,https://link.springer.com/article/10.1007/s11238-017-9631-y,Modeling purchases of new cars: an analysis of the 2014 French market,March 2018,Anna Fernández-Antolín,Matthieu de Lapparent,Michel Bierlaire,Female,Male,Male,Mix,,
84.0,3.0,Theory and Decision,20 April 2018,https://link.springer.com/article/10.1007/s11238-017-9650-8,Introduction to FUR XVII special issue,May 2018,Ganna Pogrebna,,,Female,Unknown,Unknown,Female,"For over 35 years, the Foundations of Utility and Risk Conference (FUR) serves as a beacon which helps early career as well as established scientists working on research topics in the broad area of utility and risk to navigate through the complex and interdisciplinary landscape of research on individual and group behaviour. Traditionally, the FUR Conference brings together decision theorists, behavioural scientists, economists, psychologists, mathematicians, management scientists, medical and health scientists, philosophers, statisticians, and other researchers. Currently, it is one of the most prestigious conferences in decision theory and behavioural science. Over the course of the FUR history, the conference was hosted in ten countries with over 2060 papers presented in total and six Nobel laureates giving talks at FUR at different points in time. I still remember my first FUR conference which took place in Rome in 2006. That conference not only struck me by its innovative scientific programme, but also impressed me by the common spirit of scholarship among scientists who came from different disciplines and, yet, could constructively talk to one another. I have not missed a single FUR conference since. Of course, back in 2006, I could not even imagine that in 10 years’ time, I would have an opportunity to organize an FUR Conference.Footnote 1
 I was very fortunate to be working alongside Tigran Melkonyan (Warwick Business School), Andrea Isoni (Warwick Business School), Elliot Ludvig (Department of Psychology at the University of Warwick), as well as Alexander Kharlamov (Warwick Manufacturing Group) who formed the core of the local organising team. The Conference was also supported by generous sponsorship from the Warwick Manufacturing Group, the Leverhulme Trust, Economic and Social Research Council (ESRC) Network for Integrated Behavioural Science (NIBS), Economic and Social Research Council (ESRC) Centre for Competitive Advantage in the Global Economy (CAGE) as well as Global Priorities Program in Behavioural Science at the University of Warwick. Apart from the distinguished International Conference Committee of the FUR, the local Advisory Board included such leading behavioural scientists, decision, and game theorists as Nick Chater, Graham Loomes, Zvi Zafra, Daniel Read, Peter J Hammond, and Andrew Oswald. The conference was also supported by a large technical and administrative team as well as volunteers. Thanks to this support, the FUR 2016 conference introduced a number of important innovations such as established a permanent conference domain http://furconference.org; created a mailing list for the FUR community, suggested a permanent FUR logo (see Fig. 1), and established FUR presence on social media (Facebook and Twitter). The conference was also streamed online for audiences who wanted to connect and listen to plenary talks remotely via the Internet.Footnote 2
 Suggested permanent FUR logo",
84.0,3.0,Theory and Decision,06 July 2017,https://link.springer.com/article/10.1007/s11238-017-9623-y,The role of information search and its influence on risk preferences,May 2018,Orestis Kopsacheilis,,,Unknown,Unknown,Unknown,Unknown,,
84.0,3.0,Theory and Decision,12 July 2017,https://link.springer.com/article/10.1007/s11238-017-9613-0,Lottery- and survey-based risk attitudes linked through a multichoice elicitation task,May 2018,Giuseppe Attanasi,Nikolaos Georgantzís,Daria Vigani,Male,Male,Female,Mix,,
84.0,3.0,Theory and Decision,20 June 2017,https://link.springer.com/article/10.1007/s11238-017-9611-2,Ambiguity aversion under maximum-likelihood updating,May 2018,Daniel Heyen,,,Male,Unknown,Unknown,Male,"The best-known and dominant approach for decision-making under uncertainty is subjective expected utility (SEU, Savage 1954), relying on (the formation of) a unique probability distribution. Based on descriptive (Ellsberg 1961; Camerer and Weber 1992) and normative considerations (Gilboa et al. 2008; Gilboa 2009), there have been attempts to find alternative decision rules reflecting sensitivity to ambiguity, i.e., decision set-ups in which probabilities are not known. Typical approaches are based on non-additive probabilities, also known as “capacities” (Schmeidler 1989; Eichberger and Kelsey 1999; Chateauneuf et al. 2007), and multiple priors (Gilboa and Schmeidler 1989; Ghirardato et al. 2004; Klibanoff et al. 2005; Maccheroni et al. 2006). Dynamic extensions of these static ambiguity sensitive preferences are needed, since “almost all potential applications of interest in economics involve some dynamic element. Furthermore, static expected-utility theory comes equipped with a natural, essentially “built-in” theory of updating and dynamic choice; it is quite natural to ask whether existing theories of ambiguity also allow a similarly convenient and effective analysis of dynamic behaviour” (Siniscalchi 2009). The natural updating theory of SEU preferences Siniscalchi (2009) refers to is Savage’s axiom P2, best-known as the “sure thing principle”. This axiom is the basis of Bayesian updating and the obvious way to define conditional preferences given an event E. Models with ambiguity sensitive preferences, however, drop P2, thus precluding this canonical extension to dynamic environments. Two competing approaches for updating ambiguity sensitive preferences have been suggested (cf. Machina and Siniscalchi 2014). The first is full Bayesian updating with a simple interpretation for multiple prior models: all priors in the relevant set are updated prior-by-prior according to Bayes rule. Full Bayesian updating, going back to contributions by Fagin and Halpern (1991) and Jaffray (1992), was axiomatized for general capacities by Eichberger et al. (2007) and for maxmin preferences by Pires (2002) and Epstein and Schneider (2003). The second updating approach corresponds to the Dempster–Shafer rule for capacities (Dempster 1967; Shafer 1976) and takes, for multiple prior models, the form of maximum-likelihood updating (MLU) (Gilboa and Schmeidler 1993): Bayesian updating is applied only to those priors with maximal likelihood given the observed event. This paper contributes to the debate about these update rules. Its main contribution is to design and analyse a simple example to demonstrate that MLU suffers from unintuitive characteristics. The example revolves around two urns with unknown composition. The composition in the first urn is determined via a fair mechanism like a coin toss (“risk”), while the decision-maker has no information about the mechanism that determined the second urn’s composition (“ambiguity”). In this standard set-up, an ambiguity averse decision-maker lacking experience with both urns typically (1) prefers bets on the risky over the ambiguous urn and (2) is less willing to bet on the ambiguous urn than a subjective expected utility decision-maker. The problematic feature of this example reveals is that, upon observing a draw from either urn, MLU can reverse both (1) and (2) despite the fact that the information provided by the draws was symmetric across urns and agents. It is well known that intertemporal ambiguity sensitive preferences tend to be in tension with standard rationality requirements. For ambiguity sensitive preferences, it is not possible to maintain consequentialism (preferences conditional on event E do not depend on the unrealized part of the decision-tree \(E^C\)), dynamic consistency (no reversals in preferences once event E actually happened), and full generality in the representation of ambiguity attitudes at the same time (Ghirardato 2002; Al-Najjar and Weinstein 2009; Siniscalchi 2009, 2011; Dominiak et al. 2012). Accordingly, some dynamic axiomatizations of ambiguity sensitive preferences give up consequentialism (Machina 1989; Hanany and Klibanoff 2007; Eichberger and Kelsey 1996), others dynamic consistency (Pires 2002; Eichberger et al. 2007; Siniscalchi 2011), or determine the conditions under which non-SEU preferences fulfil both dynamic consistency and consequentialism (Sarin and Wakker 1998; Epstein and Schneider 2003; Eichberger et al. 2005). It is crucial that the switch in betting preferences identified in this paper is rooted neither in dynamic inconsistency nor a violation of consequentialism. To clarify this, the paper adopts the framework of Epstein and Schneider (2007) which respects dynamic consistency as well as consequentialism.Footnote 1 The other reason to follow Epstein and Schneider (2007) is their explicit use of MLU. Concrete, they adopt the generalized and less extreme MLU, already suggested by Gilboa and Schmeidler (1993), in which also priors that only “epsilon maximise the likelihood function” are updated. This paper demonstrates that MLU, both in the strict and the generalized form, gives rise to the switch in betting preferences surrounding risky and ambiguous urns. The deeper reason is that MLU does not respect set inclusion stability over the course of the updating process. The paper proceeds as follows. Section 2 presents the simple example in which ex-ante and ex-post betting preferences are surprisingly unaligned. Section 3 presents the underlying framework of learning under ambiguity, a simplified version of Epstein and Schneider (2007). Thus equipped, Sect. 4 will revisit the example to understand the deeper reason for the switch in betting preferences. Section 5 concludes.",
84.0,3.0,Theory and Decision,21 June 2017,https://link.springer.com/article/10.1007/s11238-017-9618-8,Strategic ambiguity and decision-making: an experimental study,May 2018,David Kelsey,Sara le Roux,,Male,Female,Unknown,Mix,,
84.0,3.0,Theory and Decision,20 June 2017,https://link.springer.com/article/10.1007/s11238-017-9610-3,The Impact of Health-Related Emotions on Belief Formation and Behavior,May 2018,Elyès Jouini,Clotilde Napp,,Unknown,Female,Unknown,Female,"Health-related issues and rationality do not appear to be the best of friends. There are many well-known and documented areas where health beliefs and behaviors clearly do not follow a rational pattern. Let us mention a few of them (Table 1). Perceived (subjective) health risk differs from real (objective) health risk. It is a well-documented phenomenon that individuals faced with an identical health risk will perceive that risk in distinctly different ways. There are systematic biases in the way individuals perceive their own health risks, and those biases vary from individual to individual and depending on the illness. This misperception of objective probabilities seems irrational since it cannot lead to the best decision-making. Some individuals underestimate their health risk, while others overestimate that risk (see, e.g., Katapodi et al.Footnote 1 2004, 2009a, b, on breast cancer risk). In the extreme, hypochondriacs believe that they suffer (or will suffer) from a disease for which there is no objective risk.Footnote 2 If the benefits from under-estimating one’s susceptibility to a given disease are obvious (reduced anxiety for example), one would be hard-pressed to find the rational benefits from over-estimating one’s health risk. Testing rates are usually “too low” even when there is no cost involved and the medical benefits of screening are known and clearly defined. Empirical evidence for “irrationally” low testing rates has been documented in a variety of medical contexts (Lerman et al. 1996a, b
Footnote 3, Thornton 2008). Testing, screening, or preventive health care behavior in general does not necessarily increase with objective risk. In the case of breast self-examination, for example, heightened susceptibility has been found to be a disincentive to preventive care (Kash et al. 1992). In general, higher disease susceptibility does not necessarily generate higher prevention and those who are rationally the most “in need” of medical services are not those who use them the most (Lerman et al. 2002; Picone et al. 2004). Subjective risk increases with objective risk, testing increases with subjective risk, but testing does not necessarily increase with objective risk. These three phenomena have been abundantly documented (see, e.g., PHAROS 2006; McCaul et al. 1996; Katapodi et al. 2004, Carman-Kooreman 2014), although they seem incompatible. For mammograms or genetic screening for cancer, epidemiological probabilities have no effect on take up rates, although perceived risks of cancer increase with objective risk and do in fact have an effect on take up rates (Lerman et al. 2002). Some people overuse health services while others underuse them. Some people visit their doctor every time they have a runny nose, and use a full array of medical services.Footnote 4 On the contrary, others are true “doctor avoiders”: they do not visit the doctor when symptoms emerge, do not carry out the standard screenings nor go for yearly check-ups (see, e.g., Richard et al.Footnote 5 2000, Betti et al. 2003; Caplan 1995; Meechan et al. 2002) even though the objective medical benefit of prevention is known to them. Individuals differ not only in their preferences with respect to the timing of uncertainty resolution (for the same objective risk, some individuals choose testing, some do not) but also in their anxiety responses to information (for some individuals, the “want to knowers”, information reduces anxiety while for others, the “avoiders”, it raises anxiety).Footnote 6
 Confirmatory testing: some individuals who are certain to be ill test in order to prove what they already know, i.e., that they are sick. Oster et al. (2013) observe (using PHAROS data 2006) that testing explicitly for confirmation is fairly common. Not only do these health behaviors seem irrational at the individual level, they also have serious consequences for the public health system. Indeed, two major burdens for public health systems are (a) underutilization by a certain category of people and (b) overutilization by another category of people. This has created a costly conundrum: patients who should seek medical care do not while the system is utilized heavily by those who are not in need of medical care. We propose a model of health beliefs and behaviors that explains these biases and “anomalies”. We build on Lerman et al. (1996b)’s suggestion that “emotional reaction to bad news” is most likely at the origin of the low screening rates. Health risk, or more precisely the risk of bad news (illness), generates emotions ex-ante, in the form of anxiety or worry (that increases with the subjective perception of disease susceptibility), but also ex-post, in the form of disappointment, if the negative event (i.e., illness) occurs. We explicitly introduce these emotions in our model, in addition to the standard (physical) health consumption utility. Health beliefs and behaviors result from the optimal management of emotions, i.e., realize the best trade-off between anxiety and anticipated disappointment. In fact, in our framework, health risk generates an additional emotional risk in the future and the choice of the optimal perceived risk level can be seen as the choice of the optimal self-insurance level against emotional risk: it represents how much the individual is willing to sacrifice in terms of peace of mind at date 0 to reduce his future vulnerability to disappointment from bad news. In our model, anxiety consists in an immediate consumption of part of the future possible negative event, mitigating its future emotional impact, which is fully in line with the definition of anxiety in Barlow (2000) as “a future-oriented mood state in which one is ready or prepared to attempt to cope with upcoming negative events.” Optimists, or individuals who underestimate their risk of being ill, choose less immediate anxiety but more vulnerability to future bad news, whereas pessimists, or individuals who overestimate their risk of being ill choose increased immediate anxiety but more psychological preparedness for future bad news. The same trade-off is involved when considering preventive health behavior. Testing (or immediate resolution of uncertainty) makes it possible to avoid future bad news and associated disappointment although living with the bad news can be costly in terms of anxiety. On the contrary, not testing (or delayed resolution of uncertainty) makes it possible to keep one’s illusions but increases exposure to future bad news. Empirical studies show that subjective risk and emotional factors are key determinants of participants’ decisions to test or not to test (Lerman et al. 2002; Brewer et al. 2004; Helmes 2002; Dent et al. 1983). Emotional factors and endogenously determined subjective risk are at the heart of our model. In terms of emotions, our model supposes that individuals are disappointed in the case where they receive bad news (illness) at date 1, and that individuals experience anxiety or anticipatory feelings from their perceived risk of being ill. Concerning subjective risk, our model supposes that individuals can proactively manage their expectations ex-ante (their perceived susceptibility to the disease) to avoid negative feelings of disappointment in case of a bad outcome. In Sect. 2, we present the model and show that these assumptions are all supported by considerable evidence. Section 3 presents the results. Section 4 is devoted to extensions, numerical results and related literature. In particular, we provide numerical simulations and demonstrate that our model matches both data on perceived health risk and data on preventive healthcare behavior. Section 5 concludes. All proofs are provided in the Appendix.",
84.0,3.0,Theory and Decision,01 July 2017,https://link.springer.com/article/10.1007/s11238-017-9621-0,The curse of hope,May 2018,Fabrice Le Lec,Serge Macé,,Male,Male,Unknown,Male,"Consider an individual who has just suffered a major financial setback or a serious health shock like a sudden handicap or the beginning of a long-term disease. Under normal circumstances, she should value any small positive probability p of getting her money back or of healing. However, because a positive probability of an improvement also increases expectations and the magnitude of the disappointment if they are not met, she may prefer the certainty of the loss rather than a small probability of a return to the initial situation. The model developed by Kőszegi and Rabin (2006, 2007) allows for this possibility. In this model, an individual facing the prospect of an uncertain future updates her reference point to her (rational) expectations: her new stochastic reference point mimics the future lottery. She then evaluates this future prospect according to its expected utility, with the utility of each outcome being the weighted average of how it feels, relative to each possible realization of this stochastic reference point. The model is then able to capture the effect of a higher probability of obtaining a good outcome on the expected utility of the individual, while taking into account the influence it has on her expectations. Kőszegi and Rabin (hereafter, KR) show, under some assumption of linearity, that an individual may prefer the worst outcome of a given lottery for sure, compared to the lottery itself. This implies a violation of first-order stochastic dominance, underlined by KR (Proposition 7) and more recently by Masatlioglu and Raymond (2016). Hence, an individual may consider that she will be better off when she is certain that her situation will not improve, than when some hope exists that she will end up in a better situation. We call this effect the ‘curse of hope’. In this paper, we show that in a more general setting, with concave intrinsic utility and diminishing sensitivity in the distance to the reference point, this curse of hope is likely to occur because the marginal effect of the intrinsic term is plausibly small in many circumstances. This leads to the reference-dependent component being dominant. The effect corresponds to a strong violation of stochastic dominance, and one for which empirical evidence is scarce.Footnote 1 Furthermore, intuition suggests that a lab experiment is not needed to conclude that virtually nobody will invoke the fear of being disappointed to turn down a 1% chance of a substantial monetary gain in the future. In other domains, in particular for health, there may be some indirect evidence that such an effect plays a role (Macé 2016). For instance, it has been observed that some cancer patients, at an advanced stage of their illness, deliberately refuse a chemotherapy treatment even if it gives them a small chance of remission. The main explanation lies in the constraints and the side effects of the treatment that are weighed against the low probability of healing and the weak quality of additional life years. But, there may also be a downward adjustment of expectations, some acceptance of the situation and the will to live more peacefully during the remaining years or months of their lives (Sharf et al. 2005). It is possible to make a parallel here with the period of relief and apparent improvement which can sometimes be observed in depressive individuals once they have decided to commit suicide (Eastridge et al. 2012). Except for these particular and indirect cases, there is little evidence of a strong violation of stochastic dominance of the kind implied by KR’s model. This raises some questions, given that the model has become a reference for referent-dependent preferences, and that there is growing evidence that expectations influence reference points (Abeler et al. 2011; Crawford and Meng 2011; Card and Dahl 2011; Gill and Prowse 2012; Bartling et al. 2015; Ericson and Fuster 2011).Footnote 2 Our objective here is, therefore, twofold: (i) to investigate the general conditions under which this phenomenon and other related effects occur, and (ii) to suggest explanations for the fact that so few individuals reject a small probability of obtaining a better outcome. On the first point, using essentially binary lotteries, we show that the curse of hope is predicted to occur under very plausible specifications and in relatively frequent settings. In addition, it implies that an increase in the probability of the good outcome may reinforce the individual’s motivation to choose the less attractive outcome for sure. On the second point, we amend the model by relaxing the simplifying assumption that the weights of the possible outcomes in the multiple reference point correspond exactly to their probabilities. This may relate to psychological phenomena such as defensive pessimism (Norem 2001) and the projection bias (Loewenstein et al. 2003). Doing so, we obtain that the curse of hope can be mitigated, but also that a new effect occurs, the “blessing of fear”: in this case, an individual may prefer a little uncertainty over future outcomes rather than the certainty of the best outcome. Eventually, we consider the case when individuals update their reference point, but with some inertia. The idea is that at the time of the decision-making, the individual will use a stochastic reference point composed of the future lottery and her current situation. If the weight put on her current situation (inertia) is non-negligible as suggested by the vast literature on the status quo bias and some recent experimental investigations on the definition of the reference point (Baillon et al. 2017), then the set of situations where the curse of hope can occur shrinks, and if inertia is large enough, the effect disappears completely. The remainder of the paper is organized as follows. In the second section we present the general situations in which the curse of hope may occur. Section 3 analyses the effect of a subjective transformation of the probability of outcomes on the stochastic reference point. In Sect. 4, we introduce the idea that agents only partly incorporate expected outcomes in their stochastic reference point, at the moment of making the decision. Section 5 concludes. All proofs are set out in the Appendix.",
84.0,3.0,Theory and Decision,20 June 2017,https://link.springer.com/article/10.1007/s11238-017-9612-1,Confidence biases and learning among intuitive Bayesians,May 2018,Louis Lévy-Garboua,Muniza Askari,Marco Gazel,Male,Unknown,Male,Male,"In many circumstances, people appear to be “overconfident” in their own abilities and good fortune. This may occur when they compare themselves with others, massively finding themselves “better-than-average” in familiar domains (e.g., Svenson 1981; Kruger 1999), when they overestimate their own absolute ability to perform a task (e.g., Lichtenstein and Fischhoff 1977; Lichtenstein et al. 1982), or when they overestimate the precision of their estimates and forecasts (e.g., Oskamp 1965). Moore and Healy (2008) designate these three forms of overconfidence respectively as overplacement, overestimation, and overprecision. We shall here be concerned with how people overestimate, or sometimes underestimate, their own absolute ability to perform a task in isolation. Remarkably, however, our explanation of the estimation bias predicts the overprecision phenomenon as well. The estimation bias refers to the discrepancy between ex post objective performance (measured by frequency of success in a task) with ex ante subjectively held confidence (Lichtenstein et al. 1982). It has first been interpreted as a cognitive bias caused by the difficulty of the task (e.g., Griffin and Tversky 1992). It is the so called “hard–easy effect” (Lichtenstein and Fischhoff 1977): people underestimate their ability to perform an easy task and overestimate their ability to perform a difficult task. However, a recent literature has challenged this interpretation by seeking to explain the apparent over/underconfidence by the rational-Bayesian calculus of individuals discovering their own ability through experience and learning (Moore and Healy 2008; Grieco and Hogarth 2009; Benoît and Dubra 2011; Van den Steen 2011). While the cognitive bias view describes self-confidence as a stable trait, the Bayesian learning perspective points at the experiences leading to over- or under-confidence. The primary goal of this paper is to propose a parsimonious integration of the cognitive bias and the learning approach. We design a real-effort experiment which enables us to test the respective strengths of estimation biases and learning. People enter a game in which the task becomes increasingly difficult—i.e. risky—over time. By comparing, for three levels of difficulty, the subjective probability of success (confidence) with the objective frequency at three moments before and during the task, we examine the speed of learning one’s ability for this task and the persistence of overconfidence with experience. We conjecture that subjects will be first underconfident when the task is easy and become overconfident when the task is getting difficult. However, “difficulty” is a relative notion and a task that a low-ability individual finds difficult may look easy to a high-ability person. Thus, we should observe that overconfidence declines with ability and rises with difficulty. The question raised here is the following: if people have initially an imperfect knowledge of their ability and miscalibrate their estimates, will their rising overconfidence as the task becomes increasingly difficult be offset by learning, and will they learn their true ability fast enough to stop the game before it is too late? The popular game “double or quits” fits the previous description and will thus inspire the following experiment. A modern version of this game is the world-famous TV show “who wants to be a millionaire”. In the games of “double or quits” and “who wants to be a millionaire”, players are first given a number of easy questions to answer so that most of them win a small prize. At this point, they have an option to quit with their prize or double by pursuing the game and answering a few more questions of increasing difficulty. The same sort of double or quits decision may be repeated several times to allow enormous gains in case of repeated success. However, if the player fails to answer one question, she must step out of the game with a consolation prize of lower value than the prize that she had previously declined. Our experimental data reproduces the double or quits game. We observe that subjects are under-confident in front of a novel but easy task, whereas they feel overconfident and willing to engage in tasks of increasing difficulty to the point of failing. We propose a new model of “intuitive Bayesian learning” to interpret the data and draw new testable implications. Our model builds on ideas put forward by Erev et al. (1994) and Moore and Healy (2008). It is Bayesian like Moore and Healy (2008), while viewing confidence as a subjective probability of success, like Erev et al. (1994). However, it introduces intuitive rationality to overcome a limitation of the rational-Bayesian framework which is to describe how rational people learn from experience without being able to predict the formation of confidence biases before completion of a task. This is not an innocuous limitation because it means, among other things, that the rational-Bayesian theory is inconsistent with the systematic probability distortions observed in decisions under risk or uncertainty since the advent of prospect theory (Kahneman and Tversky 1979). Therefore, we need to go deeper into the cognitive process of decision. Subjects in our view derive their beliefs exclusively from their prior and the informative signals that they receive. However, “intuitive Bayesians” decide on the basis of the sensory evidence that they perceive sequentially. If they feel uncertain of their prior belief, they will perceive the objection to it triggered by their doubt and wish to “test” its strength before making their decision, like those decision makers weighting the pros and cons of an option. The perceived objection to a rational prior acts like a contrarian illusory signal that causes probability distortions in opposition to the prior and this is a cognitive mechanism that does not require completion of the task. As they gain experience, they keep on applying Bayes rule to update their prior belief both by cues on their current performance and by the prior-dependent contrarian signal. Thus, with the single assumption of intuitive rationality, we can account for all the cognitive biases described on our data within the Bayesian paradigm and integrate the cognitive bias and the learning approach. With this model, and in contrast with Gervais and Odean (2001), we don’t need to assume a self-attribution bias (Langer and Roth 1975; Miller and Ross 1975) combined with Bayesian learning to produce overconfidence.Footnote 1 Signals of future success and failure are treated symmetrically.Footnote 2 Finally, unlike models of confidence management (e.g. Brunnermeier and Parker 2005; Köszegi 2006; Mobius et al. 2014), we don’t have to postulate that individuals manipulate their beliefs and derive direct utility from optimistic beliefs about themselves. Section 2 lays down the structure of the experiment and incentives, and provides the basic descriptive statistics. Our large data set allows a thorough description of confidence biases and a dynamic view of their evolution with experience of the task. Section 3 describes the confidence biases and learning shown by our data. Four basic facts about confidence are reported from our data: (i) limited discrimination among different tasks; (ii) miscalibration of subjective probabilities of success elicited by the “hard–easy effect”; (iii) differential, ability-dependent, calibration biases known as the Dunning–Kruger (or ability) effect (Kruger and Dunning 1999); and (iv) local, but not global, learning. Section 4 proposes a new theory of over (under)-confidence among intuitive Bayesians which integrates doubt and learning and can predict biases, before as well as during the task, in repeated as well as in single trials. Doubt-driven miscalibration appears to be a sufficient explanation, not only for the hard–easy effect and the ‘ability’ or Dunning–Kruger effect, but also for limited discrimination and for the overprecision phenomenon. The theory is further used in Sect. 5 to predict the evolution of confidence over experience on our data set. For instance, low-ability subjects first lose confidence when they discover their low performance during the first and easiest level; but they eventually regain their initial confidence in own ability to perform more difficult tasks in the future after laborious but successful completion of the first level. Intuitive Bayesians exhibit conservatism, that is, under-reaction to received information, and slow learning. Finally, we show in Sect. 5.3 that the cues upon which subjects construct their own estimate of success, i.e. confidence, widely differ from the genuine predictors of success, which further explains the planning fallacy.Footnote 3 The conclusion follows in Sect. 6.",
84.0,3.0,Theory and Decision,22 June 2017,https://link.springer.com/article/10.1007/s11238-017-9614-z,Belief formation in a signaling game without common prior: an experiment,May 2018,Alex Possajennikov,,,Male,Unknown,Unknown,Male,"When making a decision in a situation involving uncertainty, individuals may form beliefs about the probabilities of various outcomes of uncertain events. Within game theory, situations involving uncertainty (about elements other than the actions of other players) are represented by games with incomplete information. The Harsanyi (1967) approach to such games postulates that players’ beliefs about the events describing their information are derived from a commonly known probability distribution. This approach proved fruitful by reducing the complexity of the situation. However, in many realistic situations the players do not necessarily know, or do not have a common belief about the probability distribution governing the uncertainty. If this distribution is not known to the players, how do they form (and update) beliefs about it? In a strategic situation, other players’ behavior is also uncertain (at least initially) from the point of view of a player. Effects of such uncertainty (as opposed to risk arising from a known probability distribution) on behavior and beliefs of players in one-shot strategic interactions have been investigated in experimental settings in Eichberger et al. (2008), Ivanov (2011), Kelsey and le Roux (2015) and Li et al. (2017). The findings from these works appear mixed: attitudes towards ambiguity of others’ behavior vary across players and settings, making predictions about its effects difficult to generalize. This paper contributes to the literature by exploring the entire process of belief formation and updating in a strategic situation with incomplete information. It reports on an experiment in which individuals play a signaling game. One player, the Sender, has a piece of private information (type) and can send a message to the other player, the Receiver. The Receiver sees the message but not the type of the Sender and takes an action. The payoffs of both players depend on the Sender’s type, the message and the action. To take an appropriate action, the Receiver needs to form beliefs about the Sender’s type based on the message the Sender sends. The Receiver can get an idea about the appropriate action by inferring something about the Sender’s type from the message sent. This inference may not be straightforward and the Receiver’s prior beliefs about the distribution of types are important to form beliefs about the type based on the message received. Prior beliefs about types can be explicitly induced by specifying the probabilities of the possible types of the Sender. Without explicitly induced prior beliefs, if the game is played often enough, players can learn from observations.Footnote 1 Drouvelis et al. (2012) (henceforth DMP) investigated how behavior in the signaling game can be different depending on whether the probabilities of the Sender’s types are known or not known before a series of interactions starts. The reason for the possible difference is that without explicitly induced common prior beliefs, players can use different beliefs and thus initially employ different strategies. Path dependence can then lead to different medium to long-run outcomes, even if learning from observations allows to approximate the actual probabilities.Footnote 2
 In this paper, it is further investigated how beliefs are initially formed and updated in such situations. This is important because a model of behavior in a game with uncertainty cannot be complete without specifying beliefs and their updating. Indeed, predictions about behavior in DMP were derived based on a belief updating process [with starting point based on level-1 behavior in the level-k theory, originated in Stahl and Wilson (1994, 1995), and first applied to signaling games, albeit only for beliefs about strategies, in Brandts and Holt (1996)]. However, the question of whether beliefs are really updated in the way the model suggests could not be answered without observing them more directly. In the experiment reported in this paper, subjects made choices in a signaling game, as well as reported their beliefs at regular intervals. Belief elicitation was incentivized with the quadratic scoring rule. This rule has been used for belief elicitation in game experiments with a small number of actions by, among others, Nyarko and Schotter (2002), Costa-Gomes and Weizsäcker (2008) and Hyndman et al. (2011).Footnote 3 Rutström and Wilcox (2009), Blanco et al. (2010) and Armantier and Treich (2013) discuss the methodological issues of the possible interaction between belief elicitation and actual play. Whether belief elicitation affected play is tested in this paper (to a large extent, it does not appear so). In the experiment, Receivers reported beliefs about the prior probability of the type of the Sender in their current (random) match, and after receiving a message, about the posterior probability of the type. Senders, after sending a message, reported beliefs about the matched Receiver’s action in response to this message. The Sender’s type is determined exogenously by a random device, thus its prior probability represents “objective” uncertainty. Which messages are sent by which types (and thus the posterior probabilities of the types), and which actions are taken in response to messages, on the other hand, is determined endogenously within the game. This strategic uncertainty is “subjective” and may depend on the models the players use to determine the behavior of the opponent. Drawing from psychological research, Nickerson (2004, Ch. 8) argues that beliefs about “objective” uncertainty take more time to be revised than beliefs about an individual’s performance. Since in the experiment both types of beliefs are observed, it is possible to see whether some beliefs are updated faster than others in an interactive setting. Without more explicit information about the resolution of uncertainty, the “principle of insufficient reason” (e.g., Sinn 1980, and references therein) states that if there is no reason to believe that one event is more likely than another, then they should be assigned equal probability. In the signaling game context, the principle is more applicable to beliefs about types. Beliefs about strategies can also be subject to this principle; in the level-k theory this is the starting point, representing level-1 belief that the behavior of the opponent is level-0 (uniform distribution). However, further levels of reasoning can also be used to determine which strategy is more likely to be played by the opponent, even without experience. Comparing initial beliefs about prior and posterior distributions of Senders’ types, or about Receivers’ strategies, one can tell whether there is a difference in the formation of beliefs about different types of uncertainty. Thus, the main research questions of this paper are how beliefs are formed (whether initial beliefs, both about types and about strategies, are close to being uniform), how beliefs are updated, and whether some beliefs are updated faster than others. The data suggest that beliefs about Senders’ types, both prior and posterior, indeed start close to being uniform; even beliefs about Receivers’ strategies are not far from the uniform distribution. As observations accumulate, beliefs are updated in the natural direction of the frequency of events. However, updating is not as fast as simple frequency count would suggest, indicating that initial beliefs may have a sizeable weight in the updating process. Beliefs about the posterior distribution of types appear to be updated faster than about their prior distribution; Receivers’ learning of Senders’ strategies thus has an effect on how fast beliefs are updated. Senders’ beliefs about Receivers’ strategies also appear to be updated faster than Receivers’ prior belief about Senders’ types. Given these properties of belief updating, the observed play in the game exhibits differences between the situations with known probabilities of Senders’ types and unknown ones, due to path dependence in one of the treatments. This happens because starting from the uniform initial beliefs the play is taken to a different equilibrium than starting from known correct probabilities of Senders’ types, if initial beliefs about types are not updated too fast. In the other treatments, in the long run there is no noticeable difference in behavior between the cases of known probabilities of types and of unknown ones. Therefore, the uncovered process of belief formation and updating has sometimes important consequences for long-run outcomes.",1
84.0,4.0,Theory and Decision,23 October 2017,https://link.springer.com/article/10.1007/s11238-017-9634-8,This or that? Sequential rationalization of indecisive choice behavior,June 2018,Jesper Armouti-Hansen,Christopher Kops,,Male,Male,Unknown,Male,"It is Friday and Herbert has a date for the night. He ponders the question of where to go for dinner. Knowing thyself, he wants to avoid a lengthy and detailed pairwise comparison of all dishes served across the city. Rather, he decides to directly compare entire categories of dinner options (pasta, tacos, tapas) with respect to their cuisine type (Italian, Mexican, Spanish). His experience tells him that he appreciates the Spanish cuisine more than the Italian or the Mexican one, because it is by far his favorite type of cuisine. Browsing through the online menus of Spanish restaurants he stumbles across a small selection of his favorite tapas offered at some of these restaurants and concludes that he cannot narrow down the set of available alternatives any further than to all restaurants serving such food. Our paper builds on this theme of “indecisive” choice behavior and extends previous research on sequential rationalization by Manzini and Mariotti (2007, 2012). Our goal is to explicitly model the procedure of sequential elimination ascribed to Herbert above. Specifically, we consider a decision-maker (DM) who chooses according to the following process of sequential elimination: at each stage of the elimination sequence, the DM separately or jointly removes alternatives from further consideration provided that he judges them to be inferior to other available alternatives with respect to certain stage-specific decision criteria just as Herbert uses cuisine type and tapas selection in the example above. At the last stage of such a sequence, the elimination of less attractive alternatives may leave a conflict between the remaining alternatives that is hard to resolve (Shafir et al. 1993) to the extent that the DM settles for the choice of some of the remaining alternatives instead of further pursuing the search for the most valuable alternative. This mirrors Herbert’s decision to be fine with any restaurant that offers a selection of his favorite tapas. The literature is rather agnostic about how a DM’s conflict between “remaining” alternatives should be interpreted. Eliaz and Ok (2006) suggest that the DM may either be indifferent (Kreps 1988) or indecisive (Sen 1993) between such alternatives. According to the choice process described above, the remaining alternatives are incomparable for the DM. That is, any remaining alternative is undominated by all other (initially) available alternatives and with respect to any decision criterion used in the elimination sequence. Viewed from this perspective, the term indecisiveness best describes the DM’s attitude towards these alternatives. Indeed, under the choice procedures in this paper, it is conceivable that x and y each remain after sequentially eliminating alternatives from the set \(\{x,y\}\), but sequentially eliminating alternatives from \(\{x,y,z\}\) leaves x as the only remaining alternative. If the DM were indifferent between x and y, then, in any choice problem where they are both available, we would expect him to settle for one of the two alternatives if and only if he also considers settling for the other one. It is this property of indecisiveness that our generalizations of the rational shortlist method (RSM) by Manzini and Mariotti (2007) and the categorize then choose (CTC) procedure by Manzini and Mariotti (2012) distinguish from the respective original versions. In the original formulations, a DM is always able to both identify and pick a unique best alternative, which implies that any indecisiveness has to be resolved across the corresponding sequence of elimination stages. This requires that from all alternatives that remain after the first stage of elimination, the asymmetric binary relation (rationale), which is applied at the second stage (in both models) to remove inferior alternatives, spares exactly one unique maximal element. In our restaurant example, this entails that according to the rationale at the final stage all but one of the dinner options that survive the first stage are dominated by another alternative such that, for instance, there exists only a single undominated tapa that is exclusively offered at one restaurant rather than several favorite tapas offered at different restaurants. Furthermore, the decisiveness demands that no other alternative is chosen if the same choice problem is to be faced repeatedly. The original version of the CTC by Manzini and Mariotti (2012) is fully characterized by a weak version of the weak axiom of revealed preferences (WWARP), and the characterization of the RSM by Manzini and Mariotti (2007) requires, in addition to this condition, a standard expansion axiom. The authors further note that “in general, we still lack such conditions for general choice correspondences” (Manzini and Mariotti 2007, p. 1833), so our characterization can also be interpreted as filling this gap. In it, we attempt to closely follow the original axiomatizations by directly transforming these axioms from the domain of choice functions to that of choice correspondences. Our adjusted version of WWARP keeps the general intuition of the original condition for choice functions, which is that of excluding a certain kind of choice reversals. In our interpretation, choice behavior reveals such a choice reversal if an alternative is chosen over another alternative in some set, but this relation is reversed in a superset of this set. The structure that our axiomatization imposes on choice behavior generally allows for such choice reversals to occur, but rules out choice re-reversals. In other words, our version of WWARP requires that if the same alternative is chosen in a binary comparison with some other alternative and from a set comprising either of these alternatives, then the other alternative can neither be chosen from the set itself nor from any of its subsets that comprise either alternative. Stated differently, our axiom excludes that choice reversals between two alternatives can be reversed again. Our second axiom, a transformation of the original expansion axiom to the structure of choice correspondences, shares its rather straightforward intuition, as it demands that any alternative that is part of the chosen subset of each of two sets is also part of the chosen subset of the union of these two sets. In Sect. 2, we introduce the setup and formally define our choice procedures. Section 3 gleans some intuition about their general properties and Sect. 4 provides their axiomatic characterizations. The final section illustrates a peculiar feature concerning indecisive choice behavior and relates our choice procedures to other models in the axiomatic choice theory literature",8
84.0,4.0,Theory and Decision,17 March 2018,https://link.springer.com/article/10.1007/s11238-018-9660-1,The probability of majority inversion in a two-stage voting system with three states,June 2018,Serguei Kaniovski,Alexander Zaigraev,,Male,Male,Unknown,Male,"Two-way elections conducted using two-stage voting procedures are prone to majority inversions, a situation in which the outcome of an election does not represent the will of a majority of voters. We study a model of two-stage voting with three states (constituencies, districts), derive the probability of majority inversion as a function of the population weights, and relate this probability to the inequality in the distribution of population among the states. Two-stage voting requires the voters to be grouped into states. In the first stage, each voter casts a single vote for one of the two candidates (parties, referendum options). The outcome of the first stage is called the popular vote. In the absence of ties, the simple majority rule picks a winner in each state and in the country. The hallmark of a two-stage voting system is that the popular vote does not determine the outcome of the election. It is decided by the electoral vote in the second stage. Electoral votes are cast by the electors, each representing one state in an assembly of states. The electors may command several votes. We assume that each elector casts all of his votes as a bloc for the candidate who obtained a majority in the state the elector represents, thus sidestepping the issue of ‘faithless electors’. The election is awarded to the candidate with a majority of electoral votes, who may not be the candidate that won the popular vote. Majority inversion takes place if the outcome of the election does not coincide with the popular vote. The U.S. presidential election is not a two-way election.Footnote 1 If we held the U.S. presidential election for what it essentially always was: a contest between the Democrats and Republicans, we would identify four majority inversions in the past. Since more populous states command more electoral votes, the second-stage votes in the Electoral College system are weighted. By contrast, the second-stage votes in legislative elections are not weighted. In a single-member-district majority system, each district elects one member of the parliament in the first stage. The member then casts a single vote on a series of bills over the course of a legislature term, which can be viewed as the second stage of a two-stage voting process. Majority inversion occurs if the party that obtained a majority of seats in parliament is not the party that won a majority of votes.Footnote 2 This variety suggests that a comprehensive model of two-stage voting should admit constituencies of different sizes and weighted voting in the second stage. We shall refer to the system with population-weighted second-tier votes as Electoral College (EC)Footnote 3 and the single-member-district majority system as Westminster (WM). The third model, denoted EP, is the baseline model of equipopulous states that is commonly studied in the theoretical literature on majority inversions. The forth model, denoted GM, is the most general one. It involves states of different population sizes and weighted electoral votes. 
May (1948) appears to have taken the first step towards computing the probability of majority inversion in a two-stage model with an odd number of equally sized states. May works with a discrete uniform distribution for the number of supporters of a certain candidate in each state, which became a continuous uniform distribution in the limiting case of infinitely many voters. The assumption of a uniform distribution implies that all levels of support for a given candidate, whether expressed in absolute terms in persons or in relative terms as a percentage, are equally likely. The recent wave of theoretical results placed May’s work in the context of the stochastic models of voting behavior used in the contemporary voting theory. Feix et al. (2004) and Lepelley et al. (2011) extend the calculations to an even number of states and provide numerical simulations when the number of states is large. May assumes that the levels of popular support in any two states are independent random variables. This assumption is relaxed in De Mouzon et al. (2017), who perform a comprehensive analysis of the rate of converge of the probability of majority inversion as the number of states increases, under the assumption that any two votes in the electorate correlate. The baseline model (EP) maintains equally sized states and a ‘one person, one vote’ principle at both stages of the two-stage voting procedure. The ‘one person, one vote’ principle is a natural assumption for the first stage, where it embodies the democratic principle of equal suffrage among the voters. It is also consistent with the assumption of equally sized states, since the number of electoral votes awarded to a state usually depends on its size. The next level of complexity involves states of different population sizes (WM), followed by electoral votes weighted either with the population weights (EC), or with arbitrary weights (GM). Studies of the weighted model can be found in Lepelley et al. (2014) for the case of three states, and in Kikuchi (2016) as the number of states tends to infinity. Lepelley et al. (2014) showed that weighting the second-tier votes proportionally to the ratio of the square-roots of population sizes, a method famously known to equalize the indirect voting power in two-stage voting systems, does not minimize the probability of majority inversion.Footnote 4 Consistent with the literature on the square-root rule, Lepelley et al. (2014) used the impartial culture (IC) model. While the probability of majority inversion can be computed under IC, the companion impartial anonymous culture (IAC) model is preferable, due to it being extendable to infinite populations. The continuous uniform model used in this paper arises as a limiting case of the IAC model applied at the state level, while maintaining May’s independence assumption between the states. De Mouzon et al. (2017) refer to this model as IAC*. The present paper is motivated by Kikuchi (2016), who provided several far-reaching results for weighted votes. He established that equal weights minimize the probability of majority inversion and that this probability increases monotonically with the dispersion of population weights to an upper bound of one half. Kikuchi’s results are asymptotic in the number of states and, therefore, do not apply when the number of states is small. In particular, letting the number of states tend to infinity implies that the weight of each state tends to zero. Indeed, we show that, for three states, increasing the inequality of voting weights increases the probability only if the population weight of the largest state does not exceed one half, and that the distribution of the population among the three states which attains the bounds on this probability depends on the weighting scheme at the second stage. The largest voting weight exceeding one half is equivalent to the largest state being a ‘dictator’. Our principal tool is the theory of majorization and Schur-convexity (Marshall et al. 2011), which are closely related to the theory of inequality measurement (Yitzhaki and Schechtman 2013).",8
84.0,4.0,Theory and Decision,13 November 2017,https://link.springer.com/article/10.1007/s11238-017-9640-x,Timing effect in bargaining and ex ante efficiency of the relative utilitarian solution,June 2018,Omer F. Baris,,,Male,Unknown,Unknown,Male,"An n
-person bargaining problem is typically defined as a pair \(\left( S,d\right) \), where \(S\in \mathbb {R}^n\) is the compact, convex, and comprehensive set of feasible outcomes (utility vectors), and \(d\in S\) is the disagreement point representing the payoffs to be received if no agreement is reached.Footnote 1 S is non-trivial, i.e. there exists \(s\in S\) such that \(s\gg d\). To simplify notation, d is normalized to the origin (\(d=0\)) so that the bargaining problem can be simply denoted as S instead of (S, d). The ideal point (m(S)) is the vector of maximum payoffs for each bargainer independent of what others are getting:Footnote 2 \(m_i(S)=\max \{ s_i | s\in S\}\). Let \(\Sigma \) be the set of all non-trivial, convex, compact, and comprehensive subsets of \(\mathbb {R}^n\). Elements in \(\Sigma \) are interpreted as bargaining problems. A bargaining solution F is a correspondence, assigning a non-empty subset \(F\left( S\right) \subset S\) for every bargaining problem \(S\in \Sigma \). Note that this definition allows for multi-valued outcomes.Footnote 3 It is slightly different from the original definition of a solution by Nash (1950). There are a number of alternative solutions proposed in the literature, each satisfying some desirable properties (axioms), but there is no agreement on which single solution is the best for all bargaining problems.Footnote 4 Bargainers have von-Neumann–Morgenstern preferences over outcomes and expected utility functions that represent their preferences are unique up to positive affine transformations. It is desirable for a bargaining outcome to be invariant under such transformations. The Nash (1950), hereafter \(F^\mathrm{N}\), and the Kalai and Smorodinsky (1975), hereafter \(F^\mathrm{KS}\), solutions satisfy this but egalitarian and utilitarian solutions do not. When bargainers face a lottery of two or more bargaining problems, it is possible to achieve Pareto improvements through contingent agreements today, or by delaying the agreement until the uncertainty is resolved. Linearity is known as the no-timing-effect condition (Myerson 1981), that rules out these possibilities, stating that the timing of the agreement (or the evaluation of individual utilities) should not matter. In this note I propose the Weak Linearity axiom (hereafter WLN, formally defined in Sect. 2) to characterize the solution concept known as the relative utilitarian solution (Dhillon 1998; Dhillon and Mertens 1999; Segal 2000; Sobel 2001; Pivato 2009; Rachmilevitch 2015), the modified Thomson solution (Cao 1982), or the normalized utilitarian solution (Miyagawa 2002; Thomson 2010). To be consistent with the existing standard terminology, I will call it as the relative utilitarian solution, hereafter \(F^\mathrm{RU}\). 
\(F^\mathrm{RU}\) maximizes the total sum of normalized utilities, i.e. after the bargaining problem is normalized to [0, 1] scale with \(d=0\) and \(m=1\). This is akin to \(F^\mathrm{KS}\), which is also called as the relative egalitarian solution since it chooses the egalitarian outcome for the normalized bargaining problem. \(F^\mathrm{KS}\) is obtained by replacing the independence of irrelevant alternatives (IIA) axiom with a restricted monotonicity axiom. WLN restricts linearity condition to the bargaining problems that share the same ideal point, and \(F^\mathrm{RU}\) is obtained by replacing IIA with WLN. The characterization provided in Sect. 3 is simple and direct, a consequence of Myerson (1981). It has an intuitive appeal as the results and implications are sufficiently different. First, WLN does not violate the property of individual utilities having equivalent representations. Second, the emphasis is on the timing effect, and through the no-timing-effect character of linearity, it highlights the ex ante efficiency of \(F^\mathrm{RU}\).",1
84.0,4.0,Theory and Decision,21 June 2017,https://link.springer.com/article/10.1007/s11238-017-9617-9,"Communication, leadership and coordination failure",June 2018,Lu Dong,Maria Montero,Alex Possajennikov,,Female,Male,Mix,,
84.0,4.0,Theory and Decision,17 January 2018,https://link.springer.com/article/10.1007/s11238-017-9652-6,Prosocial consequences of third-party anger,June 2018,Janne van Doorn,Marcel Zeelenberg,Tyler G. Okimoto,Female,Male,,Mix,,
84.0,4.0,Theory and Decision,22 June 2017,https://link.springer.com/article/10.1007/s11238-017-9619-7,Stability and cooperative solution in stochastic games,June 2018,Elena M. Parilina,Alessandro Tampieri,,Female,Male,Unknown,Mix,,
84.0,4.0,Theory and Decision,21 June 2017,https://link.springer.com/article/10.1007/s11238-017-9616-x,Consistency of determined risk attitudes and probability weightings across different elicitation methods,June 2018,Golo-Friedrich Bauermeister,Daniel Hermann,Oliver Musshoff,Unknown,Male,Male,Male,"Risk and uncertainty play an important role in a variety of economic decisions (Abdellaoui et al. 2011). For instance, decision-makers’ individual risk attitudes influence asset market activity (Fellner and Maciejovsky 2007), demand for insurance (Beetsma and Schotman 2001), and adoption of technology (Purvis et al. 1995). Risk-averse decision-makers might even refuse an investment with a positive expected net present value if their risk premium is not covered (Isik and Khanna 2003). Consequently, the determination of individual risk attitudes is important for predicting and explaining economic behaviour. Numerous prior studies have focused on measuring individual risk attitudes. Binswanger (1980) was among the first to use experimental approaches with real payouts to determine individual risk attitudes. Further elicitation methods followed, such as those developed by Murnighan et al. (1988), Holt and Laury (2002), Eckel and Grossman (2002), and Brick et al. (2012). The lottery task by Holt and Laury (2002) (HL method) is considered to be the “gold standard” for measuring risk attitude (Anderson and Mellor 2009). However, all of the aforementioned elicitation methods assume that individuals behave according to expected utility theory (EUT). In an EUT framework, the individual’s risk attitude is solely determined by the curvature of the value function. Thus, under EUT, it is assumed that humans do not assign distorted weights to probabilities (Abdellaoui et al. 2011), which means that when a person enters a lottery with a 1% chance of winning, his/her expectation will match the true probability. According to Andersen et al. (2006), the main advantages of EUT-compatible elicitation methods are that they are comprehensible and easy to implement. However, the literature contains well-known examples that demonstrate a systematic violation of EUT, such as the Allais paradox (Allais 1953). One of the major counter-arguments to the EUT paradigm is the phenomenon of probability weighting (Van de Kuilen 2009). EUT assumes that humans weight probabilities linearly, whereas humans often distort probabilities in a non-linear manner in reality. The non-linear distortion of probabilities can affect the measured risk attitude (Harrison et al. 2010) and suggests that elicitation methods must consider probability weighting (Abdellaoui et al. 2011). Kahneman and Tversky (1979) implemented a parameter for probability weighting according to prospect theory to remedy this problem. Subsequently, Tversky and Kahneman (1992) proposed cumulative prospect theory (CPT), an extension of prospect theory that allows for probability weighting. Depending on the value of the parameter, participants can, for instance, overweight an objective 1% chance of winning a lottery game and underweight an objective 99% chance of winning. Among other available non-EUTs, the prospect theories of Kahneman and Tversky (1979) and Tversky and Kahneman (1992) are considered the best suited to assessing risky choices in economics (Camerer 1998; Starmer 2000). There are two possibilities to measure risk attitudes according to CPT: lottery tasks with a large number of independent binary choices in the form of a multiple price list (MPL) (cf. Hey and Orme 1994; Carbone and Hey 2000; Stott 2006; Tanaka et al. 2010 and methods using a chained series of binary choices to elicit indifferences (cf. Wakker and Deneffe 1996; Bleichrodt and Pinto 2000; and Abdellaoui 2000). Compared to EUT-compatible methods, CPT-compatible elicitation techniques determine both a parameter for risk attitude (curvature of the value function) and a parameter for probability weighting, which jointly reveal the risk preference.Footnote 1 Assuming that an individual possesses a particular risk attitude as well as a particular probability weighting parameter at a specific point in time, different elicitation methods should lead to matching results under the same theoretical assumptions. However, comparisons of the results from different EUT-compatible elicitation methods in a within-subject design show that there are significant differences in the determined risk attitudes (cf. Dave et al. 2010; Reynaud and Couture 2012). Regarding CPT-compatible elicitation methods, no studies to date have compared the elicited risk attitudes or probability weighting parameters. Therefore, using CPT-compatible elicitation methods that consider individual probability weightings could lead to the desired consistency across different elicitation methods. With this in mind, the present study focuses on two different aspects. First, we perform an incentivized within-subject experiment to examine whether the elicited risk attitude parameters differ between two widely used CPT-compatible elicitation methods: the method by Tanaka et al. (2010; TCN method) and the method created by Wakker and Deneffe (1996; WD method) and extended by Abdellaoui (2000). Second, we analyse whether there are differences between the probability weighting parameters determined using the TCN and WD methods. To the best of our knowledge, this study is the first to analyse whether the measured parameters for risk attitude and probability weighting vary across different CPT-compatible elicitation methods. The present study is structured as follows: the generation of hypotheses is presented in Sect. 2, and the elicitation methods applied to determine the risk attitudes and probability weightings are described in Sect. 3. The results of the analysis are presented in Sect. 4, and the conclusions and future research prospects are presented in Sect. 5.",11
84.0,4.0,Theory and Decision,19 March 2018,https://link.springer.com/article/10.1007/s11238-018-9659-7,Dynamic consistency of expected utility under non-classical (quantum) uncertainty,June 2018,V. I. Danilov,A. Lambert-Mogiliansky,V. Vergopoulos,Unknown,Unknown,Unknown,Unknown,,
85.0,1.0,Theory and Decision,21 June 2018,https://link.springer.com/article/10.1007/s11238-018-9654-z,Introduction to the special issue “Beliefs in Groups” of Theory and Decision,July 2018,Franz Dietrich,Wlodek Rabinowicz,,Male,Unknown,Unknown,Male,,
85.0,1.0,Theory and Decision,03 July 2017,https://link.springer.com/article/10.1007/s11238-017-9615-y,Learning from others: conditioning versus averaging,July 2018,Richard Bradley,,,Male,Unknown,Unknown,Male,"When others have information or judgemental capabilities that we lack, then their opinions are a resource that we can and should exploit for the purposes of forming or revising our own opinions. But how should we do this? In this paper I will compare two types of answer to this question—Bayesian conditioning and opinion pooling—and ask whether they are compatible. Some interesting work on the question suggests a positive answer under various conditions, see for instance the studies by Genest and Schervish (1985), Bonnay and Cozic (submitted) and Romeijn and Roy (submitted). But the question remains of how restrictive these conditions are. A Bayesian treats the expressed opinions of others as evidence for and against the truth of the claim under consideration, evidence whose relevance is captured by an assignment of a conditional probability for the claim, given each possible combination of others’ opinions. She responds to this evidence by conditioning on it, i.e. by adopting as her revised opinion her conditional degrees of belief given the expressed opinions. The opinion pooler, on the other hand, adopts as her new opinion an aggregate of the expressed opinions of others (and perhaps her own), an aggregate that in some way reflects the epistemic value that she attaches to each of the expressed opinions. I shall assume here that Bayesianism provides the gold standard for coherent revision of belief in the kinds of situation in which it applies, namely when we have prior probabilities for not only the hypotheses of ultimate interest but also for all possible combinations of evidence (in this case, the expressions of opinion) that either confirm or disconfirm these hypotheses, and when everything that we learn is representable by one such possible combination. The problem is that it is not always easy to apply the Bayesian theory. In circumstances in which the evidence takes a ‘non-standard’ form, such as when it is imprecise or conditional in form, we can turn to other forms of belief revision, such as Jeffrey conditioning or Adams conditioning.Footnote 1 But when we are unable to assign prior probabilities to the possible evidence propositions, or determine associated likelihoods, then no conditioning method at all may be applicable. This can happen because there are simply too many possibilities for us to process them all, or because we do not have enough information to assign a precise probability with any confidence. These difficulties are acutely pertinent to the question of how to exploit the information taking the form of expert opinion reports. Consider, for instance, someone who claims to be an expert on wines. What is the probability that they will make any particular judgement about any particular wine? If you do not know much about wine, it will be hard to say. In the statistics literature, agents who change their beliefs by conditionalising on the testimonial evidence of experts are known as supra-Bayesians [see, for instance, Morris (1974) and French (1981)]. Supra-Bayesians must have priors over the opinion states of all those whose opinions count as evidence for them with regard to some proposition. But opinions about opinions might be evidence too, and opinions about opinions about opinions. And so on. It would be fair to say that supra-Bayesians are required to be cognitive super-Humans. A Bayesian with more limited cognitive resources has two reasons for taking an interest in opinion pooling. First, it might help her in thinking about how to assign the probabilities to the hypotheses, conditional on combinations of opinion, that she needs to revise her own opinions by conditionalisation when she gets information of this kind. Second, in circumstances in which she cannot conditionalise on expressions of opinion because she lacks the requisite conditional probabilities for the hypotheses that interest her, she might adopt opinion pooling as an alternative method of belief revision. In both cases, the diligent Bayesian will want to know whether a particular rule for opinion pooling is compatible with her commitment to conditionalisation. This is true not just when she uses opinion pooling as a guide to making conditional probability judgements, but also in the case when she uses it as an alternative to conditionalisation. For in this latter case, she will want to know that there is some way of assigning prior probabilities to combinations of expressed opinion and to the hypotheses that are evidentially dependent on them, such that the pooled opinion is what she would have obtained by conditionalisation if these had been her prior probabilities. In this paper I will address the question of whether revision of belief in response to testimonial evidence by application of a well-known form of opinion pooling—linear averaging—is compatible with Bayesian norms of belief change together with some minimal conditions on appropriate respect for expert judgement. I will begin by defining this form of opinion pooling and making more precise what is required for it to be compatible with Bayesian revision. In subsequent sections, I investigate what Bayesianism requires of agents in some rather simple situations; in particular, ones which mandate deference to the opinions of one or more experts on some specific proposition. Finally I consider the consistency of these requirements with those implied by linear averaging, drawing particularly on Dawid et al. (1995). The conclusion is somewhat surprising. Linear averaging, even when applied to a single proposition, is not (non-trivially) compatible with Bayesian conditionalisation in situations involving more than one source of expert testimony to which deference is mandated.",10
85.0,1.0,Theory and Decision,17 January 2018,https://link.springer.com/article/10.1007/s11238-017-9639-3,"Weighted averaging, Jeffrey conditioning and invariance",July 2018,Denis Bonnay,Mikaël Cozic,,Male,Unknown,Unknown,Male,"A theory of rational belief typically contains two components: a synchronic component which describes the constraints which an agent’s doxastic state should obey, and a diachronic component saying how an agent’s doxastic state should be updated upon receiving new information. In this paper, we will be concerned with the dynamics of beliefs, under the assumption that an agent’s doxastic state can be represented by a probability distribution. When one is interested in the issue of knowing how an agent should update her prior probabilities upon receiving new information, one may prima facie distinguish two kinds of situation. In the case of an individual update, the agent learns that some event has a given probability and updates her priors so as to grant that probability to the event. Conditioning (when the new probability of the event is 1) and Jeffrey conditioning (when the new probability of the event is an arbitrary new value) are the two most popular updating rules for this kind of situation. In the case of a social update, the agent learns another agent’s opinions and updates her own priors so as to take into account the other’s opinions. In this case, the most popular rules consist in averaging the two priors, using weighted arithmetic or geometric means.Footnote 1
 Let us have a closer look at social updates. An agent \(A_1\) updates her priors on the basis of the testimony of some other agent \(A_2\), who reveals her degree of belief in an event E. This process may be broken down into two different stages. First, some kind of trade-off occurs, between \(A_1\)’s own prior towards E and \(A_2\)’s prior. \(A_1\) has to decide how, and how much, she is going to change her belief in E given \(A_2\)’s degree of belief in E. The output of this process is \(A_1\)’s posterior probability for E. The second stage consists in \(A_1\)’s adjusting her probabilities towards the other events. \(A_1\) has to decide what her degrees of beliefs in all other events become, now that her belief in E has changed. Theories of individual update (like those mentioned above) typically deal with adjustment. What the posterior probability for the target event should be is taken as given, and the problem is precisely how to do the adjustment for the other events in the algebra. By contrast, theories of social update, as usually stated, only deal with the trade-off task. They answer the question what the posterior probability for E should be,Footnote 2 but they remain silent as to how the agent should complete her posterior distribution following the trade-off stage. The question how this should be done has recently been raised, by Jehle and Fitelson (2009) and Steele (2012), but a principled supplementation of trade-off rules, such as weighted averaging, is still lacking. Jehle and Fitelson (2009) are concerned with situations of peer-disagreement. As an updating rule to handle such situations, they consider unweighted arithmetic means supplemented with a constraint of distance minimization (using Euclidean distance) with respect to the agent’s priors, but they do not claim to provide a vindication of this particular way to supplement the averaging. Steele (2012) considers supplementing weighted averaging with Jeffrey conditioning, but ends up rejecting both, on account of failure of commutativity. Neither is there to be found a joint approach to individual and social updates: characterizations of individual updating rules are usually spelled out in terms which are alien to the axiomatic approach widespread in the literature on combining probability distributions. In this paper, we wish to propose such a unified view of individual and social updates on the one hand, and of trade-off and adjustment on the other hand. Our approach will be axiomatic: we aim to find principled axioms from which one can derive adjustment rules for individual updates, trade-off rules for restricted social updates (which assume that \(A_2\)’s priors are revealed for all events in the algebra), and, finally, trade-off and adjustment rules for unrestricted social updates (which generalize over restricted social updates by allowing for cases when agent \(A_2\) reveals her priors about some but not all events in the algebra). Surprisingly enough, a single invariance axiom does the job for the three kinds of situations we wish to consider. In Sect. 2, we introduce invariance under embedding and show that it axiomatizes Jeffrey conditioning (J), as an individual updating rule for an agent setting her prior for an event to some new value. This result is a discrete version of a previous characterization of Jeffrey conditioning by Teller (1973) and van Fraassen (1990) which allows itself with the resources of real analysis. Section 3 sets the stage for social updates, in their restricted and unrestricted forms. We then show in Sect. 4 that weighted averaging (WA), as a restricted social updating rule for an agent who wishes to mitigate her priors with another agent’s fully disclosed priors, is also axiomatized by invariance under embedding (this is merely a variation on known results). Fruits are ripe to show at the end of Sect. 4 that invariance under embedding axiomatizes weighted averaging extended with Jeffrey conditioning (EWA), as an unrestricted social updating rule for an agent who updates with respect to the partially revealed priors of an another agent. Finally, in Sect. 5, we lay the basis for further work on invariance and updating rules. Building on earlier results by Gilardoni (2002), we suggest how the same strategy may be applied to trade-offs based on geometric rather than linear averaging, leaving open the question what the corresponding adjustment rule would be.",3
85.0,1.0,Theory and Decision,30 October 2017,https://link.springer.com/article/10.1007/s11238-017-9633-9,All agreed: Aumann meets DeGroot,July 2018,Jan-Willem Romeijn,Olivier Roy,,Unknown,Male,Unknown,Male,"This paper is about two influential models of convergence of probabilistic opinions: consensus through opinion pooling (DeGroot 1974; Lehrer and Wagner 1981), and agreement through Bayesian updates (Aumann 1976; Geanakoplos and Polemarchakis 1982). In linear pooling, upon learning what others believe each agent forms her new belief by taking a linear combination of the opinions of herself and of others, weighted by how much she trusts or respects them. By iterating this process sufficiently often the agents will converge to a fixed point in the space of opinions. In Bayesian models of agreement, on the other hand, upon learning what the others believe, the agents update their beliefs by Bayesian conditioning. Under the assumption of a common starting point and under the further assumption that agents know the type of information, but not the information content, that others have been given in the meantime, iteratively announcing the posteriors will lead the agents to agree. While it has been claimed that both models offer a method to achieve consensus that is rational in its own right (Aumann 1976; Lehrer and Wagner 1981), doubts have been cast on the conceptual compatibility of pooling and Bayesian updating—see in particular the discussion in Bradley (2007) and Steele (2012). Moreover, Bayesian updating is often taken as the “basic normative standard” for rational belief change (cf. Bradley 2007, p. 12). This leads to the question whether iterated pooling can be given a Bayesian rationalisation. Interestingly, this is what Aumann writes in his seminal paper: It seems to me that the Harsanyi doctrine is implicit in much of [the literature on opinion pooling]...The result of this paper may be considered a theoretical foundation for the reconciliation of subjective probabilities [i.e., by means of pooling]. To the best of our knowledge, Aumann’s suggestion was never converted into a formal result. Can we indeed reconstruct iterated pooling in Bayesian terms, in such a way that the sequence of steps towards consensus taken by iterated pooling can be motivated and accounted for? Several results have already shown necessary and sufficient conditions for the Bayesian representation of a single pooling operation (Genest and Schervish 1985; Bonnay and Cozic 2015; omitted for blind review, 2015). So there is a Bayesian representation of agents engaged in pooling, and hence there is, in a strict sense, a formal account of iterated pooling as well. But what is lacking is an account of why these iteration steps should be taken in the first place. What underpins the specific social influence that is modeled by iterated pooling? The idea behind pooling is that the agents trust or respect, to various degree, the opinion of the others. But there is no such thing as trust, at least on the face of it, in the Bayesian redescription of pooling, and the iterations of pooling lack proper motivation. We provide a representation theorem that makes good on Aumann’s suggestion and thereby answers the above questions: consensus via pooling can be represented as an Aumann-style agreement through updating. Iterated pooling is thus rationalized according to Bayesian standards, as a matter of higher-order reasoning. More precisely, the repeated adjustments of pooling can be represented as steps in which the agents not only rely on their own beliefs regarding the reliability of the others, but also on their beliefs regarding the others’ beliefs in their reliability, and so on. The representation offers a concrete interpretation of the epistemology of iterated pooling: pooling steps constitute an exchange of information, and the trust that agents put in each other translates into beliefs regarding the reliability of others on different levels of the belief hierarchy. In what follows we assume familiarity with iterated pooling, and so only introduce it briefly in Sect. 3. Agreement theorems and their dynamic versions, which underlie our representation, are far less discussed in the social epistemology literature. So we present them in some detail in Sect. 2. We prove the main theorem in Sect. 3. Section 4 discusses the interpretation of pooling that the representation gives, and considers the Bayesian model as an expression of conditions of applicability.",5
85.0,1.0,Theory and Decision,27 November 2017,https://link.springer.com/article/10.1007/s11238-017-9642-8,Judgment aggregation and minimal change: a model of consensus formation by belief revision,July 2018,Marcel Heidemann,,,Male,Unknown,Unknown,Male,"Whenever agents act together, some kind of agreement is required, whether on the goals of their joint action or on how they can be attained. Sometimes, it is sufficient to reach a merely superficial agreement. For instance, when an expert panel agrees on a set of statements to include in a report, this does not mean that every member of the panel will give up their prior judgments after publishing the report. Only when speaking for the panel as a whole will they adopt the position agreed on, even if it contradicts their personal beliefs. But in other cases, individual and collective actions may be interrelated in a way that required the agents to reach a deeper consensus by changing their individual beliefs. For example, picture a lonely hiker who is in need of assistance. He does not know where he is and it is getting dark, but he was able to call a rescue team that is searching for him. Given the search area, his rescue will require a joint effort by all involved persons: not only will the rescue party look for him, he will also have to move to a position where he can be easily found and reached. To coordinate their effort, they will have to agree on a number of issues: is the dim light we all see in the distance coming from the same source? Are we on the same side of the river? Given that their evidence may be inconclusive, it would not go against their individual rationality if they initially disagreed. But after exchanging some information, it is necessary that they change their beliefs in a consistent way such that they are aligned with respect to the relevant issues. After all, their consensus would be fairly useless if they agreed on the fact that they are on different sides of the river, but the rescue team still believes that they won’t need their boat and acts on that supposition. On the other hand, there are some beliefs the rescuers should not easily give up just for the sake of agreement: if it turned out that the consensus is incompatible with the conjunction of the position indicated by the rescue team’s GPS receiver and their expected position of the hiker, they should revise the latter and not immediately throw away the device. The statements which an agent believes or accepts may depend on the role she is currently playing. A scientist may have or express different beliefs when she is speaking as the member of an expert panel than when she is in the role of a private citizen. When the panel reaches an agreement, it should not be required that the members change their private beliefs, since these relate to a different role of its members. However, in cases like the rescue operation, we are not concerned with private beliefs, but instead with individual beliefs of group members as group members. These individual beliefs affect the group and its collective efforts, but it would be neither necessary nor practically possible to include them as items on which the whole group must agree. For instance, the expected visibility within the next hour will be very important for the helicopter pilot, but perhaps not so much for the ground team. But whether the helicopter can fly or not is relevant for the whole group and all members will have to share this belief. The individuals may have differing private beliefs, such as a team member who believes that a certain standard procedure does not make sense in this situation, but nonetheless acts according to it. Our considerations do not apply to these private beliefs and we do not make any assumptions on how the members’ individual beliefs relate to them or to their beliefs in any other roles. Although many aspects of belief change addressed in this work may also be relevant for the collective revision of private beliefs, the success condition of unanimity introduced in Sect. 3.1 only makes sense for the transformation of beliefs of agents as group members. Requiring agents to align their private beliefs as specified by this condition would contradict their defining property of being private. What is the rational way to reach a consensus in cases like these? Conceptually, there are two aspects to this question (although in practice, they may be intertwined instead of appearing in distinct steps): first, the content of the consensus is determined. That is, for each relevant issue, it should be decided whether to believe it, disbelieve it or suspend belief. Second, the agents should revise their individual beliefs to match these group-level doxastic attitudes. We claim that both aspects are interrelated: the value of the consensus should be one that minimizes the extent of the changes required for the individual agents to incorporate it. As this example illustrates, the outcomes of interactions between agents are often shaped by the tension between the goal of retaining the current state and the goal of reaching a consensus. Even if the agents are willing to do whatever it takes to agree, what exactly they agree on will depend on what exactly they are more willing to change individually. The theory of judgment aggregation (List and Pettit 2002; List 2012) is concerned precisely with reaching an agreement on interrelated issues, but it is not possible in this framework to consider the extent of changes required to make a belief set of an agent conform to the group outcome. On the other hand, the theory of belief revision (Alchourrón et al. 1985; Gärdenfors 1988) is centered on minimal change, but does not include agreement among multiple agents as an epistemic goal. This paper, therefore, presents a framework we call agreement revision, which defines a consensus-generating function based on the rich structure of a belief revision model that is generalized to a multi-agent setting. The following section briefly introduces the theories of judgment aggregation and belief revision as well as extensions of the latter relevant for agreement revision. Section 3 describes the framework by introducing postulates for various transformation types as well as representation theorems. In Sect. 4, agreement revision functions based on orders of interpretation profiles are explored. Relations to other theories and further issues are discussed in Sect. 5, and Sect. 6 closes with a conclusion. Proofs are given in Appendix A.",
85.0,1.0,Theory and Decision,04 December 2017,https://link.springer.com/article/10.1007/s11238-017-9643-7,The wisdom of collective grading and the effects of epistemic and semantic diversity,July 2018,Aidan Lyon,Michael Morreau,,Male,Male,Unknown,Male,"It is well known that collective judgements can be better than individual ones. Say you want to know how many jelly beans there are in a jar. Often you will do better to take the average of your friends’ guesses rather than just rely on your own judgement; some will guess too high, others too low, and the errors will cancel out. Unless you’re a highly skilled jelly-bean estimator, the average is likely to be more accurate than your own judgement, or that of anyone else in the group. This effect is known as the wisdom of crowds or collective wisdom (Surowiecki 2004; Page 2008). The jelly beans are of course just a toy example. Collective wisdom has been documented in many different tasks, from Francis Galton’s classic example of guessing the weight of an ox at a county fair (Galton 1907) to forecasting geopolitical events (Mellers et al. 2014) and medical diagnosis (Wolf et al. 2015). The kind of judgement varies from case to case (Lyon and Pacuit 2013). In Galton’s example, the individual inputs are point estimates (“The ox weighs 200 kg”). With geopolitical events they might be subjective probabilities (“I’m 80% confident that the Democrats will win the next election”); with medical diagnosis, categorical judgements (“This patient has Lyme disease”). The aggregation method also differs: you can take the median of point estimates, the average of probabilities and a super majority of diagnoses. Collective wisdom arises in many different contexts but just how it arises varies from case to case (Lyon forthcoming). This article is about collective wisdom in groups whose members express themselves using scores and grades. These are coarse-grained expressions such as the numerical scores 1–6 used by the Arts and Humanities Research Council (AHRC) in Britain for evaluating research proposals. Other examples are qualitative probability expressions such as probable, tossup and unlikely, and the letter grades used in academic evaluation around the world. Characteristically, scores and grades come with an ordering from “top” to “bottom”: a 6 in the sense of the AHRC is better than a 5; a probable event is more likely than an unlikely one; a C is a higher grade than a F, and so on. Scoring and grading are common in juries, committees and panels. Naturally they have attracted the attention of theorists of social choice. Much work in this field has an “axiomatic” focus, being concerned with the formal properties of various aggregation methods (List 2013). Accordingly, one main goal of work on collective grading has been to characterize particular methods for aggregating grades in terms of the axioms they satisfy (see for instance Aleskerov et al. 2007; Gaertner and Xu 2012). Another goal has been to improve on evaluation procedures currently in use by introducing new and better aggregation methods. Thus, Balinski and Laraki (2007, 2011) propose the method of majority judgement, a generalization of taking medians which they recommend not only to expert panels judging wines and performance in sports and the arts, but also as a replacement for traditional voting methods in political elections. The idea of grading candidates in elections is also found in the earlier idea of approval voting (Brams and Fishburn 1978; Alcantud and Laruelle 2014). In a different line of enquiry, collective choice procedures are evaluated accordingly as they “track the truth” about the worth of the alternatives under consideration, in some choice-independent sense (Beisbart et al. 2005; Beisbart and Bovens 2007; Beisbart and Hartmann 2010). Thus, Pivato (2016) states conditions under which approval voting, in particular, may be expected to maximize utilitarian social welfare. Wide ranging though it is, none of this literature on scoring and grading in economics and political theory takes up the matter of collective wisdom, and how its emergence depends on details of people’s interpretations of the scores and grades in which they express their inputs. That is our topic here. One prominent feature of natural languages is their contextuality: different people interpret words differently, and the same people interpret them differently on different occasions. Languages of scores and grades are to a large extent continuous with natural languages, and they too are contextual. We may expect that, as a result, groups of graders will often be semantically diverse. That is, they will include people with different interpretations of the grades. In the case of probability grades, in particular, great semantic diversity has in fact been documented among doctors and their patients (Ohnishi et al. 2002), members of a science panel (Wardekker et al. 2008) and students of business and the social sciences (Fig. 1, Wallsten et al. 1986; Morgan 2014). People can mean very different things by qualitative judgements of probability such as probable, possible, and doubtful. (Figure from Wallsten et al. 1986, redrawn by Morgan 2014) Interpersonal differences in perspectives and cognitive style are known to improve the judgement and decisions of groups (Surowiecki 2004; Page 2008; Tetlock 2005; Nielsen 2011). This epistemic diversity is not the same thing as diversity in age, gender, cultural and linguistic background, or other features that determine people’s social identities, but the two go together. Epistemically and socially diverse groups will often also be semantically diverse, though, and resulting misunderstandings might be expected to pull in the opposite direction. Certainly there is support in the literature for the idea that differences in people’s understanding of scores and grades must create trouble. Thus, Hubbard and Evans (2010) cite “variability of verbal labels” as a problem in qualitative assessment of risk. Balinski and Laraki (2011) state as a requirement for using their method of majority judgement that the members of the group must share a “common language” of grades. Morreau (2016) argues that interpersonal differences in grading thresholds can make collective judgements “unsound,” in a technical sense to be elaborated soon. We consider the possibilities for collective wisdom in a quite common kind of task: selecting the top k of some given n ordered items. A committee or panel can approach this task as follows. First, each member grades each of the items under consideration. The vocabulary of grades they use is fixed in advance, but different members might interpret them differently. For each item a collective grade is determined on the basis of the grades assigned to it by all members of the group; then all of the items are ranked according to their collectively assigned grades, and k items are chosen from the top of the group’s ranking. The epistemic performance of the group—its capacity to track the truth about which k to choose—is measured by calculating how often, in a large number of trials, the k chosen events really are to be found at the top of the actual ordering of the n items. In this way we can study how features of grading languages and their interpretation by group members—such as how many labels there are, and whether everyone has the same understanding of them—affect epistemic performance. Many factors may be expected to affect epistemic performance in addition to similarities and differences in members’ understandings of grades. These include the size of the group and the individual expertise of its members. To study the sometimes complex interactions between these and other factors we approach our topic using, in addition to analytic methods, a computer simulation. Our findings are surprising. One main conclusion is that epistemic performance can be very good even with a lot of semantic diversity. Indeed, other relevant factors being equal, having a common interpretation of grade expressions can seriously depress the performance of the group. Differences in understanding can under realistic conditions actually sharpen up the epistemic edge of groups. There seem to be far-reaching consequences for the design and training of juries, committees and expert panels throughout society. It might for instance be counterproductive for members of a hiring committee to discuss and agree among themselves, say, what it is for a candidate to be excellent, or merely good or fair. Better for them to skip the coordination and work with whatever diverse understandings they happen to bring to the group—better, even, to increase semantic differences among members artificially, perhaps by exploiting the familiar contextuality of scores and grades. These differences tend to create disagreements that are, in part, verbal. Even so, they can help the group to find out which of the candidates are better than which. We proceed as follows. In Sect. 2, we first discuss some of the main features of grades that suit them well to individual and collective judgement. Then we show how semantic diversity results in violations of Morreau’s condition of soundness, but counter that this condition is much too strong to impose as a general requirement on acceptable procedures for collective grading. This prepares the ground for Sect. 3, where we set up the model of collective grading behind our simulation of risk panels. In Sect. 4, we present observations of the simulated panels. We comment as well on the effects of varying certain parameters in the simulation, and on several elaborations of the basic model. Section 5 sums up.",3
85.0,2.0,Theory and Decision,31 May 2017,https://link.springer.com/article/10.1007/s11238-017-9608-x,Are groups ‘less behavioral’? The case of anchoring,August 2018,Lukas Meub,Till Proeger,,Male,Male,Unknown,Male,"Economic research on group performance has evolved significantly in recent years, accounting for the fact that most economically and politically relevant decisions are taken by cooperating teams rather than individual actors. In their literature reviews, Kugler et al. (2012) as well as Charness and Sutter (2012) describe the general trend emerging from the growing body of literature on group performance. Across a broad range of experimental settings, it is shown that groups are more likely to follow game theoretic predictions and, as put by Charness and Sutter (2012, p. 159), are “less behavioral than individuals”. Team cooperation is consequently interpreted as a means of effectively overcoming individual cognitive and motivational limitations and leading to more frequent rational behavior. Groups’ increased rationality compared to individuals may serve as a partial vindication of the assumption of rational choice theory in reality (Charness and Sutter 2012). This argument lends strong support to those strands of literature arguing that market conditions tend to eliminate irrational behavior through monetary incentives and learning effects. Widespread team decision-making might thus further support the argument of markets as “catalyst for rationality and filter of irrationality” (List and Millimet 2008, p.1).Footnote 1 However, while numerous economic games have been considered in terms of group cooperation and rationality, the area of heuristics and biases has been neglected with respect to group performance for economic experimental contexts. Despite being assumed by Kugler et al. (2012), it remains open to question whether groups more effectively overcome individual cognitive biases. Although the current economic literature on team performance might lend support for this view, experimental evidence has yet to be provided. In this paper, we investigate group decision-making in the domain of biases by drawing on the case of the anchoring-and-adjustment heuristic. We thus elicit individuals’ and groups’ susceptibility to the bias in order to determine whether groups are able to avoid biased decisions better than individuals. The anchoring bias has initially been shown by Tversky and Kahneman (1974), whose seminal experiment showed that subjects, when asked to provide their best estimates for the percentage of African nations in the United Nations, were biased towards numbers randomly generated by a wheel of fortune. Despite the obvious irrelevance of the anchor values, they systematically influenced subjects’ estimations. Forty years of variations on the classical experimental design have found the anchoring bias to be fairly robust against experimental variations (cp. the literature review by Furnham and Boo 2011). Building on previous anchoring research, we focus on a single bias, yet derive generalizable results for the area of economic group decision-making and biases by investigating the effects of group cooperation given different task characteristics. This, in turn, allows for a transfer of our findings to other biases and a broader comparison of group and individual behavior. We chose to consider anchoring for our investigation into groups and biases due to its prominent application in explaining distortions in quite diverse economic situations. Since previous laboratory experiments have shown that the anchoring bias is strong and robust, numerous real-world behavioral effects have been attributed to anchoring. Recent examples for these applications include real estate pricing (Bucchianeri and Minson 2013), art and online auctions (Beggs and Graddy 2009; Dodonova and Khoroshilov 2004), sports betting (Johnson et al. 2009; McAlvanah and Moul 2013), earnings forecasts (Cen et al. 2013), financial forecasts (Fujiwara et al. 2013), macroeconomic forecasts (Bofinger and Schmidt 2003; Campbell and Sharpe 2009; Hess and Orbe 2013) and sales forecasting (Lawrence and O’Connor 2000). In contrast to the studies arguing that anchoring has a strong influence in non-experimental, real-world settings, a number of economic field and laboratory experiments on anchoring find no or only moderate anchoring effects (Simonson and Drolet 2004; Tufano 2010; Alevy et al. 2015; Fudenberg et al. 2012; Maniadis et al. 2014). Thus, it can be argued that the conditions prevalent in actual markets successfully correct irrationalities resulting from individual heuristics. Therefore, rationality-increasing teamwork as a ubiquitous form of decision-making in actual markets might be an additional filter for biased decisions that has not been considered in previous experimental studies. We test whether groups are more or less susceptible to externally provided anchors than individuals in three distinct economic domains, covering the domains of factual knowledge, probability estimations and price valuations. We argue that these three domains of decision-making cover well the range of economically relevant situations prone to irrationally anchored decisions as described in the recent non-experimental studies of anchoring on real-world decisions such as pricing, judgmental forecasting or auctions. Our results can, therefore, provide insight into the question of whether groups might avoid biases that are found to be robust for individuals. In the following, we review the related literature. Following Tversky and Kahneman (1974) seminal paper, a large body of literature has dealt with the causes and consequences of the anchoring bias.Footnote 2 With regard to the central features of our design, the results for individual decisions are fairly unambiguous. Exercises on factual knowledge have been repeated numerous times and lead to robust and substantial anchoring effects (Blankenship et al. 2008; McElroy and Dowd 2007). The same holds true for probability estimations, as shown by Chapman and Johnson (1999) and Plous (1989). Price valuations and willingness to pay are covered in the studies by Sugden et al. (2013), Adaval and Wyer (2011), Bateman et al. (2008), Critcher and Gilovich (2008), Nunes and Boatwright (2004), Simonson and Drolet (2004), whereby rather moderate anchoring effects are shown for valuation tasks when compared to disparities in WTA/WTP due to anchoring. Consequently, in the domain of economic valuations, anchoring effects appear to be more fragile than in judgmental domains. Since our design implements monetary incentives, their effects need mentioning. However, no clear consensus has emerged so far: Tversky and Kahneman (1974), Wilson et al. (1996) and Epley and Gilovich (2005) offer prices for unbiased decisions and find no debiasing effects. In contrast, Wright and Anderson (1989), Simmons et al. (2010) and Meub and Proeger (2016) find subjects to be less biased when given monetary incentives and a realistic opportunity of achieving better solutions through increased cognitive effort. Bias reduction through group cooperation in psychological experiments is reviewed by Kerr and Tindale (2004). While for some domains groups are less biased, such as for the hindsight bias (Stahlberg et al. 1995) and the overconfidence bias (Sniezek and Henry 1989), the overall heterogeneity of the results and experimental paradigms preclude general predictions as to whether groups more effectively avoid behavioral biases. For anchoring in groups, studies in two domains have been carried out. In a study of legal juries, Hinsz and Indahl (1995) report that legal judgments made by groups are substantially biased by anchors provided during trials and that there is no difference to the judgments made by individuals. Whyte and Sebenius (1997) also find that for a non-incentivized negotiation exercise, groups acting as a single party are equally biased as individuals. Accordingly, groups fail to effectively use competing anchors to debias their judgment; rather, they compromise between various distorted individual judgments, making the overall result similarly biased. In contrast to psychological studies, economic small group research offers fairly clear predictions for group performance, yet so far provides no evidence in terms of biases and groups. Reviewing the past ten years of economic group experiments, Charness and Sutter (2012) and Kugler et al. (2012) summarize that groups overall are more successful than individuals in achieving game-theoretical requirements for rational decision-making by alleviating cognitive limitations of individuals, as collaboration enables more rational decisions through the transfer of insight from cognitively superior individuals to the group. The effectiveness of this mechanism crucially depends on the demonstrability of task solutions (Laughlin et al. 2002). Hence, groups consistently outperform individuals in intellective tasks with a clear and demonstrable correct solution. The counterpart are judgmental tasks that have more ambiguous answers, whose solutions are not easily demonstrable to other persons (Cox and Hayne 2006). Group performance then depends on the respective task’s position on a continuum from intellective to judgmental (Laughlin 1980). Consequently, groups can mitigate individuals’ bounded rationality through the transfer of information and it can be hypothesized that groups circumvent the anchoring bias through improved intra-group information availability. The positive effect of “more heads” on the overall cognitive performance thus leads to the expectation that groups will be less biased by external anchors. Accordingly, there are two contradictory notions to be derived from previous research. The bulk of psychological research on anchoring effects in individuals and groups leads to the prediction that groups are unlikely to avoid the bias, regardless of monetary incentives. Group cooperation would thus fail to alleviate the bias regardless of task characteristics. Also, the active discussion of anchor values might as well foster the activation of anchor-consistent knowledge and even increase anchoring effects, e.g. through group polarization (Luhan et al. 2009). By contrast, following economic small group research, the cognitive superiority of groups would predict that groups successfully avoid external anchors as additional information becomes available within groups. To account for these contradictory notions, we present an anchoring study comprising three different anchoring exercises that are compatible with previous psychological experimental designs and also with the economic domains discussed recently in anchoring field experiments. In this setting, we compare the performance of individuals and three-person teams in terms of their ability to avoid anchors. We find that groups are significantly less biased for an intellective factual knowledge task. For probability estimates and price valuations, individual and group decisions are equally biased; accordingly, individual biases are perpetuated by group cooperation. It appears that a group’s ability to reduce individual biases depends on task characteristics. In the case of intellective tasks that have a clearly defined correct solution, debiasing is effective. For tasks with judgmental elements, groups approach the performance of average individuals. Overall, we suggest that groups are ‘less behavioral’ than individuals in certain domains, yet that this optimistic assumption prevalent in the literature should clearly not be generalized to all domains of decision-making and biases. The remainder of the paper is structured as follows: in section two, our experimental design is described, while section three details our results and section four concludes.",6
85.0,2.0,Theory and Decision,26 January 2018,https://link.springer.com/article/10.1007/s11238-018-9653-0,Behavioral patterns and reduction of sub-optimality: an experimental choice analysis,August 2018,Daniela Di Cagno,Arianna Galliera,Noemi Pace,Female,Female,Female,Female,"Accepting that sub-optimality is nearly universal (see Conlisk 1996; Selten 2001; and Gigerenzer 2006 for a review of the literature), we attempt to identify behavioral patterns to improve decision success, by reducing the distance between individuals’ actual choices and the optimal ones. In this paper, the individual data previously used by Di Cagno et al. (2017) via aggregating across individuals are analyzed to assess individual heterogeneity in decision-making using several criteria of behavioral improvement and of reasonable parameter dependence (Erev and Haruvy 2013; Conte et al. 2015). The experimental choice tasks are designed to unambiguously distinguish optimal and sub-optimal choices and to avoid various idiosyncratic characteristics by which one can rationalize such individual heterogeneity in decision-making. Specifically, the decision tasks experimentally induce risk neutrality, have only individual consequences and focus on numerical parameter constellations letting optimal choices not depend on probability (information). We additionally distinguish optimality with and without intra-personal payoff aggregation and partly elicit and incentivize aspiration data.Footnote 1 Moreover, since participants confront successively twice fifteen different and randomly ordered choice tasks, we also can assess experience effect. The experimental setup is designed to explore fundamental aspects of decision theory. The decision tasks are framed as portfolio choicesFootnote 2 and employ binary lottery incentives.Footnote 3 Given an initial endowment, each subject allocates it between a risky asset and a risk-free asset; both investment returns determine the probability (in points) of winning the higher payoff (€14), rather than the lower payoff (€4), in both the good and the bad state. The investment is set-optimal when one cannot obtain more in one state without having to reduce what one obtains in the other state. Since deriving the optimal investment, i.e., the investment maximizing the expected utility across both states, may be cumbersome, participants could repeatedly use a slider before committing to one slider position. Satisficing is testable in the experimental treatments eliciting aspiration levels and incentivizing optimal aspiration formation. Rather than only one aspiration in expected utility terms, participants form payoff aspirations for each realization of the binary random event (the good and the bad state), which allows to test set-optimal satisficing. Satisficing, however, must not be (set)-optimal: an investment choice is satisficing when its success in each random state guarantees the respective aspiration, i.e., when the investment return in each random state is sufficient for what one aspires. Our previous aggregate data analysis concluded that, in spite of the weak rationality requirements, optimality and optimal satisficing are rare, and there is striking heterogeneity in the individual behavior of participants within and across treatments. The aim of this study is to investigate whether and how individual participants are able to improve their investment decisions via reducing the distance between their actual choices and the optimal ones. Since the familiar tools for rationalizing individual heterogeneity in behavior are experimentally controlled or excluded, we essentially categorize boundedly rational behavior via criteria of behavioral improvement to confirm that participants learn and thereby enhance their success. Although we are not born optimizers, we may still improve our success by learning. To achieve this aim, we assess three basic criteria of behavioral improvement (improving slider use, reducing antimonotonicity, improving when experiencing the same task again). Improving according to these criteria should enhance individual success. In the treatments eliciting aspiration levels, we additionally investigate improvement in aspiration formation. Altogether, we find that our criteria of behavioral improvement are correlated and enhance average success in terms of investment choices and the consequent payoffs. The rest of the paper is organized as follows. Section 2 formally describes the choice tasks. Section 3 presents the experimental protocols. Section 4 specifies the improvement criteria, and Sect. 5 reports the evidence for the selected criteria, their correlations and their use in identifying different behavioral types. Section 6 assesses how improvement affects average success. Section 7 concludes.",1
85.0,2.0,Theory and Decision,02 January 2018,https://link.springer.com/article/10.1007/s11238-017-9646-4,The joy of ruling: an experimental investigation on collective giving,August 2018,Enrique Fatas,Antonio J. Morales,,Male,Male,Unknown,Male,"Charitable giving in the United States accounted for over $373 billion in 2015, which was 2.1% of the gross domestic product.Footnote 1 The extent of donations once puzzled economists because generous behaviour was difficult to reconcile with traditional models based on rational selfish behaviour. Hochman and Rogers (1969) and Kolm (1969) addressed this puzzle by considering that charitable giving created a public good. Once the public benefit from charity becomes an argument in individual preferences, the act of giving may be purely rational. A large body of the literature on charitable giving has focused on individual donors, who select the amount of their endowment to contribute to a public good on their own.Footnote 2 This individual approach to charitable giving is applicable in three out of four charitable gifts in the States, as individuals account for 75% of all charitable giving. However, the remaining 25% ($90 billion) comes from organizations and foundations, among others. In this paper we try to explore how different decision rules within groups or teams of individuals may influence the final donation. The small amount of experimental literature on team dictator games highlights the intricacies of the interaction within groups. Cason and Mui (1997) find that groups give more than individuals while Luhan et al. (2009) find the reverse in a fairly unstructured decision process (face to face communication and a chat mechanism, respectively). In this paper, we study how the heterogeneity of the members’ social preferences interact within a structured voting process when a variety of decision rules are considered. In a standard team dictator game n players decide how much of the group endowment nw to contribute to a public good, keeping an equal share of the rest. We reframe this setting to ask groups of n individuals, each endowed with w, to decide on a common contribution to a public good. In a sense, our framing is similar to the one used in the political economy literature of taxation, as in Romer (1975) and Roberts (1977) in which the median voter determines the tax rate.Footnote 3
 In this paper, we consider a variety of social choice mechanisms where group members vote on the common amount they all have to donate, including the extreme case in which the donation is picked randomly (the super dictatorial, see below). Our rules span from one favouring selfish participants (the mechanism MIN picks the smallest vote), to one favouring more altruistic individuals (the mechanism MAX selects the largest vote in the group). An intermediate rule is the mechanism AVG, in which the donation assigned by the mechanism is the average vote.Footnote 4
 One interesting characteristic of all these mechanisms we analyse is that a standard BayesianFootnote 5 analysis shows that they are vote equivalent: the MIN and MAX mechanisms are dominance solvable and the dominant strategy is the donation they would choose in the super-dictatorial mechanism. We find that vote equivalency is consistently violated in two different lab experiments: votes in MIN mechanisms are smaller than those in the AVG mechanism, which in fact are smaller than the votes in the MAX mechanism. In addition, votes do not depend on the size of the group. One possible explanation for the violation of the equivalence result comes from a common feature of the MAX and MIN mechanisms: a single player determines collective decisions. In other words, both mechanisms generate single winners. A joy-of-winning, defined as the extra utility that a player gets from winning the auction or contest, has been shown to explain overbidding in first price auctions (Cooper and Fang 2008) and overinvestment in contest games (Dechenaux et al. 2012). In a similar way, we define joy of ruling as the extra utility that a group member gets from winning the competition to rule. Any extra utility from becoming the ruler immediately implies that votes in the MIN and MAX mechanisms will differ because members, rather than choosing their dominating voting strategy, will choose lower (higher) votes in the MIN (MAX) mechanism to rule and gain the extra utility. The rationale for this joy of ruling is not far from recent work on the intrinsic value of decision rights. In this branch of the literature, subjects are willing to pay non-negligible amounts of money, beyond its instrumental benefits, to rule over others (Fehr et al. 2013; Bartling et al. 2014), and to avoid being controlled by other subjects (as in the control premium found by Owens et al. 2014). The suboptimal levels of delegation (as in Coats and Rankin 2016, or Bobadilla-Suarez et al. 2016) may be related to a willingness to retain (illusory) control on outcomes (as in Sloof and Siemens 2014). In our experiments, we elicit the joy of ruling by auctioning the right to impose the super-dictatorial donation of a subject on all participants in the session. In addition, we elicit subjects’ types—i.e., their donation in a standard dictator game—and carefully control for their beliefs on other subjects’ types. Our measure of joy of ruling (subjects’ bids) significantly predicts the probability that a participant will cast different votes in the MIN and MAX mechanisms. A structural model controlling for the endogeneity of beliefs (in which beliefs are assumed to depend on type) shows that joy of ruling is particularly strong in the MAX mechanism. Beliefs increase with type in all mechanisms, confirming the false consensus effect observed in other studies of group behaviour (as in Gächter et al. 2012). Interestingly, the super-dictatorial mechanism shows that we should be careful when designing collective giving institutions. Altruistic individuals feel responsible for others (as documented by Charness and Jackson 2009) and tend to act as benevolent dictators, driving down their dictatorial decisions to concur with the decisions made by others. The rest of the paper is as follows. Section 2 contains the theoretical analysis. Sections 3 and 4 describe the experimental design and discuss the experimental results. Finally, Sect. 5 concludes.",2
85.0,2.0,Theory and Decision,20 October 2017,https://link.springer.com/article/10.1007/s11238-017-9630-z,Three-valued simple games,August 2018,M. Musegaas,P. E. M. Borm,M. Quant,Unknown,Unknown,Unknown,Unknown,,
85.0,2.0,Theory and Decision,17 July 2017,https://link.springer.com/article/10.1007/s11238-017-9624-x,Two simple characterizations of the Nash bargaining solution,August 2018,Osamu Mori,,,Male,Unknown,Unknown,Male,"As is well known, Nash (1950) proposed a solution to the two-person bargaining problem, comprising a utility possibility set and a disagreement point, and proved that it is characterized by four axioms, i.e., symmetry, scale invariance, independence of irrelevant alternatives, and weak Pareto optimality. Although the weak Pareto optimality axiom appears to be an uncontroversial requirement, it has been shown that it can be substituted by another axiom, while the other three axioms remained unchanged. Roth (1977) replaced weak Pareto optimality with strong individual rationality in characterizing the Nash solution. More recently, Anbarci and Sun (2011) showed that the Nash solution can be characterized using weakest collective rationality, which is weaker than both weak Pareto optimality and strong individual rationality.Footnote 1 In this study, we provide two characterizations of the Nash bargaining solution, which improve on Nash (1950), Roth (1977), and Anbarci and Sun (2011). First, we introduce a new axiom, strong undominatedness by the disagreement point, which requires that the bargaining solution be neither weakly dominated by nor equal to the disagreement point. This axiom is weaker than both weak Pareto optimality and strong individual rationality, but it neither imply nor is implied by weakest collective rationality. However, strong undominatedness by the disagreement point seems more intuitive than weakest collective rationality. Moreover, if the bargaining problem allows for the case that the set of utility possibilities is not compact,Footnote 2 then weakest collective rationality together with three standard conditions is not sufficient for characterizing the Nash solution. We then show that the Nash solution is characterized by symmetry, scale invariance, independence of irrelevant alternatives, and strong undominatedness by the disagreement point, assuming that the set of utility possibilities is closed, convex, and bounded above. Second, we replace the independence of irrelevant alternatives axiom with two axioms, the sandwich axiom and egalitarian Pareto optimality. The sandwich axiom, proposed by Rachmilevitch (2016), is a weakening of independence of irrelevant alternatives. Egalitarian Pareto optimality, which is another new axiom, requires that the efficient solution be chosen if the bargaining problem is such that the utilities of the two persons are equal in all utility points. We then show that the Nash solution is characterized by symmetry, scale invariance, strong undominatedness by the disagreement point, the sandwich axiom, and egalitarian Pareto optimality.",2
85.0,2.0,Theory and Decision,31 August 2017,https://link.springer.com/article/10.1007/s11238-017-9628-6,Rationality with preference discovery costs,August 2018,Matthew S. Wilson,,,Male,Unknown,Unknown,Male,"“Preferences are complete and transitive.” This assumption is one of the cornerstones of microeconomic theory. However, experiments routinely find that human behavior does not conform to this axiom perfectly (Battalio et al. 1973; Andreoni and Miller 2002; Harbaugh et al. 2001). These results have been known for a while, but standard theory continues to rely on rationality. Perhaps this is because rationality seems so reasonable. How can Bundle A be both better and worse than Bundle B? It is similarly difficult to believe that someone can deliberately prefer X over Y and Y over Z, and yet find Z better than X. Alternatively, preferences could be rational but people might not pick the option that is most preferred. However, this also appears nonsensical. Why would anyone intentionally choose an inferior combination of goods, knowing that superior alternatives are available? Here is one response that economists frequently give: “Yes, there certainly are rationality violations in laboratory experiments with small stakes. However, if there was repetition or if the stakes were higher, people would learn from their mistakes and the anomalies would go away.” A number of studies have tested this claim. However, the evidence is not as clear cut as many economists commonly believe. A heavily studied example is the preference reversal phenomenon. It is usually discussed in the context of risk theory, but it can also be interpreted as a violation of completeness and transitivity. A subject chooses between a “P-bet” (safe) and a “$-bet” (risky). Then, the experimenter elicits the subject’s valuation of the bets. In many cases, the subject places a higher value on the risky bet, but chooses the safe bet. This reveals the following preferences: P-bet > $-bet > P-bet. Cox and Grether (1996) and Loomes et al. (2003) both found that preference reversals occurred less frequently as the experiment was repeated. However, repetition does not solve everything. Braga et al. (2009) reported their results in the title of their paper: “Market Experience Eliminates Some Anomalies—And Creates New Ones.” As subjects gained experience, a different kind of preference reversal occurred. Now people would often choose the risky bet, but they placed a higher value on the safe bet. Completeness and transitivity are still violated. Plott (1996) discussed the effect of repetition on rational behavior, surveying the literature on a wide variety of experiments. He found that violations tended to decrease with repetition. Nevertheless, some violations still remained at the end of the experiments. 
Camerer et al. (1999) surveyed 74 papers on the effect of raising the stakes. One of their results (emphasis in the original): “We also note that no replicated study has made rationality violations disappear purely by raising incentives.” However, they did note many cases where higher stakes lead to lower variance. Some of the outliers vanish once subjects have a larger financial motivation, but this is not the same as eliminating all the anomalies. Smith and Walker (1993) reviewed 31 studies and found that higher stakes were associated with fewer rationality violations. Another response to anomalies is to downplay their significance. Many indices have been developed to measure the severity of a violation (Varian 1991; Afriat 1972). Intransitive choices waste money; the indices measure the fraction of the wealth that is wasted. According to these metrics, the magnitude of the violations found in experiments is quite small (Battalio et al. 1973; Varian 1991; Harbaugh et al. 2001). For instance, Varian (1991) demonstrates that in Battalio’s (1973) dataset, people seldom waste more than 5% of their endowment. Andreoni and Miller (2002) and Harbaugh et al. (2001) reach the same conclusion in their experiments. I draw three main conclusions from the literature on rational choice: Rationality violations do exist and have been replicated under many conditions. Giving the subjects higher stakes or the opportunity to learn from experience might reduce the number of violations, but it will not eliminate them entirely. The violations are small in magnitude. Because departures from rationality are so small, perhaps we only need a slight modification of the theory. I propose adding preference discovery costs to the standard model of rational choice. In this new model, people do have rational preferences. However, accessing those preferences is costly. It takes effort to figure out what you want, and you dislike this work. However, if you put in more effort, you will likely receive a bundle that is better, i.e., one that gives higher utility according to your true (but imperfectly known) preferences. Effort is chosen optimally. Agents weigh the disutility of effort against its expected benefits. The cost of effort is known, but the expected benefits may be uncertain. Subjective beliefs are formed about the rewards of increased effort. These beliefs are updated optimally after consumption. Since picking the most preferred bundle requires costly introspection, using less than full effort could be optimal. Such behavior could lead to apparently intransitive choices, but perhaps the individual is actually rational once preference discovery costs are accounted for. This could explain the small deviations from standard theory that we observe in the data while still retaining rational preferences and optimization. Since these assumptions are crucial to economics, it is important to show that they are compatible with the data. Other efforts have been made to incorporate the costs of decision making. Their existence is widely acknowledged in many disciplines (Hogarth and Karelaia 2005; Bröder and Schiffer 2003). Due to these decision-making costs, many papers outside of economics argue that people do not optimize; instead, they rely on heuristics. However, in my model, the choice between optimization and heuristics is a false one. People use heuristics because they are optimal when preference discovery costs are present. One feature of my model is that it does not require that we impose any functional form on the heuristic. However, economists have made only a few attempts to model decision-making costs. Conlisk (1988) uses “optimization costs” that are a function of the time required to make a decision. Since it is one of the first papers in the genre, there is no general theory or experiment to test it; only a few applications are included and discussed. In all disciplines, a common objection to these models of decision-making cost is the “infinite recursion problem,” which Conlisk (1988) mentions frequently. If there is a cost to making a decision, why is there not also a cost to deciding how much effort to use in making the decision on effort? Similarly, there could be a cost to deciding how much effort to use in deciding how to decide on the original question, ad infinitum. Most of these models have no response other to assume it away. One feature of my model is that it can accommodate any finite number of recursions. 
Smith and Walker (1993) introduce a model of decision-making cost and find indirect evidence for it. They review 31 experiments in which different groups of subjects faced different levels of cash payoffs. As the potential payouts rose, the number of deviations from rationality consistently fell (Smith and Walker 1993). This is exactly what we would expect if effort is costly. Raising the stakes increased the benefits of making good decisions, so subjects were willing to invest more effort. Plott (1996) takes a very similar approach, but unlike Smith and Walker (1993), he does not include a theoretical model to explain the violations. In Smith and Walker’s (1993) model, preferences remain rational, but effort is not chosen optimally. Rather, the unobserved decision cost is a friction that allows choice to deviate from rational preferences. They write, “\(\varepsilon \) is not ‘error’ from the point of view of the subject weighing (albeit unconsciously) benefit against decision cost. It is the experimentalist who interprets \(\varepsilon \) as a prediction error of the theory” (Smith and Walker 1993). However, economists are certainly inclined to retain optimization if possible. The assumption that agents optimize is also relaxed in Evans and Ramey (1998). The model is about firms setting their expectations when calculation costs are present, but the same ideas can be applied to consumers making choices when there are decision costs. As calculation rises, firms make better estimates. Rational expectations are a limiting case when calculation approaches infinity. Firms use a heuristic; when the gain from a previous level of calculation exceeds a target level, the firm continues to calculate, but when it falls below the target, further calculation stops. One of their earlier papers kept optimization (Evans and Ramey 1992). A difficult issue in this model and in other models of decision cost is how to determine effort/calculation optimally. It is easy to speak of agents weighing the benefits of higher quality decisions against the cost of effort, but the mathematics reveal a stubborn problem. How can agents be aware of the utility gain from more effort and yet be ignorant of their utility function? However, if the utility function is known, then the solution to the consumer’s problem is already given by standard theory. Choosing anything else cannot be optimal. Hence, any such theory cannot explain the intransitivities we observe in the real world. Evans and Ramey (1992) had to assume the problem away. Firms costlessly knew the benefits of reoptimizing without knowing what reoptimization would require them to do. This makes the math work and also yields some implications for policy that the authors discuss. In several classical models with rational expectations, policy is neutral; firms anticipate and offset the government’s actions. However, if calculation is costly, then policy may have limited effectiveness (Evans and Ramey 1992). If the policy changes are small, then the gain from reoptimizing will not be enough to recover the calculation cost. Firms prefer sticking to their previous plan rather than offsetting the policy change (Evans and Ramey 1992). Perhaps similar results can come from a theory of consumers with decision costs. Firms and the government may extract some benefits from consumer irrationality, but if they keep trying to increase those benefits, consumers may increase their decision-making effort and the intransitivities will vanish. There is a related literature in risk theory on intransitive choices over lotteries. These non-expected utility models take a different approach to explaining anomalies. To see the difference, break up revealed preference axioms into two components: People have complete, transitive, and monotonic preferences In every budget set, people pick the bundle that they prefer the most. I.e., choices reveal preferences Standard theory starts out by distinguishing between preferences and choice, but then the second assumption unifies the two. Most of the behavioral theories keep the second assumption but relax the first one. A key element is probability weighting, introduced by Kahneman and Tversky (1979). The agent does not use the actual probabilities in the lottery; instead, the probabilities are transformed using some function. Viscusi (1989) demonstrated that a Bayesian approach to probability weighting can resolve many of the challenges to the Expected Utility Theorem. However, intransitivity is not addressed. Bordley (1992) generalizes the model to accommodate intransitivity and several other paradoxes. Both models can explain violations of monotonicity, but that is a weakness rather than a strength. Consider the two lotteries in Table 1. Clearly, the two lotteries are the same. However, if agents weight the probabilities, then they will almost surely fail to realize this. To see this, let u(x) be the agent’s von Neumann–Morgenstern utility function; \(w_{f} ( q )\) is the probability weighting function. Then, the expected weighted utility from the first lottery is \(w_{f} ( {0.3} ) u( {10} )+w_{f} ( {0.7} ) u( 0 )\). For the second lottery, it is \(( {w_{f} ( {0.2} )+w_{f} ( {0.1} )} ) u( {10} )+w_{f} ( {0.7} ) u( 0 )\). These will always be equal if \(w_{f} ( {0.2} )+w_{f} ( {0.1} )=w_{f} ( {0.3} )\). But this is not true in either model, except in the special case where they reduce to the Expected Utility Theorem. The same problem arises in other probability weighting models. The reason they allow possibilities such as \(w_{f} ( {0.2} )+w_{f} ( {0.1} )\ne w_{f} ( {0.3} )\) is that they want to explain why people overweight low probabilities and other paradoxes. The drawback is that in the model, people will make obvious and unrealistic violations of monotonicity. Suppose that Lottery 2 is strictly preferred to Lottery 1 and preferences are continuous. Then, there exists \(\varepsilon >0\) such that Lottery 1’s payoff of $10 can be changed to $ \(( {10+\varepsilon } )\) and the agent will still prefer Lottery 2. People do commit violations of montonicity and transitivity—but not when it is that obvious. Models that rely on the second assumption (choices reveal preference) cannot explain this. If choices reveal preference and preferences are intransitive, then violations will occur even in the most obvious cases. Tversky and Kahneman (1986) demonstrated the effect of transparency on violations. Table 2 shows some of the lotteries they used. In one part of the experiment, subjects chose between A and B. Without exception, they picked B. In that case, it is easy to compare the outcomes and probabilities across lotteries, so no one committed any violations. Later, they had to pick between C and D. Here, it is not quite so clear if one lottery dominates the other. The majority chose C, but a more careful look reveals that D dominates C. Thus, violations do occur, but only when people fail to exert full effort. This suggests that the problem with standard theory is not that preferences are irrational. People avoid violations as long as they are obvious or if enough effort is applied. The problem is that people do not always choose according to their underlying rational preferences. This is due to preference discovery costs. Thus, the model in this paper is more realistic than ones with intransitive preferences. There is another attempt to retain rationality: change of tastes models. It is easy to see how this might explain intransitivities. For example, after consuming Brand B, you find it better than expected, leading you to consume more of it and make choices inconsistent with your previous decisions. Conlisk (1996) observes that these models are very similar to ones with imperfect information about stable preferences, such as DeGroot (1983). “I changed my behavior because my unstable preferences changed” is difficult to distinguish from “I changed my behavior because I learned more about my underlying stable preferences.” However, this does not fully address the violations that occur in the laboratory. In a common procedure, used in Harbaugh et al. (2001) and others, subjects do not consume their payoff until the end of the experiment. None of the mechanisms that are used to explain why tastes shift—social preferences, culture, emotions (Jacobs 2016)—have greatly changed within the experiment. In addition, the participant has not gained more information about their preferences for the good, since they have not consumed it yet. In short, within such an experiment, preferences should be stable and choices should be consistent. However, intransitivities remain. This suggests that changing tastes are not the source of all intransitivities. Ballinger and Wilcox (1997) discuss a similar issue in their paper on risk: “[M]any subjects’ choices are still inconsistent across identical trials of choice problems... This suggests that, for whatever reason, discrete choice is inherently probabilistic.” The reason is preference discovery cost. When subjects do not use maximum effort, they make random errors in their choices. This is why my model can explain such behavior while change of tastes models cannot. Standard theory is incompatible with the intransitive choices observed in the data. However, many of the models that try to resolve these anomalies have issues of their own. If probability weights are used, then highly implausible violations of monotonicity can occur. Change of tastes models do not explain all of the intransitivities in the laboratory. Preference discovery cost models avoid these flaws. However, they have challenges of their own: the recursion problem and the difficulty of selecting optimal effort with an unknown utility function. The model in this paper solves both of those problems. Since rationality and optimization are retained, it is a minimal modification of standard theory, and yet it still reconciles rational preferences with the data.",
85.0,2.0,Theory and Decision,06 November 2017,https://link.springer.com/article/10.1007/s11238-017-9637-5,On nonexistence of reconsideration-proof equilibrium with state variables,August 2018,Wataru Nozawa,,,Male,Unknown,Unknown,Male,"No commitment can be an issue in problems in which the decision maker at one point in time has a conflict of interest with himself at another point in time. The first economic analysis of such a problem is done by Strotz (1955). He analyzes a saving problem in which the decision maker faces the intrapersonal conflict due to hyperbolic discounting. Kydland and Prescott (1977) show that such a conflict can arise also in macroeconomics policy problems through strategic interaction between the policymaker and the public. For solving such problems, subgame perfection has been used, treating the decision maker at different time periods as different selves playing a non-cooperative game (see, for example, Laibson 1994). Though the use of subgame perfection is successful in capturing the central feature of the problems that the decision maker cannot bind future self, it has problems. At a conceptual level, subgame perfection has been criticized for prohibiting coordination among different selves: the selves are allowed for deviating only unilaterally. As a result, the decision maker is not able to discard a current strategy even if there is a better self-enforcing alternative. At a practical level, subgame perfection is criticized for its multiplicity, which is a consequence of the limited ability of the decision maker. Subgame perfection is permissive for bad equilibria, which are often useful as punishments that support some good outcomes as equilibria. To address these problems, Kocherlakota (1996) proposed a refinement which is called strong reconsideration-proofness throughout this dissertation. It assumes that the decision maker can choose a strategy from any credible strategy. If he can do so only at the initial node, he will choose the best subgame perfect strategy. Strong reconsideration-proofness, instead, assumes that he is able to do so at every decision node. The ability implies that, in a stationary environment, any equilibrium strategy should achieve the same value at any node; otherwise, at some nodes, the decision maker would reconsider his strategy. He would find that a better alternative continuation is prescribed at some other nodes and would switch to it. Subgame perfect strategies that achieve the same value at any node are called weakly reconsideration-proof. Strong reconsideration-proofness is defined as the best weakly reconsideration-proof strategy. Under reconsideration-proofness, the ability of the decision maker is more similar to that in standard dynamic optimization problems than that under subgame perfection. Possible deviations are not restricted to be in action in one period, and he can change his entire strategy as long as the alternative is weakly reconsideration-proof. A consequence of the similarity is that the value attained by strongly reconsideration-proof equilibria is unique. At least to that extent, strong reconsideration-proofness refines subgame perfection. There is another refinement, which is called revision-proofness. It was first defined by Asheim (1997) and developed by Ales and Sleet (2014) later. Revision-proofness allows a deviation if it is Pareto improving for the current and future selves on the resulting equilibrium path. Unlike reconsideration-proofness, revision-proofness does not allow the decision maker to make a change in his strategy which hurts future selves. There are two reasons why this paper is about not revision-proofness, but reconsideration-proofness. Firstly, reconsideration-proofness is less developed than revision-proofness. Reconsideration-proofness has even not been defined for non-stationary environments. An important question is which solution concept is more attractive, but it has not been answered much because of the lack of the development. The second reason is that it is known that revision-proofness sometimes does not refine subgame perfection much. Ales and Sleet (2014) show that in a certain class of environments, many subgame outcomes are also supported as revision-proof outcomes. Though that is just a quite partial answer for the question, for which solution concept is more attractive, it motivates an exploration of alternatives. The definition by Kocherlakota (1996) has an important limitation: it is not defined for problems with state variables. The limitation is important because many problems in which no commitment is an issue, such as capital taxation problem, monetary policy problem, or saving problem with non-geometric discounting, naturally have state variables. This paper displays a natural generalization of Kocherlakota’s reconsideration-proofness to environments with state variables that evolve endogenously and deterministically, and shows that it does not exist in three examples. The nonexistence result contrasts with the general existence theorem obtained in environments without state variables in Kocherlakota (1996). In these examples, there is a pair of weakly reconsideration-proof strategies, 1 and 2, such that strategy 1 is better than strategy 2 in some states, while strategy 2 dominates strategy 1 in other states, and no weakly reconsideration-proof strategy dominates both of them. In the first two examples, the domination between the pair of strategies is asymmetric in a specific sense. In these examples, I discuss that strategy 2 should not be regarded as a legitimate alternative strategy because of the asymmetry, and the fact that strategy 2 is better than strategy 1 at some states should not prevent strategy 1 from being reconsideration-proof. The idea used to select strategy 1 as reconsideration-proof strategy in each of the two examples is generalized in a modified definition of reconsideration-proofness, which is called conditionally strong reconsideration-proofness. In the third example, the domination between the pair of two strategies is symmetric, and even conditionally strongly reconsideration-proof strategy does not exist. It is difficult to discuss which strategy should be selected as a reconsideration-proof equilibrium due to the symmetry. That tells us that any reasonable modification, which I suppose must rely on some kind of an asymmetry in the way of domination, to the solution concept would not lead to the general existence problem, and restricting to some special environments seems to be necessary.",
85.0,3.0,Theory and Decision,19 March 2018,https://link.springer.com/article/10.1007/s11238-018-9657-9,The strength of sensitivity to ambiguity,October 2018,Robin Cubitt,Gijs van de Kuilen,Sujoy Mukerji,,Male,Unknown,Mix,,
85.0,3.0,Theory and Decision,20 March 2018,https://link.springer.com/article/10.1007/s11238-018-9658-8,Ambiguous life expectancy and the demand for annuities,October 2018,Hippolyte d’Albis,Emmanuel Thibault,,Male,Male,Unknown,Male,"According to the life-cycle model of consumption with uncertain lifetime proposed by Yaari (1965), full annuitization should be the optimal strategy followed by a rational individual without altruistic motives, provided that annuities are available. Since this theoretical prediction does not meet the facts [see Benartzi et al. (2011), Inkmann et al. (2011), Peijnenburg et al. (2016) or Previtero (2014)], the initial framework has hence been extended and many explanations of the so-called annuity market participation puzzle have been proposed [see Brown (2007) or Sheshinski (2008)]. However, as shown by Davidoff et al. (2005), positive annuitization still remains optimal under very general specifications and assumptions, including intergenerational altruism. According to the authors, the literature to date has failed to identify a sufficiently general explanation for consumers’ aversion to annuities, suggesting that psychological or behavioral biases might be rather important in decisions involving uncertain longevityFootnote 1. In this paper, we consider the possibility that a fully rational individual displaying some aversion toward ambiguous survival probabilities may be likely to exhibit a low demand for annuities. Indeed, in a static life-cycle model with a bequest motive, we show that a zero annuitization strategy is optimal if the ambiguity aversion is sufficiently large. Instead, it is optimal to sell annuities short or, under some conditions [see Yaari (1965) and Bernheim (1991)], to purchase pure life insurance policies. For the sake of simplicity, we do not make any distinction between the two financial products and only deal with the demand for annuities. Before detailing our results, let us first discuss the two main assumptions of the paper: the uncertainty on survival probabilities and the aversion toward this ambiguity. Despite all the available information displayed in life tables, we think that survival probabilities are, nevertheless, ambiguous for individuals. First, there is a rather strong heterogeneity in the age at death. According to Edwards and Tuljapurkar (2005), past age 10, its standard deviation is around 15 years in the US. After having controlled for sex and race differentials and for various socioeconomic statuses, they found a residual heterogeneity that remains significant. Heterogeneity is notably explained by biological differences that are not necessarily known to the individual. According to Post and Hanewald (2013), this objective heterogeneity is positively linked to individuals’ subjective estimate of survival probabilities. Second, the large increase in life expectancy experienced by populations over the last two centuries was characterized by changes in the distribution of survival probabilities at each age. This is referred to as the epidemiological transition and features an increase in the dispersion of heterogeneity computed in the later years. Moreover, opposite factors such as medical progress versus the emergence of new epidemic diseases cause some uncertainty in the dynamics of the distribution per age. Based on life tables, an individual belonging to a given cohort may, at best, only know the true distributions of past cohorts but remains uncertain about his/her own. Moreover, due to the small number of observations, data concerning the much later years are not reliable and there is no consensus among demographers about forecasts of the mean survival rate [see especially Oeppen and Vaupel Peijnenburg (2016)]. The longevity risk, defined as the risk that people live longer than expected, is according to the IMF (2012), systematically underestimated by governments and pension managers. The assumption of an aversion toward the ambiguity of survival probabilities is also supported by a great deal of evidence. Initial intuitions concern health risks and can be found in the study by Keynes (1921), in which he considers patients who must decide between two medical treatments. Keynes argues that most individuals would choose a treatment that has been extensively used in the past and has a well-known probability of success, rather than a new one, for which there is no information about its probability of successFootnote 2. Moreover, the celebrated Ellsberg (1961) experiment has also been applied to health and longevity risks in many studies using hypothetical scenarios. Among them, Viscusi et al. (1991) show that individuals have a significant aversion to ambiguous information about the risk of lymphatic cancer. More recently, real case studies have confirmed that individuals are ambiguity averse. Riddel and Shaw (2006) have found a negative relationship between the perceived uncertainty about the risks associated with nuclear waste transportation and the willingness to be exposed to such risks. Similarly, scientific disagreement about the efficiency of vaccination [see Meszaros et al. (1996)] or screening mammography recommendations [see Han et al. (2006, 2007)] has been found to be negatively correlated with the perception of disease preventability and the decisions of preventive behaviors. We consider a life-cycle model with consumption and bequest similar to Davidoff et al. (2005), except that we eliminate consumption in the first period. This eases a lot the derivation of our theoretical results. Moreover, we do not focus on market imperfections but instead assume, as in Yaari (1965), some warm-glow altruism. Remark that a bequest motive is necessary to obtain some partial annuitization but it does not eliminate the advantage of annuities since they return more, in case of survival, than regular bonds. Except for using a static model, our main departure from Yaari (1965) and Davidoff et al. (2005)’s works hinges on the assumption of ambiguity aversion toward uncertain survival probabilities. We apply the recent model for ambiguity aversion proposed by Klibanoff et al. (2005) to a decision problem in a state-dependent utility framework yield by uncertain lifetimes. The representation functional is an expectation of an expectation: the inner expectation evaluate the expected utilities corresponding to possible first-order probabilities while the outer expectation aggregates a transform of these expected utilities with respect to a second-order priorFootnote 3. We obtain the following results. Provided that annuities’ return is sufficiently larger than bonds return, and notably when it is fair, the optimal share of annuities in the portfolio is positive if the individual is ambiguity neutral. This is not surprising as the individual maximizes a standard expected utility computed with a subjective mean survival probability. Conversely, as the index of ambiguity aversion of Klibanoff et al. (2005) increases, the optimal demand for annuities decreases and there exists a finite threshold above which the demand is non-positive. Our results are obtained under general specifications of both utility functions and survival probability distributionsFootnote 4. The aversion to ambiguous survival probabilities hence appears as a good candidate to explain the observed aversion to annuities. A numerical application of our model suggests that the impact of ambiguity aversion is likely to be quantitatively large. Our results strongly differs from those obtained with an expected utility framework, may the survival probability be deterministic (as in Yaari 1965) or stochastic (Huang et al. 2012; Post 2012), except in Bommier and Grand (2014) who consider agents that are averse to a risk on the length of life. They show that the demand for annuity decreases with risk aversion and can be negative. Applications of non-expected utility models to health and longevity uncertainties have been scarce. Among exceptions, Eeckhoudt and Jeleva (2004) study medical decisions and Treich (2010) the value of a statistical life. Interestingly, Ponzetto (2003) and Horneff et al. (2008) apply Epstein and Zin (1989)’s recursive utility framework to uncertain longevity. They show that the utility value of annuitization is decreasing in both risk aversion and elasticity of intertemporal substitution. Positive annuitization, nevertheless, remains optimal. Moreover, Groneck et al. (2016) apply the Bayesian learning model under uncertain survival developed by Ludwig and Zimper (2013) to a life-cycle model without annuity markets while Drouhin (2015) develops a rank-dependent utility model of uncertain lifetime. The dynamics of consumption and savings over the life cycle are closer to the empirical observations. Section 2 proposes a model of consumption and bequest with uncertain lifetimes and studies the annuitization decision under the assumption of ambiguity neutrality. Section 3 introduces the Klibanoff et al. (2005) framework to analyze the impact of ambiguity aversion on optimal choices. It contains our main theoretical results and a numerical application. Concluding remarks are in Sect. 4 and proofs are gathered in Appendix.",8
85.0,3.0,Theory and Decision,07 May 2018,https://link.springer.com/article/10.1007/s11238-018-9664-x,Violations of betweenness and choice shifts in groups,October 2018,Pavlo R. Blavatskyy,Francesco Feri,,Male,Male,Unknown,Male,"Systematic violations of expected utility theory, such as the Allais (1953) paradox and the common ratio effect (Kahneman and Tversky 1979), motivated the development of generalized non-expected utility models (e.g., Starmer 2000). A typical approach was to weaken the axioms of expected utility theory, most notably the independence axiom. One of the weaker versions of the independence axiom is the betweenness axiom (e.g., Dekel 1986), which can be summarized as follows: a decision maker who chooses an alternative A over another alternative B must also choose any probability mixture of A and B over B itself and can never choose a probability mixture of A and B over A itself. In other words, if A is revealed preferred to B, then A must be also revealed preferred to any probability mixture of A and B and the mixture must be revealed preferred to B. Thus, in terms of revealed preferences, a probability mixture of two choice alternatives is in between the alternatives themselves, which explains the name of the axiom. The betweenness axiom can be violated in two ways. A decision maker, who chooses a probability mixture of A and B over alternative A as well as over alternative B, reveals a quasi-concave preference or a preference for randomization. A decision maker, who chooses alternative A as well as alternative B over a probability mixture of A and B, reveals a quasi-convex preference or an aversion to randomization. Early experimental tests of the betweenness axiom found both types of violation. Systematic quasi-concave preferences were documented in Becker et al. (1963), Chew and Waller (1986), Prelec (1990) and Blavatskyy (2013b, p.63). Systematic quasi-convex preferences were documented in Conlisk (1987) and Gigliotti and Sopher (1993). Camerer (1989) found that subjects tend to reveal quasi-concave preferences in the proximity of the horizontal edge of the Marschak-Machina probability triangle (Machina 1982) and quasi-convex preferences in the proximity of the hypotenuse of the probability triangle. Note that lotteries located near the hypotenuse of the probability triangle are relatively risky compared to lotteries located on the same indifference curve but near the horizontal edge of the probability triangle. Camerer and Ho (1994) and Bernasconi (1994) also provide strong experimental evidence of a “squiggle” pattern of betweenness violations (a quasi-concave preference for a relatively safe probability mixture and a quasi-convex preference for, or rather aversion to, a relatively risky probability mixture). In this paper we link systematic violations of the betweenness axiom in revealed individual choice under uncertainty to another behavioral regularity—choice shifts in a group decision making.Footnote 1 Choice shifts are observed if an individual faces the same decision problem but makes a different choice when deciding alone and in a group (e.g., Davis et al. 1992). There are two types of choice shifts. An individual exhibits a risky shift when she chooses a safer alternative when deciding alone but votes for a riskier alternative in a group.Footnote 2 An individual exhibits a cautious shift when she chooses a riskier alternative when deciding alone but votes for a safer alternative in a group. Baker et al. (2008) and Shupp and Williams (2008) found evidence of risky shifts for relatively safe lotteries and cautious shifts for relatively risky lotteries. Masclet et al. (2009) also documented cautious shifts for relatively risky lotteries. We shall demonstrate that such empirical evidence is consistent with empirical evidence on violations of the betweenness axiom and both behavioral regularities might be the two sides of the same coin. Eliaz et al. (2006) already linked choice shifts in groups to another well-known behavioral regularity in individual decision making—the Allais paradox (Allais 1953). Eliaz et al. (2006, p. 1322, footnote 4) note that preferences that are consistent with choice shifts must violate the betweenness axiom. Eliaz et al. (2006) consider one prominent family of such preferences—those represented by rank-dependent utility with a concave probability weighting function (e.g., Abdellaoui 2002). In this framework, an individual exhibits the Allais paradox if and only if she reveals a specific pattern of choice shifts in groups (Theorem 1 in Eliaz et al. 2006). In this paper we show that an individual exhibits choice shifts in group decision making if and only if she violates the betweenness axiom, that is, as already mentioned above, a weaker version of the independence axiom. Our result generalizes the result of Eliaz et al. (2006) since we do not restrict our analysis to a specific class of preferences. If preferences are not represented by rank-dependent utility, Theorem 1 in Eliaz et al. (2006) does not apply, i.e. an individual, who exhibits the Allais paradox, may not necessarily violate the betweenness axiom (e.g., Chew 1983) and, therefore, she may exhibit no choice shift. For example, if individual preferences are captured by a disappointment aversion theory (Gul 1991), a decision maker may exhibit the Allais paradox without violating the betweenness axiom. Thus, such an individual exhibits no choice shifts. Unlike Eliaz et al. (2006), we do not assume rank-dependent utility representation of preference in this paper. Yet, our results are consistent with a rank-dependent utility representation with an inverse S-shaped probability weighting function. It is worthwhile to note that an inverse S-shaped probability weighting function is strongly supported by empirical evidence, e.g., Wu and Gonzalez (1996), Wakker (2010). In contrast, Eliaz et al. (2006) considered only rank-dependent utility with a concave probability weighting function. Such an assumption about the probability weighting function is arguably less descriptively accurate [see, however, Blavatskyy (2013a)]. The remainder of the paper is organized as follows: Section 2 defines the first behavioral regularity—violations of the betweenness axiom in individual choice under uncertainty. Section 3 defines the second behavioral regularity—choice shifts in a group decision making. Section 4 presents our main result linking the two regularities. Section 5 concludes with a general discussion.",
85.0,3.0,Theory and Decision,14 May 2018,https://link.springer.com/article/10.1007/s11238-018-9665-9,An exploration of third parties’ preference for compensation over punishment: six experimental demonstrations,October 2018,Janne van Doorn,Marcel Zeelenberg,Seger M. Breugelmans,Female,Male,Unknown,Mix,,
85.0,3.0,Theory and Decision,30 January 2018,https://link.springer.com/article/10.1007/s11238-018-9655-y,Stable coalition structures in symmetric majority games: a coincidence between myopia and farsightedness,October 2018,Takaaki Abe,,,Male,Unknown,Unknown,Male,"In this paper, we attempt to answer the following question: Which coalition structure is “stable” in majority voting? To clarify our question, we assume that there are three identical players (namely, voters). Call them 1, 2 and 3. Each player can form a coalition with some of the other players, such as a political party. In three-player symmetric majority voting, a coalition consisting of two or three players wins. All possible coalition structures are \(\{\{1,2,3\}\}\), \(\{\{1,2\},\{3\}\}\) (and its symmetries) and \(\{\{1\},\{2\},\{3\}\}\). Now, which coalition structure can we consider a “stable” coalition structure? To formally analyze this problem, we need to revisit each player’s power and stability concepts. 
Hart and Kurz (1984)’s work is a leading attempt to solve this problem. They use a coalition structure value (CS-value) to represent each player’s power.Footnote 1 A CS-value is a function that assigns a real number to every player in a given coalition structure. In their analysis, they use the Owen power index as a CS-value to evaluate each player’s power in a coalition structure, which is an extension of the Shapley–Shubik power index to games with a priori coalition structures. Note that the Owen power index is not the only CS-value. Aumann and Drèze (1974), Wiese (2007), Casajus (2009), and Kamijo (2009) propose their CS-values. We can classify these values into two classes in terms of efficiency. There are two efficiency concepts: component-efficiency and efficiency (to distinguish them, we call the latter N-efficiency). Component-efficiency requires that for each coalition in a coalition structure, the summation of values assigned to its members equals the worth of the coalition. On the other hand, N-efficiency requires that the summation of all players’ values coincides with the worth of the grand coalition. The CS-values proposed by Aumann and Drèze (1974), Wiese (2007) and Casajus (2009) satisfy component-efficiency, while Hart and Kurz (1983, 1984) and Kamijo (2009) obey N-efficiency. As mentioned in Hart and Kurz (1984), the values satisfying N-efficiency are mainly applied to measure each player’s power for the given a priori coalition structures.Footnote 2 In this paper, as well as Hart and Kurz (1984), we employ the Owen power index to evaluate each player’s power. By adapting the Owen power index to every coalition structure, we obtain a list of power index profiles as in Table 1, when the number of players n is 3 (this process is elaborated in Sect. 2). Assuming that players deviate from one coalition structure to another to maximize their power, Hart and Kurz (1984) study which coalition structure satisfies the \(\alpha \)-, \(\beta \)-, \(\gamma \)- and \(\delta \)-stabilities. However, their analysis is available only for some specific number of players: the analysis for general n is left open. Our objective is to offer the general analysis by focusing on myopia and farsightedness. Each player’s Owen power index depends on the coalition structure of the other players. This property exhibits externalities between coalitions. In the presence of externalities, deviating players must take into account the reaction from the other players. The reaction and externalities allow us to define a variety of stability concepts. In this paper, we use three myopic core concepts, the projective core, the pessimistic core and the optimistic core, and one farsighted stability concept, the farsighted vNM stable set. The myopia of the three cores reflects the myopic anticipation of deviating players: the projective core describes that the players outside the deviating coalition do not react to the deviation and not reorganize their own coalition structure. The pessimistic (optimistic) core exhibits the deviating players’ pessimistic (optimistic) expectation for the reaction of the other players. In the pessimistic (optimistic) view, every deviating player anticipates that the other players regroup their coalition structure to minimize (maximize) the power of the deviating coalition. Funaki and Yamato (1999) apply the optimistic core and the pessimistic core to analyze the tragedy of the commons. Abe and Funaki (2017) offer necessary and sufficient conditions for these cores to be nonempty. Moreover, Abe (2017) axiomatically characterize them by using reduced game consistency. Although myopia consists in the fact that a deviating coalition considers no reaction or, at most, a one-step reaction from other players, farsightedness is featured with a sequence of reactions: when some players try to deviate from a coalition structure, they consider a sequence of reactions their deviation brings about. Chwe (1994); Xue (1997), and Ray and Vohra (1997) have developed the theory of farsightedness, and Diamantoudi and Xue (2003) introduce the farsighted vNM stable set in the context of hedonic games. Moreover, Ray and Vohra (2015) reformulate the farsighted stable set as a modification of Harsanyi (1974)’s formulation and propose a necessary and sufficient condition for the farsighted stable set to exist and contain a single payoff allocation. Note that farsightedness is also investigated from the viewpoint of core. For example, Kóczy (2007) proposes the recursive optimism and pessimism and defines the recursive cores. His core concept describes that players recursively consider the reaction of residual players. In addition, as formally defined in Sect. 3, each of our core concepts is not the set of allocations, but the set of partitions. We share this approach with Greenberg (1994), Kóczy and Lauwers (2004) and Kóczy (2007). In this paper, we employ the myopic cores and the farsighted vNM stable set to analyze stable coalition structures in symmetric majority games. The difficulty of the analysis lies in its externalities: the externalities in a symmetric majority game is neither positive nor negative. Therefore, the typical result such as “the grand coalition or the partition into singletons are stable” is no longer guaranteed. Our main result is as follows: the pessimistic core, the largest myopic core, coincides with some farsighted vNM stable set in symmetric majority games for any number of players. Moreover, we show that a coalition structure belongs to the pessimistic core and the farsighted vNM stable set if and only if the coalition structure contains an exact majority coalition, namely a minimal winning coalition. The relationship with the other stability concepts is also provided. The rest of this paper is organized as follows. In Sect. 2, we formulate symmetric majority games as coalition formation games and introduce the Owen power index. Our main results, the coincidence between the stability concepts, is offered in Sect. 3. In Sect. 4, we focus on one-player deviations and analyze Nash stability and individual stability studied by Banerjee et al. (2001) and Bogomolnaia and Jackson (2002). We show that these stability concepts consist of radically different coalition structures. In Sect. 5, we provide a summary of our results, comparisons with related works, and a remark about the uniqueness of the farsighted vNM stable set.",4
85.0,3.0,Theory and Decision,02 July 2018,https://link.springer.com/article/10.1007/s11238-018-9669-5,Cheating in a contest with strategic inspection,October 2018,Guy Elaad,Artyom Jelnov,,Male,Male,Unknown,Male,"Anti-doping activity is a top priority for the International Olympic Committee (IOC), and for most national Olympic committees and sport federations. To ensure “clean sports,” the inspecting authorities adopted a zero-tolerance policy against athletes who use and/or supply prohibited substances. Several infamous doping scandals have shocked the sport world. For example, the US Anti-Doping Agency (USADA) discovered that Lance Armstrong won all of his seven Tour de France titles by doping. Another scandal occurred in the 1988 Summer Olympics in Seoul when the Canadian sprinter Ben Johnson was stripped of his 100 m gold medal due to steroid use. More recently, in January 2016, Maria Sharapova failed a drug test at the Australian Open tournament and was banned from tennis for 2 years by the International Tennis Federation (ITF). Anti-doping agencies use different methodologies to detect cheating. For example, the World Anti-Doping Agency (WADA) selects athletes for testing both randomly and according to competition results. The policy of the German anti-doping agency (NADA) is that “selection of athletes \(\ldots \) can either be decided by placing, by name (target control), as well as by drawing lots” (http://www.nada.de). A similar policy is adopted by agencies in other countries. Cheating in contests also exists in fields other than sports. For example, in a political election, campaign candidates can obtain illegal donations. In a rent-seeking contest, government officials can be bribed. We assume two athletes who are competing in a contest. Each can cheat using an illegal drug (i.e., doping). Cheating significantly increases an athlete’s chances of winning the contest. However, authorities, who are interested in fair play, conduct doping tests, and severely punish athletes who are caught cheating. The doping tests are costly for authorities, and therefore, they wish to optimize their use by maximizing the detection of cheating while minimizing test costs. Two basic approaches for doping tests are available to an Inspector (i.e., a representative of the authorities). First, she may define a probability that each athlete will be tested regardless of the results of the competition (the ex-ante approach), or she may select athletes for testing taking competition results into account (the ex-post approach). She may also use some combination of these two approaches. The formal model is described below. The main prediction is that the ex-post approach is more efficient than the ex-ante approach; namely, using the ex-post approach, the expected number of doping tests is lower with the ex-post approach and the probability of cheating is lower. We find that, in both approaches, as cost of inspection increases, the probability of cheating increases. More surprising is that the probability of testing also rises with inspection cost. This finding may be interpreted as follows. Assume that test costs decline due to technology improvements. In this case, athletes engage in less doping, and fewer tests are required to detect cheating. On the other hand, if test costs increase due to improvements in doping technologies, athletes engage in more doping and more tests are required. We assume that, if one athlete is caught cheating, it does not affect the utility of the second athlete. Typically, in sport competitions, if a winner is caught doping, another contestant is recognized as the winner. However, we believe that the sportive value of such a “technical” win is extremely low. For simplicity, we set this value as zero. In non-sports competitions, it is not clear that the second contestant is significantly better off if the first contestant’s violation is revealed. For example, when Republican President Nixon resigned following the Watergate scandal, he was not replaced by a Democrat candidate. This research belongs to the inspection game literature, where one player decides whether or not to violate some rule, and the second player (the Inspector) chooses an optimal strategy to detect the violation (see Avenhaus et al. (2002) for a survey of this literature). This paper is also related to the law enforcement literature (see Polinsky and Shavell 2007), some results of which appear surprising at the first glance. For example, Tsebelis (1989) shows that changes in the penalty to violators do not affect probability of violation. In Andreozzi (2004), if the Inspector moves first and commits to some frequency of inspection, an increase in the Inspector’s reward leads to an increase in violation frequency. A contest model by Tullock (1980) is also related to our research. In a Tullock contest, the probability to win depends on the effort invested by contestants. In our model, cheating can be interpreted as a kind of effort.Footnote 1
 Several game-theoretic papers have focused on contests with possible cheating. In Kirstein (2014), the game is between one athlete and an enforcer (the Inspector). The athlete can dope or not dope, and an imperfect signal about athlete’s decision is sent to the enforcer, who chooses whether or not to punish the athlete. In Berentsen (2002), Kräkel (2005), and Stowe and Gilpatric (2010), two players participate in a contest and can cheat, but the probability of detection by the Inspector is exogenous, and not her strategic decision. In Goetsch and Salzmann (2017), the Inspector can strategically decide to retain test results until new detecting technology is developed. 
Berentsen et al. (2008) is the research most closely related to our own. In Berentsen et al. (2008), two competitors and the Inspector are strategic players, and one of their models (Berentsen et al. 2008, Section 5.1) is similar to ours. However, their main motivation is normative: to propose a whistle-blowing methodology, where a loser may ask the Inspector to test the winner for cheating, and to pay for this request. Berensten et al. analyze only an ex-post approach to testing, while we compare ex-post and ex-ante approaches. Moreover, they assume that only the winner is tested, while we obtain this endogenously. Our results state that as testing technology becomes less expense, the probability of both cheating and inspection decline. This result is novel and surprising. The sabotage literature (see Konrad 2000) is also related to this paper. In the sabotage model, one contestant can harm (for example, injure) another contestant. However, in our model, contestants can improve their own ability by doping.",
85.0,3.0,Theory and Decision,25 June 2018,https://link.springer.com/article/10.1007/s11238-018-9668-6,"Moral judgments, gender, and antisocial preferences: an experimental study",October 2018,Juergen Bracht,Adam Zylbersztejn,,Unknown,Male,Unknown,Male,"A growing literature in psychology, economics, and philosophy investigates people’s moral judgments. For that purpose, participants are usually asked to engage in thought experiments in which they respond to hypothetical dilemmas—two possibilities neither of which is unambiguously acceptable. Typically, these stylized situations (such as the trolley dilemma outlined in the next section) involve the question of whether one should sacrifice one life to save many others, and thus highlight the conflict between utilitarianism (according to which an act is right if and only if it leads to the greatest total amount of well-being) and deontology (which prescribes that acts are inherently good or bad, regardless of their consequences). When making such judgments, people are be driven by their feelings of agreement or disagreement with a certain course of action (Haidt 2001, 2003). Hence, moral-dilemma questionnaires have become well-established methodology for eliciting people’s moral judgments.Footnote 1 A recent line of research on moral dilemmas investigates whether moral judgments are associated with gender, and how moral judgments are related to other-regarding preferences. A first set of papers concludes that there are gender differences in moral judgment: males are more prone to endorse the utilitarian solution in hypothetical moral dilemmas (see Petrinovich et al. 1993; Zamzow and Nichols 2009; Banerjee et al. 2010 and Buckwalter and Stich 2014). Furthermore, a large-sample investigation by Hauser et al. (2007) with over 2000 participants confirms a large and significant gender difference in a variety of moral dilemmas. Another large-sample study by Bourget and Chalmers (2014) finds a number of gender differences in judgments, including females being less likely to agree with the utilitarian solution in a moral dilemma. In addition, Hsu et al. (2008) report that males tend to favor the utilitarian solution in a real-world moral dilemma: a choice of actual allocation of meals among poor African kids through a charity organization.Footnote 2
 The second set of studies points to a—somewhat surprising—conclusion that people with antisocial attitudes are prone to make utilitarian judgments. Koenigs et al. (2007) conducted a study of brain-damaged patients with acquired sociopathy. These kinds of emotional deficits are similar to those observed in psychopaths (Saver and Damasio 1991). These patients are found to display unusually high levels of endorsement of the utilitarian solution to moral dilemmas. In the same vein, Glenn et al. (2010) report that psychopathic traits—measured using Levenson’s Self-Report Psychopathy Scale—predict a stronger endorsement of utilitarian solutions across several moral dilemmas. Other related studies point to a negative association between the endorsement of utilitarianism and an aversion to harming others (Crockett et al. 2010; Cushman et al. 2012) and empathic concerns (Choe and Min 2011; Gleichgerrcht and Young 2013). Kahane et al. (2015) report that high rates of utilitarian judgments are associated with the endorsement of unfair business practices, rational egoism, (hypothetically) donating less to a charity, and a weaker identification with the whole humanity. Finally, Bartels and Pizarro (2011)—in a study most closely related to ours—also report gender differences in moral judgments in a large set of moral dilemmas (that we also employ herein). However, their report also highlights the importance of individual traits: there is a positive relation between endorsement of sacrifice-one-live-to-save-many-lives and measures of psychopathic personality (low empathy, callous affect, and thrill-seeking), Machiavellianism (cynical, emotionally detached from others, and manipulative), and perceived life meaninglessness (melancholic existential concerns). Furthermore, they find that men tend to score higher in all of those psychological questionnaires and that the gender effect on moral judgments fades away when psychological traits are accounted for. Their finding, in turn, raises the question of whether the relationship between moral judgment and gender is spurious, because the research failed to account for antisocial traits as a confounding factor. In this paper, we provide a novel method of investigating the interaction between moral judgments, gender, and antisocial preferences. We elicit the propensity for other-damaging behavior (spitefulness) through an experimental game, and use this measure as a control when explaining gender difference in responses to a large moral-dilemma questionnaire. The measure of spitefulness if based on behavioral data collected in a standard economic experiment. We observe decisions in a laboratory game in which the decision maker is granted a flat payoff of 10 Euros, and—in addition—sets the payoff for another participant to any amount between 0 and 10 Euros. An amount lower than 10 Euros harms the other person and indicates a preference for spite.Footnote 3
 Following the previous studies on moral judgments and antisocial preferences, the main interest of our experimental game is to provide a direct way to classify participants into two categories: those who engage in antisocial behavior and those who do not.Footnote 4 Antisocial behavior is widespread in our setting. In line with the previous evidence from several economic experiments (reviewed in the next section), in our experiment, roughly, one participant in five purposefully reduces the other participant’s earnings. We use this behavioral variable as a control in regression analysis. We find that gender is associated with moral judgment even when we control for participants’ spitefulness: males are found to be more prone than females to endorse the act of sacrificing one life to save many other lives. Thus, in line with the previous findings from the psychology literature, our results suggest that men indicate a greater endorsement of utilitarian solutions. However, notwithstanding the previous findings, our study suggests that this does not happen, because men are more likely to display preferences that most would consider antisocial, and thus immoral.",8
85.0,3.0,Theory and Decision,30 May 2018,https://link.springer.com/article/10.1007/s11238-018-9663-y,A network ridesharing experiment with sequential choice of transportation mode,October 2018,Vincent Mak,Darryl A. Seale,Rui Yang,Male,Male,Male,Male,"A fundamental question in economics, transportation science, computer science, and management science concerns the sharing of the cost of joint investment, or the reward for joint effort, in a fair, efficient, and stable way (e.g., Hoefer 2013; Moulin 2002; Young 1994). Most research on cost-sharing in economics has followed two alternative approaches. One approach assumes that the demand and costs are commonly known, and then designs a system of incentives that maximizes some measure of social welfare. The other approach views the cost-sharing problem as a cooperative game, and then proposes game-theoretic solutions such as the core, Shapley value, and nucleolus (Shubik 1982; Young 1985, 1988). In transportation research, the issues of cost allocation have frequently been investigated in the context of ridesharing in traffic networks. With ridesharing, individual travelers share a vehicle (or fleet of vehicles) and split the cost of travel equally (see e.g., Furuhata et al. 2013, for a recent review). The practice of carpooling among commuters is one example. Ridesharing could also describe the economics behind conventional public transportation from shuttle services between suburbs and town centers to buses, trams, and trains. These are all means by which users collectively bear high total transportation costs that become small when a sufficient number of them share the same transportation mode together. Investigations on ridesharing have become especially timely with the recent upsurge in ridesharing businesses such as Uber and Lyft. Essentially the same questions have been studied in computer science for better understanding the recent development of the Internet, in particular e-commerce (Anshelevitch et al. 2008; Syrgkanis 2010; Jain and Mahdian 2007). A central issue in ridesharing, which is also important in the more general domain of cost-sharing in economics, is that the sharing of cost implies positive externalities and hence coordination problems for the users. The present study is an experimental investigation into these problems. Our choice framework involves a tradeoff that is common among transportation modes with ridesharing characteristics: one transportation mode might be very costly to individual users if a low number of users choose it, but become attractively low-cost to individual users if a sufficiently large number of them choose it; while another transportation mode is less (more) costly than the first one when a low (high) number of users choose it. The problem is to coordinate users to choose the first transportation mode, so that it does operate with sufficiently high usage. A simple example is shuttle service versus carpooling between suburbs and town centers: shuttle service might have a higher total operating cost than carpooling, but it might also incur a lower individual cost than carpooling when there is sufficiently high usage. Two of such ridesharing options with tradeoff, together with the baseline option of private car without ridesharing, form a meaningful spectrum of transportation modes for the examination of related tacit coordination problems in our study. Our work is distinguished from prior literature by our focus on sequential choice observability in ridesharing. Real-time observability of choices of ridesharing transportation modes is often feasible, for example at carpooling junctions on highways. Such observability is nowadays enhanced by online communication platforms and other forms of social media. However, there could still be a limit to which individual users might be informed about all the currently made choices of any transportation mode. As such, it is natural to examine a benchmark case in which every user has complete observability over currently made choices of any transportation mode, and another case in which such observability is only partial. Concern for partial observability in transportation networks is a recently growing development that has generated theoretical research such as Acemoglu et al. (2016). Meanwhile, previous experimental studies on cost-sharing, such as Liu et al. (2015)—which employed the choice framework of the present study—are limited to no-information cases without choice observability. Our experimental evidence, driven by theoretical analysis, shows that even a limited extent of sequential choice observability could lead to efficient coordination towards adopting the ridesharing mode. However, in terms of behavioral dynamics, convergence to efficiency would be slower with more limited observability resulting in a significant increase in travel cost. Our work contributes to two streams of research. First, we contribute to previous experimental research on strategic interactions in transportation mode choices, a relatively nascent area that involves economics, transportation, and operations management research (see Rapoport and Mak, in press, for a recent literature review). Specifically, we extend previous paradigms of decentralized choice of transportation modes in congestible networks (where the externalities that players impose on members of their group are negative) to the choice of transportation modes in directed networks with positive externalities. We also extend previous research on cost-sharing in directed networks from simultaneous (Liu et al. 2015) to sequential transportation mode choices with observability, which is arguably more applicable to real-life transportation problems in a world of instant social media communications. Second, we contribute to a considerably larger experimental economics literature on equilibrium selection in coordination games (e.g., Charness et al. 2014; Chen and Chen 2011; Crawford 1995; Goeree and Holt 2015; Van Huyck et al. 1990, 1991) that may be traced back to the original contributions on coordination in decentralized systems by Schelling (1960). Most of the research cited immediately above has documented scenarios where tacit coordination fails. In contrast, our present research, as well as previous experimental research on the market entry game (e.g., Erev and Rapoport 1998; Seale and Rapoport 2000) and the Braess paradox (e.g., Gisches and Rapoport 2012; Morgan et al. 2009; Rapoport et al. 2006, 2009, 2014) provides evidence for limited success of tacit coordination in small groups. Ridesharing was introduced in the early 20th century. With mass-produced automobiles flooding the market and shortage of public transportation, car owners in the USA started offering to share their travel for a five-cent streetcar fare. The idea spread from Los Angeles to other parts of the USA, but then declined drastically around 1919 in part due to licensing requirements and liability insurance regulations imposed by local governments. Ridesharing re-emerged in the USA during WWII and subsequently in the 1970s oil crisis with the explicit support and encouragements of local governments, but then its popularity declined sharply with the subsequent fall of the oil prices (see, e.g., Agatz et al. 2012; Berbeglia et al.,2010; Furuhata et al. 2013). Within the last decade, there has been a dramatic resurgence in for-profit ridesharing operations. These businesses have benefitted from the emergence of new technologies that allow for higher efficiency in communication and greater ability to facilitate the match of supply and demand (e.g., Waze, Google Map). As such, the Nobel Laurette Alvin Roth (http://www.econtalk.org/archives/2015/07/alvin_roth_on_m.html) views Uber as a marketplace that connects drivers and commuters through a private matching algorithm. The modern form of ridesharing (also called “dynamic ridesharing” and “online ridesharing”) is a service that is presently offered in cities all over the world. It serves areas that are not covered well, or not covered at all, by public transportation, and often acts as a transit system feeder. It is also capable of serving one-time trips, and not only scheduled trips. It attracts commuters mostly because it reduces waiting time, lowers fuel usage, and, most importantly, reduces the transport cost of the commuter. Research on ridesharing has focused on the coordination of itineraries and travel schedules, on effective methods of encouraging commuters to share travel expenses and reduce pollution, and on the use of new communication capabilities that have been introduced into the market in the last two decades. The need to understand cost-sharing in large unregulated settings such as the Internet or traffic networks has led to recent studies in algorithmic game theory on the design of strategic cost-sharing games to obtain Nash equilibrium properties (Chen et al. 2010; Hoefer 2013). Our study abstracts the essential issues of coordination in ridesharing into a directed network game with experimentally testable predictions. Specifically, we examine cost-sharing in the context of choice of transportation modes in traffic networks with a common origin (O) and common destination (D). The stylized network that we examine has three (O, D) routes corresponding to three transportation modes. One transportation mode (the baseline private car option) has a fixed cost of travel and, therefore, additional commuters choosing this mode impose no externalities. The choice of one of the other two alternative modes of transportation has positive externalities as commuters (players) who choose this mode share its cost of travel: the larger the number of commuters choosing it, the lower the individual cost of travel. As noted previously, we can interpret these two modes as carpool and shuttle. Another example of a choice framework with a similar structure, which is often observed in airports, is of a scientist attending a conference, who has just completed her flight and has to choose between taking a taxi to the conference center in mid-town and paying a fixed price or, alternatively, joining one of several ridesharing modes of transportation with other participants in the conference who travel to the same destination and agree to divide the cost of travel equally. A problem that each commuter faces in decentralized networks of this type, where there is no central authority to coordinate transportation mode choices, is as follows. Disregarding other aspects of the transportation mode choice such as convenience of travel and search time, in ridesharing transportation, the individual cost of travel is determined by the number of players choosing one of the transportation modes. In contrast, the choice of a private mode with a fixed cost of travel is straightforward. The consequences of these choices may be quite different. If relatively few commuters choose ridesharing (and, therefore, each of them incurs a relatively high proportion of the total cost of travel), then the cost of travel of each commuter choosing the private mode may be lower. This inequality is reversed if the number of travelers choosing the same ridesharing transportation exceeds some threshold value which, in turn, depends on the cost structure. Our main purpose in the current paper is to investigate a major topic in tacit coordination regarding how homogenous commuters divide themselves between private transportation and ridesharing modes of transportation. We conduct this investigation under two different variants of the sequential protocol of play, which we describe below. In previous experiments on strategic choice of route or transportation mode with either negative (congestion) or positive (cost-sharing) externalities, participants made their choices independently and anonymously under what we call the simultaneous protocol of play (e.g., Gisches and Rapoport 2012; Liu et al. 2015; Rapoport et al. 2006, 2014; Selten et al. 2007). Among these, Liu et al. (2015) examined the same choice framework as in the present study. Those researchers found multiplicity in equilibrium convergence both with their theoretical predictions and experimental data in a setup with the simultaneous protocol. In the present paper, we extend this line of research significantly by investigating the sequential protocol of play, where players choose transportation modes in an exogenously determined order. Under this protocol, which was not examined at all in Liu et al. (2015), transportation mode choices may no longer be considered independent. Instead, the choice of mode of transportation of a commuter may be affected by the choices of one or more of the preceding commuters in the sequence. To achieve independence between rounds of play, we randomized the order of the players in the sequence in every round in our experiment. This procedure also prevented a given player from gaining information about the previous choices of any particular member of her group, and provided each player equal opportunity to assume all the possible positions in the sequence. Once the choice of transportation mode is conducted under the sequential protocol of play, the information that each player has when it is her turn to make a choice is critical. By contrast, simultaneous protocol setups impart zero information on choice observability among players within their experimental game. The significance of information in sequential decision-making settings is highlighted by the investigation of the effects of memory size in a sequential search problem with relative ranks known as the secretary problem (see, e.g., Ferguson 1989, and Samuels 1991 for literature reviews). In the secretary problem a decision maker (DM) observes a set of n rankable applicants (1 being the best and n the worst), who appear one at a time in random order with all the n! permutations equally likely. The absolute ranks of the n applicants are not disclosed. Once the DM observes the ith applicant, she must decide either to accept (select) or reject it based on the observed rank of the applicant relative to all the applicants who preceded her in the sequence (1 ≤ i < n). In the rank minimization variant of the secretary problem (Chow et al. 1964), which is different from the more commonly known probability maximization variant, the DM objective is to minimize the expected (absolute) rank of the applicant selected. Chow et al. proved the astonishing result that as n→∞, the minimal expected rank of the selected applicant converges to 3.86. Rubin and Samuels (1977) considered a generalization of the expected rank minimization problem under a severe information constraint, called the memory-length-one rule, where the DM may only consider (remember) just one of the previously observed applicants, and thereby be in a position to determine only whether the current applicant is better or worse than the currently remembered one. They proved an equally remarkable result that the limiting minimal expected rank may be kept finite even with such a severe constraint on information, and that its upper bound tends to 7.41 as n→∞. Analogously to their results, all the choice options (transportation modes) in our traffic setup are available to each player, and choices are made sequentially under two conditions of choice observability. In the full choice observability condition, player j has complete information about the transportation mode choices of all the players who preceded her in the sequence. Because the number of players in the group, n, is commonly known, the choice of the mode of transportation by player j under full choice observability impacts the choices of players j + 1, j + 2, …, n who follow her in the sequence. Under the partial choice observability condition, which is examined by Rubin and Samuels (1977) in the context of the secretary problem, each player is only informed of the choice made by the (single) player who preceded her in the sequence. Therefore, player j does not know how many players who preceded her in the sequence chose ridesharing, nor may her decision directly affect the choices of players j + 2, j + 3, …, n. Under both choice observability conditions, the first player in the sequence has no information at all, and the second player is only informed of the transportation mode choice of the first player. But starting with the third player in the sequence, the information sets of the remaining players under the two observability conditions diverge.",3
85.0,3.0,Theory and Decision,26 March 2018,https://link.springer.com/article/10.1007/s11238-018-9661-0,Asymmetric endogenous prize contests,October 2018,Damian S. Damianov,Shane Sanders,Anil Yildizparlak,Male,Male,Male,Male,"A variety of economic and social settings can be described as contests in which players exert effort to increase their chance of winning a prize. Most of the literature posits a model in which the prize is exogenously given; classical contributions to contest theory in which players compete for a fixed prize include Tullock (1980), Hirshleifer (1989; 1995; 2001), Dixit (1987), and Nti (1997), among others. In the fixed prize model, rent-seeking investments represent “the unproductive use of resources to contest, rather than create wealth” (Congleton et al. 2008). That is, contestants expend resources to appropriate or defend pre-existing wealth, rather than to undertake productive activities. While the fixed prize assumption has been popular and useful due to its analytical tractability, it does not accurately reflect the strategic interaction in many common and important settings in which effort can be either productive or destructive. Labor tournaments to win a promotion, R&D races to win a patent, and sports contests are examples of contests in which effort can be productive. In R&D contests, efforts contribute to higher profits in the event of winning the patent (see, e.g., Cohen et al. 2008; Baik 1994). In labor tournaments, efforts enhance the profitability of the firm and improve the value of promotion (see, e.g., Shaffer 2006). In sports contests, larger efforts increase demand for or prize from the contest (see, e.g., Amegashie and Kutsoati 2005).Footnote 1 Military conflicts and lawsuits, on the other hand, are contests in which effort can be destructive. Military combat involves the use of weapons and warfare, causing destruction of infrastructure and resources (see, e.g., Shaffer 2006; Chang and Luo 2013; Smith et al. 2014; Sanders and Walia 2014). Lawsuits to settle industrial disputes or to dissolve partnerships are settings in which parties often invest in legal representation by expending the very resources they seek to divide. In this paper, we extend the standard fixed prize model to account for prize endogeneity. In particular, we study incentives, equilibrium behavior, and outcomes in contests that combine two features: a prize which depends on aggregate effort, and contestants with different initial strengths—a favorite and an underdog—who are vying for this variable prize. The interaction of these features has largely been ignored as the literature has focused either on endogenous value symmetric settings (see, e.g., Garfinkel and Skaperdas 2000; Shaffer 2006; Chang and Luo 2013; and Chang and Luo 2016) or on fixed value asymmetric contests (Beviá and Corchón 2013). Recent advancements in the endogenous prize symmetric contest literature include contributions which model contests as endogenous prize all-pay auctions. Baye et al. (2012) assume that this endogeneity takes the form of rank-order spillovers and present a number of contest applications including the dissolution of partnerships, R&D races, litigation, price competition, job tournaments, auctions with regret, etc. These all-pay auctions admit multiple symmetric equilibria both in pure and in mixed strategies. Baye et al. (2012) provide conditions on the payoff functions under which pure and mixed strategy Nash equilibria exist and provide new results which can be used to characterize the equilibria of contests featuring the aforementioned applications.Footnote 2 In contrast to this literature, we focus here on Tullock contests rather than on perfectly discriminating ones and show that these contests have a unique pure strategy equilibrium. It is known from the endogenous prize contest literature that the departure from the fixed value assumption changes the incentives of players to exert effort. In contests with productive effort, there are incentives to expend extra resources because effort increases the size of the rent; yet these incentives are limited because each player does not expect to receive the full return of their own increased effort. Using a symmetric model where prize is concave in effort, Chung (1996) shows that productive effort contests still generate, from a welfare perspective, excessively high aggregate efforts. Our approach allows us to draw conclusions regarding the equilibrium winning probabilities and welfare of the stronger and the weaker player. As a technical matter, when the prize changes with aggregate effort, the monotonicity properties of the payoff functions of players—including the marginal gains and losses from exerting effort—are altered in non-trivial ways. Thus, the extant results on the existence of a Nash equilibrium and the known set of conditions which ensure uniqueness of equilibrium in asymmetric fixed value contests (see, e.g., Skaperdas and Gan 1995; Szidarovszky and Okuguchi 1997; Cornes and Hartley 2003; and Cornes and Hartley 2012) cannot be applied to contests with endogenous prizes. For the case in which the contest success function takes the popular logit form (Tullock 1980), and the marginal cost of effort for each player is increasing in the effort level, we show that an equilibirum exists and is unique both in productive and destructive contests. Our general framework captures as special cases the symmetric endogenous prize models by Chung (1996) and Shaffer (2006) and the standard case of a contest with a fixed value which we will use as a reference point in our analysis. Further, we show that in equilibrium, the disadvantaged player (the underdog) exerts more effort, but his probability of winning remains below that of the player with a headstart (the favorite). A comparative static exercise allows us also to demonstrate that an underdog who faces a weaker favorite expends more effort than an underdog who faces a stronger opponent. These two properties correspond to the equilibrium behavior that would emerge in fixed value contests. We also extend the result by Chung (1996) to an asymmetric setting by showing that in equilibrium, total effort exceeds the socially optimal level. The additional insight that we obtain is that, under fairly general conditions, the deviation from socially optimal behavior (as measured by the difference between strategic and welfare optimal effort) is greater for the underdog. We also derive conditions for the endogenous prize function under which, as in the case of fixed value contests, the underdog imposes a greater welfare loss for the favorite. We note that our results are dependent on the particular logit specification of the contest success function and on our approach to modelling asymmetries. Standard ways to account for asymmetries are either to assume differences in players’ (constant) marginal costs (see, e.g., Baik 1994 or Ridlon 2016) or in headstarts (see, e.g., Siegel 2009). We adopt the latter approach here for two reasons. First, the headstart assumption is appropriate for modeling the behavior of contestants who compete by using the same production technology, but have different strengths when they enter into the contest. This scenario is relevant for applications such as labor tournaments, R&D races, lawsuits or sports contests. Second, this assumption creates an analytically tractable environment. It allows for a general representation of the cost function, but still affords equilibrium and comparative static analysis. A model of a productive contest with asymmetric, but linear cost functions, is developed by Ridlon (2016) in the context of advertising campaigns. The constant marginal cost assumption could lead to equilibrium outcomes in which only one of the players advertises (monopoly case)—an equilibrium outcome observed in the case of strong asymmetries. When players are less asymmetric, they both advertise whereby the relationship between their advertising expenditures depends on the level of asymmetry. Our focus on asymmetric contests allows us to examine how the degree of competitive balance depends on the fixed or endogenous nature of the contest prize. Competitive balance is defined as the level of uncertainty in the outcome of a contest (see, e.g., Owen and King 2015) and has important implications in sports economics, military conflicts and labor tournaments. The question of competitive balance has so far not been addressed in the literature on endogenous value contests, because the extant literature has predominantly focused on symmetric settings (see, e.g., Chung 1996; and Shaffer 2006). These settings feature perfectly balanced contests in which all players make the same choices and win the prize with the same probability. In contrast, we explicitly account for asymmetries in players’ abilities and compare equilibrium behavior in endogenous versus fixed value contests. We show that, when there is an asymmetry in headstarts, productive contests generate a higher degree of competitive balance (more uncertainty in the outcome) than fixed value contests. That is, in productive contests, the underdog wins the prize more often compared to fixed prize contests. Surprisingly, a destructive contest can lead to a higher or lower degree of competitive balance than a fixed prize contest. The remainder of the paper is organized as follows. In Sect. 2, we present the model. In Sect. 3, we discuss the existence, uniqueness, and the properties of the equilibrium. Welfare results are presented in Sect. 4. In Sect. 5, we compare productive endogenous value contests with fixed value contests. We conclude with Sect. 6. All proofs are in the Appendix.",4
85.0,3.0,Theory and Decision,26 June 2018,https://link.springer.com/article/10.1007/s11238-018-9666-8,Partial cooperation in strategic multi-sided decision situations,October 2018,Subhadip Chakrabarti,Robert P. Gilles,Emiliya Lazarova,Unknown,Male,Female,Mix,,
85.0,3.0,Theory and Decision,22 June 2018,https://link.springer.com/article/10.1007/s11238-018-9667-7,Dictatorship on top-circular domains,October 2018,Gopakumar Achuthankutty,Souvik Roy,,Unknown,Unknown,Unknown,Unknown,,
85.0,3.0,Theory and Decision,17 February 2018,https://link.springer.com/article/10.1007/s11238-018-9656-x,Implementing egalitarianism in a class of Nash demand games,October 2018,Emin Karagözoğlu,Shiran Rachmilevitch,,Male,Female,Unknown,Mix,,
85.0,3.0,Theory and Decision,05 April 2018,https://link.springer.com/article/10.1007/s11238-018-9662-z,Why do young women marry old men?,October 2018,Pavlo Blavatskyy,,,Male,Unknown,Unknown,Male,"Consider an overlapping generation’s household model. The same number of individuals is born in each period, one half of them being men and the other half—women. In the first period, neither men nor women know their type. If they wish, both men and women can enter the marriage market already in the first period. If married to a partner in the first period, they can no longer reenter the marriage market in the second period. In the second period both men and women publicly reveal their type. A man reveals a high type with a probability p∊(0,1) and a low type—with a probability 1 − p. A woman reveals a high type with a probability q∊(0,1) and a low type—with a probability 1 − q. We assume that p≥q and this is the only difference between genders. In other words, we model the pay gap as men being more likely to reveal a high type compared to women. Such a modeling approach is consistent with the latest empirical evidence on the pay gap that men are more likely to end up in relatively high-paying industries, whereas women are more likely to end up in relatively low-paying industries (Blau and Kahn 2007). Alternatively, we can model the pay gap as men having a different wage distribution compared to women. Such a modeling approach, however, would require an additional specification of risk preferences, which might differ across genders. Once their type is revealed, any unmarried man or woman enters the marriage market in the second period. Everyone has the same preference ordering of prospective partners. A partner who revealed a high type is the most preferred (ideal) partner; a partner who did not reveal his/her type is the second most preferred partner; and, finally, a partner who revealed a low type is the least preferred partner. Men regard a partner who did not reveal her type as a gamble, i.e., a simple lottery that reveals a high type with probability q and a low type—with probability 1 − q. Similarly, women regard a partner who did not reveal his type as a gamble that reveals a high type with probability p and a low type—with probability 1 − p. In choice under risk, everyone’s preferences satisfy a standard consequentialist premiseFootnote 3 and the first-order stochastic dominance.Footnote 4 In contrast to the existing literature, we do not assume the expected utility representation of preferences.Footnote 5 In other words, all our results hold true whether individuals behave as expected utility maximizers or not. Our model does not require a specific representation of risk preferences (expected utility or other) because all decision makers in the model face only binary lotteries that can be always ranked via criterion of the first-order stochastic dominance. This also allows us to abstract away from any possible gender differences in terms of risk aversion, non-linear probability weighting, etc. In each period, the marriage market operates on the principle of positive assortative matching. First of all, if there are any men and women who revealed a high type, they are matched together. Subsequently, in the case of an excess supply of either men or women who revealed a high type, they are matched with partners who have not yet revealed their type. If there are not enough partners who have not yet revealed their type, the excess supply of high types is matched with partners who revealed a low type. Second, if there are any men and women left, who have not yet revealed their type, they are matched together. Subsequently, if there is an excess supply of either men or women who have not yet revealed their type, they are matched with partners who revealed a low type. Finally, men and women who revealed a low type are matched together. It is easy to see that positive assortative matching results in stable marriages. Since the same number of individuals is born in each period and only heterogeneous couples leave the marriage market, everybody eventually finds his/her match.",
86.0,1.0,Theory and Decision,12 October 2018,https://link.springer.com/article/10.1007/s11238-018-9674-8,Risk preferences and development revisited,February 2019,Ferdinand M. Vieider,Peter Martinsson,Nghi Truong,Male,Male,,Mix,,
86.0,1.0,Theory and Decision,26 September 2018,https://link.springer.com/article/10.1007/s11238-018-9673-9,Sunk ‘Decision Points’: a theory of the endowment effect and present bias,February 2019,Peter Landry,,,Male,Unknown,Unknown,Male,"Two of the most well-known behavioral anomalies revealed by laboratory experiments are the endowment effect, i.e., the tendency of subjects who are given a good to value it more than they otherwise would (Thaler 1980; Knetsch 1989), and present bias, i.e., the tendency for subjects to exhibit disproportionately steep impatience when offered an immediate reward (Thaler 1981; Benzion et al. 1989). These behaviors are anomalous in the sense that they challenge the central assumption of rational choice models that agents’ preferences are consistent. Accordingly, the endowment effect is typically explained by models of loss aversion, in which consumption preferences vary based on a reference point from which losses hurt more than gains help (Kahneman and Tversky 1979; Kőszegi and Rabin 2006), while present bias is typically captured by models of hyperbolic or quasi-hyperbolic discounting, which feature inconsistent time preferences (Ainslie 1992; Laibson 1997; O’Donoghue and Rabin 1999). Despite their fundamentally different formulations of agents’ preferences, however, conventional decision-making models and these behavioral alternatives share an implicit assumption that agents necessarily consider the consumption decision of interest. When interpreting experimental research through this lens, this assumption implies that “cues” occurring within the experiment, such as being given a good or being asked to choose between some amount of the good now and a greater amount later (as in the typical experiments that reveal the endowment effect and present bias, respectively), would have no effect on when or whether the subject considers the decision to consume that good. This paper presents a simple model based on the idea that cues such as these may naturally compel the agent, who can be thought of as a subject in a laboratory experiment, to consider the associated consumption decision. For instance, a subject who enters an experiment is presumably not actively contemplating pear consumption on their own volition, but may be compelled to do so upon being given a pear or when asked to choose between one pear now or two pears later. In the model, each of these so-called decision points is presumed to carry an opportunity cost, reflecting the idea that “thinking about” the possibility of consumption may detract from the agent’s capacity (if only for a moment) to fully attend to other possible activities.Footnote 1 Unlike prevailing approaches, the model captures the endowment effect and present bias through a single mechanism and without the use of inconsistent (consumption or time) preferences. To understand how, first note that by redirecting the agent’s attention to a particular consumption decision, a cue makes the agent more “invested” in consumption at that time. For example, a pear is more valuable to the agent when the cost of thinking about pear consumption (a requisite for actual pear consumption) is already sunk. Consequently, the net value of having a pear increases when exposed to a relevant cue—such as being given a pear, in which case the cue-induced value increase signifies an endowment effect. Similarly, being asked to make a choice between one pear now or two pears later causes an increase in the net value of the immediate pear relative to the future pears (which would be acquired at a time when the cost of considering pears is not yet sunk), thus generating a present bias. The current work relates to several other theories that seek to rationalize one or the other anomaly without its usual behavioral model. Explaining the endowment effect, alternatives to the loss aversion model include theories based on “bad deal aversion” (Isoni 2011; Weaver and Frederick 2012), incomplete knowledge of one’s preferences (Ungureanu 2012), and evolutionary advantages (Huck et al. 2005; Gintis 2007). As for present bias, alternatives to the hyperbolic and quasi-hyperbolic discounting models include theories based on conflict between competing brain systems (Brocas and Carrillo 2008), uncertainty in the realization of future payoffs (Sozou 1998; Dasgupta and Maskin 2005), and non-constant survival rates (Halevy 2008; Robson and Samuelson 2009). With that said, the sunk decision points theory is still unique in offering a shared rationale for the endowment effect and present bias, which are typically treated as distinct phenomena. The theory’s emphasis on the role of cues in generating both anomalies also stands in contrast with the existing theoretical literature, where cues are seldom addressed, yet fits with empirical findings that physical contact or exposure to a good—as would be expected when given a good—can induce an endowment effect (Reb and Connolly 2007; Wolf et al. 2008; Peck and Shu 2009; Bushong et al. 2010), as well as similar observations regarding present bias (Loewenstein 1996; Laibson 2001). Moreover, the theory features a novel role for limited attention in linking (and understanding) the endowment effect and present bias, as both anomalies can be understood as a consequence of the fact that the agent’s attention is not always directed to the consumption decision of interest.Footnote 2 Still, the current work is not meant to suggest that “sunk decision points” is the only relevant force in understanding the endowment effect and present bias. Instead, I argue that it may be an important contributor that, as demonstrated using a highly stylized model, is sufficient to qualitatively capture both anomalies. In fact, relative to the most stripped-down version of the standard decision-making model, the framework only requires one additional parameter, representing the baseline probability of considering the decision of interest when a cue is not present. The rest of this paper proceeds as follows. Section 2 provides a basic example illustrating how an endowment effect can arise from sunk decision points. Section 3 presents the modeling framework. Section 4 shows how the model generates the anomalies of interest. Section 5 proposes methods to counteract each anomaly using a replica of the good. Section 6 formalizes a link between experimentally elicited measures of the endowment effect and present bias while also describing new predictions relating to their hypothesized overlap. Section 7 considers why a potential relationship between the endowment effect and present bias could be concealed in field settings. Section 8 concludes.",2
86.0,1.0,Theory and Decision,27 September 2018,https://link.springer.com/article/10.1007/s11238-018-9671-y,Intentional time inconsistency,February 2019,Agah R. Turan,,,Male,Unknown,Unknown,Male,"The notion of time inconsistency which is characterized by a preference reversal from a larger but later reward to an imminent one as the delays to both rewards decrease has long been recognized by the economists and has been frequently documented by psychologists in the delay discounting literature.Footnote 1 Consider this as an example of preference reversal: a majority of people say that they would say they prefer a $100 check that can be immediately cashed in over a $200 check that can be cashed in after 2 years. The same people do not prefer a $100 check that can be cashed in after 6 years over a $200 check that can be cashed in after 8 years, even though this is the same comparison with a 6-year delay (Ainslie and Haslam 1992). The dependence of decisions on time distance creates dynamic inconsistency, meaning that the individual’s future plan will be inconsistent with his current optimal plan. A rational agent may restrict his future options to compensate for these inconsistencies. While limiting his future options, an individual can engage in social commitments as well as individual commitments.Footnote 2 Social relations based on trust and reputation can make a rational agent conform to his original plan (Benabou and Tirole 2004; Bryan et al. 2010). However, social relations may not always resolve the self-control problem; furthermore, as Battaglini et al. (2005) demonstrated, they can even aggravate the problem of self-regulation. Can social interaction itself be a source of dynamic inconsistency? Can people without self-control problems actually behave as if they had self-control problems within their social relationships? For example, a person who fails to stick to family budget despite all promises to his/her spouse gets rid of self-control problem once they are divorced. As another example, a colleague who acts as if he has a severe procrastination problem is actually simply shirking work. In this paper, we propose a theoretical model to explain the usage of time-inconsistent behavior as a strategy to exploit others when reputation and trust have secondary effects on the economic outcome. Consider two agents with time-consistent preferences exploiting common resources. Assume that one of them wrongly believes that the other agent has time-inconsistent preferences. We have examined the following questions in an analytically solvable model: Does the agent who is believed to have time-inconsistent preferences, use this misinformation as an advantage? What is the degree of time inconsistency she would like to pretend? How does it depend on her own and the other agent’s impatience? Supposing that an agent is believed to have time-inconsistent preferences with probability p,  we analyze whether she uses this misinformation when she has the opportunity to use it. Since this is a game with observable actions, we use perfect Bayesian equilibrium as an equilibrium notion. We characterize the pooling equilibrium where she plays with time-inconsistent preferences irrespective of her type. This proves that if an agent has created a perception that he might have problems with self-control, she prefers to act accordingly. Hence, the choices seen as a result of the self-control problem can actually be intentional. We use a quasi-hyperbolic discounting structure (Phelps and Pollak 1968) to represent the consumption-saving decisions of an agent with time-inconsistent preferences. There are emotion-based and various cognitive mechanisms shown as the driver of time inconsistency.Footnote 3 In the emotion-based mechanism, time-inconsistent behavior has often been represented as a result of conflict between two different decision-making systems, the current and future self, which have narrow and wide temporal perspectives, respectively (Thaler and Shefrin 1981; Metcalfe and Mischel 1999). This idea provides the rationale for the quasi-hyperbolic discounting function which can be decomposed into two distinct processes: one that captures the extra weight given to immediate rewards and another that discounts exponentially (McClure et al. 2007). Using the analytically tractable version of dynamic fishery model originally provided by Levhari and Mirman (1980), we determine the optimal degree of present bias that the agent would like to have while pretending to have time-inconsistent preferences. Next, we consider the constant relative risk aversion (CRRA) class of utility form to characterize the distinction between pretending to be naive and sophisticated. While the sophisticates are aware of their self-control problem, the naifs are not. By assuming that output elasticity is one, we prove that there exists equilibrium under strategies linear in stock and that exploitation of the resources increases when agent pretends to have time-inconsistent preferences. We show that the decision to pretend to have time-inconsistent preferences and the preference between pretending to be naive and sophisticated is sensitive to the each of these parameter values: present-bias parameter, discount rate, and the degree of concavity of the utility function. We analyze how the decision to pretend to have time-inconsistent preferences and the preference between pretending to be naive and sophisticated depends on the parameters of our model by providing results derived from calculations based on our characterization of the equilibrium. There has been extensive research, started by Strotz (1955) and accelerated by Laibson (1994, 1997, 1998), studying the consumption-saving decisions of an agent with time-inconsistent preferences (see also Harris and Laibson 2001; Krusell et al. 2000, 2002; Krusell and Smith 2003). This interest has recently shifted to environmental models with imperfect intergenerational altruism (see Karp 2005; Haurie 2005, 2006; Di Corato 2012). Our paper complements the literature that studies the effects of time preference on the exploitation of common resources. The fishery model has been used as a metaphor for any kind of renewable resource on which the property rights are not well defined (For recent surveys on this topic, see Van Long 2011 and Jorgensen et al. 2010). Levhari and Mirman (1980) and Van Long et al. (1999) analyze the game between time-consistent agents having different discount rates. Nowak (2006) analyze this model by assuming that agents are hyperbolic players whose preferences change over time. Haan and Hauck (2014) consider a common pool problem and propose a solution concept for games that are played among hyperbolic discounters that are possibly naive about their own, or about their opponent’s future time inconsistency. We consider the game between time-consistent and (seemingly) time-inconsistent agents and extend the analysis to CRRA class of utility form. The article is organized as follows. The next section introduces the model and characterizes the equilibrium conditions of the game. In Sect. 3, we define the perfect Bayesian equilibrium in which the agent who is believed to have time-inconsistent preferences and use this misinformation as an advantage. Two different versions of dynamic fishery model are introduced and the main results of the paper are proven in Sects. 4 and 5, respectively. Section 6 concludes.",1
86.0,1.0,Theory and Decision,10 September 2018,https://link.springer.com/article/10.1007/s11238-018-9670-z,Experience in public goods experiments,February 2019,Anna Conte,M. Vittoria Levati,Natalia Montinari,Female,Unknown,Female,Female,"Participants in laboratory economic experiments are often recruited repeatedly. This gives them an opportunity to reflect on their past choices (and outcomes) before revisiting the laboratory. In 1984, Isaac, Walker, and Thomas brought into question whether this constitutes a form of learning that can affect subjects’ contribution behavior. Since then, no other research has specifically investigated this matter. The present study responds to an early call by Ledyard (1995) for a design focusing on the factor of experience, trying to address this noteworthy question. In particular, we investigate whether and how contribution choices and their dynamics in public goods experiments are affected by (1) previous participation in social dilemma-type experiments, which will be referred to as experience; (2) previous participation in experiments different from the social dilemmas, which will be referred to as history.Footnote 1 The process under investigation captures an essential aspect of the real world where individuals—differently than in the laboratory—are possibly familiar and/or experienced with a decision problem. Outside the laboratory, when reexperiencing a specific environment, it is likely that individuals anchor to their past experience when making decisions in the new situation. Therefore, a direct study of the effect of experience and history on subjects’ behavior allows us to tackle, in a public goods setting, the issue raised by Smith (2010) concerning the experimentalist’s interpretation of single play observations as isolated and with no precedents. According to Smith (2010), it is unwarranted to assume that the play of a specific game in the laboratory is unaffected by past experience accumulated in the world or in the laboratory. Accounting for participants’ experience and history has both a direct and indirect positive effect. As a direct effect, it permits a more comprehensive interpretation of the data generated in a certain experiment; as an indirect effect, it leads to an improvement of experimental results in terms of external validity (i.e., requiring that the qualitative relationship between two variables holds across similar environments, Guala 2002) and replicability. To delineate the present study’s perspective on the significance of experience, consider two samples: one drawn from a population of students who have never faced a similar choice situation before (the inexperienced), and another drawn from a population of students who have already experienced such a situation (the experienced). If our analysis demonstrates that the two populations share similar contribution behavior, there is no reason to forcefully select out one or the other type of subjects from the subject pool of the experiment. However, if—as we believe—they do not, drawing experimental samples jointly from the two populations can give rise to disruptive interaction effects which are especially relevant in small samples, particularly when the analysis aims to test treatment effects. Moreover, even when the samples to be compared are drawn from the same population, an additional issue raises concerns. In fact, the experienced subject pool may have been confronted with a higher number of other experiments, and probably more variegated than the inexperienced subject pool. History may matter as well as subjects’ experience and determine behavioral differences, even though the samples to be compared are drawn from the same population. For these reasons, being able to disentangle the effects of these two factors and to assess how they influence subjects’ contribution behavior is essential. The unique point of our paper is, indeed, in the attempt to capture this aspect in a laboratory experiment. Following a common approach in the public goods literature (see, e.g., Fischbacher et al. 2001), we identify different types of players, defined on the basis of their cooperative preferences. To do so, we use a finite mixture model (see Bardsley and Moffatt 2007 or Conte and Levati 2014) considering three types of player: unconditional, conditional, and selfish contributors. The mixture approach together with our data set—which contains information about subjects’ lab background—enable us to separate the effect of experience from that of history on behavior, controlling for first-order beliefs. The ultimate scope of using this approach is to assess the existence of behavioral changes due to the effect of experience and history.Footnote 2 In particular, compared to the existing studies, whose review can be found in the online Appendix A, the novelty of our contribution consists in establishing whether these behavioral changes (if any): (1) are limited to the sphere of subjects’ beliefs about the others’ contribution; (2) are due to a variation in the composition of the population in terms of behavioral types; and (3) are explicable by a combination of the previous two points. Fischbacher and Gächter (2010) show that the decline in contributions in repeated public goods games can be essentially explained by a mismatch between contributions and beliefs about others’ contributions. However, not much is known about how beliefs are affected by history and experience and whether groups that differ in the level of history and experience also differ in the composition of types. Our work aims at filling this gap, exploring the behavioral differences between experienced and inexperienced subjects with a special focus on the role of beliefs and player types. Despite the two mentioned calls for an additional research on “the factor of experience”, a little attention has been paid so far to the impact of previous participation in other experiments (both similar and dissimilar to the public goods environment) on contribution decisions. However, we are not alone in our pursuit of this question. Other experimental fields have already recognized, and thoroughly assessed, the relevance of experience. In industrial organization, it is worth mentioning the studies by Harrison et al. (1987) and Benson and Faminow (1988). Specifically, Harrison et al. (1987) find that experienced subjects are much more effective monopolists than inexperienced ones. Benson and Faminow (1988) notice that, when experiments are conducted with inexperienced subjects, collusion is rarely detected. The opposite holds when experienced subjects are recruited. Moreover, the experienced subjects seem to achieve tacit cooperation (i.e., collusion) more often than inexperienced subjects. In a threshold public goods game, Marwell and Ames (1980) and Isaac et al. (1989) do not observe significant differences when comparing subjects who have previously taken part in similar experiments and subjects who have not. In an alternating-offer bargaining setting, Bolton (1991) finds that previous participation in similar games does not lead to more frequent (equilibrium) play based on payoff maximization. Finally, in the context of allocation games (i.e., dictator and ultimatum games), Matthey and Regner (2013)’s analysis reveals that previously participation in experiments tends to increase the amount subjects reserve for themselves, especially if they already have knowledge of that particular sort of experiments. Concerning the role of experience gained in different games, Haruvy and Stahl (2012), Grimm and Mengel (2012), and Breitmoser et al. (2014) study learning across different games within the same experimental session. Haruvy and Stahl (2012) focus on dissimilar games and find that participants learn sophisticated ways to use information about the game and the history of play, updating their models of how other players reason. Grimm and Mengel (2012) demonstrate that there are learning spillovers across games and participants learn to play strategically equivalent games in the same way. Breitmoser et al. (2014) study the club game to analyze whether subjects adapt beliefs when gaining experience and find that subjects learn and change their beliefs of other players’ over time. More in general, Palacios-Huerta and Volij  (2008) and Chi et al. (1981) show that experienced players are able to extrapolate between familiar real-life situations and “similar” laboratory situations. The rest of the paper is organized as follows. Section 2 describes the experimental design, discusses the treatments implemented, and presents the hypotheses about subjects’ behavior. Section 3 sets out some descriptive statistics of the samples and draws some conclusions from the data at the aggregate level. Section 4 develops the econometric model, and presents and discusses its results and relative implications. Section 5 discusses the results of an econometric model that enables to disentangle the effect of experience from that of history on the relative composition of the two samples. Section 6 reports an aggregate analysis per type of player. Conclusion is drawn in Sect. 7.",9
86.0,1.0,Theory and Decision,22 September 2018,https://link.springer.com/article/10.1007/s11238-018-9672-x,Coalitional desirability and the equal division value,February 2019,Sylvain Béal,Eric Rémila,Philippe Solal,Male,Male,Male,Male,"
van den Brink (2007) provides a clarifying axiomatic comparison between the equal division value and the Shapley value (Shapley 1953) for cooperative games with transferable utility (simply games henceforth). He replaces the Null player axiom invoked in the classical characterization of the Shapley value, which imposes a null payoff to a player who contributes nothing to coalitions, by the Nullifying player axiom, which imposes a null payoff to a player whose coalitions have a null worth. The first axiom is defined from the marginal contributions of a player, while the second is not. The marginal contributions of players to coalitions have been at the heart of many concepts in cooperative game theory. They are the corner stone of the definitions of the Shapley value (1953) and the Banzhaf value (1965). Both values are obtained by averaging, in a certain sense, the players’ marginal contributions. The marginal contributions are also used to define popular axioms such as the axiom of equal treatment of equals: two players with the same marginal contributions should end up with the same payoffs. Another stronger axiom is the axiom of desirability introduced in Maschler and Peleg (1966). If a player enjoys marginal contributions that are at least as large as the marginal contributions of another player, then the first player should obtain a payoff at least as large as the second player’s payoff. This desirability relation among the players originates from Isbell (1958) and has been studied extensively to evaluate the influence of voters on the class of simple games (see also Berghammer and Bolus 2012; Courtin and Tchantcho 2015; Molinero et al. 2015, among others) and the ordinal equivalence of values (Nembua and Wendji 2016). The axiom of desirability is often invoked in the characterization of classes of values such as the two classes of equal sharing values (van den Brink and Funaki 2009; van den Brink et al. 2016), the procedural values (Malawski 2013), the egalitarian Shapley values (Casajus and Huettner 2013), a class of solidarity values (Béal et al. 2017), or to delimit subclasses of the linear, efficient, and symmetric values (Levínský and Silársky 2004; Radzik and Driessen 2013). This note aims to emphasize the difference between the Shapley value and the equal division value by taking a route alternative to van den Brink (2007), which is inspired by the desirability relation. We investigate to what extent the axiom of equal treatment of equals has to be reinforced so as to single out the equal division value instead of the Shapley value among the values satisfying efficiency and the classical axiom of additivity. In particular, we would like to achieve this result without relying on an extra axiom such as the nullifying player axiom. More specifically, we exploit the extension of the desirability relation to coalitions as proposed by Lapidot (1968) for simple games (see also Peleg 1980, 1981; Einy 1985; Einy and Neyman 1988; Einy and Lehrer 1989; Carreras and Freixas 1996) to construct three new axioms. These new axioms are stronger than equal treatment of equals and desirability: any value satisfying one of the new axioms also satisfies equal treatment of equals and desirability, while the converse is not true except in very specific cases. The first new axiom, called coalitional desirability, imposes that if a first coalition has contributions at least as large as the contributions of a second coalition, then the total payoff in the first coalition should be at least as large as the total payoff in the second coalition. The second new axiom is similar except that it is based on the per-capita contributions and per-capita payoffs within the coalitions. We call it average coalitional desirability. The third new axiom, called uniform coalitional desirability, is weaker than the first two in that it only applies to coalitions with the same number of players. Other axioms in which the cumulated payoffs of two coalitions are compared include balanced collective contributions (Béal et al. 2016b) for classical cooperative games, coalitional symmetry (Owen 1977) for cooperative games enriched by a coalition structure, and component fairness (Herings et al. 2008) for cooperative games enriched by an undirected graph, among others. Our results underline that the axioms of equal treatment of equals and desirability cannot be reinforced in any way. We show that the requirement imposed by coalitional desirability is too strong in the sense that there is no value satisfying this axiom and efficiency at the same time whenever the game contains at least three players. This is no longer the case when coalitional desirability is replaced by average coalitional desirability: the latter axiom is compatible with efficiency. More specifically, we show that the combination of average coalitional desirability, efficiency, and the classical axiom of additivity characterizes the equal division value. Therefore, one can move from the Shapley value to the equal division value by dropping the null player axiom and reinforcing equal treatment of equals into average coalitional desirability. Finally, for classes of games with at least five players, this characterization of the equal division value still holds when average coalitional desirability is replaced by uniform coalitional desirability. Our work continues a literature on the equal division values which has received a renewed interest in recent years, especially since van den Brink (2007). We refer to van den Brink and Funaki (2009), Kamijo and Kongo (2012), Béal et al. (2014, 2016a), Béal et al. (2015a, b), Radzik and Driessen (2016), and Kongo (2018) for recent contributions. Aadland and Kolpin (1998) provide a real-life application of the equal division value. The rest of the note is organized as follows. Section 2 presents cooperative games with transferable utility, the axioms, and the equal division value. The results are stated and proved in Sect. 3. Section 4 provides concluding remarks.",8
86.0,1.0,Theory and Decision,10 November 2018,https://link.springer.com/article/10.1007/s11238-018-9676-6,A refinement of the uncovered set in tournaments,February 2019,Weibin Han,Adrian Van Deemen,,Unknown,Male,Unknown,Male,"A tournament is presented by an ordered pair (X, P) where X is a set of alternatives and P is a complete and asymmetric binary relation on X. In this paper, it is assumed that X is nonempty and finite and that xPy is read as x dominates y for any x, y in X. A tournament (X, P) is said to be regular if any two alternatives in X dominate an equal number of alternatives. The main subject in the theory of tournaments are solutions which assign a nonempty subset of X to any given (X, P). Since the Condorcet winner, which is an alternative that dominates every other alternative, may not exist, the top cycle set was proposed by Schwartz (1972) as a generalized notion of the Condorcet winner. It has been shown in Schwartz (1986) that the top cycle set is nonempty for every possible tournament. However, this set may include all the alternatives under consideration. In this case, this solution may not discriminate among alternatives at all. Furthermore, it may contain Pareto inefficient alternatives when the dominance relation is derived from pairwise majority comparisons, as pointed out by Deb (1977). In view of these defects, the uncovered set was formulated by Miller (1980) as a refinement of the top cycle set. As shown in Miller (1980), the uncovered set precludes the presence of Pareto inefficient alternatives successfully. It may be worth noting that the uncovered set is also the largest tournament solution that cannot contain Pareto dominated alternatives (See Brandt et al. 2016b). However, the uncovered set may also include every alternative under consideration in spite of the fact that the tournament is irregular. That means, as a tournament solution, the uncovered set may not be discriminating enough. There are several noted tournament solutions in the literature that may be seen as refinements of the uncovered set. These are the Copeland winner set in Copeland (1951), the Banks set in Banks (1985), the minimal covering set in Dutta (1988), the tournament equilibrium set in Schwartz (1990) and the union of minimal extending sets in Brandt et al. (2017). A pleasant advantage of the Copeland winner set is that it is always a proper subset of X as long as (X, P) is irregular. This property is not satisfied by the uncovered set, and in this sense, the Copeland winner set is a more decisive solution than the uncovered set. However, in some cases, the Copeland winner set may be a dominated subset of the uncovered set in the sense that alternatives in the Copeland winner set are dominated by any other alternative in the uncovered set. That means, the Copeland winner set may only choose the alternative(s) from the uncovered set which are worse than any other unchosen alternatives within the uncovered set. Given this point, it might be argued that the Copeland winner set is an unsatisfactory refinement of the uncovered set. By comparison, it has been shown in Laslier (1997) that none of the Banks set, the minimal covering set and the tournament equilibrium set may be a dominated subset of the uncovered set. Moreover, due to the minimal extending sets being included in the Banks set (Brandt et al. 2017), the union of minimal extending sets is not a dominated subset of the uncovered as well. Regrettably, given an irregular tournament, all of them may choose all the alternatives under consideration. In other words, these refinements fail to remove the weakness of the uncovered set in discriminating among alternatives. In view of the above facts, it is worth exploring a restrictive theory with more discriminatory power than the uncovered set, but which avoids the weakness of the mentioned refinements of the uncovered set. With this motivation, we put forward a new solution called the unsurpassed set. It will be shown that the unsurpassed set is nested between the uncovered set and the Copeland winner set. More importantly, the unsurpassed set avoids the mentioned weaknesses of the uncovered set and that of the Copeland winner set. We also investigate how the unsurpassed set changes when the dominion of a chosen alternative is reinforced and the dominance relation among the unchosen alternatives is unaltered. Moreover, we make a comparison with the solutions of the uncovered set and of the Copeland winner set in this respect. Besides, it will be shown that the unsurpassed set fails to satisfy the choice-theoretic properties: stability, composition consistency and idempotence. The rest of this paper is organized as follows. In Sect. 2, we recall necessary definitions, notations and conclusions concerning tournaments and their solutions which are partly from Brandt et al. (2016a). Section 3 is devoted to the solutions of the top cycle set, of the uncovered set and of the Copeland winner set. Here, we display the disadvantages of these solutions in a detailed way. We propose the unsurpassed set and investigate which properties are satisfied or violated by this solution in Sect. 4. Finally, we end this paper with some short concluding remarks in Sect. 5 where our results and future research topics are roughly discussed.",5
86.0,1.0,Theory and Decision,12 October 2018,https://link.springer.com/article/10.1007/s11238-018-9675-7,The complexity of shelflisting,February 2019,Yongjie Yang,Dinko Dimitrov,,Unknown,Male,Unknown,Male,"There are at least two main research directions in recent works in economics devoted to the study of order or frame effects on consumers’ behavior. The first one adopts a choice theoretic approach and provides foundations for encompassing non-standard behavior models (cf. Masatlioglu and Ok 2005; Rubinstein and Salant 2006; Salant and Rubinstein 2008; Bernheim and Rangel 2009), while the second direction incorporates frames as part of players’ strategy spaces and analyzes the structure of the corresponding market equilibrium outcomes (cf. Eliaz and Spiegler 2011; Spiegler 2014). A complementary viewpoint is provided by experimental studies in the marketing literature (cf. Valenzuela and Raghubir 2009; Valenzuela et al. 2013), with the focus being on consumers’ beliefs about the organization of product displays and their impact on position-based consumers’ preferences over products as well as on retailers’ actual shelflistings. Our starting point in the present paper is that of a single shelf designer who has to arrange a given number of products on a shelf in a way that maximizes his profit. In doing so, he is facing a finite set of consumers who select a single product from the shelf using a pre-specified choice rule. However, in sharp contrast to the cited works, we analyze the effects of consumers’ behavior on the optimal shelflisting from a computational complexity perspective. For this, we set the shelf designer’s product arrangement problem (denoted by PA) and study its computational complexity when consumers are either rational maximizers, follow a satisficing procedure for choosing from lists, or apply successive choice when purchasing their products (see Sect. 2 for the corresponding definitions). Common examples where the way in which products are displayed does affect consumer choice include buying decisions from an e-commerce website where items are displayed sequentially, or the selection process of a combo-menu from a list of menus in some fast-food center. Imagine for instance a situation where a consumer is looking for a new laptop. If she was quite satisfied with her old laptop and operates under some time constrains, then applying a satisficing procedure completely makes sense: she just selects the first laptop from the displayed ones which is better than her old one. As another example one could consider a situation in which a consumer buys a certain type of product for first time and the corresponding comparison of products’ features is difficult and again time consuming. Then successive comparison of only two products at a time would reduce the complexity of the consumer choice (cf. Manrai and Sinha 1989; Gigerenzer et al. 1999). In the formulation of the product arrangement problem we explicitly take the mentioned choice procedures into account. We show that this decision problem is computationally easy when consumers are rational (Theorem 1), while turning to be in general more difficult when they use a satisficing choice rule (Theorems 2 and 3) as then to become hard for the case of successive choice (Theorems 4–7). The term “in general” stands as to indicate the sensibility of our results with respect to the following features of the decision problem. First, we allow a consumer to encounter the products on the shelf either from left to right or from right to left (the product arrangement problem when all consumers check the list in the same direction (from left to right) is denoted by SE-PA). Second, in the case of successive choice, allowing for non-transitive consumer preferences makes our results dependent on the size of the corresponding top cycles, that is, on the number of consumers’ top favorite products. The main contribution of this paper is the initialization of the study of optimal shelflisting where profit maximization is combined with different procedures for making consumer choices from lists. To the best of our knowledge, this is the first study in the literature in which the two aspects are combined and the computational complexity of the corresponding decision problems is analyzed. There are two exceptions with respect to the study of the (computational) complexity of choice behavior we would like to mention. The first one (Apesteguia and Ballester 2010) is concerned with the computational complexity of finding a minimal collection of rationales (preference relations) that rationalizes choice behavior, the difference however being that the set of alternatives one is choosing from has no list structure. The focus on the three types of consumers’ behavior for the case of lists has been partially motivated by the corresponding findings in Salant (2011) with respect to their state complexity. For instance, while the state (or procedural) complexity of rational choice is much higher than the one of satisficing choice [see Propositions 2 and 3 in Salant (2011)], we show that the PA problem in the latter case is NP-hard, while being polynomial-time solvable in the former case. The corresponding statement with respect to the successive choice rule also applies. The rest of the paper is structured as follows. In Sect. 2 we provide the basic definitions with respect to lists, consumer preferences and the choice functions we consider. Section 3 is then devoted to the problem formulation and the case when consumers are rational maximizers, while Sect. 4 contains our results for consumers following a satisficing procedure for product selection. Section 5 contains then the ways in which the difficulty of a shelf designer’s task depends on the number of consumers’ top favorite products as well as on the direction in which they encounter the listed alternatives. We conclude in Sect. 6 with a discussion on various ways in which the shelf designer’s problem can be extended as well as on social choice interpretations which, when attached to the model, indicate promising directions for further research",
86.0,1.0,Theory and Decision,23 November 2018,https://link.springer.com/article/10.1007/s11238-018-9679-3,Robust program equilibrium,February 2019,Caspar Oesterheld,,,Male,Unknown,Unknown,Male,"Much has been written about rationalizing non-Nash equilibrium play in strategic-form games. Most prominently, game theorists have discussed how cooperation may be achieved in the prisoner’s dilemma, where mutual cooperation is not a Nash equilibrium but Pareto-superior to mutual defection. One of the most successful approaches is the repetition of a game, and in particular the iterated prisoner’s dilemma (Axelrod 2006). Another approach is to introduce commitment mechanisms of some sort. In this paper, we will discuss one particular commitment mechanism: Tennenholtz’s (2004) program equilibrium formalism (Sect. 2.2). Here, the idea is that in place of strategies, players submit programs which compute strategies and are given access to each other’s source code. The programs can then encode credible commitments, such as some version of “if you cooperate, I will cooperate”. As desired, Tennenholtz (2004, Sect. 3, Theorem 1) shows that mutual cooperation is played in a program equilibrium of the prisoner’s dilemma. However, Tennenholtz’ equilibrium is very fragile. Essentially, it consists of two copies of a program that cooperates if it faces an exact copy of itself (cf. McAfee 1984; Howard 1988). Even small deviations from that program break the equilibrium. Thus, achieving cooperation in this way is only realistic if the players can communicate beforehand and settle on a particular outcome. Another persuasive critique of this trivial equilibrium is that the model of two players submitting programs is only a metaphor, anyway. In real life, the programs may instead be the result of an evolutionary process (Binmore 1988 pp. 14f.) and Tennenholtz’ equilibrium is a hard to obtain by such a process. Alternatively, if we view our theory as normative rather than descriptive, we may view the programs themselves as the target audience of our recommendations. This also means that these agents will already have some form of source code—e.g., one that derives and considers the program equilibria of the game—and it is out of their realm of power to change that source code to match some common standard. However, they may still decide on some procedure for thinking about this particular problem in such a way that enables cooperation with other rationally pre-programmed agents. Noting the fragility of Tennenholtz’ proposed equilibrium, it has been proposed to achieve a more robust program equilibrium by letting the programs reason about each other (van der Hoek et al. 2013; Barasz et al. 2014; Critch 2016). For example, Barasz et al. (2014, Sect. 3) propose a program FairBot—variations of which we will see in this paper—that cooperates if Peano arithmetic can prove that the opponent cooperates against FairBot. FairBot cooperates (via Löb’s theorem) more robustly against different versions of itself. These proposals are very elegant and certainly deserve further attention. However, their benefits come at the cost of being computationally expensive. In this paper, I thus derive a class of program equilibria that I will argue to be more practical. In the case of the prisoner’s dilemma, I propose a program that cooperates with a small probability and otherwise acts as the opponent acts against itself (see Algorithm 1). Doing what the opponent does—à la FairBot—incentivizes cooperation. Cooperating with a small probability allows us to avoid infinite loops that would arise if we merely predicted and copied our opponent’s action (see Algorithm 2). This approach to a robust cooperation program equilibrium in the prisoner’s dilemma is described in Sect. 3. We then go on to generalize the construction exemplified in the prisoner’s dilemma (see Sect. 4). In particular, we show how strategies for the repeated version of a game can be used to construct good programs for the one-shot version of that game. We show that many of the properties of the underlying strategy of the repeated game carry over to the program for the stage game. We can thus construct “good” programs and program equilibria from “good” strategies and Nash equilibria.",3
86.0,2.0,Theory and Decision,31 January 2019,https://link.springer.com/article/10.1007/s11238-019-09687-7,Axiomatizations of the proportional Shapley value,March 2019,Manfred Besner,,,Male,Unknown,Unknown,Male,"In contrast to Thomas (1969, 1974), who asserts that all cost allocation methods are arbitrary and no one allocation scheme can be defended against all others, we have, on the one hand, a large group of economists which prefers the traditional cost accounting practices. On the other hand, there exists a small group of economists and academics which prefers cost allocation based on solutions to cooperative games with transferable utility dominated by the Shapley value, e.g. Shubik (1962), Spinetto (1975), Roth and Verrecchia (1979), Young (1985a), Young (1985b), Leng and Parlar (2009), and Dehez and Tellone (2013). Moriarity (1975) states that: “A proposal for a new allocation procedure can be justified only on the basis of the advantages of the proposed method over existing methods.” An empirical study by Barton (1988) shows a dramatic preference for the proportional solution, called Moriarity’s method (Moriarity 1975), also known as proportional rule, compared to the nucleolus or the Shapley value. Banker (1981) uses an axiomatic approach. He further analyzed some shortcomings of the Shapley value in cost allocation, especially the additivity axiom is considered questionable. It renders the allocation sensitive to the way that cost centers are used or organized. Banker shows in an example that the allocations can differ significantly if two cost centers are merged and considered as a single entry. His own proposal for an axiomatization in the context of cost allocation contains a splitting axiom instead of additivity. It turns out that the unique value of his axiomatization is identical with the proportional rule (Moriarity 1975). In contrast, some other authors stress the disadvantages of the proportional rule and suggest the Shapley value. For example, Amer et al. (2007) criticize the restricted domain of the proportional rule, the lack of additivity, and a doubly discriminatory level, and that it does not take into account most of the marginal contributions. The last point of criticism is avoided by the proper Shapley values (van den Brink et al. 2015) or the proportional value, developed by Ortmann (2000) and Feldman (1999) simultaneously. Feldman also suggests his value to cost allocation, gives a short overview of proportional cost allocation, and points to Gangolly (1981) who introduced a new cost allocation scheme, denoted as “Independent Cost Proportional Scheme (ICPS)”. In this scheme, Gangolly used a (proportional) weighted Shapley value for each given coalition function \(\overline{v}\), where the weights are the worths of the singletons \(\overline{v}(\{i\})\) for every player i. Yet, a general formalization as a TU-value and an axiomatic characterization were still missing. Independently, there was a “rediscovery” of this value by Besner (2016) and Béal et al. (2018). Both denote their non-linear TU-value “proportional Shapley value” and give an axiomatization by efficiency and proportional balanced contributions, in spirit to the axiomatization of the weighted Shapley values with weighted balanced contributions (Myerson 1980; Hart and Mas-Colell 1989). In addition, they point out that the proportional Shapley value inherits many of the properties of the weighted Shapley values. Besner (2016) offers some extensions of the proportional Shapley value: for example, to graphs and level structures. 
Béal et al. (2018) give economic applications of the proportional Shapley value. They emphasize that an easy transfer of results, valid for the weighted Shapley values, is no longer possible if we regard a class of games with varying characteristic functions. The proportional Shapley value is not linear and does not satisfy consistency in the sense of Hart and Mas-Colell (1989). Thus, Béal et al. (2018) introduce weak versions of both axioms. With the help of a proportional potential, they can prove axiomatizations of the proportional Shapley value which differ from axiomatizations for the Shapley value only in one axiom and rely, inter alia, on weak linearity, weak consistency, or another variant of proportional balanced contributions. The aim of this paper is to establish the proportional Shapley value as an application-relevant allocation scheme, where there are asymmetries that are included exclusively in the underlying game and not in exogenously given weights. Two new axioms make a main difference to the Shapley value. The first one, called proportionality, is a proportional counterpart to symmetry: the payoffs to two weakly dependent players, i.e., the marginal contribution of one of these players to any coalition which contains only one of both players is only his singleton worth, are proportional to the singleton worths of each other. Nowak and Radzik (1995) give a similar axiom for the weighted Shapley values, called \(\omega \)-mutual dependence. The second axiom, called player splitting, is related to Banker (1981). If a player splits into two new players, the payoff to unconcerned players does not change under the condition that the new players contribute together the same to the game as the original player. Radzik (2012) presents a similar idea in the opposite direction by his amalgamating payoffs axiom: players who build a partnership (Kalai and Samet 1987) amalgamate to a new player. Our two new axioms enable axiomatic characterizations which are proportional counterparts to the famous characterizations of the Shapley value by Shapley (1953b) and Young (1985a). The paper is organized as follows. Section 2 contains some preliminaries. As the main results, we offer in Sect.  3 an axiomatization which is close to the axiomatization of the Shapley value by Shapley (1953b) and we present in Sect. 4 an axiomatization which is close to the axiomatization of the Shapley value by Young (1985a). Finally, in Sect. 5, we introduce the player splitting property which leads to two axiomatizations as corollaries. Within this section, examples illustrate the proportional Shapley value in the context of cost allocation, motivate the player splitting property and show an inconsistency of the Shapley value in this case. Section 6 gives a short conclusion. All the proofs, related lemmas, and the logical independence of the axioms used for characterization are relegated to the appendix (Sect. 7).",17
86.0,2.0,Theory and Decision,03 December 2018,https://link.springer.com/article/10.1007/s11238-018-9681-9,Rationalizable strategies in games with incomplete preferences,March 2019,Juho Kokkala,Kimmo Berg,Jirka Poropudas,Male,Male,Male,Male,"This paper examines games with incomplete preferences (Bade 2005). A player has incomplete preferences when he is unable to compare or is indecisive between some of the outcomes. This may be due to the fact that the outcomes are represented by multiple conflicting criteria, that the outcomes are simply uncertain, or that the player represents a group of individuals. Incomplete preferences have been mainly studied in the context of decision-making problems (Aumann 1962; Ok 2002; Heller 2012). In non-cooperative games, incomplete preferences have been studied in the special case of multicriteria games (Shapley 1959; Blackwell 1956; Corley 1985; Borm et al. 1988; de Marco and Morgan 2007; Marmol et al. 2017). Excluding the multicriteria games, only few papers have examined incomplete preferences in non-cooperative game models (Bade 2005; Park 2015; Shafer and Sonnenchein 1975). This paper extends the solution concept of rationalizability to the games with incomplete preferences. Nash equilibrium is the main solution concept in game theory and it assumes that the players have correct beliefs about their opponents’ strategies, i.e., they know the strategies that their opponents are going to choose. Rationalizability is a more general solution concept that allows the players to have erroneous but rational beliefs (Bernheim 1984; Pearce 1984), which means that the set of rationalizable strategies always contains the set of Nash equilibria. The standard rationalizability means that each player maximizes the expected utility given her probabilistic belief. However, the maximization of expected utility may not be possible under incomplete preferences or nonprobabilistic beliefs. This may, e.g., happen in ordinal games (Durieu et al. 2008) where the players only have a preference order but no numeric values are assigned to the outcomes. Thus, we need to generalize the definition of rationalizability. We propose a notion of rationalizable strategies, where the players have nonprobabilistic beliefs and they choose nondominated strategies given their beliefs. Nonprobabilistic beliefs mean that the players only reason about what pure strategies the opponents may choose but do not need to assign any probabilities to these strategies. Nondomination means that a player does not select a strategy if another strategy yields a better outcome with all combinations of the possible strategies of the other players given her belief. The probabilistic and nonprobabilistic beliefs have been discussed in Perea (2014). Chen et al. (2016) extend rationalizability beyond the probabilistic beliefs and expected utility maximization. The non-expected utility models have also been examined in Jungbauer and Ritzberger (2011) and Beauchêne (2016). Moreover, the closed under rational behavior (curb) sets (Basu and Weibull 1991) are closely related to the rationalizable strategies and our solution concept as well. Especially, the maximal tight curb sets are very close to our notion when the players’ preferences are complete, except we use nondominated strategies whereas the curb sets are defined by the best-response correspondences. To our knowledge, this paper is the first to study rationalizability in the games with incomplete preferences. Bade (2005) has shown that the Nash equilibria in games with incomplete preferences correspond to the union of Nash equilibria in all the completions of the game. More recently, Park (2015) has examined the existence of Nash equilibrium in potential games with incomplete preferences. Incomplete preferences have also been considered in nonmonetized games (Li 2013; Xie et al. 2013), where the preference order is defined in a common outcome space for all players, whereas in our framework, the preference orders for each player are defined directly over the strategy combinations. The literature on nonmonetized games is also focused on generalizations of Nash equilibrium. In this paper, we show that the sets of rationalizable strategies are the maximal mutually nondominated sets. The mutual nondominance is defined so that the sets of strategies are mutually nondominated if they contain no dominated strategies with respect to each other. We also show that providing more precise preference information does not enlarge the sets of rationalizable strategies. That is, if new preferences are added over some pairs of outcomes for which a player was previously indecisive and all the original preferences are maintained, then there will be no new rationalizable strategies. We apply our framework to multicriteria games (Shapley 1959; Blackwell 1956; Corley 1985; Borm et al. 1988; de Marco and Morgan 2007; Marmol et al. 2017), which is a special class of games with incomplete preferences. In the multicriteria games, the outcomes are evaluated as vector-valued payoffs, where each component describes how good the outcome is with respect to that particular criterion. Since an outcome can be better in one criterion but worse in another criterion, it is natural that the players may have incomplete preferences over the outcomes in multicriteria games. The main solution concept in the existing literature on multicriteria games is the multicriteria extension of Nash equilibrium (e.g., Shapley 1959; Corley 1985; Borm et al. 1988; de Marco and Morgan 2007; Marmol et al. 2017). A combination of strategies is an equilibrium if the strategy of each player is nondominated when other players play the equilibrium strategies. However, this equilibrium concept does not take into account information about the relative importance of criteria. In this paper, we use weights to represent the importance of the criteria and allow the weights to be set-valued. This idea has been proposed in the multicriteria/multiattribute decision analysis literature (e.g., White et al. 1982; Kirkwood and Sarin 1985; Hazen 1986; Weber 1987; Salo and Hämäläinen 1992, 2010). In the game context, Monroy et al. (2009) has considered the sets of feasible weights in a cooperative bargaining setting, while in the noncooperative setting using sets of feasible weights has been proposed only by us and independently by Marmol et al. (2017), who uses the Nash equilibrium as the solution concept. Our framework enables analyzing the impact of additional preference information about the relative importance of the criteria to the solutions of the multicriteria games. To our knowledge, this is the first paper to consider rationalizable strategies in the multicriteria games while representing incomplete preference information as sets of feasible weights. The paper is structured as follows. Games with incomplete preferences as well as the rationality concept are defined in Sect. 2, and the rationalizable strategies are defined in Sect. 3. Sect. 4 provides properties of the sets of rationalizable strategies. First, the characterization in terms of mutual dominance is given in Sect. 4.1. Then, the relation between rationalizable strategies and the iterative elimination of dominated strategies is discussed in Sect. 4.2. The existence of the rationalizable strategies in the case of finite strategy sets and possible nonexistence in the infinite case are shown in Sect. 4.3. The result that adding preference information does not lead to any additional rationalizable strategies is shown in Sect. 4.4. Multicriteria games with incomplete preference information are discussed in Sect. 5. Examples of a game with incomplete preferences with a finite number of strategies as well as of a multicriteria game with an infinite number of strategies are given in Sect. 6. Finally, concluding remarks are presented in Sect. 7.",5
86.0,2.0,Theory and Decision,12 November 2018,https://link.springer.com/article/10.1007/s11238-018-9677-5,Motives and comprehension in a public goods game with induced emotions,March 2019,Simon Bartke,Steven J. Bosworth,Gabriele Chierchia,Male,Male,Female,Mix,,
86.0,2.0,Theory and Decision,10 November 2018,https://link.springer.com/article/10.1007/s11238-018-9678-4,Preferences over procedures and outcomes in judgment aggregation: an experimental study,March 2019,Takuya Sekiguchi,,,Male,Unknown,Unknown,Male,"Social choice theory has shown that collective decisions vary depending on the decision-making rule adopted. Such findings motivate us to choose decision-making procedures carefully. Recent social choice theory focuses not only on the aggregation of preferences but also on the aggregation of judgments on logically connected issues, or “judgment aggregation” (List and Pettit 2002; List and Puppe 2009; Grossi and Pigozzi 2014). A scenario presenting what is commonly referred to as the “doctrinal paradox” is often used to illustrate judgment aggregation. Imagine a situation where a university committee consisting of three referees is discussing whether a candidate deserves a tenure-track position (This example has already been shown in Bovens and Rabinowicz 2006; List 2006; Dietrich and List 2007b). The candidate is evaluated on two criteria. The first criterion is whether the candidate is outstanding in terms of research abilities. The second criterion is whether the candidate is outstanding in terms of teaching capabilities. Each referee has to decide his/her position (true or false) on each criterion. It is assumed that the department’s constitution specifying that a candidate satisfying both criteria should be accepted has been unanimously agreed to and adopted by the referees in advance. This implies that the truth value of each referee’s judgment regarding his/her conclusion to accept or reject the candidate must be equivalent to the truth value of the conjunction of his/her judgments on the two criteria. The doctrinal paradox here consists of three parts: (1) the premises, which correspond to the two criteria in this example, (2) the conclusion, which corresponds to the issue of whether the candidate should be accepted, and (3) the connection rule, which corresponds to the department’s constitutional specification. Table 1 shows the judgments of the individual referees with regard to each criterion and their conclusions via the connection rule. Aggregating their binary evaluations for each proposition using the simple majority rule generates a logically inconsistent set of collective judgments, even though the set of each referee’s judgment is logically consistent. That is, even though the majority of the committee judges that the candidate is outstanding in terms of research abilities and outstanding in terms of teaching capabilities, the majority concludes that the candidate should be rejected. Two voting procedures designed to address such collective inconsistency have been intensively studied: the premise-based procedure (PBP) and the conclusion-based procedure (CBP) (e.g., List 2005, 2006; Bovens and Rabinowicz 2006; Spiekermann 2013). PBP derives a collectively accepted outcome from the collective judgments on the premises via the connection rule. In the above example, because the majority judges both of the premises to be true, PBP concludes that the candidate should be accepted. On the other hand, CBP aggregates only individual judgments on the conclusion, ignoring those on the premises from which individual judgments on the conclusion are derived. In the above example, CBP concludes that the candidate should be rejected according to the collective judgment on the conclusion. Throughout this paper, we distinguish between the collective outcome and the collective judgment on the conclusion. The former is the group’s decision on what will be carried out (e.g., accepting or rejecting a candidate) through a given aggregation procedure (e.g., PBP or CBP), while the latter is the proposition-wise majority vote on the conclusion (i.e., the element at the bottom-right corner in Table 1). Social choice theory, including the theory of judgment aggregation, often abstracts the origins of the individual’s preferences or judgments. By contrast, Wolff (1994) explicitly examines the situation where the motive on which an individual’s voting action is based differs among the individuals voting. This is called the mixed-motivation problem. Imagine a situation where voters face a dichotomous choice problem involving choices F and T. Among all voters, F suits the interest of 40% of the voters, while T suits the interest of the remaining 60% (“Self-interest” column in Fig. 1a). However, some voters believe that their own interests conflict with the common good. Let us assume that 80% of all voters believe T to be conducive to the common good, and the remaining 20% believe F to be conducive to the common good (“Common good” column in Fig. 1a). The “Type” column in Fig. 1a shows the various types of voters in the mixed-motivation problem: (1) F suits their individual self-interest, and they believe that F is conducive to the common good; (2) F suits their individual interests, but they believe that T is conducive to the common good; (3) T suits their individual self-interests, but they believe that F is conducive to the common good; and (4) T suits their individual self-interests, and they believe that T is conducive to the common good. If a voter’s belief regarding which alternative is conducive to the common good is assumed to be independent of his/her self-interest, then the expected frequency of each type of voter is as follows: (1) 0.08 (= 0.4 × 0.2), (2) 0.32 (= 0.4 × 0.8), (3) 0.12 (= 0.6 × 0.2), and (4) 0.48 (= 0.6 × 0.8). Wolff (1994) makes the additional assumption that all voters whose self-interest matches F vote for F (framed F in the “Self-interest” column in Fig. 1a), while all voters whose self-interest matches T vote for an alternative which they believe to be conducive to the common good (framed F and T in the “Common good” column in Fig. 1a). Based on this assumption, Types (1)–(3) vote for F, while Type (4) votes for T. Since the former is the majority (= 0.08 + 0.32 + 0.12), F becomes the collectively accepted outcome under the simple majority rule. However, this outcome is paradoxical, because both voters whose self-interest match F [40%: Types (1) and (2)] and voters who believe that F is conducive to the common good [20%: Types (1) and (3)] are in the minority. Presenting this problem, Wolff (1994) shows that the majority rule does not always serve the self-interest of the majority or the majority’s belief of what is in the public interest. a Relationship between the mixed-motivation problem and the conjunctive doctrinal paradox presented by Bovens (2006). b The disjunctive counterpart of a, which is obtained by exchanging the labels F and T. T and F surrounded by thin black lines in columns labeled “Self-interest” and “Common good” indicate the alternatives that individuals voted for. The grey rounded rectangle in a (in b) indicates that individuals voted for T (for F) if and only if they think that accepting T (accepting F) is both in their own self-interest and is conducive to the common good in both the doctrinal paradox and the mixed-motivation problem, respectively Bovens (2006) points out that the mixed-motivation problem can be recast as the doctrinal paradox. Notice that in the mixed-motivation problem, individuals vote for T, i.e., Type (4) in Fig. 1a, if and only if they think that T is in their self-interest and conducive to the common good. Let us here regard the two labels, T and F, as binary evaluations for the given issues (i.e., T is read as “true”, and F, “false”.). Then, once we assign one issue, namely whether an option is in a voter’s own self-interest, to the first premise and the other issue of whether it is conducive to the common good to the second premise, we see the situation as if three voters, who are classified into Types (2)–(4) in the mixed-motivation problem, were involved in the doctrinal paradox. Although Bovens (2006) presents only the transformation of the mixed-motivation problem into the conjunctive doctrinal paradox, based on his idea of transformation, the disjunctive doctrinal paradox can also be obtained from the mixed-motivation problem by exchanging the labels F and T in Fig. 1a. This can be attributed to the fact that in the disjunctive doctrinal paradox, a voter’s judgment on the conclusion is F if and only if s/he judges both premises to be F (Fig. 1b). Note that as demonstrated in List (2005), the event in which PBP chooses T and CBP chooses F as the collective outcome (as shown in Table 1 and Fig. 1a) is the only possible conjunctive doctrinal paradox. This is because a voter judges the conclusion to be T if and only if s/he judges both premises to be T under the conjunctive connection rule. Therefore, if voters judging the conclusion to be T constitute the majority, then more than half of the group members must judge the two premises to be T. Thus, the collective inconsistency in which PBP chooses F and CBP chooses T as the collective outcome cannot occur. For a similar reason, the event in which PBP chooses F and CBP chooses T as the collective outcome (as shown in Fig. 1b) is the only possible disjunctive doctrinal paradox. Modern social choice theory has a general theoretical construction in which collective decision-making rules are regarded as functions whose domain is the set of the possible combinations of individuals’ preferences over alternatives and whose range is the set of possible collective outcomes, i.e., social preference orders (e.g., Arrow’s general possible theorem). Recently, the scope of social choice theory has been extended to include decision-making rules leading to a collective judgment on a logically connected agenda of issues from the combination of individual judgments on each of the issues (e.g., List and Pettit 2002; Dietrich and List 2007a). On the other hand, as Bonnefon (2007) has noted, empirical studies of collective decision-making on logically connected issues have been relatively rare, in contrast to the recent concentration on theoretical and axiomatic approaches. Kameda (1991) is one of the few exceptions. In his empirical treatment of collective decision-making based on PBP and CBP, which, in his terminology, correspond to evidence/verdict-driven styles, his focus was not on the procedural preferences of the voters; rather, it was on differences in the accuracy of the collective decisions based on the two procedures (see Grofman 1985; List 2005; Bovens and Rabinowicz 2006 for the theoretical basis for this issue). In his experiments, participants had no choice regarding which decision-making procedure was to be used. Scenario-based experiments by Bonnefon (2007), which are more closely related to the present study, depicted a situation in which the administrators of a company determine whether an employee should be moved to a new position, based on the structure of a doctrinal paradox with two premises. In some of the scenarios, the administrators’ judgments are based on the condition that the employee must be young (as the first premise) and that she is trilingual (as the second premise). In the other scenarios, the employee is required to be young (as the first premise) and to have strong experience in team management (as the second premise). Given that it is difficult for a young person to be well-experienced in team management, the conditions in this latter type of scenario are referred to as incompatible, in contrast to the conditions in the first type of scenario, which can be considered compatible. Furthermore, the scenarios are characterized in terms of whether the outcome is positive or negative. In the positive condition, the new position is coveted, while in the negative condition the position is one that no one wants to fill. Each of the study participants was assigned to one of four scenarios: 2 (compatible/incompatible conditions) × 2 (positive/negative conditions). Bonnefon (2007) reported that PBP was more likely to be preferred when the two criteria were difficult to satisfy simultaneously (i.e., under the incompatible conditions) and that the procedure leading to the more lenient outcome was likely to be favored. In other words, the procedure entailing the outcome in which the employee will (will not) move to the coveted new position in the positive scenario (the position that no one wants to fill in the negative scenario) tended to be favored. Furthermore, CBP was favored from the aspect of simplicity. While these experiments were limited to the conjunctive doctrinal paradox, Bonnefon (2010) later examined both the conjunctive and disjunctive doctrinal paradoxes. Results again showed that procedures leading to the lenient outcome were likely to be preferred. In both sets of experiments, Bonnefon (2007, 2010) showed that frames and contexts can affect an individual’s procedural preferences, providing significant insights that would be difficult to obtain directly from theoretical studies. The present study conducted scenario-based experiments involving the doctrinal paradox converted from the mixed-motivation problem in order to investigate empirically which decision-making procedure is more likely to be favored—and for what reasons. This approach has two significant and mutually-related features. The first arises from the fact that, in the mixed-motivation problem, each voter’s motive is explicitly identified. Being able to examine these motives enables us to understand to what extent procedural preferences and preferences for collective outcomes are affected by the motives of the voters engaged in the collective decision-making process. Second, given the generality of the problem of conflicts between private interests and the common good, being able to examine this problem has the potential to expand the applicable range of studies on judgment aggregation. Our scenario-based experiment targeted group decision-making in a situation in which permitting or prohibiting smoking in the workplace were the two possible decisions. Participants in the experiment were asked about their smoking history. This approach has several benefits: First, we are able to examine factors affecting the participants’ opinions of the decision-making procedures and collective outcomes depicted in the scenarios by controlling for heterogeneity of past experiences that could lead to differences in their prior evaluations of the collective outcomes, i.e., a difference in their level of tolerance for smoking, in this context. In fact, a previous study showed that the degree of agreement with the statement ‘‘secondhand smoke is a health hazard for nonsmokers’’ was lower among smokers than non-smokers (Stanovich and West 2007). Given such a result, it seems reasonable to believe that the undervaluation of the negative health effects of secondhand smoke could lead smokers to readily support the permitting of smoking in the workplace. Second, because the anti-smoking movement is widespread, how such a social norm relates to participant responses can be analyzed. Preventive measures against passive smoking are now widely accepted in Japan. In fact, the Japanese Ministry of Health, Labor and Welfare (http://www.mhlw.go.jp/toukei/list/dl/h28-46-50_kekka-gaiyo01.pdf) has reported that 85.8% of the Japanese companies sampled in 2016 were taking preventive measures against passive smoking. This rate increases with the size of company (e.g., the rates for companies having 10–29 employees, 100–299 employees, and 1000 or more employees are 83.0%, 96.9%, and 99.4%, respectively). Therefore, we expect that the non-smoking movement in Japan is prevalent enough to regard it as a social norm. These two factors have not been explicitly taken into consideration in previous empirical studies on judgment aggregation. The rest of the present paper is constructed as follows: Sect. 2 describes the experimental design and introduces dependent and independent variables. Section 3 shows the results and possible explanations for them. Finally, Sect. 4 discusses the significance of the present study and possible future directions.",2
86.0,2.0,Theory and Decision,30 November 2018,https://link.springer.com/article/10.1007/s11238-018-9680-x,“But everybody’s doing it!”: a model of peer effects on student cheating,March 2019,Marcelo de C. Griebeler,,,Male,Unknown,Unknown,Male,"Plenty of evidence of the high prevalence of peer effect on dishonest behavior has been reported in the literature on Behavioral Economics (Gino et al. 2009; Innes and Mitra 2013; Lucifora and Tonello 2015). In fact, the idea that the individual decision to behave dishonestly may be affected by the realization that other individuals are behaving in the same way has been studied both empirically and experimentally in the past few years. In particular, most of the aforementioned evidence concerns academic cheating, because both of the availability of data about grades and performances, and the easiness to perform experiments with students, mostly in colleges and universities. Several studies have shown that those effects are expressive in different parts of the world (see, for example, McCabe et al. 2001 for the US, García-Villegasa et al. 2015 for Latin America and Teodorescu and Andrei 2009 for Romania).Footnote 1 In this paper, we theoretically model peer effects on academic dishonesty by assuming that there is a moral cost associated with cheating,Footnote 2 based on the empirical evidence reported, for example, by Gneezy (2005) and López-Pérez and Spiegelman (2013). More importantly, our main assumption is that the moral cost of cheating decreases as the number of peers who also cheat increases. The idea is that when there is a large number of members of a given group performing a certain action, this may be seen as standard behavior of such a group. Thus, even if the action is individually considered to be dishonest (or immoral), the perceived moral norm of the group may make the individual think something like “To do that is not that wrong, because everybody’s doing it” (Gino et al. 2009). We construct a game in which two students must choose whether or not to cheat on an exam. Whenever the other student chooses to act unethically, the cheating strategy becomes more attractive because of the lower moral cost. If the probability of being caught and punished is perfectly observed, under quite general conditions we are able to show that there is multiplicity of equilibria, including one in which both cheat and another in which both play fair. In order to select the game’s equilibrium, we assume that the true probability of being caught and punished is individually observed with some noise, such that each student receives a particular signal. Such an assumption allows us to use the Global Games approach, which provides a suitable way to study peer effects. Our findings show that the better the cheating technology and the higher the disutility of effort, the higher the probability of the cheating equilibrium being selected. Those results are robust to extensions which incorporate heterogeneous students and more than two students. By extending the model to incorporate heterogeneous students, we can also investigate whether the level of homogeneity of the class affects the “strength” of peer effect, measured by the probability of occurrence of an equilibrium with both students cheating. Our findings indicate that there is no relation among homogeneity and peer effect, result which can be seen as a theoretical base for papers such as Foster (2006), and that sheds light on a debate that remains open. The extension which allows for more than two students provides a theoretical explanation for an empirical and experimental finding (Carbone 1999) as well, namely that larger classes make dishonest behavior more likely. The literature on peer effects on dishonest behavior has found three main channels by which exposure to other people’s unethical behavior can increase or decrease an individual’s dishonesty (Gino et al. 2009). First, by observing that others have cheated and got away with it, for example, an individual may change—decrease, in this case—his estimate of the probability of being caught cheating. Holding everything else constant, the standard approach of Economics of Crime (Becker 1968) predicts that this would make the dishonest behavior more attractive, since the expected punishment has decreased. In short, knowing that other people act dishonestly raises the incentives to act the same way they do. The second source of influence of other’s individual unethical behavior on the individual decision of acting dishonestly concerns the saliencyFootnote 3 of ethicality at the moment one is considering a particular behavior. The idea is that, when dishonesty is made salient, the individual may pay greater attention to his own moral standards, which may remind him of the consequences—mostly the psychological ones—of his misconduct. Contrary to the first channel, now when people observe someone behaving dishonestly, they tend to become less prone to choosing to act unethically, given that the saliency of these acts increases the (psychological) costs of such a choice. Finally, a third possibility is that, by observing the unethicality of others, people may change their understanding of the social norms related to dishonesty. This effect, however, depends on the degree to which they identify with these other people. In particular, when an in-group member is observed acting dishonestly, other group members may make him or her the standard for the social (moral) norm, which may make them engage in unethical behavior as well. The opposite may happen when group members observe a non-group member behaving dishonestly, because in this case they may want to distance themselves from this “bad apple” in order to maintain a distinctive and positive social identity. The predictions of this third mechanism, therefore, depend on the identification with the individual who is performing the dishonest action. In our model, the channel by which peers’ behavior affects the individual decision of acting dishonestly is quite similar to the latter one. By observing that other group members—individuals who share some common characteristics—are acting unethically, an individual may think that such behavior is the standard and thus the moral cost of behaving in the same way is lower. Among the many possible dishonesty acts, we choose to study academic dishonesty in part because students in a same class see themselves as part of a homogeneous group in terms of social norms.Footnote 4 However, our baseline model may be suitably modified to explain peer effects on other different groups, as long as they are sufficiently homogeneous and thus share some social norm. Our approach, on the other, does not apply to groups whose members do not identify with each other, since we do not take into account the “bad apple” effect. The key ingredient of our model, namely the moral cost associated with the act of cheating, has been already included by other studies in different contexts and in some slightly distinct versions. Battigalli and Dufwenberg (2007), for instance, adopt the concept of Guilt Aversion, derived from the psychological Game Theory literature, which can be seen as a moral cost related to hurting or letting the other player down. The assumption that the number of other people behaving in a given way affects the cost associated with such a behavior, however, is based on the idea that people are concerned with social norms or social image. Some experimental works (Mas and Moretti 2009; Gino et al. 2009) have shown that in fact people behave by taking into account how others of the same group see that behavior—whether according to what is expected from a member, for example. In this context, our definition of moral cost is also related to the literature of Identity Economics (e.g., Akerlof and Kranton 2000). Among the related papers in the literature, the one whose approach is the closest to our definition of moral cost is the study by Anderlini and Terlizzese (2017), who build a simple trust game in which individuals have a socially determined cost of cheating which increases the prevalence of cheating in the population. The impact captured by this work is similar to ours, given that in their model when fewer people in society cheat, those who do are in some sense further away from the social norm, implying a “moral cost”. Nonetheless, our model is different from Anderlini and Terlizzese (2017) in that it focuses specifically on peer effects and in that it uses the Global Games approach. Based on the above discussion, we can state that our main contribution is to provide a reduced-form approach to analyze the peer effects due to social norm or social image. In this context, our model is agnostic about the specific channel but directly assumes the previously identified effects on incentives. To the best of our knowledge, no other study has developed a framework which allows for performing comparative statics in such contexts. In fact, the conclusion about the (lack of) association among the level homogeneity of the group and the strength of the peer effect, for instance, is only possible to be obtained in a reduced-form approach. In addition, the realism of the assumption that students observe the true probability of detection imperfectly, and thus the application of Global Games, is not only novel but also quite useful for analysis such as comparative statics. In addition to the empirical and experimental literature on peer effectsFootnote 5 on dishonesty (e.g., Innes and Mitra 2013 and Gino et al. 2009); our study is related to the broader field which studies social interactions and its effect on the aggregate behavior. The presence of positive social interactions, or strategic complementarities, implies the existence of a social multiplier where aggregate relationships will overstate individual elasticities (Glaeser et al. 2003). As an example of the application of this approach, consider research by Carrell et al. (2008), who by using data from United States military service academies from 1959 to 2002 estimate that one additional college cheater drives approximately 0.61 to 0.75 additional college students to cheat. In equilibrium, these results imply that the social multiplier for academic cheating is approximately three. Other applications include, among others, those by Mas and Moretti (2009) and Fortin et al. (2007), who study peer effect in the workplace and the relation between tax evasion and social interaction, respectively.Footnote 6 The literature on different types of peer effects in education is also close to our model. While the vast majority of the studies investigate how classmates’ behavior influences the academic performance of individuals—measured mostly by grades in exams and courses, few studies address the effects on cheating. Two remarkable and extensive surveys on the subject are those by Sacerdote (2011) and Epple and Romano (2011), who present both theoretical and empirical models. Among the theoretical models, we can cite the study by Arnott and Rowse (1987), who present the first rigorous model of peer effects, the misbehavior model by Lazear (2001) and Banerjee and Besley (1991), who present a model with an educational peer effect operating through informational externality. None of the theoretical models of peer effect in education, however, consider the channel of the moral cost. Finally, we contribute to the incipient literature on the micro-foundations of academic dishonesty. In this field, we have, for instance, the paper by Griebeler (2017a), who models cheating as a three-person game, in which two students must choose whether or not to copy the classmate’s exam, and a professor, who must choose how much effort to exert in trying to catch dishonest students. Other contributions are given by Briggs et al. (2013), who also use Game Theory, but with focus on the collusion among students to cheat on a take-home task, and by Griebeler (2017b), who studies how friendship among student can influence their probability of cheating. However, as the type of cheating analyzed by all these works is different from the one we studied in this paper, they are not able to capture peer effects. The paper is organized as follows. The next section presents our baseline model, composed of two identical students. We discuss the incentives they face and highlight the role of peer effects on their decisions. This section also presents the game’s equilibria and shows how Global Games may be used for equilibrium selection. Section 3 presents some comparative statics and Sect. 4 extends the model by incorporating heterogeneous students and more than two players. Section 5 concludes and suggests other extensions and applications. The proofs of propositions omitted in the text are presented in Appendix A.",8
86.0,2.0,Theory and Decision,09 January 2019,https://link.springer.com/article/10.1007/s11238-018-09683-3,Non-hyperbolic discounting and dynamic preference reversal,March 2019,Shou Chen,Richard Fu,Ziran Zou,,Male,Unknown,Mix,,
86.0,3.0,Theory and Decision,04 December 2018,https://link.springer.com/article/10.1007/s11238-018-9682-8,Information in Tullock contests,May 2019,A. Aiche,E. Einy,B. Shitovitz,Unknown,Unknown,Unknown,Unknown,,
86.0,3.0,Theory and Decision,01 January 2019,https://link.springer.com/article/10.1007/s11238-018-09684-2,Competition among procrastinators,May 2019,Takeharu Sogo,,,Unknown,Unknown,Unknown,Unknown,,
86.0,3.0,Theory and Decision,04 January 2019,https://link.springer.com/article/10.1007/s11238-018-09685-1,What are axiomatizations good for?,May 2019,Itzhak Gilboa,Andrew Postlewaite,David Schmeidler,Male,Male,Male,Male,"Axiomatic decision theory was pioneered in the early twentieth century by Ramsey (1926) and de Finetti (1931, 1937) and achieved remarkable success in shaping economic theory. Bolstered by the axiomatic systems of von Neumann and Morgenstern (1944), Savage (1954), and Anscombe and Aumann (1963), expected utility became the dominant model of individual decision making in economics. When the critiques of Allais (1952, 1953) and Ellsberg (1961) blossomed into a concerted effort to generalize or develop alternatives to expected utility, axiomatic foundations again played a key role. A remarkable amount of economic research is now centered around axiomatic models of decision, both in the classical framework of choices between lotteries or between “Savage acts” , and in other models of risk and uncertainty. What have these axiomatizations done for us lately? Are they leading to advances in economic analysis, or are they perhaps attracting some of the best minds in the field to deal with difficult problems that are of little import? Why is it the case that in other sciences, such as psychology, biology, and chemistry, such axiomatic work is so rarely found? Are we devoting too much time to axiomatic derivations at the expense of developing theories that fit the data? This paper addresses these questions. Section 2 defines what is meant by an “axiomatization”—an axiomatization is an equivalence result relating a theoretical description of decision making to conditions on observable data. Section 3 cites some standard justifications of the axiomatic exercise and sharpens the questions of the previous paragraph to our single central concern: how can economists who study how people behave benefit from an equivalence result? Section 4 provides our response, namely that axiomatic derivations are powerful rhetorical devices, and outlines several ways that axiomatic derivations of decision rules may be useful for economics, even when the decision models are interpreted descriptively.Footnote 1 Some real and imaginary case studies are discussed in Sect. 5, illustrating the role of axioms. This discussion suggests two criteria, presented in Sect. 6, for judging which axiomatic work is most likely to be useful. Specifically, we hold that axiomatizations of general-purpose conceptual frameworks are probably more useful than axiomatizations of specific theories and that axiomatic models that describe rational behavior are likely to be more compelling than those describing specific instances of irrationality. Some concluding comments are offered in Sect. 7.",16
86.0,3.0,Theory and Decision,01 February 2019,https://link.springer.com/article/10.1007/s11238-019-09686-8,A dynamic game analysis of Internet services with network externalities,May 2019,Tatsuhiro Shichijo,Emiko Fukuda,,Male,Female,Unknown,Mix,,
86.0,3.0,Theory and Decision,01 February 2019,https://link.springer.com/article/10.1007/s11238-019-09688-6,Folk theorems in a bargaining game with endogenous protocol,May 2019,Shiran Rachmilevitch,,,Female,Unknown,Unknown,Female,"Rubinstein’s (1982) alternating-offers bargaining game is the central piece in the non-cooperative bargaining literature. In this game, two players alternate roles as proposer and responder, and they exchange proposals on the division of a surplus until a proposal is accepted.Footnote 1 Any real-life bilateral bargaining scenario is some form of back-and-forth conversation, and Rubinstein’s game is the most basic model of such a procedure. Yet, the alternating-offers procedure, like any deterministic one, is rigid, and does not leave room for the possibility that the order of moves in the conversation is itself a strategic variable, the value of which is determined endogenously. Motivated by the need to overcome this rigidity, I study a two-player bargaining game in which the roles of the proposer and responder are determined endogenously. Specifically, the model is as follows. In the beginning of every period t (prior to which no agreement has been reached), the players play the following simultaneous-move game, hereafter the proposer-selection game (PS). Each player i announces a message \(m_i\in \{me,you\}\).Footnote 2 If both say me, no one gets to be the proposer and play moves to \(t+1\); if one says me and the other says you, the player who said me becomes the proposer; if both say you then player 1 becomes the proposer with probability \(\beta \in (0,1)\) and player 2 with probability \(1-\beta \). Given the identities of the proposer and responder (i.e., when the realized profile in the PS game is different from (me, me)), a proposal \(x\in X\) is made, where \(X\subset {\mathbb {R}}_+^2\) is the set of feasible payoffs; if it is accepted by the responder then it is implemented and the game ends, and if it is rejected then play moves to the next period. The set X is allowed to be quite general: essentially, the only assumptions about it are that it is compact and has a strictly decreasing Pareto frontier (it is allowed, for example, to be finite). Player i’s utility from agreement on x in period t is \(\delta _i^{t}x_i\), where \(\delta _i\in (0,1)\) is his discount factor. A player’s utility from perpetual disagreement is zero. The main result is a folk theorem. For every \(x\in X\cap {\mathbb {R}}^2_{++}\), there exists a cutoff \(\delta (x)<1\), such that if (at least) one of the players has a discount factor above \(\delta (x)\), then for every \(y\in X\) that satisfies \(y\ge x\) there exists a subgame perfect equilibrium with immediate agreement on y. The theorem’s proof is by construction. In the first period, a designated proposer offers the equilibrium offer, which the responder accepts; any deviation triggers the most severe punishment—a dictatorial equilibrium starts in the second period, where the identity of the dictator depends on the deviation that took play off the path. The i-th dictatorial equilibrium is such that player i always plays me and j always plays you, which is followed by i’s demanding his most preferred element of X, to which j agrees.Footnote 3\(^,\)Footnote 4 Regardless of whether the implemented payoffs are efficient or not, the implementation itself has a flavor of efficiency to it, as it is achieved by immediate agreement. This mild sense of efficiency, however, is not inevitable: a modification of the construction allows to sustain arbitrarily long delays in equilibrium. The folk theorem follows from the dictatorial equilibria, and these, in turn, exist due to the fact that a player can insist on being the proposer in every period. Thus, simple plausible behavior—being stubborn—can rationalize any bargaining outcome.Footnote 5 There is one important case, however, in which the dictatorial threats can be dispensed with: when the set of feasible payoffs is the unit simplex. In this case, every strictly individually rational and Pareto efficient payoff vector can be implemented in equilibrium with immediate agreement, without relying on dictatorial threats. Here, too, a modification of the construction allows to sustain arbitrarily long delays in equilibrium. The closest paper to the present one is by Kambe (1999). He studies two-player bargaining in infinite discrete time, where each period is divided into two stages: the nomination stage and the proposal stage. In the nomination stage, the players sequentially nominate a proposer, and the last responder is the first to make his nomination.Footnote 6 If both nominate the same player, then play moves to the proposal stage in which the agreed-upon proposer makes an offer. If there is no agreement on the proposer’s identity (i.e., the players’ nominations are different) then there is a one-period delay after which the next nomination stage begins, and a one-period delay also applies once a proposal is rejected. Kambe proves a folk theorem in this model. The main differences with respect to my paper are that (1) Kambe assumes identical discount factors, whereas I do not impose this restriction, and (2) Kambe takes the set of feasible payoffs to be the unit simplex and he only considers efficient offers, whereas here the payoff set is more general and inefficient payoffs can be implemented in the same way that efficient ones are. 
Kambe (2009) is another related paper. There, he studies two-player bargaining in infinite discrete time where in the beginning of each period the players play a simultaneous-move game in which each player has two pure strategies: take initiative and don’t take initiative. If player i takes the initiative and player j does not, then i becomes the proposer. Similarly to Kambe (1999), any other play-configuration leads to a one-period delay and move to the next period. Additionally, the probability that a player takes the initiative depends on publicly observable states of nature, which play the following role: in the first time that a state, say S, is realized, each player i choses a probability of taking the initiative, \(f_i(S)\), and he commits to play according to this \(f_i(S)\) in all future realizations of S. The main question in this paper is what conditions are conducive for alternating-offers. Kambe identifies conditions under which such offers emerge in equilibrium, and conditions under which they do not.Footnote 7 There are some other bargaining papers in which the proposer is selected endogenously, but in which the selection method is quite different from that of the present paper (and from those of Kambe). For example, Board and Zwiebel (2012) and Ali (2015) consider bargaining games in which, in every period, the players compete for the position of the proposer through an auction. Yildirim (2007, 2010) considers such competition not through an auction, but through a contest. The rest of this paper is organized as follows: Sect. 2 contains definitions, Sect. 3 presents the results, and Sect. 4 concludes. Before we proceed, a small comment is in order: the rules of the game are stated with reference to a bargaining-power parameter, \(\beta \), namely the probability that player 1 becomes the proposer when the PS-profile is \((m_1,m_2)=(you,you)\). This parameter, in fact, does not play any role in the analysis, because (you, you) is never played in the equilibria I construct. One can fix \(\beta =\frac{1}{2}\) without losing any of the substance.",2
86.0,3.0,Theory and Decision,25 February 2019,https://link.springer.com/article/10.1007/s11238-019-09689-5,Rational choices: an ecological approach,May 2019,Abhinash Borah,Christopher Kops,,Unknown,Male,Unknown,Male,"In neo-classical economics, to say that a decision-maker (DM) is rational entails that this DM has complete and transitive preferences over the set of relevant alternatives and, in any choice problem, chooses that alternative that is best according to this preference relation.Footnote 1 Since preferences are not directly observable, in terms of its empirical content, this perspective of rationality takes the DM’s choices as the primitive concept. Within the revealed preference tradition, the key theoretical step is to ask whether it is possible to back out a DM’s rational preferences from her observed choices. That is, to ask if it is possible to think of this DM’s choices as if they were the result of maximizing some such underlying preferences. A classic result in choice theory establishes that this is essentially the case when the DM’s choices are internally consistent. In other words, rationality in economics boils down to internal consistency of choice. A related observation that follows is that, within this paradigm, the focus is on what the DM chooses rather than on how she arrives at such choices in terms of the reasoning involved. As is well known, this standard model of rationality in economics has received its fair share of criticism, and here, we would like to re-visit certain aspects of that criticism. One of the earliest and most influential criticisms of this worldview came from Herbert Simon (Simon 1955, 1956). One of the key ways in which Simon’s bounded rationality perspective differed from the standard model was in terms of its focus on not just the outcome of choice—substantive rationality—but also on the decision-making procedure by which such choices are arrived at—procedural rationality. What Simon pointed out was that, when viewed from this procedural perspective, the standard model of rational choice makes way too stringent demands on a DM’s knowledge and cognitive-computational capacities—demands that typical human intelligence would find overwhelming. In this paper, we want to take this criticism of the standard model that it is blind to the decision-making procedure and the cognitive constraints faced therein seriously. Our broad goal is to see if it is possible for a DM to arrive at the stringent ideal of rationality set by the standard model by means of simple and cognitively less demanding heuristic-based reasoning that draws on information regarding choices that may be available in her environment. Specifically, we identify and engage with two types of challenges that the decision-making process may involve. First, we consider the possibility that, owing to limitations of knowledge, a DM’s preferences may not be complete and she may be unable to rank every pair of alternatives. Specifically, we consider a DM who has asymmetric and transitive preferences that are not necessarily complete. If that is so, then it may not always be possible for the DM to determine the best alternative in a given choice problem. That such incompleteness of preferences is not merely a theoretical curiosity, but, rather, a real possibility has been acknowledged going at least as far back as the seminal paper by Robert Aumann (Aumann 1962).Footnote 2 Second, we engage with the possibility that even if a DM’s preferences are complete and transitive in a particular choice problem and, therefore, (in the context of a finite set of alternatives), a best alternative theoretically exists, it may not always be possible for the DM to figure out what this best alternative is owing to cognitive limitations. This may be particularly so if the choice problem under consideration involves a large set of alternatives. In this paper, we take these two challenges that real world decision-making may involve as our motivation and propose a choice procedure that embodies the spirit of how a DM may go about meeting such challenges. Our procedure appeals to the idea of ecological rationality, which refers to decision-making processes that exploit the structure of information in a given environment to arrive at efficacious choices (Todd and Gigerenzer 2012). Specifically, the choice procedure that we propose captures the behavior of a DM who uses information about the choices that others in society make in similar situations as she faces to simplify her decision-making process. To fix ideas, think of the DM as a new entrant to a society whose entrenched individuals are all rational in the traditional sense. We assume that the choice problems that this DM faces have been previously faced by the entrenched individuals of this society and information about their choices on these problems is available to her. Under our procedure, in any choice problem, the DM uses others’ choice data on that problem to create a “small” shortlist of alternatives to consider. From a cognitive and decision- making perspective, this set of shortlisted alternatives is a much simpler and more effective object for her to consider because it is a smaller set and this smallness is produced by, presumably, drawing on useful information from others’ choices. It is to this shortlisted set that she applies her preferences to choose an alternative. Arguably, this is a much simpler task cognitively than choosing an alternative from the original set. In other words, the choice procedure that we are proposing is a two-stage procedure under which, in the first stage, the DM uses information about others’ choices to create a shortlist of alternatives and, in the second stage, chooses from this set based on her preferences. We call this choice procedure the ecological shortlist heuristic (ESH). The key ingredient of the ESH, of course, is in the way it uses the choice data of others to create a set of shortlisted alternatives in any choice problem. The question that bears answering, therefore, is about the psychological and cognitive underpinnings that determine the way in which the DM does this shortlisting. Our answer to this question draws on a very robust idea from social psychology—that groups exert a constitutive psychological influence on individual attitudes and behavior. Specifically, in any given social situation, it appears that the human mind is hardwired to organize the others of her social world within an ingroup–outgroup division. The ingroup consists of those individuals that, for instance, she likes, relates to and identifies with, thinks of as her kin or friends, etc. On the other hand, the outgroup consists of those that are outside this circle of identification and connection. When it comes to behavior and attitudes, the outcome of such hardwiring is to imitate that of those in the ingroup and differentiate from that of those in the outgroup. It is as if the DM’s inner cognition tells her, “You are (not) like them and you will (not) like things that they like.” The shortlisting that the DM does under the ESH choice procedure imbibes this social psychology. Specifically, in any choice problem, she shortlists those alternatives that she thinks of as being “similar” to the choices of members of her ingroup and “different” from those of members of her outgroup, with the procedure making precise the way in which this similarity and difference work. Much work in social psychology has talked about such hardwiring of group attitudes. For instance, one prominent line of research is the self-categorization theory (SCT) of Turner (1985). Here, self-categorization refers to an individual’s cognitive representation of herself with respect to salient social categories, i.e., as similar to certain categories—her ingroup—and as distinct from others—her outgroup. The central hypothesis of SCT is how the salience of such ingroup–outgroup categorization produces a certain depersonalization of the self, whereby an individual stereotypes herself as a representative exemplar or prototype of a social category and views the world from the perspective of such a categorization.Footnote 3 Indeed, our formulation of the ecological shortlisting process draws inspiration from the meta contrast principle of social psychology which argues that cognitively salient categories form in a way that maximizes intragroup similarities and intergroup differences (Hornsey 2008). Our analysis of the ESH focusses on two key questions. First, we delve into the question of how efficacious is the ESH in terms of leading the DM towards good choices. After all, the cognitive simplicity afforded by the ESH will not be worth much if it consistently leads the DM towards inferior choices. Specifically, think of a choice problem in which by the DM’s preferences, a best alternative actually does exist. How effective is the ESH in picking out this alternative? We show that the ESH can be very efficacious in this regard if a certain homophily-type condition that connects the DM’s preferences with her ingroup–outgroup categorization holds. If this condition holds, then the ESH is guaranteed to pick the best alternative in a choice problem, provided such a best alternative exists as per the DM’s preferences. At the same time, we point out that if the DM’s ingroup–outgroup categorization does not respect this homophily condition, then the ESH can lead her to make biased choices that contradict her preferences. Second, we look at the question of when it is the case that the ESH results in rational choices (in the traditional economic sense) on the part of the DM. We show that if, along with the homophily condition, a certain consistency condition applies to the ecological shortlisting process, then the DM’s choices following the ESH can, indeed, be rational. Therefore, when both these conditions hold, the DM’s choices under the ESH can be rationalized by a complete and transitive preference ranking, which, in turn, is a completion of the incomplete primitive preferences that the DM starts off with. Our paper relates to the fast and frugal heuristics program of Gigerenzer et al. (1999). Their central hypothesis there is that, from an operational point of view, rationality is about the use of fast and frugal heuristics through which smart inferences can be made. Furthermore, these heuristics are successful to the degree that they are ecologically rational. The ESH that we develop here is a type of fast and frugal heuristic. Where our work differs from their’s is in the contention they make that there is a radical disconnect between this and the rational choice approach. The results that we derive for the ESH in this paper seem to suggest that this need not necessarily be true as the ecological rationality embedded in the ESH can be a fast and frugal way to lead the DM towards the rational choice benchmark. The spirit of this observation connects our work to that of Mandler et al. (2012). They show how the fast and frugal heuristic of making choices by proceeding sequentially using a checklist of desirable properties is essentially equivalent to the utility maximization paradigm. Our paper also relates to the literature on the theories of behavioral choice. Like many of the papers in this area, ours too explicitly spells out a psychologically motivated sequential choice procedure under which a cognitive phenomenon—that of ecological shortlisting—constrains the available set of alternatives and it is from this constrained set that the DM chooses. In terms of this structure, our model bears resemblance to Manzini and Mariotti (2007), Manzini and Mariotti (2012), Masatlioglu et al. (2012), Cherepanov et al. (2013), and Lleras et al. (2017), amongst others. Of particular interest to us in this context is the paper by Cuhadaroglu (2017), who models social influence within the behavioral choice paradigm.Footnote 4 One way in which our ESH procedure differs from the choice procedure she introduces is in that, under her procedure, the DM first applies her preferences to the set of available alternatives and only consults other individuals’ preferences if her preferences are not decisive. In the ESH, on the other hand, reference to others’ preferences comes at the first stage, and the DM’s preferences enter the picture in the second stage and acts on the shortlisted alternatives from the first stage. We should point out one major difference between our paper and this literature when it comes to its substantive behavioral content. Whereas most of this literature has looked at heuristic-based, psychologically motivated choice procedures as a way of explaining departures from the rational choice benchmark, our focus here goes precisely in the opposite direction. One of the keys questions that we address is how the ESH may allow the DM to attain the rational benchmark. The rest of the paper is organized as follows. Section 2 introduces the setup. Section 3 formally introduces and defines the ESH. Section 4 contains our substantive analysis of the ESH and the formal results addressing the main questions of the paper. Proofs of the results are presented in the Appendix.",5
86.0,3.0,Theory and Decision,05 February 2019,https://link.springer.com/article/10.1007/s11238-019-09690-y,On linear aggregation of infinitely many finitely additive probability measures,May 2019,Michael Nielsen,,,Male,Unknown,Unknown,Male,"An important problem across a range of disciplines, including economics, political science, statistics, and philosophy, is to identify a consensus or aggregate position given data about individual attitudes. In this paper, we shall be concerned with aggregating profiles of probability measures. The main approach in this field is to proceed axiomatically, defining reasonable constraints that aggregation functions might satisfy and characterizing particular aggregation procedures in terms of these constraints. An early proponent of this approach is McConway (1981), who provides characterizations of aggregation by linear averaging. In a recent paper, published in this journal, Frederik Herzberg (2015) takes the important step of relaxing two assumptions made in older work on probability aggregation. They are Aggregate and individual probability measures must be countably additive. The set of individuals whose probabilities are to be aggregated is finite. Herzberg offers compelling arguments in favor of weakening these two assumptions to Aggregate and individual probability measures must be finitely, but not necessarily countably, additive. There are no cardinality restrictions on the set of individuals whose probabilities are to be aggregated. Our main aim is to extend both Herzberg’s and McConway’s work under the weak assumptions of Finite Additivity and Infinite Profiles. The paper is divided into two parts. In the first part, after some preliminary definitions, we will discuss Herzberg’s treatment of linear aggregation (Sect. 2). Herzberg’s notion of linear aggregation requires the existence of a countably additive probability measure defined on the powerset of the set of individuals. Since the cardinality of the set of individuals is unconstrained, this approach must contend with a number of set-theoretical difficulties. In view of Finite Additivity, we propose a weaker notion of linear aggregation that avoids the difficulties faced by Herzberg’s account. This is illustrated by our main result (Sects. 3 and 4), which generalizes McConway’s characterizations of linear aggregation for finite sets of individuals. The second part of the paper (Sect. 5) is a discussion section in which we draw some connections between linear aggregation, Pareto conditions, de Finetti’s notion of coherence, and convexity.",3
86.0,3.0,Theory and Decision,02 March 2019,https://link.springer.com/article/10.1007/s11238-019-09691-x,Strategic framing to influence clients’ risky decisions,May 2019,Kris De Jaegher,,,,Unknown,Unknown,Mix,,
87.0,1.0,Theory and Decision,15 March 2019,https://link.springer.com/article/10.1007/s11238-019-09696-6,Distance from a distance: the robustness of psychological distance effects,July 2019,Stefan T. Trautmann,,,Male,Unknown,Unknown,Male,"The concept of psychological distance is widely used in psychology, and it has been the centerpiece of one of its most successful theories, Construal Level Theory (CLT; Liberman and Trope 1998; Trope and Liberman 2003, 2010). While CLT has originally been proposed as an account of intertemporal decision-making that can account for potential violations of discounting theory, the theory has subsequently substantially expanded its domain of application to the domains of uncertainty, spatial distance, and social distance (Liberman and Trope 2008; Trope and Liberman 2010). The descriptive success of the theory has also attracted attention in the field of economic/consumer psychology and behavioral economics (Fiedler 2007; Leiser et al. 2008; Onay et al. 2013). CLT and psychological distance effects have also been discussed in the context of behavioral tools for economic policy (World Bank 2015). The application of psychological distance to descriptive applications in economics seems promising. Analyses of decision under uncertainty, time preference, as well as social interaction may benefit from taking into account the effects of psychological distance, over and beyond the standard repertoire of behavioral insights from prospect theory, present-biased time preference or social preference models. However, despite being optimistic about the theory, we claim that the current state of empirical and theoretical work on CLT and its extensions does not provide an appropriate framework for economic theorizing and policy yet. In particular, it will be argued that recent empirical evidence on CLT stretching beyond its basic domain of interest does not approximate the level of robustness typical in experimental economic research (Camerer et al. 2016), and that an underlying cause of these problems lies in the theoretical under-specification of central concepts of the theory. The empirical lack of robustness will be demonstrated by a set of replication attempts of a recent extension of CLT distance effects, the distance-from-a-distance paradigm by Maglio et al. (2013). Maglio et al. (2013) provide simple experimental demonstrations of distancing effects across various dimensions of psychological distance. We describe the distance-from-a-distance paradigm in Sect. 2, and discuss our replication attempts in Sects. 3–5. Section 6 presents theoretical considerations regarding the specification of distance manipulations that suggest that CLT may not lend itself as easily to applications in economics as is the case for well-specified models like prospect theory or discounted utility, for example. Section 7 concludes with suggestions for a more robust study of CLT effects.",9
87.0,1.0,Theory and Decision,27 February 2019,https://link.springer.com/article/10.1007/s11238-019-09693-9,Hypothetical thinking and the winner’s curse: an experimental investigation,July 2019,Johannes Moser,,,Male,Unknown,Unknown,Male,"The winner’s curse is a well-known empirical phenomenon in common value auctions, which was first described by Capen et al. (1971). They showed that many oil companies in the 1960s and 1970s had to report a drop in profit rates because of systematic overbidding in auctions for drilling rights. Later experimental evidence for the winner’s curse was also found in a large number of lab studies (see, for example, Bazerman and Samuelson 1983; Thaler 1988; Charness and Levin 2009; Ivanov et al. 2010). I show in an experimental setup that bidders are more likely to avoid the winner’s curse when they are informed, before submitting a bid, whether their bid is the winning bid or not. By giving the subjects this information, I weaken the requirement for them to condition their bid on winning the auction: winning or losing are now not hypothetical anymore, and the adverse selection issue of winning an auction with a common value for all bidders becomes more salient. The findings of my paper suggest that mistakes in hypothetical thinking seem to explain a substantial part of the winner’s curse. Thus, approaches that focus on this mental process might be more suitable for explaining this phenomenon than belief-based models such as cursed equilibrium (Eyster and Rabin 2005), which state that the winner’s curse is mainly driven by inconsistent beliefs. Herewith, this paper attempts to shed light on the ongoing debate on whether models, focusing on the erroneous belief formation of individuals, such as cursed equilibrium and level-k (Eyster and Rabin 2005; Crawford and Iriberri 2007)Footnote 1 or approaches concerning contingent reasoning on hypothetical future events (Charness and Levin 2009; Ivanov et al. 2010; Esponda and Vespa 2014) are more suitable for explaining the winner’s curse. Contingent reasoning refers to the ability of thinking through hypothetical scenarios and to perform state-by-state reasoning. There is evidence that people have difficulties engaging in this cognitive task. While this is well documented in the psychological literature (see, for example, Evans 2007; Nickerson 2015; Singmann et al. 2016), economists devoted little attention to this issue for a long time. However, in the more recent economic literature this topic appears more and more frequently (see, for example, Charness and Levin 2009; Louis 2013; Esponda and Vespa 2014; Ngangoué 2015; Levin et al. 2016; Esponda and Vespa 2016; Li 2017; Koch and Penczynski 2018). In contrast to belief-based models, such as cursed equilibrium, the concept of contingent reasoning has still received very little formal treatment. Li (2017) represents the first attempt to capture this mental process formally by introducing the concept of obviously strategy-proof (OSP) mechanisms.Footnote 2 A major contribution of Li’s paper is to explain why subjects perform better in ascending bid auctions compared to sealed bid auctions. However, it cannot explain why common value auctions might be more challenging for the bidders than private value auctions. For this reason, the concept of OSP mechanisms is not sufficient to fully capture the most relevant aspects of the winner’s curse. Can a broader definition of contingent reasoning explain why bidders fall prey to the winner’s curse? To answer this question I will go back one step from OSP mechanisms and focus only on the events of winning or losing an auction—which provide information about the true value of a good in common but not in private value auctions. Bidders in common value auctions now face two cognitive hurdles. First, they have to be able to recognize and identify several hypothetical scenarios which might possibly occur (e.g., winning or losing an auction) and second, the bidders have to be able to infer information from such hypothetical events. The question is which of these cognitive tasks is more challenging for subjects and to what extent they affect the likelihood of the winner’s curse to occur. There are three possibilities: (i) bidders are perfectly able to perform state-by-state reasoning, but they neglect the informational content of the other bidders’ actions and, hence, the informational content of winning an auction as proposed by cursed equilibrium; (ii) bidders take into account that the bids of the other players are correlated with their signals, but they are not able to identify the relevant states to condition on in the first place; (iii) bidders are neither able to perform state-by-state reasoning, nor do they take into account the informational content of winning an auction. This paper tests experimentally whether subjects in an auction are able to infer information from the events of winning or losing if these are not hypothetical anymore. For this purpose, I constructed a second-price auction in which the bidders learn whether a bid, which was considered optimal ex ante, is the winning bid or not (but without learning their payoff yet) and they receive the possibility to change this bid. This treatment intervention is similar to the sequential treatment in Esponda and Vespa (2014) where participants in an election learned whether their votes were pivotal or not before they had to cast a vote. In contrast to Koch and Penczynski (2018), my approach is weaker, since it does not remove the need for contingent reasoning at all, but only partly in a specific context. However, thereby I am able to show whether such a weak manipulation, on a feedback basis, is sufficient for subjects to improve their bids, without changing the whole structure of the game. Additionally, it has to be noted that even Koch and Penczynski (2018) did not meet the strict requirements of completely eliminating the necessity for contingent reasoning, according to the definition of Li (2017). This further emphasizes the urgent need for a consistent definition of this term. The auction model I used in the experiment is based on a second-price sealed bid auction similar to the wallet game proposed by Klemperer (1998) and the model used in Avery and Kagel (1997), which is a non-standard common value auction. The basic idea of the game is the following: two players, indexed by \(i=1,2\), receive a private iid signal, \(x_i\), drawn from some commonly known distribution. In a second-price sealed bid auction they bid for an object worth \(v = x_1 + x_2\). This game is played in two stages. In stage I, the subjects participate in the wallet game against a random opponent. In stage II, the subjects play the same auctions again, against the decisions of the former opponent, but this time the subjects in the treatment group are informed whether their initial bid was the winning bid or not. In this sense, the bidders receive information about some realized event before they have to come up with a bid.Footnote 3 Apart from knowing whether their bid is the winning or losing bid, the subjects face exactly the same decision problem as in stage I. My design allows a clear distinction between mistakes in contingent reasoning and cursed equilibrium since two crucial assumptions of the latter are that (i) no (or only a partial) correlation between the other players’ actions and types is assumed and (ii) bidders still best respond given their beliefs. Hence, for a “cursed” bidder the information on whether his bid is higher or lower than the bid of the opponent, does not provide him further information which would be relevant for updating his bid. For fully cursed bidders, this argument is straightforward, but it also holds for partly cursed bidders because of the best response assumption. By definition, the bid of a cursed bidder is already evaluated conditional on winning in stage I, albeit a partly cursed bidder implicitly assumes that winning is less informative than in equilibrium. This means, the feedback of winning in stage II provides no further relevant information, since it is already included in the decision of a partly cursed bidder in stage I. The reason for choosing the wallet game, instead of a more standard model for common value auctions, is that in this game a naïve bidding strategy can lead to both over- and underbidding, relative to the symmetric equilibrium strategy, depending on whether the private signal is low or high. In this sense, there can be both a winner’s and a loser’s curse (see also Holt and Sherman 1994).Footnote 4 This property is useful for two reasons. First, I am able to control for psychological explanations, stating that the winner’s curse is mainly driven by emotional factors of winning (see, for example, Van den Bos et al. 2008; Astor et al. 2013). Second, and more importantly, I am able to check whether bid shading in stage II is due to proper Bayesian updating or just a rule of thumb when learning that a certain bid was the winning bid. In my setup, bid shading is only advisable for low signals, but not for high signals. Thus, the subjects have to differentiate between these two kinds of signals, instead of following the simple decision rule “decrease your bid, when you learn that your bid was the winning bid”. The findings of my paper reveal two important observations: Bidders are more likely to avoid the winner’s curse and the loser’s curse in stage II when they are informed whether their bid is the winning bid or not, given that the respective information has a sufficiently high predictive power concerning the opponent’s signal. This suggests that the crucial cognitive hurdle for bidders in a common value auction is not forming beliefs about the opponents’ behavior, but identifying the relevant states to condition on in the first place. Information can also be negative for the bidders, depending on the context. The subjects in my experiment differentiated only imperfectly between situations in which decreasing (increasing) a bid is rational and those in which it is not and they often used simple heuristics instead of making strategic changes. This behavior might be partly explained by an actual joy of winning or, to be more precise, a disappointment of losing. When subjects learned that they lost an auction in stage I, most of them increased their bid in stage II, regardless of having a low or high private signal. Conversely, when subjects learned that they won an auction in stage I, they acted more strategically and bids for low signals were decreased at a much higher rate compared to bids for high signals (78.7% vs. 42.3%). This paper is organized as follows. Section 2 will provide an overview about the current literature closely related to my research topic. Section 3 will present the underlying theoretical model for the experiment. Section 4 describes the experimental design. Section 5 presents and discusses the results of the experiment. Section 6 concludes the paper.",3
87.0,1.0,Theory and Decision,19 March 2019,https://link.springer.com/article/10.1007/s11238-019-09695-7,"On some aspects of decision theory under uncertainty: rationality, price-probabilities and the Dutch book argument",July 2019,Aldo Montesano,,,Male,Unknown,Unknown,Male,"Choice under uncertainty is a topic that economics faces in various fields of research. This generates a diversity of approaches, which depends essentially on the diversity of the objectives. In this regard, it is convenient to distinguish two areas. In one, the main purpose of the analysis is the explanation of agent’s choice. In the other, this objective is secondary, because agents’ choices are introduced only to analyze the interactions of a community of agents (which determine, for example, the market equilibrium). In the first area, we can distinguish between the rationalistic (normative) point of view and the behavioral (descriptive) one. That is, between the determination of choice by assumption of rationality and the explanation of the observed individual choices by psychological motives. In the following, therefore, I distinguish between three approaches to the choice under uncertainty. The first is the individualistic–rationalistic one, which deals with the determination of the choices of an agent whose preferences under uncertainty depend on axioms of rationality. The second is the individualistic–behavioral one, which deals with the observed choices of an agent and explains them mainly taking into account psychological aspects. The third is that of social interaction, which includes individual preferences but does not have their explanation as object of the analysis. The characteristics of preferences are not specified since its main purpose is to ascertain the logical consistency of a social theory, and, consequently, the presence of uncertainty is introduced without specific assumptions on individual preferences (such as the first approach) or specific behavioral considerations (as does the second). Expected utility theory provides the typical reference of the first approach. Prospect theory provides that of the second approach. General equilibrium theory with state-contingent goods provides that of the third approach. Since all the above approaches deal with the choice under uncertainty, there are points of contact, overlaps and contrasts. To these problems is added the relationship between the subjective probability of an event and the price that the subject is willing to pay to receive (or requires to give) a unitary sum of money in case that the event under consideration occurs. Thus, Sect. 2 has to do with a brief presentation of these three approaches. Section 3 is devoted to the question of rationality with respect to choices under uncertainty. This section includes an example of preferences that some psychologists qualify irrational although they can be represented by the expected utility theory. Section 4 introduces the marginal reservation price for a bet on an event and discusses its interpretation as a subjective probability. Section 5 examines these price-probabilities and shows how the Dutch Book argument does not require always their additivity.",2
87.0,1.0,Theory and Decision,05 March 2019,https://link.springer.com/article/10.1007/s11238-019-09694-8,Individual vs. group decision-making: an experiment on dynamic choice under risk and ambiguity,July 2019,Enrica Carbone,Konstantinos Georgalos,Gerardo Infante,Female,Male,Male,Mix,,
87.0,1.0,Theory and Decision,06 March 2019,https://link.springer.com/article/10.1007/s11238-019-09692-w,Is knowledge curse or blessing in pure coordination problems?,July 2019,Swee-Hoon Chuah,Robert Hoffmann,Jeremy Larner,Unknown,Male,Male,Male,"Next to cooperation, coordination is a fundamental type of collective action problem (Abele and Stasser 2008; Thomas et al. 2014). In coordination problems, decision makers receive the highest available payoffs when they choose the same decision alternative as others. Economic applications include technological standards (Shapiro and Varian 1999; Arthur 1989), conventions (Sugden 1989; Rubinstein 2000), network externalities (Abramson 2005) and the business cycle (Cooper and John 1988). Coordination is difficult in theory because decision makers can effectively converge on any alternative and, therefore, may become stuck in an infinite regress of mutual outguessing. A number of coordination mechanisms have been proposed in the literature. When decisions makers are able to communicate then an agreement on one of many decision alternatives is in everyone’s interest (Cooper et al. 1994). When decision makers can learn from the decisions others made previously then following the majority decision can create convergence (Kandori et al. 1993). Finally, decision makers can tacitly converge if there is payoff dominance, i.e., a decision alternative that, if universally adopted, entails higher payoffs for all (Harsanyi and Selten 1988; Van Huyck et al. 1990). To illustrate these three mechanisms, people can converge on one of alternative technological standards if they are able to communicate and agree, if they are able to observe and follow the standard most others are using or if there is one standard that has inherent advantages for everyone. In practice, decision makers can coordinate surprisingly well even when none of these mechanisms are available. In these “pure” coordination problems, no equilibrium affords higher payoffs; there is no possibility of communication between decision makers or opportunity to learn from their previous interactions. In his seminal analysis, Schelling (1960, p. 54–57) presents different such problems including (among others) naming either heads or tails, circling a number or shape among those presented, selecting a meeting place and time in New York City or in a department store after spouses lose each other. Payoffs result from choosing the same as others. Mehta et al. (1994) report the first formal experiments with such problems including naming any number, day, year, flower, colour, car manufacturer, British city and boy’s name. To explain participants’ high coordination success, these and other authors (see Janssen 1998, for an overview) further develop Schelling’s (1960) idea of focal points or salience, where certain alternatives are prominent or stick out because they possess a shared cultural or psychological significance. In this paper we examine, we believe for the first time, the effect of knowledge and information on coordination. Because coordination on focal points is based on common knowledge, the degree to which individuals know decision alternatives may affect their coordination success. The examples studied by Schelling (1960), Mehta et al. (1994) and others as well as naturally occurring coordination problems differ in terms of whether or not decision makers know all possible alternatives and, therefore, equilibria. We classify these as either open-form or closed-form coordination problems. Selecting items from a list that is presented to or independently known by participants are closed-form coordination problems. Selecting heads or tails, any day of the year, a number or a shape from among presented ones belong to this category. The decision maker has full awareness of all alternatives open to her. In contrast, when there are no presented alternatives, selecting a place to meet, a city, a boy’s name or car manufacturer are open-form problems because there are a multitude of options and knowledge of these differs between participants.Footnote 1 Our motivation is that while coordination problems with asymmetric information are common in practice they have been relatively neglected in the literature. Open-form coordination problems are interesting and special because the different knowledge decision makers have may affect their coordination success. This effect may go either direction (Hargreaves Heap et al. 2017). In the traditional view greater knowledge enhances decision making success. However, the opposite may be true in coordination games. For example, Hargreaves Heap et al. (2017) find that coordination is higher when choice alternatives are not presented to experimental participants. An explanation may come from recent literature that identifies knowledge curses in a number of other economic settings (for an overview see Chuah et al. 2018). Ignorance may be bliss in pure coordination problems if it systematically narrows the choice set towards focal alternatives and avoids overestimating others’ knowledge. We are interested in the empirical question whether knowledge is a curse or rather a blessing under these conditions. We examine both possibilities using an experiment. The next section outlines the research background and questions. Section 3 outlines our method. Results, their discussion and brief conclusions are presented in Sects. 4, 5, and 6, respectively.",1
87.0,2.0,Theory and Decision,02 April 2019,https://link.springer.com/article/10.1007/s11238-019-09698-4,Bertrand competition with asymmetric costs: a solution in pure strategies,September 2019,Thomas Demuynck,P. Jean-Jacques Herings,Christian Seel,Male,Unknown,Male,Male,"The analysis of price competition is a fundamental part of oligopoly theory since Bertrand’s contribution (1883). The Bertrand duopoly with symmetric constant marginal costs, homogeneous goods, and continuous prices has a unique pure strategy Nash Equilibrium characterized by a strategy profile in which prices equal marginal costs. If marginal costs are not symmetric across firms and the market is shared if firms set equal prices, no pure strategy Nash equilibrium exists. Blume (2003) shows that there exists a Nash equilibrium in mixed strategies where the more efficient firm sets price equal to the opponent’s marginal cost and serves the entire market with probability 1. The rival randomizes on an interval above his marginal cost. Kartik (2011) strengthens the result by showing that all undominated equilibria lead to the same market price and shares. The complete set of undominated Nash equilibria is not constructed in these papers. In this paper, we analyze the same Bertrand game. We follow Blume (2003) and Kartik (2011) in that we focus on strategy profiles in which no firm chooses a predatory price, i.e., a price below marginal cost. Instead of Nash equilibrium, however, we employ a solution concept recently introduced by Demuynck et al. (2019), the Myopic Stable Set. A set of strategy profiles is myopically stable if it satisfies three conditions, deterrence of external deviations, asymptotic external stability, and minimality. Deterrence of external deviations requires that no player benefits by switching her strategy, such that the resulting strategy profile is outside the Myopic Stable Set. Asymptotic external stability makes sure that, from any strategy profile outside the set, it is possible to get arbitrarily close to a strategy profile inside the Myopic Stable Set by a sequence of better replies. Minimality requires that the Myopic Stable Set is minimal with respect to set inclusion. In Demuynck et al. (2019), we defined the Myopic Stable Set for a very general class of social environments (Chwe 1994) that allows for infinite state spaces and includes normal-form games as a special case. We proved that if the state space is compact, then the Myopic Stable Set exists, and under some mild continuity assumptions, it is also unique. Moreover, we showed that the Myopic Stable Set coincides with the set of pure strategy Nash Equilibria for supermodular games, aggregative games, and potential games. In light of these results, the Bertrand model with asymmetric costs is interesting for several reasons: it does not satisfy the compactness and continuity assumptions of Demuynck et al. (2019), it does not belong to any of the aforementioned classes of games, and the set of pure strategy Nash equilibria of this game is empty. Moreover, the set of mixed-strategy Nash equilibria is large. We prove existence and uniqueness of the Myopic Stable Set for symmetric and asymmetric Bertrand competition. We characterize the set in closed form. The set is small and gives an intuitive prediction.",5
87.0,2.0,Theory and Decision,11 April 2019,https://link.springer.com/article/10.1007/s11238-019-09701-y,"NTU core, TU core and strong equilibria of coalitional population games with infinitely many pure strategies",September 2019,Zhe Yang,Haiqun Zhang,,,Unknown,Unknown,Mix,,
87.0,2.0,Theory and Decision,04 April 2019,https://link.springer.com/article/10.1007/s11238-019-09697-5,Procedural and optimization implementation of the weighted ENSC value,September 2019,Dongshuang Hou,Aymeric Lardon,Hao Sun,Unknown,Male,,Mix,,
87.0,2.0,Theory and Decision,02 May 2019,https://link.springer.com/article/10.1007/s11238-019-09702-x,The limitations of the Arrovian consistency of domains with a fixed preference,September 2019,James Nguyen,,,Male,Unknown,Unknown,Male,"Social choice theory in the Arrovian style concerns functions from domains containing n-tuples of orders over some set of alternatives that map to an overall order on those alternatives. Arrow’s conditions of Weak Pareto (WP), Independence of Irrelevant Alternatives (IIA), and Non-dictatorship (ND) put constraints on the behaviour these functions, whilst the Universal Domain (UD) condition specifies their domain. Arrow (1963) proved that these conditions are not jointly satisfiable. It is well known that UD is stronger than is needed to generate the impossibility result, but also well known that various restricted domains admit functions satisfying the remaining conditions (Gaertner 2001; Breton and Weymark 2011). In this paper, I investigate domains where the preferences of one individual are fixed, and the rest are allowed to vary. These domains are degenerate cases of domains containing what Sakai and Shimoji (2006) call a ‘dichotomous’ preference. An individual has a ‘dichotomous’ preference in their sense if the alternatives under consideration can be partitioned into two subsets, such that the individual in question prefers every element of one of the subsets over every element of the other, across the domain of the aggregation function. Clearly, if an individual has a fixed preference across the domain of the aggregation function, they have a dichotomous preference (unless of course their fixed preference is complete indifference). These domains do admit social welfare functions satisfying the remaining Arrovian conditions (Sakai and Shimoji 2006, Theorem 1), and thus, domains including a fixed preference are guaranteed to be Arrow consistent by the same result. However, I prove here that according to any function on such a domain that satisfies WP, IIA, and ND, for any triple of alternatives, if the agent with the fixed preferences does not determine the social preference on any pair of them, then some other agent determines the social preference on the entire triple. Thus, the cost of accommodating varying social preferences over pairs of alternatives (varying in the sense of allowing disagreement with the agent with fixed preferences) is paid only by making some other agent determine the social preference over them across the domain. Equivalently, for any triple of alternatives, if the social ordering is to be sensitive to more than one agent’s preferences over them, then the fixed preference will always determine the social preference of at least one pair within the triple. This prima facie undesirable result suggests that the Arrovian consistency of such domains is guaranteed only at the cost of the preferences of the agent with the fixed preferences having significant influence on the social preference. These domains are of interest for at least four reasons, each of them stemming from motivations for UD. When faced with a situation where an aggregation function is required I take it that, as a matter of fact, each individual has a specific, actual, preference ordering over the alternatives in question. Thus, there is only one actual preference profile to be aggregated. However, Arrow (1963, Chapters III and VIII) motivated UD by noting that our knowledge about these preference might be incomplete. If we cannot rule out any other profile this motivates UD.Footnote 1 In response to this we can consider contexts where the formal machinery of social choice theory is put to work in designing an actual aggregation procedure (such as in the literature on mechanism design). Presumably there exist scenarios where a social planner tasked with such a design problem may also be a member of the group of individuals whose preferences need to be aggregated. As long as she knows her own preferences in advance, then this undercuts the above motivation for UD, because it requires that every individual may hold any possibly ordering on the alternatives. Domains where the social planner’s preferences are held fixed, and the other individuals’ preferences vary, correspond to the domains discussed in this paper. An alternative motivation for UD that applies beyond mechanism design—including, for example, cases, where the machinery of social choice theory is used to investigate issues concerning justice—is offered by Kolm (1996) (whose views are usefully discussed by Weymark (2011)). Even if it is the case that in the actual world there is only once preference profile to be aggregated, whether or not the aggregation procedure is just depends on its counterfactual behaviour: what would have resulted had the individuals’ preferences been otherwise. This corresponds to the behaviour of an aggregation function when applied to different preference profiles. If every individual involved could have had any preference over the alternatives in question, then this motivates UD. However, this might not always be the case. For some sets of alternatives, and for some individuals, their preferences may be such that they could not but hold the preferences that they do. The sorts of cases I have in mind include, for example, cases where the preferences of an individual are so central to their world view that they could not be otherwise without the individual failing to be that individual in question (this corresponds to the individual’s preferences over the alternatives in question being one of what philosophers sometimes call their ‘essential properties’). For example, for someone with strongly held religious beliefs, their preferences over some set of alternatives pertaining to those beliefs might be such that they simply could not hold different preferences without being a different person. A third reason to think about these domains stems from a recent discussion in the philosophy of science literature. Okasha (2011) suggests that the question of how scientists should rationally choose between competing scientific theories (or models, or hypotheses) can be modelled in the social choice framework.Footnote 2 His argument is remarkably simple: take theoretical virtues—good-making features of scientific theories; like accuracy/fit-with-data, simplicity, and scope—to provide (ordinal) ‘preference’ rankings over the competing theories. The ranking by each virtue depends on how the competing theories compare with respect to that virtue. Then treat the question of theory choice as a question of coming up with a function from n-tuples of these orderings to an all-things-considered ranking.Footnote 3 Crucially, Okasha suggests that the Arrovian conditions apply in such a context. If correct, Arrow’s impossibility result rules out the possibility of rationally choosing between competing scientific theories in this manner.Footnote 4 
Morreau (2015) argues against the applicability of UD in the theory choice context. If we again assume that there is a fact of the matter as to how each theoretical virtue ranks the competing theories, then again there is only one n-tuple to be aggregated. However, in analogy to Kolm’s (1996) justification for UD in the social choice application, the justification for using a particular aggregation function depends on the behaviour of the function when augmented with different profiles, profiles of rankings which might have been. In addition, in the context of theory choice, at least some of the virtues, such as simplicity, might be unable to rank the competing alternatives in the way other than the one that they, in fact, do. As Okasha (2015, p. 288) puts it, simplicity ranks theories according to their ‘essential features’. If, as a matter of fact, a given theory is simpler than a competing theory, then this is necessarily the case. Notice that this need not be the case for all theoretical virtues. Some, such as accuracy/fit-with-data, for example, could supply any ranking of the competing theories (where ‘could’ is read to indicate modal possibility). In his reply, Okasha admits that he is ‘inclined to agree with Morreau’s claim that some criteria of theory choice, such as simplicity, are rigid’ (2015, p. 288). If they are right about this, then the appropriate domain of a function from rankings of scientific theories by virtues is not UD, but one where at least one virtue—i.e., simplicity—supplies a fixed order on the alternatives, whilst the others—i.e., accuracy/fit-with-data—vary (I sharpen this in the next section). However, Okasha and Morreau disagree on the importance of this observation. Okasha claims that in ‘the absence of a proof that [such a domain] is Arrow consistent, the right conclusion to draw is that we do not know whether an Arrovian impossibility result applies in this case or not. There is an unresolved mathematical question here’ (2015, p. 290). As it happens, that such domains are Arrow consistent follows from the fact that an order fixed across such a domain is a degenerate case of a dichotomous order in the sense of (Sakai and Shimoji 2006), so the question has been resolved. However, the properties of the social welfare functions that satisfy the Arrovian conditions on such domains have not been investigated. Finally, we can consider how to model non-welfarist, or more strictly speaking, mixed non-welfarist/welfarist, accounts of whether or not an aggregation function is just. Recall that on the standard social choice model, each element in an n-tuple of preferences to be aggregated is supposed to represent the preferences of an individual. Thus, whether or not an aggregation function is just depends solely on how it aggregates individuals’ preferences involved (including what these might be in counterfactual situations). This way of addressing questions of social justice is strictly welfarist; the only relevant data concern the behaviour of an aggregation function when fed with individual preferences. However, there is nothing essential to the social choice framework that demands that the preferences within an n-tuple represent the preferences of an individual agent. We could instead take such an element to represent an objective comparison of the alternatives involved. If agreement with this objective comparison were to be the sole criterion on which to measure whether an aggregation function is just, as per a strictly non-welfarist account, then we would expect dictatorial (with respect to the objective comparison) functions to be just. However, mixed non-welarist/welfarist accounts would allow considerations of the extent to which an aggregation function delivered the objective comparison of the alternatives involved combined with the extent to which it matched the preferences of the individual agents involved. Depending on the nature of the alternatives in question, and the objective way of comparing them, it is plausible that the preference ranking representing the objective comparison of the alternatives would stay fixed irrespective of the subjective preferences of the individuals involved. Again, the domains of preferences considered in this paper correspond to such domains. To aid readability I use the terminology associated with the standard application of the Arrovian framework throughout: I talk of ‘agents’, ‘individuals’, ‘preference orderings’, ‘indifference classes’, and so on. However, given the above considerations, it should be noted that the ‘preferences’ of an ‘agent’ or a ‘individual’ can represent the way a scientific virtue or a objective comparison orders the alternatives. I first recap Saki and Shimoji’s (2006) proof that the relevant domains admit functions satisfying IIA, ND, and WP. I then prove that provided there are enough alternatives, every agent can influence the value of such functions. I finally prove that according to any Arrow consistent function on such a domain, for any triple of alternatives, if the agent with the fixed preferences does not determine the social preference on any pair within the triple, then some other agent determines the social preference over the entire triple. Equivalently, for any triple of alternatives, if the social preference over them is not to be determined by a single agent across the domain, then the agent with the fixed preferences will determine the social preference over at least one pair from the triple across the domain. Thus, the cost of accommodating varying social preferences over pairs of alternatives is paid only by making agents with varying preferences determine the social preference. I return to how these formal results should be interpreted in each domain of application in the concluding section.",
87.0,2.0,Theory and Decision,21 June 2019,https://link.springer.com/article/10.1007/s11238-019-09712-9,Delayed probabilistic risk attitude: a parametric approach,September 2019,Jinrui Pan,Craig S. Webb,Horst Zank,Unknown,Male,Male,Male,"Many decisions taken by individuals concern risky outcomes obtained at various points in time. These range from the purchase of lottery tickets, the ordering of goods that need to be delivered at certain dates, the booking of accommodation for business or holidays, to more complex financial instruments such as options or employment contracts with performance-based pay components. Traditionally, such streams of risky outcomes are evaluated by taking a weighted average of the future expected utility (EU) of each risky option, where the weights are determined using a constant discount rate. The literature has questioned this form of discounting from a descriptive point of view (Loewenstein and Prelec 1992; Frederick et al. 2002) as well as the use of EU for risk (Starmer 2000) and has called for more flexibility in modeling temporal discounting and attitudes towards probabilities. As recent experimental studies suggest that risk attitude may be affected by the time at which prospects are obtained, we propose a simple theory that combines the domains of choice under risk and over time in a specific way. A connection between the domain of risk and that of pure time preferences has been suggested before (Prelec and Loewenstein 1991; Dasgupta and Maskin 2005) and a specific role has been attributed to the treatment of probabilities (Quiggin and Horowitz 1995; Halevy 2008). Recent contributions suggest that there may be subtle differences across these choice domains and that inverse-S weighting functions, as proposed in rank-dependent models and empirically supported by prospect theory (PT; Kahneman and Tversky and Kahneman 1992; Stott 2006; Wakker 2010), can play an important role for time (Wu 1999; Epstein 2008; Baucells and Heukamp 2012; Epper and Fehr-Duda 2015; Gerber and Rohde 2018). Accordingly, we develop a model in which the empirically founded shape of the probability weighting function may be affected by time delay. In general, we can expect that time delay inflates or deflates any of the descriptively relevant parameters of a static decision theory model for risk and there are too few empirical studies to be conclusive. Earlier studies have often focused on how discounting is affected by future risk (Stevenson 1992; Shelley 1994), how static choice EU-paradoxes fare with delay (Keren and Roelofsma 1995; Weber and Chapman 2005) and, more recently, on how risk behavior is affected by delay (Noussair and Wu 2006). It is, therefore, not much of a surprise that the empirical basis for our motivation to focus on the probability weighting function is arguably thin. We are mainly motivated by the relatively recent study of Abdellaoui et al. (2011), which finds that, in a setting where discounting, attitudes towards outcomes and attitudes towards probabilities are separated, virtually all of the effect of time delay is captured by probability weighting. The experimental design of Abdellaoui et al. is such that discounting cannot be made responsible for changes in the preference among delayed prospects. Moreover, the elicited data suggest that the utility of outcomes is unaffected by delay. As a result, it can only be the treatment of probabilities that can account for changes in the observed choice behavior. This explains why our focus is on the interaction of probability weighting and time delay. In our model we want to be more precise about which aspects of probability weighting are foremost responsible for changes in probabilistic risk attitudes. Looking somewhat closer at the distortions in probabilities, the results in Table 5 of Abdellaoui et al. (2011) show that the probability weights attached to the probability value of \(p=1/3\), which has often been reported to be the least distorted probability in static binary choice under risk (Wakker 2010), are not significantly impacted by time delay either. Further, at the aggregate level of all individuals in their study, the low probability (\(p=1/6\)) is less overweighted with delay, while moderate and large probabilities (\(p\in \{1/2,2/3,5/6\}\)) are less underweighted when prospects are delayed. Thus, while confirming that delayed probabilities are also treated in accordance with the inverse-S shape of standard PT-weighting functions, albeit that the shape is less pronounced with delay, Abdellaoui et al. were able to qualify the finding of Noussair and Wu (2006) and others, which was termed “more risk tolerance with delay”. Since underweighting of probabilities is associated with pessimism (formally defined in Wakker 1994), which in PT induces risk-averse choice behavior, we prefer to use the term relatively less risk aversion with delay instead. But the complete picture on the treatment of probabilities has to include the observation that, jointly with reduced pessimism with delay, small probabilities are less overweighted (i.e., less optimism; Wakker 1994), and that the undistorted probability (i.e., \(p=1/3\)) apparently remains unaffected by delay. Such a treatment of probabilities is better attributed to a change in the index of insensitivity (Wakker 2010), a measure that captures the ability of a decision maker to discriminate among probabilities (Wu 1999).Footnote 1 Therefore, our working hypothesis is that delay mainly affects insensitivity. For a theoretical analysis concerning changes in the index of insensitivity with delay, we invoke the two-parameter constant relative sensitivity (CRS) weighting function of Abdellaoui et al. (2010). Other parametric probability weighting functions could, in principle, also be adopted,Footnote 2 but the CRS-family has the advantage that it connects optimism (overweighting of small probabilities plus concavity of the weighting function) and pessimism (underweighting of medium and large probabilities plus convexity) in a direct way to its insensitivity index, which is the parameter of our interest. The CRS-function also identifies an elevation parameter, which Gonzalez and Wu (1999) interpret as “attractiveness to gambling”, because it captures a general propensity of an individual to take risks in a particular choice environment. In accordance with the view that insensitivity and elevation represent logically independent psychological components of behavior (Gonzalez and Wu 1999, p. 139), the CRS-function also achieves a clean separation between these parameters based on behavioral preference foundations. To better understand the relation between optimism/pessimism and insensitivity/elevation it is useful to present more formal arguments. The CRS-weighting function is a power-function over probabilities that are overweighted and is the (dual of that) power-function over probabilities that are underweighted; both functions have the same exponent, \(\sigma \), as the index for (in-)sensitivity. Except for the probabilities 0 and 1 the CRS-weighting function may have a further fix-point at an intermediate probability, \(\eta \), which is the index for elevation. At one extreme with \(\eta =1\) only optimism governs choice behavior (i.e., the weighting function is concave and overweighs all probabilities) while at the other extreme with \(\eta =0\) only pessimism is revealed (the weighting function underweighs all probabilities and is convex). The empirically founded inverse-S probability weighting function will have the elevation parameter bounded away from extreme probabilities (as noted earlier, usually around probability \(p=1/3\); Wakker 2010, p. 205/206). Similarly, for the curvature parameter the empirically relevant parameter range is limited to (0, 1) to generate the empirically supported inverse-S shape. A lower value for \(\sigma \) indicates less sensitivity (or, equivalently, more insensitivity), such that at one extreme (\(\sigma =0\)) we have no sensitivity at all as each intermediate probability receives the same weight, while at the other extreme (\(\sigma =1\)) we have uniform sensitivity as no probability is distorted, i.e., preferences agree with EU. Given our objective to pragmatically incorporate some descriptive features in our model, it is worth reviewing in some more detail the few experimental findings related to temporal risk attitudes which we seek to capture. We do that in Sect. 2, however, familiar readers can choose to move ahead to Sect.  3 where the theoretical setup is presented. A foundation for time-dependent CRS-probability weighting is provided in Sect. 4. Subsequently, we provide some comparative analyses, some simple applications to bargaining (Sect. 5), some further discussion (Sect. 6), highlight relevant aspects for time preferences which have not been our focus (Sect. 7), and then conclude (Sect. 8). Proofs are presented in the Appendix.",
87.0,2.0,Theory and Decision,09 April 2019,https://link.springer.com/article/10.1007/s11238-019-09699-3,Preference orderings represented by coherent upper and lower conditional previsions,September 2019,Serena Doria,,,Female,Unknown,Unknown,Female,"Complex decisions can be defined as decisions where a preference ordering \(\succ \) between random variables cannot be represented by a linear functional, that is there exist no linear functional \(\varGamma \) such that Complex decisions arise also in decision making under ambiguity where aversion towards ambiguity can effect the preferences (Ellsberg 1961) and leads to a violation of Savage’s sure think principle (Savage 1954) and cannot be described by subjective expected utility theory. The modeling of preferences and their representations have been investigated in Seidenfeld et al. (1995). The Choquet expected utility theory has been introduced to modeling decision making under ambiguity using non-additive probabilities (Choquet 1953; Schmeidler 1989; Gilboa 1987; Mayag et al. 2011; Anscombe and Aumann 1963). Examples of non-additive measure are coherent upper and lower probabilities and non linear functional can be defined by coherent upper and lower prevision. The vantages of using coherent upper and lower conditional probabilities instead of fuzzy measures, which are not required to be coherent, to define functional which represent a preference ordering are: coherent upper and lower conditional previsions define on a given class of random variables can be extended to the class of all random variables on \(\varOmega \); coherent upper and lower conditional previsions are required to be fully conglomerable, i.e. to be conglomerable with respect to every partition even if they may fail the disintegration property. a new model of coherent upper conditional previsions based on Hausdorff outer measures has been introduced in Doria (2010, 2012) and it is proven to satisfy the disintegration property and to be fully conglomerable on every non-null partition (Doria 2017). The problem to determine if a random variable is a maximal and a Bayes random variable in a given class is related to problem to verify if the coherent upper conditional prevision verifies the disintegration property. Let \(\varOmega \) be a non-empty set and let \(\mathbf{B }\) be a partition of \(\varOmega \); disintegration property for linear prevision P requires that \(P(X)=P(P(X|\mathbf{B }))\) and it has been studied in Seidenfeld et al. (1998); for coherent lower and upper previsions it has been investigated in Miranda et al. (2012) and Doria (2017). For example in multi-criteria decision problem denoted by \(\varOmega \) the set of criteria, the elements of a partition \(\mathbf{B }\) can represent clusters or macro-criteria—which are representative of the general objectives of the decision problem, as goals to pursue through the implementation of specific policies—and the elements in each B are the criteria. To compare the random variables or the alternatives with respect to all criteria the disintegration property of the upper and lower conditional previsions can be applied to calculate \({\overline{P}}(X|\varOmega )\) and \({\underline{P}}(X|\varOmega )\) and to determine the maximal and/or the Bayes random variable with respect all criteria. In Walley (1991) different preference orderings are defined with respect to lower and upper coherent previsions. For each \(B \in \mathbf{B }\) let \({\underline{P}}(\cdot |B)\) and \({\overline{P}}(\cdot |B)\) respectively a lower coherent conditional prevision and its conjugate upper coherent conditional prevision defined on the class L(B) of all random variables on B and let K be a sub-class of L(B). For each \(B \in \mathbf{B }\) a strict order \(\succ _{*}\), (i.e. a complete antisymmetric and transitive binary relation) is defined with respect to a coherent lower conditional prevision and a weak order \(\succ ^{*}\), (i.e. a complete reflexive and transitive binary relation) is defined with respect to a coherent upper conditional prevision. A random variable \(X_{i}\) is admissible in K under \({\underline{P}}(\cdot |B)\) if no random variable \(X_{j} \in K\) with \(i\ne j \) is preferable to \(X_{i}\) with respect to \(\succ _{*}\). An admissible random variable \(X_{i}\) in K is maximal under \({\underline{P}}(\cdot |B)\) if it is preferable to \(X_{j}\) according to \(\succ ^{*}\) for all \(X_{j} \in K\), so also the coherent upper conditional prevision is involved to determine a maximal random variable in a class K. A Bayes random variable under a coherent lower conditional prevision is a random variable which is maximal under a linear prevision on the class of all random variables defined on B. Let \(\mu \) be a submodular coherent upper conditional prevision on \(\wp (B)\) and let \({\overline{P}}(\cdot |B)\) be the coherent upper conditional prevision represented as Choquet integral with respect to \(\mu \) and let \({\underline{P}}(\cdot |B)\) its conjugate coherent lower prevision. In this paper, sufficient conditions which assure that an admissible random variable is maximal if and only if it is a Bayes random variable are given. It is proven that: If K is a comonotonic class of random variables then \(X_{i}\) is maximal under \({\overline{P}}(\cdot |B)\) if and only if it is a Bayes. If K is a class of random variables such that the class \(\mathbf{C } =\left\{ X_{i}-X_{j}:X_{j} \in K \right\} \) is a comonotonic class, \(X_{i}\) is maximal under \({\underline{P}}(\cdot |B)\) if and only if it is a Bayes random variable. If K is a class containing only two random variables then \(X_{i}\) is maximal under \({\underline{P}}(\cdot |B)\) if and only if it is a Bayes random variable. Let \(\mu \) be a submodular coherent upper conditional probability on \(S\subset \wp (B)\). If K is a class containing \(\mu \)-upper measurable random variables, then \(X_{i}\) is maximal under a \({\underline{P}}(\cdot |B)\) if and only if it is a Bayes random variable.",8
87.0,2.0,Theory and Decision,07 May 2019,https://link.springer.com/article/10.1007/s11238-019-09704-9,Gerrymandering in a hierarchical legislature,September 2019,Katsuya Kobayashi,Attila Tasnádi,,Male,Male,Unknown,Male,"Many countries have adopted representative democracies. In most such countries, citizens directly elect the representatives of the legislature or head of state. However, some countries have full or partial indirect elections. In indirect elections, citizens elect “electors” or “upper electors” who elect those representatives. In the revolutionary era in France at the end of the eighteenth century, indirect electoral systems comprising two or three levels were attempted.Footnote 1 Similar examples can be found in the United States. Until the Seventeenth Amendment of 1913, the US senate also used an indirect electoral system (Ritchie 1988, p. 51). Currently, the US presidential election is based on an indirect electoral system. In the presidential primaries and caucuses in each party, citizens elect delegates, who have committed to represent a particular candidate in each state. The delegates elect a candidate as a party nominee. The general election follows a similar process (see US Federal Government 2017). However, we find few examples of such indirect elections in present legislative systems. As a system to summarize the political opinions of citizens, indirect elections may be inferior to direct elections. The purpose of this study is to examine the disadvantages of indirect elections in legislative systems. An indirect electoral system piles up multiple electoral levels. The features of indirect elections are more apparent when a legislature has multiple electoral levels, in which each representative is elected within a single-member district, as in the French system mentioned earlier. To examine them, we investigate the following types of electoral systems. At the lowest level, citizens grouped into single-member districts elect “ward representatives.” Next, ward representatives group into districts and elect county representatives from among themselves. Thereafter, county representatives group into districts and elect national (or state) representatives. Finally, national representatives elect a prime minister. The proposed model, which can be found in the related literature, is described as a device to facilitate the smooth functioning of a democracy in a large nation. Expressing his views on democracy, Jefferson (1816) describes what he calls “ward republics” and distinguishes between national, state, county, and ward levels. He describes this system as follows: “It is by dividing and subdividing these republics from the great national one down through all its subordination, until it ends in the administration of every man’s farm by himself; by placing under every one what his own eye may superintend, that all will be done for the best.” The council system is based on a similar idea [for more information on the history of this system, see Olson (1997)]. Arendt (1963) proposes a similar council system. According to Sitton (1987), Arendt’s proposal consists of a local council that would be open to all who wished to participate as well as higher councils comprising representatives of the lower councils. Consequently, federated councils would have a pyramidal shape. However, as our results suggest, there are concerns about such a system from the perspective of representative democracies. The problem with districting is allocating the authority of grouping citizens into districts.Footnote 2 If the district maker has a particular political position, she may construct the districts to provide an advantage to candidates with a similar position. This might result in gerrymandering and cause misrepresentations. Thus, a legislative democracy has a potential policy bias, which Gilligan and Matsusaka (2006) measure. They show theoretically how in an extreme gerrymandering case, a policy determined in a legislature is misrepresentative or diverges from the median voter. They assume a single-level legislature in their model. From their results, we can conjecture an increase in misrepresentation in the case of a multiple hierarchical legislative system because the opportunity for gerrymandering would expand vertically. We extend the districting model of Gilligan and Matsusaka (2006) to show that through extreme gerrymandering, a multiple hierarchical model of a representative democracy can serve minority interests. We show theoretically how the most extreme and electable position is obtained. In addition, we reveal that as the number of voters increases, the district maker can construct more intermediate levels. Accordingly, the district maker’s ability to implement her policy strengthens and even if she is in the extreme policy position, she gains dictatorial power.Footnote 3 From the standpoint of representative democracies, gerrymandering is one of the worst outcomes. The district maker’s ability to implement her policy is strong when the legislative system is hierarchical and is significantly stronger when there are many citizens in society. We conclude that by increasing this potential policy bias, the hierarchical legislative system is not a successful model in representative democracies despite offering the advantage of easy political participation. Thus, it is natural that a system of direct elections is democratically preferable to a multiple hierarchical legislative system. Notably, we do not consider the inherent geographical constraints that pose an issue in the political districting problem. Studies measuring district compactness include Chambers and Miller (2010, 2013) and Fryer and Holden (2011). Geographical constraints are considered in a normative framework by Puppe and Tasnádi (2015) and by the references therein. For instance, details on redistricting in practice can be found in Altman and McDonald (2010). The remainder of the paper is organized as follows. Section 2 presents our extended model of a multiple hierarchical representation. Section 3 considers the ability of extremists to implement their policy in the case of optimal gerrymandering. Section 4 provides the applications of our model. Finally, Sect. 5 concludes. Appendix A contains the proofs and Appendix B describes the applications.",
87.0,2.0,Theory and Decision,08 May 2019,https://link.springer.com/article/10.1007/s11238-019-09706-7,Logical independence of the axioms characterizing the degree measure in van den Brink et al. (2008),September 2019,Zhiwei Cui,Yan-An Hwang,,Unknown,,Unknown,Mix,,
87.0,3.0,Theory and Decision,20 June 2019,https://link.springer.com/article/10.1007/s11238-019-09713-8,Income inequality and risk taking: the impact of social comparison information,October 2019,Ulrich Schmidt,Levent Neyse,Milda Aleknonyte,Male,Male,Female,Mix,,
87.0,3.0,Theory and Decision,04 July 2019,https://link.springer.com/article/10.1007/s11238-019-09716-5,Condorcet efficiency of the preference approval voting and the probability of selecting the Condorcet loser,October 2019,Eric Kamwa,,,Male,Unknown,Unknown,Male,"Popularized by Brams and Fishburn (1978), the approval voting (AV) rule is a voting system under which each voter approves (any number of) candidates that he considers as acceptable and the winner is the most-approved candidate. This rule has made (and continues to be) the subject of numerous research works in political science, economics and computer science. To have a quick overview of these works, the reader may refer to the books of Brams and Fishburn (2007); Brams (2008) and to the Handbook of Approval Voting edited by Laslier and Sanver (2010). Under AV, there is no need to rank the candidates as under the scoring rules.Footnote 1 This absence of rankings gave rise to a controversy between Saari and van Newenhizen (1988a, b) and Brams et al. (1988a, b). Saari and van Newenhizen (1988b) blamed AV of hiding the real preferences of the voters which can be strict between the candidates approved by a voter. Brams and Sanver (2009) may have brought what appears as a possible response to this criticism by introducing the preference approval voting (PAV). Under PAV, each voter ranks all the candidates then indicates the ones he approves.Footnote 2 According to Brams and Sanver (2009), the winner under PAV is determined by two rules: Rule 1 The PAV winner is the AV winner ifFootnote 3 no candidate receives a majority of approval votes (i.e approved by more than half of the electorate) exactly one candidate receives a majority of approval votes. Rule 2 In the case that two or more candidates receive a majority of approval votes, the PAV winner is the one among these candidates who is preferred by a majority to every other majority-approved candidate. In the case of a cycle among the majority-approved candidates, then the AV winner among them is the PAV winner. 
Brams and Sanver (2009) noticed that it is Rule 2 that clearly differentiates PAV from AV. They pointed out that for some situations where a Condorcet winner exists, this candidate may not be a PAV winner under each of the subcases of Rule 1 and Rule 2. When she exists, a Condorcet winner is a candidate who defeats each of the other candidates in pairwise comparisons. We know that AV always elects the Condorcet winner when she exists given that voters’ preferences are dichotomous (Ju 2010; Xu 2010). This is no more the case when the voters’ true preferences are assumed to be strict orderings (Gehrlein and Lepelley 1998) or when indifference are allowed in the voters’ true preferences (Diss et al. 2010; Gehrlein and Lepelley 2015). For large electorates and three candidates, Gehrlein and Lepelley (1998) found that AV has the same Condorcet efficiency (probability of electing the Condorcet winner when she exists) as both the Plurality rule and the Antiplurality rule.Footnote 4 Going from a more general framework, Diss et al. (2010) found that for large electorates and three candidates, AV performs better that both the Plurality rule and the Antiplurality rule on the Condorcet efficiency; they also found some scenarios under which the Borda rule performs better than AV. Their results were strongly reinforced by Gehrlein and Lepelley (2015). To our knowledge, nothing is known about the Condorcet efficiency of PAV. One objective of this paper was thus to try to fill this void for voting situations with three candidates by focusing on the Condorcet efficiency of PAV when indifference are allowed as in Diss et al. (2010). So, we provide a representation of the limiting probability of the Condorcet efficiency of PAV. All the computations are done under the extended impartial culture assumption introduced by Diss et al. (2010); this assumption will be defined later. By definition, it is obvious that PAV performs better than AV on electing the Condorcet winner when she exists. It would be interesting to measure the extent of this dominance. In order to better reflect this, we first provide a more general representation of the limiting probability of the Condorcet efficiency of AV. Then, the representation provided by Diss et al. (2010) comes as a particular case of ours which appears as an alternative form of the representation provided by Gehrlein and Lepelley (2015). When she exists, a Condorcet loser is a candidate who is defeated by each of the other candidates in pairwise comparisons. Gehrlein and Lepelley (1998) showed that with more than three candidates and under the impartial culture assumption, AV is more likely to elect the Condorcet loser than the Plurality rule. For three-candidate elections, they showed that AV has the same probability of electing the Condorcet loser as both the Plurality rule and the Antiplurality rule. This result is a bit challenged by a recent paper by Gehrlein et al. (2016). Using impartial anonymous culture-like assumptionsFootnote 5 and considering a range of scenarios, Gehrlein et al. (2016) concluded that in three-candidate elections, AV is less likely to elect the Condorcet loser than both the Plurality rule and the Antiplurality rule. By definition, PAV is less likely to elect the Condorcet loser than AV. The second objective of this paper was to focus on the probability that PAV elects the Condorcet loser when she exists. We provide for AV and for PAV, analytical representations of the limiting probability of electing the Condorcet loser in three-candidate elections under the extended impartial culture assumption. By doing so, we will highlight at which extent PAV is less likely to elect the Condorcet loser than AV. The rest of the paper is structured as follows: Sect. 2 is devoted to basic notations and definitions. Section 3 presents our results on the Condorcet efficiency. Section 4 deals with the probability of electing the Condorcet loser. Section 5 concludes.",4
87.0,3.0,Theory and Decision,20 June 2019,https://link.springer.com/article/10.1007/s11238-019-09710-x,Trichotomic discounted utility,October 2019,Craig S. Webb,,,Male,Unknown,Unknown,Male,"The present bias example of Thaler (1981) has been widely accepted as convincing evidence that decision-makers frequently violate exponential discounting, and that decreasing impatience prevails. The quasi-hyperbolic discounting model elegantly captures present bias. By retaining exponential discounting for all periods after the present, the quasi-hyperbolic discounting model is immediately a familiar tool for economists. As a testament to its tractability, quasi-hyperbolic discounting is by far the most widely applied intertemporal choice model in behavioural economics (Dhami 2016, p. 645–691). The present bias example considers a specific type of comparison—short-term delays with long-term delays. More recently, a literature has emerged that has provided a more complete picture of time discounting. As well as confirming the importance of decreasing impatience, several studies have highlighted that increasing impatience is also common (Abdellaoui et al. 2010, 2013; Attema et al. 2010, 2016; Chesson and Viscusi 2003; Gigliotti and Sopher 2004; Onay and Onculer 2007; Read et al. 2005; Rohde 2018). However, one should not be tempted to conclude that individuals exhibit only decreasing impatience or only increasing impatience. Sayman and Onculer (2009) and Takeuchi (2011) found that many exhibit both increasing impatience and decreasing impatience. More specifically, increasing impatience followed by decreasing impatience. Takeuchi (2011) called this pattern inverse-S discounting. Inverse-S discounting arises, according to Sayman and Onculer 2009 and Takeuchi (2011), because of an extended notion of the “present”. Such a decision-maker exhibits present bias, in the sense that he is more impatient in this period than for longer delays. During the extended present, however, his impatience may increase. This paper studies inverse-S discounting behaviour. We provide a model, trichotomic discounted utility, that accommodates inverse-S discounting whilst retaining the elegant simplicity and tractability of quasi-hyperbolic discounting. Under trichotomic discounted utility the decision-maker distinguishes between short-term delays, medium-term delays, and long-term delays. Exponential discounting holds within each category, but not necessarily across each category. Having a tractable model that accommodates inverse-S discounting is important for both theoretical applications and empirical applications. In theoretical work, it seems desirable that applications of behavioural models capture the relevant behaviour found in experiments. In applied work, the empiricist’s choice of parametric model often influences the interpretation of the data. By fitting models that a priori exclude increasing impatience, or exclude inverse-S discounting, the empirical evidence could be misrepresented (Attema et al. 2016; Bleichrodt et al. 2009). Section 2 provides preference conditions for inverse-S discounting. Section 3 introduces trichotomic discounting to accommodate inverse-S discounting. A preference foundation is provided in Sect. 4 using the timed outcome framework. An axiomatic foundation for an integral representation, useful for macroeconomic and other applications, is provided in the “Appendix”. Section 5 presents related discussions. To readers familiar with recent developments in choice under risk, it will not have gone unnoticed that “inverse-S” sounds familiar. In experimental work on choice under risk, inverse-S probability weighting is the prevailing finding. In Sect. 5.1, a formal connection between inverse-S discounting and inverse-S probability weighting is established. In Sect. 5.2, a local version of time consistency is characterised. We show that this condition is satisfied only by discount functions with exponential “pieces” and discuss the implications for existing discount functions from the literature.",1
87.0,3.0,Theory and Decision,03 June 2019,https://link.springer.com/article/10.1007/s11238-019-09709-4,Strategic communication with reporting costs,October 2019,Winand Emons,Claude Fluet,,Male,,Unknown,Mix,,
87.0,3.0,Theory and Decision,29 June 2019,https://link.springer.com/article/10.1007/s11238-019-09714-7,Physician behavior and conditional altruism: the effects of payment system and uncertain health benefit,October 2019,Peter Martinsson,Emil Persson,,Male,Male,Unknown,Male,"Uncertainty in health and altruistic behavior by doctors are two fundamental aspects in the organization of health care (Arrow 1963). Individuals are uncertain about the incidence of and recovery from diseases and illnesses, and about the availability of treatments and their effectiveness for health improvements. Naturally, physicians have greater knowledge about the alternative treatments and their consequences for recovery. This information asymmetry between patients and physicians is a traditional principal–agent problem. Physicians will to a large extent, if not fully, influence the quantity and quality of the health care provided, and their effort is influenced by their preferences for own income as well as altruism toward the patient, which are together affected by the prevailing payment system (e.g., McGuire 2000, for a general overview of payment systems). Due to budget constraints, a central issue in the provision of health care is allocation. Many countries have implemented explicit prioritization rules based on general ethical principles, such as equal access, cost and clinical effectiveness, and severity-graded valuation of health gains. Physicians are then incentivized to provide medical treatment that is in each patient’s best interest given these rules. As physicians are the gatekeepers to the health care system, it is important for policy makers to understand how physicians trade-off own income and altruism and how their behavior in this way corresponds to general principles for priority setting under different payment systems and in the presence of uncertainty about health outcomes. Our objective is to investigate whether and to what extent a physician’s altruistic behavior depends on the payment system and uncertainty in health outcome. To this end, we conduct an experiment building on the novel framework introduced by Hennig-Schmidt et al. (2011). We contribute to filling important gaps in the literature concerning the heterogeneity of physician altruism and the effects of introducing risk and ambiguity in health outcomes for patients.Footnote 1 Existing evidence concerning the heterogeneity of physician altruism is relatively scarce.Footnote 2 Hennig-Schmidt et al. (2011) found that altruism was an important component of physicians’ choices, and Godager and Wiesen (2013) and Brosig-Koch et al. (2016, 2017) showed that the level of altruism varied across both physicians and patients. We extend these results both conceptually and empirically by categorizing physicians according to how well their treatment decisions align with three different ethical principles for priority setting: severity of illness, capacity to benefit, and ex post equality. We find that many physicians are altruistic toward their patients and that the degree of altruism varies across patients with different medical needs. Interestingly, the type classification of physicians based on ethical principles is unaffected by payment system; the common categorization is that physician altruism is guided by severity of illness, both under capitation and fee-for-service. We replicate the previous finding that patients are undertreated in capitation payment systems and overtreated in fee-for-service systems, and we show that this result is independent of whether patient’s health benefit is deterministic, risky or ambiguous. This is our main contribution and a robustness result for the particular methodology introduced by Hennig-Schmidt et al. (2011) indicating that results from previous studies relying solely on deterministic patient’s health benefits can be extended to risky and ambiguous situations. In more detailed analyses, we find substantial individual heterogeneity in physicians’ responses to the introduction of risk and ambiguity in patient health. This heterogeneity partly depends on payment system because there is a significant correlation between physicians’ behavior and their generic risk and ambiguity preferences under capitation but not under fee-for-service.",11
87.0,3.0,Theory and Decision,24 April 2019,https://link.springer.com/article/10.1007/s11238-019-09700-z,Hukou identity and fairness in the ultimatum game,October 2019,Jun Luo,Yefeng Chen,Guanlin Gao,,Unknown,Unknown,Mix,,
87.0,4.0,Theory and Decision,17 September 2019,https://link.springer.com/article/10.1007/s11238-019-09720-9,On the coalitional stability of monopoly power in differentiated Bertrand and Cournot oligopolies,November 2019,Aymeric Lardon,,,Male,Unknown,Unknown,Male,"The purpose of this article is to revisit the classic comparison between Bertrand and Cournot competition in the presence of a cartel of firms. We concentrate on industries consisting of symmetrically differentiated products represented by Shubik’s demand system (Shubik 1980), each one produced by a single firm. Furthermore, we assume that firms operate at a constant and identical marginal cost. While cartel members maximize their joint profit by correlating their strategies and play as a multiproduct firm, other firms, called outsiders, are supposed to act independently. The main interest of this competition setting is to examine the two most well-known solution concepts in non-cooperative and cooperative games, namely, the Nash equilibrium (Nash 1950) and the core (Shapley 1955). In oligopoly theory, a well-known result is that Bertrand competition is more competitive and efficient than Cournot competition. More properly speaking, Bertrand competition yields lower prices and profits and higher quantities, consumer surplus, and welfare than Cournot competition. Singh and Vives (1984) have first established these standard Bertrand–Cournot rankings which have been extended by Cheng (1985), Vives (1985), and Okuguchi (1987). Some years later, the limitations of these results have been pointed out by Dastidar (1997) exploiting cost asymmetries, and Häckner (2000), and Amir and Jin (2001) using product differentiation. Other limitations have been put forward by, among others, Lofaro (2002) with incomplete information on costs, Miller and Pazgal (2001) in environments with strategic managerial delegation, and Pal (2015) including networks externalities in the latter approach. Wang and Zhao (2007) have also compared the welfare effects of cost reductions between Bertrand and Cournot oligopolies. To date, the literature comparing Bertrand and Cournot competition has exclusively focused on environments where all firms maximize their profits individually. In the first part of this article, merely assuming that a cartel of firms has been formed and faces outsiders acting individually, we provide new limitations of the standard Bertrand–Cournot rankings discussed above. More accurately, while the standard Bertrand–Cournot rankings still hold for Nash equilibrium prices, the results may be altered for Nash equilibrium quantities and profits. Indeed, Bertrand competition yields higher quantities for cartel members than Cournot competition, but each outsider may raise or reduce its production depending on the quantity change of cartel members. As a consequence, outsiders still earn lower profits in Bertrand than in Cournot competition, but the cartel joint profit may be larger in Bertrand competition when the number of firms is sufficiently large. In spite of these results, we show that the standard Bertrand–Cournot rankings on profits always hold when the number of firms is lower or equal to 25 which corresponds, in practice, to the majority of differentiated oligopolies with symmetric costs. It is worth noting that beyond this 25-firm bound, there always exists a cartel of firms for which the standard Bertrand–Cournot rankings on profits do not hold anymore. In economic welfare analysis, it is a well-established and old idea that monopoly power can negatively affect social welfare. One of the main sources of monopoly power is collusion between firms which has long been the focus of much theoretical and empirical work. Stable horizontal mergers have traditionally been analyzed by means of non-cooperative games as in, for example, Huck et al. (2005), by modeling a Stackelberg game. Furthermore, based on an appropriate notion of individual stability where no insider has an incentive to deviate from the merger, and no outsider has an incentive to join it, several works (Donsimoni 1985; Shaffer 1995; Konishi and Lin 1999; Zu et al. 2012) have studied the size of stable mergers provided that outsiders behave individually as fringe firms. A survey of the literature on stable horizontal mergers in a Cournot oligopoly has been provided by Currarini and Marini (2015). More generally, the analysis of endogenous coalition formation has been conducted by Hart and Kurz (1983), Bloch (1995), Ray and Vohra (1997), Yi (1997), and Vasconcelos (2006) using a wide variety of equilibrium concepts. While tacit horizontal agreements have been modeled by means of repeated games (see, for example, Friedman 1971), formal collusionFootnote 1 has more recently been analyzed in the framework of cooperative oligopoly games with transferable utility, henceforth oligopoly TU-games. Besides the set of players, a TU-game consists of a characteristic function assigning to each subset of players, called coalition, a real number which represents the worth that these players can obtain by agreeing to cooperate. In oligopolies, since the decision of a cartel as well as its joint profit depend on the behaviors of outsiders, the determination of the worth that a coalition can obtain requires to specify how such outsiders act. A general approach consists in converting a normal form game to a partition function game (Thrall and Lucas 1963) by finding a quasi-hybrid solution (Zhao 1991), and then studying the stability concepts of the associated cooperative games. An appropriate approach for oligopolies, called the \(\gamma \)-approach, is proposed by Hart and Kurz (1983), Rajan (1989), and Lardon (2012). It consists in considering a competition setting in which cartel members face outsiders acting individually. The worth of any coalition is then determined by the joint profit it obtains when remaining members break up into singletons in response to the deviating players. Alternatively, many other approaches have been suggested in the literature. Initially, Aumann (1959) has proposed the \(\alpha \) and \(\beta \)-approaches which consist in computing the max–min and the min–max payoffs of each coalition, respectively. However, these two approaches are not the most appropriate with regard to the rational behaviors of firms in oligopolies as discussed by Lardon (2012). The \(\delta \)-approach (Hart and Kurz 1983; Rajan 1989; Gabszewicz et al. 2016) stipulates that remaining members stay together and maximize their joint payoff facing to the deviating coalition. Currarini and Marini (2003) have also proposed the \(\lambda \)-approach for which the remaining firms act individually and then compete with the deviating coalition in a Stackelberg game. An appropriate set-valued solution for oligopoly TU-games that deals with the stability of monopoly power is the core. Given a payoff vector in the core, the grand coalition, i.e., the cartel comprising all firms, could form and distribute its worth as payoffs to its members in such a way that no coalition can contest this sharing by breaking off from the grand coalition. In oligopoly TU-games, the stability of monopoly power sustained by the grand coalition is then related to the non-emptiness of the core. Balancedness is a necessary and sufficient condition for the core to be non-empty (Bondareva 1963; Shapley 1967). Until now, the cores of Bertrand and Cournot oligopoly TU-games have been independently studied by Zhao (1999), Norde et al. (2002), Lardon (2012, 2019), Lekeas and Stamatopoulos (2014), Watanabe and Matsubayashi (2013), and Takeda et al. (2018) among others. In the second part of this article, we aim to build bridges between the cores of Bertrand and Cournot oligopoly TU-games. More precisely, based on the previous analysis on Nash equilibrium profits of cartel members, we establish that the core of a Cournot oligopoly TU-game is strictly included in the core of a Bertrand oligopoly TU-game when the number of firms is lower or equal to 25. Furthermore, we prove that the core of Cournot oligopoly TU-games is non-empty which has not been established before under product differentiation. Afterwards, we focus on the aggregate-monotonic core, a subset of the core, introduced and characterized by Calleja et al. (2009). Whenever the core is non-empty, the aggregate-monotonic core selects point solutions in the core that satisfy aggregate monotonicity property, proposed by Meggido (1974). Roughly speaking, this natural property requires that the payoff of each player does not decrease if the worth of the grand coalition grows. We prove that the aggregate-monotonic core of a Cournot oligopoly TU-game is strictly included in the aggregate-monotonic core of a Bertrand oligopoly TU-game regardless of the number of firms. As a consequence, the core inclusion property may not hold for some oligopolies because of core payoff vectors selected by point solutions that do not satisfy aggregate monotonicity property. For the case of asymmetric costs, we study a class of three-firm oligopolies with asymmetric costs in which the core inclusion property still holds. Moreover, numerical examples tend to prove that the core inclusion bound drastically decreases to three firms for some oligopolies. However, we prove that such oligopolies are not relevant from an economic point of view since they are associated with negative equilibrium quantities. This shows that establishing a core inclusion property with asymmetric costs turns out to be a very difficult challenge. The remainder of the article is organized as follows. In Sect. 2, we introduce the non-cooperative and cooperative models of differentiated Bertrand and Cournot oligopolies. Section 3 compares Nash equilibrium prices, quantities, and profits in normal form Bertrand and Cournot oligopoly games in the presence of a cartel of firms. Section 4 is devoted to the comparison of the cores and the aggregate-monotonic cores between Bertrand and Cournot oligopoly TU-games. Section 5 gives some concluding remarks on the difficulty to extend the analysis from symmetric to asymmetric costs. Finally, Sect. 6 is the appendix where all proofs of the results are presented.",1
87.0,4.0,Theory and Decision,17 September 2019,https://link.springer.com/article/10.1007/s11238-019-09721-8,The effects of loss aversion on deceptive advertising policies,November 2019,Aldo Pignataro,,,Male,Unknown,Unknown,Male,"When consumers buy products of unknown value, before purchase—i.e., the so-called experience goods—, advertising and marketing initiatives become crucial to drive consumers’ choices. They may provide presale information that enables prospective customers to ascertain the quality of the products on sale. It is, therefore, essential that firms’ claims are truthful to prevent consumers from making bad purchases. It is widely known that misleading and deceptive advertising harms consumers: it impedes them to fairly compare the products on sale (with a consequent distortion of competition between firms) and it may prompt naive consumers to buy products at a higher price than they are willing to pay. Additionally, misleading advertising makes fair marketing practice services of no value.Footnote 1 For these reasons, competition authorities such as the Italian Antitrust Authority (AGCM) and the Competition Market Authority (CMA) have traditionally fought deceptive marketing practices.Footnote 2 Specifically, misleading and deceptive advertising has usually been subject to sanctions imposed by public authorities.Footnote 3 This kind of approach to consumer protection is even more relevant when buyers are not perfectly rational, with some cognitive biases leading them to make purchase decisions that are somehow influenced by advertising claims. If consumers are aware of the fact that ads and marketing initiatives can be misleading, they may anticipate the event of making bad purchases—i.e., buying products that are not worth their price. This could lead them to ask for a price discount, especially if they value losses more than equal-sized gains. This behavioral bias was first introduced as part of Prospect Theory (Kahneman and Tversky 1979) under the definition of loss aversion. The underlying idea is that preferences may not depend directly on products’ intrinsic value which, instead, must be compared to some reference point (which is affected by marketing claims). Individuals are loss averse if gains with respect to the reference point are less pleasant than the pain deriving from equal-sized losses.Footnote 4 A recent and growing literature has started to study the strategic interactions between firms and loss-averse consumers.Footnote 5 Although some of these papers (Heidhues and Köszegi 2014; Karle and Peitz 2014; Karle and Schumacher 2017; Zhou 2011) explain how a firm may exploit loss aversion through marketing strategies, the effects of deceptive advertising on consumer surplus in a competitive environment have not been well researched. It is particularly interesting to address this issue in markets characterized by experience goods, in which quality disclosure seems to play a crucial role in attracting loss-averse consumers.Footnote 6 Specifically, the aim of this paper is to answer the following research questions: how does consumers’ loss aversion affect advertising and pricing policies? What is the effect on consumer surplus of a regulation that punishes deceptive advertising? What is the optimal sanction for deceptive advertising according to the degree of consumer loss aversion? Surprisingly, while the degree of consumer loss aversion has no impact on firms’ behavior, the optimal enforcement level of a regulation against misleading advertising crucially depends on the consumers’ cognitive bias. We derive these results by extending the model of Piccolo et al. (2015) to a scenario in which consumers may be loss averse, while firms are perfectly rational. There are two sellers producing vertically differentiated goods. Consumers cannot distinguish the high-quality from the low-quality product before the purchase. However, the difference in products’ quality is common knowledge. Sellers may advertise their products and the low-quality seller may deceive consumers by claiming to sell a high-quality product. This allows us to provide a very general model that incorporates the results when consumers behave rationally, but also shows a consumers’ more realistic attitude in the marketplace. For instance, in two recent reports, the Office of Fair Trading (OFT) (2010, 2011) has recognized that consumers may suffer from cognitive biases which make the comparison of products harder, stressing the consequent policy implications for regulators and antitrust authorities.Footnote 7 This paper shows the effect of consumers’ loss aversion on firms’ strategies in a competitive environment. Differently from the standard literature, the degree of consumers’ loss aversion does not reflect on firms’ prices. Yet, the existence of pooling equilibria—when the low-quality seller advertises deceptively—and separating equilibria—when sellers quote different prices and advertise truthfully—depends on such a behavioral bias. Indeed, consumers dislike uncertainty and they are not willing to buy a product of unknown quality if the degree of loss aversion is sufficiently high. We show that, depending on the level of enforcement against misleading advertising, adopted by regulators and public authorities, sellers may advertise deceptively, with non-obvious effects on consumers’ welfare. Although punishing deceptive advertising harshly is apparently free for consumers, there are some “hidden” costs deriving from allowing prospective customers to recognize products’ quality. If both sellers advertise, the products on sale (behind the veil of ignorance) appear perfect substitutes to consumers, such that the downward pressure on prices may compensate the risk of making a bad purchase. Hence, soft enforcements maximize consumers’ welfare only if consumers are not too loss averse and competition on prices is sufficiently tough.Footnote 8 Increasing the monetary sanction against misleading advertising relaxes competition with a negative effect on consumers’ welfare, unless it induces firms to reveal their products’ quality truthfully. This analysis suggests that intermediate levels of enforcement are never optimal for consumers. Doing so, we point out a non-monotone relationship between loss aversion and monetary sanctions. Finally, we provide a rationale for strong enforcements when the degree of consumers’ loss aversion is sufficiently high. This allows us to understand whether and under which conditions a policy intervention has a positive effect on consumer surplus. The rest of the paper is organized as follows. In the next section, we relate our contribution to the existing literature. Section 3 introduces the model. In Sect. 4, we characterize all feasible equilibria. In Sect. 5, we discuss policy implications for a Public Authority choosing a monetary sanction against deceptive advertising. Section 6 concludes. All proofs are in Appendix.",2
87.0,4.0,Theory and Decision,27 August 2019,https://link.springer.com/article/10.1007/s11238-019-09717-4,Incentive schemes and peer effects on risk behaviour: an experiment,November 2019,Francesca Gioia,,,Female,Unknown,Unknown,Female,"Risk is inherent in a wide range of economic decisions, such as the field of study chosen at university (Saks and Shore 2005; Caner and Okten 2010; De Paola and Gioia 2012), schooling attainments (Belzil and Leonardi 2007), job decisions (Pissarides 1974), health-related behaviours (Anderson and Mellor 2008), marriage and parenthood (Schmidt 2008; Light and Ahn 2010; Spivey 2010), migration (Jaeger et al. 2010) and, of course, the allocation of financial assets (Schooley and Worden 1996; Hariharan et al. 2000). Recent research has shown that individual risk attitudes are not immutable personality traits, but are shaped at least partly by the environment, such as family structure or the gender composition of the classroom (Dohmen et al. 2012; Booth and Nolen 2012), by emotional states (Bassi et al. 2013; Conte et al. 2013; Campos-Vazquez and Cuilty 2014) and by life experiences such as poverty, violence or job loss (Haushofer and Fehr 2014; Callen et al. 2014; Hetschko and Preuss 2015). More directly relevant to this paper, an individual’s risk attitudes appear to be shaped by their peers (Cooper and Rege 2011; Lahno and Serra-Garcia 2015). Peer effects are particularly valuable from a policy perspective, because they amplify the benefits of any policy intervention thanks to the social multiplier effect (Manski 1993). We do, however, know little about what determines the existence and strength of peer effects. Peers are, by definition, part of the same social setting, but the nature of their social interactions may differ. At school or at work, peers may be competing to be the best or to be promoted. However, they may also have incentives to cooperate, to share specific knowledge or carry out complex projects. Many organizations and educational institutions often use competition for career advancement, sought-after jobs or high grades, and teamwork with compensation equally shared among group members as incentives for their employees/students to increase their performance. In this paper, we investigate the extent to which the nature of incentive schemes to which individuals are exposed affects the existence and strength of peer effects. We hypothesize that collaborators and competitors represent different types of peers and that the implementation of such incentive schemes may therefore trigger different levels of attachment to the peer group and induce peer effects of different magnitudes. In a nutshell, we ask: are you more likely to be influenced by your peers if you compete with them or if you have to cooperate? Since peer interactions outlast the competitive/cooperative task, we are not interested in peer effects on immediate productivity, but instead study peer effects on subsequent risk behaviour and, in turn, on subsequent decisions influenced by risk attitudes, which could affect future productivity. 
We answer our research question by conducting a laboratory experiment with three treatment conditions corresponding to three different compensation schemes—piece rate (Individual), the equal-split-sharing-rule (Cooperative) and a tournament (Competitive). Each of these schemes is associated with an effort task, named the Coin Task, which consists of 5 min spent recognizing the value and country of origin of a random sequence of Euro coins, displayed on a computer screen. Risk preferences are measured both before and after the incentivized task using the Bomb Risk Elicitation Task (Crosetto and Filippin 2013). Attachment to peers and perceived peer influence are elicited by a questionnaire completed at the end of the experiment. We find that participants in the Individual treatment are positively and statistically significantly influenced by their randomly assigned peers when making their risky choice following the effort task. Peer effects are very similar in size and statistical significance to the former scheme for participants in the Cooperative treatment, while they are more than halved and no longer statistically significant when performance is incentivized by competition. Thus, our findings confirm the evidence of peer effects on risk behaviour which has emerged from the economic literature and seem to point to a role of incentivizing schemes on peer influence, particularly for tournaments as these produce an economically significant attenuation of peer influence. The difference in peer effects between the Individual and Competitive treatments becomes statistically significant when we restrict our attention to the subsample of participants whose peer has an initial risk attitude at a level for which we have at least one peer for each treatment. In some specifications, working in a competitive environment more than offsets the positive effect of a random peer producing negative and statistically significant peer effects: the higher the risk-seeking behaviour of the competitor, the more risk-averse the subject is. When digging deeper to understand if the implementation of incentive schemes changes feelings of group membership, we find that after competing, participants feel significantly less attached to their peers than after the implementation of either a piece-rate compensation scheme or a cooperation-based incentive scheme. Moreover, tournaments significantly attenuate the probability of self-reported peer influence compared with the other two compensation schemes. Our findings are important for policy makers or private companies interested in influencing individuals’ choices involving risk and have implications for the design and evaluation of optimal compensation schemes. The paper is structured into five parts. Section 2 briefly presents the related literature. In Sect. 3 we describe our experimental design. Section 4 presents our empirical analysis. Section 5 provides the conclusion.",2
88.0,1.0,Theory and Decision,25 November 2019,https://link.springer.com/article/10.1007/s11238-019-09736-1,Editorial: Foundations of Utility and Risk Conference (FUR 2018),February 2020,John D. Hey,Chris Starmer,,Male,,Unknown,Mix,,
88.0,1.0,Theory and Decision,10 May 2019,https://link.springer.com/article/10.1007/s11238-019-09705-8,States of nature and states of mind: a generalized theory of decision-making,February 2020,Iain P. Embrey,,,Male,Unknown,Unknown,Male,"Neoclassical Expected Utility Theory explains all individual differences as the result of heterogeneity in tastes. However, there is now compelling evidence that Homo sapiens also exhibit heterogeneity in thought processes. Several recent papers explicitly reject the hypothesis that thought-process heterogeneity can be adequately modeled as if it was taste heterogeneity (Swait and Bernardino 2000; Swait and Adamowicz 2001; Hess et al. 2012; Kaplan et al. 2012; Vij and Walker 2014), which suggests that some decision situations may admit multiple distinct utility representations. Indeed, mixture models have already been shown to significantly increase explanatory power by operationalizing heterogeneous decision-making between individuals, even accounting for their greater degrees of freedom (Conte et al. 2011). There is also considerable evidence that individual decision-makers do not consistently apply the same decision criterion, either across time or across decision situations (Luce and Raiffa 1957; Loewenstein et al. 2003; DellaVigna 2009). We recognize these empirical findings by analyzing the simplest possible generalization of Expected Utility Theory that can admit heterogeneous thought processes, both between individuals and within individuals. Our generalized decision theory incorporates two distinct utility formulations that represent two distinct thought processes. On an individual level, this approach naturally operationalizes the quintessential human conflict between deliberative and impulsive thought processes, which motivates the extensive psychological ‘dual-self’ literature, and which has recently been popularized by Kahneman (2011) and Peters (2012).Footnote 1 Thus, under the proposed model, agents may deliberate as per Homo economicus, but they may alternatively act on impulse, and each agent’s propensity to act deliberatively is modeled as an individual- and situation-specific probability distribution. This generalized decision theory sits within the class of general random utility models that was formally defined by Manski (1977), although it is distinguished from existing theory since the choice problem generating process is explicitly modeled.Footnote 2 We apply our generalized decision theory to explain the empirical phenomenon of crowding out, and to explain a series of interrelated empirical anomalies within the broad field of human capital development. The former application demonstrates the importance of situation-specific heterogeneity in thought processes, whilst the latter demonstrates the importance of individual-specific heterogeneity in thought processes. In particular, Sect. 2 shows that crowding out will arise whenever the provision of an additional extrinsic incentive raises the probability that an undesirable extrinsic thought process will be adopted rather than a desirable intrinsic thought process. Meanwhile, Sect. 4 shows that an individual who acts impulsively rather than deliberatively could achieve educational and employment outcomes that are divergently below normatively optimal predictions. Our generalized decision theory may be particularly salient when it is applied to the decision-making of children. Although each of us will, on occasion, act without first considering the consequences of that action, children are particularly likely to do so.Footnote 3 It has already be observed on an intuitive level that this tendency could lead to normatively suboptimal levels of educational investment (Lavecchia et al. 2015). Our results in Sects. 4 and 5 provide a theoretical basis for that intuition, and they also provide intuitive yet original explanations for several other empirical truths. These include: chronic unemployment, strong inter-generational persistence of social inequalities, dynamic complementarity between cognitive and non-cognitive abilities, divergent developmental pathways dependent upon small changes in early-life experiences, and an explanation for the observed relationships between IQ, Cognitive Reflection (as measured by the Cognitive Reflection Test of Frederick 2005), behavioral biases (including present bias and risk aversion), and other social outcomes (such as health and financial decision-making). These findings suggest that the generalized decision theory has the potential not only to improve our understanding of specific behavioral anomalies, but also to bring together diverse strands from the existing literature. The concept that deliberative thought processes may not always override individuals’ impulsive responses is not new; it was discussed by Plato (ca. 380 B.C.E. 1906), Smith (1759), and Marshall (1890) amongst others. However, the psychological literature has only recently converged toward a default-interventionist paradigm to formalize that concept (Evans and Stanovich 2013).Footnote 4 The default-interventionist paradigm is also closely aligned with the perspective which Bechara (2005) distils from the neuroscientific literature; however, it is at odds with all existing economic dual-self theories (Embrey 2019). Moreover, the existing economic dual-self literature maintains the Neoclassical assumption of thought-process homogeneity, either by assuming that some meta-rational process mediates between the alternative utility formulations (e.g., Fudenberg and Levine 2006), or by assuming that context alone perfectly determines which utility formulation will predominate (e.g., Thaler and Shefrin 1981). The proposed model is, therefore, most closely related to the those of Laibson (2001) and Bernheim and Rangel (2004), since, although neither is framed as a dual-self theory, each describes addiction as an alternative, flawed, decision process. Nevertheless, in those models, addictive thinking is triggered whenever an external cue is received, which leads their authors to focus on a representative and completely informed agent’s rational response to that situation.Footnote 5 By contrast, the present model emphasizes the dynamic consequences of individual heterogeneity in thought processes, for everyday situations where individuals may not even be aware that they have made a decision, much less possess complete knowledge of their own decision processes. Such situations cannot be characterized by any single representative agent, unless impulsive and deliberative decision processes happen to coincide. The main advantage of the Neoclassical assumption of a single representative agent is that it typically affords a mathematically elegant analysis. However, that mathematical elegance should not be mistaken for parsimony. The Neoclassical approach requires three layers of assumption: firstly, the set of relevant motivations is postulated; secondly, a functional form for each motivation is prescribed; finally, the functional form of a single-valued utility function is also prescribed, whereby those disparate motivations are assumed to be traded off against each other. The generalized approach typically also requires the first two layers of assumption, but it does not impose any homogeneous rule by which disparate motivations must be traded off. Thus, ceteris paribus, the law of parsimony would favor the generalized theory (Ockham ca. 1323 1974); a conclusion which holds a fortiori since that generalized theory provides an unified explanation for a number of open empirical questions. It is rare for modern decision theory to explicitly consider the validity of the above ‘single-self’ assumption set. This is because the revealed-preference paradigm of Samuelson (1938), and its formalization by Savage (1954), demonstrated that an expected utility representation must exist whenever a number of postulates are satisfied. The generalized theory proposed here is fully compatible with those seminal observations. Our contribution is to expand the applicability of Expected Utility theory from situations in which the Savage postulates apply globally, to situations in which they apply conditional upon the decision-maker’s state of mind. Thus, we do not require preferences to be complete, transitive, and consistent through time, but only that these characteristics describe the agent’s preferences under any given thought process. This substantially more tenable assumption relaxes the neoclassical assumption in much the same way that conditional independence relaxes an econometric independence assumption.",2
88.0,1.0,Theory and Decision,07 June 2019,https://link.springer.com/article/10.1007/s11238-019-09708-5,A test of risk vulnerability in the wider population,February 2020,Philomena M. Bacon,Anna Conte,Peter G. Moffatt,Female,Female,Male,Mix,,
88.0,1.0,Theory and Decision,18 May 2019,https://link.springer.com/article/10.1007/s11238-019-09707-6,Elicitation and modelling of imprecise utility of health states,February 2020,Michał Jakubczyk,Dominik Golicki,,Male,Male,Unknown,Male,"Imagine you may live for 10 years in a wheelchair or T years in full health (both followed by sudden and painless death). What T would make you exactly indifferent? Would the choice be obvious if one extra day was added to such T, or a month, or a year? In ordinary life, people make no explicit choices between health states or between health-related quality and longevity of life. Hence, their preferences may not be precisely established. Still, in thought experiments, trade-offs are used to assign utilities to health states, to subsequently support the appraisal of health technologies (e.g. Xie et al. 2016; Shiroiwa et al. 2016). In time trade-off (TTO) method, we elicit time, T, that makes the respondent indifferent between T years in full health vs 10 years in a hypothetical state; the first paragraph demonstrated how difficult that may be. Imprecision of preferences is present in many decision contexts and can explain observed behaviours, e.g. some violations of the standard decision theory (Cubitt et al. 2015; Butler and Loomes 2007, 2011). Forcing respondents to answer may increase the dropout rate or reduce data quality (Stieger et al. 2007; Décieux et al. 2015). Additionally, when the actual value is not known precisely, how the question is framed may change the outcome.Footnote 1 In TTO, a pre-defined sequence of Ts is used; imprecise preferences may cause the respondent to accept the first in a range of indistinguishably good answers, not the single best T. Thus, what sequence of Ts is used may change the outcome. If it is imprecision that is responsible for this à la satisficing behaviour (see Simon 1956), then simply increasing the motivation of the respondents may not suffice; rather, they must be helped to understand their own preferences, e.g. via focus groups discussion prior to elicitation task. Also, the effect of imprecision may not decrease with sample size: if the sequence of Ts results in lower value of the range being selected more often (e.g. in a bottom-up approach), then using more respondents will not help. Hence, getting respondents to understand own preferences better may outweigh getting more respondents. That satisficing prevails in TTO can be seen from the indifference being found at all: a non-obvious fact, given a limited number of Ts offered and the continuum of possible utility values. Whether the rejected Ts provide useful information (on what was not satisfactory enough) was tested by Ramos-Goñi et al. (2017) in the interval regression. Still, the authors use a standard TTO protocol and assume that the disutilities of health state characteristics are regular numbers, implicitly assuming the preferences are precise. In the present paper, we address the imprecision of health preferences and contribute to the literature in two ways. Firstly, we allow the respondents to directly report in TTO their answers as imprecise, by asking them to indicate (1) the range of values they consider as equally plausible as the standard, single-number outcome T and (2) the range of values they consider somewhat plausible (i.e. not entirely precluded). To define health states, we use the EQ-5D-3L descriptive system (Brooks and De Charro 1996): a health state is described by five dimensions (hence, 5D)—mobility (MO), self-care (SC), usual activities (UA), pain/discomfort (PD), and anxiety/depression (AD)—each set on one of three levels (hence, 3L, 1 = no problems, 3 = extreme problems, see Table 1 in Supplementary Material for exact wording). Then, we may see how the characteristics of a health state are associated with the amount of imprecision, how capable of explaining the inconsistencies in TTO answers the imprecision is, and how much a path of rejected Ts reveals about the degree of imprecision. Secondly, we test several modelling approaches to assign disutilities to individual dimensions, accounting for the fact that TTO answers are imprecise, but also that the disutilities should be treated as imprecise numbers. To enable mathematical rigour in modelling, we treat utilities as fuzzy sets (notion introduced by Zadeh 1965). Our results show, e.g. that UA adds most imprecision to the valuation results—a factor that could be considered in working on a descriptive system. Representing the imprecision as fuzzy sets/numbers has been used before in health preference context. Jakubczyk and Kamiński (2017) showed how to treat the willingness to pay/accept for health as fuzzy. Understanding the imprecision of health state utilities could be yet another factor to consider in sensitivity analysis in health technology assessment. Modelling these two forms of imprecision mathematically may make it easier to combine them in a meaningful way. Regarding health state utilities, Jakubczyk et al. (2018b) estimated the disutilities of the EQ-5D-5L system (five levels per dimension) as fuzzy based on standard discrete choice experiment (DCE) data. In their study, the data came as choices between paired health states with duration; hence, the disutility of a state was not directly observed, contrary to the present study. Jakubczyk and Golicki (2018) estimated the disutilities as imprecise based on a path of answers in standard TTO. In accounting for the path, their study was similar to Ramos-Goñi et al. (2017), but the novelty lay in estimating the disutilities as imprecise. The limited amount of information derived from the path resulted in a very large error of estimates. As we show in the present paper, using a direct assessment of the imprecision from the respondents results in much narrower ranges, even in a smaller sample. The structure of the paper is as follows. Because multiple analyses were performed, we did not distinguish the methods and results sections; instead, we split the paper by the area of analysis. In the next section, we present the questionnaire design and the characteristics of the sample used. Then (Sect. 3), we discuss the raw results of the questionnaire. In Sect. 4, we analyse the determinants of the amount of imprecision. Section 5 is central to the paper: we discuss how the disutility is shaped by the worsening in individual dimensions in the EQ-5D-3L descriptive system. The main results are further discussed in the last section.",2
88.0,1.0,Theory and Decision,17 August 2019,https://link.springer.com/article/10.1007/s11238-019-09719-2,Why do people prefer randomisation? An experimental investigation,February 2020,Yudistira Permana,,,Unknown,Unknown,Unknown,Unknown,,
88.0,1.0,Theory and Decision,18 July 2019,https://link.springer.com/article/10.1007/s11238-019-09715-6,Telling the other what one knows? Strategic lying in a modified acquiring-a-company experiment with two-sided private information,February 2020,Andrej Angelovski,Daniela Di Cagno,Francesca Marazzi,Male,Female,Female,Mix,,
88.0,1.0,Theory and Decision,07 May 2019,https://link.springer.com/article/10.1007/s11238-019-09703-w,Firm’s protection against disasters: are investment and insurance substitutes or complements?,February 2020,Giuseppe Attanasi,Laura Concina,Valentina Rotondi,Male,Female,Female,Mix,,
88.0,1.0,Theory and Decision,18 June 2019,https://link.springer.com/article/10.1007/s11238-019-09711-w,An experimental investigation of the ‘tenuous trade-off’ between risk and incentives in organizations,February 2020,Subhasish M. Chowdhury,Alexandros Karakostas,,Unknown,Male,Unknown,Male,"One of the most celebrated results in personnel and organizational economics is on the optimal incentive intensity of employment contracts (Holmstrom and Milgrom 1987). A key tenet of this study is that in markets, where employees’ efforts are unobservable and outputs are subject to high volatility due to idiosyncratic market factors, the use of performance measures that are tied to outputs should be less common than in markets exhibiting less volatility and more stable performance. This is because, as agents are risk averse, they will be less willing to accept contracts that tie their earnings to stochastic factors beyond their control. Consequently, the greater the risk associated with the environment, the lower the incentive intensity or performance pay of the employment contract. In the literature, this is often termed as the Incentive Intensity Principle (Milgrom and Roberts 1992). Despite the clear intuition of the model, the empirical evidence on the relationship between riskiness in the environment and the incentive intensity of payment schemes has been rather mixed, casting doubts on the validity of the model (Prendergast 1999, 2000, 2002) reports the findings from 26 empirical studies, out of which only four find evidence for a negative relationship, as predicted in the theory. This dearth of empirical support has led to the proliferation of new theoretical models that attempt to explain why a negative relationship may not be observed in the field. For instance, Prendergast (2002) argues that the delegation of principal’s authority to an agent can explain the evidence for a positive relationship between incentive intensity and performance measure. Similarly, Budde and Kräkel (2011) show that combining risk aversion to limited liability could account for such a positive relationship. Likewise, Wright (2004) demonstrates that when one accounts for heterogeneous managers differing in their degrees of risk aversion, both negative and positive relationships are plausible. In this paper, we present a controlled laboratory experiment that tests the relationship between incentive intensity and risk while isolating any alternative explanations. Testing this relationship in the lab has two significant advantages: first, it provides enhanced control, which allows implementing precise values of the parameters of the model (Charness and Kuhn 2010; Camerer and Weber 2013); second, it allows ruling out alternative explanations (such as the ones discussed earlier) of why the relationship observed using field data is weak or non-existent (Charness and Kuhn 2010; Corgnet and Hernán-González 2018). To the best of our knowledge, this is only the second experimental study that tests the relationship between incentive intensity and risk. Very recently, Corgnet and Hernán-González (2018), henceforth C-HG, report an experiment that independently tests the trade-off between risk and incentives. Our experimental design shares various similarities and some crucial divergences with that of C-HG. Similar to them, we conduct between-subject treatments, in which output is either deterministic (baseline treatment) or stochastic (risk treatments). In one of our risk treatments, as in C-HG, we insure the principal towards the stochastic element to ensure his risk neutrality. However, as a robustness test, we additionally conduct another risk treatment, where the principal is not insured towards risk, allowing for both the principal and the agent to be risk averse. Furthermore, like C-HG, we have collected data on risk aversion for both principals and agents using the Holt and Laury (2002) task. However, whereas C-HG elicits the beliefs of principals on agents’ risk aversion, we rely on social projection (Robbins and Krueger 2005). The key difference between our designs and that of C-HGs, however, is the way the agents exert effort in a principal–agent setting. In both the studies, the principal offers a linear contract to the agent that consists of a fixed wage and a share on agent’s production. However, whereas C-HG employs a real-effort summation task to simulate the agent’s effort choice, we follow the tradition in the gift exchange literature (Fehr et al. 1998; Anderhub et al. 2002; Fehr and Gächter 2002; Fehr and Schmidt 2004; Fehr et al. 2007) in which the agent states/chooses an effort level subject to a (convex) cost of effort function. As with every design choice, there are certain advantages and disadvantages of using a stated-effort vs. a real-effort task.Footnote 1 A real-effort task is a more intuitive approach closer to the field. However, an underpinning assumption of the Incentive Intensity Principle (henceforth IIP) is that principals (and agents) are aware of the precise cost of the effort function that the agent is subjected to. Using a real-effort task introduces heterogeneity in the cost of effort and requires the principal to rely on his beliefs regarding the agent’s ability to complete the task. In essence, introducing a real-effort task transforms the moral hazard problem into a combined problem of moral hazard and adverse selection. As shown in C-HG, this may not affect the direction of the predictions of the IIP; but it complicates the analysis of the contract of the principal. In addition, in the presence of asymmetric information regarding the agent’s ability, the principal’s optimal contract does not implement the efficient level of effort even in the absence of noise. In addition, also noted by C-HG, it could be argued that the baseline treatment is not strictly deterministic, since the agents may find the solution to the task by luck, and/or the agents may not know their ability of the task with certainty. Using a stated-effort rather than a real-effort task eliminates these informational issues and brings the experiment closer to theory, though it does not come without limitations. As shown in Charness et al. (2004), stated effort in gift exchange games is sensitive to the way that payoffs are presented to the subjects. In particular, they show that providing a payoff table reduces average wages by 19% and discretionary effort by 69%. Even though we do not provide agents with an explicit payoff table, we provided calculators that allowed estimating the potential profits for given effort levels and stochastic random factor outcomes. Nevertheless, since we are interested in treatment effects, we expect such issues not to affect our results. According to the linear agency model, the optimal effort choice of the agent depends on the marginal cost of effort and is unrelated to the noise in the performance measure. Sloof and van Praag (2008) test this experimentally and compare their results with expectancy theory, a theory developed by psychologists that predicts a negative relationship between effort and noise in the performance measure. In contrast to the current study that focuses on the optimal choice of incentive intensity, Sloof and van Praag (2008) focus on the optimal effort choice in a real-effort (number adding) task with noise. Due to their divergent aim and to reduce complexity, they abstract away from the role of a principal and the subjects had to allocate effort between two different tasks. Their findings are in line with the linear agency model, as their results suggest that effort levels are invariant to the distribution of noise terms. Overall, our results are broadly in support of the theoretical predictions of the IIP. We find a negative relationship between incentive intensity and risk, in line with theoretical predictions and the findings of C-HG. The principals, on average, offer lower piece rates and higher fixed wages in the risk treatments than in the no risk treatment. However, in contrast to both C-HG and Sloof and van Praag (2008, 2010) who observe an increase in effort with higher risk; we find, again in line with the predictions of Hart and Holmström (1986), that agents respond with lower effort levels when the performance measure is noisier. The most plausible explanation for this difference in effort responses could be the fact that we used a stated-effort approach rather than a real-effort task. Finally, in line with C-HG and Sloof and van Praag (2008, 2010), we find that the effort choices of the employees are not affected by the volatility in the performance measure in contrast to the expectancy theory of motivation (Vroom 1964). The remainder of this paper is organized as follows: Sect. 2 presents the theoretical framework under the parametric restrictions in the experiment, and Sect. 3 describes the specifics of the experimental design. Section 4 presents the results and Sect. 5 concludes. The instructions as well as proofs of the theoretical predictions are provided in the “Appendix”.",5
88.0,2.0,Theory and Decision,23 October 2019,https://link.springer.com/article/10.1007/s11238-019-09726-3,A note on limit results for the Penrose–Banzhaf index,March 2020,Sascha Kurz,,,,Unknown,Unknown,Mix,,
88.0,2.0,Theory and Decision,21 October 2019,https://link.springer.com/article/10.1007/s11238-019-09724-5,Probabilities of electoral outcomes: from three-candidate to four-candidate elections,March 2020,Abdelhalim El Ouafdi,Dominique Lepelley,Hatem Smaoui,Unknown,,Male,Mix,,
88.0,2.0,Theory and Decision,04 November 2019,https://link.springer.com/article/10.1007/s11238-019-09725-4,Ambiguity and price competition,March 2020,R. R. Routledge,R. A. Edwards,,Unknown,Unknown,Unknown,Unknown,,
88.0,2.0,Theory and Decision,12 October 2019,https://link.springer.com/article/10.1007/s11238-019-09723-6,Stochastic choice over menus,March 2020,Pedram Heydari,,,Unknown,Unknown,Unknown,Unknown,,
88.0,2.0,Theory and Decision,14 October 2019,https://link.springer.com/article/10.1007/s11238-019-09722-7,A formal framework for deliberated judgment,March 2020,Olivier Cailloux,Yves Meinard,,Male,Male,Unknown,Male,"Introducing their “reason-based theory of choice”, Dietrich and List (2013) noticed that although the philosophical literature has largely illustrated the usefulness of the concepts of reasons and arguments to think through action and decisions, decision theory strives to account for the latter exclusively in terms of preferences and beliefs. Despite Dietrich and List’s (2013, 2016) efforts, the gap remains large between philosophical and choice theoretic approaches. This gap echoes a classical dichotomy in “moral sciences” between, on the one hand, first-person justifications of one’s acts in terms of reasons and arguments structuring these reasons, and on the other hand, third-person representations in terms of beliefs and preferences (Hausman 2011). By neglecting reason-based and other argumentative accounts, decision theory tends to devalue decision-makers’ understanding of their own actions. This gap has tended to insulate decision theory from important philosophical debates in the past 30–40 years. Among the most influential approaches in these debates, Scanlon (2000) highlighted the links between reasons, justification and moral notions such as fairness and responsibility, Habermas’ (1981) “theory of communicative action” articulated the importance of justification and argumentation as distinctive features of rational action, and Rawls (2005) launched the debates on the “acceptability” (Estlund 2009) of reasons and arguments for public justification. This gap also has important practical implications for decision analysis, by complicating the task for analysts to explain the recommendations they give to their clients. This, in turn, casts doubts on these recommendations, which appear to be imposed to rather than endorsed by decision-makers. In this article, we aim to participate in unlocking this situation, by elaborating a framework designed to allow decision analysts to provide recommendations that decision-makers truly endorse, in empirical reality. For that purpose, we introduce, as a commendable basis for recommendation, the “deliberated judgments” of the decision-maker. Roughly stated, these “deliberated judgments” represent the propositions that the decision-maker will consider to be well grounded, if he duly takes into account all the relevant arguments. This concept is inspired by Goodman’s (1983) and Rawls’ (1999) notion of reflective equilibrium. It also owes much to Roy’s (1996) view that an important part of the decision support interaction consists, for the analyst, in ensuring that the aided individual understands and accepts the reasoning on which the prescription is based. This article is organized as follows. In Sect. 2, we define our core concepts, including the central concept of deliberated judgments. In Sect. 3, we then explore the issue of how empirical data come into play and are involved in the validation of models. This illustrates the empirical aspect of our framework, which distinguishes it from standard prescriptive approaches. Obviously enough, at this stage, the pivotal issue is to determine how one can say anything about “deliberated judgments”, given that, for any non-trivial decision, the potentially relevant arguments are infinitely numerous. Lastly, Sect. 4 discusses the significance of our approach for the practice of decision analysis and outlines future empirical applications.",5
88.0,2.0,Theory and Decision,23 August 2019,https://link.springer.com/article/10.1007/s11238-019-09718-3,Expected discounted utility,March 2020,Pavlo Blavatskyy,,,Male,Unknown,Unknown,Male,"There is a nonempty finite set T that is called time. The elements of set T are called moments of time. The subsets of set T are called periods of time. There is a sigma-algebra \( \sum \) of the subsets of T. There is a nonempty connected set X. The elements of X are called outcomes. Choice options are streams of intertemporal outcomes that describe which outcomes happen at every moment of time. Formally, a stream of intertemporal outcomes f:T → X is a \( \sum \)-measurable function from T to X. The set of all available streams is denoted by \({\mathcal{F}} \). For a convenient notation, let \( xPf \in {\mathcal{F}} \) denote a stream that yields an outcome \( x \in X \) in all moments of time \( t \in P \) within some time period \( P \in \sum \), and outcome f(t)—in all other moments \( t \in \)T\P. A decision-maker could be an individual or a group of individuals. A decision-maker has a preference relation ≽ on \( {\mathcal{F}} \). The symmetric part of ≽ is denoted by ~ and the asymmetric part of ≽ is denoted by ≻. The preference relation ≽ is represented by utility function \( U:{\mathcal{F}} \to {\mathbb{R}} \) if f ≽ g implies U(f) ≥ U(g) and vice versa for all streams f, \( g \in {\mathcal{F}} \). A time period \( P \in \sum \) is null (or inessential) if a decision-maker is indifferent between all streams that yield the same outcomes in all moments of time that do not belong to period P.Footnote 3 Otherwise, a time period is nonnull (or essential). If there is only one nonnull time period, we additionally assume that X has a countable order-dense subset. This assumption is needed for the existence of utility function for the static case of one time period (cf. Debreu 1954, lemma II, p. 161; Krantz et al. 1971, section 2.1, theorem 2, p. 40). (Completeness) For all f, \( g \in {\mathcal{F}} \) either f ≽ g or g ≽ f (or both). (Transitivity) For all f, g, \( h \in {\mathcal{F}} \) if f ≽ g and g ≽ h then f ≽ h. Consider a decision-maker who finds one stream g to be in between the other two streams xPf and yPf in terms of desirability (xPf ≽ yPf). The latter two streams differ only in the outcomes, which they yield in period P. Lowering the more desirable outcome (x) and/or improving the less desirable outcome (y) brings two streams xPf and yPf closer and closer to each other in terms of desirability. If preferences were continuous, eventually there would be a point where one of these modified streams is exactly as good as g. Our next behavioural axiom directly states this implication of continuity so that there is no need to invoke the continuity axiom: whenever g lies in between xPf and yPf in terms of desirability we can always modify outcome x (or y) so that the decision-maker finds the modified stream to be exactly as good as g. This is known as the solvability axiom or restricted solvability (e.g. Krantz et al. 1971, p. 301, definition 12) in the so-called algebraic approach to an additively separable utility. (Solvability) For all outcomes \( x,y \in X \), time periods \( P \in \sum \), functions f:T\P → X and streams \( g \in {\mathcal{F}} \) such that xPf ≽ g ≽ yPf, there exists an outcome \( z \in X \) such that g ~ zPf. Consider a sequence of outcomes \( \{ x_{i} \}_{{i \in {\mathbb{N}}}} \) such that xiPg ~ xi−1Pf for some nonnull time period \( P \in \sum \). It is known as the standard sequence of outcomes that are equally spaced in terms of desirability (e.g. Krantz et al. 1971, p. 253, definition 4). Its outcomes can be either increasing (when x0Pf ≻ x0Pg) or decreasing (when x0Pg ≻ x0Pf). If its outcomes are equally spaced in terms of desirability, then there could be only a finite number of streams xiPf that lie in between any two given streams xPf and zPf in terms of desirability. This is known as the Archimedean axiom—another consequence of continuity of preferences (e.g. Krantz et al. 1971, pp. 309–310, proof of theorem 14). Similarly, as in the case of the solvability axiom, we assume the Archimedean property directly as Axiom 4 so that there is no need to invoke the continuity axiom. (Archimedean Axiom) For every \( x_{0} \in X \), nonnull \( P \in \sum \), and f, g :T\P → X such that either x0Pf ≻ x0Pg or x0Pg ≻ x0Pf, a sequence of outcomes \( \{ x_{i} \}_{{i \in {\mathbb{N}}}} \) recursively defined by xiPg ~ xi-1Pf is finite if there exist \( x,y \in X \) such that yPf ≽ xiPf ≽ xPf for every element of this sequence \( x_{i} \in X \). Next, let us consider a situation when xPf ≽ yPg and xPg ≽ zPf for some nonnull time period \( P \in \sum \). One interpretation of these preferences could be as follows. A decision-maker weakly prefers stream xPf to stream yPg but replacing outcome x with outcome z and replacing outcome y with outcome x reverses this preference. If the same decision-maker also weakly prefers stream yRh to stream xRk, then replacing outcome x with outcome z and replacing outcome y with outcome x should moreover reinforce the preference of a decision-maker (unless there are some substitution/complementarity effects between outcomes that are received in different time periods). This axiom is known as cardinal independence (e.g. Blavatskyy 2013) or standard sequence invariance (e.g. Krantz et al. 1971, section 6.11.2). Axiom 5 is a weaker version of the well-known Wakker (1984, 1989) tradeoff consistency condition. (Cardinal independence) If xPf ≽ yPg, xPg ≽ zPf, and yRh ≽ xRk then xRh ≽ zRk for all x, y, \( z \in X \); f, g:T\P → X; h, k:T\R → X, any nonnull time period \( P \in \sum \) and any time period \( R \in \sum \). Axiom 5 in conjunction with previous (standard) Axioms 1–4 implies the better known ordinal independence: xPf ≽ xPg implies yPf ≽ yPg for any two outcomes \( x,y \in X \); any f,g:T\P → X and any time period \( P \in \sum \). This technical result is proven as Lemma 1 in the “Appendix” (cf. Blavatskyy 2013, lemma 4). Therefore, given standard Axioms 1–4, instead of directly assuming ordinal independence, as it was traditionally done in the literature, it is sufficient only to assume cardinal independence (Axiom 5 above). A preference relation ≽ satisfies Axioms 1–5 if and only if the utility of a stream of intertemporal outcomes that yields outcome f(t) in moments of time \( t \in T \) can be written in an additively separable form: where D(t) ≥ 0 for all \( t \in T \) and function \( u:X \to {\mathbb{R}} \) is continuous. Function \( u:X \to {\mathbb{R}} \) is unique up to a positive affine transformation if at least two time periods are nonnull. The proof is presented in the “Appendix”.",2
88.0,2.0,Theory and Decision,02 November 2019,https://link.springer.com/article/10.1007/s11238-019-09727-2,The uniqueness of local proper scoring rules: the logarithmic family,March 2020,Jingni Yang,,,Unknown,Unknown,Unknown,Unknown,,
88.0,3.0,Theory and Decision,27 November 2019,https://link.springer.com/article/10.1007/s11238-019-09734-3,Robust winner determination in positional scoring rules with uncertain weights,April 2020,Paolo Viappiani,,,Male,Unknown,Unknown,Male,"Rank aggregation arises in many settings including voting, recommender systems, information retrieval, and sports. Positional scoring rules are often used in these settings: each alternative receives points based on its position on each of the input rankings and the alternative with the highest total score is deemed the winner. Scoring rules offer the advantage of being very simple to implement. Moreover, scoring rules satisfy several desirable properties (Young 1975) including monotonicity, consistency, and participation (two important properties that are not satisfied by scoring rules are Condorcet consistency and clone-proofness). In scoring rules, a vector of weights, called scoring vector, determines the amount of points attributed to the first rank, to the second rank, etc. These weights, typically monotone decreasing, can encode preferences for controlling the tradeoff between “extreme” alternatives (often in the first positions and in the last positions), and “moderate” ones (alternatives that are most of the times in the middle positions). A particular scoring rule is Borda count that uses linear weights; while it satisfies some additional properties (Young 1974; Fishburn and Gehrlein 1976) and sometimes presented as superior to other scoring rules, Borda may not be adequate in decision contexts where greater discrimination between the positions is needed. For instance, in many contexts, in particular in sport competitions, it is often assumed that weights should constitute a convex sequence (Llamazares 2016), meaning that the difference between the weight of the first position and that of the second position is at least as large as the difference between the weight of the second and the third position, and so on. Setting the weights of a scoring rule is a critical task; indeed, it is often the case that different winners may emerge when using different scoring vectors. Since fixing the scoring vector can be seen as arbitrary choice, Cook and Kress (1990) suggested evaluating each alternative according to its most favorable scoring vector, in order to avoid any subjectivity. Several authors (Cook and Kress 1990; Green et al. 1996; Hashimoto 1997; Foroughi and Tamiz 2005; Llamazares and Peña 2009, 2013; Khodabakhshi and Aryavash 2015; Llamazares 2016) have then proposed similar approaches to score alternatives by considering the space of feasible weights. In this work we view the rank aggregation problem as a decision problem under uncertainty. The uncertainty over the scoring rule is based on an explicit representation of the possible scoring vectors; this is a setting of strict uncertainty (French 1986). There are several possible criteria that can be used for decision-making under strict uncertainty; the most common are: maximax, maximin, Hurwitz’s, and minimax regret; the reader is referred to French (1986) for a critical review. In fact, some of previous approaches to rank aggregation can be seen from this perspective: Cook and Kress (1990) adopt the maximax criterion while the criterion chosen by Khodabakhshi and Aryavash (2015) is closely related to Hurwicz’s criterion. While there is no definite consensus on which criterion is best, we think that evaluating each alternative according to its most favorable scoring vector, as done in several of the papers cited above, is often too much “optimistic”. As an alternative may perform very well with some scoring vectors, but may have dismal performance according to other scoring vectors, we think that is important to provide some form of robustness. Therefore, we adopt minimax regret (Savage 1954) as a criterion for choosing the alternative to be declared as winner; with this robust criterion, we are able to give guarantees on the quality of the decision. More precisely, we assume that a central authority postulates some basic requirements about the scoring vector; in particular we focus on two settings: (1) the set of non-increasing weights and (2) the set of convex non-increasing weights. Each alternative is associated to a max regret value, corresponding to the worst-case loss (in term of score points) that we could incur if we were to choose that alternative. The alternative (or the alternatives) to be declared as “winner” is the one that minimizes such loss, i.e. achieving minimax regret. In this paper we provide a thorough analysis of winner determination with minimax regret. The main contributions of the paper consist in the characterization of regret in the two main settings (non-increasing weights, non-increasing convex weights), providing closed-form expressions for computing max regret (in Theorems 1 and 2), in analyzing minimax regret as a social choice function (in Theorem 3), and in providing experimental tests. We also discuss the case in which additional preference information (imposing a minimal value for difference between the value of two weights) is known about the weights. The paperFootnote 1 is organized as follows. First, in Sect. 2 we provide relevant background, covering scoring rules with uncertain weights, dominance relations between alternatives, and the identification of possible winners. Then, in Sect. 3 we present the aggregation with minimax regret and we provide a novel characterization under different hypothesis about the feasible scoring vectors. In Sect. 4 we present a formal analysis of minimax regret as a social choice function. Finally in Sect. 5 we describe some tests showing, with real data, the use of the proposed methods for rank aggregation. Section 6 provides some concluding remarks.",
88.0,3.0,Theory and Decision,14 November 2019,https://link.springer.com/article/10.1007/s11238-019-09732-5,The premium as informational cue in insurance decision making,April 2020,Robin Chark,Vincent Mak,A. V. Muthukrishnan,,Male,Unknown,Mix,,
88.0,3.0,Theory and Decision,14 November 2019,https://link.springer.com/article/10.1007/s11238-019-09731-6,That’s the ticket: explicit lottery randomisation and learning in Tullock contests,April 2020,Subhasish M. Chowdhury,Anwesha Mukherjee,Theodore L. Turocy,Unknown,Unknown,Male,Male,"“The hero ... is condemned because he doesn’t play the game. [...] But to get a more accurate picture of his character, [...] you must ask yourself in what way (the hero) doesn’t play the game”.    — Albert Camus, in the afterword of The Outsider (Camus 1982, p. 118) There is an active literature studying contest games using laboratory experiments (Dechenaux et al. 2015). The workhorse game in these experiments is based on the model of Tullock (1980). Under the Tullock contest success function (CSF), the ratio of the chances of victory of any two players is given by the ratio of their bids, raised to a given exponent. Most experimental studies using the Tullock CSF set the exponent to one, and find bids in the contests exceed the levels predicted by the Nash equilibrium for risk-neutral contestants, both initially and after repeated experience with the game.Footnote 1 In this paper, we explore how bid levels and dynamics depend on implementation details of the contest. The most common situation in which a typical person encounters an institution described by the Tullock CSF is in the raffle. Raffles are common as fund-raisers for schools, clubs and charities. In the typical implementation of a raffle, people purchase individually numbered tickets, which are collected in a container of some sort.Footnote 2 A ticket is drawn and the number announced or publicised, which determines the winner of the prize.Footnote 3 We design a treatment in which we describe the game in the instructions, and carry it out in the implementation, using the common terminology associated with the raffle. Participants purchase individually numbered (virtual) tickets, and one ticket number is drawn and quoted to determine the winner. We compare the performance of this contest to a baseline implementation using a protocol we designed to be typical of previous studies. We find that in the raffle, initial bids are significantly lower, and adjustments towards (risk-neutral) best responses are faster, with median group bids approaching the risk-neutral prediction. We replicate our results in two participant pools in two countries, at two universities with student bodies with contrasting profiles. As a starting point, we updated the list of Tullock contest studies initiated by Sheremeta (2013). The 37 papers are listed in Table 1. Since the value of the contest prize differs across studies, we follow Sheremeta and report a normalised “overbidding rate,” computed as (actual bid − Nash bid)/Nash bid. We augmented the survey by collecting information on the experimental protocols used for presenting the game.Footnote 4 Whether the instructions refer to a ratio of bids mapping into probabilities of winning the prize. In expressing theoretical models, the Tullock CSF is commonly written in ratio form. This custom carries over to the writing of experimental instructions. Experiments which follow this custom, by making explicit mention that the probability of winning is given by the ratio of the player’s own bid to the total bids of all players, are indicated in the “Ratio rule” column in Table 1. A majority of studies do present this ratio, with many, including Fallucchi et al. (2013), Ke et al. (2013), and Lim et al. (2014) explicitly using a displayed mathematical formula similar to (1).Footnote 5 What concrete randomisation mechanism, if any, is mentioned in the instructions. Many experiments supplement the mention of a ratio with an example mechanism capable of generating the probability. For example, instructions may state that it is “as if” bids translate into lottery tickets, or other objects such as balls or tokens (Potters et al. 1998; Fonseca 2009; Masiliunas et al. 2014; Godoy et al. 2015), which are then placed together in a container, with one drawn at random to determine the winner. The “Example” column of Table 1 lists the example given to participants in each study. How the realisation of the random outcome is communicated to participants. In representing the randomisation itself, experimenters rarely use a pseudo-physical representation of the randomisation process. Among the few that provide a representation, several do so using a spinning lottery wheel, on which bids are mapped proportionally onto wedges of a circle, which is a ratio-format representation. Among this limited sample, there is no difference in reported bidding relative to Nash compared to the body of the literature as a whole. Herrmann and Orzen (2008) report bids more than double the equilibrium prediction; Morgan et al. (2012) and Ke et al. (2013) around 1.5 times the equilibrium prediction; and Shupp et al. (2013) find bids below the equilibrium prediction. Based on the survey, the emerging standard of recent years, which we refer to as the conventional approach, is for a Tullock contest experiment to introduce the success function in terms of a probability or ratio; give a mechanism like a raffle as an example of how the mechanism works; but not to represent the mechanism of randomisation, beyond identifying the winner. The standardisation is reflective of the maturity of the literature on experimental contests. A design with features parallel to other recent papers is more readily comparable to existing results. However, by not exploring the design space of other implementations, this standardisation leaves the literature unable to distinguish whether common results, such as bids in excess of the risk-neutral Nash prediction, are behaviourally inherent to all Tullock contests, or depend on features of the environment which are not accounted for in standard models. Understanding how behaviour might be a function of implementation features is particularly relevant for Tullock CSFs because of their wide applicability, which includes not only raffles but rent-seeking, electoral competition, research and development, and sports. In the latter applications, the Tullock CSF is used as a theoretical device to represent an environment in which success in the contest depends significantly both on the bids of the players and on luck, and is attractive for such applications because it is analytically tractable. For example, in the symmetric setting with risk-neutral players and no spillovers, there is a unique Nash equilibrium in which the players play pure strategies (Szidarovszky and Okuguchi 1997; Chowdhury and Sheremeta 2011). We chose the raffle implementation not only because of its ubiquity in common experience, but also because existing results suggest consistently presenting and implementing the contest as a raffle would maximise the differentiation from the conventional design. Raffle tickets are inherently count-based representations. Differences in processing probabilistic (ratio) versus frequency (count) information have been studied extensively in psychology. For example, Gigerenzer and Hoffrage (1995) proposed that humans are well adapted to manipulating frequency-based information as this is the format in which information arises in nature. There is, therefore, an unresolved expositional tension in the instructions of many experiments, mixing ratio-based formats in expressing probabilities of winning and/or fortune wheels, and count-based formats of as-if raffles. We present the formal description of the Tullock contest game and our experimental design in Sect. 2. The summary of the data and the results are included in Sect. 3. We conclude in Sect. 4 with further discussion.",5
88.0,3.0,Theory and Decision,07 November 2019,https://link.springer.com/article/10.1007/s11238-019-09728-1,Does time inconsistency differ between gain and loss? An intra-personal comparison using a non-parametric elicitation method,April 2020,Shotaro Shiba,Kazumi Shimizu,,Male,,Unknown,Mix,,
88.0,3.0,Theory and Decision,03 November 2019,https://link.springer.com/article/10.1007/s11238-019-09729-0,Resolving some contradictions in the theory of linear opinion pools,April 2020,A. Philip Dawid,Julia Mortera,,Unknown,Female,Unknown,Female,"Suppose n different experts (who share my world view, and in particular my prior distribution, though each may have more information than I have) announce their posterior probabilities, \(\varPi _1,\ldots ,\varPi _n\), for an uncertain event A. What should be my own probability \(\varPi \) for A after hearing this information? Any coherent combination formula should be computable by Bayesian conditioning, as for some joint distribution P compatible with my joint distribution for \((\varPi _1,\ldots ,\varPi _n)\). The problem of characterising such coherent formulae \(\varPhi \) was considered in detail by Dawid et al. (1995). Particular attention was focused on the case \(n=2\) and the linear opinion pool, of the form: where \(\pi _0\) is my (and the experts’) prior probability, P(A). Theoretical results include the property \(\alpha _1+\alpha _2+\alpha _0=1\), and that not all the \(\alpha \)s can be positive. Several examples were given of non-trivial cases where (3) is a coherent combination formula. In particular, the construction in Example 1 of Dawid et al. (1995), involving conditionally binomial variables, shows that it is coherent to express where \(X_i\) can take any integer value in \(\{0,\ldots ,n_i\}\) (\(i=1,2\)), and \(a,b>0\). Furthermore, \(\varPi _1\) and \(\varPi _2\) are logically independent: in particular, it is not the case that \(\varPi _1 = \varPi _2\). It follows from (4)–(7) that (3) holds, with Results claimed by Bradley (2018) would seem to deny the existence of the above example. In this note, we identify the sources of this apparent contradiction. Section 2 shows that a first result of Bradley (2018), while formally correct, requires a strong assumption that will often be inapplicable (and is so in the above example). Section 3 identifies a mathematical error in Bradley (2018) that invalidates his more general claim.",4
88.0,3.0,Theory and Decision,12 November 2019,https://link.springer.com/article/10.1007/s11238-019-09730-7,Topologies for semicontinuous Richter–Peleg multi-utilities,April 2020,Gianni Bosi,Asier Estevan,Armajac Raventós-Pujol,Male,Male,Unknown,Male,"In the present paper, we study the existence of lower semicontinuous Richter–Peleg multi-utilities for preorders on topological spaces. The existence of Richter–Peleg multi-utilities has been recently studied by Alcantud et al. (2016) (see the introduction in this paper to find the motivations for adopting such a representation). It was already observed that this kind of representation not always exists for preorders endowed with the Upper topology\(\tau _\mathrm{u}\) (see Theorem 3.1 in Alcantud et al. 2016). On the other hand, it is well known that the weak lower contour sets of the preorder have to be closed in the more general case when there exists a lower semicontinuous multi-utility (see, e.g., Proposition 2.1 in Bosi and Herden 2016). Negative conditions for the existence of finite (Richter–Peleg) continuous multi-utility representations were presented in Alcantud et al. (2016) and Kaminski (2007). The goal of this paper is to identify some other topologies related to the preorder, with respect to which it is possible to characterize the lower semicontinuous Richter–Peleg multi-utility representability. Therefore, these topologies have to be finer than the Upper topology. Scott topologies have been used in computing to characterize the functions between lattices (in particular, dcpo-s) that preserve suprema of directed sets (Davey and Priestley 2002; Gierz et al. 2003). In any case, the present paper studies the more general case of preorders and their finite lower semicontinuous Richter–Peleg multi-utilities, so we do not assume the existence of suprema and we search for a family of functions that fully characterize the order structure (i.e., a multi-utility instead of a single utility function). In this line, we prove that if there exists a finite lower semicontinuous Richter–Peleg multi-utility for a given preorder, then the topology of the space refines the Scott topology. Thus, we achieve a significant necessary condition for the existence of the desired representation: from now, if we search for finite lower semicontinuous Richter–Peleg multi-utilities, we should start from a topological space that refines the Scott topology, and not the Upper. Furthermore, we also present an example to show that this necessary condition is not sufficient for the general case. Hence, we continue in the study of the adequate topologies to guarantee the existence of the lower semicontinuous Richter–Peleg multi-utility. For that, we prove that there always exists this kind of representation when the preorder is endowed with a topology than is finer that the Alexandroff topology. Thus, we achieved a sufficient condition. For this purpose, order and topology are interacted, and we prove that more order implies less Alexandroff’s topology, as well as the converse. Throughout the paper, we also focus on some other results related to the topic to interact with our present results. For example, several authors have work under the hypothesis in which any linear extension of the preorder is lower semicontinuous (see, for example, Mashburn 1995 on pliable spaces). From a topological point of view, this is strongly related to the Alexandroff topology. Therefore, this kind of topologies cannot be considered strange at all. Moreover, these kinds of topologies are quasimetrizable (see Romaguera and Valero 2010, for instance) and that could be an interesting tool to complement the lack of a metric. Finally, a topological study of maximal elements is included. The structure of the paper goes as follows. Section 2 contains the notation and the preliminaries. Section 3 presents necessary conditions for the existence of a (finite) lower semicontinuous multi-utility representation of a preorder. Section 4 is devoted to the sufficient conditions for the existence of such representations of preorders. Finally, in Sect. 5, a topological study of maximal elements is given. Some final comments and conclusions end the paper.",4
88.0,4.0,Theory and Decision,17 December 2019,https://link.springer.com/article/10.1007/s11238-019-09739-y,On the existence and stability of equilibria in N-firm Cournot–Bertrand oligopolies,May 2020,Anne-Christine Barthel,Eric Hoffmann,,Unknown,Male,Unknown,Male,"Competition between firms and the stability of market equilibria have been some of the most extensively studied topics in economics, starting with Cournot (1838), who considered firms who compete by choosing the amount of product to produce, and Bertrand (1883), who considered firms who compete by choosing which price to set. Modern treatments of these issues (see for example, Amir 1996; Amir and Evstigneev 2018; Vives 2001; Milgrom and Shannon 1994) have focused on the consequences of the monotonicity present in such games, in the sense that Cournot competition can be viewed as a game of strategic substitutes (GSS), where firms best respond to a higher output choice by competitors by choosing a lower quantity, while Bertrand competition can be viewed as a game of strategic complements (GSC), where firms best respond to a higher price set by competitors by choosing to set a higher price themselves. This paper exploits the fact that through a simple transformation, markets consisting of both price and quantity setting firms can be viewed as games of strategic heterogeneity (GSH), where each firm is either a strategic complements or a strategic substitutes player, and uses this framework to derive new existence, uniqueness, and stability results in this class of games. The case of when markets consist both of quantity competitors and price competitors has received much less attention, despite their prevalence. For example, following Tremblay et al. (2011, 2013), such situations persist in the market for small cars, as well as produce markets, where producers compete in quantity while local stores compete in price. One common theme in the theoretical literature has been that the existence and stability of equilibria crucially depend on the degree of product substitutability between firms. Singh and Vives (1984) give conditions under which firms will choose to compete either entirely as quantity competitors or entirely as price competitors, depending on whether the goods are complements or substitutes. Tremblay et al. (2011) study the duopoly setting, and show in the case of perfectly homogeneous goods, a unique equilibrium exists, which is also the competitive equilibrium. Concerning the stability of equilibria, recent results have focused exclusively on the duopoly case, where once again product substitutability plays a prominent role. Tremblay and Tremblay (2011) and Askar (2014) derive a bound on product substitutability which guarantees that an equilibrium will be stable under continuous time dynamics, while Naimzada and Tramontana (2012) give conditions for stability under discrete time best response dynamics. The approach of this paper is to first show that N-player Cournot–Bertrand games can be viewed as GSH. Notice that while the extreme cases of pure Cournot and pure Bertrand markets are GSH (in specific, a GSS and a GSC, respectively), it is not clear that N-player Cournot–Bertrand markets satisfy this requirement. To see this, consider a price setting firm k. Notice that if all other price competitors set a higher price, firm k will be induced to also set a higher price. However, if quantity competitors set a higher quantity, firm k will be induced to set a lower price. Thus, firm k does not best respond in a monotonic way to increases in the strategies of all opponents, and hence this game fails to be a GSH. Despite this, we show that such games can be “monotonically embedded” into a GSH, so that firms can be seen as best responding in a monotonic way. To the best of the authors’ knowledge, this paper is the first to apply monotonicity analysis to this class of games. With this observation, the main contributions of this paper are the following: By applying recent results in the GSH literature which show that many of the existence and stability properties of equilibria in GSH are preserved under monotonic embeddings, we are able to derive conditions on the product substitutability parameter \(d \in (-1,1)\) which guarantee the existence of a unique, dominance solvable equilibrium which is stable under all adaptive dynamics. These conditions offer distinct advantages over many of the results in the existing literature. Firstly, in the case of a duopoly when goods are substitutes and firms have constant marginal costs, both our results and those of Naimzada and Tramontana (2012) guarantee the stability of a unique equilibrium as long as the degree of product substutability is such that \(d<\frac{2}{\sqrt{5}}\). However, while Naimzada and Tramontana (2012) are able to guarantee stability under simple best-reply dynamics, we are able to guarantee a unique, dominance solvable equilibrium which is stable under all adaptive dynamics. Recall from Milgrom and Shannon (1994) that a learning process is adaptive as long as agents eventually begin to respond to past play by choosing undominated actions. Hence, adaptive dynamics consist of a very general and broad range of learning rules, which include best-reply dynamics as a simple case. Secondly, we allow for non-linear cost structures in the case of two firms, and show that if costs are sufficiently convex, then uniqueness and stability are always guaranteed. We further extend the literature on stability in Cournot–Bertrand markets by offering results for the general N-firm case, as well allow for product complementarities. Finally, we study how our condition changes as either an additional quantity or price competitor enters the market. We find that, in general, the addition of a price competitor promotes stability in the case of substitute products, while the addition of a quantity competitor has the opposite effect for both substitutes and complements. Formulating Cournot–Bertrand markets as monotone games offers several insights and advantages. As evidenced by Example 3 in Barthel and Hoffmann (2019), GSH with at least one strategic substitutes player, which we show to include Cournot–Bertrand markets, may fail to have a stable or a dominance solvable equilibrium, even if it the equilibrium is unique. Hence, methods such as the contraction mapping theorem or the conditions of Bramoullé et al. (2014), all of which guarantee the existence of a unique equilibrium, are not enough to address stability or dominance solvability. However, the monotonicity inherent in such markets can be exploited to construct a best-reply sequence which, when convergent, guarantees the existence of a unique, globally stable equilibrium which is stable under all adaptive dynamics. This paper is organized as follows: Sect. 2 provides the model description as well as the relevant notions concerning monotone games. Section 3 contains our first result for the case of a Cournot–Bertrand duopoly, which provides a condition which contains as a special case the condition found in Naimzada and Tramontana (2012). Furthermore, we show that this condition not only implies the stability of an equilibrium under best response dynamics, but that it also implies stability under all adaptive dynamics, as well as guarantees the dominance solvability of such equilibria. Section 4 introduces the notion of a monotone embedding which allows us to extend this methodology to the case of N-firms. Finally, we study how this condition changes as more firms of each type enter the market. Section 5 concludes.",4
88.0,4.0,Theory and Decision,02 December 2019,https://link.springer.com/article/10.1007/s11238-019-09733-4,On the \(\gamma \)-core of asymmetric aggregative games,May 2020,Giorgos Stamatopoulos,,,Unknown,Unknown,Unknown,Unknown,,
88.0,4.0,Theory and Decision,13 January 2020,https://link.springer.com/article/10.1007/s11238-019-09741-4,On the instability of majority decision-making: testing the implications of the ‘chaos theorems’ in a laboratory experiment,May 2020,Jan Sauermann,,,Male,Unknown,Unknown,Male,"For a long time, questions concerning the predictability of majority rule have inspired discussions about the viability and meaning of democratic decision-making. Already Condorcet (1785/1995) noted that transitive individual preferences can lead to intransitive social preference orders. In the twentieth century, several well-known social choice theorems, most notably Arrow’s (1963) Impossibility Theorem, formalized and extended Condorcet’s theoretical insight.Footnote 1 For instance, Black (1948) showed that the outcome of majority decision-making equals the median voter’s preference if all choice alternatives fall into a one-dimensional policy space and voters’ preferences are single-peaked.Footnote 2 The most frequently used equilibrium concept for higher-dimensional policy spaces is that of the majority core. An alternative x is in the majority core if there is no other alternative y in the policy space such that a majority of voters prefer y to x. However, preferences have to fulfill strict symmetry conditions so that a core exists (Davis et al. 1972; Plott 1967). Under simple majority rule, an alternative in the core is a median in all directions. Every line drawn through it divides the ideal points of all voters so that at least half are on either side of the line (Hinich and Munger 1997, 65). Hence, existing equilibria are highly fragile and vulnerable to minor changes of voters’ preferences. Consequently, the core is empty for most decisions under majority rule. The theoretical implications of an empty core are far-reaching. The so-called ‘chaos theorems’ by McKelvey (1976, 1979) and Schofield (1978) proof that when transitivity of collective preferences breaks down, it breaks down completely. A path consisting of a finite number of majority decisions connects any two points in the policy space. Hence, any point in the policy space can be reached given the appropriate sequence of voting. The McKelvey–Schofield theorems have several implications. For instance, McKelvey (1976) highlights the resulting importance of agenda-setting power, i.e. the right to make proposals in the voting decisions. If an individual voter controls the agenda, she can influence decision-making to her own benefit and propose a voting sequence that eventually leads to the selection of her own ideal point (see Cox and Shepsle 2007). In this paper, I focus on the theorems’ empirical implications for the instability of majority decision-making. I define majority rule instability as the extent to which groups with fixed voters’ preferences choose different outcomes over time. The theorems demonstrate the theoretical indeterminacy of the outcomes of majority decision-making. However, as Austin-Smith and Banks (1999, 184) point out, indeterminacy does not necessarily imply instability: […] it is important to emphasize that these instability and chaos theorems are results of the consistency of the various means of aggregating individual preferences. As such they are not results on individual behavior or the aggregation of such behavior, they are facts about the formal properties of preference aggregation rules on given sets of profiles. In particular, the results do not predict that political behavior is chaotic or that “anything can happen”. Consequently, it remains an open question, whether intransitive collective preferences lead to unstable majority decisions or not. Riker (1982) offers the most far-reaching interpretation of the implications of the theoretical indeterminacy of majority decision-making. He is a strong proponent of a position of democratic irrationalism and argues that the indeterminacy of majority rule leads to voting cycles which make democratic decisions arbitrary and meaningless. When the core is empty “wide swings in political choices are possible and expected” (1982, 188). In addition, Riker adds a second aspect to his argument claiming that the instability of majority decision-making is also driven by the dissatisfaction of the losers under the current status quo policy. If voters’ preferences differ sharply and conflict among group members is high, the dissatisfaction of outvoted actors is also high. Thus, the higher the dissatisfaction with the status quo, the stronger becomes current losers’ motives to upset the current outcome (cf. Riker 1982, 208). Therefore, when the core is empty, majority instability correlates with the level of conflict among actors. In this study, I test both aspects of Riker’s claim of the irrationality of democratic decisions in a comprehensive experimental framework. Only few existing studies examine the effects of the existence of a core in majority decisions and mostly find that an empty core does not lead to “chaotic” collective decisions. The influence of conflict on the stability of democratic decisions, however, has never been explored systematically. The experimental results thus add an important new insight to the collective decision-making literature by showing that the behavioral effects of the level of conflict in a decision dominate the effect of the existence of a core. The remainder of the paper is structured as follows. The following section reviews the existing empirical evidence on the instability of majority rule. Section 3 explains the experimental design, Sect. 4 presents the results, and Sect. 5 concludes.",3
88.0,4.0,Theory and Decision,05 December 2019,https://link.springer.com/article/10.1007/s11238-019-09737-0,On temperance and risk spreading,May 2020,Christophe Courbage,Béatrice Rey,,Male,Female,Unknown,Mix,,
88.0,4.0,Theory and Decision,17 December 2019,https://link.springer.com/article/10.1007/s11238-019-09738-z,Supergrading: how diverse standards can improve collective performance in ranking tasks,May 2020,Michael Morreau,,,Male,Unknown,Unknown,Male,"With a nod to Galton, imagine another competition at a fair. There is a herd of oxen and whoever picks out the heaviest wins. Now, you and I are not farmers. We are not butchers or livestock auctioneers, and cannot hope to pinpoint the weights of the different ones. The best we can do is to say imprecise things such as “this one looks heavy to me”, and “that other one I’d say is light, for an ox”. Do people like us even stand a chance? Suppose your judgments, however, imprecise, are accurate by your standards: when you say an ox is heavy its weight always falls within the range that, as you understand it, is covered by this expression; and if you say light its weight is always compatible with your understanding of that. My own understanding of these expressions is different. You and I draw the line between heavy and light at different places. But, suppose, my judgments too are accurate, taken on their own terms. Then you and I together can divide the oxen into three categories by weight, though each of us uses just the two expressions to describe them. Say for concreteness that you draw the line between heavy and light at 1000 kilos, and I at just 500 kilos. When both of us call an ox heavy it must weigh at least 1000 kilos. If on the other hand we disagree then the weight must be at least 500 but under 1000 kilos, and if we agree that an ox is light then it weighs under 500 kilos. More of us can do even better: three binary graders with pairwise differing standards can discriminate four categories, and so on. So it is that people with little expertise do indeed stand a chance. We can rank the oxen in the correct order and pick the heaviest one—provided there are enough of us, and our interpretations of language are diverse. Think of us as expressing ourselves, collectively, in a richer language than either of us uses separately. When you say an ox is light and I say heavy, you and I together, by that fact, count it light-heavy. We assign a supergrade, the concatenation of individually assigned grades. Our other supergrades are heavy-heavy, at the top, and the lowest grade light-light. This article develops the method of supergrading and explores some of its consequences for collective decision making. Some noteworthy features are already visible. First, supergrades are more precise than the individually assigned grades that make them up. Our supergrades distinguish for example among your light oxen (up to 1000 kilos) the heavier ones that are light-heavy (from 500 up to 1000 kilos) and lighter ones that are light-light (up to 500 kilos). Second, there is no need for a common language of grades. You and I used the same terms heavy and light but we need not have done. Any scores or grades whatsoever make up supergrades, no matter how diverse they are in number or interpretation, provided only that they measure the same dimension, here weight. You could just as well have scored the oxen from 1 to 10, say, or I could have graded them as large, medium or small. This tolerance with respect to the form of inputs is conducive to tapping diverse sources of information and collective intelligence (Surowiecki 2004; Page 2008). Third, to find out the group’s ranking there is no need to know just which grades people might use, or just where they draw the lines between them. What matters is each grader’s top-bottom order among the grades they do in fact use: whether, say, you take that 3 you assigned to be the higher score, or the 5. The top-bottom order of the assigned supergrades is determined by this. Finally, supergrading is not a competitor to accuracy-enhancing aggregation methods such as median grading. It is a precision-enhancing complement to them. You and I might as well be two halves of a group: everyone in one half has your understanding of the grades, and everyone in the other shares mine. Now each half aggregates its own members’ inputs by taking medians; under favorable circumstances, each half grades accurately, by its own standards—even if most members do not. The halves then contribute accurate grades that make up supergrades of the group as a whole, with differences of understanding between them boosting precision. Thus, with judicious use of both kinds of method, sufficiently large and diverse groups can reach judgments that are both accurate and precise enough for whatever task is at hand. The accuracy-precision trade-off for individuals is circumvented. Picking the heaviest ox out of a herd by grading them all is of course just a toy example, chosen to honor Galton. In a real problem of this sort we’d put each ox on a livestock scale and be done with it. There are many real classification problems though in which the use of grades cannot be avoided, whether that is because precise cardinal information is difficult to obtain or because ordinal information is all that can be had, even in principle. Take the measurement of severity of illness in clinical trials, held to assess the efficacy of medical interventions such as drugs. For instance, human “readers” of endoscopy videos score patients’ ulcerative colitis disease activity on the endoscopy component (normal-mild-moderate-severe) of the Mayo Clinic Scale. Accuracy is critical because the more accurate the scores are, the better researchers can tell effective interventions from ineffective ones. One way to increase reading accuracy that has recently been proposed is to aggregate scores assigned by several independent readers (Gottlieb and Hussain 2015). The authors propose a “2+1” collective scoring procedure that outputs the common score of two initial readers whenever they agree, and the median of their different scores together with the score of a third reader whenever they do not. Supergrading binary severity judgments is another way of achieving accuracy that could be explored. Instead of expecting individual readers to provide inputs using all four endoscopy scores of the Mayo Scale, the collective reading task could be set up so that different readers specialize on different parts of the scale. Let us say that one of three readers is responsible for determining just whether the score for a given video is above normal, another whether it is above mild, and the third whether the score is above moderate. Individual readers might be expected very often to achieve accuracy in this relatively undemanding task, and when their binary judgments are combined by supergrading, accurate scores on the endoscopy component of the Mayo Scale will be the result.Footnote 2 Besides the measurement of severity of illness there are many other matters that could be approached using the method of supergrading. These include college admissions, hiring decisions, stock evaluations, the evaluation of potential markets by venture capitalists, qualitative risk assessments by engineers, ranking sports teams, classification of loans by loan officers, and more.Footnote 3 Here, the ox-grading problem serves as a running example throughout. It stands in as a model for them all. The article develops as follows. Section 2 introduces grading languages. Section 3 characterizes grading problems and their solutions. Section 4 shows how to combine grading languages into more-precise superlanguages, and solutions into supersolutions. Section 5 states conditions under which grading problems have solutions that are both reliable and accurate. Section 6 characterizes ability in grading as reliability and accuracy together with discrimination. Section 7 shows that some groups with lower individual ability but more diversity have greater collective ability than other groups with greater individual ability but less diversity. Finally, Sect. 8 briefly remarks on some consequences for the design of committees and expert panels, and mentions directions for future research.",1
88.0,4.0,Theory and Decision,04 December 2019,https://link.springer.com/article/10.1007/s11238-019-09735-2,The two faces of independence: betweenness and homotheticity,May 2020,Daniel R. Burghart,,,Male,Unknown,Unknown,Male,"This section focuses on lotteries over a set of finite outcomes denoted as Z. Subsequent sections will focus specifically on monetary outcomes. Denote a lottery (i.e., probability distribution) as a J-tuple \((p_1, p_2, \ldots , p_J)\), where each element assigns probability mass to its respective outcome. Let \(\Delta \) be the set of all such lotteries (probability distributions). Formally, This paper discusses specific examples and provides illustrations for lotteries over three, non-negative outcomes. The set of these lotteries can be graphically represented in the probability simplex (see Fig. 1, Marschak 1950, and Machina 1982) using Abdellaoui (2002)’s orientation. The vertical leg represents the probability \(p_1\) that the best outcome \(z_1\) materializes; the horizontal leg represents the probability \(1-p_3 = p_1 + p_2\) that the worst outcome \(z_3\) does not materialize. At the upper-right, lower-right, and lower-left vertices of the triangle, the probability assigned to each of the outcomes \(z_1\), \(z_2\), and \(z_3\) is, respectively, equal to unity. Lotteries along the edges of the triangle assign probability mass to only two of the three outcomes, while lotteries on the interior of the triangle (the gray area in Fig. 1) assign strictly positive probability mass to all three outcomes. The probability triangle In what follows, preferences over lotteries \((\succsim )\) are assumed to be imbued with the following properties: Completeness For every \(A, B \in \Delta \), \(A \succsim B\), \(B \succsim A\), or both. Transitivity For \(A, B, C \in \Delta \), if \(A \succsim B\), and \(B \succsim C\), then \(A \succsim C\). Mixture continuity For every \(A, B, C \in \Delta \), the sets \(\{\alpha \in [0,1]: \alpha A + (1 - \alpha )B \succsim C\}\) and \(\{\alpha \in [0,1]: C \succsim \alpha A + (1 - \alpha )B\}\) are closed. In addition, this paper assumes the existence of a lottery W such that \(A \succ W\) for any lottery \(A \ne W\). The expected utility theorem establishes that preferences over lotteries \((\succsim )\) admit an expected utility representation \((\sum _j p_j u_j )\) if and only if preferences adhere to the independence axiom: The preference relation \(\succsim \) satisfies independence if, for all lotteries \(A, B, C \in \Delta \) and any \(\alpha \in (0, 1)\), In words, independence requires that a preference ordering between two lotteries is unchanged when mixing those lotteries with a third. One can interpret independence as requiring preferences for probabilities to have no complementarities. Indeed, Marschak (1951) and Samuelson (1952) defended independence on normative grounds, because the lack of complementarities is embedded in the objects of risky choice—only one outcome from a lottery will materialize and be consumed. Thus, preferences should respect this feature of the objects of choice (see also the discussion in Sugden (2004), Section 2.2). But others made objections to independence, the most prominent of which is due to Maurice Allais (1953, 1978) (see also Manne (1952) and Wold et al. (1952). Fishburn and Wakker (1995) provide a comprehensive historical review of the independence assumption). Various experimental studies have examined Allais-type common-ratio violations [see Fehr-Duda and Epper (2012) for a recent review]. In Kahneman and Tversky (1979)’s Problems 3 and 4, for example, experimental participants were offered a choice between a lottery with an \(80\%\) chance of receiving $4000 and a \(20\%\) chance of receiving $0 [\(A = (0.80, 0.00, 0.20)\)] or, a certain $3000 [\(B = (0,1,0)\)]. When surveyed, the majority of people indicated a strong preference for the certain $3000 (i.e., \(B \succ A\)). The participants were also presented with another pair of lotteries: a \(20\%\) chance of receiving $4000 and an \(80\%\) chance of receiving $0 [\(A^\prime = (0.20, 0.00, 0.80)\)]; or, a \(25\%\) chance of receiving $3000 and a \(75\%\) chance of receiving $0 [\(B^\prime = (0.00, 0.25, 0.75)\)]. The lotteries \(A^\prime \) and \(B^\prime \) were constructed as mixtures with the worst outcome, which is typically assumed to be the worst lottery. Specifically, \(A^\prime = \alpha A + (1- \alpha ) C\) and \(B^\prime = \alpha B + (1-\alpha ) C\) with \(\alpha = 0.25\) and \(C = (0, 0, 1)\). Given that \(B \succ A\), behavior consistent with independence requires that \(B^\prime \succ A^\prime \). When surveyed, however, most people indicated the opposite preference (i.e., \(A^\prime \succ B^\prime \)), a clear violation of independence. Allais-type violations of independence can be illustrated using the probability triangle. In Panel (a) of Fig. 2, the red circles at the end points of the red line segment depict the lotteries available in Kahneman and Tversky (1979)’s Problem 3, \(\{A, B \}\). If preferences satisfy independence, then indifference curves must be parallel and straight lines in the simplex. Thus, if \(B \succ A\), independence-satisfying indifference curves must have a slope that is steeper than that of the red line segment. The lotteries available in Kahneman and Tversky (1979)’s Problem 4, \(\{A^\prime , B^\prime \}\), are depicted as squares at the end points of the blue line segment. Note that this blue line segment has the same slope as the red line segment. Because indifference curves that satisfy independence must be parallel (and linear), the slope of an independence-satisfying indifference curve that rationalizes the original choice \((B \succ A)\) must also be steeper than the blue line segment. Therefore, if preferences satisfy independence, it should be the case that \(B^\prime \succ A^\prime \), instead of \(A^\prime \succ B^\prime \). Allais common ratio choice problems The betweenness axiom can accommodate Allais-type behavior by relaxing independence: The preference relation \(\succsim \) satisfies betweenness if, for all lotteries \(A, B \in \Delta \) and any \(\alpha \in (0, 1)\), Betweenness requires that if a chooser is indifferent to two lotteries, then they must also be indifferent to any probability mixture thereof. Several models have been proposed that build on the betweenness axiom including implicit weighted utility (Chew 1989; Dekel 1986), its special case disappointment aversion (Gul 1991), and weighted utility (Chew 1983). Graphically, the betweenness axiom requires indifference curves to be straight lines, but removes the parallelism requirement of independence. The gray lines in Panel (b) of Fig. 2 depict an indifference map that satisfies betweenness but not independence; the indifference curves fan out across the simplex [see Machina (1982)’s Hypothesis II]. Note that for this indifference map Allais-type violations of independence can be accommodated. Straight line, but not parallel, indifference curves consistent with betweenness can rationalize Allais-type behavior. This suggests that Allais-type violations of independence are, in fact, violations of the parallelism assumption embedded in the independence axiom. The following parallelism assumption—homotheticity—does not require indifference curves to be linear. This homotheticity condition is an adaptation of traditional homotheticity in that it treats the worst outcome under consideration as the origin from which indifference curves expand and contract. Let W be the lottery that pays the worst outcome with probability 1. The preference relation \(\succsim \) satisfies homotheticity if, for all lotteries \(A, B \in \Delta \) and any \(\alpha \in (0, 1)\), Homotheticity restricts preferences such that the ordering between two lotteries is retained when mixing both with the worst possible outcome. Indeed, this is exactly the assumption tested by the common ratio examples: the second choice pair, \(A'\) and \(B'\), is obtained by mixing the original lotteries A and B with the worst possible lottery \(C = (0, 0, 1)\). Homotheticity is less restrictive than independence. For example, the indifference map depicted by the gray curves in Panel (c) of Fig. 2 conform to homotheticity—they are parallel expansions of each other from the origin. These curves, however, are non-linear and therefore they are inconsistent with the linearity principle of independence and betweenness. In contrast to independence and betweenness, homotheticity is flexible enough to permit a preference for, or aversion to, probability mixtures. To discuss this issue more precisely, define the following terms by taking \(A, B \in \Delta \) with \(A \sim B\): Quasi-concavity:\(\alpha A + (1 - \alpha )B \succsim A\), for every \(\alpha \in (0,1)\). Quasi-convexity:\(A \succsim \alpha A + (1 - \alpha )B\), for every \(\alpha \in (0,1)\). Strict quasi-concavity/convexity is defined by interchanging the weak preference relation \((\succsim )\) with the strict preference relation \((\succ )\). Quasi-concavity guarantees that upper contour sets, \(\{C \in \Delta | C \succsim A\}\), are convex sets. In this way, preferences that adhere to quasi-concavity will exhibit a preference for probability mixtures. Quasi-convexity is the reverse: lower contour sets, \(\{ C \in \Delta | A \succsim C\}\), are convex sets, which accommodate an aversion to probability mixtures. If the two binary choice situations in Allais’ experiment are generalized to include a third alternative, an alternative that is a probability mixture of the two end points of the line segments, people who exhibit a preference for mixtures will prefer these lotteries. Panel (d) of Fig. 2 illustrates homothetic, quasi-concave preferences graphically—the indifference map depicted is consistent with individuals selecting the mixture alternatives (M and \(M^\prime \)) in these generalized Allais-type choice situations. Previous studies have identified conditions that are quite similar to homotheticity. Diecidue et al. (2009) identify a condition they call common ratio invariance for decumulative probability distributions that has the same principle as homotheticity: “Common ratio invariance for decumulative distributions says that shifting proportionally probability mass from good consequences to the worst consequence (or doing the opposite) leaves preferences unaffected, which precisely rules out the common ratio effect [Allais-type violations of independence]” (Diecidue et al. (2009), p.1107). That study establishes how common ratio invariance is the critical assumption for a rank-dependent utility model with a power weighting function (i.e., \(w(p) = p^\gamma \) with \(\gamma \ne 0\)), a model that is well known to rule out Allais-type violations of independence, but permits a strict preference for (\(\gamma < 0\)) or aversion to (\(\gamma > 0\)) probability mixtures. 
Safra and Segal (1998) contains a similar condition to homotheticity that is called zero independence. Zero independence requires preference orderings to be maintained when lotteries are mixed with a degenerate, zero outcome, the worst outcome in Safra and Segal (1998)’s configuration. Safra and Segal (1998) demonstrate that “Together with constant risk aversion, this axiom [zero independence] implies Yaari’s representation with a probability transformation function of the [power] form \(g(p) = 1 - (1 - p)^t\)” (Safra and Segal (1998), p.21; brackets added). Safra and Segal (1998)’s model can accommodate a strict preference for probability mixtures (i.e., strict quasi-concavity), while ruling out Allais-type violations of independence. This behavioral pattern can also be captured by some well-known models that involve direct transformation of probabilities, such as the theories introduced by Edwards (1962), Handa (1977), and original prospect theory (Kahneman and Tversky 1979). While these models can satisfy homotheticity, this comes at the cost of violations of monotonicity or transitivity, a theoretical trade-off few economists are willing to accept. In a rank-dependent utility setting, Grant and Kajii (1998) invoke a condition similar to homotheticity that rules out common-ratio violations when lotteries are mixed with the worst outcome. They coin their condition the axiom of downward scale invariance. The focus of Grant and Kajii (1998) is, however, their axiom of upward scale invariance, a condition that rules out common-ratio violations when lotteries are mixed with the best outcome. This paper follows the standard setup for common-ratio choice pairs in which the original lotteries are mixed only with an inferior outcome (see, for example, the definition in Starmer 2000 and Fehr-Duda and Epper 2012). For the present study, homotheticity (or common ratio invariance, or zero independence, or downward scale invariance) is valuable as an empirically testable condition. First, a direct assessment of homotheticity can be accomplished due to the linkage between it and traditional homotheticity assumptions—Varian’s homothetic axiom of revealed preference provides a direct assessment of necessity and sufficiency for homotheticity. Secondly, homotheticity serves as a component of an indirect assessment of the betweenness axiom. Specifically, if preferences satisfy homotheticity, but fail independence, betweenness must be violated. The independence axiom produces indifference curves that are straight lines and parallel. Intuition suggests that combining the linearity of betweenness and the parallelism of homotheticity will produce an equivalency with independence. This intuition is formalized with a proposition: Assume that preferences conform to completeness, transitivity, and mixture continuity. The preference relation \(\succsim \) satisfies independence if and only if it satisfies betweenness and homotheticity.Footnote 4 That independence implies both homotheticity and betweenness is straightforward. To show that betweenness and homotheticity imply independence take distinct \(A, B \in \Delta \) with \(A \sim B\). Fix \(C \in \Delta \) and \(\lambda \in (0,1)\). If \(C=W\), then by homotheticity \(\lambda A + (1 - \lambda C \sim \lambda B + (1 - \lambda ) C\). If \(C \ne W\), then by continuity, completeness, and homotheticity, there exists \(\alpha \in (0,1]\) and \(\beta \in (0,1]\) such thatFootnote 5 Let Note that \(\tau \in (0,1)\) and \(\gamma \in (0,1]\). Observe that where the indifference in the second line follows from betweenness. Homotheticity guarantees that \(\lambda A + (1 - \lambda ) C \sim \lambda B + (1 - \lambda ) C\). This shows that \(A \sim B\) implies \(\lambda A + (1 - \lambda ) C \sim \lambda B + (1 - \lambda ) C\) for every \(A, B, C \in \Delta \) and \(\lambda \in (0,1)\). The argument that the converse holds is standard (e.g., Lemma 1 in Dubra et al. 2004). This establishes that betweenness and homotheticity imply independence and completes the proof.    \(\square \) Figure 3 illustrates the intuition behind this proof in the simplex. The thick line segment represents a betweenness-satisfying indifference curve with \(A \sim B\). Homotheticity requires that the mixtures \(A^\prime \) and \(B^\prime \) must also be indifferent. The thin line segment represents an indifference curve with \(A^\prime \sim B^\prime \) which, by betweenness, must also be linear. This thin line segment must be parallel to the thick line segment because of homotheticity. This illustrates how Proposition 1 decomposes independence into distinct axiomatic principles. One principle, betweenness, restricts indifference curves to be linear, but not parallel. The other principle, homotheticity, restricts indifference curves to be parallel, but not linear. When these two principles are combined, they give rise to an equivalence with the expected utility’s key assumption, independence. From a more technical perspective, notice that the result above is valid even when the prize space is not finite (e.g., settings with probability distributions supported by the real number line). The result will also be valid if preferences are instead homothetic with respect to the best outcome, such as the axiom of upward scale invariance studied in Grant and Kajii (1998). It is not clear, however, whether or not the above result will hold if preferences are homothetic with respect to other lotteries, a line of inquiry left for future work. Betweenness and homotheticity are equivalent to independence",3
88.0,4.0,Theory and Decision,17 December 2019,https://link.springer.com/article/10.1007/s11238-019-09740-5,Resolving Zeckhauser’s paradox,May 2020,Yudi Pawitan,Gabriel Isheden,,Unknown,Male,Unknown,Male,"A terrorist group puts you in a Russian Roulette: a gun to your head with some bullets inside. Assume it is a standard-issue revolver with 6-bullet chambers. As part of their entertainment, they tell you there are two possibilities: (A) there are two bullets in the gun, or (B) there are four bullets. They ask, are you willing to pay more for removing the two bullets in (A) than for removing one in (B)? If you are like other normal human beings, you would say an emphatic yes. However, a calculation based on the classical utility theory implies that you are being inconsistent about money, which is close enough to being irrational. That is called Zeckhauser’s paradox. Even mathematicians, economists and game theorists report that they react like normal humans. But, perhaps unlike normal humans, they admit that they make a mistake. See, for example, a quote from Binmore (2009, Chap. 3): I made this mistake myself when the paradox was first put to me, but I changed my mind when I learned that my decision was inconsistent with my preferring being alive to being dead and having more money rather than less. And another from Landsburg (2011)’s blog: Did you pass the test? I for one did not. That leaves me (and also you, if you failed along with me) two options. Either we can maintain that there’s some flaw in the [...] argument - some way in which it fails to capture the “right” meaning of rationality - or we can conclude that we don’t always make good decisions, and that meditating on our failures can help us make better decisions in the future (including in situations more likely to arise than forced Russian Roulette). I am mostly in the latter camp. The philosopher Jeffrey (1988) ‘lingered by the rail for some time,’ but eventually ‘bit the Bayesian bullet,’ i.e. followed the direction prescribed by the utility. He proposed that a good part of rationality is strength of character: fortitude or steadfastness or nerve. So for him the ‘deviant preference’ is irrational, though in this case ‘the irrationality is no intellectual flaw, but a characterological one, i.e. not stupidity but funk.’ In summary, from all these reactions, if you do not follow the action prescribed by the utility, you are either irrational or lacking in character. Neither is admirable. We aim to show that the normal human reaction is rational in accordance with the classical utility theory, as long as we state our preferences explicitly and account for them in deriving the utilities. We also aim to extract from the paradox a general lesson about the normative role of the utility function as a rational guide of our decisions and preferences.",1
89.0,1.0,Theory and Decision,06 February 2020,https://link.springer.com/article/10.1007/s11238-020-09744-6,"Need, frames, and time constraints in risky decision-making",July 2020,Adele Diederich,Marc Wyszynski,Stefan Traub,Female,Male,Male,Mix,,
89.0,1.0,Theory and Decision,25 January 2020,https://link.springer.com/article/10.1007/s11238-020-09745-5,"Some conditions for the equivalence between risk aversion, prudence and temperance",July 2020,Marzia De Donno,Mario Menegatti,,Female,Male,Unknown,Mix,,
89.0,1.0,Theory and Decision,18 January 2020,https://link.springer.com/article/10.1007/s11238-019-09742-3,Decision making under uncertainty: the relation between economic preferences and psychological personality traits,July 2020,David Schröder,Gail Gilboa Freedman,,Male,,Unknown,Mix,,
89.0,1.0,Theory and Decision,11 February 2020,https://link.springer.com/article/10.1007/s11238-020-09746-4,Quasi-stationary social welfare functions,July 2020,Susumu Cato,,,Male,Unknown,Unknown,Male,"This paper examines collective decision-making with an infinite population. It is well known that Arrow’s impossibility theorem is not valid.Footnote 1 That is, there exists a social welfare function that satisfies weak Pareto, independence, non-dictatorship, and unlimited domain (Fishburn 1970). That is, Arrow’s axioms are compatible. One possible interpretation of the infinite-population setting is an infinite-time horizon decision-making, typically considered in the context of optimal saving or optimal growth models. Once the flow of time is considered, a set of individuals is interpreted as the set of generations. Then one should introduce another axiom associated with the time structure. Stationarity, which was proposed by Koopmans (1960), is a crucial requirement on preferences over time. It requires that if the first-period outcome is the same over two sequences of outcomes, then preferences are invariant for the tail sequences without the first-period outcome. Stationarity can be regarded as a type of rationality in a setting with the flow of time. Koopmans’s stationarity has been investigated in various fields of economics, such as intergenerational equity and optimal growth theory. Ferejohn and Page (1978) and Packel (1980) incorporate Koopmans’s stationarity into the framework of social choice with an infinite population. They demonstrate the difficulty of stationary collective decision-making. Recently, Bossert and Suzumura (2011) addressed the problem of collective decision-making in an infinite-time horizon setting by revisiting works of Ferejohn and Page (1978) and Packel (1980). There are at least two contributions of their study. First, Bossert and Suzumura (2011) require individual preferences to be selfish, in the sense that each generation cares only about outcomes in the period she lives. In Ferejohn and Page (1978) and Packel (1980), each individual’s preference is sensitive to outcomes in all periods. One contribution made by Bossert and Suzumura (2011) is to establish the ultrafilter lemma, which is fundamental to the theory of social choice, in the selfish domain. They show that if a social welfare function satisfies selfish domain, weak Pareto, Pareto indifference, and independence of irrelevant alternatives, then the collection of decisive sets is an ultrafilter. Second, Bossert and Suzumura (2011) proposed multi-profile stationarity, a natural formulation of stationarity for the selfish domain. The axiom requires that not only the first-period outcome but also the first-generation preference be excluded when it is applied. Bossert and Suzumura (2011) show an impossibility result on stationary collective decision-making: the first generation is a dictator when multi-profile stationarity and the four axioms in their ultrafilter lemma are imposed. This paper extends Bossert and Suzumura (2011). Our main motivation is to relax stationarity as social rationality to incorporate some myopic collective decision-making. Myopic behavior was first investigated by  Strotz (1955) and later incorporated with a tractable formulation of an objective function in the optimal growth theory. Recently, the development of behavioral economics (see Frederick et al. 2002) has drawn scholars to Phelps and Pollak’s (1968) formulation called hyperbolic discounting. Hayashi (2003) introduces a new axiom to capture a utility function with hyperbolic discounting. Thus, his axiom, quasi-stationarity, can be consistent with a type of myopia. We introduce Hayashi’s quasi-stationarity into the analysis developed by Bossert and Suzumura (2011). This study has three main contributions. First, we show that the collection of decisive coalitions forms an ultrafilter without Pareto indifference if there are at least four periodic alternatives in each period. That is, Pareto indifference is required only in the extreme case with three alternatives in each period. Second, quasi-stationarity is imposed instead of stationarity. Then we show that if a social welfare function satisfies selfish domain, weak Pareto, independence of irrelevant alternatives, and quasi-stationarity, then either the first or second generation is a dictator. Thus, we obtain an impossibility even when stationarity is relaxed and some myopia is allowed. This contrasts with the case of relaxing preference coherency. As Sen (1969) has shown, if transitivity of social preferences is weakened to quasi-transitivity, Arrow’s axioms are compatible even with a finite population. Relaxing transitivity as social rationality yields a possibility result, but relaxing stationarity does not. Third, we identify a very weak version of stationarity that yields a possibility result. This paper is related to two groups of works. The first line is a series of papers on Arrovian social choice with an infinite population. Since Fishburn (1970), many studies have examined the structure of the collection of decisive sets.Footnote 2 Considering an arbitrary set of individuals, Kirman and Sondermann (1972) show that an ultrafilter is a key to understanding Arrovian social welfare functions. A non-principal ultrafilter allows us to have a non-dictatorial social welfare function in a society with an infinite population. Then computability, continuity, or anonymity axioms are imposed to examine the existence of more preferable social welfare functions (Mihara 1997a, b; Gomberg et al. 2005; Salonen and Saukkonen 2005; Saukkonen 2007; Cato 2017). The second group is a series of papers on intergenerational equity that consider social indifference curves over the set of infinite utility streams. Koopmans (1960) proves that a stationary preference with some regular conditions reveals impatience. Thus, it is difficult to construct a social welfare criterion that satisfies continuity, monotonicity, and anonymity (Diamond 1965; Basu and Mitra 2003).Footnote 3 Without continuity, it is possible to construct various social welfare quasi-orderings (Basu and Mitra 2007). In particular, extending the concept of anonymity yields more complete social quasi-orderings (Kamaga and Kojima 2009, 2010; Adachi et al. 2014). Interestingly, a non-principal ultrafilter is the key to constructing a social welfare quasi-ordering (Fleurbaey and Michel 2003). Asheim et al. (2012) examine the implication of stationarity in the setting of infinite utility streams. The rest of this paper is organized as follows. Section 2 introduces the basic definitions. Section 3 examines the implications of quasi-stationarity. Section 4 generalizes the result of Sect. 3. Section 5 presents a possibility result. Section 6 concludes the paper. The Appendix includes proofs.",2
89.0,1.0,Theory and Decision,13 January 2020,https://link.springer.com/article/10.1007/s11238-019-09743-2,Independent collective identity functions as voting rules,July 2020,José Carlos R. Alcantud,Annick Laruelle,,Male,Female,Unknown,Mix,,
89.0,2.0,Theory and Decision,14 February 2020,https://link.springer.com/article/10.1007/s11238-020-09747-3,A new test of convexity–concavity of discount function,September 2020,Pavlo R. Blavatskyy,Hela Maafi,,Male,Female,Unknown,Mix,,
89.0,2.0,Theory and Decision,06 March 2020,https://link.springer.com/article/10.1007/s11238-020-09750-8,Risk aversion and the value of diagnostic tests,September 2020,Han Bleichrodt,David Crainich,Nicolas Treich,,Male,Male,Mix,,
89.0,2.0,Theory and Decision,03 June 2020,https://link.springer.com/article/10.1007/s11238-020-09757-1,A modified Monty Hall problem,September 2020,Wei James Chen,Joseph Tao-yi Wang,,,Male,Unknown,Mix,,
89.0,2.0,Theory and Decision,03 March 2020,https://link.springer.com/article/10.1007/s11238-020-09748-2,Power distribution in the Basque Parliament using games with externalities,September 2020,G. Arévalo-Iglesias,M. Álvarez-Mozos,,Unknown,Unknown,Unknown,Unknown,,
89.0,2.0,Theory and Decision,24 February 2020,https://link.springer.com/article/10.1007/s11238-020-09751-7,On the first-offer dilemma in bargaining and negotiations,September 2020,António Osório,,,Male,Unknown,Unknown,Male,"The first-offer dilemma in bargaining and negotiation is a central and debated question among academics and practitioners—should one make the first offer or wait for the opponent to do so? Interestingly, practitioners and researchers tend to disagree on this issue. (Loschelder et al. 2014) call it the practitioner–researcher paradox. There is a conventional wisdom among practitioners that moving first in bargaining is a mistake. Some practical and business-oriented literature supports the idea that it is wise to let the opponent make the first offer (Dell and Boswell 2009; McCormack 1989; among others). The argument is that first offers provide crucial information to the second-mover and may give away some bargaining zone. However, the majority of the experimental and empirical research, coming mostly from psychology and business, shows that first offers benefit from an anchoring effect (Chertkoff and Conley 1967; Galinsky et al. 2009; Galinsky and Mussweiler 2001; Kristensen and Gärling 1997; Liebert et al. 1968; Yukl 1974).Footnote 1 The argument is that the first offer drives the cognitive process towards the search for information and attributes that are consistent with this offer (Mussweiler and Strack 1999, 2000). Orr and Guthrie (2006) have found that around 25% of the variance in the final outcome is explained by first-offer effects. Galinsky et al. (2009) and Van Poucke and Buelens (2002) have found even stronger first-offer effects. Gunia et al. (2013) show that the first-offer effect is remarkably robust across cultures, bargaining powers, and types of negotiations, but some authors argue that first-movers fail to exploit the potential of the anchoring effect (Galinsky and Mussweiler 2001; Liebert et al. 1968). First offers have an even stronger anchor effect when the subjects have little or no information about the value of the object under negotiation (Gunia et al. 2013; Orr and Guthrie 2006; Strack and Mussweiler 1997). However, the strength of the first-offer anchoring effect can be mitigated if we consider information issues, experience, framing, perception and aspirations. For instance, Liebert et al. (1968) have found that less well-informed bargainers are more influenced by extreme first offers than informed bargainers (Orr and Guthrie 2006). Orr and Guthrie (2006) conclude that experts are more immune to anchoring effects, but not completely so (Neale and Bazerman 1983; Ritov 1996). For instance, Northcraft and Neale (1987) have found anchoring effects in real estate professionals (Mussweiler et al. 2000), and Englich et al. (2006) and Orr and Guthrie (2006) have found anchoring effects in professional judges. The first-offer anchoring effect can also be mitigated by inconsistent or contradictory information that does not support the proposed value (Chapman and Johnson 1999; Kristensen and Gärling 2000; Lord et al. 1984; Mussweiler et al. 2000). The theoretical literature, coming mostly from economics, also agrees with the existence of first-mover advantages in non-cooperative bargaining situations (Ausubel et al. 2002; Binmore et al. 1992; Fudenberg et al. 1985; Kennan and Wilson 1993; Rubinstein 1982, 1985; Sobel and Takahashi 1983). The existence of first-mover advantages are implicit in most studies (Muthoo 1999; Osborne and Rubinstein 1990). However, this issue has never been addressed explicitly. As opposed to other economic issues, in which first- and second-mover advantages are well understood (Lieberman and Montgomery 1988), the existence of first- or second-mover advantages in bargaining and negotiations remains far from our understanding. The present paper is the first theoretical approach aimed at addressing these issues. The objective of this paper is to study the circumstances under which the second-mover holds an advantage in bargaining. In this context, we want to understand why—despite so much evidence in favor of the first-mover position—professional bargainers seem to prefer the second-mover position. For instance, at some moment in our lives, when buying or selling an object, we have been invited to make the first offer. This invitation is very common among professional bargainers. For instance, in China and in most Eastern cultures (in some Western countries also, but to a much lesser extent), some goods do not have a price tag. In this context, when buyers demand information about the selling price, they are invited to make an offer. In other words, sellers refrain from making the first offer to become second-movers. In fact, professional bargainers seem to compete for this position. In this context, the following question arises: what are the strategic reasons behind such behavior? To address these issues, we consider the Rubinstein (1982) infinite horizon alternating offers model with incomplete information about the time preferences (i.e., the discount factor). In this model, two fully rational individuals bargain over the division of a pie with a unit value. For simplicity, we consider that individuals can be of two different types. High-type individuals, who have a higher discount factor and are consequently less likely to accept a smaller share of the pie (patient-types), and low-type individuals, who have a lower discount factor and are consequently more likely to accept a smaller share of the pie (impatient-types). We consider a one-sided incomplete information structure (Ausubel et al. 2002; Binmore et al. 1992; Fudenberg et al. 1985; Kennan and Wilson 1993; Rubinstein 1985), in which the first-mover has no knowledge about the second-mover’s type, but the second-mover has complete information about the first-mover’s type. The information structure leads to a simpler and more tractable model, and is motivated by the fact that offers provide information about the individuals’ types. Under perfect information conditions, first-movers hold a strategic advantage (Muthoo 1999; Osborne and Rubinstein 1990; Rubinstein 1982), which is explained by the fact that first-movers have an early chance to impose their preferences on second-movers. In a context with costly delay due to discounting, the second-mover is in a weaker position, because rejection implies an undesired loss of value. The first-mover advantage explores this fact. The introduction of incomplete information returns an information advantage to the second-mover. The first-mover faces a dilemma: either she proposes an aggressive offer (i.e., an offer that is relatively more favorable to herself), but that may be rejected, which leads to a cost of delay, or she proposes a soft offer (i.e., an offer that is relatively less favorable to herself, which has associated an information cost), but is sure to be accepted. Consequently, the second-mover may benefit from offers that are higher than required for acceptance, which has associated an information gain. Therefore, the existence of first- or second-mover advantages depends crucially on the strength of the strategic and information advantage. We find two different first-mover strategic behaviors. (i) If there is a strong belief that the second-mover is of the high-type, the first-mover proposes soft offers. In this case, there is no delay and no destruction of value, but the first-mover may incur an information cost and the second-mover benefits from an information gain. Consequently, second-mover advantages exist if the information advantage is sufficiently strong, which increases with the difference between the types. (ii) Otherwise, if there is no strong belief that the second-mover is of the high type, the first-mover proposes aggressive offers. However, these offers can be rejected by the high-type second-movers, and consequently lead to a cost of delay. In this case, the second-mover position is preferred if the first-mover cost of delay is sufficiently important. We also find other regularities. First, second-mover advantages always require the existence of differences in individuals’ types. Second, second-mover advantages always require sufficiently strong beliefs that the opponent is of the high-type, i.e., the type that rejects aggressive offers. These beliefs discipline first-movers to propose soft offers, which are the offers that can potentially benefit second-movers. Third, first-mover advantages tend to be more prevalent, in line with the experimental and empirical literature, but second-mover advantages tend to appear under very reasonable and realistic situations. Fourth, first-mover advantages are particularly robust in contexts of great ambiguity, which may provide a rationale for why it is so difficult to find second-mover advantages in the experimental literature. This paper is organized as follows: Sect. 2 describes the theoretical framework, Sect. 3 characterizes the bargaining equilibrium, Sect. 4 studies the existence of first- and second-mover advantages in bargaining, and Sect. 5 concludes.",2
89.0,2.0,Theory and Decision,16 March 2020,https://link.springer.com/article/10.1007/s11238-020-09749-1,Additive multi-effort contests,September 2020,Kjell Hausken,,,Male,Unknown,Unknown,Male,"Earlier rent seeking research has mostly assumed one effort for each player, which is limiting given the plethora of possible efforts. The literature gradually expands to account for multiple efforts for each player. How multiple efforts interact to impact rent seeking is currently poorly understood. This article intends to improve this understanding. Examples of rents are R&D budgets, promotions, licenses, privileges, monopoly opportunities, election opportunities, struggles for government support between different industries, competition for budgets by interest groups, and government distribution of public goods. Examples of efforts to obtain rents are multifarious, e.g. lobbying, influence strategies, interference struggles, litigation, strikes and lockouts, political campaigns, commercial efforts to raise rivals’ costs (Salop and Scheffman 1983), economic and political maneuvers (Hirshleifer 1995), coaxing, prompting, inducing, urging, extorting, exacting, persuasion techniques, pressure methods, promotions, briberies, skirmishes, battle, combat, and fighting with or without violence. This article acknowledges that each player may have available arbitrarily many efforts which may or may not overlap with the contending player’s available efforts. Each effort may be of different nature and operate according to its own logic. Analyzing multiple additive efforts supplements the earlier literature which commonly assumes one effort, or usually assumes multiplicative efforts which all have to be exerted to ensure impact. Formally in this article, efforts may have three different characteristics, i.e. different unit costs, different impacts, and different contest intensities. Efforts operate additively in the contest success function, which has been insufficiently analyzed in the literature. The model is chosen to enable each player to incur a different cost of effort, and have a different impact with a different contest intensity for each effort. In the rent seeking literature, the contest intensity or decisiveness parameter is generally a parameter at the contest level, and thus equivalent for both or all players. The authors are not aware of literature modeling different contest intensity parameters for different players. In this article, each effort operates according to its own logic with an intensity, scaling and impact independent of the other efforts. Hence, the contest intensity parameters generally differ across players. Specific efforts by one player are thus not matched against specific efforts by the other player. Instead, each player’s efforts are added up into an effort production function which competes against the other player’s effort production function. In the contest success function, additive efforts are substitutable while multiplicative efforts are complementary. However, when accounting for both the contest success function and the cost of exerting efforts, a new function emerges. For this new function, multiple efforts with different production functions and unit effort costs can generally be of the same kind or nature, can be substitutes for each other, or can complement each other in various ways. All these kinds are possible with additive efforts in the contest success function (Proposition 3) since the subtraction of effort costs in the players’ expected utilities causes linkages between the players’ efforts. To illustrate the prevalence of additive efforts and how they differ from single efforts, consider two examples. The first illustrates how a lobbying firm may hire different kinds of professionals to be able to exert multiple additive efforts, one effort for each professional. The lobbying firm evaluates hiring any combination of professionals with substitutable training causing different production functions operating additively, with different unit effort costs, e.g. professionals with any degree and experience in economics versus political science, a man versus a woman, human effort versus machine effort, etc. Since the efforts are not of the same kind, a close look at how the efforts substitute or complement each other is needed. For example, an office clerk can manually compile statistics to support a rent application, or advanced computers and software can be employed to do the same work. Second, consider multiple players interpreted to interact statically. Each player hires multiple professionals with various kinds of expertise, to enable each player to exert multiple additive efforts. The players compete for an elected office position, e.g. US president. Each player hires professionals with various kinds of expertise, i.e. political analysts to develop views and positions on issues, media professionals for spin control, social media operatives, business people to recruit donors, telephone operators to convince voters, geographically dispersed ground troops knocking on people’s doors, speech writers to tune messages for big rallies and local meetings, gossip developers, and specialists in negative campaigning. These efforts may jointly and independently add up to a campaign’s effort production function which impacts the contest with the other player(s). It is quite possible for a player’s campaign to be successful even if some efforts are missing, e.g. due to strategic choice, oversight, lacking competence, or deficient funding. For example, a player may decide to eliminate negative campaigning and ground troops. Alternatively, a player may rely on big colorful rallies applying hitherto unknown influence techniques that the other players are unable or unwilling to apply. Combining the additive contest success function with the subtraction of efforts’ costs may cause independence, substitutability, or complementarity between efforts. One alternative to additive efforts is multiplicative efforts of the Cobb–Douglas type analyzed by Arbatskaya and Mialon (2010), extended to a two-stage contest by Arbatskaya and Mialon (2012). One of their examples, also provided by Tullock (1980) and Krueger (1974), is that “firms may be able to obtain rents from the government not only by improving their efficiency, but also by lobbying or even bribing government officials” (Arbatskaya and Mialon 2010). Multiplicative efforts can be descriptive of this phenomenon when both improved efficiency and lobbying are mandatory for successful rent seeking. That is, improved efficiency without lobbying guarantees no success, and lobbying without improved efficiency guarantees no success. For some phenomena such as career promotions, requiring all efforts to be mandatory can be realistic even as the number of efforts increases. For other phenomena, as the number of efforts increases, Cobb–Douglas type multiplicative efforts may become increasingly unrealistic since each effort must be strictly positive to ensure success. The current article opens for the possibility that improved efficiency without lobbying, or lobbying without improved efficiency, may both constitute successful rent seeking, although both operating additively may be even more successful. The different assumptions of additive and multiplicative efforts cause different results regarding efforts, expected utilities, and rent dissipation. For example, for additional efforts, Arbatskaya and Mialon (2010) find increased rent dissipation when the contest becomes more balanced, whereas we find decreased rent dissipation caused by players optimizing more cost effectively across efforts. The rent seeking literature has developed fruitfully for half a century (Congleton et al. 2008). Early developments are by Krueger (1974), Posner (1975) Tullock (1980), etc., reviewed by Nitzan (1994). Skaperdas (1996) considers symmetric contests, Clark and Riis (1998) analyze asymmetric contests, Cubel and Sanchez-Pages (2016) assess difference-form contest success functions, Bozbay and Vesperoni (2018) evaluate contest success functions for networks and Münster (2009) examines group contests. Rai and Sarin (2009) allow multiple types of investments. Rai and Sarin (2009) exemplify multiple types of investments with a linear production function and the Cobb–Douglas production function. Their linear production function involves two additive efforts which contestants may substitute between. They treat one investment as fixed which causes the contest success function analyzed by e.g. Nti (2004) and Hausken and Zhuang (2012). Another example of efforts, but not of the Cobb–Douglas type, are by Epstein and Hefeker (2003). Assuming two efforts for each player, the first is conventional rent seeking. The second effort may be absent, or it may reinforce the first effort. They find that two efforts strengthen the player with the higher stake and decreases relative rent dissipation. Influenced by Dixit’s (1987) analysis of precommitment in contests, Yildirim (2005) analyzes a two-period game where both players simultaneously choose one effort each in period 1, which becomes public knowledge, and both players simultaneously choose whether to add one effort in period 2, so that the probability of winning depends on the cumulative effort levels. Melkonyan (2013) considers hybrid contests where the players forfeit one resource each ex-ante, and commit one resource each ex-ante which is expended ex-post by the winning player. He finds no rent overdissipation, and that more players cause less ex-ante and more ex-post expenditures by individual players, and more ex-ante and ex-post expenditures across all players. Hausken (2020) analyzes additive efforts for arbitrarily many players assuming contest intensity one for each effort in the contest success function. That is, each effort has proportional impact since the exponent to each effort equals one. He finds that 50% of the rent is dissipated when the players have equal ratios of unit cost divided by impact, and that rent dissipation decreases as the players’ ratios become more unequal. Clark and Konrad (2007) evaluate contests in multiple dimensions. Winning a certain number of contests is required to win the prize. Osorio (2018) analyzes a model with multiple efforts and two allocation systems. The I-system is a sum of independent contests where each player exerts one effort for each prize. The A-system resembles the approach in the current article where the players’ multi-issue efforts are aggregated additively into a single outcome. Assuming the same contest intensity for both players and across all efforts, he finds that the A-system tends to induce higher total efforts than the I-system. With decreasing returns to effort, the players distribute their efforts over all issues, while with increasing returns to effort, the players focus on only one issue. Supplementing rent seeking with sabotage is another example of multiple efforts. Konrad (2000) assumes that one effort improves the player’s contest success, whereas a second effort decreases the rival players’ success, which may increase lobbying efforts and rent dissipation. Chen (2003) considers competition for promotion involving efforts to enhance one’s own performance and efforts to sabotage the opponents’ performance. He finds that abler competitors are subject to more attacks. Amegashie and Runkel (2007) study sabotage in a three-stage elimination contest between four players. They find one equilibrium where only the most able contestant engages in sabotage, and one equilibrium without sabotage. Krakel (2005) assumes that each player in the first stage chooses help, sabotage, or no action, and in the second stage chooses effort to win the tournament, which causes a variety of equilibria. See Chowdhury and Gürtler (2015) for a survey. Multiple efforts, i.e. production and appropriation, are also present in the conflict models by Hirshleifer (1995), Skaperdas and Syropoulos (1997), and Hausken (2005), but contest success depends only on appropriation. Chowdhury and Sheremeta (2015) propose a procedure to identify strategically equivalent contests which generate the same equilibrium efforts but different equilibrium payoffs. That procedure may potentially be used to compare the equilibrium efforts in this article with efforts in other contests to identify strategically equivalent contests. Section 2 presents the model assuming multiple additive efforts. Section 3 solves the model generally and presents the structure of the solutions. Section 4 analyzes the model when all contest intensities except maximum one are less than one. Section 5 analyzes the model when all contest intensities are equal to or larger than one. Section 6 compares results of when rent dissipation, efforts and expected utilities increase or decrease. Section 7 concludes.",4
89.0,3.0,Theory and Decision,26 March 2020,https://link.springer.com/article/10.1007/s11238-020-09752-6,Always doing your best? Effort and performance in dynamic settings,October 2020,Nicolas Houy,Jean-Philippe Nicolaï,Marie Claire Villeval,Male,Unknown,Female,Mix,,
89.0,3.0,Theory and Decision,12 June 2020,https://link.springer.com/article/10.1007/s11238-020-09758-0,Responding to (un)reasonable requests by an authority,October 2020,Vittorio Pelligra,Tommaso Reggiani,Daniel John Zizzo,Male,Male,Male,Male,"It is commonly believed that compliance is ubiquitous in social life. People may respond to explicit and implicit requests by modifying their behavior according to what they are requested to do. Managers in organizations may find this particularly helpful, and there is a variety of other contexts where it can also be useful, such as tax compliance and public good contribution (Cadsby et al. 2006; Silverman et al. 2014). An overlooked factor that may influence compliance is the reasonableness of the request received by an authority. When a manager explicitly or implicitly asks someone to do something, it is likely that her willingness to fulfill your request depends on how reasonable she perceives such a request. We try to operationalize the idea of reasonableness and to study its effect on compliance in the context of a fiduciary relationship. Our baseline is a simple trust game in which the trustor has to decide whether or not to send her entire monetary endowment and the trustee, in turn, has to decide what proportion, if any, to send back. We investigate whether and to what extent if someone in a position of authority, such as a manager and in our study the experimenter, asks the trustees to send back positive amounts, this leads to an increase of trustworthiness and trust, and whether this depends on the reasonableness of the request. The request is framed in the form of a message to the trustee saying that the experimenters expect him or her to send a specific share of what she received back to the trustorFootnote 1. We make clear that the subjects are free to do whatever they want and we vary whether or not the trustor is aware of this message, with having this knowledge (or lack thereof) being known to the trustee. Our information manipulation relates to Ellingsen et al. (2010), whose manipulation of disclosing the first order expectation by the trustor to the trustee and determining whether this correlates with the return rateFootnote 2. In the context of our experiment, it works as a robustness test of our findings. This simple situation is intended to describe the basic dynamic underlying agents’ decision to comply with what is perceived to be a changing or an externally imposed norm from someone in a position of authority. Consider a manager requesting a subordinate to fulfill trust to a specific degree towards an external contractor or buyer or colleague. We are interested in describing and testing whether and to what extent, trust and trustworthiness could be fostered or hampered by external intervention, for instance, in this case, by the signals coming from a manager. In real world settings, trustors may not precisely be aware of the specific request by a manager. However, it is plausible that in many cases a customer or external contractor may know, for example, about a company’s expectation for customer service or about a company’s reputation or ethical code of conduct, which imply a managerial expectation about how the customer or external contractor’s company contact will behave towards themFootnote 3. As in Cadsby (2006), Silverman et al. (2014) and Sonntag and Zizzo (2015), and as discussed methodologically in Zizzo (2010), we deliberately use experimental demand as a treatment manipulation, i.e., in our study experimental demand is not a confound but rather a tool of the experimental design—in the case of our paper, to study the reasonableness of demands by the manager and alternative managerial strategies. This tool is additionally useful in our context is because it enables an exogenous and systematic manipulation in both the level and the order of the requests. By doing this, we can have possible requests across the whole range (from none to the whole of the pie) and have them systematically for all of the trustees; and we can have systematically different orders in which we present the requests. The real world examples that we capture are ones where the cooperative behavior by the trustees usually benefits the manager. This could take place directly, as in the example of greater cooperation within an organizational unit yielding to greater productivity, that the manager can take credit for, e.g., performance bonuses. It could also take place indirectly, as in the example of a manager requiring subordinates to behave ethically towards an external contractor, where compliance may simply improve the reputation of the company and only indirectly benefit the manager, or perhaps not at all. That said, there may be cases where the manager may be better off, in terms of financial unit performance that he or she can take credit for, by cutting corners and nudge the employee towards being less trustworthy with an outside contractor or buyer. By having exogenous requests, this paper abstracts from whatever motivation may lead the manager to recommend a higher or a lower level of trust fulfilling, though it considers both cases. We argue that the experimentally induced norm is taken into account if “reasonable,” along two different dimensions, and find evidence that this is the case. Specifically, the level of the request is taken into account by trustees but in a self-biased way: requests may lower returns back to the trustors but do not raise them. The dynamics of requests matters as well: if requests keep ratcheting up, trustees return less than if requests of different size are presented in a random or decreasing order. We also review the extent to which trustors take the static and dynamic reasonableness of the requests into account when deciding whether to trust, and we find evidence to support that they take into account the static reasonableness of the requests, as well as their priors of what is reasonable. The rest of the paper proceeds as follows. Section 2 defines more precisely static and dynamic reasonableness and our experimental hypotheses in the context of our trust games. Section 3 presents the experimental design, while Sect. 4 presents the results, Sect. 5 provides a discussion, and Sect. 6 concludes.",3
89.0,3.0,Theory and Decision,11 May 2020,https://link.springer.com/article/10.1007/s11238-020-09754-4,Effect of reduced opportunities on bargaining outcomes: an experiment with status asymmetries,October 2020,Subrato Banerjee,,,Unknown,Unknown,Unknown,Unknown,,
89.0,3.0,Theory and Decision,08 June 2020,https://link.springer.com/article/10.1007/s11238-020-09760-6,Correction to: Effect of reduced opportunities on bargaining outcomes: an experiment with status asymmetries,October 2020,Subrato Banerjee,,,Unknown,Unknown,Unknown,Unknown,,
89.0,3.0,Theory and Decision,13 April 2020,https://link.springer.com/article/10.1007/s11238-020-09753-5,Equilibrium as compatibility of plans,October 2020,Marek Hudik,,,Male,Unknown,Unknown,Male,"In a market economy, a large number of individuals independently make production and consumption plans. Even though these plans are interrelated in a complex way, they usually turn out to be successfully fulfilled. To account for this coordinating aspect of markets, Hayek (1937, 2007) defined equilibrium as “compatibility of plans.” The Hayekian view of markets has inspired various approaches, such as the theory of market process (e.g., Boettke and Prychitko 1994; Buchanan and Vanberg 1991; Ikeda 1990; Kirzner 1997; O’Driscoll Jr. and Rizzo 2002; Lachmann 1977; Langlois 1986), evolutionary economics (e.g., Loasby 2001; Potts 2000; Witt 2008), or computational economics (e.g., Arthur 2010; Bowles et al. 2017; Vriend 2002). Regrettably, very few of these works attempt to formalize Hayek’s specific notion of equilibrium. Consequently, the differences between the Hayekian and conventional views have yet to be precisely identified. This paper attempts to fill this gap. While many Hayekians are skeptical regarding the usefulness of the equilibrium concept as such, Hayek himself, in his earlier writings, considered equilibrium a useful approximation of the market order (Hayek 2007). However, later he noted that the equilibrium concept is rather unfortunate to serve this particular purpose: for one thing, order is a matter of a degree, while equilibrium does not allow for degrees; for another, order, unlike equilibrium, can be preserved even during a process of change (Hayek 2002, 15). At the same time, Hayek developed his specific notion of equilibrium as the compatibility of plans, which appears to be different from the conventional notion. To illustrate his concept, a seller’s plan to “sell a loaf of bread for at least one dollar” is compatible with a buyer’s plan to “buy a loaf of bread for at most two dollars”; hence the Hayekian condition of equilibrium is satisfied. Useful discussions and elaborations of Hayek’s views include Giocoli (2003), Hudik (2018), Lewin (1997), O’Driscoll Jr. (1977), O’Driscoll Jr. and Rizzo (2002), Rizzo (1990, 1992), and Vaughn (1999, 2013). See also Boland (2017) and Tieben (2012) for a thorough review and discussion of various equilibrium concepts in economics. This paper uses a game-theoretic framework to define the Hayekian notion of compatibility of plans formally. First, a definition of a “plan” is introduced. For this purpose, the conventional definition of a strategic game is extended by adding a set of goals for each player and associating them with the player’s actions. A plan is then defined as a “goal-oriented strategy.” “Sell a loaf of bread for at least one dollar” is an example of such a strategy. Next, two concepts representing the Hayekian notion of compatibility of plans are considered. “Overall compatibility of plans” means that all players achieve all the goals that are part of their plans under all circumstances. “Mutual compatibility of plans” means that there exists a state of nature, in which all players achieve all the goals that are part of their plans. Therefore, although all players may realize their plans under favorable circumstances, these plans may fail if circumstances are not favorable. For example, the bread may be stolen or destroyed. To formalize these concepts, for each player, a “success function” is introduced. This function determines whether players’ goals are achieved in a particular outcome. Players’ payoffs are then derived from goals and their probabilities of success. The two formalizations of the Hayekian notion of compatibility of plans are then compared with the Nash equilibrium. It is shown that the main difference between the compatibility of plans and Nash equilibrium is that while Nash equilibrium is based on the notion of utility maximization, compatibility of plans is not. Therefore, there may be outcomes in which players’ plans are mutually compatible but which are not Nash equilibria. That is, all players are successful in achieving their goals, but one or more players can attain a more valuable goal by changing their behavior. Likewise, there may be Nash equilibria in which players’ plans are not mutually compatible. That is, players cannot unilaterally improve their situation, but one or more players fail to achieve their goals. For example, an outcome in which a seller fails to achieve his goal to “sell a loaf of bread for at least one dollar” may be a Nash equilibrium if the seller has no alternative plan, which would yield him a higher payoff (i.e., either a higher probability of achieving the same goal, or achieving a more valuable goal) given other players’ plans. To account for Hayek’s observation that a social or market order can be preserved in the process of change, this paper introduces two measures, one for the overall compatibility and one for the mutual compatibility, reflecting the degree of the compatibility of plans. Each of these measures ranges from 0 (no individual achieves his goal) to 1 (every individual achieves his goal). These measures highlight Hayek’s point that compatibility of plans across all individuals is a “Platonic” notion that may be approached but is rarely reached in complex societies. They are also an attempt to quantify what it means to be “near equilibrium” and “far from equilibrium” in the Hayekian sense of compatibility of plans. Although this paper is primarily motivated by Hayek’s views on equilibrium, the approach proposed in this paper is quite general and also related to a large body of non-Hayekian literature. Since compatibility of plans and Nash equilibrium generally differ, this paper contributes to the research that expresses dissatisfaction with the Nash equilibrium concept. For example, Brams (1994), Brams and Mattli (1993), and Brams and Wittman (1981) argue that players avoid Nash equilibria that are Pareto-dominated by the status quo outcome. The notion of compatibility of plans proposed in this paper provides another reason why certain Nash equilibria may be unattractive to players—namely, a failure to realize their plans in these equilibria. More recently, Richter and Rubinstein (2019) introduce “maximal normative equilibrium,” which is characterized by the “harmony” similar to the Hayekian compatibility of plans. An important characteristic of the approach proposed in this paper is the explicit modeling of players’ goals. Deriving payoffs from goals is in line with attempts to move toward procedural models of decision-making. From this perspective, the proposed approach is related to satisficing models, in which individuals make decisions with an acceptability threshold in mind (Simon 1955; Caplin et al. 2011). In the present paper, individuals think in terms of discrete goals and make plans to achieve them. A similar approach is adopted by Castelfranchi and Conte (1998), who propose a “goal-based strategy” as an alternative to utility maximization. The model introduced in the present paper is consistent with Castelfranchi and Conte’s (1998) proposal, but, contrary to these authors, it is argued that the concept of goal-orientedness is compatible with utility maximization and can be incorporated into the standard game-theoretic models. A related model is proposed by Dietrich and List (2013a, b), who derive players’ payoffs from their “motivational states.” However, their model does not allow for the analysis of the compatibility of plans. See also Hudik (2014) for a discussion of this model. Goal-orientedness is also considered in the literature on nonstrategic behavior. Bordley and Kirkwood (2004), Bordley and Li Calzi (2000), and Castagnoli and Li Calzi (1996) include goals in the analysis of risky choices. Nevertheless, attempts to analyze behavior in terms of goals—or similar concepts, such as motives or needs—date back to Bentham (1907), Menger (1981), and Marshall (1920), among others. Becker (1998) constructs a formal model with a large number of applications. Weber (1978), Mises (1996), and Robbins (1945) emphasize a means-ends structure of behavior. The notion of goal-orientedness is also employed in many disciplines, including in psychology (Locke and Latham 2002, 2013) and biology (Mayr 1988, 1992; Monod 1972), and it has been traditionally used in cybernetics and systems theory (Rosenblueth et al. 1943; Ashby 1957; Bertalanffy 1968). Therefore, it is natural to formalize goal-orientedness explicitly also in a game-theoretic framework. The rest of the paper is organized as follows: Sect. 2 presents a formal model of the strategic game with goal-oriented strategies. That section also defines overall and mutual compatibility of plans and their relationship. The relationships between compatibility of plans, Nash equilibrium, and Pareto efficiency are also discussed. Section 3 illustrates the proposed model with examples. Two measures that account for various degrees of compatibility of plans across players are introduced in Sect. 4. Section 5 summarizes Hayek’s view of equilibrium as the compatibility of plans. Various applications and extensions of the model are considered in Sect. 6. Section 7 includes a methodological discussion, while Sect. 8 concludes the paper.",5
89.0,3.0,Theory and Decision,02 June 2020,https://link.springer.com/article/10.1007/s11238-020-09755-3,Von Neumann–Morgenstern stable set rationalization of choice functions,October 2020,Vicki Knoblauch,,,Female,Unknown,Unknown,Female,"A choice function on a domain of a set of alternatives selects a chosen set from every subset of alternatives that belongs to the domain. A choice function is rationalized by maximization (M-rationalized) if there exist strict preferences over the set of alternatives such that the chosen set of each member of the domain is the set of its most preferred alternatives, those to which no alternative in that member of the domain is preferred. M-rationalization is a commonly used means of generating choice functions. We will explore a different means of generating choice functions, von Neumann–Morgenstern stable set rationalization (vNM-rationalization), which  Is applicable when a balance between inclusiveness and quality is desired. Here inclusiveness means simply that large chosen sets are desired and quality is measured by adherence to a given set of preferences. The nature and extent of adherence provided by vNM-rationalization will be made apparent in the proof of Proposition 1. (vNM-rationalization promotes inclusiveness more strongly, and quality less strongly, than M-rationalization.) Provides a unique, easily-constructed choice function for a given acyclic, asymmetric binary relation. This will be seen in the proof of Proposition 1 where the reader can see the construction referred to here and relevant to the scenarios presented below. Is M-rationalization when preferences are transitive. This will be proven in Sect. 2.  Von Neumann and Morgenstern (1944) introduced stable sets in the context of cooperative game theory. An asymmetric binary relation on a finite set vNM-rationalizes a choice function on a domain of that set if, for every set in the domain,  no chosen alternative is preferred to any other chosen alternative and for every alternative not chosen, there is a preferred chosen alternative. Then we say that each chosen set is a von Neumann–Morgenstern stable set. Requirement (i) is known as internal stability and promotes quality within the chosen set. Requirement (ii) is external stability and promotes inclusiveness in the chosen set. Roughly speaking, M-rationalization focuses on excellence in chosen sets, while vNM-rationalization strikes a balance between quality and inclusiveness. We now present two scenarios showing how a desire for a balance between quality and inclusiveness might arise in a situation where preferences over alternatives are not transitive and therefore M- and vNM-rationalization may differ (see Example 1 in Sect. 2). Scenario A. A Post-season tournament. A professional sports league needs to select teams for a post-season tournament. The league first ranks the teams by regular season records (using a tie breaker if needed), yielding a transitive, asymmetric binary relation on the set of all teams. Using head-to-head records, the league also creates a binary relation that may contain cycles. The league faces one constraint. They will not place two teams in the tournament such that one of them has a better overall record than and a winning head-to-head record over the other team. Subject to this constraint, the league wants to balance inclusiveness (a large pool of teams is desired) and quality as measured by respect for the two given binary relations. One solution is to construct a choice function that is vNM-rationalized by the intersection of the two given binary relations. The proof of Proposition 1 will demonstrate how this choice function can be constructed and will make apparent the degree of inclusiveness and the nature and extent of adherence to the intersection relation. That the constraints hold for the solution follows from the definitions of vNM-rationalization and the intersection relation. Scenario B. A medical cocktail. A medical researcher wants to create a cocktail of drugs to treat a disease. She has a list of 15 drugs that have been tested individually and ranked according to their efficacy. She also has a list of prohibited pairs of drugs that have exhibited undesired interactions. Subject to this constraint, she wants the cocktail to be as powerful as possible, where power results from the use of a large number of highly effective drugs. Her solution is to construct a choice function that is vNM-rationalized by the intersection of two binary relations, the transitive, asymmetric binary relation representing the efficacy ranking and a binary relation constructed by declaring two drugs to be preferred to each other if they form a prohibited pair. The key feature of the above scenarios is the presence of an acyclic, asymmetric binary relation that is not necessarily transitive. Acyclicity is important because it insures the existence of a choice function vNM-rationalized by the binary relation (Proposition 1). We are interested in intransitivity because M-rationalization and vNM-rationalization of a transitive, asymmetric binary relation produce identical choice functions (Proposition 2). In each scenario the acyclic, asymmetric binary relation that is not necessarily transitive arises as the intersection of a transitive, asymmetric binary relation and a binary relation that does not necessarily possess these properties. In each of the scenarios, the domain of the choice function constructed is made up of a single set, the set of all alternatives. Von Neumann–Morgenstern stable sets were first used in cooperative game theory. In that context, a vNM stable set consists of a set of imputations or allocations satisfying internal and external stability, using an asymmetric binary relation in which allocation x is preferred to allocation y if a coalition favors x over y and a feasibility condition is met. Seminal work includes Von Neumann and Morgenstern (1944), Lucas (1969). Von Neumann–Morgenstern sets have been used in matching problems by Ehlers (2007), Herings et al. (2017), Mauleon et al. (2011), and Wako (2010) in voting studies by Anesi (2006, 2010) in bargaining studies by Anesi and Seidmann (2013), and Kultti and Vartiainen (2007) in the study of the structure of stable sets by Dutta (1988), and Han et al. (2016) and, as in the introduction of von Neumann–Morgenstern stable sets, in cooperative game theory and coalition formation by Greenberg (1990), Diamantoudi and Xue (2007), and Penn (2008). Related stability concepts have been used in the study of tournaments (asymmetric, complete binary relations) by Anesi (2012), Brandt (2011), Brandt et al. (2018), and Vartiainen (2015). The area of choice theory to which this study belongs is the theory of rational choice by a single agent, an area with pioneering works by Samuelson (1938), Houthakker (1950), and Afriat (1967) ( as opposed to social choice theory, which involves aggregating preferences of individuals to form group preferences and includes foundational work by Arrow (1950), and Sen (1970) ). Rational choice began as the theory of revealed preference, which concerned itself with utility-function rationalization of choice subject to budget constraints. The area has evolved to include rationalization of choice functions over arbitrary sets of alternatives by preferences in the form of binary relations that are not necessarily transitive, and therefore not representable by a utility function. A central position in this field is held by a body of work by a small team of authors: Bossert et al. (2006) and Bossert and Suzumura (2009, 2010, 2012). In particular, the study by Bossert et al. (2006) is notable for its broad scope. Its numerous characterizations involve arbitrary domains and binary domains whose subsets have cardinality 1 or 2; binary relations that are transitive, quasi-transitive, acyclic, reflexive or complete, or possess some combination of these properties; and two rationalization schemes: M-rationalization in which, as described above, an alternative is chosen from a set if there is no alternative preferred to it and G-rationalization, where G stands for greatest, in which an alternative is chosen if it is preferred to all others in the set. In this study we consider only vNM-rationalization by asymmetric binary relations. In that context, every asymmetric binary relation \({\mathcal {P}}\) that G-rationalizes a choice function on a given domain also M-rationalizes a choice function on that domain, since the \({\mathcal {P}}\)-greatest element of a set in the domain is also a \({\mathcal {P}}\)-maximal element of that set. However, the asymmetric binary relation \({\mathcal {P}}\) on \(X=\{a,b,c\}\) defined by \(a{\mathcal {P}}c\) and \(b{\mathcal {P}}c\) M-rationalizes a choice function C on domain \(D=\{X\}\) (namely \(C(X)=\{a,b\}\)), but \({\mathcal {P}}\) does not G-rationalize a choice function on D, since X possesses no \({\mathcal {P}}\)-greatest element. It was important that some motivation was supplied for our exploration of vNM-rationalization of choice functions by asymmetric binary relations. In Scenarios A and B, preferences were represented by acyclic, asymmetric binary relations, which arose via the intersection of two binary relations. It was also important that these scenarios gave rise to binary relations that were not transitive, since, as we will see, transitive, asymmetric binary relations generate the same choice functions under M-rationalization as under vNM-rationalization. The two scenarios show that preferences represented by binary relations with intransitivities are not as unrealistic as one might think. There is further evidence for this claim: plurality-wins, head-to-head voting by voters with transitive preferences can generate any binary relation (see McGarvey 1953; Knoblauch 2016). The rest of the paper is organized as follows. Section 2 contains a few preliminaries, a formal definition of vNM-rationalization, two propositions concerning vNM-rationalization by acyclic, asymmetric binary relations and by transitive, asymmetric binary relations respectively and a few relevant remarks and examples. Section 3 characterizes choice functions vNM-rationalizable by acyclic, asymmetric binary relations. In summary, the introduction motivates the use of vNM-rationalization to construct a choice function from a given binary relation and Sect. 2 explores the mechanics of such constructions. In contrast, the problem addressed in Sect. 3 asks whether a given choice function is vNM-rationalizable by an acyclic, asymmetric binary relation. Section 4 concludes.",1
89.0,4.0,Theory and Decision,24 June 2020,https://link.springer.com/article/10.1007/s11238-020-09756-2,Empathy and socially responsible consumption: an experiment with the vote-with-the-wallet game,November 2020,Vittorio Pelligra,Alejandra Vásquez,,Male,Female,Unknown,Mix,,
89.0,4.0,Theory and Decision,06 July 2020,https://link.springer.com/article/10.1007/s11238-020-09763-3,The differential impact of friendship on cooperative and competitive coordination,November 2020,Gabriele Chierchia,Fabio Tufano,Giorgio Coricelli,Female,Male,Male,Mix,,
89.0,4.0,Theory and Decision,17 June 2020,https://link.springer.com/article/10.1007/s11238-020-09762-4,On the characterizations of viable proposals,November 2020,Yi-You Yang,,,Unknown,Unknown,Unknown,Unknown,,
89.0,4.0,Theory and Decision,13 July 2020,https://link.springer.com/article/10.1007/s11238-020-09761-5,Violations of coalescing in parametric utility measurement,November 2020,Andreas Glöckner,Baiba Renerte,Ulrich Schmidt,Male,Female,Male,Mix,,
89.0,4.0,Theory and Decision,26 July 2020,https://link.springer.com/article/10.1007/s11238-020-09767-z,Explaining satisficing through risk aversion,November 2020,Yudistira Permana,,,Unknown,Unknown,Unknown,Unknown,,
89.0,4.0,Theory and Decision,13 August 2020,https://link.springer.com/article/10.1007/s11238-020-09768-y,Correction to: Resolving Zeckhauser’s paradox,November 2020,Yudi Pawitan,Gabriel Isheden,,Unknown,Male,Unknown,Male,"On page 599 in the original publication of the article, we state that This is a typo: the inequality sign is incorrect. For example, according to Preference P1, \( x_{10} > x_{51} \). So \( A^{*} x_{10} \) means being alive with a larger debt than in \( A^{*} x_{51} \). So, by Preference P2, \( u(A^{*} x_{51} ) > u(A^{*} x_{10} ) \). The same argument should hold for the other inequalities. So, we should have: The specific utilities in the paper were constructed using these correct inequalities, so there is nothing wrong in the numbers. We apologize for any confusions from this typo.
",
90.0,1.0,Theory and Decision,04 January 2021,https://link.springer.com/article/10.1007/s11238-020-09789-7,Philippe Mongin (1950–2020),February 2021,Jean Baccelli,Marcus Pivato,,Male,Male,Unknown,Male,,1
90.0,1.0,Theory and Decision,08 September 2020,https://link.springer.com/article/10.1007/s11238-020-09774-0,Set and revealed preference axioms for multi-valued choice,February 2021,Hans Peters,Panos Protopapas,,Male,Male,Unknown,Male,"This paper contributes to a question with a long history. A (single-valued) choice function assigns to each choice set (i.e., subset of the set of all alternatives) an element of that choice set. The condition of independence of irrelevant alternatives (IIA) requires that if the alternative chosen from a choice set is still available in a subset of the choice set, then it should also be chosen from that subset. This condition already occurs as condition no. 7 in Nash’s axiomatic bargaining model (1950).Footnote 1 Closely related is the Weak Axiom of Revealed Preference (WARP), which says that if x is chosen from some choice set where also y is available, then y should never be chosen from any choice set where also x is available.Footnote 2 Another way of phrasing this, is that the preference relation revealed by the choice function has no cycles of length two. As is well-known and easy to show, for collections of choice sets that are closed under nonempty intersection, IIA and WARP are equivalent.Footnote 3 In this paper, we consider choice correspondences instead of choice functions: these assign to each choice set a nonempty subset, rather than a unique element. In the classical choice literature, often the expression ‘choice function’ is used where we use ‘choice correspondence’. The main interpretation that we have in mind is that, indeed, a set is chosen, rather than a short-list from which later a singleton has to be chosen—for instance, a set of courses (starter, main, dessert) and a selection of wines in a restaurant; or a committee of fixed or variable size in a society or university. This differs from the, often employed, interpretation that the chosen set is a kind of preamble or, indeed, short-list for the final winner, although this interpretation is not excluded by our approach.Footnote 4 Our purpose is to consider extensions of IIA to this model, and their relation with an extended version of WARP. All these extensions can already be found in the existing literature, but we will establish some relations which, to the best of our knowledge, are new. Moreover, in the last part of the paper (Sect. 4) we study, in detail, choice correspondences that satisfy the strongest extension of IIA, by means of so-called strong sets: choice sets that uniquely determine the choice correspondence, as will be explained in more detail below. We consider three extensions of IIA for choice functions to choice correspondences. The probably earliest extension of IIA to choice correspondences has again been proposed by Nash in an informal note in 1950 (see Shubik 1982, p. 420): if F is the choice correspondence, X is a choice set, and F(X) has a nonempty intersection with a subset Y of X, then F(Y) is equal to this intersection. The same condition also appears as Postulate 6 in Chernoff (1954) and Condition C4 in Arrow (1959). This extension is a natural one if the set of alternatives chosen by a choice correspondence is viewed as the set of best alternatives (in some sense or another) among the available ones: consequently, each of these alternatives is also best in any subset of available alternatives to which it belongs. We refer to this extension simply again as Independence of Irrelevant Alternatives (IIA).Footnote 5 The second extension we consider requires, in the same situation, that, F(Y) be contained in the intersection of F(X) and Y. Following a similar interpretation, F still chooses among the best alternatives, but not necessarily all available ones. Think of choosing a committee within a society: for a subset of the society one may need to choose a strictly smaller committee, even if more members of the original committee are still available. Or, in terms of a restaurant’s menu choice, not only the lunch menu may be a subset of the dinner menu, but also lunch itself may be lighter than dinner: one may want to consume wine of just one kind instead of several, even if more kinds are still available. This extension appears as condition W2 in Schwartz (1976). Clearly, it is a weakening of the first extension, and we call it Weak IIA (WIIA). The third extension to be considered, says the following. If Y is contained in X and contains F(X), then F(Y) is equal to F(X). One interpretation is clear: if we regard F(X) as the set of best choices from X, and all these choices are still available in a smaller set Y, then they are also the best choices in Y. In terms of the restaurant’s menu: if the collection of available dishes decreases but the courses that we chose earlier are still available, then we choose them again. Clearly, this extension is another weakening of IIA, and we call it Restricted IIA (RIIA). Also this condition has appeared before in the literature. See Aizerman (Aizerman 1985, Definition 1), and for instance Johnson and Dean (Johnson and Dean 2001, Lemma 2). For the case of finitely many alternatives, it coincides with the ‘attention filter’ condition in Masatlioglu et al. (2012). It is also equivalent to ‘Condition \(\hat{\alpha }\)’ in Brandt and Harrenstein (2011), see below; and it appears as ‘Irrelevance of Rejected Items’ in Alva (2018).Footnote 6 As to revealed preference for sets, our main condition is the following. If, from some choice set Z, the subset X is chosen while also Y is available (i.e., Y is also a subset of Z), then Y should never be chosen from some other choice set \(Z'\) if also X is available. This is an obvious extension of the Axiom of Revealed Preference for choice functions to choice correspondences, and we use the same name for it, again abbreviated to WARP. The expression WARP is also used in Alva (2018) for the same condition. Brandt and Harrenstein (2011) consider the condition of ‘set-rationalizability’, which looks very similar to WARP. Indeed, the two conditions turn out to be equivalent (see Sect. 2). In Sect. 2, we formally introduce the model, and the conditions IIA, WIIA, RIIA, and WARP. The set of alternatives is an arbitrary finite or infinite set without any additional structure imposed. In Sect. 3, we establish relations between these conditions. Theorem 3.1 says that RIIA and WARP are equivalent. This is in line with Theorem 1 in Alva (2018). It is also in line with results in Brandt and Harrenstein (2011). Specifically, they consider a property called Condition \(\hat{\alpha }\) and show (their Theorem 2) that this condition is equivalent to what they call ‘set-rationalizability’. They also show (their Lemma 1) that Condition \(\hat{\alpha }\) is equivalent to RIIA (they do not use this name). Since RIIA is equivalent to WARP, it follows that their set-rationalizability condition is equivalent to WARP: i.e., it excludes cycles of length two in the revealed preference relation on choice sets. We also show that IIA is stronger that WARP; Theorem 3.6 says, that IIA is equivalent to WARP combined with a condition called preference axiom (PA).Footnote 7 This condition PA says that if a set X is revealed preferred to a set Y and a subset Z of X contains the intersection of X and Y, then also Z is revealed preferred to Y. In the special case that X and Y are disjoint, PA thus implies that every subset of X is revealed preferred to Y. On a general note, consider the interpretation of a choice correspondence as picking the set of ‘best’ alternatives (from which, perhaps, later on a definite choice can be made, like in the case of a short-list). As mentioned, this interpretation is reflected in particular by IIA. Put differently, complementarities between alternatives are not considered. Revealed preference on sets, however, may reflect such complementarities. To illustrate this, suppose a diner must choose a meal from two dishes, steak s or fish f, and two wines, red r or white w. When everything is available the diner selects steak and red wine, hence \(F(\{s, f , r,w\}) = \{s,r\}\) and \(\{s,r\}\) is revealed preferred to \(\{f,w\}\). However, if the restaurant runs out of red wine the diner selects fish and white wine, hence \(F(\{s, f , w\}) = \{f,w\}\) and \(\{f,w\}\) is revealed preferred to \(\{s\}\).Footnote 8 This is a violation of IIA but it is not difficult to extend the definition of F so that it satisfies WARP. Our axiom PA on revealed preference excludes such a situation: with \(X=\{s,r\}\), \(Y=\{f,w\}\), and \(Z=\{s\}\), it would require that \(\{s\}\) is revealed preferred to \(\{f,w\}\). We further show (Theorem 3.8) that IIA implies that the revealed preference relation is transitive and acyclic.Footnote 9 The remainder of Sect. 3 is devoted to studying the relation between WIIA and WARP. One of the results (Theorem 3.11) says that if the choice correspondence F is a projection (i.e., \(F = F \circ F\)), then WIIA implies WARP. In turn, WARP implies that F is a projection, as is also observed in Brandt and Harrenstein (2011). If the set of all alternatives is finite, then WIIA implies that a sufficiently often repeated version of the choice correspondence satisfies WARP (Corollary 3.13). In Sect. 4, we return to the IIA condition and consider so-called ‘strong sets’. A set of alternatives S is a strong set at a choice correspondence if to any choice set X which has a nonempty intersection with S, the choice correspondence assigns exactly this intersection. We show that under IIA these strong sets form a partition of the set of all alternatives, the elements of which are completely and acyclically ordered by the revealed preference relation (Theorem 4.6). This way, these strong sets characterize the (IIA) choice correspondence. Following a tradition initiated for consumer theory by Samuelson (1938) and Houthakker (1950), and continued for general choice problems by—among others—Arrow (1959) and Richter, (1966), there is a large literature focusing on ‘rationalizability’: when does a choice correspondence pick the set of those alternatives that are maximal for some binary relation on the set of alternatives? Arrow (1959) shows that a choice correspondence is rationalizable by a complete and transitive binary relation if and only if it satisfies IIA. Sen (1971) shows that a choice correspondence is rationalizable by a binary relation if and only if it satisfies the Chernoff property (see Sect. 1.6.2) and a condition, proposed as Property \(\gamma \), but later also referred to as the Expansion condition (e.g., Moulin 1985).Footnote 10 Adding to this the Aizerman condition (see again Sect. 1.6.2) results in the choice correspondence being rationalizable by an ordering which is complete and has a transitive strict part (Schwartz 1976; Moulin 1988). Finally, Aizerman and Malishevski (1981) show that a choice correspondence satisfies both the Chernoff property and the Aizerman condition if and only if it is pseudo-rationalizable by a collection of single-valued, complete, and transitive orderings; that is, if in each choice set, the choice correspondence picks the maximal elements of all the orderings in this collection. In our context, rationalizability would naturally mean that there is a binary relation on the set of choice sets (rather than single alternatives) such that the choice correspondence picks that subset from a choice set that is maximal according to this binary relation. In our model, this is equivalent to WARP. We refer to Theorem 1 in Alva (2018) for a formalization of this observation. A further requirement would be that this relation is representable by a (utility) function, but for this, the binary relation should be acyclic. Apart from IIA, WIIA, and RIIA, there are other possible extensions of IIA from choice functions to choice correspondences. A fourth possible extension says that, if \(Y\subseteq X\) and \(F(X) \cap Y \ne \emptyset \), then F(Y) should at least contain the intersection of F(X) and Y. As an interpretation, it could be that additional best alternatives become available in the smaller set. For instance, a first preferred choice of wine from a restaurant’s menu is no longer available, making a second preferred choice a best alternative (additional to the still available best menu choices). This condition has first been proposed as Postulate 4 in Chernoff (1954), and has consequently been referred to as the Chernoff property (e.g., Moulin 1985; 1988). It also appears as Property \(\alpha \) in Sen (1971). Fifth, a still weaker version of the first three conditions is the following (e.g., Fishburn 1973): if F(X) is contained in Y, then F(Y) should be contained in F(X). This condition, studied in Aizerman and Malishevski (1981), is usually referred to as the Aizerman condition; it is implied by Condition W3 in Schwartz (1976). In Sect. 2, we introduce the model and the main conditions on a choice correspondence that we consider. Section 3 studies the relations between these properties. Section 4 introduces the collection of strong sets and studies its relation with IIA. Section 5 concludes with a summary of the main results of the paper and a few open questions.",
90.0,1.0,Theory and Decision,17 September 2020,https://link.springer.com/article/10.1007/s11238-020-09773-1,Good decision vs. good results: Outcome bias in the evaluation of financial agents,February 2021,Christian König-Kersting,Monique Pollmann,Stefan T. Trautmann,Male,Female,Male,Mix,,
90.0,1.0,Theory and Decision,06 August 2020,https://link.springer.com/article/10.1007/s11238-020-09769-x,A unified epistemological theory of information processing,February 2021,Áron Tóbiás,,,Male,Unknown,Unknown,Male,"Uncertainty is at the heart of virtually all decision-making problems. In order to describe how much information is possessed by economic agents in abstract mathematical terms, a rich selection of technical apparatuses is available to inform decision-theoretic models of choice under uncertainty. Two leading statistical paradigms of modeling information and knowledge stand out. One of these paradigms represents information as measure-theoretic structures (\(\sigma\)-algebras) with the implicit informal understanding that finer measure-theoretic structures (larger \(\sigma\)-algebras) represent “more information.”Footnote 1 This intuition is ubiquitous in modern probability theory (Billingsley 1995), as witnessed by such concepts as conditional expectations, filtrations, and martingales, and their applications in economics, finance, and decision theory. Another widely used approach to the problem of restricted information relies on partitions of an abstract space modeling the unknown state of the world (for a summary, see Aumann 1999a, b). While both of these approaches are widely recognized and used, the rigorous operationalization of precisely what a person “knows” can be elusive and prone to inconsistencies. Indeed, Dubra and Echenique (2004) highlight and elaborate on a puzzling paradox—building upon a counterexample originally proposed by Billingsley (1995)—in which the measure-theoretic and partition-based interpretations of information come into conflict with each other, demonstrating how the construction and structure of \(\sigma\)-algebras are at apparent odds with the intuitive interpretation of the information they supposedly “contain.” Such inconsistencies lead to a perplexing conclusion according to which a decision-maker may wind up preferring “less” information to “more.”Footnote 2 These examples illustrate that the question of how to reconcile partitions and \(\sigma\)-algebras to model information is a controversial issue ridden with paradoxes—an issue of burning concern, given that many economic and epistemological models operationalize knowledge by means of combining the two approaches.Footnote 3 The goal of this paper is to contribute to the resolution of the apparent contradictions between the two mainstream approaches to information by proposing a unified epistemological theory of information processing in terms of abstract statistical concepts—“unified” in the sense that it combines the measure-theoretic and partition-based paradigms in a meaningful manner. In the paradoxical example presented by Dubra and Echenique (2004), partitions are converted into \(\sigma\)-algebras by means of the smallest \(\sigma\)-algebra containing the partition cells. I argue that this \(\sigma\)-algebra may be too small for the reason that it allows the combination of only countably or co-countably many partition cells (Proposition  1), resulting in the failure of finer partitions to generate larger \(\sigma\)-algebras. From an epistemological perspective, this may bring about a subtle underestimation of the capabilities of the observer’s mind with regard to performing an uncountable number of aggregative operations.Footnote 4 One solution to this problem has been offered by Hervés-Beloso and Monteiro (2013), who propose the concept of informed set. The family of informed sets consists of those subsets of the state space that are given as (not necessarily countable or co-countable) unions of the partition cells. This family is a \(\sigma\)-algebra and the informed-set operation is order-preserving, in the sense that finer partitions correspond to larger \(\sigma\)-algebras and vice versa. However, the \(\sigma\)-algebra of informed sets presents a different methodological challenge: it often turns out to be too large to admit meaningful probability measures. The approach I propose in this paper to modeling the information encoded by a partition as a \(\sigma\)-algebra relies on the concept of information generated by the partition. This construction is based on the observation that any partition can be equivalently represented as a signal—a function f mapping from the state space into an outcome space Y a decision-maker can observe. Given a base \(\sigma\)-algebra \({\mathscr {M}}\) on the state space, the function f can be used to construct a natural measurable structure on the signal space by considering the \(\sigma\)-algebra \(f({\mathscr {M}})\) of those subsets of Y whose pre-images under f are \({\mathscr {M}}\)-measurable subsets of the state space. The information generated by the signal is then defined as the smallest \(\sigma\)-algebra on the state space that makes f measurable when the target space is endowed with \(f({\mathscr {M}})\). It turns out that this \(\sigma\)-algebra on the state space coincides with the collection of those sets that (i) can be expressed as unions of the cells of the partition corresponding to the signal; and (ii) are also \({\mathscr {M}}\)-measurable (Proposition 2). Therefore, by intersecting the family of informed sets (as defined by Hervés-Beloso and Monteiro 2013) with a suitably chosen base \(\sigma\)-algebra \({\mathscr {M}}\), one can keep the resulting measurable space representing information encoded by a partition from becoming too large, thereby making non-trivial probabilistic analysis possible. The \(\sigma\)-algebra thereby generated by a partition (or, equivalently, by a signal) turns out to coincide with the collection of K-informed events introduced by Lee (2018). Nevertheless, there are three important differences. First, the construction of Lee (2018) relies on the concept of knowledge operator adopted from Aumann (1999a, b), expressing the set of states of the world in which an observer is informed of the occurrence of a given event, whereas the present paper motivates the construction of generated information using a signal-processing argument. Second, Lee (2018) restricts attention to Polish spaces and assumes that the base \(\sigma\)-algebra \({\mathscr {M}}\) on the state space is a strongly Blackwell \(\sigma\)-algebra.Footnote 5 By contrast, the concept of generated information does not rely on any topological or regularity assumptions imposed on the state space or the base \(\sigma\)-algebra on it, thereby generalizing the insights gleaned from Lee (2018) beyond the realm of strongly Blackwell \(\sigma\)-algebras and countably generated \(\sigma\)-subalgebras thereof. Third, upon fixing a given strongly Blackwell \(\sigma\)-algebra on the state space, Lee (2018) establishes an equivalence between its countably generated \(\sigma\)-subalgebras and the family of partitions satisfying a measurability condition. I show that this equivalence ceases to hold in general upon dispensing with these regularity conditions (Proposition  3), indicating that generic \(\sigma\)-algebras tend to encode more information about an observer’s overall knowledge than can be captured by partitions alone. I also revisit the fundamental point Blackwell (1951, 1953) made, according to which if one statistical experiment is more informative than another, then any decision-maker prefers having access to the former to having access to the latter, and vice versa. Dubra and Echenique (2004) and Hervés-Beloso and Monteiro (2013) present a reformulation of Blackwell’s theorem in which decision-makers’ preferences over information sources are expressed in order-theoretic terms. I demonstrate how the concept of information generated by a signal proposed in this paper allows the presentation and extension of Blackwell’s theorem in a way that is consistent with expected-utility maximization—a procedure that requires measure-theoretic and probabilistic arguments (Proposition  4). Such a characterization of decision-makers’ preferences for information can prove useful in a variety of contexts, given the prevalent reliance of models of choice under uncertainty on expected-utility theory. The rest of the paper proceeds as follows. Section  2 reviews the two paradoxical examples presented by Billingsley (1995) and Dubra and Echenique (2004). Section 3 presents the setup. Section  4 introduces the concept of generated information. In more detail, Section 4.1 motivates the need for representing the information content of a partition by means of a \(\sigma\)-algebra intermediate between the traditional notion of generated \(\sigma\)-algebra and the family of informed sets introduced by Hervés-Beloso and Monteiro (2013). Section 4.2 provides the construction of generated information. Section 4.3 discusses its relation to the theory of Lee (2018), offers a resolution to the paradox presented by Dubra and Echenique (2004), and illustrates the information content of non-measurable partitions. Section 4.4 examines how generated information interacts with the combination of multiple signals and highlights a potential shortcoming of the concept. Section  4.5 explores the other direction: how to construct a partition from a \(\sigma\)-algebra taken as given. Section 5 investigates the decision-theoretic aspects of the concept of generated information and revisits Blackwell’s theorem from an expected-utility-maximization perspective, according to which signals can be unambiguously ordered in an intuitive way and a decision-maker always prefers signals containing larger amounts of generated information. Section 6 concludes.",2
90.0,1.0,Theory and Decision,14 July 2020,https://link.springer.com/article/10.1007/s11238-020-09766-0,Welfare implications of non-unitary time discounting,February 2021,Ryoji Ohdoi,Koichi Futagami,,Male,Male,Unknown,Male,"Father: “Could you mow the yard tomorrow instead of playing football? After completing the job, I will give you $20.” Son: “Really? I will. Then, I can buy a new computer game!” Tomorrow has come. Father: “Why are you going out to play football? Mow the yard! You promised yesterday, didn’t you?” Son: “Sorry Dad. I no longer think $20 is enough for the job.” Why did the boy break his promise? Is it because he is a liar? Of course, there are a number of possible answers to this question. One possibility, suggested by a large body of experimental evidence, is that preference reversals frequently occur over time in people’s decision-making. As such, it could be that the boy first regarded $20 (or purchasing a new game) as preferable, but by the following day, preferred his leisure activity. Although only one among many hypothetical answers, this possibility becomes convincing once we consider the domain effect, or domain independence, often referred to in the experimental psychology literature. The domain effect emerges when the discount rates (or factors) differ depending on their domains. For example, the research group led by Gretchen B. Chapman, in a string of the studies, reports that discount rates are specific to money and health status, respectively.Footnote 1 In addition, according to Soman (1998, 2004), such domain-specific discounting is also observed for money and time effort. Both of these studies indicate the possibility that people generally discount future monetary rewards/costs differently from future health status or time effort. In the abovementioned example, the domain effect emerges if the boy discounts the utility from the monetary reward ($20) and that from enjoying the leisure activity (football) differently. For expositional convenience, let R denote the utility from the monetary reward and F denote the utility from the leisure activity. We assume \(R<F\); that is, the boy will never mow the yard if asked to do so right now. Next, suppose that, on the first day, he evaluates the utility from receiving $20 as \(\beta _1 R\), and that from playing football as \(\beta _2 F\), where \(\beta _1 \in (0, 1)\) and \(\beta _2\in (0, 1)\) are the discount factors specific to the monetary reward and leisure, respectively. Then, if the boy discounts enjoying leisure steeply enough such that \(\beta _1 R>\beta _2 F\), he will accept his father’s job offer on the first day. Hereafter, we refer to such domain-specific discounting as non-unitary discounting. If an individual discounts her future utilities in a non-unitary way, this can make her decisions time inconsistent. There has been a recent upsurge of interest in models of time-inconsistent preferences, as pioneered by Strotz (1955) and Pollak (1968).Footnote 2 In this context, the individual’s decision-making process is formulated as a dynamic non-cooperative game played by her different selves across time, where the current self is aware that her preferences might change in future, and takes this into account when making the current decision.Footnote 3 However, much of the literature focuses on a class of quasi-hyperbolic discounting proposed by Phelps and Pollak (1968), and popularized by Laibson (1997). Therefore, the purpose of this study is to develop a simple dynamic theory of non-unitary discounting. In this study, we develop a simple model of non-unitary time discounting and pursue its welfare implications. As in Hori and Futagami (2019), an individual discounts her one-period utility functions of consumption and leisure differently. As the boy does in the earlier example, the individual changes her mind about the relative importance of consumption and leisure as time progresses. Within this framework, we compare welfare achieved in the market economy from welfare in the planner’s allocation from the perspective of all selves across time. The results are no longer straightforward, because as a result of a lack of commitment, each self of the social planner is also involved in strategic interactions with her other selves. In fact, in their model with quasi-hyperbolic discounting, Krusell et al. (2002) show that the allocation in the market economy surprisingly attains strictly higher welfare than that in the planning allocation. Hiraguchi (2014) extends Krusell et al. (2002) to a general model of non-constant discounting, including the original as a special case, and shows that their result is robust. At the same time, a welfare comparison between the competitive and planning economies gives rise to the following problem. To correctly identify which achieves higher welfare in each period, we must control the difference in the dynamics of the state variables between the two economies. In other words, we cannot evaluate which of the economies performs better if we focus only on their overall paths.Footnote 4 Then, we conduct a welfare comparison in two distinct ways. First, we consider the hypothetical situation in which, in an arbitrarily given period, a self faces the same value of a state variable in both economies. It is shown that welfare in the social planning case is always strictly higher than that in the market economy. This means that welfare improvement is always possible from the realized allocation in the market economy, which contrasts sharply with the findings of Krusell et al. (2002). Second, we undertake a welfare comparison between the overall paths of the two economies. We show that whether the planning allocation is more Pareto efficient than the allocation in the market economy depends on the relative degree of impatience. The following two cases arise. If the individual discounts future leisure more steeply than she does future consumption, the planning allocation is preferable to the laissez-faire allocation for all her selves. However, if the reverse is the case, they are not Pareto ranked. In this case, we show that there is a unique threshold period before which any selves strictly prefer the planning allocation. However, after this period, they strictly prefer the laissez-faire allocation. This means that the allocation in the market equilibrium may achieve a more desirable outcome than the social planner does for the selves in later periods. As already stated, the most closely related literature to our study is the set of studies on time-inconsistent preferences resulting from non-geometric discounting. However, our model is also related to a class of preferences exhibiting temptations. Among others, Banerjee and Mullainthan (2010) consider a two-period, many-good economy, and classify the goods into two types. The first is a standard good, the consumption of which in both periods yields the individual’s lifetime utility in period 1. The second is a “temptation good,” the consumption of which in period 2 is not valued in period 1, but yields utility once period 2 has arrived. In their two-period model, temptation goods are interpreted as those with a discount factor of 0. If we set \(\beta _2=0\) in the example at the beginning of the introduction, playing football is a temptation good for the boy. Thus, our model of non-unitary discounting is closely related to their notion of temptation.Footnote 5 The remainder of this paper is organized as follows. Section 2 gives the illustrative example of non-unitary discounting, and explains why the time-inconsistency problem arises. It also provides the Euler equation in this model. Section 3 then extends the framework to a dynamic general equilibrium model and characterizes the market equilibrium. Section 4 compares welfare in the market economy to that in the social planner’s allocation. It also shows the existence of the time-consistent policies by the benevolent government. Section 5 concludes.",1
90.0,1.0,Theory and Decision,28 August 2020,https://link.springer.com/article/10.1007/s11238-020-09771-3,Rank-dominant strategy and sincere voting,February 2021,Yasunori Okumura,,,Male,Unknown,Unknown,Male,"Gibbard (1973) and Satterthwaite (1975) show that when there are three or more alternatives, sincere voting is always the voter’s dominant strategy only if it is dictatorial. This result is well known as the Gibbard–Satterthwaite theorem.Footnote 1 This theorem implies that it is hard to make a voting rule that is not dictatorial and any voters always sincerely vote. We propose a weaker solution concept than dominant strategies to escape from the Gibbard–Satterthwaite theorem. We roughly introduce the concept of rank-dominant strategies, which is newly introduced in this study and reasonable when a voter has no information on the preferences of the other voters. For each fixed strategy of a player, a vector of possible payoffs of the player sorted in the descending order is defined, where the number of elements of the payoff vector for each strategy of a player is equal to the number of possible combinations of the strategies of the other players. A rank-dominant strategy is a strategy where the kth element of its payoff vector is not lower than that of any other strategy for each k. Ballester and Rey-Biel (2009) discuss a similar problem to ours.Footnote 2 See also Majumdar and Sen (2004) with regard to a discussion of a similar problem to ours. Ballester and Rey-Biel (2009) derive a class of voting rules where a player without any information about other players’ preferences sincerely votes. They assume a simple way for players to resolve the uncertainty about others’ preferences. To be more precise, a player without any information is assumed to (i) maximize his/her expected utility given his/her belief, (ii) believe that any possible combination of others’ strategies is equally probable and (iii) have an additive belief. To explain the way to resolve the uncertainty, we give a two-voter and two-candidate example. The voting rule is simple plurality. Then under the assumptions, a player believes that the other player votes each of the candidates with the probability of 1/2 and maximizes her/his expected utility under the belief. In other words, they restrict their attention to an uncertainty (or ambiguity) neutral player. On the contrary, we do not assume the third assumption of them; that is, the belief of a player allowed to be non-additive. This implies that in this study, players with various uncertainty attitudes are considered. Since the belief of a player is non-additive, the Choquet expected utility, which is introduced by Schmeidler (1989), of a player is considered. To be more precise, a player (i) maximizes his/her Choquet expected utility given his/her belief and (ii) believes that any possible combination of others’ strategies is equally probable. We show that if the player has some rank-dominant strategies, then he/she chooses one of them. Moulin (1981) also studies strategic voting in a low information situation. He introduces the concept of prudent strategies.Footnote 3 A prudent strategy is a strategy that the payoff vector of the strategy of a player, which is sorted in the ascending order, is lexicographically higher than that of any other strategy. An interpretation of the concept is that a player decides a strategy by considering the worst outcome of the strategies in the first place, the second worst outcome of them in the second place, ... . On the other hand, Naeve (2000) and Naeve and Revilla (2004) assume that a player decides a strategy by considering the first-best outcome of the strategies in the first place, the second-best outcome of them in the second place, ... In this study, we examine a more general way to decide a strategy such that a player decides a strategy by considering the xth-best outcome of the strategies in the first place, the yth-best outcome of them in the second place, ... , where x and \(y(\ne x)\) are arbitrary integers that are not more than the number of combinations of strategies of the other players. If there are some rank-dominant strategies of a player, then one of them is chosen by the player and, therefore, is also a prudent strategy of the player. We summarize our results on voting rules. First, we consider a deterministic tie-breaking rule. We show that in a class of voting rules discussed by Majumdar and Sen (2004), any sincere strategy of a player is a rank-dominant strategy of the player. However, in this class, anonymity is not always satisfied. Therefore, second, we consider a tie-breaking rule that is allowed to be probabilistic. We show that the set of sincere strategies of a player is equivalent to that of rank-dominant strategies of the player under the plurality rule with equal probability random tie-breaking, which satisfies three basic properties including anonymity. Further, we consider general scoring rules, but we fail to generalize the result; that is, sincere voting may not be a rank-dominant strategy of the player even under the equal probability tie-breaking rule. Hence, whether a player without any information sincerely votes or not is also dependent on weights of the scoring rule. This result contrasts sharply to a result of Ballester and Rey-Biel (2009), because a voter who resolves the uncertainty in the simple way briefly introduced above must sincerely vote under the scoring rule regardless of the weights.",
90.0,1.0,Theory and Decision,07 September 2020,https://link.springer.com/article/10.1007/s11238-020-09772-2,"A theory of instrumental and existential rational decisions: Smith, Weber, Mauss, Tönnies after Martin Buber",February 2021,Elias L. Khalil,Alain Marciano,,Male,Male,Unknown,Male,"How and how effectively organizations function depends on how the respective members of the organization behave given the context of transactions and social goals that organizations set up. This is not a particularly new insight. Indeed, the literature across social theory, behavioral decision theory, behavioral economics, social exchange theory, and organization theory has largely emphasized that the prosocial behavior of members matters for the functionality of organizations (e.g., Homans 1961; Blau 1974; Coleman 1990; Etzioni 1986; Granovetter, 1973; Bowles and Gintis 2002; Brief and Motowidlo 1986; Bowler and Brass 2006; De Dreu 2006; De Dreu and Nauta 2009; Ehrhart and Naumann 2004; Grant 2008; Meglino and Korsgaard 2004, 2006; Michel 2017). This wide-ranging literature commonly draws a line between behaviors that are self-regarding as opposed to other-regarding. While the self/other juxtaposition has been useful in delineating self-interest from other-interest (i.e., altruism), the juxtaposition has led generally to the hegemony of a narrow twofold taxonomy in the social sciences and, particularly, in economics (see Khalil and Marciano 2021). The canonical expression of this twofold taxonomy in economics is the oft-quoted phrase of Adam Smith from the outset of his Inquiry into the Nature and Causes of the Wealth of Nations, concerning the self-interest motive of market-centered transactions with the butcher, the brewer, or the baker: It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner, but from their regard to their own interest. We address ourselves, not to their humanity but to their self-love, and never talk to them of our own necessities but of their advantages. Nobody but a beggar chuses to depend chiefly upon the benevolence of his fellow citizens (Smith 1976b, pp. 26–27). With this, Smith identifies two structures of transactions: the self-interest motive and its contract context; the altruism motive and its aid context. Of course, for scholars of Smith (see for example Khalil 1990, 1996, 2017), the motivations that Smith has unraveled are more complicated and varied than this dichotomy. Actually, the set of motivations that Smith has uncovered are the grits of the modern fields of behavioral economics and social psychology. What matters for this paper is not what Smith actually believed, but how Smith’s twofold taxonomy in the above quote became entrenched in economic theory. Namely, economic theory became limited, especially at the hands of Gary Becker (1981), to the broad category “interest” or, equivalently, “wellbeing”: either “self-interest” when the objective of the decision-maker (DM) is own-consumption, or “altruism” when the objective of the DM is the other-consumption whose interest enhances the DM’s utility. Despite the differences between these two varieties of interest, they need not be dichotomous. In fact, they are pecuniary motives lying along a continuum that aims to maximize what this paper calls “corporeal utility” (see Khalil 1990). The DM satisfies her own corporeal utility or the corporeal utility of others, depending on the relative costs. Whether the DM is pursuing self-interest or other-interest (altruism), the DM is engaged in what is called here “instrumental rational decision.” It is called “instrumental” because the DM stands externally to her own pecuniary satisfaction as well as externally to the pecuniary satisfaction of the other. As insightful Smith’s twofold taxonomy is, it remains limited to instrumental rational decision (Khalil and Marciano, 2018). This paper proffers that there are other motives that go beyond the satisfaction of interest, whether it is self-interest or other-interest. Such motives consist broadly of the longing for friendship, community, and distinction of achievement. Organization theory and the field of organizational behavior literature have discussed the longing for friendship, the aching for community, and the desire of distinction (e.g., Rocha and Ghoshal, 2006; Tasselli 2019). The motives to satisfy longing for friendship, community, and distinction are also regulated by rational choice as shown below, but this should not entail that they are pecuniary in the sense of belonging to corporeal utility. To see why, we need to question the assumption, so common in the literature, that rational choice is limited to corporeal utility. It is so common to suppose that if a theory uses the rational choice approach, it then reduces all costs and benefits to the corporeal dimension. This common supposition has caused a great hinderance to the advancement of a more generalized theory of human behavior. This paper expressly employs the separation of the rationality principle from the utility concept or, more generally, the pursuit of ends or products. We define rational choice as applicable to any calculation, not limited to corporeal utility. Rational choice also applies to motives listed above that concern existential ends, what is called here “transcendental utility.” In each respective section below, this paper shows how the rational choice principle is pertinent to existential ends. Existential ends transcend the corporeal self—i.e., a self that is defined by the biological skin—to fuse with others and produce another kind of pleasure, transcendental utility. The DM promotes her own transcendental utility when the DM is engaged in what is called here the “existential rational decision.” In this mode, the DM satisfies at least three existential motives: the geniality motive and its friendship or companionship context; the camaraderie motive and its grant context; the distinction motive and its status context. This paper effectively proposes a dialogical theory of rational decisions: decisions can be either instrumental or existential. The instrumental rational mode of decisions gives rise to Smith’s two motives: self-interest and other-interest (altruism). The existential rational mode of decisions gives rise to the three motives: geniality, camaraderie, and distinction. Section 2 links the three motives with Smith’s dichotomy via the philosophical anthropology of Martin Buber and shows how it differs from Max Weber’s notion of rationality. Sections 3, 4, and 5 discuss successively the three motives and their corresponding three structures that typify the existential decision mode of action. Section 6 concludes.",1
90.0,2.0,Theory and Decision,23 September 2020,https://link.springer.com/article/10.1007/s11238-020-09776-y,Preference for flexibility and dynamic consistency with incomplete preferences,March 2021,Fernanda Senra de Moura,Gil Riella,,Female,Male,Unknown,Mix,,
90.0,2.0,Theory and Decision,07 October 2020,https://link.springer.com/article/10.1007/s11238-020-09779-9,On the aversion to incomplete preferences,March 2021,Ritxar Arlegi,Sacha Bourgeois-Gironde,Mikel Hualde,Unknown,,Male,Mix,,
90.0,2.0,Theory and Decision,26 October 2020,https://link.springer.com/article/10.1007/s11238-020-09785-x,Signal extraction: experimental evidence,March 2021,Te Bao,John Duffy,,,Male,Unknown,Mix,,
90.0,2.0,Theory and Decision,21 November 2020,https://link.springer.com/article/10.1007/s11238-020-09782-0,A strategic justification of the constrained equal awards rule through a procedurally fair multilateral bargaining game,March 2021,Makoto Hagiwara,Shunsuke Hanato,,,Male,Unknown,Mix,,
90.0,2.0,Theory and Decision,06 November 2020,https://link.springer.com/article/10.1007/s11238-020-09783-z,Learning and dropout in contests: an experimental approach,March 2021,Francesco Fallucchi,Jan Niederreiter,Massimo Riccaboni,Male,Male,Male,Male,"Settings characterized by high uncertainty on outcomes are likely to impair fully rational decisions, which renders learning from previous attempts oftentimes not fruitful. As a consequence, this process may lead to an inefficient allocation of efforts in a trial-and-error search regime, that can easily result in losses (Dosi et al. 2001). A prominent example of such setting is the pharmaceutical industry (Pammolli et al. 2011), where the races for drug discovery lead to enormous investments by companies that are not always fruitful and may induce companies to abstain from investing.Footnote 1 Similar industries, characterized by a high investment in research and development (R&D), may vary in the structure of earnings: if the competition is based on incremental innovation, then the successful agents manage to capture a larger market share (Breitmoser et al. 2010). In other cases, the successful competitor gains the whole market similar to a lottery (Sutton 1998), e.g., by patenting new first-in-class drugs. The aim of this study is to identify, by means of experiments, how the payoff structure and the level of competition affect the learning and participation dynamics of agents. Our candidate setting is the Tullock rent-seeking contest in which subjects compete for a single prize, whose assignment probability depends on the relative share of subjects’ efforts (Tullock 1980). In rent-seeking contests, subjects persistently deviate from what standard game theoretical models predict. A survey of the experimental contest literature by Dechenaux et al. (2015) has highlighted that contestants spend on average considerably more than the theoretical equilibrium.Footnote 2 Most studies find an overall decrease in expenditures when subjects repeatedly play this game (e.g., Cason et al. 2012), which is usually attributed to learning, without further specifying the process behind it. Another empirical regularity that has not yet received much attention is that many participants choose not to spend any resources to win the contest. By looking across a subset of experimental studies on winner-take-all contests (Abbink et al. 2010; Cason et al. 2012; Mago et al. 2016; Sheremeta 2010; Sheremeta and Zhang 2010; Price and Sheremeta 2011; Sheremeta 2011), we find that zero expenditures are indeed a frequent, sometimes modal, choice of participants. The fraction of zeros is higher in larger competing groups and persistent even in later stages of the experiments. If both stylized facts are the consequence of learning, then we should investigate more carefully this process, forming the main contribution of this paper. To explore how the uncertainty on outcomes and the number of opponents affect learning strategies and behavioral patterns, such as zero expenditures, in contest settings over a long time horizon, we set up a laboratory experiment in which we compare, over 60 periods, participants’ expenditures choices in the standard winner-take-all (WTA) Tullock contest versus a non-probabilistic equivalent proportional-prize (PP) contest. The WTA contest allows only one winner of the prize, whose winning probability is proportional to the share of own investments over the total group investments. Applications range from the seminal rent-seeking hypothesis by Tullock (1980), to political polls (Snyder 1989), sport tournaments (Szymanski 2003), patent races (Fudenberg et al. 1983; Harris and Vickers 1985) and cryptocurrency mining (Dimitri 2017). In the deterministic PP contest, contestants receive a fraction of the prize proportional to their share of total group investments. The PP contest provides a ‘replication’ of standard oligopoly settings by varying the payoff structure. The early work by Friedman (1958) constitutes a first attempt to use the PP contest to model the allocation of advertisement budget across media. Proportional-prize assignments are also observed in electoral schemes (Schram and Sonnemans 1996), lobbying (Krueger 1974) and labor compensation (Kruse 1992). Under the assumption of risk neutrality, both contest settings are equivalent in terms of equilibrium predictions. Varying the contest type and the group size of three and five contestants, we create a 2 \(\times \) 2 experimental design, which allows us to test how styles of learning, and consequently contestant behavior, change in environments characterized by uncertainty over outcomes compared to those with a tighter link between effort and outcome. We find that the average levels of effort in PP contests are well described by the standard game theoretical predictions. Conversely, in the WTA contests we are unable to distinguish total group expenditures between the two group sizes. The decline of average expenditures in the five-player WTA contests coincides with a significant increase in what we label ‘dropouts’, i.e., zero expenditures that are not justified either by the myopic best response or weighted fictitious play. Dropouts are instead significantly less frequent in PP contests and, if anything, decrease over time. The distinct expenditure patterns found between the two settings suggest that differences in the contests’ payoff structures affect subjects’ learning process. As our main contribution to the literature on learning in games, we test this hypothesis by estimating the experience-weighted attraction model (EWA, Camerer and Ho 1999). The results reveal that WTA contestants learn significantly more from their own past payoffs than players in the PP contests (experiential or reinforcement learning Roth and Erev 1995). Moreover, our results support recent findings by Alós-Ferrer and Ritschel (2018) on subjects’ frequent use of the reinforcement heuristic ’win–stay, lose–shift’ rather than a more reasoned approach based on myopic best response. Therefore, the strong reliance on experiential learning in WTA settings can explain both the decreasing expenditures over time and the increasing propensity of zero expenditures choices. The more often a WTA participant loses, the more she will discourage positive expenditures up to non-participation. Further analyses confirm that expenditures decline significantly with an increase in prior accumulated losses. Since experienced losses are more frequent in larger competing groups, expenditures are expected to decrease at a faster rate. These results carry practical relevance for contest designers, who wish to maximize repeated participation, and are comparable to empirical regularities found in industrial dynamics, so-called ‘industry shakeouts’, in which many firms decide to drop out of an industry during its competitive expansion phase. Previous experimental studies have explored subjects’ behavior in contests, whose design or methods partially overlap with ours by varying the group size (e.g., Lim et al. 2014), the payout function (Chowdhury et al. 2014; Ghosh and Hummel 2018), and the matching protocol (Baik et al. 2015), but no one has so far explored the interaction between treatments varying the group size as well as the outcome uncertainty. More importantly, differences in behavior across contest structures have been proposed to stem from differences in learning (Fallucchi et al. 2013), which has not been rigorously tested yet. Similarly, the high fraction of zero expenditures has often been attributed to myopic best reply without proper analysis. In this paper, we investigate experimentally how expenditures dynamics, such as frequently observed zero expenditure choices, can be explained by distinct learning mechanisms across contest structures. The paper is organized as follows: Sect. 2 introduces the contest forms and offers a brief review of the related experimental literature on contests. We analyze subjects’ behavior in the two contest structures and their equivalence in expected payoff terms. Section 3 presents the experimental design and procedures. The experimental results are presented and discussed in Sect. 4. The first, descriptive, part of our result section highlights differences in group expenditures and in the fraction of zero expenditures. The second part presents the EWA model estimations and shows support for different learning modes across contest games. We conclude in Sect. 5 with a discussion of our findings and highlight how these results may well represent some empirical regularities in winner-take-all settings outside the experimental literature.",4
90.0,2.0,Theory and Decision,18 November 2020,https://link.springer.com/article/10.1007/s11238-020-09784-y,What to tell? Wise communication and wise crowd,March 2021,Chen Li,Ning Liu,,,,Unknown,Mix,,
90.0,3.0,Theory and Decision,02 April 2021,https://link.springer.com/article/10.1007/s11238-021-09807-2,Special Issue on Ambiguity and Strategic Interactions in Honor of Jürgen Eichberger,May 2021,Adam Dominiak,Ani Guerdjikova,,Male,Female,Unknown,Mix,,
90.0,3.0,Theory and Decision,06 October 2020,https://link.springer.com/article/10.1007/s11238-020-09777-x,Objective and subjective rationality and decisions with the best and worst case in mind,May 2021,Simon Grant,Patricia Rich,Jack Stecher,Male,Female,Male,Mix,,
90.0,3.0,Theory and Decision,17 April 2021,https://link.springer.com/article/10.1007/s11238-021-09808-1,Pessimism and optimism towards new discoveries,May 2021,Adam Dominiak,Ani Guerdjikova,,Male,Female,Unknown,Mix,,
90.0,3.0,Theory and Decision,01 June 2020,https://link.springer.com/article/10.1007/s11238-020-09759-z,Signaling probabilities in ambiguity: who reacts to vague news?,May 2021,Dmitri Vinogradov,Yousef Makhlouf,,Male,Male,Unknown,Male,"News affects behavior of individuals, organizations, and whole markets even if the conveyed message lacks precision.Footnote 1 A special type of imprecise news can be seen in on-screen notifications common in online stores and booking platforms: “Booked 4 times on your dates in the last 6 h on our site. Last booked for your dates 26 min ago.” (booking.com), “In high demand: 28 booked in the last day” (rentalcars.com), “Customer reviews: 4.1 out of 5; total 4354 customer ratings” (amazon.co.uk), etc. We place messages of this type in the context of decisions in uncertainty and experimentally show that ambiguity attitudes explain reaction to them. Ambiguity-neutral subjects are less likely to change choices in response to vague news, implying that somehow vagueness matters for them despite neutrality to ambiguity; they also disregard messages that communicate very high probabilities of success. A possible explanation is in the lack of updating of prior beliefs by subjects with high confidence, which appears to correlate with ambiguity-neutrality. Models of decisions in uncertainty agree that when ambiguity is resolved, decisions of non-neutral to ambiguity subjects become indistinguishable from those of ambiguity neutrals. Empirically, for example, Baillon et al. (2017) show, using real data, that as soon as more information about the dynamics of stock option performance becomes available, ambiguity-averse subjects form beliefs close to those of their ambiguity-neutral peers. It is not clear though, if ambiguity-neutral subjects fully incorporate all incoming information in their decision; in our data, for example, a significant fraction of ambiguity-neutral subjects respond to positive news but this fraction is nowhere close to 100 per cent (see Fig. 1). Theoretical models offer qualitatively different predictions in this regard. In the multiple-priors framework (Gilboa and Schmeidler 1989), a signal (communication of information) affects decisions of ambiguity-averse subjects only if it alters the lowest expected utility. Therefore, a vague signal that changes the set of priors but leaves the worst expectation unchanged might lead to a change in choices of ambiguity-neutral subjects (if their belief is affected) but no change in the ambiguity-averse behavior. In the second-order models (e.g. Klibanoff et al. 2005; Nau 2006; Neilson 2010) response of ambiguity-averse subjects depends on how exactly signals affect the whole second-order distribution; theoretically, they can both underreact and overreact to news compared to the response of ambiguity-neutrals. Neo-additive capacities (Chateauneuf et al. 2007) explicitly weigh the probabilistic and the non-probabilistic components of the decision functional. As long as the worst and the best outcomes remain unaffected, the neo-additive approach implies a higher impact of news that communicate a probability value on probabilistically sophisticated subjects, as the latter assign a weight of unity to the probabilistic component. Importantly, the above (and other) theoretical models assume homogeneity of ambiguity-neutral subjects in terms of their dealing with ambiguity, which appears in conflict with our data. Fractions (y-axis) of ambiguity-averse and ambiguity-neutral subjects choosing the urn with an unknown composition (a) in Ellsberg-type two-color experiments. “Ellsberg (color)”, where color = red or blue, depict choices in treatments without signals: “If a [color] ball is drawn you will get the prize. Would you prefer to draw the ball from Urn A or Urn B?” Further questions are conditioned on color = red and include signals (formulated on the x-axis) about choices and draws of hypothetical other participants. Subjects are classified as ambiguity-averse if in the standard Ellsberg experiment they prefer the unambiguous urn independent of the color on which the prize is conditioned; subjects who choose different urns in the two Ellsberg questions are classified as ambiguity-neutral. The total number of ambiguity-neutral (AN) subjects is 204 and ambiguity-averse (AA) is 1035. Number of ambiguity-averse subjects choosing urn A in “Ellsberg (Red)” and “Ellsberg (Blue)” is zero by definition The fact that ambiguity-neutral subjects by definition neither like nor dislike ambiguity, does not have to imply they neglect it. When subjects receive information, they first evaluate it, before making the decision. The information is tested against a prior belief (the probability of success in the ambiguous urn) subjects have: if there is no contradiction between the prior and the data, the prior stands; if the data contradict the prior, the latter is updated to fit the data.Footnote 2 This belief then informs decision-making. Subjects who are more confident in their prior are less likely to update beliefs in response to new information. Literature (e.g. Dominiak et al. 2012) and our observations suggest these are more likely to be ambiguity-neutral; thus they effectively take ambiguity of news into account by discarding the news. This approach is similar to ideas used in Ulrich (2013), where investors receive signals (observe inflation data) and apply a likelihood ratio test to identify if their reference model is trustworthy, and in Fryer et al. (2019) where agents receive an imprecise (open to interpretation) signal, which is first interpreted and then used to form beliefs. In our case the signal is interpreted from the perspective of whether it confirms the prior belief or not. As highlighted in Fig. 1, we extend a standard two-color Ellsberg (1961) experiment by adding signals about the probability of success in the ambiguous urn and investigate how subjects with different ambiguity attitudes respond to variations in ambiguity, in the communicated probability of success, and in both. A number of studies have previously analyzed the impact of varying levels of ambiguity on decisions. Early studies by Curley and Yates (1985) and Bowen et al. (1994) represented ambiguity as an interval of possible values of probability and varied both the length and the centerpiece of this interval with an objective to detect changes in the average ambiguity attitude of the sample. Budescu et al. (2002) and Du and Budescu (2005) use the same approach, yet focus on subjects’ sensitivity to gain/losses framing, as well as to the domain of uncertainty (outcomes or probabilities).Footnote 3 Another approach to vary ambiguity is used by Ahmed and Skogh (2006) who make subjects' payoffs dependent on a draw from an urn, the composition of which is either unknown, or described in a way that limits but does not fully reveal the likelihood of success, or described well enough to give a precise probability of it. In their data, when ambiguity is high, subjects prefer to share losses, but as ambiguity reduces, subjects switch towards insurance. However, no distinction between ambiguity-neutral or ambiguity-averse subjects makes it difficult to judge to which extent subjects' decisions are governed by ambiguity attitudes.Footnote 4 Our focus is on differences between ambiguity-neutral and non-neutral subjects, and in particular on the way signals about the likelihood of success and the ambiguity of those signals affect decisions. To signal probabilities in ambiguity, we imitate messages of online stores and booking websites who often tempt buyers with messages like ""5 people are looking at this item at the moment"", ""This hotel was booked 13 times on our site"" or ""Score based on 527 reviews: 7.9/10"". These messages may be ordered according to their information strength: the first one tells us about other people making a decision but not the decisions or their outcomes; the second one reports on some decisions previously made but ignores the outcomes. The third message is the most informative out of the three, reporting the average outcome and its reliability (the number of “trials”). We use this idea to compose signals in our experiments. For example, we tell our subjects that 12 [hypothetical] participants before them chose the ambiguous urn in the Ellsberg task, or 12 out of 20 actually drew a red ball from that urn. As messages of this type are common in everyday life, they are easy to understand. From a statistical perspective, they may be conveniently interpreted as a frequentist representation of probability or as reports on the number of successes in Bernoulli trials. Subjects make decisions sequentially, starting with the standard Ellsberg task, which is used to distinguish between ambiguity-averse, -neutral, and –seeking participants. They further progress through signals akin to those described above. We collect data from five independent experiments, both lab-based and online, with and without monetary incentives, with the number of participants ranging from 109 to 892, giving us a total of 1182 valid responsesFootnote 5 of ambiguity-averse and ambiguity-neutral subjects. The large total number of responses is crucial as ambiguity-neutral subjects are a key benchmark in our approach, and usually their fraction in experiments on ambiguity attitudes is not large; online experiments offer a good opportunity to collect large samples. We then conduct a difference in means analysis, complemented with probit regressions controlling for gender, age and knowledge of statistics, to establish the effects of signals on decisions. On the one hand, providing some information, however vague, potentially reduces ambiguity, for which reason we expect a positive impact on ambiguity-averse subjects. Indeed, in all tasks we detect a significant effect of signals on subjects' choices; ambiguity-averse subjects are likely to react even to very vague news bearing little information about the probability of success, see Fig. 1 for a preview of our results. On the other hand, vague news lacks reliability, for which reason a vague signal may be not strong enough to reject prior beliefs about the fundamentals, thus implying limited effect through the change in fundamentals (probability of success) channel. Theoretically, ambiguity-neutral subjects only respond to changes in fundamentals. In our data, a large part of them remain unaffected by the news, although the communicated probability of success is evidently above 0.5, as there is also a significant fraction of those who change decisions. This highlights the heterogeneity in the ambiguity-neutral cohort, which we attribute to confidence. The difference between ambiguity-averse and ambiguity-neutral subjects in their responses to signals becomes less significant once they face signals that convey some probability of success; subjects with a better knowledge of mathematical statistics and probabilities are more likely to respond to them. The strongest response is observed for the signal that communicates the highest likelihood of success. Varying the precision of the signal also produces a significant effect on subjects' choices. All main results hold at the aggregate level for the pooled data, controlling for experiments, at the split level for the subsamples of lab versus online, and incentivized versus unincentivized experiments, and at the level of individual experiments, to ensure consistency of findings across them.",1
90.0,3.0,Theory and Decision,09 October 2020,https://link.springer.com/article/10.1007/s11238-020-09778-w,Savage vs. Anscombe-Aumann: an experimental investigation of ambiguity frameworks,May 2021,Jörg Oechssler,Alex Roomets,,Male,Male,Unknown,Male,"The Savage (1954) and the Anscombe and Aumann (1963) frameworks are the two most popular approaches when it comes to modeling ambiguity. The latter is a two-stage model where acts are maps from states to objective lotteries over consequences. It is often preferred for its simplicity, but the Savage model provides more flexibility. Gilboa and Schmeidler (1989) and Schmeidler (1989) used the Anscombe and Aumann approach as a basis for their seminal contributions to ambiguity theory. Eichberger and Kelsey (1996) show that, for standard ambiguity models like Choquet-expected utility (CEU) and Maxmin Expected Utility, ambiguity aversion implies a strict preference for randomization when looked at in the Anscombe–Aumann framework. They also show that the same need not hold in the Savage framework. Eichberger and Kelsey (1996) argue against the plausibility of a general preference for randomization but also admit the need for further experiments on this question.Footnote 1 We implement an experiment in which some choices are inconsistent with ambiguity models that are based on the preference framework of Anscombe and Aumann (1963). We show that these choices can be consistent within a Savage framework using, e.g., a CEU model as in Eichberger and Kelsey (1996). The experiment involves subjects choosing from among six options that each relates to the outcomes of a coin flip and a draw from an ambiguous, 2-color urn. Two of the six options result in a clearly ambiguous act. Two more of the six options result in a clearly risky act. The last two options would be considered risky acts within the Anscombe–Aumann framework, but would be treated as ambiguous acts within the Savage framework. By manipulating the payoffs within the various acts, we are able to create a dominance relationship between the four risky acts using the Anscombe–Aumann framework. We find that dominated acts are still chosen by subjects more than a third of the time. The same subject choices can be explained with ambiguity models using the Savage framework, where the dominance relationship does not necessarily hold. The two acts that highlight the differences between the two frameworks involve ambiguity hedging (see Oechssler and Roomets 2014, and Oechssler et al. 2019). These acts are akin to betting on one color when a coin flip comes up heads, and a different color when the coin flip comes up tails. Within the Anscombe–Aumann framework, subjects making such a combination exploit the complementarity of the probabilities of the two colors of balls in the urn to arrive at a believed 50:50 chance to win the bet. Within the Savage framework, such complementarity need not to be assumed. Subjects are allowed to believe that the probabilities of the two colors depend on the coin flip. Therefore, when a subject considers choosing an act that combines bets on blue (when the coins shows heads) and yellow (when the coin shows tails), the subject could believe that blue is unlikely when the coin shows heads and also that yellow is unlikely when the coin shows tails. Therefore, while the hedge acts represent risk using the Anscombe–Aumann framework, the same acts represent ambiguity using the Savage framework. While it may seem we are pitting one framework against the other in a fair fight, we caution readers that the way we have been able to design choices leaves Savage mostly out of harms way while placing Anscombe and Aumann in jeopardy. Some may point out that the flexibility of the Savage framework is what keeps it out of the fray, and that this flexibility should be considered an advantage. We cannot disagree, but we leave discussions of the relative flexibility of the frameworks to more theoretical papers. As a fundamentally experimental endeavor, this paper should be viewed primarily as a test of the Anscombe–Aumann framework. Our results are not supportive of the Anscombe–Aumann framework in this context. This represents our main finding and contribution. It is, of course, interesting that the Savage framework could have explained our subjects’ behavior when the Anscombe–Aumann framework could not. However, this should not be considered direct support for the Savage framework as there was no way it could have failed in our experimental setting.Footnote 2",
90.0,3.0,Theory and Decision,03 March 2021,https://link.springer.com/article/10.1007/s11238-021-09800-9,"The evolutionary stability of optimism, pessimism, and complete ignorance",May 2021,Burkhard C. Schipper,,,Male,Unknown,Unknown,Male,"The motivation for this work is twofold: first, we seek an evolutionary foundation for why humans maintain in some situations an optimistic or pessimistic attitude toward uncertainty and are ignorant to strategic aspects. Second, at a more theoretical level, we want to study how we can restrict by evolutionary arguments degrees of freedom in models with Knightian uncertainty, ambiguity, or imprecise beliefs. In particular, we want to endogeneize a player’s attitude toward Knightian uncertainty as well as the amount of Knightian uncertainty over opponents’ actions in strategic games by asking which of those parameters would maximize material payoffs (or fitness) and thus prosper and thrive. In the literature on social psychology, there is evidence for both optimistic and pessimistic attitudes in the face of uncertainty and their relation to “success”. For example, Seligman and Schulman (1986) found that more optimistic health insurance agents sold more policies during the first year of employment and were less likely to quit. Cooper et al. (1988), using interviews, found that self-assessed chances of new entrepreneurs’ success are uncorrelated with education, prior experience, and start-up capital, and are overly optimistic. Taylor and Brown (1988) found that mentally healthy individuals maintain some unrealistic optimism, whereas depressed individuals have more accurate perceptions. Studies on individual decision-making show that the majority of subjects shy away from uncertain prospects like in the Ellsberg’s paradox (for a survey, see, for example, Camerer and Weber 1992). To summarize, there is evidence on optimism in psychology and evidence on pessimism in the literature on individual decision-making. This may suggest that both types of belief biases are present in the majority of the population and are not stable across situations. An individual may hold optimistic beliefs in some situations but pessimistic beliefs in some other situations. In this article, we seek an evolutionary explanation and show that these biases may depend on the strategic situation. We model Knightian uncertainty, ambiguity, or imprecise beliefs by Choquet expected utility theory (CEU) with non-extreme-outcome-additive (neo-additive) capacities introduced by Chateauneuf et al. (2007). Knightian uncertainty or ambiguity refers to situations where probabilities may be unknown or imperfectly known as opposed to situations under risk where probabilities are known, a distinction made by Knight (1921). CEU with respect to neo-additive capacities by Chateauneuf et al. (2007) includes as special cases Subjective expected utility (SEU) as well as various preferences for decision-making under complete ignorance such as Minimax (Wald 1951), Maximax, and Hurwicz preferences (Hurwicz 1951; Arrow and Hurwicz 1972). More importantly, because of its parameterized form, it lends itself well to applications. Since CEU with neo-additive capacities by Chateauneuf et al. (2007) is a generalization of conventional Subjective expected utility theory (SEU), it has more degrees of freedom like the degrees of ignorance and the degrees of optimism and pessimism defined in the next section. It is natural to ask how to select among the degrees of optimism/pessimism and ignorance. A possible answer could be provided in an indirect evolutionary framework: if evolution (including social learning processes and market selection) chooses preferences parameterized by those degrees of freedom, which one would it choose? To study such questions, we make use of the literature on Choquet expected utility in strategic games, in which players face Knigthian uncertainty about opponents’ actions (see, for instance, Dow and Werlang 1994; Eichberger and Kelsey 2000, 2002, 2014; Marinacci 2000; Eichberger et al. 2009).Footnote 1 While it is intuitive that players face Knightian uncertainty in an unfamiliar one-shot situation, we show that even after a very large number of repeated interactions implicitly assumed to be behind evolution, players may be still have biased beliefs and be completely ignorant. We focus on submodular games and supermodular games (see Topkis 1998) both with aggregation and positive or negative externalities. These classes include many games prominent in economics and social sciences in general. Many games in economics involve ordered action sets like prices, quantities, qualities, contribution levels, appropriation levels, etc. Often, there is a natural aggregate of all players actions like total market quantity, total contribution or appropriation, etc.Footnote 2 Moreover, many games with ordered action space have either some version of strategic substitutes or strategic complements (Bulow et al. 1985) or can be brought into a framework of supermodular or submodular games.Footnote 3 We show that sub- and supermodularity is preserved when extending these games to Choquet expected utility with neo-additive capacities. Moreover, Choquet expected utility with neo-additive capacities features distinct parameters such as the degree of ignorance and degree of optimism. As we will show using results from Topkis (1998), these parameters lend themselves well to monotone comparative statics. The monotone comparative statics of equilibrium under Knightian uncertainty with respect to the degree optimism/pessimism is just a first stepping stone in studying the restriction of parameters of CEU with neo-additive capacities by evolutionary arguments. It allows us to learn about changes in behavior when mutants with different preference parameters enter the population of players. We use the indirect evolutionary approach (Güth and Yaari 1992; Güth 1995; Heifetz et al. 2007a, b; see Alger and Weibull 2019, for a recent survey). In this literature, players drawn randomly from a large population are matched to play a game and behave rationally with respect to their preferences, but their survival is judged by their material payoff. In our contexts, preferences are given by CEU w.r.t. neo-additive capacities, while material payoffs are payoffs in games. Instead of working with a large population, we employ Schaffer’s (1988, 1989) notion of evolutionary stability for finite populations, because we believe that, in many situations of economic relevance, players “play the field”, i.e., all players in a finite set of players play a game. A strategy is finite population evolutionary stable if it maximizes relative (material) payoffs among all players. The technical motivation is that finite population evolutionary strategies are relatively easy to work with in sub- and supermodular aggregative games (Schipper 2003; Alós-Ferrer and Ania 2005). With this apparatus, we can then ask which parameter configurations of CEU with neo-additive capacities yield actions in equilibrium under Knightian uncertainty that maximize relative material payoffs. These parameter configurations are the finite population evolutionary stable preferences. To build an intuition for our results, consider, for example, a version of a Nash bargaining game. Let there be two players who simultaneously demand a share of a fixed pie. If the demands sum up to less than 100% of the pie, then the pie is shared according to the demands. Otherwise, players receive nothing. This game has strategic substitutes, because the marginal benefit from a player’s larger demand is decreasing in the demand of the opponent. Let each player faces Knightian uncertainty about the opponents’ demand. Suppose that a player is a pessimist. Then, she overweighs large demands by the opponent. The more pessimistic the belief, the lower is the best-response demand, because the player fears the incompatibility of demands, which would result in a zero payoff. Could such a pessimistic belief be evolutionary stable? Suppose the opponent (the “mutant”) is not as pessimistic, and then, his best response is a larger demand. If demands add up to less than 100%, this opponent is strictly better off than the pessimist; otherwise, both get nothing and he is not worse off. Thus, pessimism cannot be evolutionary stable in this two-player game. Is optimism evolutionary stable? Suppose the opponent is an extreme optimist in the sense that he believes that the opponent will demand zero, and then, the best response is to demand 100%. If the opponent does indeed demand zero, the extreme optimist is strictly better off. Otherwise, if the opponent does demand some strict positive share, then both receive nothing and the extreme optimist is not worse off. Thus, there is no preference with an attitude toward Knightian uncertainty that would successfully invade a set of extremely optimistic players, because an optimist cannot be made worse off relative to other player. The example of the Nash bargaining game is also an example for the evolutionary stability of complete ignorance. A completely ignorant player behaves as if she is alone and there is no opponent. We will show that evolutionary stability of (to some degree) optimistic preferences with complete ignorance holds with qualifications not only for the Nash bargaining game but for an entire class of games characterized by a general notion of strategic substitutes, submodular aggregative games. Similarly, we show that preferences reflecting extreme pessimism and complete ignorance are finite population evolutionary stable in games with some general notion of strategic complements, supermodular aggregative games. The evolutionary stability of ignorance is somewhat disappointing given that we put in considerable effort in modeling games with a rich class of preferences. At a second glance, it is not surprising though. Ignorance yields commitment power. As the example of the Nash demand game illustrates, there are important classes of games where commitment to an aggressive action maximizes relative material payoffs. We should point out though that we present just sufficient conditions. Moreover, we demonstrate with examples that complete ignorance is not necessary for finite population evolutionary stability. Yet, to really take advantage of the rich class of CEU preferences in strategic settings and go beyond complete ignorance, we believe that it will be necessary to go beyond the important classes of games that we consider. The evolutionary stability of optimism versus pessimism depends on the classes of games that we study. In submodular aggregative, a bolder aggressive action maximizes relative material payoffs and, hence, optimism becomes evolutionary stable. This insight goes back essentially to von Stackelberg (1934). In contrast, in our class of supermodular aggregative games, a timid action maximizes relative material payoffs and, hence, pessimism is evolutionary stable, an insight that becomes immediately intuitive when considering, for instance, minimum effort coordination games. In both classes of games, the evolutionary stable preference leads to Walrasian behavior, meaning that players behave as if they maximize material payoffs taking the aggregate of all players actions as given. How to interpret this evolutionary approach? While we trust that the formal results will become clear in the following sections, it is less immediate how to understand the notion of evolution. In an economic context, evolution may not necessarily be understood in a biological sense. Rather, we may be primarily concerned with the question as to what extent a trait (here either optimism or pessimism and ignorance) is relatively more successful than (or yields a strategic advantage over) other traits (i.e., other attitudes toward uncertainty). These successful traits may then be spread through social imitation (Vega-Redondo 1997; Schipper 2009; Duersch et al. 2012). Beyond “social learning”, there is some empirical evidence on a positive correlation between economic success and reproductive success from both today’s developing countries (Hull and Hull 1977; Mace 1996; Mulder 1987) and some European countries prior to the industrial revolution (Boone 1986; Hughes 1986). Therefore, even a biological interpretation of our notion of evolution may not be too far-fetched after all. While we are not aware of any prior results seeking to restrict parameters of Choquet expected utility by evolutionary arguments in games, the associate editor kindly made us aware that Eichberger and Guerdjikova (2018) study the evolution of optimism and pessimism in a financial market using a case-based \(\alpha\)-MEU model and show that pessimists may dominate the market in the long-run. Our results on the monotone comparative are closely related to Eichberger et al. (2009) and Eichberger and Kelsey (2014). We extend earlier work by Eichberger et al. (2009) who study Choquet expected utility with neo-additive capacities in particular examples such as Cournot duopoly, Bertrand duopoly, strategic delegation, a “peace-making” game, and some two-player concave games with strategic complementarities. Eichberger and Kelsey (2014) model optimism and pessimism in games with Choquet expected utility featuring JP capacities (Jaffray and Philippe 1997), which are more general than neo-additive capacities. They also make use of Topkis’ (1998) results on monotone comparative statics in supermodular games but restrict to finite one-dimensional action sets. They do not consider submodular games though. The article has two main sections: The next section provides a comprehensive step-by-step exposition of the theory of sub- and supermodular aggregative games with Choquet expected utility with neo-additive capacities including monotone comparative statics. Section 3 focuses on evolutionary stable preferences. We conclude with a discussion in Sect. 4. Proofs are relegated to the appendix.",1
90.0,3.0,Theory and Decision,02 July 2020,https://link.springer.com/article/10.1007/s11238-020-09764-2,Persuasion under ambiguity,May 2021,Jonas Hedlund,T. Florian Kauffeldt,Malte Lammert,Male,Unknown,Male,Male,"The recent literature on Bayesian persuasion, pioneered by Kamenica and Gentzkow (2011) (henceforth, K&G), is concerned with situations where a sender (he) controls the beliefs of a receiver (she) by conducting an informative experiment. The bulk of this literature assumes that sender and receiver update beliefs given a well-defined common prior about the payoff-relevant state. The present paper departs from this tradition by studying a persuasion model in which the sender has a well-defined prior on the payoff-relevant state, but the receiver is uncertain about the prior distribution (she perceives ambiguity) in the sense of Ellsberg (1961).Footnote 1 Several situations of information control are likely to feature such an asymmetry between the sender’s and the receiver’s prior beliefs. For example, suppose a doctor (sender) conducts a test to convince a patient (receiver) that she is a suitable candidate for a certain treatment. The prior probability of the patient’s suitability may depend on age, symptoms, and medical history, and may be unclear from the point of view of the patient. The doctor, however, is an expert and deals with similar situations on a daily basis, and is more likely to have a clear prior about the patient’s suitability. Similarly, a car salesman who controls the conditions under which a customer can test drive a used car seems more likely than the customer to have a clear prior about the possibility that the car is a lemon. Can the doctor and the car salesman exploit the receiver’s ambiguity to their advantage? Is it easier to persuade a receiver that responds more optimistically to her uncertainty about the prior distribution? Our analysis sheds light on these and related questions. We examine the effects of receiver ambiguity in a persuasion framework with binary payoff-relevant states and binary receiver actions. The sender’s objective is to persuade the receiver to take the “high” action. The sender has a well-defined prior on the payoff-relevant state, whereas the receiver perceives ambiguity and has too little information to form a unique prior. Instead, she considers an interval of priors possible. The receiver observes an outcome of a signal chosen by the sender, updates her prior set pointwise according to Bayes’ ruleFootnote 2, and chooses an action that maximizes her objective function. The receiver has \(\alpha \)-maxmin preferences and, therefore, maximizes a convex combination of her worst and best expected payoff with respect to the priors in her prior set.Footnote 3 The convex combination parameter, \(\alpha \), is the weight that the receiver puts on the worst expected payoff. We may, therefore, interpret \(\alpha \) as a measure of receiver’s ambiguity attitude, ranging from full optimism (\(\alpha =0\)) to full pessimism (\(\alpha =1\)). We take the size of the receiver’s prior set as a measure of the magnitude of the receiver’s ambiguity and refer to this as the receiver’s perceived ambiguity (see Ghirardato and Marinacci 2002). The main objective of our analysis is to understand the effect of the receiver’s ambiguity attitude and perceived ambiguity on the sender’s optimal strategy of persuasion and equilibrium expected payoff. Our first observation is that there are similarities and differences between the fundamental logic of optimal persuasion of a receiver who perceives ambiguity, and the benchmark where the sender and the receiver share a common prior (analyzed by K&G). As in the common prior benchmark, an ambiguity perceiving receiver is persuaded by pushing the prior set up until she becomes indifferent between both actions.Footnote 4 The characterization of the receiver’s indifference, however, is more complex, and different, when the receiver perceives ambiguity.Footnote 5 Our main results reveal that the effect of the receiver’s ambiguity attitude and perceived ambiguity on the sender’s optimal strategy of persuasion is organized by the sensitivities of the receiver’s actions, i.e., the receiver’s payoff differences across states given each action. In particular, the qualitative effect of the receiver’s ambiguity attitude on the equilibrium signal depends only on the sensitivities of the receiver’s actions. The sender finds it easier to persuade a more pessimistic receiver if and only if the sender’s preferred receiver action is the least sensitive one, i.e., if that action has the smallest payoff difference across states. Intuitively, when the sender’s preferred receiver action is the least sensitive one, the receiver perceives less uncertainty given that action. As the receiver becomes more pessimistic, she tends to view the small uncertainty more favorably. This makes it easier to persuade the receiver to take the sender’s preferred action. The comparative statics with respect to the receiver’s perceived ambiguity are remarkably rich, given the simplicity of our framework. We evaluate the effect of increased perceived ambiguity by uniformly expanding the receiver’s prior set around an arbitrary midpoint. We find that the sender’s equilibrium payoff can be decreasing, increasing, or non-monotonic (with a unique maximum), in the receiver’s perceived ambiguity. This means that the sender’s preferred degree of receiver ambiguity is either zero, maximal, or there is an interior ambiguity bliss point. The distinction between these cases is organized by the receiver’s ambiguity attitude \(\alpha \) and the sensitivities of her actions. Specifically, suppose the sender’s preferred receiver action is the least sensitive one. The sender’s preferred degree of receiver ambiguity is then zero if \(\alpha \le 1/2\), continuously increasing in \(\alpha \) for intermediate values \(\alpha \in (1/2,\hat{\alpha } ]\), and maximal for \(\alpha >\hat{\alpha }\). That is, the sender’s preferred degree of receiver ambiguity is (1) increasing in the receiver’s pessimism and (2) tilted towards a preference for zero receiver ambiguity. This result can, roughly, be understood in terms of two main effects. First, the smaller payoff uncertainty of the sender’s preferred action is more appealing to a more pessimistic receiver. This tendency magnifies as the receiver perceives more ambiguity, i.e., more uncertainty about the relevant probability distribution. This effect is behind the fact that the sender’s preferred degree of receiver ambiguity is increasing in the receiver’s pessimism (when the sender’s preferred receiver action is the least sensitive one). Second, because of a concavity property of Bayes’ rule in the prior, it becomes inherently more difficult to persuade the receiver as her prior set expands. The concavity property implies that as the receiver’s prior set expands uniformly, the posterior set expands faster at the lower bound than at the upper bound. This makes it more difficult to persuade the receiver to take the high action. This effect is responsible for the tilt of the sender’s preferences towards zero receiver ambiguity. The tilt of the sender’s preferences is a robust feature of our model, which arises inevitably as the sender needs to push the receiver’s posterior set away from the prior set, to induce the desired action. Consequently, when, in contrast to the case discussed above, the sender’s preferred receiver action is the most sensitive one, the sender’s preferred degree of receiver ambiguity is decreasing in the receiver’s pessimism, but again tilted towards a preference for zero receiver ambiguity. Finally, an intuitive way of summarizing the comparative statics with respect to the receiver’s ambiguity attitude and perceived ambiguity is the following: If the sender’s preferred receiver action is the least (most) sensitive one, then the sender’s equilibrium payoff as well as the sender’s preferred degree of receiver ambiguity are both increasing (decreasing) in the receiver’s pessimism, with a tilt in the sender’s preferred degree of receiver ambiguity towards zero ambiguity. The paper is organized as follows. We first review the related literature. Section 2 describes our model in detail. In Sect. 3, we present our results. Finally, Sect. 4 concludes with a summary. All proofs are in the appendix. Our paper contributes to the literature on Bayesian persuasion, initiated by K&G. Since their pioneering contribution, the Bayesian persuasion model has been extended in many directions, including sender private information (Alonso and Camara 2018; Hedlund 2017; Perez-Richet 2014), receiver private information (Kolotilin 2018; Kolotilin et al. 2017), multiple senders (Gentzkow and Kamenica 2016, 2017; Li and Norman 2018, 2018) and multiple receivers (Alonso and Camara 2016). A few recent papers (mostly working papers so far) within this literature deal with ambiguity in a K&G persuasion setup. Beauchene et al. (2019) assume that the sender can introduce ambiguity using an ambiguous communication device, which generates ambiguity for both the sender and the receiver.Footnote 6 Hence, in contrast to our model in which ambiguity is exogenously present in the prior distribution, ambiguity in their model is endogenously introduced by the sender through the signal. Sender and receiver are maxmin expected utility maximizers. Beauchene et al. show that it can be beneficial for the sender to use an ambiguous device, even though this generates ambiguity for him. 
Hu and Weng (2018) analyze a model in which the receiver is privately informed and the sender perceives ambiguity about the source of the receiver’s private information. As in Beauchene et al., the sender is a maxmin expected utility maximizer. Hu and Weng show that the sender fully discloses the payoff-relevant state whenever he perceives full ambiguity about the receiver’s private information. 
Kosterina (2018) introduces a model in which the sender perceives ambiguity about the receiver’s prior. Again, it is implicitly assumed that the sender has maxmin expected utility preferences. Kosterina characterizes the sender’s optimal signal in this setting. Finally, the paper closest to ours is Laclau and Renou (2016). In Laclau and Renou’s model, the sender’s payoff depends on a set of receiver beliefs. Laclau and Renou point out that their setup applies both to persuasion of multiple receivers, and to situations where a single receiver perceives ambiguity about the state of the world—as in our model. Whereas our models are related, however, our points of departure are different. Laclau and Renou focus on extending K&G’s concavification technique to situations where the sender wishes to simultaneously control multiple beliefs. Their main result is a characterization of the sender’s maximal payoff along these lines. In contrast, our main objective is to conduct comparative statics exercises within the specific context of an ambiguity-sensitive receiver, to understand the concrete effects of ambiguity on optimal persuasion. It is worth emphasizing that the papers reviewed above assume maxmin expected utility preferences. That is, they consider agents who are extremely averse to ambiguity.Footnote 7 To the best of our knowledge, our model is the first within this literature to explicitly examine ambiguity-loving behavior. The \(\alpha \)-maxmin approach allows us to examine the impact of both different levels of perceived ambiguity and of different ambiguity attitudes on agents’ optimal behavior.Footnote 8 Our paper also relates to the small literature on cheap talk communication in the presence of ambiguity. The most related papers in this literature are by Kellner and Le Quement (2017, 2018). Kellner and Le Quement (2017) consider a simple cheap talk game in which both sender and receiver perceive ambiguity about the state. Kellner and Le Quement (2018) consider a sender facing an ambiguity averse receiver. The sender is able to choose an ambiguous strategy a la Riedel and Sass (2014), see footnote 5.",2
90.0,3.0,Theory and Decision,17 August 2020,https://link.springer.com/article/10.1007/s11238-020-09770-4,Correction to: Persuasion under ambiguity,May 2021,Jonas Hedlund,T. Florian Kauffeldt,Malte Lammert,Male,Unknown,Male,Male,"In Proposition 4, lines 3, 4 and 8, 9 were scrambled by mistake during the pagination process of the online published article. The original article has been corrected accordingly. We apologize for the error introduced during the production process.",
90.0,3.0,Theory and Decision,27 June 2020,https://link.springer.com/article/10.1007/s11238-020-09765-1,Ambiguity when playing coordination games across cultures,May 2021,Joanne Peryman,David Kelsey,,Female,Male,Unknown,Mix,,
90.0,3.0,Theory and Decision,17 November 2020,https://link.springer.com/article/10.1007/s11238-020-09787-9,Participation in risk sharing under ambiguity,May 2021,Jan Werner,,,Male,Unknown,Unknown,Male,"Expected utility hypothesis together with (strict) risk aversion and common probabilities have strong implications on efficient risk sharing among multiple agents. First, agents’ consumption plans are comonotone with aggregate resources. Second, every agent participates in risk sharing by holding at least a small fraction of the aggregate risk. These results are at odds with empirical observations. Individual consumption often deviates from positive correlation with the aggregate consumption.Footnote 1 A large fraction of population in the US is not participating in asset markets thereby abstaining from sharing the aggregate financial risk. Ambiguity of beliefs has been suggested as a way to reconcile the differences between the observed patterns and the rules of efficient risk sharing. Two closely related, standard models of decision making with ambiguous beliefs are the multiple-prior expected utility of Gilboa and Schmeidler (1989) and the variational preferences of Maccheroni et al. (2006). Under the multiple-prior expected utility hypothesis, an agent has a set of probability measures (or priors) as her beliefs and evaluates an uncertain prospect by taking the minimum of expected utilities over the set of beliefs. One of the main implications of the multiple-prior model is the possibility of non-participation in trade. A simple illustration of this is the portfolio inertia of Dow and Werlang (1992). An agent with multiple-prior expected utility and risk-free initial wealth does not invest in a risky asset for a range prices. As long as the expected return on the risky asset under the most pessimistic belief is below the return on the risk-free asset and the expected return under the most optimistic belief is above the risk-free return, the agent will choose zero investment in the risky asset. Mukerji and Tallon (2001, 2004) and Cao et al. (2005) have shown that non-participation in trade can occur in an equilibrium in asset markets with multiple-prior expected utilities. Cao et al. (2005) considered a CARA-normal model of asset markets where agents know the true variance of the payoff of a risky asset but have ambiguous beliefs about its mean. Those ambiguous beliefs are specified by intervals of values around the true mean. There is heterogeneity of ambiguous beliefs. Agents with high ambiguity have bigger intervals than those with low ambiguity. In equilibrium, agents with high ambiguity do not participate in trade of the asset. The threshold for non-participation depends on the variance of the payoff of the outstanding asset supply, the dispersion of ambiguous beliefs, and the (common) degree of risk aversion. Low variance of asset supply, low risk aversion, and high dispersion of beliefs all lead to greater non-participation. A related CARA-normal model has been considered by Easley and O’Hara (2009)—with similar results—in their study of financial regulation and its role in mitigating the effects of ambiguity on market participation. Ozsoylev and Werner (2011) consider asset markets with asymmetric information and show that ambiguous beliefs may lead to limited participation and market illiquidity in rational expectations equilibrium. In this paper we focus on non-participation in efficient risk sharing. The question we ask is whether and how can ambiguity of beliefs give rise to some agents not participating in risk sharing, that is, having risk-free consumption in Pareto optimal allocations. First, we show that an agent whose set of priors is a strict superset of another agent’s set of priors is more likely not to participate in risk sharing in the sense that she does not participate whenever the other agent chooses so. Our main result says that if the aggregate risk is small, then agents with the highest ambiguity—those whose sets of priors are supersets of some other agents’ sets of priors—do not participate in risk sharing in interior Pareto optimal allocations. The bigger the set of priors of an agent with the highest ambiguity, the greater is the aggregate risk for which she does not participate in risk sharing. Another factor leading to non-participation of agents with the highest ambiguity is low risk aversion of agents with less ambiguous beliefs. Because of the First Welfare Theorem, properties of pareto optimal allocations hold for equilibrium allocations in assets markets if markets are complete. If the aggregate risk is small, agents with the highest ambiguity will have risk-free consumption in an equilibrium. Whether those agents will or will not trade the assets depends on their initial endowments. If the initial endowment is risk free, then the agent will not trade. Otherwise, if her initial endowment is risky, then she will trade so as to achieve a risk-free equilibrium consumption. Thus, she will purchase full insurance in asset markets. Our results are in concordance with the findings of Cao et al. (2005) in their specialized setting. Properties of efficient allocations for multiple-prior expected utilities and other non-expected utilities have been extensively studied in the literature over the past two decades. Billot et al. (2000) show that if agents have at least one prior in common and there is no aggregate risk, then all interior Pareto optimal allocations are risk free. Rigotti et al. (2008) extend that result to other models of convex preferences under ambiguity such as variational preferences and the smooth ambiguity model of Klibanoff et al. (2005). Recent paper by Ghirardato and Siniscalchi (2018) provides further extensions to non-convex preferences under ambiguity. They identify a sufficient behavioral condition for risk-free optimal allocations with no aggregate risk. Gierlinger (2018) studies risk sharing and trade in complete markets with sunspot uncertainty. Comonotonicity and measurability of individual consumption plans with respect to the aggregate endowment when there is aggregate risk have been studied in Chateauneuf et al. (2000) and Dana (2004) for non-additive (or Choquet) expected utilities of Schmeidler (1989), and, in greater generality, in Strzalecki and Werner (2011) for multiple-prior utilities, variational preferences, and the smooth ambiguity model. Chakravarty and Kelsey (2015) study efficient risk sharing for Choquet expected utilities with heterogeneous ambiguity. Relationship between ambiguity aversion and trade in complete markets is the subject of a paper by de Castro and Chateauneuf (2011). They show that if initial endowments are unambiguous, then the set of individually rational net trades gets smaller when agents become more ambiguity averse in the sense of Ghirardato and Marinacci (2002). Kajii and Ui (2006) study the existence of Pareto-improving “agreeable” bets between two agents with Choquet expected utilities when there is no aggregate risk. Dominiak et al. (2012) provide some extensions of the results to non-convex preferences. Kajii and Ui (2009) and Martins-da-Rocha (2010) study interim efficient allocations in an economy with asymmetric information and multiple-prior expected utilities. Araujo et al. (2017) consider uncertainty described by infinitely many states and study the existence of Pareto optimal allocations with convex and non-convex preferences under ambiguity. The paper is organized as follows. In Sect. 2 we introduce the multiple-prior expected utility and define risk-adjusted beliefs that are the basic tool in the analysis of Pareto optimal allocations. In Sect. 3 we review properties of Pareto optimal allocations for multiple-prior expected utilities. Our main results about non-participation in risk sharing are presented in Sect. 4. In Sect. 5 we extend the results to variational preferences. We conclude in Sect. 6 with comments on the assumptions.",1
90.0,3.0,Theory and Decision,29 September 2020,https://link.springer.com/article/10.1007/s11238-020-09775-z,Put–call parity and generalized neo-additive pricing rules,May 2021,Emy Lécuyer,Jean-Philippe Lefort,,Female,Unknown,Unknown,Female,"Cerreia-Vioglio et al. (2015) (CMM for brevity) generalized the Fundamental Theorem of Asset Pricing (FTAP) (see Ross 1976; Harrison and Kreps 1979) to financial markets with frictions in which put–call parity is satisfied. They developed an explicit asset-pricing formula, that states, the price must equal the Choquet expectation of the asset payoff with respect to a so-called ‘risk-neutral capacity’. We contribute to this discussion by further studying the relationship between the Choquet expectation and asset pricing. We study a special case of CMM financial markets when the set of prices is given by a generalized neo-additive capacity (GNAC). These capacities were developed by Chateauneuf et al. (2007) and Eichberger et al. (2012), resulting in streamlined price formulas with only two parameters and a probability. These limited parameters—the price of an asset is a weighted sum of the expected value (a frictionless price), and the maximal and minimal revenues—make the results simpler to understand and easier to calibrate, and to estimate (see the end of Sect. 3) than a Choquet expectation. The expected value parameter measures the effects of friction on pricing. The revenue parameters permit a natural interpretation when they are between 0 and 1, that is, when prices are given by a neo-additive capacity (NAC) but this additional requirement is compelling because it constraints the value of the bid–ask spread (viz. the difference between the price for an immediate purchase and the price for an immediate sell). We provide more insights into interpreting these parameters in both frameworks in Sect. 5. When prices are given by a GNAC, there is a theoretical connection between asset prices and risk. We show that the bid–ask spread is proportional to the range of an asset’s revenues. This is consistent with empirical evidence suggesting that bid–ask spreads vary linearly with risk (see Benston and Hagerman 1974; Stoll 1978, 1985; Amihud and Mendelson 1986), the range being a simple (albeit imperfect) measure of risk. Stoll (1978) and Amihud and Mendelson (1986) showed that this relationship is positive—i.e. the higher the risk, the broader the spread. This relationship can be understood naturally when prices are given by a GNAC: it means that there is no arbitrage opportunity in the spread. In terms of pricing, this is equivalent to placing a higher emphasis on maximal revenues than minimal revenues. In Sect. 6, we analyze the compatibility of a general-capacity price formula with a price formula given by a GNAC with frictions. A general capacity can be represented by the Weber setFootnote 1 of probabilities. We show that there is no friction among a subset of assets if, and only if, the Weber set probabilities coincide for a specific set of events. We conclude that, even when prices are represented by a general capacity, there might exist subsets of assets without frictions. Put in another way, the FTAP applies only over specific parts of a financial market when put–call parity is satisfied. Naturally, the set of prices given by a GNAC is less flexible. We show that either there is no friction in the market (and the FTAP applies everywhere) or there does not exist a risky frictionless asset. This apparent shortcoming is not particularly concerning because, in practice, there is unlikely to be a risky asset which can be added without friction to any other portfolio. The price formula remains compatible with the absence of friction among a subset of assets. We demonstrate, overall, that prices are given by a GNAC if, and only if, in addition to satisfying put–call parity, there is no friction among assets which yield extreme revenues in the same states of nature. Our work relates to asset pricing literature which aims to generalize the FTAP to markets with frictions. The principal contributions to this field are from Garman and Ohlson (1981) who proposed a model when prices are linear in the number of shares traded (positively homogeneous); Jouini and Kallal (1995) who generalized the FTAP to sublinear prices; and Prisman (1986) who proposed an extension of the FTAP for markets with convex fees such as taxes and, more recently, to CMM, whose model appears better suited for empirical research. We build on this literature by analyzing a special CMM circumstance when the price is given by a GNAC. The corresponding price formula is simpler and easier to calibrate, and it explicitly connects price with risk. We provide a characterization of these price formulas, and we compare them with the price formula given under a general capacity. This paper is organized as follows. Section 2 presents the framework and Sect. 3 presents the FTAP of Harrison and Kreps (1979) and the theorem of asset pricing of CMM. In Sect. 4, we present the price formula given by a NAC and the price formula given by a GNAC, and study how bid–ask spreads relate to risk. In Sect. 5, we interpret the parameters of a price formula given by a NAC and a price formula given by a GNAC, and we consider situations where price formulas given by a GNAC are better suited for asset pricing. In Sect. 6, we discuss the relationship between pricing formulas, under put–call parity, and the FTAP. Finally, in Sect. 7, we characterize the GNAC pricing formula. The mathematical proofs are presented in Appendix.",1
90.0,3.0,Theory and Decision,05 January 2021,https://link.springer.com/article/10.1007/s11238-020-09797-7,Feddersen and Pesendorfer meet Ellsberg,May 2021,Matthew Ryan,,,Male,Unknown,Unknown,Male,"McLennan (1998) distinguishes two types of Condorcet Jury Theorem (CJT). In McLennan’s typology, a CJT of the Second Type asserts that, under certain conditions, majority voting by privately but imperfectly informed jurors produces a correct decision with probability approaching unity as the jury size increases. Such theorems provide one possible formalisation of the “wisdom of crowds” . Condorcet assumed “sincere” voting—each juror votes as if deciding the outcome unilaterally—but a CJT of the Second Type also obtains when jurors vote strategically, with attention confined to symmetric Bayesian Nash equilibrium (henceforth, “equilibrium”) outcomes; see Austen-Smith and Banks (1996) and McLennan (1998).Footnote 1 More recently, Ellis (2016) proved a version of Condorcet’s result when jurors have ambiguous prior beliefs about the defendant’s guilt, but are not too imperfectly informed. In the canonical jury model, jurors share a common prior probability on the defendant’s innocence. They cast their votes without consultation,Footnote 2 after observing private signals drawn independently from a known state-conditional distribution, with the state being “guilty” or “innocent”. Jurors also share the common objective of maximising the expected probability of a correct decision (i.e., convict if guilty and acquit if innocent). In Ellis’s model, the common prior is replaced by a common set of priors; there is a common interval of probabilities on the innocent state. The canonical model is obtained when this interval is a singleton. After observing their private signals, jurors update each prior in the set and vote so as to maximise the minimum (over posteriors) of the expected probability of a correct decision. Relative to the standard model, the introduction of ambiguity alters the formal analysis in two fundamental respects: first, the voting decision of an individual juror can no longer be determined by conditioning on her vote being pivotal; second, a voter’s best response may necessitate randomisation. These features substantially complicate the analysis and Ellis does not give a complete characterisation of the equilibria of his model. However, he does prove some interesting general properties of these equilibria. When jurors “lack confidence” in their information (Ellis 2016, Definition 4)—that is, when the posterior interval contains a neighbourhood of \(\frac{1}{2}\) following any signal—there exists an equilibrium in which jurors cast either vote with equal probability (Ellis 2016, Proposition 1). Voting is completely uninformative about the jurors’ signals. For jurors who lack confidence, such voting is both sincere and a strict best response. Ellis (2016, Theorem 1) further shows that if jurors lack confidence, then in any equilibrium jurors vote for the incorrect decision at least as often as for the correct one, conditional on either state. These results apply to juries of any size: ambiguity and lack of confidence completely undermine the “wisdom” of the crowd. Conversely, provided the posterior intervals for different signals have disjoint interiors, then signals can be unambiguously ordered by the implied likelihood of innocence and a Condorcet-like Jury Theorem holds: as the number of jurors approaches infinity, there exists a sequence of equilibria along which the probability of a correct decision converges to unity (Ellis 2016, Theorem 2). Note the two italicised caveats. The results of Condorcet and Ellis assume that jurors apply equal utility penalties to each type of error: convicting the innocent and acquitting the guilty. However, Blackstone’s maxim exhorts us to guard against the former error more strenuously than we guard against the latter, and, therefore, to apply a higher utility penalty to conviction of the innocent than to acquittal of the guilty. If we follow Blackstone’s maxim then it is also natural to consider raising the voting hurdle for conviction; to trade off some decision accuracy for a reduced likelihood of the more grievous error. Surprisingly, however, Feddersen and Pesendorfer (1998) show that such a trade-off may be illusory. Raising the conviction hurdle may sometimes  increase the likelihood of convicting an innocent defendant; even more paradoxically, the probability of convicting the innocent always remains asymptotically bounded away from zero under the unanimity rule.Footnote 3 We call this latter result the Jury Paradox. It stands in stark contrast to Condorcet’s theorem. In this paper, we re-visit the Jury Paradox in the presence of ambiguity. We analyse the equilibria of Ellis’s model when conviction requires unanimity rather than a simple majority of guilty votes. To bring Ellis’s model in line with the framework of Feddersen and Pesendorfer, we make two other adjustments. First, we generalise Ellis’s juror utility function so that convicting the innocent may attract a higher utility penalty than acquitting the guilty. Second, we specialise Ellis’s information structure by assuming only two possible signal realisations. Feddersen and Pesendorfer’s (1998) model is a special case of ours in which prior ambiguity vanishes (the prior interval is a singleton). We prove that the Jury Paradox persists in the more general model: the equilibrium probability of convicting an innocent defendant is strictly bounded away from zero independently of the jury size; see Theorem 5.1. This is the main result of the paper. Our analogue of Feddersen and Pesendorfer’s paradox does not require either of the caveats in Ellis’s version of Condorcet’s theorem. Along the way to proving our main result, we also characterise the  strictly mixed equilibria of our model—the equilibria in which jurors randomise following either signal. Such equilibria are of particular interest since they cannot exist in the absence of ambiguity. In our model, there may even be multiple strictly mixed equilibria. There may exist a strictly mixed equilibrium with uninformative (or, in the language of Feddersen and Pesendorfer, “non-responsive” ) voting, just as there is under the majority rule (Ellis 2016, Proposition 1), and this equilibrium may co-exist with another strictly mixed equilibrium in which votes are informative (“responsive”): jurors randomise differently for different signals. Importantly for our purposes, responsive strictly mixed equilibria may exist asymptotically—for arbitrarily large jury size—so such equilibria cannot be ignored when proving the Jury Paradox. The next section introduces our model: it is a slight modification of Ellis’s which nests that of Feddersen and Pesendorfer as a special case. Section 3 describes the best response (to symmetric profiles) correspondence of a voter in our model, summarised in Fig. 1. This figure is an important guide for the reader through the subsequent analysis. Section 4 characterises a range of equilibria of the model. It is not an exhaustive stocktake but does describe all of the strictly mixed equilibria, which come in a variety of forms; Fig. 3 provides an overview of conditions for the existence of these equilibria. Our main result (the Jury Paradox) is contained in Sect. 5. Section 6 concludes. Several appendices contain longer proofs and other technical details that would otherwise disrupt the flow of the text.",2
91.0,1.0,Theory and Decision,28 June 2021,https://link.springer.com/article/10.1007/s11238-021-09830-3,Decision-making: from neuroscience to neuroeconomics—an overview,July 2021,Daniel Serra,,,Male,Unknown,Unknown,Male,"The question of how we make, and how we should make, decisions has occupied researchers for many centuries, with different disciplines approaching the topic by their characteristically methods and techniques. By the late 1990s, several converging trends in economics, psychology, and neuroscience had set the stage for the birth of a new field known as “neuroeconomics” for dealing with the decision-making problem by integrating insights from these three disciplines. The take-off of experimental and behavioral economics in the 1980s undoubtedly favored the emergence of the first studies in neuroeconomics by offering a set of well-codified experimental designs for dealing with individual and social decision-making in economic environments. Nevertheless, it is generally agreed that this new field is mainly based on the neuroscientific revolution of the 1990s, with the provision of sophisticated investigating tools, primarily functional magnetic resonance imaging, for visualizing what is happening inside the brain when humans make decisions. In a more fundamental way, neuroeconomics has largely been built on the fundamental knowledge developed by several branches of modern neuroscience. Neuroscience has always been a multi-disciplinary field, covering different explanatory goals, concepts and vocabularies, and different techniques and methods. One explicit aim of the Society for Neuroscience, which came into existence in 1970, was to integrate all these fields with the common goal of understanding the nervous system (Craver, 2007). Overall, neuroscience is usually divided into two vast fields: molecular/cellular neuroscience and cognitive neuroscience. The former studies neurons at a cellular level and examines the biology of the nervous system, while the latter is devoted to the study of neural mechanisms of mental and behavioral activities, or more generally, the relationships among the brain, mind, and action (e.g., Gazzaniga & Mangun, 2014). Neuroeconomics is closely, but not exclusively, associated with cognitive neuroscience. More specifically, within the mosaic of neuroscience, several branches support the field of neuroeconomics, including neurobiology, neuroimaging, neuroanatomy, neuropsychology, neurophysiology, neuroendocrinology, and computational/theoretical neuroscience. On the other hand, cognitive neuroscience includes several other sub-disciplines, in particular decision neuroscience, affective neuroscience, and social neuroscience, three cutting-edge fields whose boundaries with neuroeconomics are occasionally blurred due to the shared focus on decision-making, emotions, and behaviors. Decision neuroscience is broadly defined as a wide converging field between cognitive neuroscience and decision sciences (such as psychology and economics), while affective and social neurosciences pursue neighboring but more limited ends: the former studies neural mechanisms involved especially in emotion and affects, whereas the latter is devoted to understanding how biological systems implement social processes and behavior. Although neuroeconomics is still a nascent scientific field, 2 decades old at the most,Footnote 1 its domain is gigantic. It has already become the subject of a large number of papers, chapters in collective books, and of monographs providing overviews of the entire field, either as some of its parts, or as some major topics covered therein. Just a few years after the emergence of this new interdisciplinary domain, the first wave of surveys testified its double origin, with reviews from the perspective of economics on one hand (e.g., Glimcher & Rustichini, 2004; Camerer et al., 2005; Kennin & Plassman 2005) and from the perspective of neuroscience on the other (e.g., Glimcher, 2003; Montague & Berns, 2002; Montague, 2007). Over the past decade, a second wave of surveys on neuroeconomics has put forward the complementary strengths of its contributing disciplines (e.g., Sanfey et al., 2006; Sanfey, 2007; Camerer, 2008b; McCabe, 2008; Rangel et al., 2008; Loewenstein et al., 2008; Clithero, Tankersley, & Huettel 2008; Schultz, 2008; Rustichini, 2009). Last but not least, before the end of the decade, the first handbook of neuroeconomics was published by a neuroscientist (Paul Glimcher), a psychologist (Russell Poldrack), and two economists (Colin Camerer and Ernst Fehr), who grouped together important reviews on many topics, highlighting the strong interdisciplinary background of this new approach (Glimcher et al., 2009). The field of neuroeconomics matured intellectually after this first wide synthesis. In the 2010s, a third wave of surveys outlined new advances for understanding how, where, and when decision-making was accomplished in the brain and what remained to be done in the discipline as a whole (e.g., Huettel, 2010; Camerer, 2013; Dean, 2013; Rose & Abi-Rached, 2013; Smith et al., 2014; Krajbich & Dean, 2015) or more specially in some sub-fields, including neuroeconomics for social decisions (e.g., Rilling & Sanfey, 2011; Sanfey & Rilling, 2011; Singer, 2012; Declerck & Boone, 2016; Engelmann & Fehr 2017; Allos-Ferrer, 2018), neuroeconomics of emotion (e.g., Pessoa, 2013, 2017; Kragel and LaBar, 2016; Saarimäki et al., 2018; Adolphs & Anderson, 2018), or neural reward, learning, and computational models (e.g., Kable & Glimcher, 2009; Fehr & Rangel, 2011; Schultz, 2016; Padoa-Schioppa & Conen, 2017; Konovalov & Krajbich, 2019). In the meantime, the second edition of the Handbook edited by Glimcher and Fehr (2014a, 2014b) has widely reported many of these great advances. It includes well-documented specialized contributions (but often rather technical) on core concepts, methods, and tools used in neuroeconomic research (e.g., Rangel & Clithero 2014; Tobler & Weber, 2014; Kable, 2014; Fehr & Krajbich, 2014; Lempert & Phelps, 2014; Rustichini, 2005; Platt & Plassmann 2014; Crockett & Fehr, 2014) and a deal of new advances on how we learn and represent “value” in the brain (i.e., what is supposed to guide “economic behavior”) (e.g., Berridge & O’Doherty, 2014; Daw & Tobler, 2014; Daw, 2014), on the neural process of choice itself (e.g., Glimcher, 2014a, c; Wallis & Rushworth, 2014; Wang, 2014), and on social decision-making within the framework of game theory (Camerer & Hare 2014; Singer & Tusche, 2014). This paper is designed to be complementary to these previous studies, which are often referred to in the literature. Its particular edge is to focus mainly on the neuroscientific foundations of neuroeconomics to provide economists, particularly decision-making specialized economists, with a concise synthesis of these unfamiliar neuroscientific works the knowledge of which hopefully will allow them to easily get well versed with the neuroeconomic literature. This is not a survey of the great deal of neuroeconomic results available as on date; it just mentions some findings for illustrating applications to questions that are important to economists and how this new interdisciplinary approach can potentially improve our understanding of various economic decisions (several surveys cited above provide more thorough insights into neuroeconomic findings). The paper also does not address the thorny methodological and epistemological issues that neuroeconomic approach is likely to raise, including the “mindless economics” argument that non-behavioral data (such that neural and physiological data) are not “relevant” in economics (Gul & Pesendorfer 2008) (on this issue, refer to Harrison & Ross, 2010; Mäki, 2010; Serra, 2021). Another singularity of the paper is to group approximately three non-independent broad topics as the main neuroscientific progress from which neuroeconomics has benefited. The first topic is devoted to emotion processing by the brain and the findings regarding the interconnection of emotions and higher cognitive processes. The key role of emotion in economic decisions is now unanimously recognized as a teaching of neuroeconomics, in the wake of behavioral economics. Yet, neuroanatomy, affective neuroscience, and neuropsychology are at the heart of identifying neural structures and mechanisms involved in both cognitive processes and emotional responses. The second topic refers to the human brain’s considerable flexibility and ability to undertake complex patterns of social cognition. Social neuroeconomics focuses on decisions made in a social context and seeks to explain in particular prosocial behaviors. This neuroeconomic subfield relies partly on findings of social neuroscience about the neural networks that are responsible for interpreting other people’s thoughts and feelings, sympathizing with their states of mind, and acting in a moral manner, namely “mentalizing”, mirror neurons, and empathy systems respectively. The third topic deals with reward learning as a new theoretical framework for neuroscience and the identification of brain mechanisms deployed for learning and valuing the many stimuli that the brain is continuously subjected to. Neuroeconomics can now draw the contours of a computational model of how the brain makes simple economic choices, and recent studies have explored how this structural model may extend to more complex decisions, such as risky decisions, intertemporal choices, and social decisions respectively. These works are mainly rooted in neurobiology and computational/theoretical neuroscience while taking into account findings from studies relating to the other two topics. The paper is organized as follows. After a brief mention of the aim of neuroeconomics from a historical perspective (Sect. 2), it will deal with these three topics by talking about the “emo-rational” brain (Sect. 3), “social” brain (Sect. 4), and “computational” brain respectively (Sect. 5). To make the paper accessible to a large audience, the various neuroscientific notions used are defined and briefly explained accordingly. In the same way, the definition of the main economic models referred to in the text is recalled for researchers not specialized in experimental and behavioral economics,",3
91.0,1.0,Theory and Decision,07 November 2020,https://link.springer.com/article/10.1007/s11238-020-09781-1,Redistribution to the less productive: parallel characterizations of the egalitarian Shapley and consensus values,July 2021,Koji Yokote,Takumi Kongo,Yukihiko Funaki,Male,Male,Male,Male,"In modern societies, (income) redistribution plays a key role in preserving equity. As such, it is one of the representative roles of public offices and is implemented through policies such as taxes and subsidies. In simple models of redistribution, Casajus (2015, 2016) and Yokote and Casajus (2017) characterize proportional taxation. Cooperative games with transferable utilities (henceforth, TU games) describe problems related to allocating an economic surplus within a group of players, which includes redistributing the surplus among them. To analyze redistribution in as clear a manner as possible, we confine our attention to nonproductive agents (called null players in TU games), and examine their payoffs under various solutions. Crucially, the Shapley value (Shapley 1953), one of the central solutions in TU games, assigns a zero payoff to null players. From the Shapley value, there are two well-established ways of implementing redistribution: using egalitarian Shapley values (Joosten 1996; van den Brink et al. 2013), and using consensus values (Ju et al. 2007). These values are convex combinations of the Shapley value and the equal division value or the equal surplus division value, respectively. These sets correspond to situations where each player obtains his/her Shapley value payoff, pays a constant rate as tax, and then receives either the same amount of refund as everyone else, or an amount that depends on his/her individual productivity, respectively. Although the two are closely related, no parallel characterizations of the two classes of values currently exist. Therefore, this study attempts to fill this gap in the literature by extending the axiomatic framework of Yokote et al. (2018), who characterize a wide class of solutions, including the egalitarian Shapley values, but not the consensus values. Together with efficiency, they focus on weaker variations of the balanced contributions property (Myerson 1980) and covariance. The balanced contributions property requires that a player’s contribution to the payoff of the other player is balanced with the reverse contribution. The weaker balanced contributions property of Yokote et al. (2018), called the balanced contribution property for equal contributors, requires the same condition as the original, but only for equal contributors (i.e., players whose marginal contributions to the grand coalition are the same). For the covariance, we change the game such that a player’s contribution to any coalition varies in a fixed amount, keeping the contributions of all other players fixed. For such a change, all of a player’s increased/decreased contributions belong to the player, and are not redistributed to the other players. The weaker covariance of Yokote et al. (2018), called weak covariance, distributes the player’s increased/decreased contributions to the other players, following the solution applied to the original game with homogeneity. Weak covariance can be viewed as a requirement for the redistribution of the surplus of a productive player to all null players. This property focuses on redistribution in one-person unanimity games, which are special cases of unanimity games for singleton coalitions, and which are equivalent to dictator games. Here, we add a further requirement on redistribution in one-person unanimity games. The property of proportionately decreasing redistribution in one-person unanimity games requires that the redistribution that each null player receives decreases as the number of players increases. Adding this axiom to the three of Yokote et al. (2018), we refine the characterized solution as the class of affine combinations of the Shapley value and the equal division value. Furthermore, by adding desirability (Maschler and Peleg 1966), which requires that players with higher (or equal) marginal contributions obtain higher (or equal) payoffs, and null players in unanimity games, which requires that null players obtain nonnegative payoffs, to the four axioms, the class of egalitarian Shapley values is characterized. In contrast, the class of affine combinations of the Shapley and the equal surplus division values is characterized by efficiency, an alternative balanced contributions property focusing on other players, and axioms on redistribution in the opposite situation, where the player is null and all others are together productive (i.e., \((n-1)\)-person unanimity games). Because the equal surplus division value for a game is obtained as the equal division value for its zero-normalized game, we require the balanced contributions property for equal contributors in the latter game (i.e., equal contributors to social surplus). For the requirements on redistribution, we introduce complement weak covariance and a proportionately decreasing redistribution in \((n-1)\)-person unanimity games. Each of these is a parallel axiom to weak covariance or to proportionately decreasing redistribution in one-person unanimity games. Furthermore, by adding desirability and null players in unanimity games, the consensus values are characterized. Other related works include the following. First, the differences between the equal division and equal surplus division values for the class of games with a fixed player set are explained by the differences between players who affect the worth of coalitions (van den Brink 2007; Casajus and Huettner 2014a). In addition, the two values are compared in terms of the average coalitional desirability and the average coalitional surplus desirability (Béal et al. 2019; Hu 2019). For the class of games on a variable player set, the differences between the two values are explained by those between players whose existence does not change the remaining players’ payoffs (Béal et al. 2015). Next, we discuss the literature on the egalitarian Shapley values and the consensus values. Casajus and Huettner (2013, 2014b) characterize the egalitarian Shapley values for the class of games on a fixed player set. These studies focus on the null player’s payoff when the worth of the grand coalition is nonnegative, and on a variation of the monotonicity of Young (1985), respectively. Yokote and Funaki (2017) characterize convex combinations of the Shapley, equal division, and equal surplus division values for games with six or more players. They also incorporate variations of the monotonicity introduced by Young (1985). For the class of zero-monotonic games on a variable player set, each of van den Brink et al. (2013) and Ju and Wettstein (2009) discuss a framework for implementing the egalitarian Shapley values or the consensus values by generalizing the bidding mechanism introduced by Pérez-Castrillo and Wettstein (2001). Yokote et al. (2019) characterize the affine combination of the three values. They use a weaker balanced contributions property and the weak null player out property of van den Brink and Funaki (2009), which is a generalization of the null player out property of Derks and Haller (1999).Footnote 1 The rest of the paper is organized as follows. Section 2 presents the preliminaries, and Sect. 3 introduces axioms. Section 4 presents results, and Sect. 5 concludes. The Appendix provides proofs of the results.",2
91.0,1.0,Theory and Decision,21 December 2020,https://link.springer.com/article/10.1007/s11238-020-09793-x,Correction to: Redistribution to the less productive: parallel characterizations of the egalitarian Shapley and consensus values,July 2021,Koji Yokote,Takumi Kongo,Yukihiko Funaki,Male,Male,Male,Male,"In Sect. 3.3, the terms “one-person” and “(n − 1)-person” were incorrectly updated by mistake during the correction stage in the online published article. The correct sentences are given below: Proportionately decreasing redistribution in one-person unanimity games (PDR1): For any \((N,u_{i} ),(M,u_{j} ) \in G\), with \(i,k \in N\), \(i \ne k\), \(2\, \le \,|N|\), \(j,\ell \in M\), \(j \ne \ell\), and \(2\; \le \;|M|\), \(|N|f_{k} (N,u_{i} ) = |M|f_{\ell } (M,u_{j} )\). Proportionately decreasing redistribution in \(\left( {{{n}} - {1}} \right)\)-person unanimity games (PDR\(^{n - 1}\)): For any \((N,u_{{N{ \setminus }i}} ),(M,u_{{M{ \setminus }j}} ) \in G\), with \(i \in N\), \(3\; \le \;|N|\), \(j \in M\), and \(3\; \le \;|M|\), \(|N|f_{i} (N,u_{{N{ \setminus }i}} ) = |M|f_{j} (M,u_{{M{ \setminus }j}} )\). The original article has been corrected accordingly. We apologize for the error introduced during the production process.",
91.0,1.0,Theory and Decision,17 November 2020,https://link.springer.com/article/10.1007/s11238-020-09788-8,How do risk attitudes affect pro-social behavior? Theory and experiment,July 2021,Sean Fahle,Santiago I. Sautua,,Male,Male,Unknown,Male,"Individuals often undertake pro-social behavior even when its effectiveness is uncertain. As an example, consider the situation of a teacher choosing whether to expend time and energy to provide extra after-hours help to a struggling student. On the one hand, by helping the student, the teacher incurs a certain cost in terms of her forgone time. On the other hand, the benefit to the student is uncertain: the teacher is only able to improve the student’s chances of a successful outcome. Similar situations are common and include physicians performing risky operations for patients and parents making risky investments in their children. In this paper, we are concerned with how this type of social decision is impacted by the decision-maker’s risk preferences. Very few empirical studies have directly examined the link between individual risk preferences and pro-social behavior. Bolton and Ockenfels (2010) found that individuals are more risk-averse when taking risks on behalf of others. By contrast, the results from two earlier studies (Brennan et al. 2008; Güth et al. 2008) suggest that while players care about their own risks, they do not appear to respond to risks faced by others. Most recently, in settings similar to our own, Freundt and Lange (2017) and Cettolin et al. (2017) both found that a giver’s risk preferences do appear to matter for giving behavior.Footnote 1 An important limitation of the existing empirical literature is that it generally explores the relationship between risk attitudes and pro-social behavior outside of the context of any particular model of behavior. As a result, the literature has remained largely silent on the precise mechanism underlying this connection. We seek to address this limitation here. In this paper, our aim is to build upon these existing results by taking a more model-driven approach. This approach allows us to provide (and experimentally test) a more precise characterization of how a decision-maker’s risk preferences interact with her other-regarding preferences to affect her pro-social behavior in risky environments.Footnote 2 Our first contribution is to embed reference-dependent risk attitudes into a standard social preferences model and to derive testable implications for the giving behavior of a dictator in a risky dictator game. In the augmented model, the dictator evaluates both her own sure payoff and the recipient’s risky payoff relative to a reference point. Because we consider situations in which the giver knows the risk faced by the recipient but does not know the recipient’s risk preferences, our basic premise is that the giver projects her own risk preferences onto the recipient when deciding how much to give. This premise relies on the concept of social projection, which social psychologists have defined as the tendency to expect similarities between oneself and others. The use of social projection to make predictions about others has been widely documented, especially in situations in which information about others is limited (Robbins and Krueger 2005; Krueger 2007). With regard to risk attitudes, we concentrate on the giver’s loss aversion. We do so for two reasons. First, the stakes in our experiment are small, and in such settings, the diminishing marginal utility of wealth should not play a role (Rabin 2000; Rabin and Thaler 2001; Kőszegi and Rabin 2007). By contrast, loss aversion has been identified as a key determinant of risk attitudes under small- or modest-scale risk.Footnote 3 Second, a sizeable literature has documented the importance of reference-dependent preferences, and loss aversion in particular, for rationalizing experimental anomalies and non-standard behavior across a variety of domains.Footnote 4 To date, Cettolin et al. (2017) have provided the only formal model incorporating both social preferences and risk preferences.Footnote 5 As the authors themselves have acknowledged, their approach has two important limitations. First, they develop the model after conducting their laboratory experiments in an attempt to explain the observed link between risk attitudes and giving behavior. Hence, by design, their experiments do not provide a sharp test of their model. Second, although the model is primarily intended to explain behavior under small-scale risk, it retains classical expected utility assumptions, which imply risk neutrality in such a setting. By contrast, we allow for reference-dependent risk preferences, so our model is well-suited to making predictions in situations with small- or modest-scale risk. Illustrating the value of taking a model-oriented approach, we find that our model makes a distinctive prediction about the link between social preferences and loss aversion in our setting. For dictators who perceive giving as a loss, giving entails a trade-off between their own losses and the recipient’s potential losses. In these cases, our model implies that the net effect of loss aversion on giving behavior is mediated by the strength of the dictator’s social preferences. Dictators with weak social preferences will weigh their own losses more heavily than the recipient’s potential losses. For these dictators, therefore, the probability of giving will be decreasing in their degree of loss aversion. By contrast, among dictators with strong social preferences, those who are more loss averse will be more likely to give. As we show in Sect. 5, this nuanced relationship between social preferences and risk preferences may not hold in settings with large-scale risk, where the diminishing marginal utility of wealth is the main driver of risk aversion. There, we provide an example in which, when the stakes are large, a more risk-averse dictator will generally give more than a less risk-averse dictator.Footnote 6 A strength of our approach is its generality. The theoretical results we derive for situations with small- or modest-scale risk are robust across numerous different motives for giving, such as inequality aversion (Fehr and Schmidt 1999; Bolton and Ockenfels 2000), efficiency (“utilitarianism,” or total surplus maximizing), social-welfare or quasi-maximin (Charness and Rabin 2002), and ego-centric altruism (Cox et al. 2007). In addition, the predictions hold regardless of whether decision-makers evaluate utilities ex-ante, ex-post, or both. Our second contribution is to test the predictions of the augmented social preferences model using data collected from a laboratory experiment. Our primary experimental tasks are a series of modified dictator games in which a dictator can allocate tokens to an anonymous recipient to increase the chances that the recipient wins a lottery. The dictator herself faces no risk. This intentionally simple design is intended to enhance the salience of the risks faced by the recipient and lessen the potential for “cognitive crowd-out” to lead givers to ignore risks faced by recipients, as may have occurred in previous work (Brennan et al. 2008; Güth et al. 2008). In contrast to other recent studies that have analyzed similar risky dictator games (Krawczyk and Le Lec 2010; Brock et al. 2013; Freundt and Lange 2017; Cettolin et al. 2017), we focus on situations in which behavior is largely unaffected by the giver’s precise motive for giving. To test our augmented social preferences model, we combine the data from the dictator game tasks with data from an additional suite of tasks that elicit measures of loss aversion and other-regarding preferences. We then compare giving behavior across dictators who exhibit varying degrees of loss aversion, controlling for individual differences in the strength of social preferences. We find results broadly supportive of our augmented social preferences model. Most significantly, and consistent with the predictions of the model, we find that the effect of a dictator’s loss aversion on her giving behavior is mediated by the strength of her social preferences. While loss aversion reduces the probability of giving for less pro-social dictators, among more pro-social dictators, we observe that those who are more loss averse are more likely to give. The rest of the paper proceeds as follows. Section 2 presents our social preferences model augmented with reference-dependent risk preferences. Section 3 describes our experimental design. Section 4 discusses the experimental results and assesses their robustness. In Sect. 5, we broaden the scope of our analysis beyond reference-dependent risk preferences and consider how our model can be extended to settings with large-scale risk, in which another dimension of risk aversion, the curvature of the utility function over wealth, plays a key role. Section 6 concludes.",2
91.0,1.0,Theory and Decision,20 November 2020,https://link.springer.com/article/10.1007/s11238-020-09786-w,"Risk aversion, downside risk aversion, and the transition to entrepreneurship",July 2021,Claudio A. Bonilla,Marcos Vergara,,Male,Male,Unknown,Male,"The connection between entrepreneurship and risk aversion is an old idea, initially discussed by Knight (1921) and later formalized by Kihlstrom and Laffont (1979). The main idea behind this theory is that the wealthy are, on average, less risk averse than the poor because well-behaved utility functions present decreasing absolute risk aversion (DARA) and, therefore, the wealthy are more prone to starting risky ventures. In addition, most of the recent literature that uses microeconomic models to study the transition from secure employment to entrepreneurship builds either on the DARA assumption (Cressy 2000; Ahn 2009, or Hvide and Panos 2014) or on prudent behavior (Bonilla and Vergara 2013), which is also induced by DARA. This paper uses results from the apportioning risk literature to dive deeper into the problem of self-selection of occupations and entrepreneurship. In particular, we argue that the choice of self-selection of occupations can be interpreted as the decision of choosing between lotteries. In the real world, an individual decision maker may have a portfolio of different potential entrepreneurial ventures to undertake, each representing a different lottery. However, for the purposes of this paper, we will study the case in which the individual compares only two alternatives: continuing to be employed, with no risk—this lottery is simply a degenerate probability distribution that collapses at some point—; and transitioning to his best entrepreneurial option, which represents the risky lottery that we will analyze in Sect. 3. We think that our theory can be tested in a laboratory experiment, given that our result is connected with the difference between the strength of downside risk aversion (prudence) and the strength of risk aversion. Moreover, since there are accepted experimental methodologies to test risk aversion and prudence, our result could also, in principle, be tested in the laboratory. Examples of the experimental literature that deals with risk aversion and prudence are the works of Deck and Schlesinger (2014), Ebert and Wiesen (2014) and Krieger and Mayrhofer (2017). Running experiments are beyond the scope of our own paper but we believe that, starting from the above mentioned experimental papers, a specific experiment can be designed to analyze the occupational self-selection decision expressed as a behavioral choice between lotteries. Almost all previous literature on occupational self-selection assumes that when the marginal decision-maker experiences a small increase in wealth, he transitions from secure employment to risky entrepreneurship, as long as his utility function exhibits the DARA property (as a sufficient condition). In this paper, we expand on the self-selection occupational decision. From Bonilla and Vergara (2013), we know that there are other cases where prudent agents with CARA or IARA preferences can also transition to entrepreneurship when wealth increases. This is based on the desegregation of the income and substitution effects produced by random risk shocks. The idea was first developed in Dreze and Modigliani (1972) and later explained in Davis (1989) and Snow (2003). In this paper, we delve deeper into the analysis of the transition to entrepreneurship. We use the risk apportionment perspective and the main result in Eeckhoudt et al. (2009) [herein, EST (2009)] to show that, as long as the strength of downside risk aversion exceeds the strength of risk aversion, the indifferent decision-maker will decide to self-select into entrepreneurship. Our result highlights the idea that when using the behavioral approach of lotteries to model entrepreneurship, we have to go beyond the risk aversion and DARA (or prudence) assumptions in order to guarantee the transition from secure employment into risky self-employment. Therefore, even though the apportioning risk literature is a novel and intuitive way to model higher-order risk preferences (Deck and Schlesinger 2014), we have to be cautious in the applications of this method to applied economic problems because differences with the traditional models may occur. These potential differences highlight the fact that public policies promoting entrepreneurship are going to be—at least in part—dependent on the approach that policy-makers use in the study of entrepreneurship. In consequence, empirical and experimental studies become of key importance to clarify in what context one approach or the other is the right one to apply and also what polices better promote entrepreneurship in each context. In the next section, we present the basic model of occupational self-selection. Section 3 presents the features of increases in risk and the main result of this paper, and Sect. 4 concludes.",5
91.0,1.0,Theory and Decision,21 December 2020,https://link.springer.com/article/10.1007/s11238-020-09794-w,Construal level theory and escalation of commitment,July 2021,Nick Benschop,Arno L. P. Nuijten,Harry R. Commandeur,Male,Male,Male,Male,"Escalation of commitment to a failing course of action can be a costly decision error, both at an individual and at an organizational level (Brockner, 1992; Keil et al., 2000a; Sleesman et al., 2012; Staw, 1976). One of the reasons for such escalation is the sunk cost fallacy (Arkes & Blumer, 1985; Johnstone, 2002; Staw, 1976). This fallacy states that people are more likely to invest in a project if they have invested in this project previously. Various other project, psychological, social, and organizational factors have been shown to drive escalation of commitment as well (Hollar et al., 2012; Sleesman et al., 2012). A study by Hollar et al., (2012) suggests that, amongst these, psychological factors are the strongest driver of escalation of commitment behavior. A potentially important psychological factor that has remained unexplored thus far, is construal level. Construal level involves the degree of psychological distance that people experience at the time of making a decision (Trope & Liberman, 2010). Psychological distance increases as the object that is being construed mentally, is further away (e.g., temporally, geographically, socially or in hypotheticality). In this paper, we investigate how construal level affects escalation of commitment. Consistent with the widely held view of construal level as a primed effect, we employed a commonly used prime for manipulating this construct in a laboratory experiment. In addition, we measured subjects’ construal levels using the Behavior Identification Form (BIF) (Vallacher & Wegner, 1989). As explained later, while we found no evidence for the primed effect of construal level on escalation, we did find evidence for the effect on escalation of construal level as a trait. To probe the mechanism through which construal level may influence escalation, we examined three potential mediators that have previously been associated with construal level. The remainder of the paper is organized as follows: first, we offer a brief overview of the relevant literature and the theory base that we draw upon. Next, we introduce our hypotheses and research model. Then, we describe the results that were obtained. Finally, we discuss the implications of our study.",5
91.0,2.0,Theory and Decision,03 January 2021,https://link.springer.com/article/10.1007/s11238-020-09798-6,A measure of ambiguity (Knightian uncertainty),September 2021,Pavlo Blavatskyy,,,Male,Unknown,Unknown,Male,,
91.0,2.0,Theory and Decision,09 February 2021,https://link.springer.com/article/10.1007/s11238-020-09799-5,Experimental evidence of behavioral improvement by learning and intermediate advice,September 2021,Daniela Di Cagno,Werner Güth,Noemi Pace,Female,Male,Female,Mix,,
91.0,2.0,Theory and Decision,22 December 2020,https://link.springer.com/article/10.1007/s11238-020-09792-y,When do the expectations of others matter? Experimental evidence on distributional justice and guilt aversion,September 2021,Riccardo Ghidoni,Matteo Ploner,,Male,Male,Unknown,Male,"A large body of literature has demonstrated that individuals are not only motivated by self-interest but also care about the consequences of their actions for others (e.g. Fehr and Schmidt 1999; Bolton and Ockenfels 2000; Charness and Rabin 2002). More recently, experiments have highlighted that also what others expect from us can influence the choices we make. Individuals tend to adjust their behavior not to let down others and avoid feeling guilty (see, among others, Baumeister et al. 1995; Charness and Dufwenberg 2006). However, according to social psychology literature, the emotion of guilt has a context-specific component, with some contexts being more conducive to guilt than others (Tangney 1992). Understanding under which circumstances the emotion of guilt plays an economically relevant role is an under-investigated issue. Here, we conjecture that others’ expectations can be perceived as more or less legitimate, depending on the context faced by the decision-maker, and test whether decision-makers fulfill others’ expectations even when they clash with justice principles (see Bicchieri 2006, for a similar conjecture). We focus on a fundamental justice principle that motivates individuals to seek an equitable (proportional) allocation in terms of effort exerted to create an output and reward obtained for this effort (Konow 2000). This general distributional principle captures the essence of Locke’s law of nature, i.e. that property rights on goods originate in the effort exerted to generate them (Hoffman and Spitzer 1985). Our study investigates whether others’ expectations are more likely to be fulfilled when they are not in conflict with this acknowledged justice principle. The interaction between guilt feelings and justice considerations might shape behavior in relevant economic interactions. Think, for example, of an employer who must choose between promoting an overconfident employee or an underconfident one. If the employees have similar performances, a guilt averse employer should give the promotion to the overconfident employee to minimize guilt for letting down one of the two employees. Similarly, if the best performing employee has (correctly) higher expectations of getting the promotion, this can further motivate a guilt averse employer to give the promotion to her. However, if the underconfident employee is the best performing one, the employer could give her the promotion, neglecting the employees’ expectations. Another example may come from charity giving. Think of donations to individuals who are facing the consequences of a natural disaster. Likely, a guilt averse individual will donate to meet the expectations of those in need. Yet, the emotional rush to give may be held back by considerations about potential corruption in the allocation process: if donations are likely to end in the wrong hands, even a guilt averse individual may refrain from giving. We investigate the interplay between guilt and justice considerations in two distinct laboratory experiments. Study 1 builds on a modified dictator game where there is a probability with which a “lost wallet” is restored in the hands of the entitled owner, conditional upon the dictator choosing to return it. A returned wallet can also be misplaced by Nature to an unentitled recipient—who did not exert any effort to earn it—leading to an inequitable allocation. Only the dictator knows this specific, exogenous, restoring probability. Therefore, the entitled recipient cannot condition her expectation (and hence her disappointment for a missed return) upon the restoring probability. We communicate the entitled recipient’s expectation to the dictator to causally identify the effect of expectations. Moreover, we control for potential confounds linked to dictators’ self-serving biases by running a robustness check experiment that replicates the essential features of Study 1 but replaces the dictator with an external spectator with no material stake in the game (e.g., Almås et al. 2020). In Study 2, an external spectator must allocate a reward to one of two individuals that may differ in their expectations of being rewarded and in their desert, as captured by their relative productivity. Study 2 allows for a cleaner empirical identification than Study 1 and allows us to check the robustness of our conjecture across different setups. In all our studies, simple guilt aversion predicts that decision-makers should try to fulfill expectations regardless of justice considerations (Battigalli and Dufwenberg 2007). According to our hypothesis, instead, they should be more likely to fulfill expectations when doing so also ensures a proportionality between effort exerted and rewards obtained. When fulfilling others’ expectations leads to a violation of justice principles, we expect optimistic expectations to become less relevant. In Study 1, returning the wallet to meet the optimistic expectations of the recipient may entail the risk of violating entitled ownership. In Study 2, meeting optimistic expectations may penalize the best performing worker. Thus, in both studies, expectations seem legitimate when they do not conflict with justice considerations based on effort-related entitlement. While the literature on guilt aversion is rapidly growing, we are aware of only a few recent experiments that touch upon the issue of expectations’ legitimacy (Balafoutas and Fornwagner 2017; Pelligra et al. 2020). These studies focus on the nature of the requests made by recipients/trustees to dictators/trustors. When requests are too ambitious, they may not trigger guilt feelings because they are perceived as not legitimate. Another related study is the experiment by Danilov et al. (2018), who study the impact of descriptive norms and guilt feelings on giving in the dictator game. We share with these studies the attempt to refine the definition of guilt. However, our work differs from previous studies in the approach to expectations’ legitimacy. We adopt a widely acknowledged justice principle according to which outputs of the production should be allocated in proportion to individual inputs (Konow 2000), and define beliefs’ legitimacy in terms of accordance with this principle. Our data show that both guilt aversion and justice considerations are key in driving allocation choices. Study 2 provides us with a direct assessment of the importance of the two sources and clearly shows that guilt is of secondary importance relative to justice. Furthermore, in contrast to our initial hypothesis, we do not identify any positive interaction between the two motivational sources. In fact, our studies show the opposite. Dictators in Study 1 and external spectators in Study 2 tend to neglect counterparts’ expectations when the distributional norm is clear, namely when the restoring probability is high in Study 1 and when a worker is better than the other. However, guilt aversion is still relevant in cases in which the distributional norm is less clear. These results are overall confirmed also by the robustness check of Study 1. In the concluding section, we discuss these findings and call for further research on the interaction between distributional norms and expectations. The remainder of the paper is organized as follows. In Sect. 2, we position our contribution in both the literature on guilt aversion and on distributional justice. Sections 3 and 4 report design, hypotheses, and results for Study 1 and Study 2, respectively. General conclusions are discussed in Sect. 5.",1
91.0,2.0,Theory and Decision,01 January 2021,https://link.springer.com/article/10.1007/s11238-020-09795-9,Experimental cheap talk games: strategic complementarity and coordination,September 2021,Francisca Jiménez-Jiménez,Javier Rodero Cosano,,Female,,Unknown,Mix,,
91.0,2.0,Theory and Decision,02 January 2021,https://link.springer.com/article/10.1007/s11238-020-09791-z,Measuring rationality: percentages vs expenditures,September 2021,Roy Allen,John Rehbeck,,Male,Male,Unknown,Male,"Since the work of Afriat (1973) there has been research to define various “measures of rationality.”Footnote 1 The majority of this research uses a revealed preference approach. In particular, when a dataset of individual choices cannot be described by a given class of preferences, the “measure of rationality” describes the size of the violations. Recently, different “measures of rationality” are checked for correlation with observable information (e.g. age, education, income, gender, etc.).Footnote 2 We believe that interest in correlating measures of rationality and observable information arises to inform policy questions, including the scope of paternalistic policies. However, for correlations between a “measure of rationality” and observable information to inform policy, it is important that the policy recommendations are robust to details of the “measure of rationality.” Unfortunately, policy recommendations can differ in important ways by changing the units that describe a violation from rationality. We present an intuitive discussion of how measures can differ. Consider trying to answer the following question: Are individuals with low expenditures more or less rational than those with high expenditures? In this case, an intuitive hypothesis may be that low expenditure individuals make errors in their choices, but the errors are of small monetary value. However, even when the errors are of small monetary value, the errors may be a large percentage of an individual’s expenditure. This means that one may find that lower expenditure individuals are “less rational” when one measures errors in percentages, but “more rational” when one measures the errors in dollars. Thus, the answer to the question “Who is more rational?” can depend on the units the researcher uses to measure “rationality.” Since this is an emerging literature, we provide a proof of concept of this empirical issue by examining grocery store scanner data from the Stanford Basket Dataset used previously in (Echenique et al. 2011) and data from the experiment on risk preferences by Choi et al. (2014). Rather than comparing results across different measures, we fix attention to the Afriat Inconsistency Index (AII) (Afriat 1973; Halevy et al. 2018). The AII measures violations of rationality using a measure in “percentage of expenditures,” but it is easily transformed to dollars by multiplying by total expenditures. Thus, this paper looks purely at the issue of whether it matters to measure rationality in percentages versus dollars. Fixing the measure of rationality and changing units addresses our fundamental measurement question, and may be relevant for sensitivity of units for other “measures of rationality.” We provide a high level description of the results here. For the Stanford Basket Dataset, we find that the correlation with the measure of rationality is sensitive to units. First, expenditure has a positive or negative correlation with rationality depending on the units of measurement. In addition, the coefficient on family size switches signs when changing the units of the measure of rationality. In contrast, when we examine the experimental results from Choi et al. (2014), we find that there are no qualitative changes between the analysis for either measure of rationality. Overall, these results show that the units at which rationality is measured can affect the analysis and that experimental design can potentially mitigate the issues.Footnote 3 The empirical results suggest that one should be careful in taking measures of rationality to other settings. For example, it may be tempting to use a single “measure of rationality” when studying the economics of poverty. In particular, there is a long debate on whether individual characteristics or structural forces are the main force that drives poverty.Footnote 4 It seems like decision making quality is an important piece of the puzzle to understanding poverty, so a “measure of rationality” seems useful to examine this question. However, the issue presented in this paper suggests that one needs to carefully consider the units over which scarcity arises in order to use one of these “measures of rationality” effectively. Which units are preferred in an application is likely to be a judgment call on the part of the researcher. For example, economists often prefer unit-free measures (e.g. percentages) since they allow researchers to abstract from the level of expenditures and income of an individual. However, there is recent evidence from Shah et al. (2012, 2018a, b) that suggests individuals who experience scarcity of money may focus more on some economic problems and have a more consistent valuation of what something is worth than those who are less monetarily constrained. This research suggests it may be important to account for expenditure/income and use a “measure of rationality” in a dollars unit since individuals face different pressures based on their monetary constraints. Ideally, empirical analysis would be robust to units used to measure rationality, but this will not always be the case and it will be left to the discretion of the researcher. The remainder of the paper proceeds as follows. Section 2 provides the relevant revealed preference definitions and a description of the measure of rationality. Section 3 compares measures for the Stanford Basket Dataset using the main analysis from Echenique et al. (2011). Section 4 compares the measures from the experiment of Choi et al. (2014). Section 5 examines how omitted variable bias could cause the change in correlation between the percentage and dollars measures. Section 6 contains our final remarks.",2
91.0,2.0,Theory and Decision,10 January 2021,https://link.springer.com/article/10.1007/s11238-020-09790-0,Nash implementation via mechanisms that allow for abstentions,September 2021,Jianxin Yi,,,Unknown,Unknown,Unknown,Unknown,,
91.0,3.0,Theory and Decision,23 April 2021,https://link.springer.com/article/10.1007/s11238-021-09812-5,Organizational refinements of Nash equilibrium,October 2021,Takashi Kamihigashi,Kerim Keskin,Çağrı Sağlam,Male,Male,Male,Male,"The notion of Nash equilibrium is the most prominent solution concept in game-theoretic analysis (see Nash 1951). Motivated by the fact that having multiple equilibria might be undesirable, numerous papers in non-cooperative game theory focus on approaches for refining the set of Nash equilibria (see Aumann 1959; Selten 1965, 1975; Myerson 1978; Kohlberg and Mertens 1986; Bernheim et al. 1987, among others). These equilibrium refinements utilize different methods to decrease the number of equilibria; for example, there are refinements that allow players to form coalitions and make joint deviations. Among such coalitional refinements, we are mainly concerned with strong Nash equilibrium (SNE) (see Aumann 1959) and coalition-proof Nash equilibrium (CPNE) (see Bernheim et al. 1987) in this paper.Footnote 1 To elaborate on these notions, one can say that the members of any particular coalition would prefer not to deviate collectively from a SNE strategy profile. As coalitions do not face too many restrictions in choosing their joint deviations, the set of SNE generally turns out to be empty. Expanding from this observation, Bernheim et al. (1987) propose the notion of CPNE according to which coalition members cannot make binding commitments to deviate (i.e., the deviation agreements must be “self-enforcing”Footnote 2). Accordingly, a strategy profile is said to be coalition-proof, if no coalition is able to deviate from that profile via self-enforcing agreements. An important observation would be that both of these refinements consider cases in which any coalition can be formed. Yet in real-life situations, we see many instances where some coalitions cannot be formed. Moreover, even when a particular coalition is formed, this does not necessarily imply that all of its subcoalitions would be formed. Indeed, a game might have players that hate/dislike each other or who simply cannot communicate to participate in a coalition. Consider, for example, two countries with a history of bad relations. These countries might not prefer to create a two-player coalition; or even if they meet at a global association, they might still refuse to form the two-player subcoalition.Footnote 3 Following the studies on “conference structures” in the vein of Myerson (1980), consider also two academic scholars at a conference, neither of whom has met the other or any colleague who could have brought them together. Even if they belong to the same society, these two scholars cannot collaborate. We can also provide examples in which some coalitions cannot be formed because of some institutional rules or regulations. For example, the competition laws in many countries prohibit cooperation between firms that compete in the same market. However, firms are free to cooperate with any other firm with whom they do not compete. Along a similar line, in sport competitions, a player in a team is forbidden to form a coalition with a player of the opponent team while he or she freely cooperates with his or her teammates. Another important observation on SNE and CPNE lies within the actions of the players. SNE allows a coalition to choose a joint strategy profile, or a deviation, that may be inconsistent with the rational choices of its subcoalitions. In other words, even if a coalition wishes to choose a Pareto deviation, one of its subcoalitions may not be willing to follow that deviation, which may not be Pareto optimal given the actions of the non-members. In such a case, some of the members of that subcoalition face what we call a “vertical conflict of interest” (one coalition tells you to do something, but another tells you to do something else). CPNE actually overcomes this conflict by restricting each coalition to respect the rationality of all of its subcoalitions. Be that as it may, CPNE still allows a coalition to choose a joint strategy profile that may be inconsistent with the rationality of another coalition that is neither a subcoalition nor a supercoalition, but that shares some players. In such a case, some of the common members of those coalitions face what we call a “horizontal conflict of interest” (once again, one coalition tells you to do something, but another tells you to do something else). With regard to the former observation, it seems worthwhile to consider an equilibrium refinement that does not require the formation of all possible coalitions. With regard to the latter observation, there may be a restricted set of coalitions that contains neither vertical nor horizontal conflicts of interests. In this paper, we seek to address these concerns by formulating a new equilibrium refinement. The refinement that we propose (i) does not suffer from the problems embedded in vertical and horizontal conflicts of interests; and (ii) can be a useful alternative to the notions of SNE and CPNE, and even to the notion of Nash equilibrium, in cases where some coalitions are active (i.e., formed) and the others are not. Note also that the former observation calls for a general coalitional structure that does not necessarily include some coalitions, whereas the latter observation calls for a specific framework that restricts the set of coalitional structures to be studied. More precisely, to eliminate vertical conflicts of interest, every coalition should respect the rationality of its members and subcoalitions (as in the case of CPNE). In addition to this, to avoid horizontal conflicts of interest, the coalitional structure should be formulated in such a way that for any pair of active coalitions, either the coalitions are disjoint or one coalition contains the other. This is referred to as an organizational structure. Intuitively, in a non-cooperative game, players may form coalitions if they are allowed to. We are particularly interested in situations where if a player is a member of a coalition, then he or she cannot be a member of another coalition. Accordingly, the set of all active coalitions turns out to be a partition of the player set. As coalitions may unite to form greater coalitions, in the next step, we have another partition of the player set, which is coarser than the former partition. This recursively leads to a collection of partitions such that each partition is coarser than the partitions that precede it. We call it an organization. For an example, consider a university as a set of faculty members such that each faculty member belongs to one department and each department belongs to one school. Another example is a football team with footballers and their positions (e.g., defenders, midfielders, and strikers); or a company with employees, units, departments, and divisions. In this paper, given any organizational structure, we define the notion of organizational Nash equilibrium (ONE),Footnote 4 for which we utilize strict Pareto dominance to describe the coalitions’ preferences (Sect. 2.3).Footnote 5 We then study some examples of normal form games through which we can understand how our organizational refinement works and how its predictions are different from those made by SNE/CPNE (Sect. 2.4). We analyze the existence of equilibrium for a subclass of games with strategic complementarities (Sect. 3.1). We also provide a monotonicity property indicating that as one considers “greater” organizations, the equilibrium set is more refined (Sect. 3.2). We conclude in Sect. 4.",
91.0,3.0,Theory and Decision,26 March 2021,https://link.springer.com/article/10.1007/s11238-021-09804-5,Necessary and sufficient conditions for pairwise majority decisions on path-connected domains,October 2021,Madhuparna Karmokar,Souvik Roy,Ton Storcken,Unknown,Unknown,Male,Male,"We consider standard social choice problems where a group of agents have to collectively decide an alternative from a set of feasible alternatives. A choice function selects an alternative for every collection of individual preferences. We impose desirable conditions on choice functions such as unanimity, anonymity, symmetry, and group strategy-proofness. A choice function is unanimous if, whenever all the individuals have the same preference, their common top-ranked alternative is chosen. It is called anonymous if it treats all the individuals equally. Symmetry ensures that if the role of two alternatives (at the top of preferences) is interchanged at certain type of profiles, the outcome is also interchanged accordingly. A choice function is called group strategy-proof if no group of agents can be strictly better off by misrepresenting their preferences and is called strategy-proof if no individual can be better off by misrepresenting his/her preference. A preference is called single-peaked on a tree if the alternatives can be arranged on a treeFootnote 1 so that preference declines as one moves away from the top-ranked alternative. Such preferences are well known in the literature for their usefulness in modelling public good location problems. We assume a mild structure called path-connectedness (see, Aswal et al. (2003)) on the domains we consider in this paper. Theorem 4.1 shows that a path-connected domain admits unanimous, anonymous, symmetric, and group strategy-proof choice functions if and only if it is single-peaked on a tree and the number of agents is odd. It follows as a corollary of this result that there exists no unanimous, anonymous, symmetric, and group strategy-proof choice function on a path-connected domain if the number of agents is even. When the number of agents is odd, Theorem 4.2 characterizes all unanimous, anonymous, symmetric, and group strategy-proof choice functions on single-peaked domains on trees as the tree-median rule. Finally, we investigate what happens if we replace group strategy-proofness by strategy-proofness. Theorem 5.1 says that if we strengthen the notion of path-connectedness in a suitable manner, then the conclusion of Theorem 4.1 can be achieved with strategy-proofness, that is, a strongly path-connected domain admits unanimous, anonymous, symmetric, and strategy-proof choice functions if and only if it is a single-peaked domain on a tree. An alternative is called the pairwise majority winner at a profile if it beats every other alternative according to pairwise majority comparison and a choice function is called the pairwise majority rule if it selects the pairwise majority winner at every preference profile. Condorcet (1785) argued that if such a majority winner exists at a profile, we should choose it on the basis of “straightforward reasoning.” The analysis of the pairwise majority rule dates back to Borda (1784), Condorcet (1785), and Laplace (1820). Black (1948) shows that the pairwise majority rule exists on domains that are single-peaked on a line. Later, Demange (1982) generalizes this result by showing that the pairwise majority rule exists on a domain even if the domain is single-peaked on a tree. Hansen and Thisse (1981) consider the problem of locating a public facility and show that the outcome of the pairwise majority rule on a single-peaked domain on a tree minimizes the total distance traversed by the users to go to the facility. They further prove that this property holds for a single-peaked domain only when the underlying graph is a tree. Moulin (1980) characterizes the pairwise majority rule on domains that are single-peaked on a line. Danilov (1994) shows that strategy-proof and tops-only SCFs on a single-peaked domain on a tree can be recursively decomposed into medians of constant and dictatorial rules. Schummer and Vohra (2002) consider single-peaked domains on tree when preferences are Euclidean with respect to the graph distance and show that an SCF on such a domain is strategy-proof and unanimous if and only if it is an extended generalized median voter scheme. Nehring and Puppe (2007) introduce a class of generalized single-peaked domains based on an abstract betweenness property and show that an SCF is strategy-proof on a sufficiently rich domain of generalized single-peaked preferences if and only if it takes the form of voting by issues. They also provide a characterization of such domains that admit SCFs satisfying strategy-proofness, unanimity, neutrality, and non-dictatorship/anonymity. We provide a detailed discussion on the connection of our paper with these papers in Sect. 6. May (1952) considers the problem of preference aggregation with exactly two alternatives and characterizes the pairwise majority aggregation rule in this setting by means of always decisiveness, equality, symmetry, and positive responsiveness. Later, Inada (1969) and Sen and Pattanaik (1969) provide necessary and sufficient conditions on a domain so that the pairwise majority aggregation rule is transitive. It is worth mentioning that the tree-median rule coincides with the pairwise majority rule on domains that are single-peaked on a tree.Footnote 2 Thus, the main contributions of our paper can be considered as (i) a characterization of domains that are single-peaked on trees by means of choice functions satisfying natural conditions such as unanimity, anonymity, symmetry, and group strategy-proofness/strategy-proofness and (ii) a characterization of the pairwise majority rule on these domains as the only choice function satisfying the above-mentioned properties. Thus, in addition to the existing results where single-peakedness on trees is proved to be sufficient for the existence of the pairwise majority rule, we show that under some natural conditions, it is also necessary for the same. Characterizing domains by means of the choice functions that they admit is considered as an important problem in the literature. Chatterji et al. (2016) characterize single-peaked domains on arbitrary trees by means of strategy-proof, unanimous, tops-only random social choice functions satisfying a compromise property, and Puppe (2018) shows that every minimally rich and connected Condorcet domain which contains at least one pair of completely reversed orders must be single-peaked. The rest of the paper is organized as follows. Section 2 presents the notion of single-peaked domains on trees, and Sect. 3 introduces the notion of the tree-median rule. Main results of the paper are presented in Sect. 4. Section 5 shows how group strategy-proofness can be replaced with strategy-proofness in our main result. All the proofs, as well as the independence of the axioms used in our main result, are collected in Appendix.",
91.0,3.0,Theory and Decision,31 March 2021,https://link.springer.com/article/10.1007/s11238-021-09801-8,"Rationality, preference satisfaction and anomalous intentions: why rational choice theory is not self-defeating",October 2021,Roberto Fumagalli,,,Male,Unknown,Unknown,Male,"The critics of rational choice theory (henceforth, RCT) frequently claim that RCT is self-defeating in the sense that agents who abide by RCT’s prescriptions are less successful in satisfying their preferences than they would be if they abided by some normative theory of choice other than RCT (e.g. Bratman 1999, 2000; Gauthier 1984, 1997; Kavka 1978, 1983; McClennen 1990,  1997). The idea is that abiding by RCT’s prescriptions hampers (rather than enhances) agents’ ability to satisfy their preferences, and that an agent who abides by RCT’s prescriptions will often “end up satisfying his preferences less well than he would have done, had he [abided by] some other [theory]” (Sugden 1991, 752; also Bradley 2007 and 2017; Broome 2007a and 2007b; Dietrich et al. 2013 and 2019; Rabinowicz 1995 and 2019; Spohn 2009 and 2012, for recent discussions). In this paper, I combine insights from philosophy of action, philosophy of mind and the normative foundations of RCT to rebut this often-made criticism. I then explicate the implications of my thesis for the wider philosophical debate concerning the normativity of RCT for both ideal agents who can form and revise their intentions instantly without cognitive costs and real-life agents who have limited control over the formation and the dynamics of their own intentions.Footnote 1 The paper is organized as follows. In Sect. 2, I examine one issue that figures centrally in the debate as to whether RCT is self-defeating, namely whether an agent who abides by RCT’s prescriptions can rationally form what I call anomalous intentions, i.e. intentions to perform actions that maximize the total stream of payoffs the agent can get over the entire course of a decision problem, but fail to maximize the payoffs the agent can get from some subsequent choice nodes onwards (e.g. Bratman 1999, 2000; Gauthier 1984, 1997; Kavka 1978, 1983; McClennen 1990, 1997). I then explicate my thesis that, despite prominent criticisms of RCT, agents can rationally form anomalous intentions, and therefore prominent attempts to demonstrate that RCT is self-defeating do not withstand scrutiny. In Sects. 3–6, I defend my thesis that agents can rationally form anomalous intentions against four major objections put forward in the specialized literature, namely: the objection from temporal situatedness (e.g. Bratman 1998; Mintoff 1997); the objection from bootstrapping (e.g. Bratman 2009; Broome 2013); the objection from psycho-physical inability (e.g. Farrell 1989; Shah 2009); and the overdemandingness objection (e.g. Mongin 2000; Steele 2006).Footnote 2 My thesis that agents can rationally form anomalous intentions has at least three implications of general interest for the wider philosophical debate concerning the normativity of RCT (e.g. Bradley 2007, 2017; Broome 2007a, b; Dietrich et al. 2013, 2019; Rabinowicz 1995, 2019; Spohn 2009, 2012). First, anomalous intentions figure in a vast range of decision problems where the payoffs agents can get if they form the intention to perform an action are at least partly independent of the payoffs agents can get if they actually perform such action (e.g. Andreou 2006; Clarke 2007; van Hees and Roy 2009). Hence, showing that agents can rationally form anomalous intentions would have significant bearing on the normativity of RCT across several decision problems. Second, the claim that agents cannot rationally form anomalous intentions is commonly premised on the assumption that instrumental rationality requires agents to maximize the payoffs they can get from the choice nodes they face onwards irrespective of whether maximizing these payoffs maximizes the total stream of payoffs agents can get over the entire course of a decision problem (e.g. Bratman 1998, 62–66, Mintoff 1997, 624–5, Williams 1981, 35). Below I critically examine this widely endorsed conception of instrumental rationality and argue that in presence of anomalous intentions such conception has implications that contrast with independently plausible requirements of payoff maximization. And third, several authors build on the claim that RCT is self-defeating to argue that this theory must be revised or even rejected (e.g. Bratman 1999, 2000; Gauthier 1984, 1997; Kavka 1978, 1983; McClennen 1990, 1997). If my thesis that agents can rationally form anomalous intentions is correct, prominent attempts to demonstrate that RCT is self-defeating do not withstand scrutiny. This result does not per se vindicate RCT as our best available normative theory of choice. Still, it challenges RCT’s critics to put forward more convincing reasons and evidence to support their claim that RCT is self-defeating.Footnote 3 Before proceeding, one preliminary remark is in order. Various characterizations of preferences and intentions have been advocated in the economic and philosophical literatures (e.g. Cozic and Hill 2015; Dietrich and List 2016; Guala 2019; Hausman 2011; Jeffrey 1965; Savage 1954; Thoma 2017, on preferences; Anscombe 1963; Bratman 1987; Davidson 1978; Holton 2009; Roy 2009; Searle 1983; Tenenbaum 2018, on intentions). I shall expand on these characterizations wherever my evaluation directly rests on those characterizations (e.g. footnote no.7 on the so-called belief constraint on intending; also footnote no.17 for a comparison between intentions and beliefs). For now, it suffices to note that although intentions do not figure in all applications of RCT, many applications of RCT model decision-making in terms of the formation and the dynamics of intentions (e.g. Audi 1991; Bales 2020; Cullity 2008; Mele 2000; Pink 1991), and the debate concerning the putative self-defeating character of RCT often targets such applications (Sects. 2–6).",
91.0,3.0,Theory and Decision,04 April 2021,https://link.springer.com/article/10.1007/s11238-021-09809-0,"Three doors anomaly, “should I stay, or should I go”: an artefactual field experiment",October 2021,Andrea Morone,Rocco Caferra,Paola Tiranzoni,Female,Male,Female,Mix,,
91.0,3.0,Theory and Decision,27 April 2021,https://link.springer.com/article/10.1007/s11238-021-09805-4,Seeking consistency with paired comparisons: a systems approach,October 2021,Donald G. Saari,,,Male,Unknown,Unknown,Male,"For reasons that include cost and convenience, paired comparisons are widely used to make decisions even though examples exist that cast doubt on the trustworthiness of certain approaches. As these techniques continue to be used, a useful goal (developed here) is to determine how they can be modified to yield more reliable outcomes. The basic idea mimics the least-squares methodology by projecting information (e.g., data, results about pairs, etc.) into a space where consistent outcomes are assured. Unfortunately, an appropriate “consistency space” is not known even for the widely required condition of transitivity. This motivates finding a natural “transitivity consistency” space. Even after identifying a desired space of outcomes, the appropriate projection need not be obvious. This tends to be true with nonlinear structures. The expectation is that where, if the summation is an orthogonal vector addition, standard projections apply. But if Eq. 1 fails (which, as shown below, is true with AHP, the Analytic Hierarchy Process), rather than helping, projections can aggravate the analysis by introducing new types of mistakes. To avoid these problems, the structure of the error term must be found. Some techniques use modified forms of paired comparisons. An example is the Pugh matrix method (Pugh, 1991) where a particular alternative (often the status quo) serves as the base from which other alternatives are compared over several criteria. A standard approach is, for each criterion, to assign a \(``-, 0, +""\) score to an alternative where The final score is the number of \(+\)’s minus the number of −’s. Refined options include “double −’s and double \(+\)’s,” or perhaps 1, 2, 3, 4, 5, where 3 is equivalent to the base comparison. It is shown how to identify and overcome weaknesses of these approaches. . A search to improve decision outcomes is desirable, but is it futile? The pessimism derives from Arrow’s Theorem (Arrow, 1963), which often is described as asserting that no decision approach is fair with three or more alternatives. Fortunately for this project, it is shown that this negative commentary is overstated. Addressing these difficulties is part of a general project to understand systems whether from the social sciences, engineering, or biology. Toward this end, paired comparisons are treated as “subsystems;” i.e., the decision methods combine information from the subsystems to create an answer for the whole system. An advantage of first studying decision methods for this project is that their structures are specified, so they form a more tractable test bed to discover why system problems arise and how to correct them. Taking this point of view, Arrow’s Theorem’s negative conclusion becomes the standard “the whole can differ from the sum of its parts” concern (Sect. 2). The goal is to understand what causes conflicts between a system and its subsystems and how to avoid them. A first step (Sect. 3) is to analyze the structure of the subsystems, which here is the space of paired comparisons. Guided by Eq. 1 and emphasizing summation methods, this space is orthogonally divided into two components—informally, call them the “desired” and “error” subspaces. Nothing goes wrong when any standard decision method uses information from the desired component. Consequently, all difficulties are caused by decision methods using information about pairs, or portion of pairs, from the error space. Accompanying the discussion are easily used computational tools. An obvious message is to avoid subsystem outcomes (i.e., collections of paired outcomes) that rely upon the error space: this is how (in Sect. 4) the approaches described above are modified. A second class of decision methods (such as AHP) uses multiplicative procedures. In Sect. 5, results about these systems are derived by transferring material from Sect. 3. Most proofs are in Sect. 7.",2
91.0,3.0,Theory and Decision,09 April 2021,https://link.springer.com/article/10.1007/s11238-021-09811-6,A simple non-parametric method for eliciting prospect theory's value function and measuring loss aversion under risk and ambiguity,October 2021,Pavlo Blavatskyy,,,Male,Unknown,Unknown,Male,"First proposed by Bernoulli (1738/1954) to solve the St. Petersburg paradox, expected utility theory became one of the most well-known decision theories. The descriptive validity of expected utility theory was challenged inter alia by the Allais (1953) paradox and the common ratio effect (Kahneman & Tversky, 1979). Several generalizations of expected utility theory, known as non-expected utility theories, were proposed in the literature (cf. Starmer, 2000). Among these generalizations, prospect theory (Kahneman & Tversky, 1979; Tversky & Kahneman, 1992) raised to prominence for its ability to accommodate a large set of behavioral regularities falsifying expected utility theory. Prospect theory is closely related to rank-dependent utility proposed by Quiggin (1981) and to Yaari’s (1987) dual model. By prospect theory, this paper refers to Tversky and Kahneman (1992) version, also known as the cumulative prospect theory. Kahneman and Tversky (1979) version, also known as the original prospect theory, is its special case (when probability weighting is the same for gains and losses) if prospects (acts, lotteries) have (at most) one gain and (at most) one loss, as it is the case in this paper. Tversky and Kahneman (1992) version is also more general in the sense that it applies to ambiguity (Knightian uncertainty) whereas Kahneman and Tversky (1979) version only applies to risk (when events are characterized by objective probabilities). In prospect theory, the desirability of various outcomes is captured by its value function. The latter is traditionally normalized to zero at a reference point where it is assumed to have a kink reflecting loss aversion. Prospect theory’s value function is a ratio scale—it can be normalized for one outcome other than the reference point. Methods for eliciting prospect theory’s value function are invaluable tools in decision analysis but it has long been a challenging task so that empirical studies often make simplifying assumptions (e.g., Pennings & Smidts, 2003). A major breakthrough is a non-parametric tradeoff method of Wakker and Deneffe (1996) for eliciting prospect theory’s value function either for gains or for losses under ambiguity. Abdellaoui (2000), Bleichrodt and Pinto (2000) and van de Kuilen and Wakker (2011) extend this method to measuring probability weighting in choice under risk and Abdellaoui et al. (2005)—under ambiguity. Abdellaoui et al. (2007) propose the first non-parametric method for eliciting prospect theory’s value function both for gains and losses in choice under risk. Their method is a four-step procedure. First, the method of Abdellaoui (2000) is used for eliciting two probabilities with decision weights one half (in gain and loss domains). Second, prospect theory’s value function for losses is elicited by the midpoint method. Third, one gain is elicited that has the same utility (in absolute value) as one of the losses with a known utility (elicited in the previous step). Fourth, the value function for gains is elicited by the midpoint method. Abdellaoui et al. (2016) propose the first non-parametric method for eliciting prospect theory’s value function both for gains and losses in choice under ambiguity. Their method is a three-step procedure. First, similar to the third step in the elicitation method of Abdellaoui et al. (2007), a decision analyst elicits one gain and one loss that have the same utility in absolute value. Second, the value function for gains is elicited by the tradeoff method of Wakker and Deneffe (1996). Third, the value function for losses is elicited by the same method. This paper presents a simpler method for eliciting prospect theory’s value function under ambiguity. We simplify elicitation questions, which has practical advantages for a decision analyst and it might reduce the propagation of errors (cf. Blavatskyy, 2006). The rest of the paper is organized as follows. Section 2 presents our mathematical notation. Section 3 presents our elicitation method. Section 4 describes an experimental design for this elicitation method. Section 5 summarizes how this experiment was implemented. Section 6 presents experimental results. Section 7 concludes.",4
91.0,4.0,Theory and Decision,29 April 2021,https://link.springer.com/article/10.1007/s11238-021-09806-3,Just society,November 2021,Rakesh K. Sarin,,,Male,Unknown,Unknown,Male,"Imagine that you do not know your place in society, your income and wealth or your social status. You are to choose a design of society (principles of justice and basic structure) behind a veil of ignorance. As a rational person you are asked to engage in reflective soul searching in the original position (Rawls, 1971, revised edition 1999). The principles of justice determine how the benefits and burdens are shared between members of a society and thus influence the economic and social systems. Your choice in the original position is impartial as you are not able to design principles of justice to favor your particular situation. Thus, the principles of justice you choose are fair and the society regulated by these principles is a just society. Consider for simplicity two states of the world. In one state you are at a more advantaged situation (MAS) in terms of income and wealth but more generally on an index of basic goods (rights and opportunities, income and wealth, security, health, education, and culture). In the other state you are at a less advantaged situation (LAS). You therefore face a choice under uncertainty. Clearly your utility (preference or value) is higher under MAS and lower under LAS. You wish to maximize your expected utility (Savage, 1954; von Neumann & Morgenstern, 1947). As a rational person you are aware that even though you are choosing a structure of society behind the veil of ignorance, you will live within the rules of the society that you chose. You must affirm the principles of justice you agreed upon regardless of your eventual position in the society (MAS or LAS). Though you are in the dark about your eventual position in the society, you do have a knowledge of the general laws of human nature. You know that people have different plans for their lives; some wish to be teachers and others entrepreneurs; they practice different religions or spiritual practices; they respond to incentives and weigh effort and leisure uniquely. Thus, the principles of justice chosen in the original position must provide sufficient flexibility for individual aspirations (Mill, 1859). A paramount objective in the design of a society is that no matter where you eventually end up, you wish to lead a life of dignity with self-respect and wish to be treated as a free and equal person. I will examine three alternatives for the foundations of a theory of justice. These are: (1) maximize total wealth of the society (free market with minimum government intervention), (2) secure equal liberty and opportunity for all then use the maximin principle for distribution of income and wealth and other basic goods, (3) secure equal liberty and opportunity for all then use expected utility for distribution of income and wealth and other basic goods. Each of the three alternatives require equal basic liberties (freedom of speech and assembly; liberty of conscience and freedom of thought; freedom from arbitrary arrest and seizure defined by the rule of law; freedom of the person and the right to hold property; political liberty (the right to vote and equal eligibility for public office); and equal access for positions in private and public sectors). Thus, the three alternatives differ chiefly in the distribution of income and wealth. It is worth noting that the principles of justice apply to the basic structure of society and not to the actual specific allocative decisions of society. Alternative 1, laissez-faire, has its roots in Adam Smith’s book The Wealth of Nations, 1776. It has been vigorously defended by Nozick (1974) and Friedman (1980). Alternative 2 has been proposed by Rawls (1971) and is regarded as the most profound investigation of justice as fairness in modern times. Alternative 3 may be viewed as a modification of alternative 2 by replacing maximin with expected utility as a principle of distributive justice. In Sect. 2, I will provide a brief review of the literature relevant to this paper. In Sect. 3, I define the decision problem from the perspective of a rational individual in the original position. To be consistent, assumption of self-interest is made throughout; though, altruism and other-regarding behavior often influences human choice. Maximization of expected utility of wealth is taken to be the objective function. Arrow (1971) defines utility over wealth in his examination of the measure of risk aversion. This convention is widely used in economics and finance literature. Since Bernoulli, 1738, there has been a tradition to define utility over wealth. Since in the original position, the problem is formulated at a high level of abstraction, income per year could also be used as the outcome. Subsequent to the formulation of principles of justice, in actual policies either income or wealth or both may be relevant based on the details and intended goals of the policy. The key assumption is that it is possible to assign expectations of wealth (or more generally primary goods) to representative individuals holding various social positions (unskilled labor or managerial class, for example). These expectations are likely to be very far in the laissez-faire system and very close in the maximin system. Since there is a continuity of systems, I have provided a coherent approach on how these expectations of wealth move in Sect. 4. Two assumptions—utility is concave and redistribution reduces total wealth provide a clarification of alternative principles of distribution. The main results are presented in Sect. 5. Rawls’s criticism that probabilities cannot be assigned in the original position is addressed in Sect. 6. An individual seeks more than just income and wealth for her overall well-being and happy life. In Sect. 7, I provide a preliminary investigation when utility is impacted by multiple attributes or primary goods. A dedicated scout could generalize the results in this paper considerably by relaxing that even liberty has tradeoffs. Having laid out the implications of the expected utility rule on distribution policies, a detailed comparison with the maximin rule is provided in Sect. 8. In Sect. 9, I respond to criticism of the social minimum policy. Much of these criticisms are for a welfare state where the social minimum is ex-post for victims of ill-luck and the aim is to ensure basic necessities of life for everyone. In my formulation the social minimum is a consequence of a willful ex-ante choice of a rational individual who would ensure for herself provision of education, training, and ownership of productive assets needed for a flourishing life. Finally, in Sect. 10, I provide a summary.",2
91.0,4.0,Theory and Decision,15 May 2021,https://link.springer.com/article/10.1007/s11238-021-09818-z,Why do consumers not switch? An experimental investigation of a search and switch model,November 2021,Irene Maria Buso,John Hey,,Female,Male,Unknown,Mix,,
91.0,4.0,Theory and Decision,27 April 2021,https://link.springer.com/article/10.1007/s11238-021-09814-3,On anonymous and weighted voting systems,November 2021,Josep Freixas,Montserrat Pons,,Male,Female,Unknown,Mix,,
91.0,4.0,Theory and Decision,07 May 2021,https://link.springer.com/article/10.1007/s11238-021-09819-y,Pricing algorithms in oligopoly with decreasing returns,November 2021,Jacques Thépot,,,Male,Unknown,Unknown,Male,"In the digital economy, consumers are better informed on products and prices. Shopping can be done everywhere on smartphones. Information transparency and consumer reactivity stimulate fair competition. But on the supply side, Big Data, search engines, social networks and tutti quanti offer tremendous opportunities to erect monopoly rents and accumulate subnormal profits. Firms are, by nature, better organized than the consumers to exploit or process overwhelming flows of data. The dubious effect on welfare deeply questions competition policies in the digital economy. In the line of a growing literature on algorithmic pricing (e.g. Brown & MacKay, 2021; for a survey, see Calvano et al., 2019), the book by Ezrachi & Stucke, (2016) exemplifies the role of algorithms in sustaining specific forms of tacit collusion through supra-competitive prices. Pricing algorithms are computerized procedures that a seller may use to adapt instantaneously its price to market conditions, particularly to prices quoted by its rivals. Ezrachi and Stucke identify three collusion scenarios according to the level of AI involved in the pricing bots that operate as substitute for human day-to-day decision-making. This tends to dampen the manager’s antitrust liability since no intent of explicit collusion is involved. Under the third scenario, called “tacit collusion on steroids”, each firm unilaterally adopts its pricing algorithm, which sets its own price. Pricing algorithms act as predictable agents and continually monitor and adjust to each other’s prices and market data. This scenario is motivated, in the real world, by the Martha’s Vineyard case (White V. R.M. Packer Co., Inc, 2011)Footnote 1 (see the recension by The Economist, 2017). Martha’s Vineyard is an island off the coast of Massachusetts, which is deemed to be a nice place for vacationers. Only 9 gas stations operate on the island. Each station prominently posts its prices so that all the consumers are perfectly informed; furthermore, any seller can instantaneously match any deviation of the rivals. The demand from the residents is highly inelastic because alternative sources of gasoline are located on the mainland. The residents constitute a captive segment of the market. In addition, entry of new gasoline sellers is restricted by specific rules and procedures. As a result, gas prices on the island exceed prices on the mainland by an average of 56 cents per gallon. Some residents filed a complaint for collusion among the stations’ owners. The plaintiffs’ antitrust claims failed. The trial and appellate court found no evidence of explicit collusion. The gasoline market in Martha’s Vineyard is oligopolistic and the price fixing mechanism at work is the pure result of a rational, unilateral decision by each competitor, the court said. In the last decades, pricing algorithms have been increasingly used in the digital economy, from services to online retailing. For example: Since the 2000s, search aggregators such as Kayac, Expedia and Orbitz operate as pricing support tools for airline companies; these aggregators run as pricing algorithms since they mechanically make adjustments to the best offer. In 2001, the four major US airlines decided to team up and create Orbitz which went public in 2003. This has awakened the scrutiny of regulatory authorities (Akca & Rao, 2020): on a similar basis to the Martha’s vineyard case, the Department of Justice ruled in 2003 that Orbitz was not a cartel and that there was no evidence of price fixingFootnote 2 although this have led to increases in markups in the airline industry. Brown and MacKay (2021) present an empirical analysis of hourly prices of allergy drugs delivered by five online retailers in the United States, in a period of one and a half years. The pricing strategies of these retailers are governed by algorithms which operate in real time. These authors show how the prices of the drugs were consequently boosted in that period. Third-party software developers of pricing algorithms are now spreading on the net. ChannelAdvisor boasts its software as “constantly monitoring top competitors online.” Repricer.com claims to ”react to changes your competitors make in 90 seconds.” Intelligence Node gives the retailers the opportunity to “have eyes on competitor movements at all times and...automatically update their prices.” Many other software solutions can be found online and advertised as ”dynamic pricing”. Thus, in various oligopoly situations, prices are likely to be supra-competitive when pricing algorithms are used; explicit collusion is not necessarily involved, as ruled by the antitrust authorities in recent yearsFootnote 3. In Martha’s Vineyard case, the court of Appeals based its decision on the analysis of downward (undercutting prices) and upward price-matching policies (raising prices): if one station drops its price to attract more business, the others can quickly drop their prices in response. Conversely, a station acting as a price “leader” risks little by raising its price; if the competitors do not follow the increase, the leader can quickly drop its price again to match his rivals. This argumentation holds more significantly again in the digital economy, with some specificities:  Digital economy lacks from insularity; demand in most markets is elastic. Downward matching policy leads to a cut of the market price, which attracts new consumers. Upward matching policy leads to an increase which repels current consumers. The equilibrium price level strongly depends on these marginal consumers. Following a leading firm which raises its price never constitutes a rational and unilateral decision. In classic industries, the “do nothing” option remains the best one any seller can choose in response to rivals’ raising prices. Online driving forces fueled by price web aggregators are more oriented towards price wars than price escalations. It remains, however, that the digital economy opens avenues for pricing algorithms computerizing upward matching policies, especially if these algorithms are opaque to antitrust enforcers. Diseconomies of scale play a key role in some sectors of the digital economy (e.g. in travel industry with online booking) and may interact with the matching policies. As we know from the oligopoly literature, the cost structure of the firms strongly matters. According to the well-known Bertrand–Edgeworth paradox (cf. Tirole, 1988, p. 214), when the firms face strictly decreasing returns to scale, there is no deterministic oligopoly equilibriumFootnote 4; the perfect competitive price is not a pure strategy price equilibrium as the rivals do not want to attract any consumer flying from a price raising firm. This phenomenon may limit the effectiveness of upward matching policies to sustain supra-competitive prices. In this paper, we want to investigate the theoretical foundations of these issues. Our objective is to analyze the equilibrium conditions when pricing algorithms operate in an oligopoly with decreasing returns. We will prove that the use of algorithms leads to a multiplicity of equilibria with supra-competitive prices. The literature on price-matching policies mainly deals with analyzing the price-matching guarantee, namely the commitment made by the seller to sell its product at the lowest price on the market upon proof. Price-matching guarantee is considered an anti-competitive practice as it actually dampens the impact of price undercutting. Salop (1986) shows that the market price ranges from the monopolistic to the Bertrand price when the firms have options to price-match. Monopolistic price may emerge as the dominant strategy (Doyle, 1988). Batsaikhanz and Tumennasan (2018) study the effects of price-matching in a setting in which each firm selects both its price and output simultaneously. They show that the availability of a price-matching option leads to the Cournot outcome (see also Tumennasan, 2013). These contributions consider price-matching guarantees designed to match any vector of prices listed by the rivals (and eventually outputs) in the industry. In a game theoretic setting, this defines specific pricing strategies and equilibrium conditions. In this literature, the price-matching guarantee is only available to the consumers who are ready to buy the product before the price rebate. Pricing algorithms under consideration in this paper make the rebate to apply to all the potential consumers. This affects the equilibrium conditions especially in the decreasing returns to scale context in which the analysis is made. This paper considers a price setting oligopoly model with decreasing returns to scale (cf. Thépot, 1995 for a Bertrand competition analysis). We formalize the pricing algorithm competition in a static homogeneous product oligopoly as follows: we start from any current state of the oligopoly defined by (i) the clearing market price; (ii) the outputs of the firms. It is an equilibrium if no firm is better off when moving its price (downward and eventually upward) when the rivals systematically use algorithms to match this move. Equilibrium conditions result from a fixed point reasoning similar to that used in general equilibrium theory. The equilibrium price and outputs depend on the firms’ customers response to market price variation. The original feature of this paper is to distinguish upward and downward matching. Two cases are considered.  The case where bidirectional matching (upward and downward) is implemented by the algorithms; we get a family of equilibria; these equilibria define a set of oligopoly outcomes including Cournot and Pareto efficient solutions. Dynamical stability in the linear demand constant return case is studied. In the line of the work by Theocharis (1960), we prove that Pareto-efficient equilibria are dynamically stable. The monodirectional matching case where only downward matching is allowed. The family of equilibria extends to a set of prices lower than Cournot but higher than a bottom value (which dominates the competitive price). This bottom price depends on the cost structure of the firms. It is the socially best price achievable through pricing algorithms. It deserves consideration from the antitrust authorities as it deals with a substantial welfare improvement with respect to alternative solutions (Cournot, among others). The existence of pure equilibria in a context of decreasing returns to scale is worth noting: pricing algorithms may solve the Bertrand–Edgeworth paradox; accordingly, they contribute to the market stability. The remaining of the paper is divided into three parts: the statement of the model is presented in Sect. 2. Sections 3 and 4 are, respectively, devoted to the bidirectional and the monodirectional matching cases; equilibrium analysis are developed and analytical results are illustrated in the linear-quadratic case. Comments are given in Sect. 5.",1
91.0,4.0,Theory and Decision,12 May 2021,https://link.springer.com/article/10.1007/s11238-021-09810-7,‘NEXT’ events: a cooperative game theoretic view to festivals,November 2021,Luc Champarnaud,Amandine Ghintran,Frédéric Jouneau-Sion,Male,Female,Male,Mix,,
91.0,4.0,Theory and Decision,18 May 2021,https://link.springer.com/article/10.1007/s11238-021-09822-3,Collusive stability of cross-holding with cost asymmetry,November 2021,Jianxia Yang,Chenhang Zeng,,Unknown,Unknown,Unknown,Unknown,,
92.0,1.0,Theory and Decision,27 November 2021,https://link.springer.com/article/10.1007/s11238-021-09854-9,Introduction to the special issue on “Poverty and Economic Decision-Making”,February 2022,Ferdinand M. Vieider,Erik Wengström,,Male,Male,Unknown,Male,,
92.0,1.0,Theory and Decision,09 March 2021,https://link.springer.com/article/10.1007/s11238-021-09802-7,Poverty and economic decision making: a review of scarcity theory,February 2022,Ernst-Jan de Bruijn,Gerrit Antonides,,Unknown,,Unknown,Mix,,
92.0,1.0,Theory and Decision,20 July 2021,https://link.springer.com/article/10.1007/s11238-021-09836-x,The duality of poverty: a replication of Mani et al. (2013) in Colombia,February 2022,Felipe González-Arango,Javier Corredor,Jhonathan Jared González,Male,,Unknown,Mix,,
92.0,1.0,Theory and Decision,13 April 2021,https://link.springer.com/article/10.1007/s11238-021-09813-4,Groups discipline resource use under scarcity,February 2022,Florian Diekert,Kjell Arne Brekke,,Male,Male,Unknown,Male,"Scarcity means that available resources are insufficient to meet the needs. During a drought, farmers have to decide which plots to water and which plots to give up. In times of financial stress, firms and households have to decide which bills to pay or which creditors to serve first, and under time pressure only some tasks can be completed while others have to be dropped. Mullainathan and Shafir (2013) argue that scarcity changes the way humans think and act: It puts us in a specific state of mind where we tend to focus narrowly at the most urgent needs and neglect other, equally important but less pressing, needs. They call this state of mind “tunneling”. In this paper, we ask whether groups “tunnel” to the same extent as individuals. Tunneling affects the allocation of resources between more pressing and less pressing projects,Footnote 1 highlighting the double-edged nature of scarcity: On the one hand, scarcity increases focus and available resources are used more effectively (Shah et al. 2012; Gabaix 2019). Most readers can probably relate to that a pressing deadline can lead to significant improvements in short-term productivity (Kurtz 2008). Rational inattention theories predict that decision quality improves with scarcity (Sims 2003). Indeed, using data from a randomized controlled trial among small-scale farmers in Zambia, Fehr et al. (2019) show, for example, that the endowment effect decreases under scarcity. On the other other hand, scarcity-induced focus comes at a cost. Tunneling means that decision makers lose oversight and neglect important, but less pressing, long-term projects. For example, Laajaj (2017) shows that an exogenous relief of financial scarcity leads to longer planning horizons among poor (but not among wealthy) maize farmers in Mozambique. Neglect of issues that are not in the immediate focus may also be a result of finite mental resources. Building on this idea, Spears (2011) points out how depleted willpower is associated with economic decisions for the poor, but not for the rich. Similarly, Mani et al. (2013) show that financial scarcity negatively affects cognitive performance, both in the lab and in the field. In fact, already the uncertainty about a future shortage of rainfall can lead to significant behavioral biases (Lichand and Mani 2016). In sum, tunneling may be a mechanism that allows us to push through dire straits, but over time, it may lead into a psychological poverty trap (Haushofer and Fehr 2014). So far, the literature has analyzed individual behavior. Here, we study how scarcity affects decisions in groups. Do groups neglect long-term projects under scarcity to the same extent as individuals do? Groups and teams are a basic unit of human organization, and in many developing countries natural resources that are prone to fluctuations and periods of acute scarcity, such as groundwater, grazing lands, or fisheries, are owned or used collectively. Do groups also borrow in order to meet the needs of the present, and thereby compromise the ability to meet their needs in the future? Our study is based on the design of Shah et al. (2012), where participants solve a series of tasks within a given time budget, which is either abundant or scarce. To solve the current task they can borrow time from future tasks. This is exactly the tradeoff between the here and now task versus equally important future tasks. Even if such borrowing is harmful for overall performance, participants do borrow when time is scarce but not when it is abundant. As a consequence, participants’ overall performance is worse under scarcity than under abundance. A number of previous experiments use exogenous time pressure to study decisions.Footnote 2 Importantly, several studies that analyze whether cooperating in social dilemmas is fast and intuitive, or slow and deliberate, use tight time budgets to induce fast decisions (Rand et al. 2012; Rand 2016; Tinghög et al. 2013; Bouwmeester et al. 2017; Brozyna et al. 2018). Our study differs from this literature because we are not asking whether decisions that are taken under a tight time constraint are different from decisions when time is abundant, but we study the effect of time scarcity on time management as such. Specifically, our focus is on the effect of “tunneling”. Does scarcity increase focus on the immediate task while neglecting long-term consequences also for groups? On the one hand, research has shown that groups are better at self-control than individuals (Kugler et al. 2012; Charness and Sutter 2012). Excessive borrowing may be seen as an issue of self-control. If the mechanism is that scarcity affects the ability to self-control (Spears 2011), we would expect less borrowing in the group setting than in an individual setting. On the other hand, groups must work together to be successful. If scarcity increases selfishness (Prediger et al. 2014; Roux et al. 2015), we would expect to see more borrowing and lower performance in the group than in an individual setting. Note that the groups in our setting are rather minimal. They do not work together to solve the sequence of tasks. They are only a group in the sense that they share a joint time budget and are paid by their joint performance. A relevant natural setting with such a structure is a paper that is prepared by two coauthors for a tight conference deadline. Sequential work dictates that the time spent by the first author cannot be used by the second author, and the quality of the paper may diminish when the first author takes too long such that the second author has only very little time left to work on the manuscript. On the other hand we can also feel more obligation towards a paper with a coauthor, which can make us better at meeting the tight deadline when we have a coauthor. Sequential work characterizes many production processes and time is generally a valuable resource. Parallel to the poor borrowing money that costs interest, the busy borrow time by taking extensions. And similarly to banking money, finishing a task earlier allows spending more time on other tasks. Shah et al. (2012) use these properties of sequential production under a binding time constraint to experimentally investigate whether scarcity leads participants to overborrow. Here, we replicate the findings of Shah et al. (2012).Footnote 3 Further, we recover the average production function in the experimental task. Doing so allows us to pinpoint how tunneling yields a short-term gain in increased productivity in the first seconds at the task, while leading to an overall drop in performance due to harmful borrowing. Our two main results are first that we document that scarcity affects groups in a similar way as individuals and secondly that we show that the negative effect of scarcity is weaker for groups than for individuals. Our study is pre-registered, and we discuss the pre-registered hypotheses below. While we did know that self-control in groups could have an impact, we did not have specific hypotheses. Thus, parts of the results surprised us; the effect of groups on self-control was stronger than we initially thought. To ensure that this result was not spurious, we ran a follow-up study, with a slightly altered design and with the effect in question pre-registered. The results in the follow-up study were corrupted by an unfortunate comment on the MTurk message board, and the conclusion remained unclear. We thus collected a new sample outside of MTurk, which gives some support to our initial results. As the follow-up studies are motivated by the first study, we present each study and the main results consecutively.Footnote 4",1
92.0,1.0,Theory and Decision,27 May 2021,https://link.springer.com/article/10.1007/s11238-021-09815-2,Scarcity and consumers’ credit choices,February 2022,Marieke Bos,Chloé Le Coq,Peter van Santen,Female,Female,Male,Mix,,
92.0,1.0,Theory and Decision,04 June 2021,https://link.springer.com/article/10.1007/s11238-021-09820-5,Blinded by worries: sin taxes and demand for temptation under financial worries,February 2022,Sergiu Burlacu,Austėja Kažemekaitytė,Lucia Savadori,Male,Unknown,Female,Mix,,
92.0,1.0,Theory and Decision,09 April 2021,https://link.springer.com/article/10.1007/s11238-021-09803-6,A tale of two cities: an experiment on inequality and preferences,February 2022,Maria Bigoni,Stefania Bortolotti,Veronica Rattini,Female,Female,Female,Female,"Inequality has been increasing steadily in industrialized societies over the last decades, and this has been recognized as one of the main societal challenges (OECD 2011). Cities have grown divided, to the extent that household income inequality measured at the city level may be even larger than inequality at the country level.Footnote 1 Thus, affluent and extremely poor districts often coexist next to one another, divided only by an invisible line. Here, we study how being socialized in a more or less wealthy area of the same metropolitan city correlates with residents’ preferences in strategic and non-strategic situations. This is informed by the recent empirical literature on neighborhoods effects, showing that childhood exposure to different environments has a long-term effect on earnings, college attendance, fertility, and marriage patterns (Chetty et al. 2016). In an online study, we exploit the existing differences in socio-economic status (SES) between areas of an Italian city. In particular, we recruit participants who reside in different areas of the same city and we ask them to participate in a Trust Game and a Dictator Game, and we elicit their time and risk preferences by means of standard, incentive compatible procedures. Our main goal is to test if the preferences of participants from high and low SES areas display systematically different patterns. Furthermore, we want to test whether participants discriminate by conditioning their behavior on the socio-economic background of their counterpart. A link between socio-economic status (of the participant or of the family) and preferences has been documented in a few recent studies. Using the General Social Survey (GSS) from the US, Alesina and La Ferrara (2002) show that being economically unsuccessful in terms of income is associated with low level of trust. In a survey conducted in Sweden, respondents in the bottom half of the income distribution report a significantly lower level of generalized trust with respect to those at the top (Gustavsson and Jordahl 2008). This result is also confirmed in Butler et al. (2016) who look at the European context more broadly, using both the European Social Survey and the SOM Survey, finding that people with low levels of trust have significantly lower income than those with intermediate levels of trust. Finally, Ananyev and Guriev (2019) look at the case of Russia and using the survey data from the Public Opinion Foundation (Fond Obschestvennogo Mneniya, or FOM), they show that a 10% decrease in income is associated with a five-percentage point decrease in social trust. The correlation between wealth and generosity, instead, is still debated (Piff et al. 2010; Andreoni et al. 2021). A small but growing literature has documented that family socio-economic characteristics correlate with children’s risk attitudes, impatience, self-control, and social preferences (Castillo et al. 2011; Delaney and Doyle 2012; Bauer et al. 2014; Kosse et al. 2020). Differences along these dimensions may have important economic consequences, since these traits have been shown to have a long-term effect on a wide variety of life outcomes. Longitudinal studies for example show that children’s ability to postpone a gratification is a good predictor of important life-long outcomes, ranging from educational attainments (Mischel et al. 1989), to labor outcomes and lifetime income (Golsteyn et al. 2014), to health conditions, substance dependence, personal finances, up to criminal behavior (Moffitt et al. 2011). Sutter et al. (2013) confirm that an association exists between children’s and adolescents’ impatience and their consumption of alcohol and cigarettes, their body mass index, their saving behavior, and their school conduct. Prosociality also correlates with labor market success, in a large-scale cross-country study (Kosse and Tincani 2020), while a field intervention promoting children’s non-cognitive skills proved to foster trust among the treated sample and improved their education achievements, as well as other outcomes in early adulthood such as criminality and employment (Algan et al. 2014). A peculiarity of our experiment is that our subject pool is composed of well-educated young adults, who are currently enrolled in the university or have just completed their studies. Most of our participants still live with their family. This is an appealing feature of our sample, as it excludes any possibility of self-selection into a given area. While the parents of our participants made a conscious choice in terms of housing—driven by income, preferences, convenience, etc.—it is safe to assume that their children had little or no saying in this decision. Moreover, our subject pool is remarkably homogeneous in terms of education, ethnicity, and religion; dimensions that could affect cooperative behavior and preferences in general (Fershtman and Gneezy 2001; Koopmans and Veit 2014; Weng and Yang 2014; Chakravarty et al. 2016; Chuah et al. 2016). Since the majority of our sample is composed of college students, it is also likely that our participants have been exposed to daily interactions with peers from different backgrounds for years. While we do not make any causal claim based on our data, we believe that any difference in preferences and beliefs between participants from areas with different SES would provide some support in favor of the long-lasting effects of environmental conditions and socialization. Furthermore, experimental economists are increasingly using laboratory experiments to measure the importance of the recipient’s identity in cooperation and coordination games.Footnote 2 Our study adds to this strand of the literature by additionally testing whether participants’ behavior in the Trust and the Dictator Game depends on the signal they receive on the socio-economic background of their counterpart. We build on the design developed by Falk and Zehnder (2013), who ran a large experiment involving a random sample of the adult residents of Zurich, and found evidence that trustor condition their behavior on the trustee’s district of residence.Footnote 3 Differently from Falk and Zehnder (2013), however, we focus more on differences in senders’ behavior depending on their own socio-economic background, rather than on the recipients’. Our main goal is to verify if participants from low SES areas are less likely to show preferences and beliefs that are conducive to cooperation—in general or specifically when they face counterparts from wealthier neighborhoods—hence potentially missing opportunities for their development and bolstering poverty and social fragmentation. As the area of residence is an observable characteristic that might affect behavior and interpersonal interactions in the “real world,” providing this information to subjects may strengthen the external validity of our results. Hence, both in the Trust and in the Dictator Game, we provide participants with information on their counterpart’s area of residence, which may be perceived as a signal of socio-economic status. We report three main results. First, participants from the high SES area trust more are more trustworthy, and expect more trustworthiness than their peers from the low SES area. No significant differences emerge in decision tasks that do not imply any strategic interaction, through which we measure generosity, risk attitudes, and time preferences. Second, we find that most of the heterogeneity in trust behavior between the two areas is explained by beliefs about trustworthiness. We also find support for the consensus effect in explaining the belief formation process. Finally, we do not find any evidence of systematic out-group discrimination. Participants do not change their behavior in the Trust and Dictator Game if they are matched with someone from their same area of residence or a different one. The paper is structured as follows. Section 2 describes the experimental location, the subject pool, and the design of the online experiment. Section 3 reports the main results of the paper and the conclusion is drawn in Sect. 4.",1
92.0,1.0,Theory and Decision,15 December 2020,https://link.springer.com/article/10.1007/s11238-020-09796-8,Probability weighting for losses and for gains among smallholder farmers in Uganda,February 2022,Arjan Verschoor,Ben D’Exelle,,Male,Male,Unknown,Male,"How do poor people in the rural areas of developing countries, whose livelihoods are subject to the vagaries of nature, thin or absent markets, dysfunctional institutions and erratic governments, evaluate risky prospects? Contrary to a stereotype of what keeps them poor, smallholder farmers in developing countries may exhibit remarkable tolerance of risk, and are considerably less risk averse than typical Western populations (Vieider et al. 2019). In this study, we investigate, for a sample of smallholder farmers from a developing country, not risk aversion as such but a different aspect of decision-making under risk with important implications for livelihoods decisions: probability weighting. In risky choice decisions, how probabilities are evaluated is crucial. Economists typically think of this as probabilities of outcomes being transformed into decision weights on those outcomes, through some psychologically plausible nonlinear probability-weighting process. Following Allais (1953), violations of the independence axiom of expected utility theory have been observed in a large number of experimental studies, which points to the prevalence of nonlinear probability weighting.Footnote 1 The rationale for probability weighting that is currently most commonly invoked is due to prospect theory (Kahneman and Tversky 1979; Tversky and Kahneman 1992). In prospect theory, the so-called “diminishing sensitivity” relative to a reference point is the central organising principle, which governs both the distinct shape of the value function and the shape of the probability weighting function (PWF). In the context of probability weighting, this means that people are less sensitive to changes in probabilities that take place further from a reference probability. For instance, if the reference probability is 0, then weighted probabilities will increase by more when a probability of an outcome changes from 5 to 6% than when it changes from 10 to 11%. Typically, the reference probabilities 0 and 1 are jointly postulated: the certainty that an outcome will not occur and the certainty that it will occur. Diminishing sensitivity relative to these two reference probabilities produces what Fehr-Duda and Epper (2012, p. 569) call the “famous inverse S”: a PWF that is steep near the reference probabilities of 0 and 100%, and relatively flat in the middle, giving rise to the overweighting of small probabilities and the underweighting of large probabilities (Kahneman and Tversky 1979; Tversky and Kahneman 1992; Wu and Gonzalez 1996; Gonzalez and Wu 1999; Starmer 2000; Takahashi 2011). The inverse S-shaped PWF has been confirmed in the bulk of the large number of experimental studies of probability weighting.Footnote 2 The robustness of this finding suggests that diminishing sensitivity relative to the reference probabilities of 0 and 1 provides a good account of how humans transform probabilities into decision weights. In this study, we investigated as thoroughly as we deemed feasible the probability-weighting habits of small-scale farmers in a poor country. We were motivated by three considerations. First, these farmers are “experts” on decision-making under risk. Frequent hazards such as droughts, floods, pests, and diseases affect their investments, and getting investment decisions right is a matter of life and death (Fafchamps 2003). To the extent that probability weighting is a bias that experienced decision-makers should be less affected by, nonlinear probability weighting should be less pronounced for such farmers than for typical experimental subjects. Second, most of the experimental literature on nonlinear probability weighting does not obtain a separate PWF for gains and losses; to the best of our knowledge, Vieider et al. (2019) is the only previous experimental study of risky choice in a developing country that obtains a separate PWF for losses. Since hazards are so common in rural areas of developing countries, losses are a frequent occurrence. Knowing the PWF for losses is therefore important, and we investigate it in this study as well as for gains. Third, we were motivated by the intriguing possibility that, in nonstandard subject pools, probability weighting could be very different from that typically found in Western labs. In particular, we wondered whether the all-pervasive nature of risk in the rural areas of developing countries would lead to the emergence of reference probabilities not equal to but somewhere between 0 and 1. If certainty is at best an abstract concept for a population, then perhaps more realistic (historically informed) probabilities would act as referents. If so, then the PWF may no longer resemble an inverse S. For investigating probability weighting, we designed and implemented common consequence effect tests (Wu and Gonzalez 1998). There are two reasons that we decided to choose this method. The first is that we needed a simple method for a subject pool with low levels of literacy and numeracy (Dave et al. 2010). Humphrey and Verschoor (2004a, 2b) had previously successfully implemented common consequence effect tests in similar subject pools to ours. Based on extensive piloting, we found that we could considerably increase the refinement of these tests while maintaining excellent subject comprehension. Conceptually attractive alternatives such as Van de Kuilen and Wakker (2011)’s midweight method were deemed to be cognitively too demanding for our subject pool. The second is that we needed a method with sufficient tracking ability of the PWF to investigate whether more than two reference probabilities exist (e.g., 0, 1, and one or more others). For that reason, we decided not to make use of functional forms of the PWF, since these rule out a priori plausible shapes influenced by reference probabilities at the extremes as well as in between those extremes of the domain of the PWF (as explained below). Each subject faced ten choices between two three-outcome lotteries. A pair of lotteries may be thought of as representing a “rung” on a “common consequence ladder”. The rungs are related to each other through an identical manipulation of both lotteries, a so-called common consequence shift. In our case, the manipulation consists in shifting identical probability mass from the worst outcome to the intermediate outcome of both lotteries on one rung, which yields the lotteries on another rung. A preference reversal violates the independence axiom of expected utility theory. Assuming cumulative prospect theory (Tversky and Kahneman 1992), preference reversals permit pronouncements on the relative steepness of the PWF in precisely defined probability intervals.Footnote 3 Importantly, we implemented two game conditions, in a between-subject design. One condition is in the domain of losses, the other in the domain of gains. In the losses version of the experiment, the best outcome of the three-outcome lotteries is equal to the neutral outcome, so that the intermediate outcome and worst outcome represent losses. In the gains version, the worst outcome is equal to the neutral outcome, so that the intermediate outcome and best outcome represent gains. We established the neutral outcome by giving each subject a voucher 3 weeks before the day of the experimental session in which that subject would participate. The voucher showed the name, address, and portrait photo of the face of the subject, as well as the figure of 8000 shillings prominently displayed. 8000 Ugandan shillings are about twice the median daily wage in the study area, in which waged labour is moreover hard to come by. In a scripted, orally delivered message when the vouchers were handed over, subjects were informed that, depending on the decisions they would be asked to take in three weeks’ time, their final earnings could be higher or lower than 8000 shillings. By making the figure of 8000 shillings salient in this way, we aimed to establish this amount as a reference point, so that lower amounts would be thought of as losses, and higher amounts as gains. The reason we handed over the vouchers 3 weeks before experimental days was to alleviate concerns about a house-money effect (Thaler and Johnson 1990) through inducing a sense of entitlement to the 8,000 shillings through the passage of time. In the domain of losses, our investigations focused on probabilities between 0 and 0.8. This is mainly for the sake of realism: investment prospects in which losses are more than 80% likely would not be considered by small-scale farmers in a poor country. We find that probability weighting for losses is pronounced and consistent with diminishing sensitivity relative to the reference probability of 0. Weak evidence is obtained for a second reference probability of 0.5, but we deem it to be insufficient for basing a firm conclusion on. In the domain of gains, we investigated the curvature of the PWF for probabilities between 0.35 and 1. This is again for realism in that investments that have a success probability lower than 35% would not be contemplated in real life. We find no evidence for probability weighting near the probability of 1, so no evidence that 100% acts as a reference probability relative to which diminishing sensitivity is at work. We do find evidence for a PWF that is relatively flat in the middle, which is consistent with an inverse S shape. However, more striking than that is the overwhelming support for expected utility theory (EUT): 42 out of a total of 45 common consequence steps (comparisons between rungs) are consistent with EUT and only 3 point to nonlinear probability weighting. When investigating correlates of probability weighting, we find that traditional farmers, those who farm mainly for subsistence and with a minimal reliance on purchased inputs, evaluate probabilities differently from the rest of the sample. Whereas concavity near zero characterises the PWF for the sample as a whole in the domain of losses, convexity does so for traditional farmers. In the domain of gains, whereas linearity of the PWF when approaching 100% characterises the aggregate sample, convexity does so for traditional farmers. We argue in the paper that the contrast between how traditional and nontraditional farmers evaluate probabilities makes sense of the observed difference between them in livelihoods strategies. The main lesson from our study combines our findings from the domains of losses and gains. In a sample of experienced decision-makers under risk, risky choice in the domain of losses points to pronounced probability weighting that is largely consistent with experimental evidence from Western labs, whereas risky choice in the domain of gains is largely consistent with EUT. Of particular relevance to this special issue, we argue that our results suggest that poor farmers in developing countries should not be seen as shying away from risk but as particularly adept decision-makers under risk (cf. Vieider et al. 2019). We see the contribution of our study as follows. To begin with, we contribute to the literature on individual risky choice in developing countries. Previous studies have investigated probability weighting (Humphrey and Verschoor 2004a, b; Harrison et al. 2010; Tanaka et al. 2010; Liu 2013; l’Haridon and Vieider 2018; Vieider et al. 2018), but none of these have tracked the PWF with the refinement offered here. Humphrey and Verschoor (2004a, 2004b) use only the probabilities 0, 1/4, 1/2, 3/4, and 1. In other studies of probability weighting in nonstudent samples in developing countries, the parameters of a functional form are estimated as those most likely to generate the experimental data given some assumptions on errors; for instance, Prelec (1998)’s two-parameter functional form of the PWF is estimated both in Harrison et al. (2010) and in Vieider et al. (2018). In the design of Tanaka et al. (2010), which they used in rural Vietnam and Liu (2013) used in rural China, the parameter of Prelec (1998)’s one-parameter functional form of the PWF is instead obtained directly based on a series of paired lottery choices. Shapes with more than one convex or more than one concave area cannot be captured by these functional forms. For that reason, they cannot capture shapes influenced by reference probabilities at the extremes as well as in between those extremes of the domain of the PWF. We deem such shapes plausible in our subject pool; see Sect. 2.2 for further discussion.Footnote 4 Moreover, imposing a functional form may hide linearity of the PWF for ranges of its domain for which decision-makers do not transform probabilities nonlinearly into decision weights. We argue in Sect. 5 that this is a plausible reason for the difference between our findings in the domain of gains and those studies for farmers in developing countries that report finding an inverse S-shaped PWF. We also contribute to the fairly small literature on probability weighting in the domain of losses (Tversky and Kahneman 1992; Abdellaoui 2000; Abdellaoui et al. 2005; Etchart-Vincent 2004; Abdellaoui et al. 2011). We make a small theoretical contribution by spelling out common consequence conditions for probability weighting in the domain of losses, make a methodological contribution by providing real incentives in the domain of losses without practising deception (cf. Etchart-Vincent and l’Haridon 2011), and expand the evidence base on probability weighting in the domain of losses by investigating it for a nonstandard subject pool. The paper proceeds as follows. In Sect. 2, we derive our hypotheses in the context of cumulative prospect theory and develop common consequence conditions for testing these hypotheses. Section 3 shows how we implemented these common consequence conditions in our experimental design, and contains details of auxiliary data collection as well as the study area, sampling, and other fieldwork implementation. Section 4 presents descriptive statistics, a balancing test across game conditions, and the univariate and multivariate analyses for testing our hypotheses. Section 5 discusses our main results in terms of the theoretical expectations and the related empirical literature, and concludes.",3
92.0,1.0,Theory and Decision,13 May 2021,https://link.springer.com/article/10.1007/s11238-021-09821-4,Endowment effects in the risky investment game?,February 2022,Stein T. Holden,Mesfin Tilahun,,Male,Unknown,Unknown,Male,"The term “endowment effect” was first used by Thaler (1980) and he related this effect to the fact that losses are weighted more heavily than gains and associated this with prospect theory (PT) and loss aversion in settings without risk. The loss in utility associated with giving up one good is greater than the gain in utility from getting the same good; “losses loom larger than gains”. Third-Generation Prospect Theory (PT3) (Schmidt et al. 2008) provides the basis for endowment effects existing for monetary endowments and risky and uncertain prospects such as lottery tickets. Our study investigates whether there can be significant endowment effects in the one-shot risky investment game (Gneezy et al. 2009), that built on the dynamic version of the game (Gneezy and Potters 1997). While the purpose of the dynamic version of the game was to illustrate myopia due to loss aversion and thereby endowment effects, it is less obvious that the one-shot version of the game has the same effect. The beauty of the one-shot game is its simplicity and it has been proposed as particularly useful in field settings for respondents with limited numeracy skills (Charness and Viceisza 2016). An applied development economist may easily ignore the potential endowment effect associated with loss aversion in the game and frame the one-shot game results with Expected Utility Theory (EUT). While most studies using the risky investment game have not used the game to estimate a parameter for risk aversion based on EUT (capturing the curvature of the utility function) of respondents, this may be tempting and there are a few studies that also do this (e.g., (Crosetto and Filippin 2016; Dasgupta et al. 2019). If endowment effects/loss aversion plays a role in the one-shot game, they get biased estimates of the utility curvature. Our experiment was implemented as a field experiment in rural Ethiopia with young business group members as subjects. These subjects had on average 6 years of education and were computer illiterate. The experiments were introduced to them by trained experimental enumerators using classrooms in local schools as “labs”. Our “lab-in-the-field” experiment was designed to allow us to distinguish between safe and risky money that were provided as initial endowments. We assess whether an endowment effect is found for safe initial money provided as well as for an initial 50–50 lottery or whether the first only or none of these monetary endowments are associated with an endowment effect. In our baseline treatment T1 (“Safe Base”), the respondents were provided an initial endowment, of which they were free to invest any share of the initial endowment in a 50–50 lottery that would pay out three times the invested amount or nothing. This treatment is compared to an alternative treatment T2 (“Full Risk”) where the respondents were initially provided the full 50–50 lottery of three times the safe amount provided in treatment T1. In T2, they could sell themselves out of the risky lottery at the same exchange rate between risky and sure money as in T1. Our results show a highly significant treatment effect demonstrating endowment effects and reference dependence in the game. EUT should therefore not be used to estimate risk aversion in the narrow sense of utility curvature based on the game. The paper is organized as follows. Section 2 of the paper outlines the experimental treatments and theoretical framework. Section 3 describes the experimental procedures of the field experiment. Section 3 explains the estimation strategy. Section 4 presents the results and Sect. 5 discusses the findings in view of the alternative theories and Sect. 6 concludes and makes some suggestions for further work.",6
92.0,2.0,Theory and Decision,28 June 2021,https://link.springer.com/article/10.1007/s11238-021-09816-1,Inequality of decision-makers’ power and marginal contribution,March 2022,Shmuel Nitzan,Tomoya Tajika,,Male,Male,Unknown,Male,"The analysis of power indices may explain, at least partly, inequality among professionals, for example, the variability in the status and salaries of experts. Classic analysis of committee voting quantifies the power of each voter, for a given voting rule, as the probability that others will vote in such a way that the communal outcome depends on that individual’s vote (the probability of being pivotal). In common interest settings, where voting reflects voters’ independent signals of what is socially optimal, Nitzan and Paroush (1982) and Shapley and Grofman (1984) identified the voting rule that optimally weights the voters according to the precision of their signal. In this study, it is assumed that the rule applied by the committee to reach the collective decision is optimal, namely, maximizes the performance of the committee. This assumption is plausible provided that the committee efficiently exploits the heterogeneous skills of its members. It implies, in particular, that the committee recruits only useful members that make a positive contribution to the performance of the group. Using this optimal decision rule, Ben Yashar and Nitzan (2019) define a voter’s “skill-dependent (s-d) power” as his power under the optimal decision rule, given a particular vector of signal quality. This measure takes into account not only the relative number of the decision profiles enabling the decision-maker to be pivotal but also the different probabilities of these profiles. This power measure is certainly relevant to the status of a decision-maker within his group. In more recent work, Ben Yashar et al. (2021), define an alternative type of a decision-maker’s power, viz., his marginal contribution (mc)—the amount by which group accuracy increases when the individual joins the group. This alternative measure of (economic) power seems more pertinent to the remuneration of the decision-makers. A plausible model justifying salary differentials among decision-makers may require the introduction of principals who offer salaries to members in a group of experts and, then, perhaps as a result of a kind of Bertrand competition, the payment of mc to each expert (group member) could be justified as an equilibrium strategy. Ben Yashar and Nitzan (2019) showed that slight heterogeneity in signal precision can lead to large differences in skill-dependent power. This is important because rewards may be related to power. Although this finding sheds new light on the possible justification of unequal distributions of reward within groups of professionals of seemingly inconsequential diverse personal qualifications (in our case, their decisional competencies), it did not distinguish between s-d power and marginal contribution, which seems more relevant to salary determination. To overcome this shortcoming, Ben Yashar et al. (2021) added to the picture the marginal contributions of the decision-makers and exposed their formal relationship to s-d power. This extension revealed that not only that rewards (in terms of status or monetary payment) dependent on s-d power can be very unequal, the distribution of rewards that depend on the marginal contributions can be even more unequal. The current paper clarifies that the increased inequality of s-d power and mc is due to the fact that small differences in decision-makers’ competence lead to large differences in their optimal voting weights, which, in turn, result in large differences in their s-d power. The main objective of this paper is to considerably extend the preliminary comparison undertaken in Ben Yashar et al. (2021) between inequality of the distributions of the two measures. It turns out that in small (large) groups, on average, mc is more (less) unequal than s-d power and both are more unequal than the optimal weights of the decision-makers. Initially, this is shown via simulations, drawing signal precisions randomly from a uniform distribution. We then check the robustness of the finding by applying alternative symmetric beta distributions. Some policy implications dealing with the required extent of reduction in skill – inequality under particular environments that differ in the prevailing norms and institutional characteristics that determine the dependence of rewards on the two types of power are also derived. The paper then goes on to show that (1) one measure of inequality declines with the number of voters while other increase and (2) improving the distribution of voter skill in a neutral way exacerbates inequality, In the next section, we present the symmetric version of the uncertain dichotomous choice model. The skill-dependent and independent power indices and the second type of power, mc, are introduced in Sect. 3. Section 4 focuses on the comparison between the distributions of skill, s-d power and mc. It contains the test of the robustness of the results and the derivation of some policy implications. This is followed by examining, in Sects. 5 and 6, the effect of group size and neutral skill improvement.",
92.0,2.0,Theory and Decision,22 June 2021,https://link.springer.com/article/10.1007/s11238-021-09825-0,Small group forecasting using proportional-prize contests,March 2022,Ronald Peeters,Fan Rao,Leonard Wolk,Male,,Male,Mix,,
92.0,2.0,Theory and Decision,15 June 2021,https://link.springer.com/article/10.1007/s11238-021-09828-x,Majority properties of positional social preference correspondences,March 2022,Mostapha Diss,Michele Gori,,Unknown,Female,Unknown,Female,"Consider a society where \(n\ge 2\) individuals have to determine a social preference on a set of \(m\ge 2\) alternatives by means of their individual preferences. Assume that individual and social preferences are required to be linear orders (strict rankings) and call a preference profile any list of n linear orders each of them associated with a specific individual in the society. Any correspondence from the set of preference profiles to the set of social preferences represents then a particular decision process that determines a possibly empty set of social preferences whatever preferences individuals express. Such correspondences are called social preference correspondences (spcs). Among all the conceivable spcs, the positional ones are frequently used in practical situations. A positional spc is associated with a scoring vector \(w = (w_r)_{r=1}^m\in {\mathbb {R}}^m\) such that \(w_1\ge w_2\ge \cdots \ge w_m\) and \(w_1 > w_m\). Given a preference profile, each time an alternative is ranked r-th by one individual it obtains \(w_r\) points; the spc selects those social preferences that are consistent with the total score obtained by alternatives, namely if the total score of an alternative is greater than the one of another alternative then the former alternative has to be socially preferred to the latter one. Due to the importance of positional spcs, a deep understanding of their properties is certainly crucial. In this paper, we investigate the majority properties of positional spcs. The qualified majority principle, which generalizes the simple majority principle, has received a significant amount of attention in the social choice literature (Bubboloni & Gori, 2014, 2015; Craven, 1971; Ferejohn & Grether, 1974; Greenberg, 1979). For spcs such a principle can be formalized as follows. Given \(\nu \in {\mathbb {N}}\cap (\frac{n}{2},n]\), a spc is said to satisfy the qualified majority property associated with the majority threshold \(\nu\) (briefly \(\nu\)-majority property) if, for every preference profile p, the fact that an alternative x is preferred to another alternative y by at least \(\nu\) individuals implies that x is ranked over y in any social preference selected by the spc. Note that the \(\nu\)-majority property coincides with the simple majority principle when \(\nu =\lceil \frac{n+1}{2}\rceil\). It is known that the condition \(\nu >\frac{m-1}{m}n\) is necessary and sufficient for the existence of a nonempty valued spc satisfying the \(\nu\)-majority property.Footnote 1 As a consequence, since positional spcs are nonempty valued, the condition \(\nu \le \frac{m-1}{m}n\) implies that no positional spc satisfies the \(\nu\)-majority property. On the other hand, when \(\nu >\frac{m-1}{m}n\), understanding whether a positional spc fulfills the \(\nu\)-majority property requires a careful analysis. Quite surprisingly, a study of the majority properties for positional spcs has not been developed yet.Footnote 2 In this paper, we fill the gap by fully characterizing, for every majority threshold \(\nu\), the positional spcs that satisfy the \(\nu\)-majority property (Propositions 5 and 7). In particular, we get that the Borda spc fulfils the \(\nu\)-majority property for all \(\nu >\frac{m-1}{m}n\) and that the plurality and the antiplurality spcs do not satisfy the \(\nu\)-majority property for all the values of \(\nu\). We focus then on a further property for spcs, called minimal majority property, introduced by Bubboloni and Gori (2015). A spc is said to satisfy the minimal majority property if, for every preference profile p, it associates with p social preferences that are all consistent with the qualified majority principle for all majority thresholds \(\nu\) which guarantee for p the existence of at least a social preference consistent with the \(\nu\)-majority principle (see Sect. 2 for an illustrative example). In other words, a spc satisfies the minimal majority property if, for every preference profile p, it always selects social preferences that obey the strictest possible version of the qualified majority principle which allow to get for p at least a social preference. Thus, the minimal majority property is remarkable as, in particular, it is stronger than the \(\nu\)-majority property for all the values of \(\nu\) greater than \(\frac{m-1}{m}n\). In the paper we characterize the positional spcs that satisfy the minimal majority property (Propositions 9, 10, 12 and 13). In particular, we show that the Borda spc fulfils the minimal majority property if and only if \(m=2\) or \(n=2\) or \((n,m)=(4,3)\) and that if a positional spc satisfies the minimal majority property then also the Borda spc does. The last part of the paper is devoted to the evaluation of the probability that the Borda, the plurality, and the antiplurality spcs fulfil the qualified and the minimal majority properties using the well-known Impartial and Anonymous Culture (IAC) assumption which is often used to model the distribution of individuals’ preferences for such studies. We believe that studying the probability of the agreement between some well-studied spcs and the two considered variants of the majority principle is an important research direction. To the best of our knowledge, our paper can be considered the first to explore such a framework. The paper is organized as follows. Section 2 describes the basic framework and some preliminary results. Sections 3 and 4 describe our main results regarding the conditions under which a positional spc satisfies the qualified and the minimal majority property. Section 5 presents our computational analysis related to the probability that the Borda, the plurality and the antiplurality spcs fulfil the two aforementioned conditions. Finally, the last section presents our conclusions.",
92.0,2.0,Theory and Decision,17 June 2021,https://link.springer.com/article/10.1007/s11238-021-09824-1,Every normal-form game has a Pareto-optimal nonmyopic equilibrium,March 2022,Steven J. Brams,Mehmet S. Ismail,,Male,Male,Unknown,Male,"The standard solution concept in noncooperative game theory is that of Nash equilibrium (NE). However, what might be considered a “cooperative outcome” in a significant number of games is not an NE.Footnote 1 The best-known example of such a game is Prisoners’ Dilemma. To justify cooperation in such games, one approach is to posit repeated play of a game. According to the folk theorem of noncooperative game theory, all Pareto-optimal outcomes become NEs if (i) the repetition is infinite or has no definite end and (ii) the players are sufficiently patient. However, most real-life games are not played, de novo, again and again; moreover, the resulting plethora of NEs in repeated play has little predictive power. Later, we discuss other approaches to the dynamic analysis of games. In this paper, we assume that play starts at an outcome, from which players make farsighted calculations of where play will terminate after a finite series of moves and countermoves within a game. This assumption differs radically from the usual assumption of normal-form games—represented by a payoff matrix—in which the players are assumed to make independent strategy choices. By contrast, we specify alternative rules of play that yield nonmyopic equilibria (NMEs), which always exist in pure strategies in normal-form games. In 45 of the 57 distinct 2 × 2 strict ordinal games of conflict with a cooperative outcome, this outcome is an NME, but not necessarily an NE, in all except one. We begin by defining and illustrating NMEs using Prisoners’ Dilemma and other 2-person and n-person games. Although the noncooperative outcome in Prisoners’ Dilemma is an NME that coincides with the NE, the news is not all bad: the cooperative outcome is also an NME and maximin: it maximizes the minimum payoff that the players receive. (Note that a maximin outcome differs from a maximin strategy, which maximizes the minimum payoff a player can guarantee in a game.) NMEs are based on rules wherein play commences at an outcome, called an initial state, and players can move or countermove from that state according to rules that we specify. If players would not move from an initial state, anticipating all possible moves and countermoves in a game of complete information and common knowledge, then that state is an NME. A state may also be an NME if players would move to it from another state—not just stay at it if they start there—as we discuss later. In Prisoners’ Dilemma, the cooperative outcome is an NME when play commences at it or at one of the two win–lost outcomes. However, there are games in which a cooperative outcome, like that in Prisoners’ Dilemma, is not an NME. Generally speaking, NMEs are at least middling outcomes for the players, though there are exceptions, which we will illustrate. Unlike NEs, at least one is always Pareto-optimal (our main result, which has been conjectured but not previously proved). In Sect. 2, we spell out the rules of play and rationality rules for calculating NMEs in 2 × 2 games, which we apply to Prisoners’ Dilemma. In Sect. 3, we use another 2 × 2 game and two larger games (more strategies, more players) to illustrate other properties of NMEs, such as that they need not be maximin. We then extend the analysis to all finite games and prove that they always contain at least one Pareto-optimal NME. Our proof, which is very simple, does not identify one but does show that one must exist. In Sect. 4, we compare our results with those of other theorists who have incorporated dynamic reasoning into the play of games. In Sect. 5, we discuss the normative implications of choosing Pareto-optimal NMEs, especially of cooperation, when they are not myopically stable. This more expansive view for ameliorating conflict offers decision-makers an incentive to consider the long-term consequences of their choices, especially when the NEs in a game are not Pareto-optimal or do not exist in pure strategies.",1
92.0,2.0,Theory and Decision,08 July 2021,https://link.springer.com/article/10.1007/s11238-021-09833-0,Strategic manipulation in judgment aggregation under higher-level reasoning,March 2022,Zoi Terzopoulou,Ulle Endriss,,Female,Unknown,Unknown,Female,"In many instances of their social life, individuals—being members of various groups—need to reach collective decisions by aggregating their private judgments on several issues: from choosing what kind of food to have during a dinner with friends, to reaching an agreement with their colleagues about what policy their company should implement. Judgment aggregation is a formal framework in which such settings are modelled and studied, especially when the issues to be decided upon are logically interconnected (List, 2012; Grossi & Pigozzi, 2014; Endriss, 2016). The aggregation of individual judgments carries further complications when individuals behave strategically, trying to manipulate the collective decision to obtain a better outcome for themselves. These manipulation acts may be achieved by lying, i.e., by reporting a judgment that is different from the individual’s truthful one. This paper focuses on the various levels of reasoning that take place in an individual’s mind prior to making a final decision about which judgment to report, and investigates the manipulability of rules that are used for the aggregation of the multiple judgments of a group. To illustrate the idea, consider the following example. Suppose that in the office of a political party, a decision needs to be taken concerning the opening of a new secretary position. These kinds of issues are typically settled by the leader (Alice) and the deputy (Bob) of the party. But it is also commonly known among all members of the party that both their leader and their deputy only truly want the opening of a new position if they plan to hire a specific person close to them. So, to avoid political tension caused by fights between the party’s two main figures, the established rule is that the position will be announced if and only if exactly one of the two individuals declares in favour of it. Assume now that the leader wants the position to open (she is trying to have her cousin hired), while the deputy is not interested in the position at the moment and would prefer to have the opening rejected, and this is common knowledge among them. Before reporting her opinion officially, the leader may think that—since the deputy does not want the position—she can be truthful, because she will be the only one in favour of it and thus she will finally be able to hire her cousin (this thought corresponds to level-1 reasoning). However, the leader could also think that—since the deputy knows that she wants the position but he prefers to not have it announced yet—the deputy has an incentive to lie by declaring that he also agrees with the new position, so that the rule prevents it from being announced (this thought of the leader corresponds to level-2 reasoning). In this case, an incentive for the leader to lie is created. But she may think that the deputy has already followed the previous reasoning in his mind, expecting her to lie and therefore making the decision to tell the truth that he does not want the position. Then, it would again be better for the leader to tell the truth as well. Continuing with this reasoning process, Alice and Bob will easily end up applying higher-level reasoning.\(\triangle\) Already from the example above, we realise that it is not clear how to determine at which level the reasoning process of an individual terminates. Theoretically, the interactive reasoning of the individuals in a group could proceed indefinitely. The question about which level of reasoning can be expected in practice by rational individuals is addressed by behavioral scientists (e.g., Camerer et al., 2004; Costa-Gomes & Crawford, 2006; Costa-Gomes et al., 2001), whose empirical results are often not able to provide a categorical global answer. Despite the limitations that the identification of the exact computational abilities of human beings presents, it is generally accepted that in common real-life strategic situations individuals engage in thinking of at most three levels (Arad & Rubinstein, 2012; Camerer et al., 2004; Stahl & Wilson, 1995). In any case, it is undeniably true that individuals can only reason within finitely many levels. Thus, in this paper, we focus on finite levels of interactive reasoning. Under this assumption, we explore basic judgment aggregation problems and study the incentives of sophisticated individuals to lie, or in other words, to manipulate the aggregation rule used. We follow the concept of level-k reasoning, first introduced by Nagel (1995) and Stahl and Wilson (1995).Footnote 1 It is usually accepted that manipulation is undesirable in contexts of collective decision-making, since it can drastically distort the collective decision. From this perspective, the main question that we wish to address here is the following. To what extent can higher-level reasoning protect an aggregation rule from being susceptible to manipulation? To answer our question, we need to refine the standard framework of strategic manipulation in judgment aggregation (pioneered by Dietrich and List 2007b) and formally account for the notion of higher-level reasoning. Moreover, aiming for a fully fledged model and building on our previous work (Terzopoulou & Endriss, 2019), we incorporate the potential lack of information the individuals may exhibit with respect to the truthful judgments of the other members of their group (note that this was not the case in Example 1, where both individuals had complete information about each other’s judgments). For any type of partial information that may manifest itself in a given scenario, we prove two main results: first, on the positive side, any aggregation rule that is immune to manipulation under first-level reasoning will remain resistant to manipulation under all higher levels of reasoning; that is, higher-level reasoning is never detrimental to a rule’s immunity to manipulation. Second, on the negative side, for any rule that is manipulable under first-level reasoning, it unfortunately never holds that we can find a higher level from which on the rule becomes immune to manipulation; even if there exists a “safe” level in such a case, the immediately next level will still allow for manipulation. Hence, roughly speaking, we conclude that higher-level reasoning cannot guarantee immunity to manipulation. It is worth stressing here that, as is well-known, preference aggregation can be embedded into judgment aggregation (List & Pettit, 2004; Dietrich & List, 2007a), and notable implications hold regarding our results. In particular, when we talk about “any aggregation rule”, the reader should feel free to think about preference aggregation (and voting) rules as well—our results can be immediately transferred to these domains.Footnote 2 The remainder of this paper is structured as follows. Section 2 discusses previous literature that is pertinent to our work and Sect. 3 illustrates the central ideas, assumptions, and results of this paper by means of examples. Section 4 then recalls the standard formal model of judgment aggregation and introduces our definition of strategic manipulation in the presence of partial information under higher-level reasoning. Our results are presented in Sect. 5, and we conclude in Sect. 6.",
92.0,2.0,Theory and Decision,19 June 2021,https://link.springer.com/article/10.1007/s11238-021-09823-2,The triple-store experiment: a first simultaneous test of classical and quantum probabilities in choice over menus,March 2022,Ismaël Rafaï,Sébastien Duchêne,Andrei Khrennikov,Unknown,Male,Male,Male,"Quantum decision theory is a field that has considerably expanded over the last 15 years, supported by psychologists, physicists, mathematicians, economists and philosophers (see Busemeyer and Bruza 2012; Haven and Khrennikov 2013; Pothos and Busemeyer 2013; Ashtiani and Azgomi 2015; Bruza et al. 2015; Yearsley and Busemeyer 2016, for reviews and explanations). In contrast to classical models of judgment, which are based on Bayesian probabilities and which state that agents exhibit defined preferences, quantum cognition models employ an innovative mathematical formalism, applied in quantum mechanics, in which individuals’ preferences and beliefs are dependent on the context and the order of information. These models are mathematically sophisticated and can deal with numerous well-established paradoxes, including order effects (e.g., the answers to two questions depend on the order in which they are asked) and the conjunction fallacy (e.g., the fact that Linda is both a feminist and a bank employee is incorrectly considered more likely than just the fact that Linda is a bank employee). These theoretical models have grown rapidly and in many interdisciplinary fields (decision theory, game theory, cognitive biases and financial market prediction models). They are promising because they enable several paradoxical behaviours to be accounted for within the same mathematical framework, without the need to resort to ad-hoc models, thus paving the way for a more generic theory on human behaviour. For example, Wang and Busemeyer (2013) and Wang et al. (2014) have developed a quantum model for attitude questions in surveys. Conte et al. (2009) presented a model for mental states of visual perception, and Atmanspacher and Römer (2012) discussed non-commutativity. In addition, quantum models have been developed to account for the conjunction fallacy (Franco 2009; Busemeyer et al. 2011, 2015; Busemeyer and Bruza 2012; Pothos and Busemeyer 2013; Yukalov and Sornette 2010, 2011; Yearsley and Trueblood 2018) and the violation of the sure-thing principle (Busemeyer et al. 2006; Busemeyer and Wang 2007). Aerts and Sozzo (2011); al Nowaihi and Dhami (2017); Aerts et al. (2018), and Eichberger and Pirner (2018) modelled the famous Ellsberg paradox, while Khrennikov and Haven (2009); Yukalov and Sornette (2010), and Aerts et al. (2011) discussed the Allais paradox. In a more general framework, models of decision theory and bounded rationality have been introduced (Danilov and Lambert-Mogiliansky 2008, 2010; Mogiliansky et al. 2009; Yukalov and Sornette 2011; Asano et al. 2017; Basieva et al. 2018; Pisano and Sozzo 2020). Finally, in game theory, quantum models that account for paradoxical strategies have also emerged (Piotrowski and Sładkowski 2003; Landsburg 2004; Pothos and Busemeyer 2009; Brandenburger 2010; Piotrowski and Sładkowski 2017; Denolf et al. 2017; Khrennikov and Basieva 2014a; Khrennikov 2015). Despite the explanatory power of these models, it is important to investigate the theoretical constraints underlying the quantum formalism and then compare them with empirical data. This is absolutely required to determine whether quantum cognition models can be justified and to evaluate to what extent they can be used as descriptive or predictive models. In this regard, Boyer-Kassem et al. (2016b) studied non-degenerate quantum models of order effects (Conte et al. 2009; Busemeyer and Bruza 2012; Pothos and Busemeyer 2013; Wang and Busemeyer 2013; Wang et al. 2014), and derived mathematical constraints called Grand Reciprocity (GR) equations from the law of reciprocity. To be acceptable, models must respect these constraints. The authors then analyzed large empirical data sets and observed that the non-degenerate models did not satisfy these equations. Boyer-Kassem et al. (2016a) and Duchêne et al. (2017) investigated the empirical validity of a large proportion of the non-degenerate and degenerate quantum models of the conjunction fallacy (Franco 2009; Busemeyer et al. 2011, 2015; Busemeyer and Bruza 2012; Pothos and Busemeyer 2013) and proposed several tests. The authors then conducted different types of experiments, with or without monetary incentives and on computer or on paper, in different universities and subsequently compared the experimental data with these tests. The results of Boyer-Kassem et al. (2016a) indicate that some of these models, exploiting order effects, do not meet these tests. The debate about whether these models fail to account for the conjunction fallacy is still an open issue with pros and cons arguments. In view of the limitations of standard quantum models, new promising directions have been proposed with generalized observables or a more general measurement theory; for example based on positive operator-valued measures (POVMs) and quantum instruments (Khrennikov and Basieva 2014b; Khrennikov et al. 2014; Ozawa and Khrennikov 2021; Busemeyer and Wang 2019), or on the modification of the Born rule (Aerts and de Bianchi 2017). In that direction, the Busemeyer et al. (2011) quantum model for the conjunction fallacy has been updated using POVM’s, instead of projectors (Miyadera and Philips 2012). Aerts (2009) also offered an interesting alternative approach—with respect to most of the models—and does not require any order effects. In this crucial debate on the extension of quantum models, it is therefore central for the scientific community to continue the exploration and evaluation of the empirical robustness of these models. A particularly intriguing opportunity to test simultaneously the empirical validity of both the classical and quantum probabilistic frameworks comes from Sorkin’s equalities (Sorkin 1994). Those mathematical conditions are well-represented by a triple-slit extension of Young’s famous double-slit experiment studying the interference patterns of light waves. Sorkin theoretically derived, from the framework of quantum mechanics, that the interference pattern formed in a multi-slit experiment is a simple combination of patterns observed in two-slit experiments. Therefore, in an observed three-slit physical system, a violation of quantum mechanics can be identified if interference terms of a higher order than the two-slit term are measured experimentally. Similarly, classical probability (CP) is violated if interference terms of any order are measured. Indeed, the intriguing idea is to use a three-slit experiment and the derived Sorkin’s equality to measure and interpret potential deviations from either Kolmogorov’ axioms or, at a higher order, Born’s rule and the superposition principle (Sinha et al. 2015; Skagerstam 2018). In recent years, the multi-slit modelling framework has gained momentum in the physics literature. It has been exploited to run precise experiments for testing quantum properties of several physical systems such as photons, nuclear spins, nitrogen-vacancy centres in diamond, and large molecules and atoms (Sinha et al. 2010; Söllner et al. 2012; Park et al. 2012; Gagnon et al. 2014; Kauten et al. 2014, 2017; Cotter et al. 2017). Inspired by this literature, we conducted an experiment with human participants to investigate Sorkin’s equality in the context of quantum cognition (Basieva and Khrennikov 2017). As far as we know, this is the first scientific contribution that tests the validity of quantum probabilities (QP) in human decision-making by adopting the three-slit framework. In the next section, we present the Sorkin’s equality and its interpretations in physics and decision theory. Then, we propose and describe the triple store experiment in Sect. 3 and in Sect. 4 we detail its implementation. Section 5 highlights the main results and the statistical analysis. The last section concludes by discussing the results and potential further investigations.",3
92.0,2.0,Theory and Decision,15 June 2021,https://link.springer.com/article/10.1007/s11238-021-09826-z,Identification and welfare evaluation in sequential sampling models,March 2022,Jetlir Duraj,Yi-Hsuan Lin,,Unknown,Unknown,Unknown,Unknown,,
92.0,3.0,Theory and Decision,18 March 2022,https://link.springer.com/article/10.1007/s11238-022-09886-9,Introduction to the Special Issue in Honor of Peter Wakker,April 2022,Mohammed Abdellaoui,Han Bleichrodt,Horst Zank,Male,,Male,Mix,,
92.0,3.0,Theory and Decision,10 February 2022,https://link.springer.com/article/10.1007/s11238-022-09879-8,A comment on the axiomatics of the Maxmin Expected Utility model,April 2022,Shiri Alon,,,Unknown,Unknown,Unknown,Unknown,,
92.0,3.0,Theory and Decision,10 February 2022,https://link.springer.com/article/10.1007/s11238-021-09862-9,A framework for the analysis of self-confirming policies,April 2022,P. Battigalli,S. Cerreia-Vioglio,T. Sargent,Unknown,Unknown,Unknown,Unknown,,
92.0,3.0,Theory and Decision,08 November 2021,https://link.springer.com/article/10.1007/s11238-021-09844-x,Law of demand and stochastic choice,April 2022,S. Cerreia-Vioglio,F. Maccheroni,A. Rustichini,Unknown,Unknown,Unknown,Unknown,,
92.0,3.0,Theory and Decision,31 March 2022,https://link.springer.com/article/10.1007/s11238-022-09871-2,A gene–brain–behavior basis for familiarity bias in source preference,April 2022,Robin Chark,Songfa Zhong,Soo Hong Chew,,Unknown,,Mix,,
92.0,3.0,Theory and Decision,07 March 2022,https://link.springer.com/article/10.1007/s11238-022-09880-1,"Optimality of deductible: a characterization, with application to Yaari’s dual theory",April 2022,Alain Chateauneuf,Michèle Cohen,Mina Mostoufi,Male,Female,Female,Mix,,
92.0,3.0,Theory and Decision,17 March 2022,https://link.springer.com/article/10.1007/s11238-022-09888-7,"Correction to: Optimality of deductible: a characterization, with application to Yaari’s dual theory",April 2022,Alain Chateauneuf,Michèle Cohen,Mina Mostoufi,Male,Female,Female,Mix,,
92.0,3.0,Theory and Decision,15 January 2022,https://link.springer.com/article/10.1007/s11238-021-09856-7,The impact of experience on decisions based on pre-choice samples and the face-or-cue hypothesis,April 2022,Ido Erev,Ofir Yakobi,Nick Chater,Male,Male,Male,Male,"In many natural choice tasks, ranging from buying sneakers to important policy decisions, the information available to the decision-makers comes from three main sources: descriptions of the possible alternatives, retrospectively consulting experience of similar decisions in the past, and prospective exploration of new experiences to help inform the decision. For example, while choosing between pairs of sneakers in the store, the decision-makers can rely on verbal descriptions of the pairs, use past experiences with similar pairs, and collect new experiences by trying the available pairs on. Similarly, when world leaders considered alternative reactions to the 2008 subprime crisis, they relied on reports concerning the risk that different financial institutions will collapse, recalled the outcomes of previous crises, and searched for new information, including expert judgments and public opinion, about the current crisis to help decide which policy to choose. Most experimental studies of choice behavior concentrate on the impact of only one of these three sources of information. Mainstream studies of the impact of description (e.g., Kahneman & Tversky, 1979) used the “description” paradigm presented in the left-hand side of Fig. 1. The clearest analyses of the impact of past experiences used the “consequential full feedback clicking” paradigm described in the center of Fig. 1 (see review in Erev & Haruvy, 2016), where each new decision about which button to click is informed by retrospective recall of what reward was received when the buttons were clicked in the past. The clearest analyses of the impact of new experiences used the sampling paradigm described in the right-hand side of Fig. 1 (see review in Wulff et al., 2018), in which, when faced with a decision, people can prospectively gather evidence about which button to click by costlessly sampling reward values from each button. In this exploration phase, they do not actually receive any reward. The purpose of the sampling is purely to inform the decision to be made. Typical screens in the description, consequential full feedback clicking (clicking for short), and sampling paradigms used in previous research Comparison of the three lines of research has highlighted a “description-experience gap” (Hertwig & Erev, 2009): studies that used the description paradigm document significantly higher sensitivity to low probability (rare) outcomes than studies that used the clicking and sampling paradigms (Barron & Erev, 2003; Hertwig et al., 2004). In addition, previous research documents significant differences between the two types of “decisions from experience” (de Palma et al., 2014). To clarify these differences, it is useful to distinguish between the measurement of the decision-experience gap, and the definition of “underweighting of rare events.” In binary choice, the basic gap is the difference between the choice rate of the option that “pays more with higher probability” when the decision is made from experience and from description. The term underweighting of rare events refers to a tendency to prefer that option that pays more with higher probability when this option does not maximize expected return.Footnote 1 The main difference between the two experience paradigms involves the conditions that trigger underweighting of rare events. Studies that used the consequential full feedback clicking paradigm suggest that when the feasible prospects are of similar EV, the tendency to underweight rare events is rather robust.Footnote 2 This bias occurs even when the decision-makers receive accurate information concerning the feasible payoffs (e.g., Erev et al., 2017), and when the observed rate of the rare outcomes is larger than the expected rate. In contrast, studies that used the sampling paradigm document clear underweighting of rare events only when three additional conditions hold: the decision-makers are not presented with a description of the possible outcomes (Abdellaoui et al., 2011; Erev et al., 2008), the rare events are underrepresented in the sample (Wulff et al., 2018), and only one of the options is risky (Glöckner et al., 2016).Footnote 3 Meta-analysis (Wulff et al., 2018) shows that the main properties of decisions from prospectively drawing samples (rather than sampling retrospectively from memory) can be summarized by the assertion that people use the “natural mean” heuristic (Hertwig & Pleskac, 2008) and select the option with the higher sample mean. To derive practical implications from these experimental results, it is important, of course, to understand how these different sources of information interact. It is known, for example, that the impact of the description can be modified by past experiences (Yechiam et al., 2005; Jessup et al., 2008; Lejarraga & Gonzalez, 2011; Marchiori et al., 2015; Erev et al., 2017; Cohen et al., 2020). In one demonstration of this effect (Marchiori et al., 2015), participants faced 60 distinct decisions from description tasks involving a safe prospect (e.g., “5 with certainty”) and a long-shot gamble (“100 with p = 0.05; 0 otherwise”). After each choice, participants received full feedback concerning the realized payoffs of each option. The results reveal an initial tendency to prefer the gamble that implies overweighting of rare gains (as predicted by prospect theory; Kahneman & Tversky, 1979; Wakker, 2010). Yet, after a few trials, most participants preferred the sure payoff, and behaved as if they underweighted the rare event. Marchiori et al. note that these results can be explained with models that share the following assumptions: (1) the descriptions are used as memory cues, and lead people to select the strategies that provided the best outcomes in small samples of similar situations in the past. (2) The availability of feedback changes the most similar situations. Before receiving feedback, the most similar situations include natural decisions made (before the beginning of the experiment) based on noisy estimations, and after receiving feedback the most similar situations are the previous trials (in the current experiment) that are made based on precise estimations. This account implies that the initial overweighting of rare events reflects the fact that the occurrence rate given low noisy estimate tends to be larger than the estimate (Erev et al., 1994), and this bias overrides the impact of the tendency to rely on small samples. The current paper extends this research to consider whether new samples can also cue memory for past experiences, which in turn influence choice. Our analysis rests on the assumption that people tend to select the alternatives that led to the best outcomes in similar situations in the past (Skinner, 1953; Plonsky et al., 2015; Chater et al., 2020). But which similar situations are retrieved from memory may depend, in part, on which new samples are experienced. Thus, it is possible that new samples that favor one of the options may trigger memories of bad experiences with that option that will in turn decrease the tendency to select it. For instance, a pleasant feeling while trying on a new pair of sneakers can remind the consumer of an old pair that felt similarly comfortable in the store but lost its comfortable feeling after a few weeks. Our experimental analysis reveals strong interactions between prior experience and new samples. In Study 1, repeated experience with pre-choice samples appears to reduce sensitivity to the average outcome in the samples and enhances underweighting of rare events. Study 2 shows that repeated experience with pre-choice samples can reverse the impact of the new information (and decrease the tendency to select the alternative that provides the best outcome in the new sample). The paper concludes with a discussion of the common elements of decisions from experience and decisions from description.",4
92.0,3.0,Theory and Decision,23 August 2021,https://link.springer.com/article/10.1007/s11238-021-09839-8,Individual-level loss aversion in riskless and risky choices,April 2022,Simon Gächter,Eric J. Johnson,Andreas Herrmann,Male,Male,Male,Male,"Loss aversion—the psychological propensity that losses loom larger than equal-sized gains relative to a reference point—can occur in riskless and in risky choices, as argued in two seminal papers by Amos Tversky and Daniel Kahneman (Kahneman & Tversky, 1979; Tversky & Kahneman, 1991). An example for loss aversion in riskless choice is the ‘endowment effect’—the observation that experimental subjects who are randomly endowed with a commodity ask for a selling price that exceeds substantially the buying price of subjects, who merely have the possibility to buy the commodity (see Kahneman et al., (1990) for a very influential study and Ericson and Fuster (2014) for a survey). An example of loss aversion in risky choices is the observation that people reject small-scale gambles that have a positive expected value but may involve losses (e.g., Fehr & Goette, 2007; Rabin, 2000; Tom et al., 2007; see Wakker, (2010), for a comprehensive review of models and empirical relationships).Footnote 1 In this paper, we document loss aversion in a lab-in-the-field experiment with a large non-student sample (660 randomly selected customers of a car manufacturer) in (1) two riskless endowment effect experiments, and in a risky choice task. The riskless task is an endowment effect experiment where we elicit the ‘willingness-to-accept’ (WTA) and/or the ‘willingness-to-purchase’ (WTP). The gap between WTA and WTP has been interpreted as evidence for loss aversion in riskless choice (e.g., Rozin & Royzman, 2001; Tversky & Kahneman, 1991). We do these experiments in two versions: between subjects (Study 1; n = 300) and within subjects (Study 2; n = 360). The elicitation of both valuations from the same individual in Study 2 distinguishes us from previous literature that focused predominantly on aggregate-level measures from between-subject designs. In these experiments (and in our Study 1), different respondents were asked either the WTA or the WTP question. Thus, unlike most of previous literature on the endowment effect, we can compare estimated loss aversion in between- and within-subjects experiments. The within-subjects experiments also allow us to address the importance of individual differences in loss aversion because we can investigate individual, not only aggregate WTA–WTP gaps. We report evidence for WTA/WTP > 1 that is similar in the between- and within-subjects experiments (Result 1). Based on population averages of WTA and WTP, WTA/WTPbetween = 2.00 and WTA/WTPwithin = 1.70. This is our Result 2. Result 3 reports estimates of individual-level loss aversion (\({\uplambda }^{{{\text{riskless}}}}\)) based on an individual’s WTA/WTP ratio. The estimated mean individual \({\uplambda }^{{{\text{riskless}}}}\) = 2.12 and the median is 1.73; 82% of our participants are loss averse. The risky choice task consists of six simple low-stake lotteries with a 50–50 chance of a fixed gain of €6 and losses that vary from €2 to €7. Subjects must indicate for each of the six lotteries whether they want to play this lottery or not (in case they reject a lottery their payoff is zero). This lottery choice task arguably measures loss aversion in risky choices (e.g., following Rabin, 2000; Wakker, 2005). Subjects who reject lotteries with losses €2 to €5 are loss averse, which is the case for 71% of our subjects. The estimated median loss aversion \(\lambda^{{{\text{risky}}}} \, = \,1.33\) (in our benchmark model). This is our Result 4. It suggests that the popular estimate of 2.25 of Tversky and Kahneman (1991) is probably too high. Our within-subjects elicitation of \({\uplambda }^{{{\text{riskless}}}}\) and \({\uplambda }^{{{\text{risky}}}}\) allows for a test of whether they are correlated at individual level. There are strong theoretical considerations why these two measures should be correlated; if probability weighting is independent of utility, loss aversion should operate similarly in riskless and riskless choices. Moreover, neuroscientific analysis have suggested that loss aversion is encoded independently of risk (Tom et al., 2007). However, there are also reasons that might suggest a limited correlation. The idea that preferences are constructed (see, e.g., the collection by Lichtenstein & Slovic, 2006) challenges the hypothesis that we would see significant correlations across different ways of measuring loss aversion. For instance, numerous experiments have demonstrated that preferences differ across response modes. The endowment effect experiment involves two pricing tasks, while the lotteries involve a series of choice tasks. These two response modes are central to the classic demonstrations of preference reversals. It is plausible that they trigger different cues or different weighting schemes for the attributes (Tversky et al., 1988). Consistent with this, Chapman et al. (2017) find no correlation between the endowment effect (for a lottery) and loss aversion in risky choice. Second, research in psychology and experimental economics has shown that different measures of risk taking behavior are often not strongly correlated across different elicitation methods and domains (e.g., Crosetto & Filippin, 2016; He et al., 2018; Johnson & Schkade, 1989; Weber et al., 2002). We find clear evidence for a correlation between \({\uplambda }^{{{\text{riskless}}}}\) and \({\uplambda }^{{{\text{risky}}}}\). The Spearman correlation is 0.677 and significant at any level. \({\uplambda }^{{{\text{risky}}}}\) is lower than the estimated loss aversion \({\uplambda }^{{{\text{riskless}}}}\). This is our Result 5. As a last step, we also examine the robustness of our findings by investigating the relationship between socio-demographic variables and loss aversion. In most studies, the experimental participants are undergraduates who share very similar socio-demographic backgrounds (exceptions are, e.g., Booij & Van De Kuilen, 2009; Dohmen et al., 2011; Kovalchik et al., 2005; Li et al., 2013, 2015). Using undergraduates precludes inferences about how socio-demographic variables affect loss aversion. By contrast, the participants of our experiments are a random sample of 660 customers of a German car manufacturer.Footnote 2 Our subjects comprise a large age, education, income, and wealth spectrum. Of course, car customers may not be representative for the population at large, but we can answer how in our sample socio-demographic variables affect loss aversion both in riskless and in risky choices. In Result 6, we show that the socio-demographic variables affect both measures of loss aversion similarly. Loss aversion is positively correlated with age, wealth and income, and negatively correlated with education. We find no significant gender differences in loss aversion. Our data and results provide novel input into a debate about whether loss aversion is a real phenomenon (Ert & Erev, 2013; Gal, 2006; Gal & Rucker, 2018; Plott & Zeiler, 2005; Yechiam, 2019), which some even called a “fallacy” (Gal, 2018). We will address some of the criticisms in our discussion section. We also refer the reader to our companion paper (Mrkva et al., 2020), which uses the within-subject data of this paper plus a host of new survey experiments intended to measure moderators of loss aversion. Taken together, this paper, and Mrkva et al. (2020), provide strong evidence for the empirical relevance of loss aversion, while acknowledging that loss aversion has important moderators.",31
92.0,3.0,Theory and Decision,24 June 2021,https://link.springer.com/article/10.1007/s11238-021-09817-0,No-betting Pareto under ambiguity,April 2022,Itzhak Gilboa,Larry Samuelson,,Male,Male,Unknown,Male,,1
92.0,3.0,Theory and Decision,21 March 2022,https://link.springer.com/article/10.1007/s11238-022-09873-0,Composition rules in original and cumulative prospect theory,April 2022,Richard Gonzalez,George Wu,,Male,Male,Unknown,Male,"The St. Petersburg’s Paradox challenged the notion that decision makers should choose the risky alternative that maximizes expected value (Bernoulli, 1738). In the nearly four centuries since Bernoulli, expected value as a descriptive theory of decision-making under risk has advanced in several important respects. A family of modern descriptive theories of decision under risk depart from expected value maximization in three essential ways: the transformation of outcomes, the transformation of probabilities, and the composition rule that combines the two transformations. Prospect theory (Kahneman and Tversky 1979; Tversky and Kahneman 1992) has emerged as the frontrunner of these descriptive theories (see reviews of empirical evidence in Barberis (2013); Camerer (1992, 1995); Dhami (2016); Fox et al. (2015); Starmer (2000); Wu et al. (2004); for alternative viewpoints, see Birnbaum 2004, 2006, and Birnbaum 2008). In prospect theory, outcomes are transformed by an S-shaped value function, typically concave for gains, convex for losses, and steeper for losses than gains, whereas probabilities are transformed by an inverse S-shaped probability weighting function, most commonly concave for small probabilities and convex for medium and large probabilities (see also Edwards 1954; Preston and Baratta 1948). A substantial empirical and theoretical literature has generally supported these two aspects of prospect theory. In contrast, the third piece, the composition rule, has received relatively little direct empirical attention (a review of relevant work appears later in this paper). Moreover, the choice of the composition rule is not trivial: original prospect theory (OPT; Kahneman and Tversky 1979) and its descendent, cumulative prospect theory (CPT; Tversky and Kahneman 1992) differ in how these two transformation functions are combined, and, consequently, in how gambles with three or more outcomes are valued. This paper investigates the empirical merits and shortcomings of the two representations. We perform a test that exploits the following observation: CPT and OPT coincide for two-outcome gambles but differ for gambles with three or more outcomes. This linkage allows us to perform a novel test—for each of our experimental participants, we estimate prospect theory parameters for two-outcome gambles, a domain where the two models concur. We then apply these estimates to three-outcome gambles, a domain where the models diverge. As a result, we can test whether CPT and/or OPT fit three-outcome gamble data well and document the nature of systematic discrepancies between actual and predicted cash equivalents. The paper proceeds as follows. We first present the two models and review previous empirical research. We then review one of our previous studies (Gonzalez and Wu 1999) in which we estimate the probability weighting function on two-outcome gambles (where the two models coincide). We supplement this analysis with previously unreported data from 37 participants. We then use three-outcome gambles as a holdout sample to test how the two models perform. At the aggregate level, both models perform well, but are biased in a predictable manner: CPT slightly under-predicts and OPT slightly over-predicts the cash equivalents of three-outcome gambles. This pattern, however, masks the considerable heterogeneity across types of gambles and across participants. A large part of this heterogeneity can be linked to differences in the curvature and elevation of the probability weighting function across individuals. The two models also perform differently depending on whether the worst outcome is zero or non-zero. Finally, we conclude with thoughts on how this analysis offers new insights about psychological processes underlying choice among complex gambles, as well as recommendations on which prospect theory to use for applications.",1
92.0,3.0,Theory and Decision,08 June 2021,https://link.springer.com/article/10.1007/s11238-021-09829-w,Incomplete risk attitudes and random choice behavior: an elicitation mechanism,April 2022,Edi Karni,,,Male,Unknown,Unknown,Male,"Decision makers are sometimes confronted with the need to choose among alternatives that, because of their complexity or novelty, make them impossible to compare. von Neumann and Morgenstern, recognized this possibility, admitting that “it is conceivable—and even in a way more realistic—to allow for cases where the individual is neither able to state which of two alternatives he prefers nor that they are equally desirable” (von Neumann and Morgenstern 1947). Aumann questioned not only the descriptive validity of the completeness axiom but also its normative justification. “ Of all the axioms of utility theory,” he wrote, “ the completeness axiom is perhaps the most questionable. Like others of the axioms, it is inaccurate as a description of real life; but unlike them, we find it hard to accept even from the normative viewpoint” (Aumann 1962).Footnote 1 In the context of decision making under risk, the source of incompleteness is the decision maker’s risk attitudes. Specifically, because of either the complexity of the alternatives or the ambiguity about her own attitudes, the decision maker lacks a clear sense of how to assess the risks associated with the prospects in the choice set. The representations of incomplete preferences under risk, dubbed expected multi-utility representations, were characterized by Shapley and Baucells (1998), Dubra et al. (2004) and Galaabaatar and Karni (2013). In all these instances, risky prospects are represented by probability measures on the set of outcomes and the representation involves a set, \(\mathcal {U}\), of utility functions. Specifically, one risky prospect is preferred over another if and only if the expected utility of the former exceeds that of the latter according to each and every function in the set \(\mathcal {U}\). When two risky prospects are noncomparable, one has higher expected utility according to some elements of \(\mathcal {U}\) and lower expected utility according to other elements. In such cases, the aforementioned models do not yield predictable choice behavior. Karni (2021) proposed a model of stochastic choice behavior that is attributable to preference incompleteness. Applied to the case of incomplete preferences under risk, this model advances the proposition that decision makers are characterized by the set of utility functions \(\mathcal {U}\) and a personal probability measure \(\lambda ,\) on \(\mathcal {U}\). When facing a choice among noncomparable risky prospects, decision makers behave as if a function in the set \(\mathcal {U\,}\) is drawn randomly, according to \(\lambda \), and the prospect that attains the highest expected utility according to that function is chosen. The random choice model is tested by the accuracy of its predictions. Making predictions based on the random choice model requires the elicitation of the range of incompleteness represented by \(\mathcal {U}\) and \(\lambda .\) These predictions may then be compared to observations of actual choice behavior in an experimental setting. This paper addresses these requirements. The main novelty is the introduction of an incentive-compatible elicitation scheme by which the range of a decision maker’s incomplete risk attitudes and personal perception of the measure \(\lambda \) are identified and on the basis of which predictions of the model are quantified. As far as I know, this is the first incentive-compatible scheme designed to elicit decision makers’s beliefs about the likely realizations of their own risk attitudes. In this sense, this work complements the elicitation mechanism of decision makers’ second-order beliefs proposed by Karni (2020). In addition, this paper outlines experimental designs by which observations of actual choice behavior in the presence of incomplete risk attitudes are generated. Recent years witnessed increasing interest in modeling stochastic choice behavior.Footnote 2 With few exceptions, however, these studies do not attribute this behavior specifically to preference incompleteness. One exception is Ok and Tserenjigmidz (2020), who introduced random choice functions, which they define and characterize for stochastic choices induced by indifference, indecisiveness, and experimentation. Their axiomatic characterization of stochastic choice functions induced by lack of strict preference asserts merely that the maximal elements of the menu will be chosen with positive probability. Karni (2021) complements this work by proposing a random choice model that predicts the likelihoods of the choices of the alternatives in the maximal sets. Karni and Safra (2016) study stochastic choice under risk and under uncertainty based on the notion that randomly selected states of mind determine decision makers’ actual choices. They provide axiomatic characterization of the representation of decision makers’ perceptions of the stochastic process underlying the selection of their state of mind. In the context of decision making under risk with incomplete preferences, the states of mind are depicted by the utility functions in \(\mathcal {U}\). Hence, the work of Karni and Safra may be regarded as providing axiomatic foundations of representation of a decision maker’s perception of the probability distribution on \(\mathcal {U}\) by the probability measure \(\lambda \) and the hypothesis that the probability of choosing a risky prospect P out of the set of risky prospects \(\{P,Q\}\,\ \)is the value assigned by \(\lambda \) to the subset of functions in \(\mathcal {U}\ \)whose expected utility of P exceeds that of Q.",
92.0,3.0,Theory and Decision,18 February 2022,https://link.springer.com/article/10.1007/s11238-022-09877-w,On the cardinal utility equivalence of biseparable preferences,April 2022,Fabio Maccheroni,Massimo Marinacci,Jingni Yang,Male,Male,Unknown,Male,"Ghirardato and Marinacci (2002) argued that when comparing the ambiguity attitudes of two decision makers (DMs) with biseparable preferences, it is important to keep the cardinal utilities equivalent to factor out risk attitudes that, otherwise, would intrude in the comparison and confound it. With this motivation, they establish the desired cardinal utility equivalence via a trade-off condition. In this work, we present a simpler condition, based on the willingness to bet on events, that ensures this equivalence (Theorem 6). We consider a state space S, finite or infinite, endowed with an event algebra \(\Sigma\), and a connected topological space X of consequences. We denote by \(\mathcal {F}\) the collection of (simple) acts, i.e., of the \(\Sigma\)-measurable maps from S to X that assume finitely many values.Footnote 1 The DM’s preference on \(\mathcal {F}\) is modeled by a binary relation \(\succsim\), with symmetric part \(\sim\) and asymmetric part \(\succ\). As usual, consequences are identified with constant acts and so, with a standard abuse of notation, \(\succsim\) ranks also consequences. In particular, throughout, we assume that the preference \(\succsim\) is non-trivial, i.e., there exist two consequences x and y with \(x\succ y\). Given any two consequences x and y, with \(x\succsim y\), we denote by xAy the binary act that yields x if event A occurs and y otherwise, interpreted as a bet on event A. An event A is essential if there exist two consequences \(x\succsim y\) such that A functional \(V:\mathcal {F}\rightarrow \mathbb {R}\) represents \(\succsim\) if, for all \(f,g\in \mathcal {F}\), The preference \(\succsim\) is monotone if, given any two acts f and g, \(f(s)\succsim g(s)\) for all s in S implies \(f\succsim g\). A functional V represents a monotone preference \(\succsim\) if and only if it is itself monotone: given any two acts f and g, \(f(s)\succsim g(s)\) for all s in S implies \(V\left( f\right) \ge V\left( g\right)\).",
92.0,3.0,Theory and Decision,26 July 2021,https://link.springer.com/article/10.1007/s11238-021-09837-w,Risk aversion for losses and the Nash bargaining solution,April 2022,Hans Peters,,,Male,Unknown,Unknown,Male,"In this paper, we propose a variation on the concepts of loss aversion and of risk aversion, called risk aversion for losses. A decision maker is risk averse for losses if this decision maker downgrades payoffs below a given reference level by a nondecreasing concave transformation. We first formulate and apply this concept in the context of the two-person Nash bargaining model (Nash, 1950), as follows. Given a bargaining solution, assume that the bargainers regard the assigned payoffs as their reference levels. If they are risk averse for losses, then the problem (feasible set) is corrected by applying the associated concave transformations. We call a bargaining solution invariant under risk aversion for losses if after this correction the assigned outcome does not change, provided that the disagreement point only changes proportionally, i.e., the new, corrected, disagreement point is still on the straight line through the original disagreement point and the bargaining solution outcome. This last restriction is reasonable, and without it the axiom would be overly demanding. Invariance under risk aversion for losses is satisfied by many well-known bargaining solutions, including the Nash bargaining solution and the Kalai–Smorodinsky solution (Kalai & Smorodinsky, 1975). We next concentrate on the Nash bargaining solution and provide an axiomatic characterization with invariance under risk aversion for losses as one of the axioms. The other axioms are Pareto optimality, symmetry, covariance, and expansion independence. This last condition requires the solution not to change if we add only payoff pairs exceeding the utopia payoff (i.e., maximally possible payoff) of one of the bargainers. We also show that the axioms are logically independent. We finally present a decision-theoretic characterization of the new concept of risk aversion for losses, closely related to Yaari’s (1969) characterization of comparative risk aversion. Our results are closely related to many other results in the literature, both on bargaining and on risk and loss aversion, but we postpone discussion of this literature until the relevant parts of the paper. Section 2 introduces the Nash bargaining model, and Sect. 3 the concept of risk aversion for losses within this model. Sect. 4 presents the characterization of the Nash bargaining solution based on this concept. In Sect. 5, we provide the decision-theoretic characterization of risk aversion for losses.",1
92.0,3.0,Theory and Decision,28 February 2022,https://link.springer.com/article/10.1007/s11238-022-09875-y,Production under uncertainty and choice under uncertainty in the emergence of generalized expected utility theory,April 2022,John Quiggin,,,Male,Unknown,Unknown,Male,"Over the course of the 1970s, the expected-utility hypothesis came under increasing attack. The long-neglected and much-misinterpreted criticisms of Allais (1953) were reinforced by new empirical evidence, which demonstrated the robustness of the ‘Allais paradox’. Other long-standing problems, such as the co-existence of insurance and lottery gambling (Friedman and Savage 1948) received renewed attention. Attempts to elicit utility functions using the assumption of preferences ‘linear in the probabilities’ produced systematic violations of the consistency assumptions of expected utility theory. A number of attempts were made to develop generalizations of expected utility theory that could account for the Allais paradox and related phenomena such as the common ratio effect. One approach which fell into the category of ‘fruitful error’ was that of Handa (1977), who proposed a model based on the idea of probability weighting. Unfortunately, Handa’s model implied violation of statewise dominance, that is, the requirement that if one prospect yields a better outcome than another in every state of the world, it must be preferred. However, Handa’s proposed axioms include the weaker property of preservation of first-order stochastic dominance.Footnote 1 The Journal of Political Economy (JPE) was deluged with comments pointing out this error, of which the most elegant, and the only one published, was that of Fishburn (1978). Other contributions came from Mark Machina, then a graduate student at MIT, and from me, then a civil servant with the Bureau of Agricultural Economics (BAE) in Canberra, Australia, and working on an Honours thesis focusing on the Sandmo model. Shortly afterwards, the classic paper of Kahneman and Tversky (1979) introduced the world to ‘prospect theory’ which became the most widely used alternative to expected utility theory. The key idea was that of a reference point, along prospects to be separated into gains and losses, with the idea that risk aversion would prevail for gains, and risk seeking for losses. Along with this and other changes to the expected utility model, Kahneman and Tversky proposed a model of probability weighting broadly similar to that of Handa. The problem of first-order stochastic dominance was dealt with by the inclusion of an ‘editing phase’ in which dominated prospects are eliminated from choice sets. It was thinking about how to correct the Handa model that led me to develop rank-dependent utility, which I initially described as ‘anticipated utility’ (AU).Footnote 2 The crucial idea was that the weight associated with an outcome should depend on its rank in the distribution as well as on its probability. As was observed in Quiggin (1982a): The following example illustrates further the notion that equally probable events should not necessarily receive the same weight. Suppose an individual’s normal wage income is uniformly distributed over a range from $20,000.01 to $21,000.00. There is also a 1/100,000 chance that the person will win a contest for which the prize is a job paying $1m a year. The probability of receiving any specified income in the relevant range $20,000.01 to $21,000.00 (e.g. $20,439.72) is also 1/100,000. Nevertheless, it seems reasonable that the extreme outcome will not be weighted in the same way as an intermediate outcome such as $21,439.72. The idea that extreme outcomes might be overweighted led to the anticipated utility (AU) or rank-dependent expected utility (RDU) model. Using the notation of Quiggin and Wakker (1994), the model may be described as follows. Consider a set of outcomes X and a set Y of prospects, consisting of all probability distributions over X with finite support. A typical element of Y is a pair \(\left\{ \left( x_{1},x_{2}\ldots x_{n}\right) ;\left( p_{1},p_{2}\ldots p_{n}\right) \right\} =\ \ \left\{ {\mathbf {x}};\mathbf {p }\right\}\) yielding outcome \(x_{j}\) with probability \(p_{j}.\)By \(\succeq\) denote a binary preference relation over Y, with associated strict preference \(\succ\) and indifference \(^{\sim }\) and use \(\succeq\) also to represent the induced ordering on X. The crucial innovation was to focus attention on rank-ordered prospects, in which \(x_{1}\le x_{2}\le \cdots \le x_{n}\). RDU holds if the relation \(\succeq\) can be represented by a function \(V:Y\rightarrow {\mathbb {R}}\) of the form for a utility function \(U:X\rightarrow {\mathbb {R}}\) and a non-decreasing transformation function \(f:[0,1]\rightarrow [0,1]\) with \(f(0)=0\), \(f(1)=1\). Note that, with the convention \(\sum \nolimits _{j=1}^{0}p_{j}\) so that the decision weights \(w_{i}=\) \(f\left( \sum \nolimits _{j=1}^{i}p_{j}\right) -f\left( \sum \nolimits _{j=1}^{i-1}p_{j}\right)\) sum to 1. With this representation, AU is the special case where \(f\left( \frac{1}{2} \right) =\frac{1}{2}\). The functional form associated with the rank-dependent model was independently rediscovered on several separate occasions, with quite different motivating arguments. Examples include Allais (1987), Yaari (1987) and, in the context of social choice, Weymark (1981) and Ebert (1988). Thus, the model represents an interesting example of the theory of multiple discoveries, put forward by Merton (1973). Tversky and Kahneman (1992) modified the original prospect theory to include rank-dependent probability, yielding cumulative prospect theory. Cumulative prospect theory was axiomatised by Wakker and Tversky (1993). Kahneman received the 2002 Nobel Memorial Prize in Economic Sciences for this work, and Tversky a rare posthumous mention. Wakker (2010) provides the best guide to the literature. The rank-dependent model was successful in resolving many of the concerns raised by critics of Expected Utility Theory. Quiggin (1982a, 1982b) showed how AU could resolve the paradoxes noted by Friedman and Savage (1948) and Allais (1953). Wakker and Deneffe (1996) show how to take rank-dependent probability weighting into account when eliciting utility functions over outcomes. Quiggin (1979, 1982b) proposed an axiomatic basis for anticipated utility, in which the standard independence axiom was restricted to apply only to 50–50 mixtures between rank-ordered prospects. This set of axioms implied the restriction \(f(1/2)=1/2\), dropped in subsequent formulations of RDU. Even with this restriction, the derivation of the AU model from the axioms was incorrect. A more accurate statement is given by Quiggin and Wakker (1994).Footnote 3 Quiggin and Wakker (1994) propose the following structural restrictions, which are a modified version of those in Quiggin (1982b). Axiom 1: (Completeness): The binary relation \(\succeq\) is a complete weak order Axiom 2: (Dominance) 2\(^{\prime }\)a: If \(p^{\prime }\ge p,\) then \(\left\{ \left( x_{1},x_{2}\right) ;\left( 1-p^{\prime },p^{\prime }\right) \right\} \succeq\) \(\left\{ \left( x_{1},x_{2}\right) ;\left( 1-p,p\right) \right\}\) 2\(^{\prime }\)b: \(\left\{ \left( x_{1}^{\prime },x_{2}^{\prime }\right) ;\left( \frac{1}{2},\frac{1}{2}\right) \right\} \succeq\) \(\left\{ \left( x_{1},x_{2}\right) ;\left( \frac{1}{2},\frac{1}{2}\right) \right\}\) whenever \(x_{2}^{\prime }\succeq x_{2},x_{1}^{\prime }\succeq x_{1}\) where the former preference is strict if one of the latter two is strict. Axiom 3: (Continuity) If \(x_{1}\preceq x_{2}\preceq x_{3}\exists X\), then there exists \(p^{*}\) such that \(x_{2}^{\sim }\left\{ \left( x_{1},x_{3}\right) ;\left( 1-p^{*},p^{*}\right) \right\}\). Axiom 4: (Independence) Whenever \(x^{\sim }\left\{ {\mathbf {x}};{\mathbf {p}} \right\} ,x^{\prime \sim }\left\{ {\mathbf {x}}^{\prime };{\mathbf {p}} ^{\prime }\right\} ,\) and \(c_{i}^{\sim }\left\{ \left( x_{i},x_{i}^{\prime }\right) ;\left( \frac{1}{2},\frac{1}{2}\right) \right\}\) for all i,  then \(\left\{ {\mathbf {c}};{\mathbf {p}}\right\} ^{\sim } \left\{ \left( x,x^{\prime }\right) ;\left( \frac{1}{2},\frac{1}{2}\right) \right\}\)Footnote 4 Axioms 2\(^{'}\) a and 2\(^{'}\) b replace a weaker axiom proposed by Quiggin (1982a), which is insufficient to ensure that f is monotonic, or that it is bounded above by 1. Axiom 4 is the modification of the von Neumann–Morgenstern independence axiom proposed by Quiggin (1982a). Axioms for the general case of RDU are given by Quiggin and Wakker (1994). The literature on generalized expected utility arose mainly out of debates about the von Neumann–Morgenstern model of expected utility, in which the objects of preference were probability distributions. In particular, the counterexamples proposed by Allais, and the violations of EU predictions elicited by Kahneman and Tversky were all developed in this context. The von Neumann–Morgenstern framework took probabilities as primitive. In contrast, Savage (1954) began with preferences over acts, represented as mappings from a set of states of nature (presumed to be exhaustive and mutually exclusive) to a set of outcomes. Savage presented axioms which simultaneously implied the existence of both well-defined subjective probabilities for states and a utility function (unique up to an affine transformation) over outcomes, such that the preferred element of any set of acts was that which maximized expected utility. Although the RDU model dealt with preferences over probability distributions, the weight placed on any given state depended on the ranking of the outcome associated with that state under a given act. Particularly in cases of comonotonic acts, where the ranking of states is the same for both acts, the effect is to pay more attention to the state-act pair associated with a given probability distribution. Diecidue and Wakker (2001) use the idea of comonotonicity as an intuitive basis for RDU. The Choquet Expected Utility model developed by Schmeidler (1989) extended the idea of rank dependence to the Savage model. Wakker (1990) showed that the Choquet expected utility model of is equivalent to RDU if and only if preferences are probabilistically sophisticated in the sense of Machina and Schmeidler (1992), that is, consistent with first-order stochastic dominance for some probability distribution.",
92.0,3.0,Theory and Decision,25 February 2022,https://link.springer.com/article/10.1007/s11238-022-09874-z,How we decide shapes what we choose: decision modes track consumer decisions that help decarbonize electricity generation,April 2022,Crystal Reeck,Karoline Gamma,Elke U. Weber,Female,Female,Female,Female,"Theory development across disciplines, say psychology and economics, is no easy task. It requires some training and competence in both disciplines as well as willingness and ability to cross barriers of disciplinary language and frameworks to combine valuable insights in an effort to better describe and predict empirical realities. Behavioral economics was designed to enrich and extend economic theory and models with psychological constructs and insights, but we can count the number of economists who seriously engage with psychology in such efforts on two hands. Peter Wakker is a pioneer and leader in this small group. It is very fitting to dedicate this special issue of Theory and Decision to his work, as these two topics (“theory” and “decision”) have been central to his efforts to put the psychological analysis of decision processes on a sound axiomatic measurement basis that allows for a concise understanding of where and how the decisions of homo sapiens differ from those of homo economicus. Of the three modes of decision-making analyzed in this paper and described in greater detail below, Peter has focused on calculation-based decision processes, axiomatizing rank-dependent utility maximization and analyzing phenomena like risk-aversion, loss-aversion, and regret. However, in the process of doing so, his theoretical extensions of rational choice processes make contact with both emotion-based and role- and rule-based decision processes, the two decisions modes contrasted to calculation-based decision processes in this paper. In December 2015, leading politicians and environmental policy makers from around the world met in Paris to discuss how to battle climate change and craft an agreement to limit global temperature increase to below 2 °C above pre-industrial levels (UNFCCC, 2017). Several countries have set ambitious goals to fulfill their commitment to this agreement and reduce their carbon emissions. For example, Sweden aims to become the first fossil-fuel-free nation by 2045 (Sweden, 2017). In the United States, more than 50% of businesses have increased their commitment to renewable energy (Deloitte, 2019). Achieving the global goal of limiting temperature rise will require action not only by governments and industries: consumers will also need to modify their energy consumption behavior (Grubler et al., 2018). Fortunately, the majority of consumers view themselves as environmentalists (Mackoy et al., 1995), suggesting a broad willingness to embrace pro-environmental options. Indeed, there is support across the spectrum of political ideology for environmentally-friendly electricity (Sunstein and Reisch, 2014). Surveys show that 88% of customers in the United States and Europe want brands to help them make more environmentally-friendly decisions (Townsend, 2018). Policy makers as well as marketers need to understand what influences consumers’ decisions to promote environmentally-friendly choices and curb global temperature rise. In the United States, electricity generation is the largest single source of carbon emissions, with residential energy use accounting for 19% of all emissions from fossil fuel combustion in 2017 (Agency, 2018). Given the primary contribution of electricity generation to emissions, the present research focused on consumer decisions regarding electrical energy plans. Prior research has demonstrated that people are more likely to adopt environmentally-friendly energy plans when they are set as the default or status quo (Ebeling & Lotz, 2015). Additionally, emphasizing environmental or health benefits of energy conservation effectively promoted energy efficiency, leading to greater conservation than other appeals that emphasized financial savings (Asensio & Delmas, 2015, 2016). Appeals to decrease energy usage have also been shown to be more effective when they highlight how one’s energy use compares with one’s neighbors, appealing to social norms to encourage conservation (Allcott, 2011). While such findings are helpful for suggesting interventions that can be used to promote environmentally-friendly behaviors, an organizing framework that spans different interventions has been elusive. The present research examines how the ways in which consumers consider options influence their choices. By shifting the focus to how consumers make decisions, we offer insight into the psychological precursors associated with environmentally-friendly decisions. Towards that end, we examine decision modes—qualitatively different ways in which people approach decisions. Prior research has established a taxonomy of three different decision modes: Calculation Modes, Affect Modes, and Role Modes (Ames et al., 2004; Weber & Lindemann, 2007; Weber et al., 2005). These decision modes can operate in parallel during decision-making (Krosch et al., 2012; Weber & Lindemann, 2007) and may have ramifications for choice. Existing research supports the notion that different decision modes may lead to different choices. Calculation Modes promote a focus on economic exchange which may reduce the likelihood of acting to promote others’ best interests (Clark & Mills, 1979; Clark et al., 1987). As environmentally-friendly options typically come at a personal cost to generate benefit towards others or society broadly, this implies that use of Calculation Modes will reduce the likelihood of environmentally-friendly choices. Affect Modes promote the use of feelings in decisions, which may promote environmentally-friendly choices, given the potential for a “warm glow” to promote prosocial decision-making (Andreoni, 1990). Role Modes typically involve making a decision on the basis of one’s identity or role with respect to others. Given the rich literature on the relationship between social identity considerations and conservation (Goldstein et al., 2008; Griskevicius et al., 2010; Van Vugt, 2001), Role Modes seem likely to promote environmentally-friendly choices by heightening awareness of social norms and enhancing consideration of one’s actions on the broader community. The present research tests these propositions in a series of six studies. The present research makes four important conceptual contributions. First, it extends the decision modes’ framework to consumer decisions. Prior research has not characterized how and whether people use different decision modes when facing consumer decisions. Second, this is the first work to tie general decision mode use directly to choice. While previous work characterized how different types of decisions might prompt people to use different decision modes (Weber & Lindemann, 2007), it stopped short of connecting decision mode use with choices. Here, we examine the effect of decision mode use on consumer choices. Third, we establish a causal link between decision mode use and choices by directly manipulating decision mode usage. Fourth, the present research has implications for marketing managers and policy makers seeking to understand how to promote environmentally-friendly decisions and combat global temperature rise. Consumers make decisions using qualitatively different psychological processes, referred to as decision modes. Decisions can be reached in a highly diverse set of ways, but efforts to create a taxonomy of decision modes have identified three general classes: Calculation Modes, Affect Modes, and Role Modes (Ames et al., 2004; Weber & Lindemann, 2007; Weber et al., 2005). Calculation Modes utilize detailed evaluations and trade-offs between advantages and disadvantages, probabilities, and time delays of each course of action to calculate the best (or at least a good) decision. The quantitative optimization models of rational choice theory, such as expected utility theory (von Neumann & Morgenstern, 1944), are a prime example of Calculation Modes, but psychological models that grew out of the realization of bounded rationality, such as prospect theory (Kahneman & Tversky, 1979; Tversky & Kahneman, 1992; Wakker, 2010) and query theory (Johnson et al., 2007; Weber et al., 2007), also belong to the same family. Other variants of Calculation Modes were characterized by the adaptive decision-making framework (Payne et al., 1988), which highlights how decision-makers may tailor their option evaluation and information integration approach to the costs and benefits of expending more or less effort and cognitive capacity on a decision. In general, detailed use of a Calculation Mode can result in increased demands on cognitive resources. While Calculation Modes may incorporate affective evaluations of different choice alternatives (such as reflections on how an option may make one feel in the future), the momentary and often incidental emotional experience of the choice process plays no role. Conversely, Affect Modes utilize feelings experienced at the time of choice to guide decisions (Weber & Lindemann, 2007; Weber et al., 2005). For example, a consumer may experience excitement or fear when faced with a risky option, and that immediate emotional response may guide their choice (Damasio, 1993; Finucane et al., 2000; Loewenstein et al., 2001). Reliance on affect during decision-making can also make people less sensitive to magnitude changes (Hsee & Rottenstreich, 2004). Different emotions will evoke different cognitive or behavioral tendencies (Han et al., 2007; Lerner & Keltner, 2001; Lerner et al., 2004), and thus color the decision-making process. Fight or flight responses (Cannon, 1929) belong to the set of Affect Modes, as does impulsive shopping (Hausman, 2000). Affect Modes are inextricably linked to the emotional state that the consumer is experiencing at the time of choice, and therefore distinct from other classes of decision modes. They may also be relatively automatic, or can be consciously engaged. Finally, Role Modes utilize recognition of the decision as a member of a category for which a decision rule exists. This recognition may be based on expertise in a particular domain, say chess (Chase & Simon, 1973; Simon, 1990), in which case the expert decision-maker retrieves which response is necessitated by the situation. Alternately, recognition can be based on a social role, say that of a doctor, in which case the professional decision-maker retrieves and executes the response that is associated with his or her role, e.g., providing medical care to anyone in need, as specified by the Hippocratic Oath (March, 1994). Other Role Modes include the use of a moral code of conduct, or the application of standard operating procedures in work settings. Aspects of role mode use are also apparent in the Theory of Planned Behavior (Ajzen, 1991), as it incorporates considerations related to normative beliefs. Role Modes can be automatically engaged and thus involve a few cognitive resources, or they can be consciously engaged and require more cognitive resources. To summarize, the three classes of decision modes involve qualitatively different choice processes. Calculation Modes involve evaluation, comparison, and integration, of outcome dimensions of different choice alternatives; Affect Modes register and react to the decision-maker’s emotional response to the choice situation; Role Modes evaluate the choice situation for category information associated with previously learned decision rules that are then executed. Previous theorizing has examined the genesis of different decision modes and characterized the variability in which they are applied across individuals and contexts. Weber (1998) hypothesized that the three classes of decision modes co-exist, because they allow the decision-maker to satisfy different needs or goals, with Calculation Modes satisfying material needs, Affect Modes satisfying the need for autonomy, and Role Modes satisfying social needs. This functional perspective also provides predictions for expected variation in decision mode usage across individuals and cultures. For example, since the use of a Role Mode affirms one’s social identity, one would expect its use to be more frequent in cultures that value affiliation and social identity and in choice domains where social identity is seen as more central. Evidence supporting these hypotheses was provided by an analysis of the frequency by which the decision modes were described as being employed by characters making decisions in a broad range of content domains in American versus Chinese novels (Weber et al., 2005). There is additional evidence that people’s reported use of decision modes in real-world decisions varies across different decision contexts (Weber & Lindemann, 2007; Weber et al., 2005). Prior research has demonstrated that different decision modes are more likely to be used in different contexts. Given that the three modes utilize quite different features of the choice situation, this divergence of mode use is intuitively appealing. For example, when making financial decisions such as choosing an investment option, people are more likely to adopt a Calculation Mode, in which they explicitly trade off costs against benefits and select the option that maximizes the personal benefit (Weber & Lindemann, 2007). In contrast, when making more hedonic decisions such as selecting a dish at a restaurant, people are more likely to adopt an Affect Mode, focusing on their immediate emotional responses to the options available and choosing accordingly. When deciding on a matter related to their personal identity, such as religious or political matters, people are more likely to adopt a Role Mode, reflecting on what someone in their role should do under the circumstances or has habitually done in the past, and often choosing options that go against their personal self-interest or comfort. However, while prior work has demonstrated that different decision modes are used in different contexts, it stopped short of showing that use of different decision modes influenced choices. In many consumer choices as in other decision contexts, multiple decision modes likely operate in parallel to guide people’s considerations. This parallel processing is likely especially true of decisions about green consumer products or energy products or services, which may trigger all three of the decision modes, by virtue of involving quantitative trade-offs, strong emotional reactions, and issues related to social identities (Litvine & Wustenhagen, 2011; Ozaki, 2011; Wustenhagen et al., 2007). In such circumstances, the relative influence of each decision mode on choice remains uncharacterized. A key question thus arises: does it matter by which mode a decision is made? It is certainly plausible that the decision mode employed influences which choice option is selected. There have been some empirical demonstrations that using conflicting decision modes while making an ethical decision leads to greater post-decisional worry and regret and that specific moral decision mode use shapes such ethical decisions (Krosch et al., 2012). However, prior research has stopped short of linking decision mode use to consumer choice. There are reasons to expect that the use of different decision modes would trigger different considerations when choosing between options that can lead to decisions that are more or less environmentally-friendly. Importantly, decision modes may provide a cohesive framework that can account for disparate findings regarding how to promote environmentally-friendly decisions. Calculation Modes promote a focus on economic exchange as the basis for making one’s selection. Exchange-based approaches in other domains typically lead to less generous behavior and reduced likelihood of acting to promote others’ best interests (Clark & Mills, 1979; Clark et al., 1987), suggesting that Calculation Modes may therefore diminish environmentally-friendly choices, since such options typically come at a personal cost to generate benefit towards others or society more generally. Indeed, other research has shown that economic appeals to embrace environmentally-friendly options have been less successful than appeals that highlight environmental or health benefits (Asensio & Delmas, 2015; Ungemach et al., 2018). Affect Modes may promote environmentally-friendly choices, given the potential for a “warm glow” to promote prosocial decision-making (Andreoni, 1990). Positive emotions are often associated with helping others, including future generations, and have been associated with selecting environmentally-friendly alternatives (Schneider et al., 2017; Zaval et al., 2015). It is possible, therefore, that emotions arising at the time of choice may guide consumers using Affect decision modes to embrace options that minimize harm to the natural environment. Role Modes prompt consideration of one’s social identity and rules for guiding one’s behavior. Prior research has shown that information about others’ consumption can promote conservation (Allcott, 2011). Defaults have also been shown to evoke consideration of social norms (McKenzie et al., 2006), and people are more likely to adopt an environmentally-friendly energy plan when it is the default (Ebeling & Lotz, 2015). Appeals to social identity generally encourage conservation (Goldstein et al., 2008; Griskevicius et al., 2010; Van Vugt, 2001), consistent with the notion that Role Modes may promote environmentally-friendly choices. The present paper examined the relative use of different decision modes when making decisions between different energy plans offered by electric utility companies and how reliance on different decision modes shapes such consumer choice. This research pursued three key aims. First, we sought to characterize the relative use of each of the three classes of decision modes when consumers were faced with decisions between residential electricity plans that differed in their environmental impact and other attributes. Policy makers and marketing managers from utilities implicitly assume that consumers rationally weigh costs against benefits (a Calculation Mode) as they choose whether to adopt an offered environmentally-beneficial energy plan. But instead, consumers may also use Affect or Role Modes, which may lead to different energy choices. Determining the relative use of each of three decision modes would provide key insight into the psychological processes typically underlying these consumer decisions. Second, we sought to establish the relationship between use of each of the three types of decision modes and environmentally-friendly choices. Documenting links between specific mode use and environmentally-friendly decisions enhances understanding of the precursors to pro-environmental behaviors and suggests a potential means to influence decisions. Our final aim was therefore to establish a causal link between the use of specific decision modes and environmentally-friendly choice. Such a connection between decision mode use and pro-environmental choice would provide a means for policy makers and energy market managers to impinge upon consumer decision-making and promote environmental conservation. As electricity generation is a major contributor to greenhouse gas emissions (Agency, 2018), the present research focused on consumer decisions regarding electrical energy plans. Energy plans can be environmentally-friendly either because they offer electricity generated by non-carbon renewable sources and thus do not contribute to greenhouse gas emissions (Batley et al., 2001; Litvine & Wustenhagen, 2011) or because they promote less energy use during peak consumption hours and thus reduce grid peak load and the need to construct additional power stations (Newsham & Bowker, 2010). The present research examines decisions about the adoption of both types of environmentally-friendly electricity plans. Electricity plans featuring renewable sources of energy are common and available to consumers in most Western countries (MacDonald & Eyre, 2018). Energy plans that aim to control grid load through pricing that varies based on customer demand have gained importance in recent years, as fluctuating energy supply from renewables and increasing peak demand have increased the need for more flexible and adaptive energy demand from consumers (Newsham & Bowker, 2010; Roscoe & Ault, 2010; Wolsink, 2012). Different forms of peak pricing have shown to be effective in this regard (Newsham & Bowker, 2010). The decision between energy plans that offer electricity generated by renewable energy only versus plans that offer electricity generated mainly by other sources that increase greenhouse gas emissions involves a trade-off between an environmentally-friendly option which usually comes at higher cost versus a cheaper option with a larger carbon footprint (Litvine & Wustenhagen, 2011; MacDonald & Eyre, 2018). In the peak scenario choice, the choice is between a time of use plan that entails higher effort or reduced comfort during peak service hours as consumers are only able to use a limited number of electric appliances at a given time for which they received some financial benefit (Newsham & Bowker, 2010) and a standard services plan which does not involve any restrictions on use of appliances but comes either with an additional service fee or without a financial benefit. The present research examines decisions between electricity plans that feature both types of trade-offs as well as decisions that vary with respect to their financial ramifications for consumers, with some environmentally-friendly plans incurring additional financial costs and others resulting in financial savings. We propose that people will use different decision modes in parallel when making decisions about electrical utility plans. We anticipate that use of the Calculation Mode will lead to less adoption of environmentally-friendly options, while use of the Affect or Role Modes will lead to more adoption of environmentally-friendly options. Additionally, we examine the extent to which people have lay theories of how decision modes influence choice. We conducted six online studies in two different countries: Switzerland and the United States. Table 1 summarizes the findings from the four studies that investigate the relationship between decision mode use and choice. Across our six experiments, we demonstrate (a) that qualitatively different choice processes operate in parallel, (b) that the degree to which a consumer deploys different decision modes correlates with consumer choices of environmentally-friendly options, and (c) that encouraging the use of different decision modes can influence selection of environmentally-friendly electrical plans.",4
92.0,3.0,Theory and Decision,13 August 2021,https://link.springer.com/article/10.1007/s11238-021-09832-1,Frames and decisions under uncertainty in economics theory,April 2022,David Schmeidler,,,Male,Unknown,Unknown,Male,"Hillel the elder used to say: ""Do not judge your fellow until you reach his place"". This saying has many interpretations, and we use the one Biden's mother used to tell him: do not judge a person till you enter his shoes (that is, you are him). The impossibility of full communication between people is not a new topic. Modern psychology noticed that memory may dictate what a person sees or hears. For example, John and Bob are good friends. John asks Bob a question, but he hears the reply not as Bob said it (if asked). He 'reads between the lines' and finds in Bob's reply hints dictated by his own memory, which Bob does not share. A work of art, say a painting, is considered an extension of the artist. Some experts on art claim that here too the viewer is blocked from seeing it as it is by his own limitation, as if he looked at it through distorting, framed glasses that exclude part of the painting. This is more pronounced when the work of art is sufficiently deep to offer multiple possible interpretations, which is the case we are interested in here. The same applies to a book or a poem or a piece of music. Repeated viewing (reading, listening) discovers aspects not noticed before. This also applies to attending performances such as plays, concerts and operas, where it may not be known who the artist is (or are). What a person equipped with his distorting framed glasses sees, while looking at a work of art depends to a large extent on his education in art. A listener well acquainted with Bach music cannot react to sixteenth century music as did its sixteenth century listeners or some contemporary listeners not exposed to classical music. Similarly, most contemporary people viewing medieval art, say a fourteenth century painting of the holy family, through their distorting framed glasses do not or will not comprehend the artist's goal (only experts of late medieval art will look at such paintings as that period person) As the title says, in this note, we deal with decisions under uncertainty in economics. The digression to art allows us to clarify our intuition with respect to limitations we face in our analysis. PS. I chanced upon a 2004 reference where music suggests insight to finance: http://www.ffcapital.com/about/.",
92.0,3.0,Theory and Decision,28 February 2022,https://link.springer.com/article/10.1007/s11238-022-09876-x,Debiasing or regularisation? Two interpretations of the concept of ‘true preference’ in behavioural economics,April 2022,Robert Sugden,,,Male,Unknown,Unknown,Male,"One of the biggest problems in behavioural economics is to find a method of normative analysis that can be used even when individuals’ revealed preferences contravene the rationality axioms of neoclassical economics. Before behavioural economics took off in the 1980s and 1990s, there was a firmly established consensus about how to do normative analysis in economics. That analysis was interpreted as a study of welfare, and there was general agreement about the basic principles of welfare economics. Those principles rested on the assumption that, with respect to whatever outcomes were the subject of economic analysis, each individual’s preferences were complete (i.e., every pair of outcomes was ranked by strict preference or indifference), consistent (i.e., compatible with standard rationality axioms), context-independent (i.e., independent of welfare-irrelevant properties of particular decision situations), and revealed in choice. A person’s preferences were interpreted as providing (or defining) an ordinal measure of her welfare. However, we now know from the findings of behavioural research that the preferences revealed in people’s actual choices show systematic patterns that contravene consistency and context-independence and which, although susceptible to psychological explanation, cannot plausibly be interpreted as relevant for assessments of welfare. If normative analysis in economics is to be reconciled with behavioural findings, something in the previous consensus has to be given up. This paper is concerned with one broad strategy for tackling this problem, characterised by Infante et al. (2016) as behavioural welfare economics. This approach retains the assumption that individuals have complete, consistent and context-independent preferences, and treats those preferences as indicators of welfare. What is given up is the assumption that those preferences are reliably revealed in choice. Instead, some concept of true (or ‘underlying’ or ‘latent’) preference is invoked. Completeness, consistency, context-independence and welfare relevance are attributed to true preferences, but actual choices are explained as resulting from the combined effects of true preferences and ‘biases’ or ‘errors’. In this paper, I compare two different ways of interpreting the concept of true preference. It is generally held that the founding contributions to behavioural welfare economics were made in 2003 in two remarkably similar papers by American behavioural economists and legal scholars, proposing ‘asymmetric paternalism’ (Camerer et al., 2003) and ‘libertarian paternalism’ (Sunstein & Thaler, 2003). However, my focus will be on a paper by Bleichrodt, Pinto-Prades and Wakker (BPW) which advocated a form of behavioural welfare economics two years before the American papers (Bleichrodt et al., 2001). One significant feature of the earlier paper is that, unlike the later two, it proposes an operational method by which an analyst can identify individuals’ true preferences. Variants of that method have subsequently been advocated by other behavioural economists (e.g., Beshears et al., 2008: 1790–1791; Kőszegi & Rabin, 2008), and BPW’s pioneering work has not received the full credit it deserves. In this paper, however, I am more concerned with another important difference between BPW’s paper and its successors. It is now often forgotten how much early work in behavioural economics was carried out in response to problems uncovered by stated preference studies. Such studies use survey methods to elicit individuals’ preferences for non-marketed goods, typically for use in cost–benefit or cost-effectiveness analysis. Common applications include the elicitation of individuals’ valuations of changes in the provision of environmental public goods, changes in health states, and changes in accident risks. At a time when most of the data used by economists was highly aggregated, stated preference surveys were unusual in generating individual-level preference data that were capable of revealing direct violations of rationality axioms. Since the whole project of stated-preference research presupposed the existence of consistent and context-independent preferences, the discovery of systematic inconsistencies in the data (for example, disparities between willingness-to-pay and willingness-to-accept, part-whole disparities, and the dependence of survey responses on the scale on which those responses are elicited) created serious problems. Initially, many economists attributed these inconsistencies to supposed defects in the survey instruments used, or to the fact that stated-preference surveys necessarily use hypothetical questions. An early research programme in behavioural economics investigated whether the same anomalies occurred in controlled experiments in which subjects made real choices over familiar consumer goods. It found the same qualitative effects (e.g., Bateman et al., 1997a, 1997b). One response to this problem was to look for ways of retrieving consistent ‘true’ preferences from survey responses. BPW’s work belongs to that project. In contrast, Camerer et al. (2003) and Sunstein and Thaler (2003) start from the premise that, as a result of psychological mechanisms identified by behavioural economics, individuals make choices that are contrary to their best interests. Their papers are written as justifications of paternalistic interventions designed to steer people away from such choices. Thus, Camerer et al. say: ‘Recent research in behavioral economics has identified a variety of decision-making errors that may expand the scope of paternalistic regulation. To the extent that the errors identified by behavioral research lead people not to behave in their own best interests, paternalism may prove useful’ (pp. 1211–1212). Similarly, Sunstein and Thaler say: ‘Drawing on some well-established findings in behavioral economics and cognitive psychology, we emphasize the possibility that in some cases individuals make inferior decisions in terms of their own welfare’. In such cases, they recommend ‘private and public planners’ to ‘self-consciously attempt to move people in welfare-promoting directions’ (p. 1162). For these authors, the distinction between revealed preference and true preference is conceptualised in terms of the presence or absence of error on the part of the individuals whom the proposed interventions are to help. ‘Error’ and ‘helping’ are recurrent tropes in both papers. Evidence of inconsistency or context-dependence in an individual’s preferences is treated as evidence of error and of a consequent need for help. There is a fundamental difference here. The premises of BPW’s project are not paternalistic. Their running examples are about cost-effectiveness in a European-style health care system in which health care is free at the point of delivery and is financed through taxation or social insurance. In such a system, it is a fact of life that decisions have to be made about the allocation of resources between classes of patients with different medical conditions and between alternative treatments for given conditions. If, in making those decisions, policy-makers believe that they should be guided by information about citizens’ preferences, that is the opposite of paternalism. The problem is to find a way of extracting relevant information from inconsistent survey responses. Ultimately, it is the policy-makers, not the citizens, who are in need of help. In the current paper, I explore some of the implications of that difference by examining BPW’s formal analysis and the arguments by which they justify it. The first aim of my paper, and the subject of Sects. 1 to 4, is interpretative—to elucidate BPW’s own understanding of the method they propose. I consider two alternative interpretations. The debiasing interpretation assimilates BPW’s method to the form of behavioural welfare economics proposed by Camerer et al. and Sunstein and Thaler in their 2003 papers. It treats BPW’s method as an attempt to reconstruct individuals’ true preferences by removing the effects of errors that those individuals make when taking decisions for themselves, or when reporting their own preferences. In contrast, the regularisation interpretation, first discussed by Infante et al. (2016), treats BPW’s method as a means of constructing indices of individual welfare that are broadly consistent with individuals’ revealed preferences and that satisfy principles of rationality that are judged to be appropriate for public decision-making.Footnote 1 If individuals’ revealed preferences contravene standard rationality conditions, that shows that some regularisation is needed in order to create useful welfare indices, but it is not treated as evidence that those individuals are making errors. I find textual support for each of these interpretations in different parts of BPW’s paper, but argue that the most consistent interpretation of their method is in terms of regularisation. In Sect. 5, I characterise a general method of regularisation that is consistent with (and, I suggest, implicit in) BPW’s work. In the final section I argue that this method is coherent and defensible, and that, unlike the debiasing approach to behavioural welfare economics, it is not vulnerable to the criticism that it invokes psychologically ungrounded concepts of true preference and bias. But I must emphasise that the content of my paper is not a proposal from me about how welfare economics should be adapted in the light of behavioural findings. It is a reconstruction—I hope, a sympathetic reconstruction—of a proposal made by BPW.",3
93.0,1.0,Theory and Decision,19 November 2021,https://link.springer.com/article/10.1007/s11238-021-09851-y,Peter C. Fishburn (1936–2021),July 2022,Steven J. Brams,William V. Gehrlein,Fred S. Roberts,Male,Male,Male,Male,,
93.0,1.0,Theory and Decision,02 August 2021,https://link.springer.com/article/10.1007/s11238-021-09835-y,Driving a hard bargain is a balancing act: how social preferences constrain the negotiation process,July 2022,Yola Engler,Lionel Page,,Unknown,Male,Unknown,Male,"The usual haggling process is based on imperfect information, the hagglers trying to propagandize each other into misconceptions of the utilities involved. Nash (1953), Two-person cooperative games Lest readers think erroneously that it’s always wise to bargain tough. Raiffa (1982), The Art and Science of Negotiation Bargaining is pervading in economic and social interactions and it has been a natural object of study for economists. The large economic literature on economic bargaining has brought many insights about how bargaining outcomes are determined depending on the bargaining power, exit options and preferences of the bargainers. Still, little is known about the negotiation process itself: what is a good opening offer, how long should you stick to an offer, and how much should you change your offer when it is a deadlock? Negotiations in real economic situations often seem an art which requires expert practice to excel. They are typically characterised by incomplete information and the use of messages which are hard to measure and quantify. As a consequence, studying the negotiation process in the field is challenging. This paper investigates a critical aspect of negotiations: the “haggling” process where players exchange offers which are costless to reject. We focus here on the effect of first offers. To do so, we design a bargaining game where the zone of possible agreements is known (similarly to an ultimatum game). Under standard assumptions of common knowledge of rationality and payoff-maximisation, first offers should not influence the final outcome. However, the fact that bargainers may have social preferences implies that it is actually a game of incomplete information where players’ preferences are not common knowledge. Players can use the haggling process to try to influence each others’ beliefs about their personal preferences (Nash, 1953). To assess the effect of first offers on the bargaining process, we study the effect of a wide range of first offers on the reaction of the player receiving it. We elicit both the actions of the receiver and his first and second-order beliefs. This design allows us to study whether and how the level of the opening offer influences bargainers’ beliefs, their actions and the final bargaining outcome. This study contributes to several important strands of literature. First, it extends the literature on the role of social preferences in bargaining. A large body of work has shown that social preferences limit the range of acceptable outcomes. Bargaining experiments have shown that bargaining outcomes are influenced by players’ preferences over payoff distributions (Camerer, 2003); and by their preferences over the intentions of other players (Blount, 1995; Offerman, 2002). Here we show that they also constrain the negotiation process itself. Social preferences can play a role during the sequence of offers and counter-offers in a negotiation. Intention-based preferences influence the bargaining process because offers can be perceived as signalling something about the other player’s intentions. As a consequence, the history of offers can influence the final outcome of the bargaining process. Second, this paper adds to the literature on bargaining with reputation where a player has the possibility to build a reputation for stubbornness by sending an initial message to the other player (Abreu and Gul, 2000; Wolitzky, 2012; Embrey et al., 2015). Opening offers in real-world negotiations are often intended to signal the toughness of ones’ bargaining strategy. Yet, not much is known about whether the first offer has indeed an impact on the other bargainer’s beliefs.Footnote 1 We elicit the belief of the player receiving the first offer (Responder) about the minimal amount the player making the offer (Proposer) would accept. Doing so, we can measure whether first offers convey some information about the player’s final bargaining stance. Third, our study complements the research on the role of communication in bargaining games. Experimental studies have found that “cheap talk” phases before the bargaining itself influence players’ strategies and, therefore, the bargaining outcome (Croson et al., 2003; Rankin, 2003; Anbarci et al., 2015). Moreover, offers themselves can be used to communicate feelings and intentions to the other player (Xiao and Houser, 2005). In the context of our laboratory experiment we isolate and study a simple and precise piece of information: the level of an opening offer which is costless to reject. Such an offer does not formally affect the bargaining power of the player making the offer. However, it can have an effect on the bargaining process if it has a role as a communication tool. Our results are striking in what they reveal about the bargaining process. We find that the first offer has a substantial effect on the bargaining outcome even though it is costless to reject. Proposers’ first offers are correlated with their minimum acceptable amounts and therefore carry some information about the Proposer’s likely refusal of unfavourable splits. We also find that Proposers are credibly obstinate as they tend to reject unfavourable counter-offers with a high probability. In the end, Proposers are able to get a high proportion of the pie to be divided even though, under standard assumptions, they have as much bargaining power as in the ultimatum game (none). We also find evidence that the Responder’s intention-based social preferences are triggered by first offers. Offers which favour the Proposer are not only rejected but often lead to low counter-offers from the Responder. In a substantial number of cases, the Responder chooses a “punishing” counter-offer which is lower than what he believes is the Proposer’s minimum acceptable amount. We are able to investigate different theoretical explanations for such behaviour. We find that the players’ first and second-order beliefs do not seem to drive players’ behaviour as suggested by psychological game-theoretic models of reciprocity (Rabin, 1993; Dufwenberg and Kirchsteiger, 2004). The Responder’s reaction to the Proposer’s offers appears, instead, compatible with Levine's model of reciprocity and spitefulness where agents care about the type of the other player (Levine, 1998). We also find evidence suggesting that players react negatively to offers which can be perceived as disrespectful as suggested by Yamagishi et al. (2012). This last result is in line with recent research pointing to individual preferences for self-esteem (Bénabou and Tirole, 2006; Ellingsen and Johannesson, 2008), and the demand for respect and status in social interactions (Eriksson and Villeval, 2012; Besley and Ghatak, 2008; Heffetz and Frank, 2008; Charness et al., 2014). The remainder of the paper is organised as follows: The next section inserts our research in the context of the existing literature. Section 3 introduces our experimental design and outlines our research hypotheses. It is followed by the analysis of our data and the obtained results in Sect. 4. Section 5 concludes with a short summary and discussion of our findings.",2
93.0,1.0,Theory and Decision,08 September 2021,https://link.springer.com/article/10.1007/s11238-021-09841-0,Individual cheating in the lab: a new measure and external validity,July 2022,Andrea Albertazzi,,,Female,Unknown,Unknown,Female,"Cheating permeates many social and economic interactions of daily life (DePaulo et al. 1996; Ariely 2012). Examples range from corporate scandals (e.g., Dieselgate, Facebook-Cambridge Analytica), tax evasion (Slemrod 2007) and consumer misbehaviour (Mazar and Ariely 2006). To make things worse, endeavours to study cheating in natural contexts are hindered by its secretive nature. Therefore, controlled experiments represent an attractive instrument to study individual attitudes towards cheating. The die-roll paradigm (Fischbacher and Föllmi-Heusi 2013) represents the most popular measure of cheating used in the laboratory. Participants are asked to roll a die in private and to report the result to the experimenter. As the true outcome is observed by subjects only, there is a monetary incentive to lie by reporting those outcomes associated with higher rewards. Despite its simplicity, this type of task presents a considerable limitation: cheating can only be inferred at the aggregate level by comparing the empirical distribution of actual reports with its theoretical prediction. Hence, it is not possible to know, by design, if a particular subject lied or not.Footnote 1 Whether or not laboratory measures of cheating extend to non-controlled environments is still under investigation. For instance, the experimenter scrutiny or the artificiality of the lab environment might trigger different ethical norms. If this is the case, then laboratory results on cheating might not generalise to the field (Levitt and List 2007). Our paper aims to address these two limitations. First, we design a novel task that, in contrast to the existing literature, allows us to observe cheating at the individual level. In our task, subjects have five seconds to choose, in their mind, one out of 60 colours (e.g. Yellow) from a list displayed on their screen. Once this list disappears, three new lists containing four colours each (e.g. White, Beige, Milk, Plum) are displayed. Every new list is associated with a different positive payoff. If subjects claim their chosen colour to be in one of the three new lists, they receive the payoff associated with that list; otherwise, they receive nothing. We know that the participants have cheated if they pick a list of colours on the second screen that does not contain any colour that was already present in the first larger list. Second, we use the fact that in our task cheating is observable at the individual level and ask to what extent cheating in the lab predicts cheating in the field within the same population. Participants are not paid immediately after the experiment. Instead, after a few days, they have the opportunity to cheat in the field by self-reporting their earnings. Subjects are paid according to the amount of money they claim to have earned in the laboratory. We use two field variations that differ in the degree of anonymity of the field decision. In the first, the self-reporting procedure is completely anonymous, while the second field variation requires participants to meet in person with the experimenter. The main contribution of this paper is twofold: (i) it develops a laboratory task that allows for individual level observations of cheating, and (ii) it allows for a comparison of both the extensive and intensive margins of cheating between the laboratory and a non-controlled environment.Footnote 2 In line with previous findings on individual dishonesty, we find that a considerable fraction of subjects cheat in our laboratory task but, for some, not to the fullest extent. However, no significant correlation of dishonest behaviour between the lab and the field is observed. Although more than half of the subjects cheat to some extent in our task, most of them refrain from over-reporting their experimental earnings. Moreover, for those who do so, we find no difference in the extent of cheating between subjects that are honest in the laboratory and those who are not. Interestingly, when using a variation of the die-roll task that only allows to infer cheating at the aggregate level, we do find a weak correlation between lab and field behaviour. To the best of our knowledge, only few other studies examine the correlation between dishonest behaviour in the lab and cheating in the field within the same population.Footnote 3 Dai et al. (2018) perform an artefactual field experiment where passengers of public transportation are asked to play a modified version of the die-roll task. As a main result, the study finds that fare dodgers, on average, are more likely to report the most profitable outcome than ticket holders. Similar to our study, Potters and Stoop (2016) use a student subject pool to correlate self-reported performance in a mind game implemented in the lab with a field measure of cheating. After the experiment, payments are issued via bank transfer and some subjects are deliberately overpaid by an amount of €5. A significant correlation of 0.31 between performance in the mind game and not reporting the overpayment is found. In contrast to Potters and Stoop (2016), our study allows for the observance of cheating at the individual level, measures cheating at both the extensive and intensive margins, provides full anonymity in the lab and in one of the field tasks and requires active misreporting in both environments. These new features allow for a deeper understanding of whether lab measures of cheating are reliable predictors of dishonesty in other environments. The extent to which laboratory results on cheating can be generalised to other settings remains unclear.Footnote 4 Laboratory evidence shows persistent patterns on dishonesty across subjects. Some individuals are completely honest, while others either lie to the maximum extent possible, or forfeit part of the monetary gains when they do cheat (Gneezy et al. 2018; Abeler et al. 2019; Gerlach et al. 2019). Instead, studies that focus on dishonesty in the field provide mixed results. While some find substantial cheating among subjects (e.g., Drupp et al. 2019; Bucciol and Piovesan 2011), other studies report different findings. For example, Abeler et al. (2014) report no evidence of lying in a randomised field experiment where subjects are called at home and have a monetary incentive to misreport the outcome of a privately tossed coin. Similarly, Cohn et al. (2014) show that bankers cheat in a coin-flip task when they are reminded about their professional identity. However, when such cue is not emphasised, reported outcomes do not differ from their truthful distribution. The remainder of this paper is organised as follows: Section 2 describes the experimental design, Sect. 3 presents the main results of the paper, Sect. 4 discusses the main findings and Sect. 5 concludes.",
93.0,1.0,Theory and Decision,21 August 2021,https://link.springer.com/article/10.1007/s11238-021-09838-9,Spatial bargaining in rectilinear facility location problem,July 2022,Kazuo Yamaguchi,,,Male,Unknown,Unknown,Male,"When a facility is planned to be built somewhere, it is impossible to choose a location granting the wishes of all the interested parties, because each interested party would want the facility be built as near as possible to her own residential location. Then, if the facility location is collectively chosen by the interested parties through some method coordinating their interests, which location would they choose? Hansen and Thisse (1981) considered a model where individuals collectively choose a facility location on a network through voting using the majority rule, and sought a Condorcet location (i.e., a location such that no other location is closer to a strict majority) as the solution. A Condorcet location hardly exists on a network in general, but Hansen and Thisse (1981) showed that such a location does exist on a tree network. Furthermore, Hansen and Thisse (1981) showed that on a tree network, a Condorcet location is a minisum distance location (i.e., a location that minimizes the average distance from an individual’s residential location). Kawamori and Yamaguchi (2010) considered a model where individuals collectively choose a facility location on a tree network through bargaining using the unanimity rule, and sought a stationary subgame perfect equilibrium (SSPE) location as the solution. Kawamori and Yamaguchi (2010) showed that on a tree network, as individuals become infinitely patient, the SSPE location converges to a minimax distance location (i.e., a location that minimizes the maximum distance from an individual’s residential location). Chepoi and Dragan (1996) considered a model where individuals collectively choose a facility location on a two-dimensional distance space through voting using the majority rule, and sought a Condorcet location as the solution. A Condorcet location hardly exists on a two-dimensional distance space in general, but Chepoi and Dragan (1996) showed that such a location does exist on a two-dimensional rectilinear distance space, which can be a reasonable approximation of a grid planned city where streets run at right angles to each other forming a grid. Furthermore, Chepoi and Dragan (1996) showed that on a two-dimensional rectilinear distance space, a Condorcet location is a minisum distance location. With these results in mind, this paper considers a model where individuals collectively choose a facility location on a two-dimensional rectilinear distance space through bargaining using the unanimity rule, and seeks an SSPE location as the solution. This paper shows that on a two-dimensional rectilinear distance space, as individuals become infinitely patient, their SSPE utilities converge to the utilities that satisfy the lexicographic maximin utility criterion, which is a natural refinement of the maximin utility criterion, introduced by Sen (1970). The model presented in this paper is related to the spatial bargaining models where players with single-peaked preferences on a set of alternatives collectively choose an alternative through bargaining (see Section 4.1 in Eraslan and Evdokimov (2019) for a survey of these spatial bargaining models). Baron (1991) and Banks and Duggan (2000) considered these as variants of the transferable resource allocation bargaining models formulated by Rubinstein (1982) and Baron and Ferejohn (1989). Baron (1991) explicitly characterized the SSPEs in the case where the set of alternatives is two dimensional, the number of players is three, the players’ utility functions are quadratic in their Euclidean distances, proposer selection follows the random-proposer protocol, and voting follows the majority rule. Banks and Duggan (2000) proved the existence of no-delay SSPE and the upper hemicontinuity of the players’ no-delay SSPE proposals in the players’ discount factors in the case where the set of alternatives is compact and convex, the players’ utility functions are continuous and concave, proposer selection follows the random-proposer protocol, and the voting rule is monotonic. Furthermore, Banks and Duggan (2000) proved the coincidence of the players’ no-delay SSPE proposals and an alternative in the core (i.e., the set of alternatives unbeatable by the underlying voting rule) in the case where the set of alternatives is one dimensional; the players’ utility functions are continuous, concave, and strictly quasi-concave; the players are perfectly patient; proposer selection follows the random-proposer protocol; and the voting rule is monotonic and proper. As a result of their theorems, Banks and Duggan (2000) showed that as the players become infinitely patient, their SSPE proposals converge to the median player’s ideal alternative in the case where the set of alternatives is one dimensional; the number of players is odd; the players’ utility functions are continuous, concave, and strictly quasi-concave; proposer selection follows the random-proposer protocol; and voting follows the majority rule. However, Banks and Duggan (2000) did not explicitly characterize the SSPEs in the case where the set of alternatives is multidimensional. Furthermore, the other papers confined their attention to the case where the set of alternatives is one dimensional because of its tractability; for example, see Cho and Duggan (2003, 2009), Cardona and Ponsati (2007, 2011), Herings and Predtetchinski (2010), Predtetchinski (2011), and Cardona and Polanski (2013).Footnote 1 Thus, although the set of alternatives is often actually multidimensional in facility location and other problems, almost all papers, excluding Baron (1991), did not explicitly characterize the SSPEs in the case where the set of alternatives is multidimensional. However, this paper explicitly characterizes the SSPEs in the case where the set of alternatives is two dimensional, the number of players is three, the players’ utility functions are linear in their rectilinear distances, proposer selection follows the rejecter-becomes-proposer protocol, and voting follows the unanimity rule. Thus, an essential difference between Baron (1991) and this paper is that in Baron (1991), proposer selection follows the random-proposer protocol and voting follows the majority rule, whereas in this paper, proposer selection follows the rejecter-becomes-proposer protocol and voting follows the unanimity rule. Another essential difference between Baron (1991) and this paper is that a player’s utility is determined by her Euclidean distance to the chosen alternative in Baron (1991), but by her rectilinear distance to the chosen alternative in this paper. The remainder of this paper is organized as follows. Section 2 describes the model, and Sect. 3 presents the result.",
93.0,1.0,Theory and Decision,07 August 2021,https://link.springer.com/article/10.1007/s11238-021-09827-y,Harsanyi support levels solutions,July 2022,Manfred Besner,,,Male,Unknown,Unknown,Male,"Many institutions, companies, governments, and so on are organized in hierarchical structures. Typically, there is one unit at the top. In the following levels, each unit of the parent level is divided into two or more subordinate units, which usually have a lower rank than the higher ones. We can see a similar organizational structure in supply chains in some respects, and queuing problems or electricity and other networks often have hierarchical structures as well. Effectiveness increases by sharing or pooling physical objects, resources, and information. A central characteristic of all these forms of organization is that a cooperating unit can itself be an actor to gain advantages of cooperation for the members of the unit. The question arises: how should we share the benefits and allocate the costs? To distribute the profits of cooperating coalitions, the application of a cooperative game seems to be a natural approach. Winter (1989) defined a model for cooperative games with a level structure (LS-games). A level structure comprises a sequence of coalition structures (the levels). At each level, the player set is partitioned into components where each higher level is coarser than the previous one (see also Fig. 1). Winter’s value (Winter, 1989) for LS-games, we call it Shapley levels value, extends the Owen value (Owen, 1977), which is itself an extension of the Shapley value (Shapley, 1953b). This value satisfies adaptations of the symmetry axioms, satisfied by the Owen value, for values for LS-games. To treat symmetric players differently when there are exogenous weights for the players, Shapley (1953a) introduced the weighted Shapley values. Vidal-Puga (2012) established a value for games with a coalition structure with weights given by the size of the coalitions. With a step by step top–down algorithm, Gómez-Rúa and Vidal-Puga (2011) extended it for games with a level structure. Besner (2019) generalized this value to the class of the weighted Shapley hierarchy levels values for arbitrary, exogenously given, weights. These values satisfy an extension of the consistency property of the weighted Shapley values in Hart and Mas-Colell (1989). Interestingly, the class of the weighted Shapley hierarchy levels values contains the Shapley levels value but the values from this class do not satisfy the null player axiom in general. The weighted values for games with a coalition structure in Levy and Mclean (1989) and McLean (1991) behave the other way round; they satisfy the null player property but do not correspond to a consistency property in the above sense. Levy and Mclean (1989) examine several classes of weighted values for games with a coalition structure which use the same weight system as the weighted Shapley values: either for the players within a component or the components themselves if the components act as players. The combined use of such a weight system, both for players and components, is only mentioned. This latter class of extensions of the weighted Shapley values and an extension of the class of random order values (Weber, 1988) for games with a coalition structure is discussed in McLean (1991). Dragan (1992) called McLean’s extensions of the weighted Shapley values McLean weighted coalition structure values. He presented a formula for them related to that of the Owen value. For a fixed coalition structure, these values coincide with a multiweighted Shapley value (Dragan, 1992). Harsanyi (1959) introduced a new perspective on coalition functions. He used so-called (Harsanyi) dividends, assigned to all feasible coalitions of a player set according to the coalition function. Singletons receive the singleton worth as their dividend and the dividend of each larger coalition S amounts to the worth of S minus the sum of all dividends of the proper subcoalitions of S. The weighted Shapley values give the players as payoffs a share of the dividends from the coalitions in which they are members. Two players’ shares of dividends from coalitions containing both players are always in the same ratio. In comparison, the Harsanyi solutions (Hammer et al., 1977; Vasil’ev, 1978) are more flexible. By these values, two players can receive dividend shares from coalitions, containing both, in various ratios for each coalition. In this article, we introduce the class of Harsanyi support levels solutions. To the best of our knowledge, the values of this class are the first values for games with level structure (LS-values) that extend the Harsanyi solutions. We represent each Harsanyi support levels solution by a formula with dividends. The coefficients in the formulas constitute a dividend sharing system, i. e., all coefficients are non-negative and amount to one for each coalition. According to the definition of a Harsanyi solution, each LS-value from this class corresponds to a Harsanyi solution for a fixed level structure. Therefore, Harsanyi solutions inherit all properties (adapted for LS-values) of these values for a fixed level structure. Within the framework of LS-games, we can also consider the components of a level as players in so-called induced LS-games. Winter (1989) has shown that the Shapley levels value satisfies the level game property. This property means that the payoff to a component in an induced LS-game is equal to the sum of the payoffs to all players included in the component. We show that if an LS-value satisfies the level game property and coincides for a fixed level structure with a Harsanyi solution, the value is a member of the class of the Harsanyi support levels solutions. As a corollary, our main result extends an axiomatization of the class of the Harsanyi solutions by Vasil’ev (1981) and Derks et al. (2000) to a characterization of the class of the Harsanyi support levels solutions. Since the Harsanyi solutions are no random order values in general, we cannot adopt the proof procedures for characterizations, e.g., in Winter (1989) or McLean (1991). We base our proofs on dividends where two new lemmas are a significant help. As a further result, we present the class of the weighted Shapley support levels values as a proper subset of the class of the Harsanyi support levels solutions. The LS-values from this class coincide with the McLean weighted coalition structure values on a level structure with only three levels if we count the partition containing all singletons and the partition containing only the grand coalition as levels. To offer a characterization for this class, we adapt an axiomatization of the class of the weighted Shapley values in Nowak and Radzik (1995). In the concluding section, it is briefly explained that the values presented in this article can have an interesting solidarity characteristic: players who have formed a group support the other group members even in situations where they are not active and do not expect any direct reward for their support, as with the weighted Shapley Hierarchy levels values. In this way, they differ fundamentally from solidarity-based solution concepts such as the equal division value, the equal surplus division value (Driessen and Funaki, 1991), the egalitarian Shapley values (Joosten, 1996) or the solidarity value (Nowak and Radzik, 1994) and the Shapley-solidarity value (Calvo and Gutiérrez, 2013). The outline of the paper is structured as follows. Section 2 contains preliminaries. As the main part, we introduce in Sect. 3 the Harsanyi support levels solutions with an appropriate class axiomatization. In Sect. 4, we offer the weighted Shapley support levels values as a further result. A numerical example in Sect.  5 compares different values. Section  6 discusses the results and concludes with a fundamental principle of group solidarity which is satisfied by our new values. The Appendix (Sect.  7) provides all the proofs, related properties and lemmas, and shows the logical independence of the axioms in our axiomatizations.",2
93.0,1.0,Theory and Decision,20 July 2021,https://link.springer.com/article/10.1007/s11238-021-09831-2,Trading transforms of non-weighted simple games and integer weights of weighted simple games,July 2022,Akihiro Kawana,Tomomi Matsui,,Male,,Unknown,Mix,,
93.0,1.0,Theory and Decision,20 July 2021,https://link.springer.com/article/10.1007/s11238-021-09834-z,Multidimensional social identity and redistributive preferences: an experimental study,July 2022,Fuhai Hong,Yohanes E. Riyanto,Ruike Zhang,Unknown,Unknown,Unknown,Unknown,,
93.0,1.0,Theory and Decision,13 September 2021,https://link.springer.com/article/10.1007/s11238-021-09840-1,Sharing the surplus and proportional values,July 2022,Zhengxing Zou,René van den Brink,Yukihiko Funaki,Unknown,Male,Male,Male,"Equal and proportional divisions are two basic principles in allocation problems. In cooperative games with transferable utility (TU-games), usually these principles are applied to a remainder of the surplus after each individual player is assigned an individual entitlement which can be equal to zero. For two-player games, this can be formalized in axioms, such as standardness (assigning each player its stand-alone worth and allocating the surplus equally over all players), egalitarian standardness (ignoring individual entitlements and allocating the full worth equally over the players), and proportional standardness (allocating the full surplus proportional to the stand-alone worths of the players). For example, the Shapley value (Shapley, 1953) and the equal surplus division value (Driessen & Funaki, 1991) satisfy standardness, the equal division value [axiomatized in van den Brink (2007)] satisfies egalitarian standardness, and various proportional values, such as the proportional value (Ortmann, 2000), the proportional Shapley value (Béal et al., 2018; Besner, 2019), the proportional Harsanyi solution (Besner, 2020), and the proper Shapley values (Vorob’ev & Liapunov, 1998; van den Brink et al., 2015) satisfy proportional standardness. The values can be extended to games with more than two players by, for example, reduced game consistency or balanced contributions type of axioms that relate payoffs of players in a game with their payoffs in a game on a reduced player set. There is a large literature on ‘equal sharing of the surplus’ type of values. In contrast, values that appear to be ‘proportional’ are studied less, although proportional considerations play a central role in fair division problems as pointed out by a group of economists and academics, e.g., Brams and Taylor (1996), Chun (1988), Moulin (1987), Moulin (2004), Thomson (2015), Tijs and Driessen (1986) and Young (1995). However, recently there is a growing literature on values that are based on proportionality, such as the literature mentioned above. In this paper, we provide a new family of values, called the proportional surplus division values which make a trade-off between a player’s stand-alone worth and the average stand-alone worth, and allocate the remainder proportional to the stand-alone worths. Extreme cases of values in our family are the proportional division value, based on Moriarity (1975) and Gangolly (1981), and shortly denoted by PD value, and the egalitarian proportional surplus division value, shortly denoted by EPSD value. The PD value allocates the worth of the grand coalition in proportion to players’ stand-alone worth. The EPSD value is a new value that assigns to each player the average stand-alone worth, and then allocates the remainder of the worth of the grand coalition in proportion to players’ stand-alone worth. The EPSD value focuses on egalitarianism in allocating the stand-alone worths by first assigning to every player the average of all stand-alone worths, whereas the PD value applies an egocentric principle and first assigns to each player its own stand-alone worth. Both values apply proportionality in the allocation of the remaining surplus. Besides these two extreme values, our family consists of all convex combinations of the PD value and the EPSD value, and thus can be viewed as making a trade-off between egocentrism and egalitarianism. This family of values is in line with a recent and growing literature that combine different allocation principles by considering convex combinations of two extreme values, such as the egalitarian Shapley values [being convex combinations of the Shapley value and equal division value, see Joosten (1996) and van den Brink et al. (2013)], the consensus values [being convex combinations of the Shapley value and equal surplus division value, see Ju et al. (2007)] and the family of convex combinations of the equal division value and the equal surplus division value [axiomatized in, e.g., van den Brink and Funaki (2009), van den Brink et al. (2016), Xu et al. (2015) and Ferrières (2017)]. Also, our family of values is in line with a recent and growing literature on non-symmetric surplus sharing values, such as the weighted division value (Béal et al., 2015; Béal et al., 2016a), the weighted surplus division value (Calleja & Llerena, 2017, 2019), the weighted equal allocation of non-separable contributions value (Hou et al., 2019), and the PD value (Zou et al., 2021). Besides several known axioms from the literature, we introduce new axioms concerning the separatorization of a player. SeparatorizationFootnote 1 refers to a player’s obstruction of cooperation in the sense that the worth of any coalition containing him equals the sum of the stand-alone worths of the players in this coalition, while the worth of any coalition without this player remains unchanged. This is not to be confused with dummification as introduced in Béal et al. (2018) [strengthening nullification studied in, e.g., Gómez-Rúa and Vidal-Puga (2010), Béal et al. (2016b) and Ferrières (2017)], where a player becomes a dummy player. The first axiom, called proportional loss under separatorization, requires that if a player becomes a separator, then all other player’s payoff change in proportion to their stand-alone worths. The second axiom, called proportional balanced contributions under separatorization, requires that, for any two players, the effects of one of them becoming a separator on the payoff of the other, are proportional to their stand-alone worths. We identify the consequence of imposing either of the aforementioned axioms in addition to the classical axiom of efficiency.  Moreover, any member of the resulting family is uniquely determined by a value defined on additive games (being games where all players are separators and thus the worth of every coalition equals the sum of the stand-alone worths of the players in that coalition). Subsequently, we characterize a family of values for quasi-additive games (being games where the worth of every coalition that does not contain all the players equals the sum of the stand-alone worths of the players in that coalition) by means of known axioms of efficiency, anonymity, a weak version of no advantageous reallocation, and continuity, which generalizes a remarkable result for rights problems in Chun (1988). By combining the axioms in these results and using weak linearity instead of continuity, the family of affine combinations of the PD and EPSD values is characterized. Adding superadditive monotonicity (Ferrières, 2017) and replacing anonymity by weak desirability, we derive an axiomatization of the family of convex combinations of the PD and EPSD values. Besides, we show how specific values are singled out using a parametrized axiom which puts a certain lower bound on the payoffs of individual players. The paper is organized as follows. Section 2 provides some notation and definitions. Section 3 introduces the concept of proportional surplus division values. Section 4 contains the results. Section 5 presents a conclusion. All proofs and the independence of the axioms in the characterization results are provided in an Appendix.",2
93.0,2.0,Theory and Decision,09 August 2022,https://link.springer.com/article/10.1007/s11238-022-09896-7,David Schmeidler’s contributions to decision theory,September 2022,Edi Karni,Fabio Maccheroni,Massimo Marinacci,Male,Male,Male,Male,"Our dear friend David Schmeidler, one of the greatest economic theorists of our times, suddenly passed away on the night of March 16, 2022, in his home in Tel Aviv. He was 82. David’s contributions span much of economic theory, with path-breaking contributions in each area he touched. Here we focus on an area that he shaped, the theory of rational decision making under uncertainty.Footnote 1 David’s interest in decision theory began early in his career. In his first publication on the subject, a one page article in Econometrica in 1971, he considered a transitive binary relation \(\succsim\) on a connected topological space X, with nontrivial asymmetric part \(\succ\). He proved that if for each x in X the upper and lower contour sets \(\{y:y\succ x\}\) and \(\{y:x\succ y\}\) are open and \(\{y:y\succsim x\}\) and \(\{y:x\succsim y\}\) are closed, then the relation \(\succsim\) is complete. Interpreting \(\succsim\) as a (weak) preference relation, this remarkable observation is puzzling in that a technical, behaviorally unfalsifiable condition, continuity, has behaviorally falsifiable implications. Based on David’s proof, analogous results were derived in the context of decision making under risk and under uncertainty. It also led to a new understanding of the structure of incomplete preference relations. Specifically, let \(\succ\) be a transitive and irreflexive binary relation on X and define \(\succsim'\) by \(x\succsim'y\,\)if \(z\succsim{x}\) implies \(z\succsim{y}\). Then \(\succ\) may be incomplete even if, for all x in X, the upper and lower contour sets of x defined by \(\succ\) are open and those defined by \(\succsim'\) are closed.
",
93.0,2.0,Theory and Decision,23 September 2021,https://link.springer.com/article/10.1007/s11238-021-09842-z,Either with us or against us: experimental evidence on partial cartels,September 2022,Georg Clemens,Holger A. Rau,,Male,Male,Unknown,Male,"When the Supreme Court of the United States ruled the Sugar Institute illegal in 1936, it terminated one of the most stable cartels in the history of US antitrust. As opposed to other large cartels such as the Folic Acid cartel or the Vitamin B6 cartel, which collapsed under the competitive pressure of emerging Chinese cartel outsiders, the Sugar Institute was characterized by a long-lasting “external stability.” “Internal stability” was secured through an internal court system, which ensured that members would stick to the cartel or be sanctioned internally. The cartel also dealt with free-riding by the outside firm Hershey, that profited from the cartel price without participating. Genesove and Mullin (1999) outline that sugar refiners from Florida suggested that the cartel should either force the outside firm Hershey to stop its “unethical” behavior or convince it to join them. Thereby the Sugar Institute overcame a significant coordination challenge. As d’Aspremont et al. (1983) stated “...however by free-riding, fringe firms enjoy higher profits than cartel members.” This raises the question: How can firms coordinate the formation of partial cartels when the latter would be better off if they were the free-riding outsider? In this paper, we experimentally analyze the effectiveness of an institutional structure (Selten 1973) to form cartels as described in the Sugar Institute case. More precisely, we investigate the conditions under which subjects may form a partial cartel. We study whether they would accept a situation of asymmetric payoffs as described by d’Aspremont et al. (1983). A partial cartel is defined as a situation in which a stable cartel is formed by a subset of all firms interacting in a market. In this experiment, we focus on a design which is a modified version of a two-stage mechanism introduced by Kosfeld et al. (2009). This mechanism was adapted to a cartel environment in Clemens and Rau (2019), facilitating the formation of an “internally” and “externally” stable partial cartel. In the first stage, it allows potential cartel members to figure out the potential cartel insiders and cartel outsiders before ultimately implementing the cartel in the second stage. Undesirable constellations, where outsiders may free-ride at the expense of cartel insiders, can be prevented by renouncing the formation of the partial cartel. Our paper thereby contributes to the experimental literature on partial cartels, which has provided only limited evidence on this subject.Footnote 1 Our paper provides important insights for antitrust policy. Although the Sherman Antitrust Act led to the adoption of more aggressive enforcement of antitrust policies, cartels are still a severe problem in the USA (Levenstein & Suslow, 2016). The US Department of Justice (henceforth “DOJ”) underlines that: “Appropriate sanctions for cartel activity in any particular case depend on a variety of factors, the most important of which is the severity of the offense [...]. Since the purpose of the sanction is to deter the offense, the ideal measure of offense severity arguably is the gain to the defendants from the unlawful cartel activity.”Footnote 2 Although the DOJ acknowledges that the gains of the cartel activity should serve as a measure of the sanctions, they point at the difficulties of estimating the damage: “But basing sanctions on gain or harm would require costly efforts to estimate such effects. Such efforts, however, should be avoided because they would seriously undermine the efficiency of the legal system.”Footnote 3 Consequently, the US Sentencing Guidelines set the cartel fine based on the annual turnover of the products covered by the cartel activity over the period it occurred, irrespective of whether the cartel created any harm. Our experimental results highlight that potential payoff asymmetries, resulting from a partial cartel, may prevent cartelists from forming the cartel and ultimately from implementing the cartel strategy at all. More precisely, our findings emphasize that even when cartel formation is attempted and, therefore, sanctionable by antitrust laws free-riding by outsiders may ultimately jeopardize the formation of the cartel, resulting in limited harm for consumers.Footnote 4 This yields an important insight for antitrust authorities and the DOJ: If a partial cartel was instigated but yet failed to emerge because it was subject to excessive free-riding behavior by outsiders, the harm resulting from the cartel is limited. When setting the fine for a cartel, antitrust authorities should therefore take into consideration that partial cartels may ultimately have failed in overcoming the coordination challenge. Moreover, our results suggest that communication is not the only factor determining the success of cartels. Our results show that the combination of communication and an institutional sanctioning mechanism yields cartelization rates between 83 and 97%, which largely exceed those cartelization rates where the success of the cartel relies solely on communication.Footnote 5",1
93.0,2.0,Theory and Decision,24 September 2021,https://link.springer.com/article/10.1007/s11238-021-09843-y,Empirical tests of stochastic binary choice models,September 2022,Addison Pan,,,,Unknown,Unknown,Mix,,
93.0,2.0,Theory and Decision,29 September 2021,https://link.springer.com/article/10.1007/s11238-021-09845-w,Strategic investment decisions in multi-stage contests with heterogeneous players,September 2022,Christian Deutscher,Marco Sahm,Hendrik Sonnabend,Male,Male,Male,Male,"Contests are common in real-life settings, such as labour markets, industrial economics, sports, and public choice. In the majority of cases, ability levels vary across contestants, and this heterogeneity has adverse effects on contestants’ willingness to invest resources, often referred to as the ‘discouragement effect’ (e.g. Konrad, 2009).Footnote 1 In a dynamic (tournament) setting with multiple stages and asymmetric abilities, a contestant’s investment in a current stage can also be affected by the strength of the opponents in future stages. In US presidential primaries for example, a candidate may ‘skip’ a state, by reducing campaign expenditures in that state, because it offers a low probability of victory and instead save and reallocate those resources to subsequent efforts in states where the outcomes of the election are less certain. More generally, disadvantaged subjects tend to drop out of competitions to save effort. In team sports, coaches are more likely to spare top players in very unbalanced matches to avoid injuries and reduce (mental) fatigue for the upcoming game—in case the upcoming game is anticipated to be a close contest. Such occurrences are most recently discussed as ‘load management’ in the media. We thus hypothesise that contestants self-restrict in the current stage of a dynamic battle as long as the gap in abilities at this stage is sufficiently large. This hypothesis corresponds to the discouragement effect. We also predict a spillover effect, such that contestants tend to self-restrict in the current stage when heterogeneity in the subsequent contest (upcoming stage) is sufficiently small.Footnote 2 To establish empirical evidence, we use data from the German Bundesliga, a professional association soccer league with a double round-robin tournament structure.Footnote 3 Professional soccer leagues offer a formalised structure and full information about the number of rounds and opponents at the beginning of the tournament; they also entail contests with high incentives. To test our predictions, we leverage a unique rule stating that a player who accumulates a critical number of yellow cards (also called ‘bookings’) will be suspended for the next match. Teams thus may self-restrict by not fielding a player threatened by a ban or by strategically provoking a booking to obtain a suspension for the subsequent match. In line with our predictions, we find that teams systematically self-restrict in round t if the difference in ability, or heterogeneity, in t is sufficiently large. Results regarding the effect of heterogeneity in t + 1 on the decision to hold back resources in t are somewhat mixed, but they provide strong evidence of anticipating behaviour. This article thus contributes to three strands of literature. First, it relates to work on the impact of heterogeneity in contests in general, starting with Rosen (1986), who clearly suggests that unbalanced match-ups result in lower effort levels. Theoretical work strongly supports this discouragement effect, including contributions by Baik (1994), Stein (2002), Szymanski (2003), or Konrad (2009). Intuitively, underdogs lower their willingness to invest when winning becomes too costly, whereas favourites respond by lowering their investment when the outcome appears predefined. Empirical evidence obtained in laboratory settings (e.g., Dechenaux et al., 2015; Hart et al., 2015) and from the field (e.g., Ehrenberg & Bognanno, 1990; Iqbal and Krumer, 2019; Sunde, 2009) similarly affirms these discouragement effects. We therefore expect that the investment of resources depends critically on heterogeneity in the competition. Second, this paper contributes to a growing body of research on anticipating behaviour and inter-temporal effort provision in multi-period contests with fixed prize structures with theoretical framework provided by Szentes and Rosenthal (2003) and Konrad and Kovenock (2009). Laboratory setups show that participants in multi-stage contests exert effort in sub-optimal ways (e.g., Altmann et al., 2012; Mago and Sheremeta, 2017). Third, and most directly, our study also relates to ‘spillover’ or ‘carryover’ effects in tournaments with heterogeneous players. While there is much evidence suggesting strategic behaviour in sport contests in general (e.g., Genakos & Pagliero, 2012; Malueg & Yates, 2010; Taylor & Trogdon, 2002), three papers are explicitly studying the effect of past and future opponents on players’ current effort levels. In their pioneer work, Harbaugh and Klumpp (2005) consider budget constraints in a tournament model and show that both underdogs and favourites have distinct incentives to reserve resources for upcoming battles; the underdogs benefit more from investing more of their resources in initial stages. Most existing field evidence comes from sport settings, where actions and performance are observable for a schedule (or tournament tree) that is known in advance. For instance, Harbaugh and Klumpp (2005) show that the introduction of a ‘rest day’ improves the performance of the favourites in NCAA basketball, indicating a shift in incentives to conserve resources. However, direct evidence of such strategic behaviour is missing. Brown and Minor (2014), using data from top-level tennis tournaments, find that the probability that the favourite wins the current stage decreases with the strength of an expected future opponent. They argue that taking the competitor’s ability in the next round into account changes the favourite’s valuation of the tournament and hence optimal effort provision.Footnote 4
Lackner et al. (2015) also find that the intensity of play (measured by personal fouls) increases when the expected relative ability of the next-stage contestant decreases in NBA and NCAA playoffs. Similar to Brown and Minor (2014), their approach builds on the idea that future opponents affect the probability of winning the tournament, or rather the continuation value, and thus the incentives to exert effort in the current stage. The main novelty of our empirical approach is that we provide direct evidence of strategic action for achieving an optimal allocation of resources across the tournament. Existing articles use observable characteristics like scores (Brown & Minor, 2014) or personal fouls (Lackner et al., 2015) as discrete proxies for unobservable effort provision, which is a continuous decision. In contrast, we consider a discrete decision that requires self-restriction (in the line-up or by deliberately picking up a fifth yellow card), which is directly observable. This article proceeds as follows: In Sect. 2, we explain how self-restriction works in the case of soccer and derive the main hypotheses. We empirically test them with field data in Sect. 3. Finally, Sect. 4 concludes.",1
93.0,2.0,Theory and Decision,07 November 2021,https://link.springer.com/article/10.1007/s11238-021-09849-6,Reasonable Nash demand games,September 2022,Shiran Rachmilevitch,,,Female,Unknown,Unknown,Female,"Allocating scarce resources is a fundamental, classic economic problem. The divide-the-dollar game, hereafter DD, is one of the simplest strategic models of this problem. In this game, each of n players submits a demand (or bid) \(x_i\in [0,1]\), if \(\sum _{i=1}^nx_i\le 1\) then each player obtains what he asked for, and otherwise no one gets anything. Each player’s utility equals the amount of money he receives. It is clear that any split of the dollar amongst the players is supportable as a Nash equilibrium, and it is also clear that the rules of the game can be challenged on the grounds of being harsh, even simplistic. These shortcomings led researchers to modify the game in various ways. One way to modify it is to re-define its payoff function in case of demands-incompatibility. Brams and Taylor (1994) introduced the following reasonableness conditions, to be applied to such modifications:  Equal treatment of equal demands. No player’s payoff is more than his demand. If the sum of the demands does not exceed one, then each player receives his demand. If the sum of the demands exceeds one, then the dollar, nevertheless, is completely disbursed to the players. If all demands are greater than the egalitarian level demand, the player with the highest demand does not receive a higher payoff than the player with the lowest demand. Brams’ and Taylor’s conditions are meaningful and intuitive. Their analysis, however, can be extended in several directions. First, it is carried out in a model with discrete bids, in contrast to the more common approach to bidding games and resource allocation games, where bids are continuous.Footnote 1 Second, it considers particular variants of DD—some that respect the reasonableness conditions, some that do not—but provides only limited account of what these conditions imply generally. Third, DD is a particular instance of a more general game—the Nash demand game (Nash 1953, NDG for short)—which is essentially the same as DD, except that the set of feasible utility allocations need not be the unit simplex.Footnote 2 Reasonableness, then, can be studied in a more general context, not restricted to DD and its variants. In what follows, I refer to demand games in which the players obtain their demands in case the demands are jointly feasible, but in which no further restrictions (other than feasibility) are imposed, as NDGs.Footnote 3 My goal is to investigate the implications of (the counterparts of) conditions (1)–(5) on NDGs. The analysis is carried out in the continuous bids model, but at the end of each section I will be specific regarding whether or not (or under what conditions) the section’s results can be re-stated for discrete bids. It is worth mentioning that modifying the classical NDG by re-defining its payoff function is just one way to “fix” it. Alternative approaches include perturbing the game, creating a dynamic version of the game, and adding to it a post-disagreement continuation game. The first strand of the literature includes Abreu and Pearce (2015), Andersson et al. (2018), Binmore (1987), Carlsson (1991) , Duman and Trockel (2020), Nash (1953) and Rachmilevitch (2020b) . The second strand includes Anbarcı et al. (2019), Bossert and Tan (1995) and Rachmilevitch (2020a). The third strand includes Anbarcı and Boyd (2011), Cetemen and Karagözoğlu (2014), Howard (1992), Karagözoğlu and Rachmilevitch (2018) and Rubinstein et al. (1992).Footnote 4 Following Brams and Taylor, I call an NDG that satisfies the (NDG-counterparts of) conditions (1)–(5) reasonable. I show that in any reasonable NDG, the egalitarian demand profile cannot be obtained via iterated elimination of weakly dominated strategies. Thus, given reasonableness, weak dominance is of little use in NDGs,Footnote 5 and I turn to Nash equilibrium. I show that every 2-person reasonable NDG that has an equilibrium also has a value, in the sense that each player can secure the egalitarian payoff, regardless of his opponent’s behavior. Here, “value” is analogous to the minmax value in 2-person zero-sum games. This result—like the classic result about the minmax value in 2-person zero-sum games—does not extend to games with more than two players: for every \(n\ge 3\) there exists an n-person reasonable NDG that has an equilibrium, but does not have a value. An immediate implication of the aforementioned 2-person result is that the only possible equilibrium payoffs in 2-person reasonable NDGs are the egalitarian payoffs. It turns out that this is also true for three players: if a 3-person reasonable NDG has an equilibrium then the equilibrium payoffs are egalitarian. By contrast, when there are at least four players, existence of an equilibrium does not imply that equilibrium payoffs are egalitarian. This is quite unusual: game theoretic results that depend on the number of players typically draw a distinction between \(n=2\) and \(n\ge 3\), not between \(n\le 3\) and \(n\ge 4\).Footnote 6 Finally, I show that the reasonableness conditions do not guarantee existence. Specifically, I construct an example of a 2-person reasonable NDG in which no equilibrium exists, even if mixed strategies are allowed.Footnote 7 The rest of the paper is organized as follows. Section 2 contains definitions, Sect. 3 is dedicated to iterated weak dominance, Sect. 4 is dedicated to equilibrium and value, and Sect. 5 concludes.",1
93.0,2.0,Theory and Decision,05 November 2021,https://link.springer.com/article/10.1007/s11238-021-09848-7,How choice proliferation affects revealed preferences,September 2022,Fabrice Le Lec,Marianne Lumeau,Benoît Tarroux,Male,Female,Male,Mix,,
93.0,2.0,Theory and Decision,19 October 2021,https://link.springer.com/article/10.1007/s11238-021-09846-9,"Hyperplane games, prize games and NTU values",September 2022,Chaowen Yu,,,Unknown,Unknown,Unknown,Unknown,,
93.0,2.0,Theory and Decision,23 November 2021,https://link.springer.com/article/10.1007/s11238-021-09847-8,Risk aversion over finite domains,September 2022,Jean Baccelli,Georg Schollmeyer,Christoph Jansen,Male,Male,Male,Male,"The traditional risk attitude concepts of economics are defined with reference to the more primitive notion of an increase in risk. The most widespread notion of an increase in risk is that of a mean-preserving spread (Rothschild and Stiglitz 1970). This notion is usually introduced assuming that the underlying domain of payoffs is a convex subset of the reals, as is the case when the payoffs form a monetary interval, for instance. The resulting ideas of risk aversion, risk seeking, and risk neutrality have led to numerous applications in insurance theory, finance, and other areas of economics (e.g., Eeckhoudt et al. 2005). These model-free ideas can also be used to axiomatically analyze the structural properties of the main models of decision-making under risk. Specifically, they help better understand the fundamental divide between expected and non-expected utility. This is because risk attitudes turn out to be treated differently across this divide. In particular, one can define various logically nested kinds of increase in risk, accordingly, various degrees of risk aversion (or risk seeking), and subsequently prove that under expected utility, all degrees of risk aversion (or risk seeking) are characterized by the same condition, while such is not the case under non-expected utility (see esp. Chateauneuf et al. 1997). However, many risky decisions are made with respect to finite domains of non-numerical payoffs.Footnote 1 Economically relevant examples include non-divisible consumer goods, health conditions, or social positions, for instance. As the mean of an option then becomes a meaningless notion, it is not obvious how to define an increase in risk in those cases. Consequently, it is not obvious how to define risk attitudes. A coarse notion of increasing risk seems readily available, however. It is that (under suitable restrictions) any risky prospect is riskier than any riskless prospect, i.e., any payoff given with certainty. One can retrospectively interpret Yaari as building on that notion of increasing risk in his pioneering exploration (Yaari 1969) of a comparative approach to risk aversion. In this approach, rather than the absolute notion “being risk averse”, the central concept of interest becomes the more fundamental comparative notion “being more risk averse than”. Importantly, comparative notions do not apply only when the domain of payoffs is finite and non-numerical, but also over convex real domains (e.g., Diamond and Stiglitz 1974). However, in the former case, unlike in the latter, they turn out to be the only useful risk attitude notions available. The literature has now explored notions of increasing risk that are more refined than the coarse notion given above. The most important references are Allison and Foster 2004, Mendelson 1987, and Bommier et al. 2012, with motivations coming from health economics, general statistics, and insurance economics, respectively. While only the first of these papers focuses on the finite case, the remaining two nonetheless also provide analytical tools that are applicable to that case. All three papers propose to define increasing risk by a simple single-crossing condition on the distribution functions associated with the options (see our Def. 1). Surprisingly, these three important contributions are independent of one another and—the last one excepted—Yaari’s preexisting pioneering work. Overall, the current literature on risk attitudes over finite domains is not conceptually unified or systematically organized. Arguably it should, like the classic literature on absolute risk attitudes (e.g., Chateauneuf et al. 1997), be structured with reference to logically nested kinds of increasing risk. Gathering insights from the above papers, we contribute to conceptually unifying the currently available notions of comparative risk attitudes over finite domains (see Sects. 3 and 4). In particular, building especially on Bommier et al. 2012 on the one hand and the classic literature on absolute risk attitudes on the other, we propose to distinguish between weak and strong comparative risk aversion (see Def. 2). We then investigate (in Sect. 5) the characterization of these notions in a large class of non-expected utility preferences, viz. the rank-dependent utility preferences (Quiggin 1982). Such preferences include both expected utility and the so-called dual expected utility model (Yaari 1987) as special cases. To the best of our knowledge, when applied to finite domains of payoffs, our notion of strong comparative risk aversion has not been hitherto characterized under expected utility, dual expected utility, or general rank-dependent utility. Building on Chateauneuf et al. 2005, we provide the missing characterizations (see Theorem 1, together with Corollaries 1 and 2). Those are the main results in our paper. As regards our notion of weak comparative risk aversion, it has already been characterized under expected utility (Peters and Wakker 1987), but not under dual expected utility or rank-dependent utility, to the best of our knowledge. We provide partial, yet instructive results on weak comparative risk aversion under dual expected utility (the non-necessity result in Observation 2) and general rank-dependent utility (the sufficiency result in Observation 3). Furthermore, bringing together these new results and preexisting ones, we reach two novel conclusions (Facts 2 and 3). First, under expected utility, weak and strong comparative risk aversion are characterized by the same condition, which is not the case under non-expected utility. This conclusion holds not only when the utility functions have convex range, which was already known (e.g., Chateauneuf et al. 1997), but also when the utility functions have finite range, which had not been hitherto established. Second and more novel, under expected utility, weak (respectively: strong) comparative risk aversion is characterized by the same condition when the utility functions have finite range and when they have convex range (alternatively, when the payoffs are numerical and their domain is finite or convex, respectively). By contrast, such is not the case under non-expected utility. The latter kind of comparisons had not been hitherto studied in the literature, to the best of our knowledge. Accordingly, it is worth making immediately clear that it does not reduce to the former. Weak and strong comparative risk aversion could be characterized by the same condition both in the finite and in the convex case, yet the common condition differ across these two cases. Conversely, weak and strong comparative risk aversion could be characterized by different conditions both in the finite and in the convex case, yet the distinctive condition for each attitude remain the same across these two cases. The take-home message from our conclusions on these two distinct issues is that like absolute risk attitudes, comparative risk attitudes help better understand the fundamental divide between expected and non-expected utility, more generally, the structural properties of the main models of decision-making under risk. We consider this general conceptual insight, which we explain to raise several interesting questions for further research, to be the main contribution of our investigation of risk attitudes over finite domains. The rest of the paper is organized as follows. Section 2 gathers some necessary preliminaries. Section 3 presents the notion of increasing risk on which our analysis relies. Section 4 introduces our notions of weak and strong comparative aversion. Section 5 contains our main results and their discussion. Section 6 concludes.",
93.0,3.0,Theory and Decision,18 January 2022,https://link.springer.com/article/10.1007/s11238-021-09857-6,A theory of unanimous jury voting with an ambiguous likelihood,October 2022,Simona Fabrizi,Steffen Lippert,Matthew Ryan,Female,Male,Male,Mix,,
93.0,3.0,Theory and Decision,09 January 2022,https://link.springer.com/article/10.1007/s11238-021-09863-8,The effect of unconditional preferences on Sen’s paradox,October 2022,Keith L. Dougherty,Julian Edward,,Male,Male,Unknown,Male,"2020 marked the fiftieth anniversary of Sen’s seminal work “The Impossibility of a Paretian Liberal” (Sen, 1970). Unlike other works that demonstrate a paradox of voting (Arrow, 1951; Gibbard, 1973; Satterthwaite, 1975), Sen’s paradox illustrates a fundamental conflict between freedom and efficiency. Sen shows that efficient choice, as reflected by the unanimity of the Pareto criterion, and liberty, as captured by his liberalism conditions, fail to produce a most preferred outcome for society over an unrestricted domain. Put differently, Sen shows that it is impossible to have a consistent social choice if the rule simultaneously protects a specific type of liberty, the Pareto principle, and an individual’s ability to rank their preferences in any order they please. Google scholar lists more than 1700 citations of Sen’s classic work, with more than 700 of those citations appearing in the last 10 years. Scholars have investigated ways of relaxing Sen’s conditions (Farrell, 1976; Piggins & Salerno, 2016; Sen, 1979; Saari, 2008); made his notion of liberty conditional (Blau, 1975; Gibbard, 1974; Saari & Petron, 2006); claimed that his liberty condition, or his treatment of preferences, are not consistent with common conceptions (Barry, 1986; Chung, 2019; Dowding, 2016; Munger, 2008; Risse, 2001); or generalized his result (Hammond 1998; Igersheim, 2013; List, 2004; Schwartz, 2018; Sher, 2018). See Suzumura (2011) for a review of such works. Our paper makes both analytic and numeric contributions to this literature. On the analytic side, our paper refines the analysis of Gibbard (1974), and his notion of Unconditional Preferences. In our use of the term, an individual expresses conditional preferences if her preferences depend on the actions of another individual or a non-private, environmental condition. For example, Anne has conditional preferences if she wants to wear a shirt that is the same color as Bob’s. Bob has conditional preferences if he wants to wear a shirt that differs in color from Anne’s. The two would have unconditional preferences if they preferred a shirt color independent of the colors worn by others. We show that if individual preferences for a private attribute are not conditioned on the private actions of others, then in the absence of Pareto, there can be no cycles. Furthermore, if individuals have a preference for a private attribute that is independent all of other attributes, private or non-private, then there can be no cycle with or without Pareto. Under such conditions social decision functions exist that are consistent with liberalism and Pareto. We also determine the probability of a cycle assuming a third, weaker independence condition. This probability converges to one rapidly as the number of non-private attributes in the social states increases, suggesting that if the weaker form of independence holds, then Sen’s paradox may be likely. We introduce the concept of “levels” to account for variation in the non-private dimensions. Points within levels differ by the private attributes of at least one individual, while points across levels differ by environmental conditions for which individuals may or may not condition their preferences. This allows us to easily examine degrees of unconditional preferences that are independent of private attributes alone or both the private and non-private attributes. On the numeric side, we use simulations to determine the magnitude of the probabilities for best elements, maximal elements, and transitivity separately. Assuming preferences are drawn according to a new partially conditional preference distribution, we find that the probability of a cycle is large even in cases with few individuals and relatively few social states. We conclude that Sen’s conundrum is not only possible, across a domain of all preference profiles, it may be highly probable—particularly in settings with conditional preferences or a large number of social states.",1
93.0,3.0,Theory and Decision,06 January 2022,https://link.springer.com/article/10.1007/s11238-021-09859-4,Matching with contracts: calculation of the complete set of stable allocations,October 2022,Eliana Pepa Risma,,,Female,Unknown,Unknown,Female,"This paper deals with a general two-sided many-to-many matching model with contracts. Closely related settings are the many-to-one matching model with contracts introduced by Hatfield and Milgrom (2005) and the many-to-many matching models studied , e.g., in Roth (1984, 1985), Sotomayor (1999), Echenique and Oviedo (2006) and Blair (1988). All of them can be considered to be special cases of the model studied here. In this setting, there is a bilateral market, whose disjoint sides are typically referred to as doctors and hospitals, and all these agents have preferences over sets of bilateral contracts involving them. Here, doctors and hospitals do not only choose multiple partners from the opposite side of the market, or partners and wages, but they can agree on further characteristics of the relationships through contracts. The sets of doctors, hospitals and contracts are finite. As in previous studies of this model, such as Pepa (2015) and Hatfield and Kominers (2017), a given doctor and hospital are allowed to sign multiple contracts with each other. This is a difference with the model considered by Klaus and Walzl (2009), where each pair of agents can sign one contract together at most. A discussion about the implications of this difference can be found in Hatfield and Kominers (2017). So, a solution or allocation is a set of bilateral contracts to be signed. Since two agents wishing to sign a contract are always free to do it, and also the agents are free to unilaterally terminate previous contracts, we focus on the stable allocations, i.e., allocations that are sustainable over time, supposing the market remains unchanged. There are several concepts of stability. In fact, Klaus and Walzl (2009) consider different notions of setwise stability for many-to-many matching markets with contracts and provide an analysis of the relations between the resulting sets of stable allocations for general, substitutable, and strongly substitutable preferences. They also introduce a weak setwise stability as a new stability concept and prove that for substitutable preferences the set of pairwise stable allocations is nonempty and coincides with the set of weakly setwise stable allocations. In this paper, we will assume that all agents have substitutable preferences and restrict our attention to pairwise stable allocations. The fact that substitutable preferences are sufficient for the existence of pairwise stable allocations is also proved by Pepa (2015) through a constructive proof, and by Hatfield and Kominers (2017). Other results from previous literature which are relevant for the development of the present work, are those regarding the structure of the set of stable allocations. Hatfield and Kominers (2017) construct a generalized deferred acceptance operator defined over the cartesian product of the power set of the set of all existent contracts with itself, and show that its fixed points correspond to stable allocations; then, they use Tarski’s fixed point theorem (Tarski 1955) to show the existence of a nonempty lattice of fixed points. They verify that the correspondence between fixed points and stable allocations is one-to-one and obtain, as a consequence, the lattice structure for the set of stable allocations. However, in this work, we will rely mainly on the results obtained in this respect by Pepa (2015). Since a suitable adaptation in terms of contracts of the model by Blair (1988) becomes a particular case of the many-to-many matching with contracts model that we consider here, an extension of Blair’s results can be expected. In fact, Pepa (2015) introduces respective procedures obtaining the meet and join operators between any pair of stable allocations, relative to Blair‘s partial orders (redefined in terms of contracts). In this way, it is formally proved that the set of stable allocations has a lattice structure with respect to the previously mentioned partial orders. Moreover, it is shown that such lattices are dual, implying the existence of certain counterposition of interests between both sides of the market. These findings are crucial to prove our results. In the present paper we develop an algorithm to compute the full set of stable allocations in the setting described before. McVitie and Wilson (1971) were the first in obtaining an algorithm to compute the complete set of stable matchings for the marriage model; then, Martinez et al. (2004) extended such result to a many-to-many matching model without contracts with substitutable preferences. Our algorithm is inspired in the last one, although it has its own particularities. In fact, we show through an example that the natural extension of the algorithm provided in Martinez et al. (2004) to a model with contracts fails to find all stable allocations even in a very simple market. A potential use of this kind of algorithms is the generation of conjectures and counterexamples to deepen the knowledge about the stable outcomes in the corresponding models. It is also a contribution to give a direct procedure that allows to compute stable allocations which does not prioritize one side of the market, unlike the deferred acceptance algorithms. The procedure starts by obtaining both optimal stable allocations. The algorithm works independently of the method used to calculate the optimal stable allocations; however, we propose suitable deferred-acceptance algorithms to perform these computations. Then, each remaining stable allocation can be obtained by finding the optimal-for-hospitals stable allocation in a reduced market, where some contracts have been removed. This paper is organized as follows. In Sect. 2, we introduce the model, definitions, notations and some basic properties. Section 3 contains the main finding: we formally define the algorithm to compute the entire set of stable allocations. We prove that it works in the general model with contracts considered here. Moreover, we provide an example of its application and illustrate the need to modify the process introduced in Martinez et al. (2004) when working with contracts. The appendix contains the description of two symmetrical deferred-acceptance algorithms introduced in Pepa (2015). There we explain how these algorithms were used in such paper for computing the optimal stable allocations and for proving the results concerning the lattice structure of the set of stable allocations.",1
93.0,3.0,Theory and Decision,01 December 2021,https://link.springer.com/article/10.1007/s11238-021-09850-z,Classification by decomposition: a novel approach to classification of symmetric \(2\times 2\) games,October 2022,Mikael Böörs,Tobias Wängberg,Marcus Hutter,Male,Male,Male,Male,"What makes a game such as the Prisoner’s Dilemma interesting? It is the tension between the common interest and self-interest, we argue in this paper. Indeed, in the Prisoner’s Dilemma, the players’ self interest directly opposes the common interest, often leading to mutual defection and tragedies of the commons. Meanwhile, in Stag Hunt, the conflict part pulls the players away from mutually more beneficial outcomes by counteracting the players’ common interest. The Leader and Hero games, also referred to as the symmetric Battle of the Sexes, exhibit yet another kind of tension, where the most payoff can be gained from alternating between outcomes in repeated games. Unfortunately, the conflict part makes such cooperation more difficult. Based on these observations we hypothesize that games with different tensions between common interest and self-interest should be interesting in different ways, and that differences in tensions is what distinguishes the standard games from each other. Indeed, all standard games (Prisoner’s Dilemma, Chicken, Stag Hunt, Leader and Hero) exhibit different tensions between common interest and self-interest. To explore this idea further, we establish a simple way to enumerate the possible tensions, and consider the classification of the space of \(2\times 2\) games that these tension classes induce (Sect. 4). Analysis of the regions show that they correctly separate the standard games, and make several further distinctions previously considered in the literature (Sect. 5). Computer experiments of iterated versions of the games give preliminary empirical supportFootnote 1 for our hypothesis that tension classes separate the space of symmetric \(2\times 2\) games in strategically cohesive classes (Sects. 6 and 7). A well-established scientific principle (e.g. Baker, 2007) says that a good scientific hypothesis should be:  simple and parsimonious, and explain relevant observations. Simplicity is important, as simple scientific theories have a much better track record, and tend to generalize better (cf. Occam’s razor). In particular, classifications should avoid adding conditions ad hoc, as this increases complexity and tends to reduce generalisability. As for 2., a hypothesis that does not explain the relevant observations is either false or too vague. In the case of game classifications, a good classification hypothesis will separate significantly different games into different classes, while keeping similar games in the same class. Similarity between games has not been formally defined, but a consensus has emerged around which games should be grouped together and not (Harris, 1969; Huertas-Rosero, 2003; Rapoport et al., 1978). Previous classifications have arguably failed to simultaneously satisfy both 1. and 2. Several works have suggested similar classes of games (Harris, 1969; Huertas-Rosero, 2003; Rapoport et al., 1978), but without basing the groupings on a simple principle. They therefore fail to meet 1. Other works have classified games based on simple mathematical principles (Borm, 1987; Robinson & Goforth, 2003), but with less convincing classes as a result, thereby failing to satisfy 2. Section 3 discusses these works in more detail. In this paper, we argue that our classification based on the tension between common interest and conflict yields the right classes based on a simple principle, thereby satisfying both 1. and 2. Outline. We begin with some background (Sect. 2), followed by a review of previous classifications of \(2 \times 2\) games (Sect. 3). Focus is then shifted to our classification, defined in Sect. 4. The resulting regions are analyzed in Sect. 5. Computer experiments based on genetic algorithms provide preliminary empirical support for our classification (Sects. 6 and 7). We conclude the paper by summarizing our findings and presenting some open questions for further research (Sect. 8).",
93.0,3.0,Theory and Decision,16 January 2022,https://link.springer.com/article/10.1007/s11238-021-09858-5,Buck-passing dumping in a garbage-dumping game,October 2022,Takaaki Abe,,,Male,Unknown,Unknown,Male,"A bad is a commodity or an object that causes disutility to its owner. Typical examples of bads include garbage, industrial waste, and pollutants. For example, Philippines president Duterte ordered his government to send 69 containers of garbage back to Canada in 2019. Canada said that the garbage dumped to the Philippines was a commercial transaction which was exported without the government’s consent. Another example of garbage-dumping problem is garbage-disposal in Tokyo. In 1970s, most garbage produced in all 23 wards in Tokyo was dumped onto Koto ward, which is also one of the 23 wards. This concentration of garbage caused serious pollution in Koto ward. Moreover, Cramton et al. (1987) associate garbage-dumping problem with the framework of mechanism design and theoretically address a question: “When some municipalities jointly need a hazardous-waste dump, which town should provide the site and how should the others compensate?” This paper seeks to address another question: “Why does buck-passing dumping behavior exist everywhere?” More specifically, why do a small number of individuals or nations receive and dispose of a large quantity of bads? We attempt to answer this question in terms of stability and dumping strategy. Therefore, we first need a model for people’s dumping behavior. One of the pioneering models that formally deal with bads is the garbage disposal game introduced by Shapley and Shubik (1969). They assume that each player has a bag of garbage, namely, an initial endowment of bads. Each player dumps his or her bads into someone’s yard. Shapley and Shubik (1969) model this situation as a cooperative game with transferable utility, in which if a coalition S of players is formed, then the players outside S dump all of their bads to coalition S, and the members of S similarly dump their bads to the outside players. Therefore, the quantity of bads the players in S have to dispose of is the sum of all bads dumped by the outside players to S. If the coalition of all players is formed, they dispose of all bads by themselves. Shapley and Shubik (1969) show that the core of this game is empty. Hirai et al. (2006) focus on strategic dumping. They replace goods by bads in the pure exchange game of goods analyzed by Scarf (1971). Therefore, a strategy of each player is to distribute his or her bads over the players. In a pure exchange game of goods, keeping all initial endowments is a dominant strategy for every player. However, if players are endowed with bads, keeping bads is not a rational strategy, and dumping all of one’s bads to someone else is a dominant strategy. Hirai et al. (2006) show that if the number of types of bads is one (e.g., garbage), then cycle dumping in which each player dumps his or her bads to the next player is a strong Nash equilibrium for any ordering of players. Moreover, the authors offer a sufficient condition that an initial distribution of bads should satisfy for self-disposal to be an \(\alpha \)-core element. Given that we are interested in players’ dumping behavior, we should focus on “who dumps bads to whom” in a model. Therefore, we use the model proposed by Hirai et al. (2006) and consider a strategy to be a distribution of one’s initial bads. Moreover, we formally introduce the notion of a dumping function. A dumping function describes a dumping behavior/policy that a player consistently follows for all initial distributions of bads: a dumping function assigns a strategy profile to every initial distribution of bads. Introducing this notion enables us to regard players’ dumping behavior as a class of dumping functions and formally analyze its properties. Besides the model, what stability notion should we use to analyze a strategy profile generated by each dumping function? There are six stability notions that have been widely accepted in the literature: \(\alpha ,\beta ,\gamma ,\delta \)-cores, strong Nash equilibrium (SNE), and coalition-proof Nash equilibrium (CPNE). The \(\alpha \)-core (\(C^\alpha \)) and the \(\beta \)-core (\(C^\beta \)) concepts were proposed by Aumann and Peleg (1960) to model the payoffs that deviating players can achieve independent of the reaction of non-deviating players. These concepts can be regarded as stability notions that describe cautious players: when the players deviate, they expect the worst reaction, that may minimize the payoff of the deviating players, from the non-deviating players. In the above concepts, non-deviating players are assumed to react to minimize the deviating players without considering their own preferences. Chander and Tulkens (1997) and Currarini and Marini (2004) propose the \(\gamma \)-core (\(C^\gamma \)) and the \(\delta \)-core (\(C^\delta \)) concepts, respectively, in which the non-deviating players are assumed to behave rationally to increase their own payoffs or, at least, do not dare decrease their own payoffs to decrease the deviating players’ payoffs. Fortunately, the following equivalences hold for every pure exchange game of bads:Footnote 1 \(C^\alpha =C^\beta \), \(C^\gamma =C^\delta =\emptyset \), \({SNE}={CPNE}\). Therefore, we focus on the concepts of \(\alpha \)-core and strong Nash equilibrium in this paper. The definitions of these two stability notions are provided in Sect. 2.Footnote 2 In this paper, a stable profile or an equilibrium does not necessarily mean a desirable profile, because if a dumping profile in which a small number of players suffer a large quantity of bads is stable, then stability prevents individuals and coalitions from splitting off from the dumping profile. In contrast, if a dumping profile refers to an acceptable profile, such as self-disposal, then we might consider stability to be a desirable property. The rest of this paper is organized as follows. In Sect. 2, we introduce the model of a pure exchange game of bads and the definitions of \(\alpha \)-stability and strong Nash equilibrium. The proposition of Hirai et al. (2006) is also discussed. In Sect. 3, we discuss the dumping functions that generate an \(\alpha \)-stable strategy profile for every exchange game of bads. In Sect. 4, we show that introducing the second exchange may facilitate self-disposal. We conclude this paper by summarizing our results and proposing a direction for future research in Sect. 5. All proofs are provided in the appendix.",
93.0,3.0,Theory and Decision,15 January 2022,https://link.springer.com/article/10.1007/s11238-021-09852-x,Should I stay or should I go? Congestion pricing and equilibrium selection in a transportation network,October 2022,Enrica Carbone,Vinayak V. Dixit,E. Elisabet Rutstrom,Female,Unknown,Unknown,Female,"When imposing traffic congestion pricing around downtown commercial centers, there is a concern that commercial activities will have to consider relocating due to reduced demand. Concerns like these were important aspects of the debates before the introductions of congestion charges in both London and Stockholm and influenced the final policy design choices (Eliasson et al., 2009; Safirova et al., 2006). Relocations to city perimeters can be both risky and costly since the effects of the congestion charges are uncertain, and additional changes to the charges may occur over time requiring additional relocations. City center merchants who underestimate the drop in demand and remain in the center will face sales losses, as will merchants who overestimate the drop in demand and relocate to the perimeter unnecessarily. Analyses of the experiences in London and Stockholm find no evidence that economic activities were reduced on average after the introduction of congestion charges. However, there is evidence that particular activities that rely on consumers who travel by car and who cannot shift to shopping at hours outside of charging hours were negatively affected. In London, only a very small portion of consumers relied on car travel, and in Stockholm, consumers were able to shift to times outside of the charging hours, but this is not the case in many other cities. A complementary approach to analyzing field data from past implementations of congestion pricing is to use experiments. While analyzing field data is a better tool for looking at net effects in complex situations, such analyses are limited to the specific circumstances of the cases studied. Many factors influence net results, all of which are not observed so that causal factors cannot fully be inferred. Theory-guided experiments can provide universally applicable insights into some causal forces that are at play. Limitations on lessons from experiments arise from the particular assumptions and parameter values that are used in the experimental design. The present experiment is focused on the effect of changes in congestion charges on merchants that interact with consumers who have to travel by car to do their shopping. This is particularly relevant for cities with more limited urban transit systems, such as many cities in the US or in some less developed countries. We are interested in the reactions to changes in congestion charges and how these reactions depend on past experiences. We model a sequential game where merchants make location decisions first and consumers then choose where to shop. In the initial phases of the game, consumers have dominant strategies that differ across treatments. In one treatment, the congestion charge is relatively high and the dominant strategy is to shop outside of the congestion zone, on the perimeter. In the other treatment, the congestion charge is zero and the dominant strategy is to shop in downtown. In a final phase, which is the same across treatments, the congestion charge is intermediate with multiple Nash equilibria where merchants are indifferent between the equilibria, and consumers’ best responses are to follow the merchants. We are interested in how the merchants and shoppers react to the introduction of the congestion charge when it is initially zero, and then compare that to reactions when the charge is reduced from a higher level. One of the equilibria is for merchants to not relocate and for consumers to continue to shop from the same merchants, thus not reducing demand. Thus, despite our game being based on consumers who drive, it is possible for the experiment to show no effect on retail activities from the introduction of the congestion charge. There are several behavioral possibilities that can affect the equilibrium selection apart from remaining in the initial equilibria. One possibility is that play converges on the one equilibrium which is efficient, the perimeter one. Alternative hypotheses arise in our experiment due to payoffs being non-linear and non-transparent, generating outcomes with uncertain values. This can lead to players either constructing or discovering preferences as they play the game. In such cases, Tversky and Kahneman (1974) propose that psychological anchoring behaviors can occur. If consumers anchor on past prices, as investigated by Sitzia and Zizzo (2012), it can lead to reduced demand adjustments following a price change, such that for any given final price a higher quantity demanded will result when consumers have previously experienced higher price than when they have experienced lower prices. Consumers’ perception of the value of the good is influenced by past prices. Anchoring behavior seems particularly prevalent when consumers have unclear preferences or perceive a product as having an uncertain value (Sitzia and Zizzo, 2012). In such cases, the revealed preferences would be quite sensitive to the framing of the task (Slovic and Lichtenstein, 1983) or to priming from past experiences, even when the past situation is no longer relevant (Ariely et al., 2003). Cooper and Kraker Stockman (2011) find evidence that experimental participants are discovering their preferences and that these are influenced by priming in a prior task. Our observations lead us to reject that merchants choose locations randomly despite being indifferent between equilibria, and to reject that merchants do not relocate after the charge is imposed. Most of the consumer decisions are to follow merchants. Our findings show that immediate reactions to the change in congestion charges follow a standard inverse price–demand relationship. When the price of entering downtown increases, it reduces commercial activities in downtown and when the price of entering downtown decreases, it increases commercial activities in downtown. The reaction is quite strong in our main treatment with increasing price. The fact that merchants react to the price is interesting since merchants are indifferent between staying in the initial equilibrium or changing to a new equilibrium, and consumers’ best responses are to follow the merchants. We do see dynamic behavior that leads to a convergence on the efficient perimeter equilibrium in the second phase of both treatments. Consumers are signaling their preferences for the perimeter location, and merchants respond to those signals. The adjustment process that is caused by the change in the congestion charge is costly to merchants, consistent with the concerns expressed in the debates about congestion pricing. We do not find any systematic evidence of price anchoring, where consumers’ valuations are influenced by past prices, as in Sitzia and Zizzo (2012)). Such anchoring would be expected to increase activities inside the congestion charging zone in the treatment where price is decreasing relative to the treatment where price is increasing. Instead, we see the opposite, a higher proportion of activities inside the congestion charging zone in the treatment with a price increase. In this treatment, players have prior experience with a dominant strategy equilibrium inside the charging zone. This finding is consistent with a portion of our players being influenced by priming: the unique downtown equilibrium in the first phase of the game has a lingering influence on equilibrium selection in the second phase of the game. Since Singapore introduced congestion charging zones in 1998, several other cities have followed: London in 2003, Stockholm in 2006, Milan in 2008, Bergen in 2016 and Oslo in 2017. The charging schemes in London and Stockholm have been well analyzed. London introduced a £5 cordon charge in February 2003 with an increase in the rate charged to £8 in July 2005 £8. The charging window was set to 7 am until 6.30 pm. The effect on traffic after the initial cordon was put in place was an 18% reduction and no evidence of increases in traffic outside of the charging hours or in the area immediately surrounding the charging zone. There was no sign that this effect weakened over the following years (Transport for London, 2006). Stockholm introduced a 6-month trial period of congestion charging around the city center in 2006, followed by a permanent installation in 2007. There are three charging windows with differentiated rates, plus periods with no charge. The charges were initially set to achieve a 15% reduction in the traffic volume but the result was a full 22% reduction which has been sustained in the long run. A temporary removal of the charge system after the 6-month trial quickly returned traffic to close to previous levels, showing how quickly traffic flows can change. Both average travel times and the dispersion of travel times were reduced by 1/3 to 1/2 due to the charges (Eliasson, 2014; Eliasson et al., 2009). This demonstrates that congestion charges can be effective ways of reducing congestion in downtown areas. In both London and Stockholm prior to the introduction of congestion charging, there were expressed concerns about possible detrimental effects on the commercial sector, and especially the retail sector, in the city centers. Subsequent analyses ex post of introducing the charging show no, or little, evidence of net effects on the economy as a whole or on the retail sector as a whole (Daunfeldt et al., 2009, 2013; Quddus et al., 2007a, b; Transport for London, 2006). However, these net effects are a result not only of the congestion charges themselves, but also of a set of complementary policies improving public transit as well as larger social and economic changes during the same time. The lack of effects in London is likely associated with the very small proportion of shoppers in the city center who travel there by car. Quddus, Carmel, and Bell (2007a, 2007b) report that only 3–6% of shoppers in London travel by car. The lack of effects in Stockholm is likely associated with the longer opening hours of downtown stores allowing car-borne shoppers to do so outside of charging hours (Daunfeldt et al., 2009, 2013). In addition, the expressed concerns about effects on the retail sector directly affected the design of the congestion charges so as to minimize any such effects. Nevertheless, a few pieces of evidence speaks to a direct effect of congestion charges on retail in city centers: a significant decrease in sales for a particular store in central London which has a high proportion of shoppers who travel by car (Quddus et al., 2007a, b), and a 17% drop in travel for shopping purposes to central Stockholm during the trial portion of the congestion charging (Smidfeldt-Rosqvist et al. (2006)). That this reduction in Stockholm did not affect retail revenues in a significant way could be due to the fact that opening hours extend well beyond the congestion charging range. Further evidence of negative effects on retail is found in Singapore which experienced a 19% drop in retail real estate prices within the charging zones relative to outside of these zones (Agarwal et al., 2015). Our experimental findings that, in cities where consumers travel by car, there are behavioral forces that cause shifts of commercial activities and adjustments over time, complement and add to this evidence. The paper is organized as follows. The next section describes our experimental game and the predictions we make assuming payoff maximizing behavior. Section 3 discusses our findings, Sect. 4 relates to the experimental literature and Sect. 5 concludes.",
93.0,3.0,Theory and Decision,27 November 2021,https://link.springer.com/article/10.1007/s11238-021-09853-w,On the economic foundations of decision theory,October 2022,Aldo Montesano,,,Male,Unknown,Unknown,Male,"In economics, the choice theory is normally represented by assuming that the decision-maker's preferences on his/her possible actions are described by an ordinal utility function. The economic theory of choice aims not to determine preferences, which the theory can assume as “given”, but the economic state determined by the choices of decision-makers. Preferences can be any. Their representation employing a utility function is a convenient hypothesis to make rational analyses, maximizing the utility function on the set of actions among which the decision-maker can choose. It is a convenient hypothesis similar to that one employed in celestial mechanics where planets are represented as material points, not as dimensional bodies as they are. What matters is to use assumptions that produce good means to represent chosen actions. The use of the utility function excludes certain types of preferences and it could be said that their exclusion denoted the bounded rationality of the theorist. If the observed actions are in contrast with the results generated by the theory, the theorist must modify the hypotheses on which the theory is based to obtain a good approximation to the observable reality. To obtain the convenient representation of preferences provided by the ordinal utility function it is necessary to assume that the preferences have certain characteristics that can qualify them, in some respects, as rational. The main rational aspect involved by the utility function is transitivity (i.e., if action a is preferred by the decision-maker to action \(a^{\prime}\) and \(a^{\prime}\) to \(a\)”, then action a is preferred to action \(a\)”), which excludes the irrational behavior considered by the money pump argument. However, this rationality of the preferences does not stem from a hypothesis that requires decision-makers to be rational, but only from the convenience of having a practical theory to use. In other words, it is assumed that the transitivity of the preferences makes the work of the theorist easier, not that the theorist believes that the decision-makers are, or should be, rational. The preferences can be imagined as the result of mental experiments and be made up of the answers to the question of what alternative or set of alternatives is chosen among those proposed to the decision-maker, an experiment to be carried out from each possible set of actions to choose from. In mathematical terms, \({\mathcal{B}}\) indicating a non-empty collection of non-empty sets B of actions that may be the object of choice, so that the set of all actions taken into account is \(A = \cup_{{B \in {\mathcal{B}}}} B\), the mental experiment associates with each \(B \in {\mathcal{B}}\) the set of actions \(c(B) \subseteq B\) that the decision-maker, having to choose from the actions belonging to B, is willing to do. This introduces the choice correspondence \(c:B \to B\). In other words, the set \(c(B) \subseteq B\), which is composed of all those actions that the decision-maker is willing to choose when he has to choose an action in B, can be determined by imagining the mental experiment consisting of asking the decision-maker: ""Being able to make one of the actions contained in B, is it okay to take the action \(a \in B\)?"", and asking this question for each action \(a \in B\). The set \(c(B)\) brings together all the actions \(a \in B\) for which the response is positive. As already indicated, the approach followed in this paper, which is the approach followed by current economic theory, does not intend to give an explanatory theory of choices, that is, it does not intend to show which are the factors that generate the choices (which can be psychological, habitual, the effect of reasoning, superstition, etc.), but only to represent choices.Footnote 1 That is, this economic approach aims to produce a representation of the choice correspondence convenient for the economist to use. This representation is limited to cases that satisfy certain conditions (such as those indicated in Sect. 1.1). There is, therefore, a notable difference in comparison to theories whose goal is to explain the nature of the choices, not just to represent them. The explanatory theory of the choices, which was deferred by Pareto to sociology (as well as any evaluation, including the social welfare function), is the task of other scientific fields, including especially psychology. The approach with a “given” choice correspondence allows having analyses for determining the state of the society (the economic state is represented by prices, exchanges, production, incomes, etc.) which adopt the same means to describe the choices of different decision-makers and those referring to different situations. Economic analysis can be made more specific, if necessary, also introducing results obtained by other sciences including psychology. The pure representation of choices makes not relevant, for instance, the difference between situations of certainty and those of uncertainty in the representation of an agent's choices through his/her ""given"" preferences. It is not even, in general, necessary to introduce the notion of probability to determine the choice under uncertainty (see Debreu, 1959 for the choice of contingent goods, as indicated in Sect. 2.1). However, a kind of probability can be deduced from the decision-maker’s preferences through his/her reservation price for the purchase or sale of an act to which an uncertain outcome is assigned (through the notion of price-probability, Montesano, 2019). In this case, the probability is detected by given preferences, it is not one of the factors used by the theory to generate preferences. In the approach followed in this paper, uncertainty is highlighted only when the information possessed by the decision-maker is represented, information that may include the presence of uncertainty (as done in Sect. 2), and it is desired to describe his/her preferences taking into account the consequences associated with each action. In very concise terms, there are two approaches to propose a theory: the rational approach and the empirical one. The rational approach employs the axiomatic method, whereby the relations describing reality are generated by a logical deduction using hypotheses (axioms) that are considered valid for the ""given"" situations to which they are applied. The theoretical relationships so determined are tested with empirical observations and if the result is unsatisfactory, the assumptions are modified. The empirical approach makes observations and/or carries out experiments from which it derives relations that are extended to unobserved situations. If this extension proves unsatisfactory, then it proposes less specific relationships to also include situations previously not observed or it distinguishes between different types of situations, establishing different relationships for each type. This paper deals with the first approach, the rational one (an adjective that concerns the theory, not what is examined by the theory, as are the decision-makers). The second approach is followed by behavioral economics. The two approaches are at the same time alternative and complementary. The first approach modifies the proposed decision theory if it is refuted by the observed behavioral reality. The second approach starts from observed behaviors and introduces rational (that is, non-contradictory) relations based on them without deducing these relations from axioms. However, these relations could also be deduced using axioms. Moreover, even if psychology and other sciences can be useful to suggest explanations for the choices of decision-makers, this paper excludes these explanations because they are not necessary to represent choices and any of them can be substituted by other explanations. The preferences on the set of actions A, that are implied by the choice correspondence \(c:B \to B\) with \(B \in {\mathcal{B}}\) and \(A = \cup_{{B \in {\mathcal{B}}}} B\), can be represented, under some conditions, by a binary preference ordering \(\left\langle {A,{ \succsim }} \right\rangle\), using symbols \({ \succsim }\) for weak preference, \(\succ\) for strong preference, and \(\sim\) for indifference. If \(a \in c(B)\), it is \(a\)\({ \succsim }\)\(a^{\prime}\) for every \(a^{\prime} \in B\), it is \(a \succ a^{\prime}\) for every \(a^{\prime} \in B{\backslash }c(B)\) and it is \(a \sim a^{\prime}\) for every \(a^{\prime} \in c(B)\). The binary ordering \(\left\langle {A,{ \succsim }} \right\rangle\) can be represented by a utility function \(u:A \to {\mathbb{R}}\), and this function is continuous if the choice correspondence meets some conditions. The relevant conditions are as follows. The choice correspondence is decisive. This condition requires that the correspondence \(c:B \to B\) have \(c(B) \ne \emptyset\) and \(c(B) \subseteq B\) for every \(B \in {\mathcal{B}}\). The empty set symbol \(\emptyset\) indicates the refusal to make a choice (i.e., the refusal of the decision-maker to take part in the mental experiment carried out by the observer), not the refusal of the decision-maker to change his initial situation. This action (that is, not to change the initial situation) can be indicated with \(a_{0}\) and could belong to set \(B\) and, if it is the case, also to \(c(B)\). The condition \(c(B) \subseteq B\) requires that each chosen action belongs to the set of those actions that can be chosen, that is, that the decision-maker does not declare that he wants to make an impossible action. The choice correspondence satisfies (weak and strong) revealed preference axioms. The weak axiom requires, for every pair of actions \(a,a^{\prime} \in B \cap B^{\prime}\) with \(a \in c(B)\) and \(a^{\prime} \in c(B^{\prime})\), that it is also \(a \in c(B^{\prime})\) and \(a^{\prime} \in c(B).\) That is, if \(a \in c(B)\) and \(a^{\prime} \in B\), then there must be no set \(B^{\prime} \in {\mathcal{B}}\) for which \(a^{\prime} \in c(B^{\prime})\) and \(a \in B^{\prime}{\backslash }c(B^{\prime})\), which means that, if a is found weakly preferred to \(a^{\prime}\) since \(a \in c(B)\) and \(a^{\prime} \in B\), then \(a^{\prime}\) cannot be detected strongly preferred to a, that is, it is not \(a^{\prime} \in c(B^{\prime})\) and \(a \in B^{\prime}{\backslash }c(B^{\prime})\). The strong axiom requires that the weak axiom applies and, for every sequence \(a_{1} ,a_{2} ,...,a_{m}\) with \(a_{i} \in c(B_{i} )\) for \(i = 1,...,m\) and \(a_{i} ,a_{i + 1} \in B_{i} \cap B_{i + 1}\) for \(i = 1,...,m - 1\), that there is no set \(B^{\prime} \in {\mathcal{B}}\) containing a pair of actions \(a_{1} ,a_{m} \in B^{\prime}\) such that \(a_{m} \in c(B^{\prime})\) and \(a_{1} \notin c(B^{\prime})\), that is, that if \(a_{i}\) is found weakly preferred to \(a_{i + 1}\) for every \(i = 1,...,m - 1\), then \(a_{m}\) cannot be detected strongly preferred to \(a_{1}\). If these two conditions are satisfied, then the choice correspondence \(c:B \to B\) agrees with a binary preference ordering \(\left\langle {A,{ \succsim }} \right\rangle\) that is complete and transitive.Footnote 2 It is immediately clear that the preference ordering can be represented by an ordinal utility function \(u:A \to {\mathbb{R}}\) if the set \(A\) is countable. (A set is countable if every point can be associated with a natural number that differs from those associated with the other points in the set). A utility function is ordinal if we are interested in classifying actions only in their order, so actions with equal utility have the same position in the ordering and those with higher utility have a higher position. This implies that every increasing monotone transformation of a utility function is also a utility function that represents the same preference ordering.) The regular preference ordering \(\left\langle {A,{ \succsim }} \right\rangle\) is separable. This condition requires that there exists a countable set \(Z \subseteq A\) such that for every pair \(a,a^{\prime} \in A{\backslash }Z\) with \(a \succ a^{\prime}\) there is a \(a^{\prime\prime} \in Z\) with \(a{ \succsim }a^{\prime\prime}{ \succsim }a^{\prime}\). The indicated separability condition is necessary and sufficient for a regular preference ordering \(\left\langle {A,{ \succsim }} \right\rangle\) be represented by an ordinal utility function \(u:A \to {\mathbb{R}}\).Footnote 3 The (ordinal) utility function is continuous if a stronger condition than separability is assumed. The utility function is continuous if the preference ordering is continuous, that is, if it satisfies the following condition. The regular preference ordering \(\left\langle {A,{ \succsim }} \right\rangle\) is continuous. This condition requires that set A is connected and that sets \(\left\{ {a^{\prime} \in A:a^{\prime}{ \succsim }a} \right\}\) and \(\left\{ {a^{\prime} \in A:a{ \succsim }a^{\prime}} \right\}\) are closed in \(A\) for every \(a \in A\).Footnote 4 If the weak axiom of revealed preference does not apply, then the choice correspondence may not exist compatible with a binary preference ordering. For example, if \({\mathcal{B}} = \left\{ {B,B^{\prime}} \right\}\), with \(B = \left\{ {a,a^{\prime},a^{\prime\prime}} \right\}\) and \(c(B) = \left\{ a \right\}\), \(B^{\prime} = \left\{ {a,a^{\prime}} \right\}\) and \(c(B^{\prime}) = \left\{ {a^{\prime}} \right\}\), then the weak axiom of revealed preference does not apply because it is not also \(a \in c(B^{\prime})\) (and not even \(a^{\prime} \in c(B)\)) despite being \(a,a^{\prime} \in B \cap B^{\prime}\) with \(a \in c(B)\) and \(a^{\prime} \in c(B^{\prime})\). There is not, in this case, a preference ordering since \(c(B) = \left\{ a \right\}\) and \(c(B^{\prime}) = \left\{ {a^{\prime}} \right\}\) would need \(a \succ a^{\prime}\) and \(a^{\prime} \succ a\), which are inconsistent preferences. If the weak axiom of revealed preference applies, but not the strong one, then there may be a non-transitive preference ordering. For example, if \({\mathcal{B}} = \left\{ {B_{1} ,B_{2} ,B^{\prime}} \right\}\) with \(B_{1} = \left\{ {a_{1} ,a_{2} ,a_{3} } \right\}\), \(B_{2} = \left\{ {a_{2} ,a_{3} ,a_{4} } \right\}\), \(B^{\prime} = \left\{ {a_{1} ,a_{4} } \right\}\), \(a_{1} ,a_{2} ,a_{3} \in c(B_{1} )\), \(a_{2} ,a_{3} \in c(B_{2} )\) and \(c(B^{\prime}) = \left\{ {a_{4} } \right\}\), then the weak axiom of revealed preference applies, but not the strong axiom, since \(a_{1} \notin c(B^{\prime})\). The binary relationships of preference \(a_{1} \sim a_{2}\), \(a_{1} \sim a_{3}\), \(a_{2} \sim a_{3}\), \(a_{2} { \succsim }a_{4}\), \(a_{3} { \succsim }a_{4}\) and \(a_{4} \succ a_{1}\) do not satisfy the condition of transitivity since they require \(a_{1} \sim a_{2} \sim a_{3} { \succsim }a_{4} \succ a_{1}\). If the weak and strong axioms of revealed preference apply, but the resulting preference ordering is not separable, then a utility function may not exist. This is the case of a lexicographic preference ordering (that is, like that of the words in a dictionary). For example, let \({\mathcal{B}}\) be a collection of closed sets \(B \subset {\mathbb{R}}_{ + }^{2}\), that is, consisting of points \((x_{1} ,x_{2} ) \in {\mathbb{R}}_{ + }^{2}\), and let \(c(B) = \left\{ {(x_{1} *,x_{2} *) \in {\mathbb{R}}_{ + }^{2} :x_{1} * = \max_{{(x_{1} ,x_{2} ) \in B}} x_{1} , \, x_{2} * = \max_{{(x_{1} *,x_{2} ) \in B}} x_{2} } \right\}\). Then, the preferred point of B is the point with the highest \(x_{1}\), and, if there are several such points with the same \(x_{1}\), the one among them with the highest \(x_{2}\). It turns out a complete and transitive preference ordering for the set \(A = \cup_{{B \in {\mathcal{B}}}} B\), in which, for every pair \(a,a^{\prime} \in A\) with \(a = (x_{1} ,x_{2} )\) and \(a^{\prime} = (x_{1} ^{\prime},x_{2} ^{\prime})\), it is \(a^{\prime} \succ a\) if \(x_{1} ^{\prime} > x_{1}\) or if \(x_{1} ^{\prime} = x_{1}\) and \(x_{2} ^{\prime} > x_{2}\). It is, then, \(a^{\prime}{ \succsim }a\) if \(a^{\prime} \succ a\) or \(a^{\prime} = a\) and \(a^{\prime} \sim a\) only if \(a^{\prime} = a\). This preference ordering cannot be described by a utility function \(u:A \to {\mathbb{R}}\). Being \(a = (x_{1} ,x_{2} ) \in {\mathbb{R}}_{ + }^{2}\), it cannot be \(u(a) \ne u(x_{1} )\) since it is \(a^{\prime} \succ a\) for \(x_{1} ^{\prime} > x_{1}\) whatever \(x_{2}\) and \(x_{2} ^{\prime}\), and it cannot be \(u(a) = u(x_{1} )\) since it is \(a^{\prime} \succ a\) for \(x_{1} ^{\prime} = x_{1}\) and \(x_{2} ^{\prime} > x_{2}\). The economic theory of choice normally employs a binary preference ordering \(\left\langle {A,{ \succsim }} \right\rangle\) and a utility function \(u:A \to {\mathbb{R}}\) that represents the preference ordering, be the decision-maker in a condition of certainty or uncertainty (certainty means that only one consequence is associated with each action, uncertainty means lack of certainty, so that uncertainty here also includes simply risky situations). However, this hypothesis, as indicated in the preceding sections, may not be valid if the choice correspondence and the preference ordering do not satisfy some conditions (the conditions of decisiveness and the weak and strong axioms of revealed preference, for the correspondence choice, and, for the preference ordering, the condition of separability). There is no lack of experimental results that are incompatible with the hypothesis that the decision-maker's choice can be represented by a utility function.Footnote 5 However, only theories that assume the representativeness of the decision-maker's preferences through an ordinal utility function are considered in the following. What has been said so far applies both to the theory of choice under certainty and to the theory of choice under uncertainty. In both cases, if \(B \subseteq A\) is the set of alternative actions that the decision-maker can choose from and his/her preferences are represented by the continuous ordinal utility function \(u:A \to {\mathbb{R}}\), the solution to the problem \(\max_{a \in B} u(a)\), that is, \(\arg \max_{a \in B} u(a)\), determines the chosen action, or a set of actions that is equivalent for the decision-maker to choose. So far, the preference ordering and the utility function have been applied to the actions of the set \(A = \cup_{{B \in {\mathcal{B}}}} B\) without highlighting the characteristics of each action with reference to its consequences and the decision-maker's knowledge of them. In other words, it is now a question of taking into consideration the information (subjective, which is, therefore, an opinion) that the decision-maker has on the consequences of possible actions. A decision under certainty is such that the agent associates a unique consequence with each possible action. The decision determines a situation that the agent deems predefined with no possible alternatives. A decision under uncertainty is such that the agent associates various possible consequences to some actions, that is, he/she does not believe to know what will happen. Decision theories, especially those under uncertainty, are often not limited to determining the choice but are interested in representing the preference ordering to the possible consequences of every action. Under certainty, the decision-maker considers that each action corresponds to only one consequence, which is known and is achieved with certainty. Under uncertainty, the decision-maker considers that there are actions which do not correspond to a single consequence, but to a multiplicity of possible consequences, of which only one will be realized: the decision-maker may have information on these eventualities, but that is not enough to know the consequence that will take place. The theory of decisions under uncertainty is aimed at specifying the decision-maker's preferences also in relation to this information, considering several cases. That is, theories of choice under uncertainty differ, although they all identify the choice as the subset \(\arg \max_{a \in B} u(a)\), mainly because they specify differently the preference ordering and, therefore, the utility function, in relation to the information that the decision-maker owns on the possible consequences of the actions.",
93.0,4.0,Theory and Decision,09 February 2022,https://link.springer.com/article/10.1007/s11238-022-09868-x,Risk aversion and equilibrium selection in a vertical contracting setting: an experiment,November 2022,Nicolas Pasquier,Olivier Bonroy,Alexis Garapin,Male,Male,Male,Male,"A key issue for every analysis of vertical relationship frameworks with secret contracts is how retailers react to “unexpected” (i.e., out of equilibrium) offers. Their reaction to such offers depends on their beliefs about the contracts offered to their rivals. The theoretical literature usually assumes that their beliefs may take three forms: passive, symmetric, or wary (see McAfee and Schwartz 1994), and finds that the equilibrium outcomes are very sensitive to the assumptions made about beliefs.Footnote 1 Rey and Tirole (2007) show for instance that where passive beliefs support a Cournot outcome, symmetric beliefs may support monopoly outcome. Recently, Eguia et al. (2018) proposed a criterion, the Largest Set of Beliefs (hereafter LSB), avoiding restricting the set of beliefs. The selected equilibrium is then the one supported by the largest set of beliefs. By avoiding imposing a specific form of beliefs, such a criterion opens the door to an analysis of the effect of retailers’ risk aversion on the set of beliefs that support the equilibria, and hence on the equilibrium selection. Few studies have considered the role of risk aversion of retailers in vertical relationships with secret contracts, because this raises the problem of the correlation between beliefs and risk aversion. Yet it is well established that retailers can behave as if they were risk averse,Footnote 2 and that this behavior is not neutral in vertical relationships. In their seminal article, Rey and Tirole (1986) establish that retailers’ risk aversion modifies manufacturers’ strategies, the latter having to provide insurance to retailers. Thus, if contracts are public, the need to share risk hinders the manufacturer’s ability to exploit its market share. What about secret contracts? To our knowledge, only Lømo (2020) considers simultaneously a secret contract framework and risk-averse retailers, and shows that retailers’ risk aversion may mitigate the well-known opportunism problem of secret contracts. In his work, the author assumes passive beliefs. But are these beliefs plausible for risk-averse retailers? What is the link between the retailers’ risk sensitivity and the beliefs that support the equilibrium? Our work attempts to answer these questions by extending the LSB criterion to risk-averse retailers. In this paper, we consider a vertical contracting game with secret contracts, adapted from Rey and Tirole (2007), and characterize the effect of players’ level of risk aversion on the equilibrium selection. Specifically, we consider a risk-neutral supplier making secret offers to two risk-averse retailers. This game has two equilibria in pure strategy: one supported by symmetric beliefs and characterized by a high-price offer and the monopoly outcome, and one supported by passive beliefs and characterized by a low-price offer and the Cournot outcome.Footnote 3 These equilibria are standard in the literature on secret contracting (see, e.g., Rey and Tirole 2007). We then select the equilibrium by generalizing the LSB criterion so that it takes into consideration the retailers’ risk aversion. Our paper revisits the Eguia et al. (2018) experiment by considering the effects of the risk-aversion retailers on the sets of beliefs that support the “high-price offer equilibrium” or the “low-price offer equilibrium”, and on the equilibrium selection. To do this, we use the initial endowment as a treatment variable. The variation of this endowment (that can be either high or low) captures the “risk sensitivity” of risk averse retailers. Compared to the treatment with a high endowment, the retailer’s risk sensitivity is stronger in the treatment with a low endowment. Overall, our experiment confirms our treatment effect. We find that: (1) when the retailers are risk averse, participants play more low-price-offer equilibrium in the treatment with a low initial endowment than in the treatment with a high initial endowment, (2) by extending the LSB criterion to risk-averse retailers, we improve its predictive power on the equilibria played out, particularly for a population of moderately to extremely risk-averse retailers. Our results stress that in environments where the retailers’ risk sensitivity is strong, the equilibrium supported with passive beliefs seem more plausible than the equilibrium supported with symmetric beliefs. Our research contributes to the experimental literature on vertical contracting. Much of this literature investigates the effect of vertical mergers in different vertical industry structures (see Mason and Phillips 2000; Durham 2000; Martin et al. 2001). Other contributions have focused on communication in vertical contracting (see, e.g., Moellers et al. 2017). Like Eguia et al. (2018) and Martin et al. (2001) suggest that assuming specific out-of-equilibrium beliefs does not enable one to capture the complexity of strategies in vertical relationships. In an experiment on vertical contracting games, Martin et al. (2001) provide evidence that assuming homogeneous beliefs may be inappropriate. Our contribution to this literature is twofold. First, we analyze the relation between the retailers’ risk sensitivity and the set of beliefs that support the equilibrium. We show that the greater the sensitivity towards risk, the more the equilibrium played will be consistent with the set of beliefs that includes the passive beliefs. Second, we use incentivized methods during the game to elicit subjects’ beliefs. We show that elicited passive beliefs do not explain the treatment effect predicted theoretically and observed in the experiment. Our results on elicited beliefs highlight the limitations of the ad hoc specification of beliefs (whether passive or symmetric) as a good predictor of the equilibrium played. Our paper contributes also to the literature that investigates, in the lab, wealth effects on the curvature of the utility function, using variations of payoffs in experimental tasks to test their effect on risk aversion (Harrison and Rutstrom 2008; Harrison et al. 2017). Our contribution to this literature is to consider that variations of initial endowment can modify the locations of the payoff values of the (retailers’) utility function. This variation makes them more or less sensitive to a same level of risk aversion and impacts the set of beliefs that supports the selection of one equilibrium.Footnote 4 The paper is organized as follows. Section 2 introduces the theoretical framework extending the Eguia et al. (2018) contracting game to risk-averse retailers. Section 3 details the experimental design. Section 4 presents the results of the experiment, and Sect. 5 concludes.",
93.0,4.0,Theory and Decision,31 January 2022,https://link.springer.com/article/10.1007/s11238-021-09864-7,Accountability as a Warrant for Trust: An Experiment on Sanctions and Justifications in a Trust Game,November 2022,Kaisa Herne,Olli Lappalainen,Juha Ylisalo,Female,Male,Male,Mix,,
93.0,4.0,Theory and Decision,22 January 2022,https://link.springer.com/article/10.1007/s11238-021-09855-8,Revealed desirability: a novel instrument for social welfare,November 2022,Guy Barokas,,,Male,Unknown,Unknown,Male,"The conventional wisdom in the field of normative economics is that evaluating resource allocation of a society based solely on revealed preference information must involve some subjective judgment on how different values, such as efficiency and equality, are compromised. In effect, the Pareto criterion is still the common ground in such evaluations. This paper offers a novel instrument, called revealed desirability, which is observable from choice and can provide useful information for refining the set of Pareto optima. The theory behind revealed desirability is that for each individual, options can be categorized as either desirable or undesirable in a meaningful way. Of course, desirable options are preferred to undesirable options; however, we claim that categorizing options in this manner provides additional useful information for welfare analysis. Consider, for example, the following judgment: \((*)\) a resource allocation is strictly socially preferred to another if only the former allows all individuals to obtain a desirable bundle. A question that immediately arises is whether the aforementioned two categories are well defined. This is analogous to question one of the core assumptions of classical economics; that is, whether agents have well-defined, consistent preferences. We believe that in this respect, desirability is at least as an appealing concept as preference. For example, casual observations suggest that people change their preferences regarding, for instance, what they would like to eat for dinner, while the set of dishes they find desirable (or acceptable, see below) remains rather constant. In a preliminary experiment (presented in online Appendix C), we find evidence that agents are significantly more consistent with regard to what they report as (un)desirable compared to their stated preferences for the same set of products.Footnote 1 A second possible answer to the question of the existence of well-defined preferences is that preferences are measured by choices, and as long as the latter are consistent, they should provide guidance for social planning, regardless of whether or not they reflect one’s welfare preferences (Gul & Pesendorfer, 2008). Like the revealed-preference principle, which asserts that a preference between two objects can be revealed by offering a choice between them, we claim that desirability can be revealed from certain observable choices; specifically, choices from situations in which rejecting all the available options is possible (henceforth, nonforced-choice [NFC]). For example, consider Alice, who is taking a walk on the promenade and looking for a place to have dinner, realizing that only McDonald’s is open, and deciding not to eat there. Alice’s choice provides no information on her preferences, but it does tell us that she finds McDonald’s undesirable. Contrasting, if Alice is at home and decides to drive for half an hour to eat at restaurant x,  even though several closer eateries are open, she reveals that she finds x desirable. A preliminary experiment (reported in online Appendix B) shows that the NFCs’ capture stated desirability well. Specifically, we find no significant difference between an NFC and stated desirability in predicting future stated desirability. This result is in line with the previous results on NFCs; for example, Zakay (1984) found that choosing an option in an NFC situation is negatively correlated with that option’s distance from some “ideal option.” As with revealed preference, revealed desirability through NFCs can be justified without relying directly on the concept of desirability. For example, judgment \((*)\) can be rewritten as follows: a benevolent social planner should prefer allocations in which all agents obtain a bundle that they would choose for themselves in NFC situations, over allocations in which some agents receive a bundle that they would reject in NFC situations. Of course, not all choice situations allow for revealed (un)desirability. For example, if one is unable to leave one’s home due to the coronavirus and is, therefore, faced with a limited number of options for dinner, then choosing an option in this forced-choice (FC) situation does not imply that it is desirable. Revealed preference is almost always studied in an FC setting. Recently, however, Gerasimou (2018, Proposition 2) provides a model in which NFCs allow to reveal an individual’s desirability set (i.e., the set of options she finds desirable) as well as her preferences between the options in that set. In this paper, we first axiomatically extend Gerasimou’s model to include the standard domain of forced-choice (FC) situations to obtain, in addition to the agent’s desirability set, her preferences over all choice objects, which is crucial for welfare analysis. Then, in the main section, we provide an example for the potential implications revealed desirability has on the theory of fair allocations. Specifically, we offer an allocation rule, called the Disjunction Pareto (DP) rule, which selects all the Pareto optimal allocations with the property that no unsatisfied agent can be better off without making an unsatisfied agent worse off (or turning a satisfied agent into an unsatisfied agent). We provide an axiomatization of this rule along with another normative motivation for considering it, based on its leximin properties. The two allocation rules that dominate the literature are free envy (Varian, 1974; Cole & Tao, 2021) and egalitarian equivalent (Pazner & Schmeidler, 1978), both of which use only information on preferences. In fact, while NFCs have been extensively studied,Footnote 2 this is the first study to discuss their social implications. We find an unexpected connection to List (2001) and Brams and Sanver (2009), whose data are mathematically equivalent to those considered by our model, but without any reference to choice. Motivated by a need to escape Arrow’s impossibility theorem, List (2001) offers an axiom requiring that the social ordering be invariant to increasing transformations of the individual utility functions that do not distort the signs of the utility levels (i.e., positive, negative, or zero). Thus, the social welfare function may depend on individuals’ preferences and a “zero line,” which is mathematically equivalent to our setting. List (2001) shows that Arrow’s dictatorship can be avoided in this setting, but anonymity remains impossible. This result motivates our focus on allocation rules rather than social welfare functions, as the former do not require a complete ranking of social alternatives. Brams and Sanver (2009) data are mathematically equivalent to List’s (2001) data. However, Brams and Sanver (2009) study voting choice rules, which select the most preferred candidate within each set of possible candidates. Because they study voting systems, their focus is different from ours. For example, their choice rule is nonaxiomatic. In addition, because their data are not observable from individuals’ choices, they are susceptible to voting manipulation, and Brams and Sanver (2009) study how pools that precede actual voting may affect the social choice. In this context, they study the preference-approval voting rule, Footnote 3, which is closely related to the much-discussed concept of approval voting (Brams & Fishburn, 1978). Approval voting asks each voter to indicate the set of candidates that she finds acceptable and chooses the candidate with the greatest number of approvals. Similarly, the preference-approval voting rule selects the candidate with the most approvals when she is the only candidate receiving approval from the majority of voters or when no candidate receives the majority of approvals. When many candidates receive approval from the majority of voters, one of the majority-approved candidates is chosen based on voters’ preference rankings. Thus, this rule prioritizes “approval"" over preferences. In contrast, as will be clear from our axiomatization, the DP rule prioritizes the rankings of unsatisfied agents over those of satisfied agents. For example, our rule may not rank the unique majority-approved alternative higher than another alternative if some unsatisfied agents prefer the latter to the former. The remainder of this note is organized as follows. Section 2 provides the choice theoretic foundations of our model, Sect. 3 discusses its social implications, and Sect. 4 informally discusses three open questions and possible solutions related to revealed desirability. Proofs are relegated to the Appendix.",2
93.0,4.0,Theory and Decision,24 January 2022,https://link.springer.com/article/10.1007/s11238-021-09861-w,Representing preorders with injective monotones,November 2022,Pedro Hack,Daniel A. Braun,Sebastian Gottwald,Male,Male,Male,Male,"The set of all preordered spaces \((X,\preceq )\) is structured according to how well their preorder can be represented by real-valued monotones, that is, functions \(u: X \rightarrow \mathbb {R}\) such that \(x \preceq y\) implies \(f(x) \le f(y)\) \(\forall x,y \in X\) (Evren & Ok, 2011; Ok, 2002). Two major classification methods can be distinguished depending on whether one considers a single monotone (Alcantud et al., 2016) or a whole family U of monotones encapsulating all the information in \(\preceq \), called a multi-utility (Evren & Ok, 2011). More precisely, if U is a multi-utility for \((X,\preceq )\) then \(\forall x,y \in X\) we have \(x \preceq y\) if and only if \(u(x) \le u(y)\) \(\forall u \in U\). Without further constraints, monotones and multi-utilities are, however, not very useful from a classification perspective as they exist for any preordered space. They become more useful when adding constraints. For example, there are preordered spaces without strict monotones, that is, without monotones u such that, \(u(x) <u(y)\) whenever \(x\prec y\).Footnote 1 Strict monotones, also known as Richter–Peleg functions, have been extensively studied (Alcantud et al., 2013, 2016; Peleg, 1970; Richter, 1966) and are related to other features of the preorder such as its maximal elements. In the case of multi-utilities, the cardinality is an important property for the classification of preordered spaces, with countable multi-utilities playing a central role (Bevilacqua et al., 2018c). Of particular importance are utility functions (Debreu, 1954, 1964), that is, multi-utilities consisting of a single function.Footnote 2 Here, we introduce injective monotones, which are monotones u such that \(u(x)=u(y)\) implies both \(x \preceq y\) and \(y \preceq x\). Preorders for which they exist form a category between preorders with strict monotones and preorders with countable multi-utilities, as we show in Propositions 1,  5 and  8. Hence, we improve on the existing classification of preorders by adding a new distinct class. More precisely, in Sect. 3, we define injective monotones and prove some simple properties. After discussing their relation to optimization in Sect. 4, we take a look at the role of multi-utilities in Sect. 5, in particular, we construct injective monotones from countable multi-utilities and show that the converse does not hold. Finally, in Sect. 6, we consider separability properties of preorders that are sufficient for the existence of strict and injective monotones, introducing a new notion of Debreu separability, that allows to extend previous results on strict monotones to corresponding analogues for injective monotones. In the following section, we introduce our running example to which we come back several times throughout the development of the general theory. In particular, we discuss the relation between the uncertainty preorder from majorization theory (Arnold, 2018), which has Shannon entropy as a strict monotone, and the maximum entropy principle that appears in many different parts of science.",
93.0,4.0,Theory and Decision,31 January 2022,https://link.springer.com/article/10.1007/s11238-022-09865-0,Optimization implementation of solution concepts for cooperative games with stochastic payoffs,November 2022,Panfei Sun,Dongshuang Hou,Hao Sun,Unknown,Unknown,,Mix,,
93.0,4.0,Theory and Decision,08 February 2022,https://link.springer.com/article/10.1007/s11238-022-09869-w,Weakly continuous security and nash equilibrium,November 2022,Rabia Nessah,,,Female,Unknown,Unknown,Female,"This paper presents results on the existence of a pure strategy Nash equilibrium and it characterizes the existence of pure strategy equilibrium in games when the strategy space is not necessarily compact and/or convex and payoff functions may be discontinuous and/or nonquasiconcave. Game theory has been successfully applied in many areas in economics including oligopoly theory, social choice theory and incentive mechanism design theory. These applications lead researchers from different fields to investigate the possibility of weakening equilibrium existence conditions to further enlarge its domain of applicability, see for example the symposium of Reny (2016b) and/or Carmona (2011b). Nishimura and Friedman (1981) considered the existence of Nash equilibria in games, where the payoff functions are not quasiconcave but satisfy a strong condition. Dasgupta and Maskin (1986) established the existence of pure and mixed strategy Nash equilibria in games, where the strategy sets are convex and compact and payoff functions are quasiconcave, upper semicontinuous and graph continuous using an approximation technique. Simon (1987) and Simon and Zame (1990) used a similar approach to consider the existence of mixed strategy Nash equilibria in discontinuous games. Simon and Zame (1990) showed that if one is willing to modify the vector of payoffs at points of discontinuity so that they correspond to points in the convex hull of limits of nearby payoffs, then one can ensure a mixed strategy equilibrium of such a suitably modified game. Vives (1990) established the existence of Nash equilibria in games, where payoffs are upper semicontinuous and satisfy certain monotonicity properties. Baye et al. (1993) provided necessary and sufficient conditions for the existence of pure strategy Nash equilibria and dominant strategy equilibria in noncooperative discontinuous and/or nonquasiconcave games. It is shown that diagonal transfer quasiconcavity is necessary, and further, under diagonal transfer continuity and compactness, sufficient for the existence of pure strategy Nash equilibrium. Both transfer quasiconcavity and diagonal transfer continuity are very weak notions of quasiconcavity and continuity and use a basic idea of transferring nonequilibrium strategies to a securing profile of strategies. Reny (1999) established the existence of Nash equilibria in compact and quasiconcave games, where the game is better-reply secure, which is a weak notion of continuity. Reny (1999) showed that better-reply security can be imposed separately as reciprocal upper semicontinuity introduced by Simon (1987) and payoff security. Bagh and Jofre (2006) further weakened reciprocal upper semicontinuity to weak reciprocal upper semicontinuity and showed that this condition together with payoff security, implies better-reply security. Reny (2009) introduced a game property that is weaker than better-reply security, called the lower single-deviation property and proved that if a game is bounded, convex, compact, quasiconcave and has the lower single-deviation property, then it possesses a pure strategy Nash equilibrium. Prokopovych (2011) introduced the transfer reciprocal upper semicontinuity and established the existence of Nash equilibrium in compact and quasiconcave games, where the game is payoff secure and transfer reciprocal upper semicontinuous. Carmona (2011a) introduced the weak better-reply security. He showed that a bounded, convex, compact, quasiconcave game and weakly better-reply secure has a Nash equilibrium. He also proved that, when players’ action spaces are metric and locally convex, this implies the existence results of Reny (1999) and Carmona (2009) and it is equivalent to the result in Barelli and Soza (2009). Nessah (2011) and Nessah and Tian (2016) introduced the generalized weak transfer continuity and showed that a bounded, compact, convex, quasiconcave and generalized weak transfer continuous game has an equilibrium. Tian (2015) characterized the existence of equilibria in games with general strategy spaces and payoffs. He established a single condition, called recursive diagonal transfer continuity, which is both necessary and sufficient for the existence of equilibria in games with arbitrary compact strategy spaces and payoffs. McLennan et al. (2011) characterized the existence of Nash equilibrium in compact and convex games and established a single condition, called MR-secure, that is both necessary and sufficient for the existence of equilibrium in games under the compactness and convexity conditions. More recently, Barelli and Meneghel (2013) introduced the continuous security condition and proved that a convex, compact and continuously secure game has a pure-strategy Nash equilibrium. Other papers on the existence of equilibrium with discontinuous payoff functions include Carbonell-Nicolau and Mclean (2018a), Carbonell-Nicolau and Mclean (2018b), Baye et al. (2012), Carmona (2016), Carmona and Podczeck (2009, 2012, 2016), Prokopovych (2013, 2015), Balder (2011), de Castro (2011), Reny (2011), Carbonell-Nicolau (2011), Bich (2009), Duggan (2007), Monteiro and Page (2007, 2008), Jackson and Swinkels (2005), Athey (2001) and He and Yannelis (2016; 2017). Like other existing characterization results, these are mainly for the purpose of providing a way of understanding equilibrium and identifying whether or not a game has an equilibrium. In general, the weaker the condition in an existence theorem, the harder it is to verify whether conditions are satisfied in a particular game. This paper investigates the existence of pure strategy Nash equilibria in discontinuous and/or nonquasiconcave games. We introduce a new notion of very weak continuity, called weakly continuous secure, which holds in a large class of discontinuous games. We establish that the game has a pure strategy Nash equilibrium if and only if there exist a function \({\overline{u}}\) and a compact, convex and dominant subset \(X^0\) in X such that the game is weakly continuous secure on \(X^0\). The condition of weak continuous security is more flexible and easy to check compared to the recursively diagonal transfer continuity of Tian (2015), continuous security of Barelli and Meneghel (2013) and the multiply restrictionally security of McLennan et al. (2011). We show that our result strictly generalizes Barelli and Meneghel (2013), McLennan et al. (2011), Bich and Laraki (2017), Carmona (2011a, 2009), Barelli and Soza (2009), Reny (1999; 2009), Nessah (2011), and Prokopovych (2011, 2013). In Reny (1999) it is shown that better-reply security can be imposed as two separate conditions. We introduce here a new condition called pseudo upper semicontinuity and prove that the pseudo upper semicontinuity together with generalized payoff security, implies weakly continuous security under the quasiconcavity of payoffs. We show that pseudo upper semicontinuity is strictly weaker than the weakly reciprocal upper semicontinuity of Bagh and Jofre (2006) and transfer reciprocal upper semicontinuity of Prokopovych (2011). Consequently, we strictly generalize the results of Proposition 1 of Bagh and Jofre (2006), Corollaries 3.3–3.4 of Reny (1999) and Corollary 8.5 of McLennan et al. (2011). These conditions are satisfied in many economic games and are often simple to check. The remainder of this paper is organized as follows. Section 2 describes the notations. Section 3.1 introduces the notion of weakly continuous security, provides the main result on the characterization and existence of pure strategy Nash equilibrium, examples illustrating the theorems are also given. Section 3.2 offers the sufficient conditions for the weak continuous security. Section 4 describes the related results. Section 5 will briefly present the conclusion. All proofs are given in the appendix.",
93.0,4.0,Theory and Decision,31 January 2022,https://link.springer.com/article/10.1007/s11238-022-09867-y,"Book review on Michalis Drouvelis: ""Social preferences: an introduction to behavioural economics and experimental research""",November 2022,Michael Kosfeld,,,Male,Unknown,Unknown,Male,,
94.0,1.0,Theory and Decision,07 March 2022,https://link.springer.com/article/10.1007/s11238-022-09883-y,Desirability relations in Savage’s model of decision making,January 2023,Dov Samet,David Schmeidler,,Male,Male,Unknown,Male,"Savage (1954) and Jeffrey (1965) each studied a model in which a preference relation is associated with subjective probability. We begin by describing the two models and delineate the model that we study in this paper which is a compromise between Savage’s and Jeffrey’s models. The building blocks of Savage’s theory are consequences, acts, and states of the world. The agent in this theory faces various acts each of which may result in some possible consequences. The agent is uncertain about which consequence will be realized by her acts. This uncertainty is modeled by specifying a set of states of the world. Each state is a rich enough description of the world that resolves these uncertainties, namely, each state determines the consequence of each one of the acts. Thus, the uncertainty about the consequence of an act is translated into uncertainty about the state of the world. Probability in this theory is defined on events, that is, subset of states, while utility is defined over consequences. The expected utility of acts defines a preference relation over acts. These preferences satisfy certain axioms, and fulfilment of these axioms makes it possible to find a probability–utility pair that gives rise to the preferences. The primitive notion of Jeffrey’s model is a set of propositions. Two measures are defined on this set: A signed measure and a probability measure. The signed measure of a proposition is thought of as the total utility it provides, although the theory does not explicitly define utility. The ratio of the signed measure and the probability measure is called the desirability function. The desirability of propositions naturally defines a preference relation over propositions. We call such a relation a desirability relation. The desirability relation satisfies certain axioms, and when a preference relation on propositions satisfies these axioms then the relation can be shown to be defined by the ratio of the two measures as described above. Here we propose a model which integrates elements from the two models that at first glance seem to have nothing in common. First we show what is indeed common to them. The set of proportions is a Boolean algebra, and so is the set of subsets of a state space. Thus, disregarding the mathematical details, we can think of the events in Savage’s model as being Savage’s propositions.Footnote 1 Hence, the signed measure and the probability measure in Jeffrey’s model can be thought of as measures defined on events. We said before that the signed measure in Jeffrey’s model can be though of as total utility delivered by the proposition. Now that we think of propositions as being events, we can define a utility function, defined on the state space, as the derivative of the signed measure with respect to the probability. This way, the signed measure of an event becomes the integral of the utility over the event, which is indeed the total utility provided by the event. The ratio of the signed measure and the probability, is the expected utility given the event. At this point the affinity between the two models breaks down. The utility thus derived is defined on states, while in Savage’s model it is defined on consequences. In Jeffrey’s model there are no consequences and therefore no acts. In our model we keep the state space with the Boolean algebra of its events and the probability on events as in Savage’s model. As we have seen this also agrees with Jeffrey’s model though we gain the concept of states of the world that is exclusive to Savage’s model. But we also want to keep consequences and utility defined on them as in Savage’s model, and at the same time have a relation of desirability on events as in Jeffrey’s model. The difficulty in combining together all these elements is the nature of states of the world in Savage’s model. A state is a description of the world that enables us to determine the consequence of each act. Thus, the description of the world in a state does not and should not include a specific consequence and it is consistent with any consequence. But, then, we cannot talk about the expected utility given an event, when the utility is defined on consequences and the states in the event are not related to any specific consequences. Therefore we cannot talk also about the desirability of an event or about a desirability relation. If, however, we fix one act in Savage’s model, then each state, combined with the act, determines a consequence. We can now think of the state as being comprehensive, that is, it is a full description of the world, including a consequence. With this it is possible to talk about the expected utility given an event, and hence to define a desirability function and a desirability relation on events in the set of comprehensive states, à la Jeffrey. We formulate several axioms on a preference relation on events that guarantee that it is a desirability relation defined by a probability on events and utility over consequences. Comprehensive states, that is, states that also describe a consequence, should come as natural objects when we consider an interaction of decision makers, rather than an a acto of a single decision maker. There is nothing in Savage’s model that excludes the possibility of other agents being there whose acts may influence the consequence of the agent’s act. In other words, the theory can be applied even when the agent is a player in a game. In this case, however, in order to determine the consequence of the player’s chosen act, the description of the world should specify the acts of the other players. An extension of a very well known example, taken from Savage (1954), illustrates this. Our agent considers the problem of breaking an egg and adding it to a bowl with five eggs previously broken for making an omelette. There are two states of the world: the sixth egg can be good or rotten. The omelette maker has three possible acts: breaking the egg into the bowl, breaking it into a saucer for inspection, or throwing it away. The consequences describe the number of eggs in the omelette and the need to wash the saucer if it was used. Imagine now another agnet, the egg seller, who sold the sixth egg to the omelette maker The egg seller has new good eggs and old rotten eggs, and he can discern between them. The egg seller is facing two acts: selling a good egg or a rotten one. The states of the world in the omelette maker’s model that specify whether the egg is good or rotten, describe the acts available to the egg seller. Consider now the model that describes the egg seller’s decision problem. He has two acts to choose between. The consequences that matter to him concern whether the omelette maker will come again to buy eggs or not. These consequences depend on the acts of the omelette maker. For example, if the omelette maker throws away the egg, he will never discover whether it is rotten or not and he will continue to buy eggs from this seller. Thus, a state of the world in the model describing the egg seller’s decision problem should specify the acts of the omelette maker. The two models that describe the decision problems of the omelette maker and the egg seller are different. In the model of each of the players the states specify the acts of the other player, but not those of the player whose decision is modeled. One may conclude that subjective probabilities of agents in interaction can be derived for each of the agents separately and game theory is not needed for Bayesian agents. But if we analyze each player in a separate model, we miss an important aspect of the interaction, namely the reasoning of players about each other’s choices. To understand this, we note that a player’s beliefs are given by the probability she assigns to the various events in her space of states. If we want the player to reason about another player, and in particular about another player’s beliefs, then these beliefs should be described by an event in the space of the first player. But this means that the state space of the other player should be the same as the state space of the first player. Thus, interaction of reasoning requires one model for all players. As each state in the state space of a player should include the acts of the other players, and as all the players should share the same state space, each state in this space describes the acts of all players, and hence also the consequence that results from their acts. Thus, a model that describes the interaction of actions requires a space of comprehensive states. Comprehensive states were first studied in Aumann (1987) to facilitate the analysis of the interactive reasoning of the players in a game. Aumann claimed in this paper that the use of comprehensive states was the main novelty of his proposed model. The chief innovation in our model is that it does away with the dichotomy usually perceived between uncertainty about acts of nature and of personal players. \([\, \dots ]\) In our model \([\, \dots ]\) the decision taken by each decision maker is part of the description of the state of the world. (Aumann 1987) However, in order to analyze the implications of Bayesian rationality on the players’ behavior, Aumann needs each player to have a subjective probability distribution on states of the world. In this he relies on Savage’s framework: Assume that ... as in Savage (1954), each player has a subjective probability distribution over the set of all states of the world. But the subjective probability and the utility in Savage (1954) are derived for a state space in which neither actions nor consequences are associated with states. How can such probability and utility be derived in a comprehensive state space in Aumann (1987)? This question is partially answered here by laying the basis for a full-fledged study of interactive decision making in a comprehensive state space. Modeling interaction of multiple agents, like Aumann (1987), requires the introduction of knowledge structures. Here we study a comprehensive state space of a single agent, which does not require the introduction of knowledge structures. The results of this research will be used in subsequent work to study the derivation of probability and utility in comprehensive state spaces of several players with knowledge structures. It is possible to give the relation of desirability several informal intuitive meanings. We can think of one event as being more desirable than another event if learning that the first happened would make the agent happier, or more pleased, or more content, than learning that the second event happened. Alternatively, we can think of desirability as reflecting a counterfactual choice. Although the agent does not usually have control over the events that will obtain, or at least not all of them, she can entertain the counterfactual situation in which she can choose one of two events to obtain. Saying that one event is more desirable than another means that had the agent had the opportunity to choose, she would have chosen the first event to obtain. Note that even in Savage’s setup we can hardly think of the preference over acts without resorting to counterfactuals. We cannot really put the agent in situations where she can choose from only two acts, for any two acts. The claim that the agent prefers one act to another involves counterfactual choice: had the agent been offered only two acts from which to chose, she would have chosen this act. With this interpretation, both desirability of events and preference over acts reflect counterfactual choices. Since both desirability and preferences over acts cannot be put to an empirical test, we need to rely on the reports of the agent about her counterfactual choices. Such reports are usually considered to be of limited reliability. However, two recent technological innovations made closer the possibility to find out more reliably the preferences and desires of agents. New technologies of recent years suggest that reports can be validated by somatic indicators, and moreover, desirability and preferences can be found without any direct report. The first one is what we call here the biological-somatic revolution. The other is the information and communication technology (ICT) revolution. The use of MRI, and mainly fMRI, which started about 40 years ago for brain research, was partially applied to research in decision theory. Later, and mainly in the present century, fMRI was replaced by less expensive tools like EEG, glasses to follow eye movements, etc. In many experiments the participants were asked to state their preferences between or among consumption goods. These preferences are reports of desirability. In the experiment the researcher can observe the subject’s reply before the subject is aware of it. This kind of research was carried out by teams including brain researchers and researchers in departments of business schools. ICT is an ongoing revolution that is changing its emphases. Our interest is focused on a phenomenon that has been particularly prominent in the last decade: The participation of a large majority of the population in two-way traffic on the web. (The use of smart phones exceeds that of PCs, which intensifies the participation.) Part of this traffic consists of what we termed desirability reports. The two technologies used in tandem mitigate the problem of the reliability of reports. See Telpaz and Levy (2015), Carlaw et al. (2007) and Webb et al. (2019). We illustrate the notion of desirability by the following example. Consider Eve, who is contemplating the submission of her new paper to one of several equally reputed journals between which she is indifferent. The choice of a journal is an act. There are only three consequences that matter to her: acceptance of the paper, rejection, or a request for a revision. Each state of the world determines the consequence of submitting the paper to each one of the said journals. In this work we assume that there are finitely many consequences. Aumann (1987) (in comment (c) of the Discussion section) suggested several reasons for finiteness. We are now meeting Eve after making her decision to submit the paper to journal J. Now, each state is comprehensive, namely, it specifies which of the three consequences holds. In particular, the state space is partitioned into three consequence events: the event that consists of all states in which the paper is accepted, the event of rejection, and the event of a required revision. Eve has a desirability relation over events and in particular over the consequence events. It is quite natural to assume that she prefers the event of acceptance over the event of revision, and the latter over the event of rejection. But the desirability relation concerns other events too. We may assume that each state of the world specifies who is the associate editor handling the paper, as this is one of the factors that determines the consequence. It is possible that Eve desires the event that Alice rather than Bob will be the associate editor handling the paper. Note that Alice handling the paper or Bob doing so, are not consequences. Eve’s desire that the first event will obtain rather than the second reflects the different ways in which these two events are associated with the three consequences. For example, if it is more likely that the paper will be accepted when Alice is the associate editor, Eve may find this event more desirable than the event of Bob being the associate editor. We present in Sect. 2.2 seven axioms, A1–A7, on a desirability relation on a fixed comprehensive state space. Here, we sketch their gist. These axioms appear to hold for the intuitive meanings of desirability discussed above. Before the axioms are introduced we define for any binary relation on events its null events. Roughly, an event is null for a given relation between two events if it does not affect the relation. More specifically, set theoretical addition (union) or subtraction of any subset of the null event to any of the two events in the relation do not change the relation between them. Axioms A1–A3 are not special to desirability. They have analogues in other axiomatizations like Savage’s, de Finneti’s axioms of qualitative probability, von Neumann and Morgenstern’s axioms, and many other binary relations. The first three axioms are analogous to Savage’s P5, P1, and P\(6'\), in this order. The Non-degeneracy axiom (A1) requires that the relation is non-trivial. This axiom implies that there are non-null events, which makes the next axiom of Weak Order (A2) non-vacuous. The latter says that the desirability relation is a complete and transitive order on the non-null events. Such axioms predate Savage and von Neumann and Morgenstern. One of the innovative axioms of Savage that plays a crucial role in proving the existence of a probability is P6\('\). Our axiom A3 is similar to P6\('\) and we name it Small events, as it says that the space can be partitioned into “small"" events.Footnote 2 Axioms A4–A7 are special to desirability relations. The first among them, the axiom of Intermediacy (A4) says that mixed news, good and bad, lies, in terms of its desirability, between the good news and the bad news. Thus, in our example the event that the paper is either accepted or rejected is less desirable than the event of acceptance, but more desirable than the event of rejection. Formally, such an axiom is common in works that study relations on subsets. Next, the axiom of Persistency (A5) is a first glimpse into the notion of likelihood, which is part of our intuition about desirability of events. Before we discuss this axiom we demonstrate how a likelihood relation between certain events can be deduced from the desirability relation. Let E and F be events that the agent equally desires, and H be an event disjoint of E and F and more desirable than both. At first glance the event \(E\cup H\) cannot be more desirable than \(F\cup H\). But on closer examination there can be a reason for that. If F is more likely than E, then the relative likelihood of the good news H is higher in \(E\cup H\) than in \(F\cup H\). Of course, likelihood is not defined in our setup, but the phenomenon just described can be used to define it. If E, F, and H are as described, and \(E\cup H\) is more desirable than \(F\cup H\), we say that F is more likely than E. This definition has one drawback, it depends on the event H. The axiom of persistency removes this drawback by requiring that the concept of being more likely is independent of the event H that is used to define it. Axiom  A6 of Consequence Events says that a consequence event is as desirable as any of its non-null subevents. Let us illustrate this axiom in our example. Consider the event ‘the paper is accepted’, and the more informative event ‘the paper is accepted and Alice handles it’. Set-theoretically, the second event is a subevent of the first. The axiom requires that these two events are equally desirable. The reason is simple. In both events the paper is accepted. The event that Alice handles it is not a consequence and it has no value of its own, and therefore it does not change the desirability of the event that the paper is accepted. The axioms already presented enable us to find out what the consequences are for the agent, which is impossible in Savage’s theory. Savage’s theory purports to derive probability and utility from observed choices. However, in order to construct the model in which this derivation takes place one needs to know in advance the consequences for the agent. But these consequences are neither observed or deduced from observations of choice. In Savage’s omelette story, for example, one cannot conclude what the consequences are for the omelette maker by just observing the choices he makes about the egg. Thus, the derivation of probability and utility in Savage’s model is based on one hand on the observation of choice, and on the other hand on the guesswork of consequences. In the model of desirability, we can find out what the consequence events are, using the data of the desirability relation. By axiom A6, a consequence event C is one that is as desirable as any more informative event. We say that such events are complete. It turns out that consequence events are, roughly speaking, maximal complete events. The formal details are in Sect. 2.2. Here we illustrate how in our example we can verify that the event ‘Alice handles the paper’ is not a consequence event, while ‘the paper needs a revision’ is a consequence event. When we find that the event ‘Alice handles the paper and it needs a revision’ is more desirable than the event ‘Alice handles the paper’, we conclude that the latter is not complete and therefore is not a consequence event. By contrast, we find that the event ‘Alice handles the paper and it needs a revision’ is as desirable as the event ‘the paper needs a revision’. Moreover we find that for any event X, the event ‘X and the paper needs a revision’ is as desirable as the event ‘the paper needs a revision’. Thus, the latter event is complete and is a candidate for being a consequence event. We only need to check that it is a maximal complete event. Indeed, suppose that A is a superevent of the event ‘the paper needs a revision’. Then it contains some subevent B of either ‘the paper is accepted’, or ‘the paper is rejected’, or both. Suppose B is a subevent of ‘the paper is accepted’. Then, by axiom A4 of Intermediacy, the event ‘B and the paper needs a revision’ is more desirable than the event ‘the paper needs a revision’ and hence, there are two subevents of A that are not equally desirable. We conclude that A is not a consequence event. This shows that ‘the paper needs a revision’ is a maximal complete event, namely a consequence event. To make this example rigorous we need to address issues concerning null events. It is easy to complete the formal description along the lines presented in Sect. 2.2. While the axiom of Persistency (A5) enables us to define likelihood relation of equally desirable events, the axiom of Likelihood Ratio (A7) emphasizes the role of likelihood ratios in desirability. It amounts to saying that if the likelihood ratio of the consequence in one event is the same as in another event then the two events are equally desirable. This is done, of course, in terms of the desirability relation. Our first theorem is: A desirability relation satisfies axioms A1–A7 if and only if it is represented by a probability-utility pair (P, u). A given desirability relation can be represented by more than one probability-utility pair. Our next two theorems describe the structure of all representing pairs. We note that the probability on the state space can be given by the finitely many conditional probabilities on the consequence events, and the finite dimensional vector of the probabilities of the consequence events, which we shall call consequence probabilities. For two consequence probability vectors p and q, we say the p is more optimistic than q, if for any pair of consequences, the likelihood of the more desired one in p is higher than that likelihood in q. Our second theorem characterizes, by two properties, a set of probabilities that can arise as the probabilities in representing pairs.  All the probabilities in the set have the same conditional probability on the consequence events, and thus differ only in their consequential probabilities; The set of consequence probability part of the probabilities in the set is an interval, namely the convex hull of two points, and the consequence probabilities are ordered in the interval according to optimism. We say that utility u is more content than v if the gains from moving to a more desirable consequence, measured by the ratio of utility difference, is higher in u than in v. In the third theorem we characterize the utilities in the set of the representing pairs. For any representing pair (P, u), the utility u is uniquely determined by P up to a positive affine transformation, and the probability P is uniquely determined by u. If (P, u) and (Q, v) are representing pairs, and the consequence probability part of P is more optimistic than the consequence part of Q, then the utility u is less content than v. Thus the optimism in consequence probabilities is balanced by the contentment of the utility function. In our fourth theorem we show that a certain product of optimism and contentment is the same for all representing pairs.",
94.0,1.0,Theory and Decision,30 March 2022,https://link.springer.com/article/10.1007/s11238-022-09866-z,Relatively robust decisions,January 2023,Thomas A. Weber,,,Male,Unknown,Unknown,Male,"Decisions under uncertainty aimed at providing absolute performance guarantees, so the standard logic goes, must be preoccupied with the most unfavorable states, however unlikely they might be. This focus on worst-case outcomes implies a “tunnel vision,” which not only leads to conservative strategies to mitigate negative contingencies, but also to a lack of scanning for positive opportunities—as favorable outcomes remain of (almost) no concern. The idea of relative robustness is to evaluate a decision based on how well it would perform as a fraction of the best payoff—viewed over all possible states. Its goal is to reach a relative performance guarantee which places the consequences of the optimal robust decision within the tightest possible percentage range of an ex-post optimal outcome, where the latter would have required perfect foresight and thus a complete absence of uncertainty. The notion of measuring the success of an outcome against the possible rewards of an ex-post optimal action is what defines “regret,” and our approach is thus equivalent to using “relative regret” as a yardstick to evaluate all available actions. And while this criterion has been used sporadically in the past to determine robust actions by purely computational means, the contribution proposed here is to use simple structural properties, some of which have their roots in the field of lattice programming, to construct a general method for finding and analyzing “relatively robust decisions.” The following simple example illustrates our ideas. Consider an action space \({{\mathcal {X}}} = \{1,2,3\}\) which describes the available strategies and a state space \({{\mathcal {S}}}=\{s_1,{\hat{s}},s_2\}\) that contains all “states of nature.” The decision maker’s payoffs, denoted by u(x, s) for all (x, s) in \({{\mathcal {X}}}\times {{\mathcal {S}}}\), are given in Table 1 below, together with evaluations in terms of the performance ratio \(\varphi (x,s) = u(x,s)/u^*(s)\) (where \(u^*(s) = \max _{x\in {{\mathcal {X}}}} u(x,s)\)), and the performance index \(\rho (x)\) which is defined as the minimal performance ratio \(\varphi (x,s)\) over all states \(s\in {{\mathcal {S}}}\). We can see that maximizing the performance index \(\rho (\cdot )\) leads to \(x={\hat{x}}^*=2\) as the (unique) optimal robust action, which is not “ex-post optimal” contingent on any particular state.Footnote 1 In addition, we highlight that in this example (which satisfies certain properties) the overall performance index depends only on the performance ratios \(\varphi (\cdot ,s_1)\) and \(\varphi (\cdot ,s_2)\) in the “extremal states” \(s_1\) and \(s_2\), which here feature the lowest and the highest possible payoff, respectively. The optimal robust action occurs where—as the action increases (from \(x=2\) to \(x=3\))—the “boundary spread,” \(\Delta (\cdot ) = \varphi (\cdot ,s_2) - \varphi (\cdot ,s_1)\), changes sign (from \(\Delta (2)<0\) to \(\Delta (3)>0\)). Taking the optimal robust action \({\hat{x}}^*=2\), which achieves a performance index of \(\rho ^* = 1/3\), guarantees that for any state s in \({\mathcal {S}}\) the payoff is never less than 1/3 of what could have been achieved under perfect foresight. As alluded to in the example, our main purpose here is to firmly establish relative robustness as a useful decision criterion under full ambiguity (i.e., in the absence of any distributional information). We further show that attractive representation results obtain under fairly general and natural assumptions, which are compatible with the theory of monotone comparative statics. In the presence of complete ignorance about which state of nature might realize, based on a “principle of insufficient reason” by Bernoulli (1738), Laplace (1825) suggested to assign equal probabilities to all states. This does account for the various possibilities on average, but not for the potentially large payoff differences between different feasible decisions across contingencies, and it offers no performance guarantee. The suggestion of dealing with uncertain decision problems by assigning weights to decisions that might be suboptimal was introduced by Neyman and Pearson (1933) who also floated the idea of a distribution-free approach by minimizing the maximum loss (viewed as negative payoff). This minimax-loss idea was formalized by Wald (1939, 1945, 1950) who also related it to the theory of zero-sum games against nature (von Neumann and Morgenstern 1944; Milnor 1951). Instead of focusing attention directly on the objective function, Savage (1951, 1954) applied the minimax approach to the difference of the ex-post optimal payoff (under perfect state information) and the payoff achieved by a given decision in a given state, which he referred to as “regret.” Both of these minimax approaches provide absolute performance guarantees, which have been actively employed in applications (see, e.g., Snyder 2006; Lim et al. 2012). Decision making under uncertainty based on minimizing regret was introduced to the managerial sciences by Bell (1982) who at the time may have well been unaware of Savage’s earlier contribution. In economics, Wilson (1987, 1992) criticized the widespread strong assumptions in models of strategic interaction under asymmetric information, including the widespread premise of common knowledge about all agents’ beliefs. Minimax-regret has since been deployed in monopoly pricing (among others) by Bergemann and Schlag (2008, 2011) and Caldentey et al. (2017). Somewhat in contrast to the aforementioned approaches centered on minimizing absolute regret, Kahneman and Tversky (1984) noted that human decision makers tend to better respond to relative gains than absolute gains. The corresponding idea of using relative regret, or equivalently an achievement rate, goes back to the “competitive ratio” to evaluate the relative performance of algorithms (Sleator and Tarjan 1985; Ben-David and Borodin 1994). A relative achievement ratio has also been used in robust linear programming by Inuiguchi and Sakawa (1997), as well as Mausser and Laguna (1999). Kouvelis and Yu (1997) present a scenario-based approach using a relative-regret objective. More recently, relative performance objectives have been useful for fair allocations (Goel et al. 2009), dynamic trading (Park and Van Roy 2015), and inventory management (Levi et al. 2015). In the extant literature, the consideration of relative-regret objectives has been largely scenario-based and viewed almost entirely from a computational and algorithmic perspective.Footnote 2 Here we seek structural insights, based on the theory of monotone comparative statics (see, e.g., Topkis 1998), so as to obtain a parsimonious representation of the fairness objective as a function of a few “extremal” states. This ultimately yields a simple characterization of the set of “optimal robust actions” which maximize a relative performance index. The paper proceeds as follows. Sec. 2 introduces the model primitives and the agent’s robust decision problem. Sec. 3 provides a simple representation of the performance index as lower envelope of extremal performance ratios. It also characterizes the agent’s optimal robust actions as a function of extremal performance ratios at the boundary. Sec. 4 illustrates our findings using a simple example, and Sec. 5 concludes.Footnote 3",1
94.0,1.0,Theory and Decision,22 February 2022,https://link.springer.com/article/10.1007/s11238-022-09870-3,Expected return—expected loss approach to optimal portfolio investment,January 2023,Pavlo Blavatskyy,,,Male,Unknown,Unknown,Male,"Existing models of portfolio investment (Konno & Yamazaki, 1991; Markowitz, 1952; Yitzhaki, 1982) use statistical measures of dispersion (variance or standard deviation, mean absolute deviation, Gini (1912) mean difference) to capture the undesirable attribute (risk) of financial portfolios. Such statistical measures, however, count both unexpectedly low and unexpectedly high returns. Arguably, only the former are undesirable (cf. Roy, 1952) and investors may be even attracted to the latter. A model of portfolio investment based on the tradeoff between expected return and expected loss considers only returns below the reference point as undesirable. Such an approach can be backed by a large literature in behavioral economics finding evidence of loss aversion (Kahneman & Tversky, 1979). For example, willingness to accept often exceeds willingness to pay, a behavioral regularity known as the endowment effect (Kahneman et al., 1990; Thaler, 1980); decision-makers often prefer to retain the status quo (Knetsch, 1989; Samuelson & Zeckhauser, 1988); investors demand higher returns from stocks with downside risk (Ang et al., 2006); traders who experience losses in the morning take extra risks in the afternoon to recover (Coval & Shumway, 2005); investors are prone to the disposition effect (Odean, 1998; Shefrin & Statman, 1985) etc. Markowitz (1952) mean–variance approach always violates the first-order stochastic dominance (Borch, 1969). Such violations are normatively unappealing and rarely observed in the data (Carbone & Hey, 1995; Loomes & Sugden, 1998, Table 2, p. 591; Hey, 2001, Table 2, p.14; see, however, Tversky & Kahneman, 1986, p. 264; Birnbaum & Navarrete, 1998, p. 61). A model of portfolio investment based on the tradeoff between expected return and expected loss has an important normative advantage over other models—a first-order stochastically dominant portfolio always has a higher expected return and a lower expected loss (Proposition 1 below). Behavioral finance literature typically models aversion to negative returns by using the elements of Kahneman and Tversky (1979) prospect theory that aggregates positive and negative returns together, with “losses looming larger than gains”. For example, Benartzi and Thaler (1995) combine loss aversion with mental accounting to rationalize the equity premium puzzle (Mehra & Prescott, 1985). Barberis et al. (2001) combine loss aversion with a sensitivity to prior outcomes to explain anomalies in asset prices. This paper models aversion to negative returns in the spirit of Markowitz (1952) mean–variance approach: investors tradeoff expected returns versus expected losses. The remainder is organized as follows. Section 2 presents the expected return–expected loss model of portfolio investment. Section 3 compares this model with other models. Section 4 shows that this model can rationalize the equity premium puzzle. Section 5 concludes.",
94.0,1.0,Theory and Decision,23 March 2022,https://link.springer.com/article/10.1007/s11238-022-09872-1,An application of simple majority rule to a group with an even number of voters,January 2023,Ruth Ben-Yashar,,,Female,Unknown,Unknown,Female,"The most simple and popular application of Condorcet’s jury theorem (CJT)Footnote 1 is to the case of a group consisting of an odd number of homogeneous members, i.e., who possess identical competence to identify the “better” alternative and vote sincerely and independently, using simple majority rule (henceforth SMR) to aggregate the various votes.Footnote 2 In many decision-making contexts, there is an even number of members in the group. However, the literature on SMR avoids talking about an even number of voters in a committee.Footnote 3 The first contribution of this paper is its application of SMR to collective decision making by a group with an even number of homogeneous voters. In particular, we show that the probability that an odd-numbered set of voters who are homogeneous (in the sense that they are each associated with the same probability of voting “correctly,” i.e., for the better alternative) reach a correct decision under SMR is the same as the probability that an even-numbered set (one more) of homogeneous voters reach a correct decision by a majority (more than half of the number) of members and a uniformly random decision is taken when there is a tie (henceforth SMRE). This result also holds in a more general, asymmetric framework where the voters are also assumed to be homogeneous in their decision-making skills, but their skills are dependent on the state of nature; i.e., each of the members is associated with two probabilities of voting correctly that correspond to two possible states of nature.Footnote 4 Several studies relax the assumption of identical competence and discuss heterogeneous groups of voters.Footnote 5 In many decision-making contexts, the competence structure of a group of decision-makers is not common knowledge. Ben-Yashar and Paroush (2000) show that in such cases, a majority of an odd number of jurists is more likely to choose the better of two alternatives than a single jurist selected at random from the jurists.Footnote 6 However, the literature avoids talking about an even number of voters in a committee. The second contribution of this paper is a justification for using SMR when the competence structure of the group of decision makers is not common knowledge, for an even-numbered fixed-size committee. In particular, we show that the probability that an even-numbered set of voters will reach a correct decision under SMRE is the same as the expected probability that an odd-numbered set (one less) of voters will reach a correct decision under SMR. In other words, dropping one member from an even-numbered set of voters does not affect the expected probability of reaching a correct decision under SMR. This result also holds in a more general, asymmetric framework where the voters are also assumed to be heterogeneous in their decision-making skills, but their skills are dependent on the state of nature; i.e., each of the members is associated with two probabilities of voting correctly that correspond to two possible states of nature.",
94.0,1.0,Theory and Decision,23 February 2022,https://link.springer.com/article/10.1007/s11238-022-09878-9,Constructing large peak-pit Condorcet domains,January 2023,Alexander Karpov,Arkadii Slinko,,Male,Male,Unknown,Male,"The famous Condorcet Paradox shows that if voters’ preferences are unrestricted, the majority voting can lead to intransitive collective preference, in which case the majority rule introduced by Condorcet (1785), despite all its numerous advantages, is unable to determine the best alternative, i.e. it is not always decisive. Arrow (1963) introduced the concept of a universal domain and domain restrictions emphasising the importance of the latter. He argued that in many instances, voters or economic agents cannot have arbitrary preferences. In particular, the condition of single-peaked preferences Black (1948), which prevents Condorcet paradox, was especially useful in political science modelling voters on the one-dimensional left–right political spectrum. Domain restrictions is nowadays an important topic in economics and computer science alike Elkind (2018). In particular, for artificial societies of autonomous software agents there is no problem of individual freedom and, hence, for the sake of having transitive collective decisions, the designer can restrict choices of those artificial agents to make the majority rule work every time. Condorcet domains represent an ultimate solution to this problem as they are sets of linear orders with the property that, whenever the preferences of all voters belong to this set, the majority relation of any profile with an odd number of voters is transitive. In particular, the domain of single-peaked preferences is one of the Condorcet domains. Maximal Condorcet domains historically have attracted a special attention since they represent a compromise which allows a society to always have transitive collective preferences and, under this constraint, provide voters with as much individual freedom as possible. However, maximal Condorcet domains may have different cardinalities which can be as low as four Danilov and Koshevoy (2013). Thus, the question: “How large a Condorcet domain can be?” has attracted even more attention (see the survey of Monjardet (2009) for a fascinating account of historical developments). Kim et al. (1992) identified this problem as a major unsolved problem in the mathematical social sciences. Fishburn (1996a) addressing this question introduced the function and put this problem in the mathematical perspective asking for the maximal values of this function to be found or at least estimated. Abello (1991) and Fishburn (1996a, 2002) managed to construct some “large” Condorcet domains based on different ideas. Fishburn, in particular, taking a clue from Monjardet example (sent to him in a private communication), came up with the so-called alternating scheme domains, later called Fishburn’s domains Danilov and Karzanov (2012). This scheme produces Condorcet domains with some nice properties, which, in particular, are connected and have maximal width (these concepts will be defined later in the paper). Fishburn (1996a) conjectured (Conjecture 2) that among Condorcet domains that, for any triple of alternatives, do not satisfy the so-called never-middle condition (these in Danilov and Karzanov (2012) were later called peak-pit domains), the alternating scheme provides domains of maximum cardinality. Labbé and Lange (2020) showed that Fishburn’s domain has the largest size among domains associated with the weak order of a finite Coxeter group. The focus of attention on maximal peak-pit domains was justified by their important connections to such classical combinatorial objects as rhombus tilings, arrangements of pseudolines and maximal separated set systems (Leclerc and Zelevinsky 1998; Galambos and Reiner 2008; Danilov and Karzanov 2012). In early work, Condorcet domains on the set of n alternatives were usually required to have maximal width which (up to an isomorphism) means that they must contain two completely reversed linear orders \(12\ldots n\) and \(n\ldots 21\). Monjardet (2006) introduced (in slightly different terms) the function It was hypothesised Fishburn (1996a) that \(g(n)=|F_{n}|\), which means that Fishburn’s domains are the largest in the class of peak-pit Condorcet domain of maximal width. Recently, the necessity of the requirement of maximal width was questioned for a number of reasons. In particular, Arrow’s single-peaked domain (which is a Condorcet domain whose restriction on each triple of alternatives is single-peaked (Arrow 1963)) is a very natural class of Condorcet domains without this requirement (Slinko 2019). In addition, not every society is liberal enough to allow completely opposite views. Thus, we introduce another function It is known that \(f(n)=g(n)=h(n)\) for \(n\le 7\) (Fishburn 1996a; Galambos and Reiner 2008) and it was believed that \(g(16)<f(16)\) (Monjardet 2009). This is because Fishburn (1996a) showed that \(f(16)> |F_{16}|\). Thus, if Fishburn’s hypothesis was true, we would get \(f(n)>g(n)\) for \(n\ge 16\). However, this hypothesis appeared to be false. Danilov and Karzanov (2012) introduced the class of tiling domains which are peak-pit domains of maximal width and defined an operation of concatenation on tiling domains that allowed them to show that \(g(42)>|F_{42}|\) refuting Fishburn’s conjecture. In the present article, we give an algebraic definition, a generalisation, and an extension of the Danilov–Karzanov–Koshevoy construction (abbreviated DKK-construction), which we call concatenation + shuffle scheme, and investigate its properties. In our interpretation, the DKK-construction \(\otimes_{1}\) involves two peak-pit Condorcet domains \({{\mathcal {D}}}_{1}\) and \({{\mathcal {D}}}_{2}\) on sets of n and m alternatives, respectively, and two linear orders \(u\in {{\mathcal {D}}}_{1}\) and \(v\in {{\mathcal {D}}}_{2}\); the result is denoted as \(({{\mathcal {D}}}_{1}\otimes_{1} {{\mathcal {D}}}_{2})(u,v)\). It is again a peak-pit Condorcet domain on \(n+m\) alternatives whose exact cardinality we can calculate. A drawback of the DKK-construction is that when \({{\mathcal {D}}}_{1}\) and \({{\mathcal {D}}}_{2}\) are maximal Condorcet domains, \(({{\mathcal {D}}}_{1}\otimes_{1} {{\mathcal {D}}}_{2})(u,v)\) may not be maximal. We fix this shortcoming by introducing a new construction \(({{\mathcal {D}}}_{1}\otimes_{2} {{\mathcal {D}}}_{2})(u,v)\) that always contains \(({{\mathcal {D}}}_{1}\otimes_{1} {{\mathcal {D}}}_{2})(u,v)\) and is always maximal. It allows us to prove the inequality \(h(13)>|F_{13}|\). Using the new construction, we can also show that \(g(34)>|F_{34}|\) further improving the result of Danilov and Karzanov (2012). We also prove that for large n, we have lower bounds \(g(n)\ge {2.0767^{n}}\), \(h(n)\ge {2.1045^{n}}\) and \(f(n)\ge {2.1890^{n}}\), where the first confirms the unpublished result of Ondrej Bilka announced in Felsner and Valtr (2011) and the third improves Fishburn’s (1996a) bound while the second result is completely new. We also get an upper bound \(g(n)<{2.4870^{n}}\) for function g. For other functions, it is only known that they are bounded by \(c^{n}\) for some constant c. Concatenation + shuffle scheme reveals the structural properties and generation algorithm for Arrow’s single-peaked domains. We show that all Arrow’s single-peaked domains can be constructed by concatenation + shuffle scheme starting from the trivial domain. The paper is organised as follows. Section 2 contains preliminaries, Sect. 3 gives the algebraic description and properties of the original Danilov–Karzanov–Koshevoy construction and proves the lower bound for function g. Section 4 introduces the concatenation + shuffle scheme, studies its properties and proves that all Arrow’s single-peaked domains can be obtained iteratively using the concatenation + shuffle scheme from the trivial domain. Section 5 is devoted to construction of Condorcet domains of cardinality larger than Fishburn’s domains. It also contains a new bound for function h and improved bound for function f. A short Sect. 6 outlines what is known in relation to upper bounds, most notably, an upper bound for function g, and Sect. 7 concludes.",3
94.0,1.0,Theory and Decision,04 March 2022,https://link.springer.com/article/10.1007/s11238-022-09882-z,Self-designation and group allocation,January 2023,John Craven,,,Male,Unknown,Unknown,Male,"There is significant controversy in public debate about the right of people to be included in certain categories of, for example, gender, religion, race or sexual orientation, only on the basis of their own self-designation. For practical purposes, inclusion can allow or restrict access to particular facilities, such as religious premises and gender-specific changing rooms, or affect participation in sports and other activities. There are many people who believe that an individual has the right to decide their category without intervention from others, and that non-discrimination principles then influence the consequences. Others disagree, arguing that not all self-designations should be allowed to pass unchallenged. In some circumstances, such as national censuses or large social surveys, the only information collected is individual self-designations. Then critics do not accept conclusions based on that information base, so that, for example, they do not agree that a national census gives a true picture of the percentage of the population who are ‘really’ in some category. Kasher and Rubinstein (1997) establish a social choice framework to analyse this topic. They formalise a context which arises from the Israeli 1950 ‘Law of Return’ that states criteria for eligibility as a citizen (see for example, Jewish Agency 2019). These criteria are more extensive than the definition of Jewishness under Halachic law, and the resulting immigration has led to political controversy around, for example, the observance of the Sabbath (Reeves 2011; Galili 2020). Following Kasher (1993), K&R (page 386) present the problem as follows: ...each of n individuals in a society holds a view with respect to every individual, including oneself, whether the latter is a J. The collective identity of J is determined by the individual views of “who is a J”. The method of determining who is a J is viewed as a function which assigns a meaning to “who is a J” for each profile of all the individual views. In other words, the function takes account of both self- and other-designation. K&R base their analysis on this important and controversial context, and their principal result (theorem 1(a) page 389) is to show that five conditions (axioms in K&R) together entail that self-designation is the only method of determining group membership. So a critic who wants to deny that it is always appropriate to use self-designations in this way must reject one of more of these conditions. This paper develops this topic by considering designations into more than two categories, and presents alternative versions of the conditions that suffice to show the principal result. Social choice problems conventionally involve the aggregation of individual opinions concerning set of objects. In the tradition of Arrow (1951) and Sen (2017), the opinions are expressed as rankings of the set of objects. In the tradition of Wilson (1975) and Rubinstein and Fishburn (1986), the opinions are expressed as classifications or partitions of the set of objects. The unconventional aspect of K&R is that the set of objects is identical to the set of individuals. Then the opinions are expressed as a set of self- and other designations of each individual by each individual. Although most of the conditions introduced by K&R are related to conditions in conventional social choice, they must be adapted to allow for the identity of the objects and the individuals. These adaptations change the focus from conditions that necessarily entail that the outcome is determined by a dictator to conditions that necessarily entail that the outcome is determined by self-designation alone. This unconventional extension of social choice does not lose the typical assumption that the only information that is relevant to the determination of the outcome of the social choice process are the individual opinions. If a critic of Israel’s ‘Law of Return’ wishes to assert that an individual can be considered to be a J just if their mother is a J, the information base used to determine ‘who is a J’ extends beyond (and maybe excludes) the self- and other-designations invoked by K&R. Of course, any of the individuals in the society might base their self- and other-designations on that maternal principle, but the social choice approach allows that this opinion might not prevail, given the opinions of others. Although our concern is with the basic ethical question of the consequences of rejecting liberalism—that is rejecting self-designation as the basis for devising categories—a similar framework can be used in decision-making contexts. For example, a musicians’ convention might wish to divide its delegates into groups such as jazz-musicians and blues-musicians, and decide to do so on the basis of each delegate’s designation of herself and of all the other delegates. If the convention adopts the five conditions presented here, then the outcome must be that every delegate is located where she self-designates. In common with conventional social choice, the methods can be used to make practical decisions and devise social outcomes as well as to offer a critique of principles that might be held by a single individual. An alternative decision-making context might reject the possibility that the outcome is determined by self-designation. For example,Footnote 1 students engage in a group project and each student evaluates her own and the other students’ contributions as ‘fail’, ‘pass’, ‘merit’ or ‘distinction’. The tutor aggregates these evaluations into grades, and it is possible that the grade for one student depends in part on her own self-evaluation, but tempered by the views of others. It might seem unacceptable that there are any circumstances in which a student’s grade depends only on her self-evaluation given the temptation to overstate her own contribution. In that context, one or more of the five conditions must be rejected to avoid an unacceptable outcome. The same argument would apply in the two-category case of K&R, where the grades available are ‘pass’ and ‘fail’. The extension of the framework introduced by K&R to multiple categories has been examined by, for example, Houy (2007) and Cho and Ju (2017). Some of the conditions that they use are different from those considered here, and are arguably less closely related to the five conditions in K&R. Miller (2008) also considers multiple categories and the allocation of individuals when categories are subdivided or combined. Then we could compare the membership of the jazz group at the musicians’ convention with the membership of the trad-jazz and modern-jazz groups if such a subdivision were introduced or abolished. Much of the literature following K&R has been concerned with decision-making issues. For example, Samet and Schmeidler (2003) consider the extent of consent from others that is needed to allow an individual to self-designate, Alcantud and Laruelle (2020) consider the formation of ‘clubs’ using voting rules and involving vetoes, and Houy (2007) and Fioravanti and Tohmé (2020) constrain the influence of individuals so that, for example, existing members of a category have the privileged ability to admit others to that category. These are not our question, which remains the examination of the choice between accepting that the outcome results only from self-designation and rejecting one or more of the conditions.",
94.0,1.0,Theory and Decision,10 March 2022,https://link.springer.com/article/10.1007/s11238-022-09881-0,Progressive stopping heuristics that excel in individual and competitive sequential search,January 2023,Amnon Rapoport,Darryl A. Seale,Leonidas Spiliopoulos,Male,Male,Male,Male,"The heuristics literature in cognitive psychology is concerned with two different but related issues. First, are heuristics predictive of human behavior? Second, how does heuristic performance compare to the optimal solutions of the decision-making tasks? When testing models in the controlled environment of the laboratory, the focus of the literature typically is on the former question, their predictiveness, namely, how well the predictions of the model match systematic and replicable patterns of behavior that are observed in the raw data. In contrast, our manuscript focuses on the second question, namely, the relative performance of heuristics (simple rule-of-thumb models) vis-à-vis an optimal benchmark. One viewpoint emphasizes the performance shortcomings of descriptive heuristics, arguing that they often lead to biases and sub-optimal behavior (e.g., Kahneman, 2003a, b, 2011; Tversky & Kahneman, 1974). An alternative viewpoint contends that heuristics can be fast and frugal, exhibiting excellent performance and even outperforming normative models in environments of irreducible uncertainty arising from nature (Gigerenzer et al., 1999, 2011; Hertwig et al., 2013; Todd et al., 2012) or imperfect knowledge of opponents’ strategic behavior and payoffs in games (Spiliopoulos & Hertwig, 2020). This latter strand of the literature emphasizes that heuristics perform best when their processes match relevant characteristics of the environment, thereby exploiting them efficiently in the spirit of Simon’s ‘scissors’ metaphor, particularly in the face of uncertainty—see Hertwig et al. (2019) for examples of this across a wide range of different decision-making tasks. Ultimately, moving away from a binary antagonistic stance regarding heuristic performance, research should be directed at demarcating the boundary between environments where heuristics perform well and environments where they may be inappropriate. Where does this boundary lie? Are heuristics restricted to performing well only in relatively simple environments or do they excel in exactly the opposite type, i.e., complex environments? In choosing a new domain of complex problems to test heuristics, one restriction is that some problems are intractable within reasonable time and computational limits (e.g., NP-hard problems such as the Traveling Salesman problem). Consequently, the optimal solutions are unknown, rendering relative performance an undefinable property. We have opted to study the taxing, yet numerically solvable, domain of sequential search problems, which sometimes are dubbed as optimal stopping problems (Ferguson, 2002), where the decision maker (DM) interviews the choice alternatives (items, applicants) sequentially. There is a very large body of theoretical research on the standard secretary problem, which originated about 60 years ago (Chow et al., 1964; Lindley, 1961). We narrow down this domain to a class of no-information sequential search problems (Ferguson, 2002) in which the DM has no prior knowledge about the probability distribution of the choice alternatives and, therefore, cannot acquire information from experience about its parameter values. Rather, in deciding whether to accept or reject a given item, the DM is only informed of the relative rank of the present item in comparison to all the items preceding it in the sequence. The optimal stopping rules for these problems may, in principle, be numerically calculated by dynamic programming (see, e.g., Chow et al., 1964; Gilbert & Mosteller, 1966). As Simon (1990) has noted, optimal strategies provide important insights into the nature of the decision-making environment under investigation, but the mathematical methods for discovering them often use a formal language which is alien to most DMs and at times incomprehensible. Consequently, optimal strategies are inaccessible to most humans and organizations, both in the laboratory and in practice, because of their formal language, computational costs, and the constraints imposed by the users’ cognitive abilities. This begs the question whether there exist simple heuristics that are both accessible to DMs and capable of excellent performance in this domain. Our paper presents three contributions. First, we investigate the performance of heuristics in a non-competitive decision-making domain, consisting of four problem variants (presented in Sect. 2), that is considerably richer and more complex than the domains already studied. One set consists of three heuristics proposed by Seale and Rapoport (1997, 2000) and further investigated by Stein et al. (2003)—these three heuristics have been found to be used by subjects in experiments. Two of these heuristics perform extremely well in variants of sequential search problems for which the optimal decision rule consists of a single threshold. However, they perform poorly in problem variants whose optimal decision rules call for multiple (rather than a single) threshold. Second, in response to this finding, we investigate for the first time the performance of a heuristic originally proposed only for a specific sequential search problem (the expected rank minimization problem; Krieger & Samuel-Cahn, 2009) in three other variants with multiple thresholds—we call this the Progressive Stopping (PS) heuristic (Sect. 3). We establish that this heuristic achieves more than 95% of the optimal performance across all these variants and for sequences with different numbers of items (Sect. 4). Third, we investigate a competitive secretary problem where multiple employers compete with one another to hire the best job applicant (Sect. 5). We then construct an optimal solution (a subgame-perfect Nash equilibrium) for this game following (and correcting) the proof of Karlin and Lei (2015). We propose a new heuristic related to the PS heuristic—the Inverse Progressive Stopping heuristic (Sect. 5.2)—and show that it exhibits remarkably high performance for the whole set of employers. Readers conversant with the literature on sequential search problems and/or heuristics may skip the following Sects. 1.1 and 1.2, respectively, and continue from Sect. 2. The class of no-information sequential search decision problems has been studied theoretically in applied probability and operations research (see literature reviews by Ferguson, 1989, 2002; Freeman, 1983; Samuels, 1991) and experimentally in the disciplines of psychology and behavioral economics (see, e.g., Corbin, 1980; Bearden & Rapoport, 2005; Lee, 2006; Mak et al., 2019; Palley & Kremer, 2014; Seale, 1996; Seale & Rapoport, 1997, 2000). Non-distributional models of sequential search can be divided into two streams depending on the objective of the DM conducting the search. In one stream (e.g., Lindley, 1961), commonly referred to as the probability maximization problem (PMP), or the best choice secretary problem, the DM’s objective is to maximize the probability of selecting the best item. This may appear very restrictive; however, in the business world, returns to many investments are increasingly described by an all-or-nothing distribution of returns in the long run, e.g., due to first mover advantage or a superior technology dominating the industry. Or at the very least, there are strongly increasing returns to choosing higher relative ranks; consider a sigmoidal relationship, which approximates an all-or-nothing objective function. Alternatively, the second stream of research (Chow et al., 1964), which is called the expected rank minimization problem (ERMP), relaxes the all-or-nothing assumption as the DM’s objective is assumed to minimize the expected absolute rank of the item selected. Under this objective, the payoffs are either the same (equal weight) or they decrease monotonically in the absolute rank of the selected items; the smaller the rank, the higher the payoff. These two objectives differ from each other in their assumptions about the DM’s objective, and so do the methods for computing their optimal solutions. At first sight, the assumption of knowledge of relative ranks—and ignorance of the distribution of items—may seem overly restrictive, particularly to economists. Traditionally, the economics literature deals with distributional models, which allow for normative solutions derived from applications of Bayesian updating, e.g., work on incomplete information in game theory. By contrast, research in Operations Research and Applied Statistics often consists of non-distributional models, where the utilities of items and their associated probability distribution are unknown—see Bearden and Rapoport (2005) for a more detailed comparison of distributional and non-distributional models of search. Consequently, neither computation of utilities and probabilities is required nor complex Bayesian updating after the presentation and inspection of each item. There are three major problems that severely restrict the applicability of distributional models to sequential search in the field. First, full-information solutions are very sensitive to the right extreme tail of the distribution of item valuations leading to non-robustness if the distribution is not known perfectly. In the wild, distributions are almost never presented by description to a decision-maker, at best s/he may approximately learn the distribution through observation. However, the tail ends of distributions consisting of rare events are virtually impossible to learn with any reasonable degree of precision even with large number of observations. Second, Knightian uncertainty is considerable in large worlds, therefore attaching cardinal valuations to items even before the sequential search commences is difficult. Third, and most importantly, distributional models are restricted to a single attribute, whereas non-distributional models of sequential search (e.g., searching sequentially for a date; searching sequentially for an apartment after moving to a new location; attempting to choose the best k, k > 1, proposals which are evaluated sequentially) do not have this restriction. We argue that the majority of important managerial decisions are based on incomplete information due to the considerable state of flux in economic conditions, the unpredictability of innovation, etc., leading to limited managerial control over organizational processes (March & Simon, 1993). A relevant example would be a venture capital firm, sifting through start-ups with considerably uncertain valuations and uncertain likelihoods of the future states affecting the valuations in an attempt to decide which one to invest in. In short, we believe that non-distributional models are more frequently applicable in practice and more realistically capture the nature of sequential searches than distributional models. The important characteristics of search problems are preserved in non-distributional models that allow for the sequential search of multi-attribute items while by-passing the problem of integrating them into a single value and still allowing for a rich set of variants in terms of alternative objective functions, probabilistic knowledge of the number of items and other assumptions—this will be apparent in Sect. 2 where we describe the different variants that we investigate. One viewpoint of the descriptive heuristics literature argues that they often lead to biases and sub-optimal behavior (e.g., Kahneman, 2003a, 2003b, 2011; Tversky & Kahneman, 1974) as they are subject to an effort-performance tradeoff (Payne et al., 1988). While the reduction in effort may outweigh the loss in performance for some applications, the contention is that heuristics necessarily suffer a considerable degradation in performance. This viewpoint has been challenged by the fast and frugal heuristics literature, which argues that heuristics can achieve excellent performance and even outperform normative models in environments of irreducible uncertainty (Gigerenzer et al., 1999). Consequently, an effort-performance tradeoff is not a given; quite the opposite, less can be more. Earlier work investigated fast and frugal heuristics in considerably simpler decision-making tasks with a focus on tasks of inference (Gigerenzer et al., 1999, 2011; Todd et al., 2012), individual decisions under risk and uncertainty (Hertwig et al., 2019; Payne et al., 1988; Thorngate, 1980) and social environments (Hertwig et al., 2013). In parallel research, economists have turned their attention to the axiomatization of heuristics (and, more generally, principles of bounded rationality) from the psychology literature Manzini and Mariotti (2007, 2012a, 2012b, 2014); see also Mandler et al., (2012). More recent work has extended the domain of inquiry from individual to strategic decision making. Theoretical work by Spiliopoulos and Hertwig (2020) shows that bounded-rational heuristics are more robust than sophisticated decision rules—including the normative Nash equilibrium—to both strategic and payoff uncertainty in one-shot strategic interactions. A burgeoning management literature concludes that managers often employ heuristics (Bingham & Eisenhardt, 2011) and that they can be effective decision-making tools in the face of uncertainty (Artinger et al., 2015). For example, Åstebro and Elhedhli (2006) show that simple heuristics can be more effective than complex regression models in predicting the success of risky ventures. That is, even in complex real world managerial domains, heuristics are not necessarily a second-best solution, particularly in large world environments were uncertainty and noise reign supreme.",1
94.0,1.0,Theory and Decision,11 April 2022,https://link.springer.com/article/10.1007/s11238-022-09884-x,A field study on the role of incidental emotions on charitable giving,January 2023,Michael Kurtz,Steven Furnagiev,Rebecca Forbes,Male,Male,Female,Mix,,
94.0,2.0,Theory and Decision,25 March 2022,https://link.springer.com/article/10.1007/s11238-022-09885-w,The correct formula of 1979 prospect theory for multiple outcomes,February 2023,Peter P. Wakker,,,Male,Unknown,Unknown,Male,"When Kahneman and Tversky (1979) introduced (original) prospect theory, they formally restricted their theory to lotteries with at most two nonzero outcomes. The extension to more outcomes is straightforward, as they wrote, but they only stated it in words and did not provide the actual formulas. Up to today, over 40 years after, and 20 years after the shared prize in memory of Nobel, the formulas of original prospect theory have never yet been written in public. While understood by specialists, the formulas nevertheless continue to cause numerous misunderstandings.Footnote 1 This note provides the formulas, hoping to put an end to the confusions.",2
94.0,2.0,Theory and Decision,19 May 2022,https://link.springer.com/article/10.1007/s11238-022-09890-z,Continuity postulates and solvability axioms in economic theory and in mathematical psychology: a consolidation of the theory of individual choice,February 2023,Aniruddha Ghosh,M. Ali Khan,Metin Uyanık,Unknown,Unknown,Male,Male,"This paper is motivated by two themes. First, by integrating the various fragmentary usages of the solvability and continuity axioms across the two disciplines of mathematical psychology and mathematical economics, respectively, we aspire to create a unitary discourse of properties that have taken different forms across the two disciplines. We weave these fragments into a coherent set of results and aspire to bring together two communities which should not have moved away to begin with. Second, the analysis presented in this paper contribute to Luce-Narens’ conceived research agenda of a systematic investigation of the possibilities with weaker forms of solvability (for eg., restricted solvability) in 1983. While much work has been done on forms of solvability and its implications for additive utility representation,Footnote 1 we believe that a deeper investigation of the form and content of the solvability axioms has been unexplored and its intimate connections with the continuity postulates in mathematical economics most certainly remain uninvestigated to the extent that they ought. The leitmotif of this paper, therefore, is to go back to the most raw, primitive and undefined, meanings of the terms solvability and continuity, and establish analytical and substantive connections between them and their subsequent conceptual proliferations.Footnote 2 The axiom of solvability has been a cornerstone assumption of the measurement theory literature since it was first introduced in the seminal work of Luce and Tukey (1964). The idea of the existence of solutions to equations for “fundamental quantities""Footnote 3 dates back at least to Helmholtz (1887), but it was not until Hölder (1901) who formally introduced the axiomatic approach to measurement theory.Footnote 4 With Luce and Tukey’s 1964 axiomatization and its culmination in the 1971 treatise Foundations of Measurement (Krantz et al. , 1971), the solvability axiom was concretized in mathematical psychology. The motive was to impose enough richness on the algebraic structure of the space concerned such that solutions to certain equations can be found. Solvability was then categorized in two forms, a restricted and an unrestricted version, with most of the applications in the literature working with the restricted version. Much of our work in this paper investigates the restricted solvability axiom while establishing the strength of the unrestricted solvability axiom. Just as with the solvability axioms in mathematical psychology, continuity postulates in mathematical economics are used to enforce enough richness on the topological structure being investigated. However, in the absence of a coherent and unified formulation between the two conceptions, different versions of both the notions are invoked in different settings.Footnote 5 The literature is replete with the usage of the adjective continuous, applied to a function as well as to a binary relation. In keeping with the theme of understanding the richness of a structure, we detail the intricacies of the different conceptions of continuity of a binary relation as comprehensively investigated in Uyanik and Khan (2022) with a special emphasis on full, separate, Wold, weak Wold, Archimedean and mixture-continuous manifestations. This work constitutes an important backdrop for carrying out the investigation pursued in this paper. With this framing of the project and its underlying motivation in place, we can now turn to the results themselves and inquire how they contribute to mathematical psychology and mathematical economics. Theorems 1, 2, 3 and 4 in Section 4 are the main results, with Theorems 1 and 4 dispensing with the dimensionality requirement of the choice space. Theorem 1 presents some well-known and some new results by connecting continuity with solvability, the latter being of primary interest for mathematical psychologists working in abstract decision theory. Along with five examples, it exhaustively documents connections between and across various notions of continuity and solvability under the maximally parsimonious assumptions on a binary relation. Theorem 2 takes the primitive notion of a separately-continuous function and uses it to define separate continuity of a relation and delineates conditions under which it is fully continuous in a finite-dimensional setting. Theorem 3 provides, under weak monotonicity, a portmanteau theorem on the equivalence between the six notions of continuity and restricted solvability of a binary relation with Theorem 4 providing partial equivalences for the infinite dimensional setting. It is our hope that the theorems presented will further the aims of Luce and Narens’ research agenda, as elucidated in the epigraph. Similarly, we aim to complement Köbberling and Wakker (2003) by providing a ranking of the continuity and solvability axioms which we hope serves as a go-to reference for future representation theorems.Footnote 6 The rest of the paper is organized as follows. Section 2 recapitulates some well-known solvability axioms and continuity postulate in mathematical psychology and mathematical economics, respectively. Section 3 exhaustively links the two axioms and presents our main results. In Sect. 4, we present six applications of our work spanning, (i) Walrasian equilibrium theory, (ii) consumer theory, (iii) representation of preferences in economics, and (iv) mathematical psychology. Finally, Sect. 5 concludes with some observations on future research directions. Appendix A provides the proofs of the results, while Appendix B pins down some observations that arise in the course of the results.",2
94.0,2.0,Theory and Decision,08 June 2022,https://link.springer.com/article/10.1007/s11238-022-09891-y,Thoughts matter: a theory of motivated preference,February 2023,Matthew G. Nagler,,,Male,Unknown,Unknown,Male,"When we humans act, we tend to adjust mentally to our actions. We buy a home, choose a spouse, or take a position on a political issue. We then—or sometimes in anticipation—get “psyched up,” rehearse the best qualities of our selected course, and rationalize. Adjustment takes a variety of forms. It may be specific and top-of-mind, or broad-based and ambient. A consumer may actively rationalize the additional expense associated with an all-electric vehicle shortly after purchasing a new Tesla. Meanwhile, over several months and almost without being aware of it, the same individual may find he is “growing into” being a Tesla owner, becoming more accustomed to and accepting of the car’s various features and thus enjoying them more. It may occur as an instantaneous and almost imperceptible process, as when the purchaser of a roll-on quickly assembles an argument for choosing deodorant rather than anti-perspirant. Or it may be extended and obvious to all, as in the case of marital engagement. The desire to improve one’s attitude toward a recently-taken or impending action may motivate people to seek resources external to themselves, such as friends’ advice, information on the Internet, or persuasive images in television commercials. Whether or not they seek external inputs, individuals invest scarce resources of attention and energy in the process. In addition, while even the smallest purchases engender a modicum of supportive thinking, the bigger the commitment one makes, the harder one endeavors to learn to love it.Footnote 1 Experimental evidence suggests that individuals treat actions and cognitive processes that alter perceptions of actions as complements. Subjects asked to re-rate alternatives following a decision or in anticipation of one increase their ratings of chosen alternatives and in some cases diminish ratings of non-chosen alternatives (Kitayama et al., 2004; Lieberman et al., 2001; Sharot et al., 2010; Wakslak, 2012). Studies employing functional magnetic resonance imaging (fMRI) indicate preference-related brain activity contemporaneous with the changes in individuals’ subjective rating of stimuli accompanying decisions or actions (Izuma et al., 2010; Jarcho et al., 2011; Kitayama et al., 2013; Qin et al., 2011; Sharot et al., 2009; Tompson et al., 2016; Van Veen et al., 2009). Festinger ’s (1962) theory of cognitive dissonance explains some of these phenomena conceptually in terms of individuals preferring their actions to be aligned with their beliefs; when they are not aligned, the theory contends, people may become uncomfortable and so alter their beliefs to restore a sense of comfort. The tri-component model of attitude similarly reflects the notion that action moves hand-in-hand with adaptive changes in beliefs and feelings (see, e.g., Chih et al., 2015; Grimm, 2005); that model has been applied extensively to explain consumer behavior and, as such, has formed the basis for a substantial amount of marketing strategy. Despite the evidence that people adjust to their actions and its growing acceptance by psychologists and marketers, economists have yet to incorporate attitude improvement in a robust way into the individual decision-making model. While recent interest in reference dependence has made the tectonics of preferences a matter of wide acceptance in the discipline, economists remain generally agnostic about the role of object-level cognitions.Footnote 2 This may stem from the perceived difficulty of measuring people’s beliefs and the discipline’s corresponding preference for inferring object preferences from, and speaking about them in terms of, observable actions and valuations. In this paper I offer a theory of individual decision-making in which cognitive adjustment complements choice of action. I circumvent some of the thornier issues associated with preference change by modeling a consumer who rationally chooses both a quantity of product and a quantity of adjustment as complementary inputs to utility. In my framework, adjustive thinking quasi-changes preferences in the sense of increasing the consumer’s marginal utility for the product; but in the context in which the consumer operates, tastes may be said to be fixed. While product consumption only affects utility contemporaneously, adjustive thinking creates a durable stock of product-specific attitude that affects the utility of future consumption. The key innovation of this paper is to characterize attitude improvement as a facet of bounded rationality. Agents in my theory can discretionarily change their preferences, but to do so requires the use of scarce cognitive resources and so is costly.Footnote 3 This proposition fits well with introspective common sense: an individual can construct and rehearse arguments, make certain facts and perceptions more (or less) salient in consciousness, work on acclimating, and so forth, to like an object or activity better. But to do so is not an effortless exercise, as focusing attention is difficult; and good opportunities for attitude improvement are not inexhaustible. This paper’s modeling approach conceives of rational individuals trading off motivated alteration of their preferences against other uses of effort.Footnote 4 The approach lends itself to simple elaborations to capture realistic aspects of relevant situations. Along these lines, it may be observed that people do not always adjust simply to enhance their current and future consumption: sometimes they do so to justify a past decision to themselves. For example, subjects in forced compliance experiments change their beliefs to feel better about actions they took that were incongruent with their beliefs or values at the time (Festinger & Carlsmith, 1959). To capture such motivations, I allow regret minimization to play a role in the agent’s objective function, and I introduce endowments—exogenous actions occurring in the initial period of the model prior to the realization of quality/taste. Endowments can represent choices the individual made under uncertainty or subject to a constraint; they may in the extreme represent actions forced on the individual. Such actions are suboptimal given the realization of quality, and they may be perceived as “mistakes” in hindsight. The regret-driven individual attempts to use adjustment to directly rationalize the endowed decision, in essence rendering it post-hoc optimal. As reflected in these primitives, motivated preference solves several important empirical puzzles involving individual choice. First, it offers an advance in explaining the endowment effect—the finding that people tend to demand more to relinquish an owned item than they would be willing to pay to acquire an identical item if they did not own it (Kahneman et al., 1990, 1991). The widely replicated effect is rightly held up as conclusively indicating that reference points matter to how people value goods; however, its standard attribution to loss aversion fails adequately to explain some essential observations. If above-normal valuations of endowments accrue simply to losses looming larger than gains, then those who are selling their own possessions should invariably require more in trade than what buyers, independent of what they currently own, are willing to pay. But then why, as Morewedge et al. (2009) find, do traders—whether buyers or sellers, and whether what they are trading is their own or not—value objects more when they own an identical item? And, if ownership affects individuals’ relative valuations only, because they view losses and gains asymmetrically, why then are those valuations vulnerable to treatments aimed at affecting subjects’ cognitive basis for caring about the things they own (Chatterjee et al., 2013; Dommer & Swaminathan, 2013)? The adjustment model demonstrates that increased valuations accrue specifically to endowments, because endowments entail complementary adjustment; moreover, they increase consumption of the same good at the margin in future periods, because the adjustment is durable. This simple mechanism is able to explain the pure effect of ownership on valuation observed by Morewedge et al. (2009)Footnote 5; and, because the mechanism is cognitive, it can explain the effects that cognitive treatments have in influencing the super-normal valuations that accrue to owning things. Rational expectations impact the perceived value of adjustments just as they do the undesirable prospect of loss. As a consequence, the motivated preference theory explains on par with the loss aversion account the dependence of object valuations on individuals’ rational expectations of ongoing ownership (Ericson & Fuster, 2011), the absence of an endowment effect for experienced traders (List, 2003, 2004), and the absence of an endowment effect with respect to money (Kahneman et al., 1990; Svirsky, 2014). Second, the model provides a robust rational-agent explanation for persuasive advertising. Traditional economic theories have conceived of two roles for advertising: to provide information about a product, and to convince consumers to prefer the product. Both propositions have limitations. The information theory cannot explain advertisers’ costly efforts devoted to crafting message and image in ads otherwise devoid of informational content.Footnote 6 The persuasion theory offers no explanation as to why advertising should elicit a response at all from a rational consumer. From the motivated preference conception of a (boundedly) rational consumer who seeks to increase the utility she obtains from the products she chooses emerges a new explanation of advertising as facilitating self-persuasion. One posits advertising expenditures as reducing consumers’ adjustment costs with respect to the advertised product. Intuitively, advertisements provide “fodder”—in the form of helpful arguments, gut appeals, and seductive images—for a consumer who is trying to become as enamored of a product as possible. Thus purely persuasive advertising influences long-term demand and serves an efficient purpose. These findings break with traditional economics, which has perceived persuasive advertising as wasteful and only informative advertising as welfare-enhancing.Footnote 7 Third, the motivated preference theory provides a more robust account of so-called “sunk-cost effects”—situations in which people make decisions that depend on past history—than previous accounts that ignore the role of cognitions. Existing rationality-based explanations rely largely on expectation-based reference dependence, according to which actions establish reference points against which future actions are judged (e.g., Baliga & Ely, 2011; Eyster, 2002). This approach explains Thaler ’s (1980) classic example of the family that decides to go to a basketball game during a snowstorm, but that would have stayed home had they received the tickets for free rather than purchasing them. The family’s purchase of tickets creates a rational expectation of attendance that receiving the tickets for free does not. But reference points based on actions come up short as an explanation of sunk-cost effects in cases where a cognitive layer is essential to understanding the mechanism of post hoc justification. Consider, for example, a scenario described by Akerlof and Dickens (1982) in which individuals face a cognitive dissonance-producing decision of whether to work unprotected in a hazardous industry. Subsequently, an opportunity arises to purchase safety equipment. Based on reference dependence, one would expect workers to adopt the equipment in some cases when the direct benefits do not exceed equipment costs, because adoption has the added benefit of rendering more prudent in retrospect one’s decision to work in the industry. Yet, consistent with the anecdotal evidence on safety-related behavior in a range of situations (e.g., motorcycle helmet adoption, hockey headgear use, AIDS testing), workers in such situations typically avoid the equipment even when its isolated net benefit is positive. The motivated preference explanation of this behavior, discussed in Sect. 3.2 of this paper, centers on the initial action in such scenarios being inconsistent with the individual’s preferences: the resultant regret precipitates compensatory adjustment around the constraint posed by the initial action. Because the adjustment is durable, it leads to a reduction in the adoption of future behaviors that might have been rational otherwise. Critically, then, it is cognition (i.e., adjustment) that, in compensating for the initial problematic action in retrospect, sets in motion the observed behavioral dynamic.Footnote 8 Along similar lines, the model recognizes escalation of commitment as a pattern of repeated action driven by adjustment that is invoked to reduce ongoing regret. The model’s predictions in this regard are consistent with experimental evidence that regret fosters repeat purchase behavior (Mittelstaedt, 1969) and fit broadly with descriptive accounts of escalation of commitment from the literature (e.g., Staw, 1976). This paper relates to several literatures. In conceiving of an individual who curates her thoughts, my theory bears a close relationship to the concept of motivated reasoning (see Epley & Gilovich, 2016 for a survey of the literature). The idea behind motivated reasoning is that certain beliefs are desirable and will be held when it is possible for the individual to hold them rationally. Two key differences distinguish the approach of the motivated reasoning literature from the motivated preferences model proposed in the present paper. First, in contrast with the typical consumer choice context, motivated reasoning deals with a relatively broad range of decision situations involving the valuing of beliefs, some of which depend upon the explicit modeling of feasible relevant beliefs. For example, a person might consider behaving according to a restrictive moral code so that in the future she will be able to view herself (desirably) as a moral person. Here, the desirability of the following the moral code depends upon the extent to which it makes more rationally feasible the desired future belief that one is in fact moral.Footnote 9 A typical approach in the literature is to endogenize cognitions by modeling explicitly the demand for and supply of relevant cognitions (e.g., Bénabou & Tirole, 2011), but other approaches to describing the assignment of beliefs are also used.Footnote 10 In the consumer choice context of the motivated preference model, on the other hand, feasibility as a qualitative strategic issue is typically secondary. The relevant question is not whether it is going to be possible to get “psyched up” about a certain action, but how much one will, and at what cost. My bounded rationality approach, which posits a reduced-form “cost of cognition,”Footnote 11 is better suited to evaluating the dynamics of action in such bread-and-butter contexts. Second, in those cases, where the motivated reasoning approach is applied to deal with the adjustment of beliefs relating to actions—the precise case of the consumer choice scenario—it presumes (i) that the actions are given, and (ii) that the adjustments are per se justifications.Footnote 12 The motivated preferences model uses a more general approach that assumes optimization of the appropriate objective, which in turn depends upon the applicable case. This approach can accommodate the simultaneous determination of actions and beliefs about actions, which the model treats as a baseline case. Alternatively, it can be applied to the special case in which the actions are endowed exogenously; this in turn can involve discounted future utility maximization subject to that constraint, or a justification sub-case that involves the maximization of an alternative objective that incorporates regret. As with the bounded rationality approach, this flexibility is better suited to the consumer choice problem than motivated reasoning—and, in particular, to the range of possible consumer choice situations. Put another way, my model recognizes that not all adjustment of cognition to action is about justification. The model’s endowment effect and escalation of commitment findings—that high consumption today begets high consumption tomorrow—are reminiscent of habit formation mechanisms, used by Becker and Murphy (1988) in their rational addiction paper, and in the broader habit formation literature (e.g., Rozen, 2010; Wathieu, 2004). There is, however, a critical distinction between habit formation approaches and the present theory. The habit formation models posit present consumption that—via creation of an enduring quasi-capital stock or, alternatively, a reference point—per se complements future consumption. Thus the scope of the theory is properly a specific class of “habit-forming” goods for which no cognitive involvement by the consumer is needed to propagate ongoing consumption. For example, an individual will likely become addicted to heroin if she uses it because of the inherent qualities of the good—whether she psyches herself up for it or not. The contribution of the motivated preference theory is to identify a mechanism according to which the broader class of all activities demonstrates some characteristics of habit formation—that is, because the boundedly rational individual optimally “leans in,” or else reacts cognitively based on the experience of regret. The model thus is able, inter alia, to offer a robust account of both the endowment effect and cognitive dissonance reactions, something habit formation models cannot do.Footnote 13 A third literature posits a need to rationalize decisions. According to this literature, decision-makers are constrained in their choices to those that they are able to justify (Cherepanov et al., 2013; Lleras et al., 2017; Manzini & Mariotti, 2007, 2012; Spiegler, 2002). These papers, in effect proposing a “demand” for rationalization, are naturally paired with a literature that deals with a “supply side” consisting of frames, or available sources of rationalization. Huber et al. (1982) identify an “attraction effect”, whereby the introduction of an asymmetrically dominated alternative increases the probability of selection of the dominating alternative. Simonson (1989) describes a “compromise effect” by which an extreme alternative, when added to a set of alternatives, increases the choice probability of an intermediate alternative.Footnote 14 Rather than viewing their role through the lens of constraints on choice, the motivated preference theory interprets rationalizations as functioning, like advertising, to reduce adjustment costs, thereby facilitating particular choices relative to alternative options. The rest of this paper is structured as follows. Section 2 lays out a model of individual decision-making involving motivated preference. Section 3 applies the model to characterize and explain four core phenomena: the endowment effect, advertising (cum framing and rationalization), escalation of commitment, and cognitive dissonance reactions. Section 4 discusses empirical measurement, alternative assumptions, and some possibilities for future work. The Appendix contains proofs of all results.",
94.0,2.0,Theory and Decision,09 June 2022,https://link.springer.com/article/10.1007/s11238-022-09892-x,Goals and guesses as reference points: a field experiment on student performance,February 2023,Gerardo Sabater-Grande,Nikolaos Georgantzís,Noemí Herranz-Zarzoso,Male,Male,Female,Mix,,
94.0,2.0,Theory and Decision,30 May 2022,https://link.springer.com/article/10.1007/s11238-022-09889-6,Original position arguments and social choice under ignorance,February 2023,Thijs De Coninck,Frederik Van De Putte,,Male,Male,Unknown,Male,"When a social planner chooses between different policies, there are two fundamental dimensions she needs to take into account:  her uncertainty about the state of the world (including its underlying mechanisms) and, hence, about the consequences of her choices for society; the welfare levels that will be enjoyed by the different members of society, relative to each specific state of the world and each policy. For instance, when evaluating a national tax law that caps the higher incomes and redistributes the resulting financial resources, the social planner may be uncertain about the way the global economy will evolve—so she needs to consider the effects of such a law if the economy grows at a steady pace, but also if growth is hampered or worse. Moreover, she should consider, for each of these possibilities, how the tax law will affect the incomes of citizens, not only of the extremely rich, but also of various other classes in society. Decision-making under uncertainty typically comes in two types: decision-making under ignorance and decision-making under risk. The latter refers to cases where we know the probabilities of each possible state (cf. Resnik 1987; Peterson 2017). In this paper, we limit ourselves to decision-making under ignorance. In particular, we study a number of specific rules for social choice under ignorance, that are all based on (variants of) John Rawls’ famous Difference Principle (Rawls 1971). We ask when and how the rules in question give different choice recommendations, and if they can be grounded in a well-known type of argument, viz. original position arguments. In the remainder of this introduction, we clarify these terms and our overall aim. Difference Principle(s) The Difference Principle states that we should “arrange social and economic inequalities in such a way that they are to the benefit of the least advantaged” (Rawls 1971, p. 266). According to Rawls, this principle should be applied to distributions of primary goods, which are “what persons need in their status as free and equal citizens, and as normal and fully cooperating members of society over a complete life” (Rawls 1971, p. xiii). Others, following Sen (1970) have understood the Difference Principle as specifying how we should choose between distributions of welfare or utility. In what follows, we interpret the principle in terms of welfare, though our insights apply mutatis mutandis to the interpretation in terms of primary goods as well.Footnote 1 As an illustration of the Difference Principle, consider the basic scenario depicted in Fig. 1, where the social planner has to choose between three different alternatives a, b, and c. In this case, the Difference Principle recommends alternative b and c over alternative a because the least advantaged individual under b and c is better off than the least advantaged individual under a.Footnote 2 Choice under certainty. Here, the couples (n, m) represent the welfare levels of the two individuals under consideration According to Rawls, the Difference Principle is not only intuitively appealing, but it would also be applied by any rational decision-maker in the original position. The latter is understood as a situation in which the decision-maker is placed behind a “veil of ignorance”, and so fully ignorant about her own position and the level of welfare she will enjoy under any alternative. In the context of our example in Fig. 1, it means that the decision-maker does not know whether under alternative a her personal welfare is given by 1 or 3. Rawls argues that given such a “fair” initial setup of the choice problem, and for the specific type of institutional choices that he focuses on, the decision-maker would choose the same alternatives as recommended by the Difference Principle. In this sense, the Difference Principle is said to be “grounded” in an original position argument.Footnote 3 One critique of the Difference Principle, as noted by Sen (1970), is that it violates the strong Pareto Principle.Footnote 4 In our example alternative b is not Pareto optimal but recommended by the Difference Principle. The Lexical Difference Principle has been put forward as an alternative candidate which is consistent with the strong Pareto Principle. It states that one should first maximize the welfare of the worst-off individuals and, in case of equal welfare, maximize the welfare of the second worst-off individuals, and so on. On the Lexical Difference Principle, only alternative c would be recommended in our example. Parfit (1991) and Van Parijs (2001) claim that Rawls’ original position argument supports the Lexical Difference Principle, rather than the Difference Principle.Footnote 5 Intuitively, this seems to make sense: the individual may turn out to be among the worst-off—in which case either principle will maximize her welfare—but she may also turn out to be better off—in which case only the Lexical Difference Principle ensures that her welfare is maximized (conditional on maximizing the welfare of all the worse-off). This paper We will study original position arguments for (Lexical) Difference Principles, in the context of social choice under ignorance. In doing so, we stick to Rawls’ basic assumptions about the original position as much as possible. In particular, we assume interpersonal comparability of welfare levels on an ordinal scale, we exclude any information about the likelihood of states, and we exclude any probabilistic way of handling uncertainty as e.g. given by the well-known principle of insufficient reason.Footnote 6 In terms of the famous Rawls vs. Harsanyi-debate (Harsanyi 1975), this means that we side with Rawls on how to represent the reasoning and uncertainty of individuals in the original position.Footnote 7 We do so for the sake of the argument: it turns out that even if we grant all this, the prospects for grounding a Lexical Difference Principle in an original position argument are still fairly meagre. The upshot is that, while we agree with Parfit and Van Parijs that the Difference Principle should be strengthened in order to accommodate more “lexical” intuitions, it is not clear what such a strengthening looks like in the case of genuine ignorance about the state of affairs, and how such a strengthening can be grounded in an original position argument. Our contribution can be summarized as follows. In Sect. 2, we present a general format for individual and social choice rules, we recall the well-known maximin and leximin rules for individual choice, and we show how original position arguments can be formalized. In Sect. 3, we argue that in the context of choice under ignorance, there are three approaches to social choice that are in line with the Difference Principle: a basic approach, an ex ante approach, and an ex post approach. After introducing these three approaches in general terms, we single out three corresponding social choice rules that all incorporate the Difference Principle in some sense. We show that, while they are conceptually distinct, these three social choice rules give the same choice recommendations and can be grounded in an original position argument in combination with the well-known maximin rule. Analogously, in Sect. 4 we discuss specifications of the Lexical difference principle. Here, it turns out that the ex post approach can be further subdivided into two distinct approaches. Moreover, the four resulting social choice rules turn out to give distinct choice recommendations. As we explain, the specification of the Lexical difference principle that follows the basic approach can be grounded in an original position argument using leximin as the underlying individual choice rule. In contrast, none of the three more refined specifications can be grounded in any original position argument, regardless of the underlying notion of individual rationality. On the basis of these results and their proof, we conclude that original position arguments—at least following our characterization—face important shortcomings in the context of choice under ignorance (Sect. 5). Related work Most of the formal literature on Rawls is focused on the Rawls/Harsanyi debate over how exactly to characterize the original position and how agents would choose, once placed in such a situation (cf. supra) Moehler 2018; Moreno-Ternero and Roemer 2008; Roemer 2002. Within this debate, it is often presupposed that there is no uncertainty about the state of affairs, before the veil of ignorance is imposed. Recall that Rawls claims that the Difference Principle would be chosen by any rational person in the original position. Rawls (1974) suggests that this claim could be supported by formal proof and considers the work of Arrow and Hurwicz (1972) to be a step in that direction. In Arrow and Hurwicz (1972), it is shown that if a decision rule satisfies certain plausible axioms it only takes into account the worst and best outcomes of each alternative to rank them. However, Arrow and Hurwicz do not really formalize the notion of an original position argument itself. While there is a substantive literature on social choice under uncertainty, most of it focuses on cases of risk, i.e. cases where we know the probabilities of each possible state of affairs (cf. Mongin and Pivato 2021; Ben-Porath et al. 1997; Fleurbaey 2018; Gajdos and Maurin 2004; Gajdos and Kandil 2008; Hayashi and Lombardi 2019; Bovens 2015). As will become clear, our distinctions and examples bear many similarities with this strand of work. However, as is often the case, the devil is in the details. We refer to Maskin (1979) for a general, axiomatic characterization of individual choice rules under ignorance. As he indicates, these axiomatizations are strongly linked to results in social choice theory, but Maskin does not consider the issue of social choice under ignorance per se, let alone the Rawlsian notion of an original position. In Gustafsson (2018), Gustafsson argues that in situations of risk, a rational individual in the original position will not always make choices that agree with the Difference Principle—whether it is spelled out according to an ex ante or an ex post approach. In this sense, although his argument concerns decisions under risk and ours concerns decisions under ignorance, we agree with Gustafsson that there is a mismatch between the original position on the one hand, and principles of justice on the other. However, while Gustafsson uses this insight against the Difference Principle, our conclusion is in a sense the opposite: we argue that, since the most natural model of individual reasoning in the original position cannot accommodate certain distinctions that seem to be crucial for our intuitive notion of justice, this model should be refined if not revised. We leave the latter enterprise for future work. In Strasnick (1976), Strasnick argues that the Difference Principle follows naturally from the properties of the original position once suitably defined. While our modelling choices differ from his, our work can be conceived as a continuation of this line of research. A key difference with our work is that Strasnick assumes social choice in the absence of any uncertainty. As will become clear, it is precisely the dimension of uncertainty that causes trouble for arguments based on the original position.",
94.0,2.0,Theory and Decision,20 August 2022,https://link.springer.com/article/10.1007/s11238-022-09893-w,Decompositions of inequality measures from the perspective of the Shapley–Owen value,February 2023,Rodrigue Tido Takeng,Arnold Cedrick Soh Voutsa,Kévin Fourrey,Male,Male,Unknown,Male,"In the economic literature, there are two main approaches to decomposing inequality measures. The first one, the decomposition by sub-population, consists of splitting the observed inequality between the groups composing the society. For instance, it may be useful to know whether income inequality is more or less strong within the group of men than it is in the group of women, and to estimate the share of inequality attributed to the income differences between men and women. When this decomposition is considered in the literature, the inequality index often used is the Theil index as its decomposition, first proposed by Theil (1967), allows to split inequality into two components: within-group inequality and between-group inequality. However, there are other measures which are decomposable into sub-populations, such as the Gini index with the decomposition proposed by Dagum (1997), a decomposition that has been extended by Chameni (2006a, b), Mussard et al. (2006), Ebert (2010) and Mornet et al. (2013). The other dominant approach decomposes inequality measures into a set of factors (e.g., income sources). For instance, with a view on public policies, it might be worthwhile evaluating the importance of each income source of income inequality (e.g., labor, capital, social transfers, etc.), especially to appreciate the effectiveness of social transfers in reducing inequality. This decomposition has been notably axiomatized by Shorrocks (1982), but only the decomposition of (the square of) the coefficient of variation respects these axioms. Some authors have departed from these axioms to propose a decomposition of relative inequality measures. For example, Lerman and Yitzhaki (1985) proposed a factor decomposition of the Gini index. Moreover, the multiple decomposition is an approach that takes into account both sub-population effects and factor effects on inequality (e.g., Jenkins & Van Kerm, 2006), and sometimes an additional dimension (e.g., time with Mussard & Savard, 2012 or Mussini, 2013). Furthermore, all inequality decomposition measures do not use the same methodologies. There are various processes to decompose an inequality index; some are based on mathematical arrangements (all the previously cited authors; with the exception of Lerman & Yitzhaki, 1985), some use statistical tools (e.g., Lerman & Yitzhaki, 1985; Firpo et al. 2018), while others originate from game theory (Chantreuil & Trannoy, 2013; Shorrocks, 2013). In the present article, our innovative decompositions are based on the last category, i.e., cooperative game theory. In the continuity of Auvray and Trannoy (1992), Chantreuil and Trannoy (2013) and Shorrocks (2013), they proposed the Shapley decomposition. This methodology allows to identify and rank the factors that are sources of inequality—in other words, to evaluate the importance of each inequality factor, which is determined by the Shapley value (Shapley, 1953). It should also be noted that this decomposition works with a wide set of inequality measures. Next, this decomposition method was extended by Chantreuil et al. (2019). They pointed out that the importance of a factor in an income inequality does not match with the variation of inequality that would be observed if the income differences associated with it were dropped. This variation corresponds in absolute terms to the pure marginal contribution (PMC) of the source, i.e., the inequality difference between a situation where all factors are a source of inequality and a situation where one considered factor is no longer a source of differences between individuals. As highlighted by Chantreuil et al. (2019), the importance of the source is equal to its PMC minus a weighted sum of pairwise interactions. We know that a pairwise interaction between two factors is equal to the inequality created by both of them in the presence of another set of factors T, minus the inequality created by each of them in the presence of T, plus the inequality generated by T (see Definition 4). The pairwise interactions highlight the nature of the relation of each pair of factors in the inequality game (complementary, substitutable, and independent) and the intensity of this link. In other words, it indicates to what extent two sources of inequality might favor the same individuals or, on the contrary, to what extent one source can compensate the inequality created by the other. Thus, the inequality variation observed after such a change would be equal to the importance of the considered attribute only if it had no interaction with the other income sources considered in the decomposition. Recently, Courtin et al. (2020) worked on the decomposition of interaction indices. Interaction indices can be viewed as values, which not only assign a payoff to every source but also to every coalition or group of sources (for more information, see Courtin et al. (2020)). However, the decomposition of the Shapley value proposed by Chantreuil et al. (2019) is not well suited when the income sources have a hierarchical structure. For instance, when the income can be divided into three sources (e.g., capital income, labor income, and social transfers), it is valuable to know the contribution of all “secondary” sources (e.g., financial and estate capital, wage and bonus, family and housing transfers) to their respective “primary” sources to better appreciate the source of inequality. In such cases, Shorrocks (2013) and Chantreuil and Trannoy (2013) proposed to use the Shapley–Owen value (Owen, 1977), notably because the Shapley–Owen value of one primary source is independent from the desegregation of the other primary sources, which is not the case with the Shapley value. Following the previous example, this means that the contribution of labor income to the overall income inequality will not change according to the number of secondary sources considered in the capital income. Our contribution In this paper, we propose three new decompositions of inequality measures which split into two parts the importance of each source of inequality: its PMC and a term of pairwise interactions (Propositions 5, 6 and 7). However, unlike Chantreuil et al. (2019), we do not decompose an inequality index with the Shapley value but do so more globally using the Shapley–Owen valueFootnote 1 (Owen, 1977), following the propositions of Shorrocks (2013) and Chantreuil and Trannoy (2013). The Shapley–Owen value belongs to the class of coalitional semivalues introduced and axiomatically characterized by Albizuri and Zarzuelo (2004), while Dubey et al. (1981) defined semivalues. Coalitional semivalues link the classic, well-known coalitional values [Shapley–Owen value (Owen, 1977) and Banzhaf–Owen value (Owen, 1981)], since most of the properties satisfied by the Shapley–Owen value are found to hold also for the Banzhaf–Owen value and most of the coalitional semivalues. From a theoretical point of view, coalitional semivalues can be considered as an alternative or complement to the Shapley–Owen and Banzhaf–Owen values. We propose three decompositions of the Shapley–Owen value and we show that the second decomposition remains valid for the class of coalitional semivalues. Following each decomposition, we provide an axiomatic characterization of additive interaction decomposable (AID) coalitional values. The rest of the paper is a numerical illustration of the proposed methodology, using hypothetical data. Organization of the paper The next section is mainly devoted to notations and recalls needed to obtain our three decompositions. Three innovative Shapley–Owen decompositions and axiomatic characterizations of AID coalitional values are proposed in Sect. 3, while Sect, 4 provides an application of our decompositions to analyze the inequality of a given hierarchical income distribution. Finally, Sect. 5 concludes the paper, and all the proofs are presented in the Appendix.",
94.0,2.0,Theory and Decision,29 March 2022,https://link.springer.com/article/10.1007/s11238-022-09887-8,Delegation based on cheap talk,February 2023,Sookie Xue Zhang,Ralph-Christopher Bayer,,Unknown,Unknown,Unknown,Unknown,,
94.0,3.0,Theory and Decision,05 August 2022,https://link.springer.com/article/10.1007/s11238-022-09900-0,On the strong \(\beta\)-hybrid solution of an N-person game,April 2023,Bertrand Crettez,Rabia Nessah,Tarik Tazdaït,Male,Female,Male,Mix,,
94.0,3.0,Theory and Decision,22 July 2022,https://link.springer.com/article/10.1007/s11238-022-09894-9,Optimal equilibrium contracts in the infinite horizon with no commitment across periods,April 2023,Subir K. Chakrabarti,Jaesoo Kim,,Male,Unknown,Unknown,Male,"We study equilibrium contracts in an infinite-horizon adverse selection model when there is no commitment across periods. In each period the principal offers a menu of contracts to the agent. The contract is valid only for the single period, and enforceable only in that period. At the beginning of each period, the principal updates beliefs based on the past history of contracts offered and choices made by the agent. As there is no commitment across periods, the principal can offer a contract based on these updated beliefs. This leads to the possibility that the past history can reveal information about the type of the agent. The agent recognizes this and takes this into consideration when choosing from the menu of contracts offered by the principal. Such calculations on the part of both the principal and the agent raise some very interesting issues about what kind of contracts are offered in equilibrium, whether any information is revealed in equilibrium, and if so in what way, and how quickly. It also raises questions about whether the principal can at all offer contracts that would lead the agent to quickly reveal the agent’s type and what incentives, if any, the principal needs to offer in order to induce such a response from the agent. The nature of contracts when there is no commitment across periods is of interest for several reasons, but two stand out. First, when there is no commitment across periods, there is the question of how the contracts evolve over time, especially given the potential for updating beliefs. Second, while enforceability of the contracts, and thus commitment is a fairly reasonable assumption in the short run, it is less likely to be the case that contracts are fully enforceable in the long run. For instance, when the interaction between a principal and an agent is repeated over many periods, it is quite likely that the principal offers a series of short run contractsFootnote 1. Bolton and Dewatripont (2005), Hart and Tirole (1988), and Laffont and Tirole (1988) and Laffont and Tirole (1993) have studied the dynamic adverse selection problem between a principal and an agent over a finite horizon, mostly in the case of finitely many types.Footnote 2 They have shown that if there is no commitment across periods, then there is the possibility that contracts reveal information about the type of an agent, and as the principal can use this information in the future, the ratchet effect comes into play. Further, since information rents may have to be paid in advance in period 1, some types of agent may have the incentive to use the take the money and run strategy. In the labor market for example, an employer has an incentive to offer a tougher contract if she learns that the employee’s productivity is high. Because of this ratchet effect, the high productivity employee would hesitate to reveal his/her type, and as a result, the employer may then have to offer a large compensation in the beginning. But this then leads to the problem that the less productive employee takes the offer meant for the high productivity employee, and leaves the employer after the first period. Therefore, equilibrium contracts are often pooling contracts in which types are not separated. This problem is especially acute when there is a continuum of types as shown in Laffont and Tirole (1988). Among the papers that study equilibrium contracts in which the time horizon is infinite and there is no commitment across periods, Battaglini (2005) has some elements that are common with our result. In Battaglini (2005) the type of the buyer can change from one period to the next in a Markov process. The paper shows that in the limit, as the types become persistent and constant, the equilibrium sequence of contracts becomes the single period second-best contract in each period like ours. The paper by Gerardi and Maestri (2020) also studies equilibrium contracts when there is limited commitment across periods and the interaction can last over an infinite number of periods. They show that if the prior probability that the worker has low productivity is low then a pooling contract is offered. If the prior probability that the worker has low productivity is high, then the firm fires the unproductive worker. The result here shows that in a Perfect Bayesian equilibrium, the principal offers the second-best optimal contract in every period even after learning the type of the agent. Therefore, there is neither pooling nor termination of the contract in the perfect Bayesian equilibrium of our result. Thus the result we obtain here is distinct from that in Gerardi and Maestri (2020). As the single-period optimal contract (the second-best contract) usually separates types, the principal can infer the type of the agent at the end of the first period, if the agent chooses from the menu of contracts offered. The principal then updates beliefs and then offers a contract from period 2 onwards that is consistent with that belief. If the monotone hazard rate condition holds then in equilibrium the second-best contract is offered in period 1, after which the principal fully updates beliefs, and then offers the contract in the menu chosen by the agent in period 1 in the subsequent periods. The result shows that the equilibrium strategy is a “carrot and stick” strategy that makes it unprofitable for both the agent and the principal to deviate from it. It is important to note at this point that even though the principal is fully able to infer the type of the agent in a separating contract, the principal does not use this information to ratchet up the terms of the contract in the following periods. When a separating contract is offered in period 1, the contract offered in all the subsequent periods retains all the information rent and the terms of the contract offered in period 1. Thus if the second-best contract is offered in period 1, then the contract offered in the future is the one the agent selected from the menu in period 1, together with any information rent that is part of the contract. This feature, that information rents will continue to be paid, is what induces the agent to pick an action consistent with its type. If this was not the case then the agent would have an incentive to conceal his/her type in order to prevent the principal from extracting the surplus from the agent in the future. The payment of the information rent in each period, which is consistent with the incentive constraints, prevents another potential problem that could arise in such situations. If all the rent is paid as a lump sum payment at the start, a less efficient agent may have the incentive to mimic the type of a more efficient agent, take the large lump sum payment and then leave. This strategy, often called the take the money and leave strategy becomes a possibility when all the information rent is paid in the beginning. Since the information rents of the second-best optimal contract are paid in each period, and not as a lump sum in the beginning, the problem of take the money and leave strategy does not arise in this case. Therefore, when the interaction between the agent and the principal is over a long period, even when there is no commitment across periods, separating equilibrium contracts are such that neither the ratchet effect, nor the problem of take the money and run strategy arises, as the principal offers the same set of contracts to the agent in each period, even after learning the type of the agent. We believe that this is consistent with the observation that wage contracts in some occupations tend to be quite stable, for example for teachers, college professors and civil servants. It is also worth noting that these are jobs in which the employer and the employee both expect that the employee will be in the job for a fairly long period. Further, although the employer is quite likely to infer the productivity of the employee quite early, there is little evidence of the ratchet effect, nor is there evidence of a large compensation paid to the employee at the start of employment. The evidence also suggests that employees who reveal themselves to be a high-productivity type are likely to get a premium built into the wages or salaries; which can be interpreted as the information rent that is paid to the more productive types in the separating equilibrium contracts. Therefore, unlike the finite-horizon equilibrium contracts, in the infinite horizon, equilibrium contracts are not subject to either the ratchet effect or take-the-money-and-run strategy. As already mentioned, the equilibrium strategies of the perfect Bayesian equilibrium analyzed here use a “carrot and stick strategy” in which deviations by the principal and the agent is deterred by the threat of credible punishments. The principal is deterred from deviating from the optimal contract, even after updating beliefs about the type of the agent, by the agent producing an output of zero for several periods. This is the “stick” part of the strategy. However, for the threat to produce zero output to be credible, the agent has to receive some payment back for carrying out the threat, as otherwise the payoff of the agent is zero in each period. Therefore, when the agent gets back to producing a positive output, after the punishment phase is over, the agent is given a slightly higher rent than before. This is the “carrot” part of the strategy. The strategy used by the principal to deter deviations by the agent is similar. The agent is deterred from deviating by the threat that the principal will offer a rent-free contract irrespective of the type of the agent for several periods, in case the agent deviates. Such “carrot and stick” strategies are similar to the ones used to analyze subgame perfect equilibrium of infinite horizon games of complete information as in Fudenberg and Maskin (1986), as well as in an incomplete information setting as in Chakrabarti (2010). The result here shows that the general idea of such “carrot and stick” strategies can also offer useful insights about perfect Bayesian equilibrium in the case of adverse selection models by properly specifying the “stick” and the “carrot” and how these can be designed when the type of the agent is private information. Thus the result here shows that the general idea of the “carrot” and “stick” strategy can be adapted not only to the case of finitely many types as in Chakrabarti (2010), but also to the case of the continuum of types in adverse selection models. We show that the results can be widely applied by analyzing the model of regulation in Laffont and Tirole (1988) in which the cost parameter \(\beta \) is drawn from the interval \([{{\underline{\beta }}}, {{\bar{\beta }}}]\).Footnote 3. We also study the nonlinear optimal pricing model in which the type of the buyers are drawn from a continuum. The main result is based on the continuum-type case but the result also holds for the discrete-type case. In fact the result for the discrete-type case can be quickly derived from the result for the continuum of type case. Our reason for focusing on the continuum of type case derives from the fact that the result is less obvious for the continuum of type case, and the fact that the result for the discrete-type case is an immediate corollary of the continuum of type case. In the case of the model studied by Laffont and Tirole (1988), in which the type of the firm is drawn from a continuum, it is much harder for separation of types to hold in an equilibrium in the finite horizon, when there is a continuum of types, and pooling becomes the norm. This makes the result we present here of much greater interest and indicates the sharp contrast between the finite horizon models and the infinite horizon models. The paper is laid out as follows. In Sect. 2 we describe the main definitions, the general framework and the main results. In Sect. 3 we discuss the two main applications, with the infinite horizon version of the Laffont and Tirole model (Laffont & Tirole, 1988) in Sect. 3.1 and the optimal nonlinear prices in Sect. 3.2. In Sect. 4 we conclude.",
94.0,3.0,Theory and Decision,12 July 2022,https://link.springer.com/article/10.1007/s11238-022-09895-8,Myopic-farsighted absorbing networks,April 2023,Pierre de Callataÿ,Ana Mauleon,Vincent Vannetelbosch,Male,Female,Male,Mix,,
94.0,3.0,Theory and Decision,24 August 2022,https://link.springer.com/article/10.1007/s11238-022-09897-6,When punishers might be loved: fourth-party choices and third-party punishment in a delegation game,April 2023,Yuzhen Li,Jun Luo,Hang Ye,Unknown,,,Mix,,
94.0,3.0,Theory and Decision,09 August 2022,https://link.springer.com/article/10.1007/s11238-022-09899-4,Extracting the collective wisdom in probabilistic judgments,April 2023,Cem Peker,,,Male,Unknown,Unknown,Male,"Decision-making is often a problem of assessing the chances of uncertain events. Scientists make probabilistic projections on natural phenomena, such as the occurrence of a major earthquake or the effects of anthropogenic climate change. Strategists assess the likelihood of important geopolitical events. Investors form judgments on the risks involved in investments. Economists and policy makers need probabilistic predictions on policy outcomes and macroeconomic indicators. Individual judgments may be subject to biases such as optimism, overconfidence, anchoring on an initial estimate, focusing too much on easily available information, neglecting an event’s base rate, and many more (Kahneman and Tversky 1973; Tversky and Kahneman 1974; Kahneman et al. 1982). Combining multiple judgments to leverage ‘the wisdom of crowds’ is known to be an effective approach in improving accuracy (Surowiecki 2004; Makridakis and Winkler 1983). The use of collective wisdom involves choosing an aggregation method that combines individual predictions into an aggregate prediction (Armstrong 2001; Clemen 1989; Palan et al. 2019). Previous work found simple averaging to be surprisingly effective, typically outperforming more sophisticated aggregation methods and showing robustness across various settings (Makridakis and Winkler, 1983; Mannes et al. 2012; Winkler et al. 2019; Genre et al. 2013). Intuitively, simple averaging allows statistically independent individual errors to cancel, leading to a more accurate prediction (Larrick and Soll 2006). However, in some prediction tasks, forecasters may have common information through shared expertise, past realizations, knowledge of the same academic works, etc. (Chen et al. 2004). Then, individual errors may become correlated, resulting in a bias in the equally weighted average of predictions (Palley and Soll 2019). In theory, the decision maker in a given task can select and weight judgments such that the errors perfectly cancel out (Clemen and Winkler, 1986; Mannes et al. 2014; Budescu and Chen 2015). However, optimal weights depend on how experts’ prediction errors are correlated and are typically unknown to the decision maker. Some existing methods aim to estimate appropriate weights using past data from similar tasks (Budescu and Chen, 2015; Mannes et al. 2014). The effectiveness of this approach is limited by the availability and reliability of past data. Another line of work proposed competitive elicitation mechanisms (Ottaviani and Sørensen 2006; Lichtendahl Jr and Winkler 2007), which may improve the calibration of the average forecast when forecasters have common information (Lichtendahl Jr et al. 2013; Pfeifer et al. 2014; Pfeifer 2016). Such competitive mechanisms are sensitive to strategic considerations of forecasters (Peeters et al. 2022). This paper develops the Surprising Overshoot (SO) algorithm to aggregate judgments on the likelihood of an event. I consider a setup where experts form their judgments by combining shared and private information on an unknown probability. When shared information differs from the true probability, experts are likely to err in the same direction, resulting in a miscalibrated average prediction. The SO algorithm relies on an augmented elicitation proposed in recent work (Prelec, 2004; Prelec et al. 2017; Palley and Soll 2019; Palley and Satopää 2022; Wilkening et al. 2022): Experts report a prediction of the probability as well as an estimate of the average of others’ predictions, which is referred to as a meta-prediction. I show that when the average prediction is a consistent estimator, the percentage of predictions and meta-predictions that overshoot the average prediction should be the same. An overshoot surprise occurs when the two measures differ, which indicates that the average prediction is an inconsistent estimator. The SO estimator uses the information in the size and direction of the overshoot surprise to account for the shared-information problem. It does not require the use of past data. I test the SO algorithm using experimental data from two sources. Palley and Soll (2019) conducted an experimental study where subjects are asked to predict the number of heads in 100 flips of a biased coin. Their experiment implements shared and private signals as sample flips from the biased coin. The second source is Wilkening et al. (2022), who conducted two experimental studies. The first experiment replicates the earlier study by Prelec et al. (2017) which asked subjects true/false questions about the capital cities of U.S. states. However, unlike Prelec et al. (2017) they also ask subjects to report probabilistic predictions and meta-predictions, which allows an implementation of the SO algorithm. In the second experiment, Wilkening et al. (2022) generate 500 basic science statements and ask subjects to report probabilistic predictions and meta-predictions on the likelihood that a given statement is true. Results suggest that the SO algorithm outperforms simple benchmarks such as unweighted averaging and median prediction. I also compare the SO algorithm to alternative solutions for aggregating probabilistic judgments, which elicit similar information from individuals (Palley and Soll 2019; Martinie et al. 2020; Palley and Satopää, 2022; Wilkening et al. 2022). The SO algorithm compares favorably to alternative aggregation mechanisms in prediction tasks where individual predictions are highly dispersed. Experimental evidence suggests that the SO algorithm is especially effective in extracting the collective wisdom from strongly disagreeing probabilistic judgments in moderate to large samples of experts. This paper contributes to the literature of judgment aggregation mechanisms that utilize meta-beliefs to improve prediction accuracy. The Surprisingly Popular (SP) algorithm picks an answer to a multiple choice question based on predicted and realized endorsement rates of alternative choices (Prelec et al. 2017). The Surprisingly Confident (SC) algorithm determines weights that leverage more informed judgments (Wilkening et al. 2022). The SP and SC algorithms aim to find the correct answer to a binary or multiple-choice question while the SO algorithm produces a probabilistic estimate on a binary event. Recent work developed aggregation algorithms for probabilistic judgments as well. Pivoting uses meta-predictions to recover and recombine shared and private information optimally (Palley and Soll 2019). Knowledge-weighting constructs a weighted average such that the accuracy of weighted crowd’s aggregate meta-prediction is maximized (Palley and Satopää 2022). Meta-probability weighting also attaches weights to individual predictions where the absolute difference between an individual’s prediction and meta-prediction is considered as an indicator of expertise (Martinie et al. 2020). In testing the performance of the SO algorithm, pivoting, knowledge-weighting and meta-probability weighting are considered as benchmarks. As mentioned above, the SO algorithm performs especially well when individual judgments are highly dispersed. In practice, such problems are likely to be the most challenging ones, where expert judgments disagree substantially and it is not clear how judgments should be aggregated for maximum accuracy. The rest of this paper is organized as follows: Sect. 2 introduces the formal framework. Sect. 3 develops the SO algorithm and establishes the theoretical properties of the SO estimator. Sect. 4 introduces the data sets and benchmarks we consider in testing the SO algorithm empirically. The same section also presents some preliminary evidence on how overshoot surprises relate to the inaccuracy in average prediction. Sect. 5 presents experimental evidence testing the SO algorithm. Sect. 6 provides a discussion on the effectiveness of the SO algorithm. Section 7 concludes.",1
94.0,3.0,Theory and Decision,21 August 2022,https://link.springer.com/article/10.1007/s11238-022-09901-z,When are two portfolios better than one? A prospect theory approach,April 2023,Luc Meunier,Sima Ohadi,,Male,Female,Unknown,Mix,,
94.0,3.0,Theory and Decision,24 August 2022,https://link.springer.com/article/10.1007/s11238-022-09898-5,Intertemporal choice with savoring of yesterday,April 2023,Pavlo R. Blavatskyy,,,Male,Unknown,Unknown,Male,"The problem of intertemporal choice arises when outcomes are received in different moments of time. Samuelson (1937) proposed to evaluate intertemporal prospects with discounted utility that is also known as constant or exponential discounting. However, already Samuelson (1937, p. 159) acknowledged that “… it is completely arbitrary to assume that the individual behaves so as to maximize [discounted utility]. This involves the assumption that at every instant of time the individual’s satisfaction depends only upon the consumption at that time, and that, furthermore, the individual tries to maximize the sum of instantaneous satisfactions reduced to some comparable base by time discount. As has been suggested, we might assume that the individual maximizes an integral which contains not only consumption per unit of time but also the rate of change of consumption per unit of time…”. This paper goes essentially in this direction by considering a decision maker who derives utility not only from current consumption but also from “residual” consumption in the previous moment of time. “Complete arbitrariness” of Samuelson (1937) model became more acceptable in neoclassical microeconomic theory after Koopmans (1960) provided the preference foundation (axiomatization) of discounted utility. Yet, Koopmans (1960, p. 292) also was rather skeptical about the descriptive realism of the model: “… we are willing to postulate that the particular bundle of commodities to be consumed in the first period has no effect on the preference between alternative sequences of bundles in the remaining future, and conversely. One cannot claim a high degree of realism for such a postulate, because there is no clear reason why complementarity of goods could not extend over more than one time period.” Baucells and Sarin (2007, p. 170) nicely summarize this descriptive problem: “Simply stated, consumption independence requires that the utility of current consumption does not depend on past consumption. It is easy to see that the utility of current consumption (spicy food today) may depend on past consumption (spicy food yesterday), especially when the time interval between periods is small. For some consumption goods, such as a vacation or a particular movie, consumption independence may not hold even when time periods are separated by as much as a year.” Baucells and Sarin (2007) proposed a generalization of discounted utility theory where a decision maker derives utility not only from current consumption but also from the “satiation level” of consumption, which is the cumulative discounted consumption in all previous periods. Our model overlaps with the model of Baucells and Sarin (2007) in two ways. First, in our model, only the consumption in the previous moment of time contributes to utility evaluation of consumption in the current moment whereas in Baucells and Sarin (2007) the consumption in all past moments of time contributes to utility evaluation of consumption in the current moment. Second, in our model utilities are discounted with weights that are essentially a quasi-hyperbolic discounting function whereas Baucells and Sarin (2007) use constant (exponential) discounting as in discounted utility. Koopmans (1960, pp. 293–294) showed that constant (exponential) discounting in discounted utility is essentially due to the assumption of stationarity: “We … require that the preference ordering be the same as the ordering of corresponding programs obtained by advancing the timing of each future consumption vector by one period (and, of course, forgetting about the common first-period vector originally stipulated). This expresses the idea that the passage of time does not have an effect on preferences.” Thaler (1981, 202) was one of the first to show that the passage of time does, in fact, affect time preferences, which is known as dynamic inconsistency. Thaler (1981, 202) argued that a decision maker may prefer to consume one apple today over two apples tomorrow and have a reversed preference when both consumptions are delayed for 1 year. This particular descriptive limitation of discounted utility became known as the common difference effect (Loewenstein and Prelec 1992, section II.1, p.574). Quasi-hyperbolic discounting (Elster 1979; Laibson 1997; Phelps and Pollak 1968) generalizes constant discounting by allowing the discount factor between the present and the following moment of time to be different from constant discount factor thereafter. Inter alia this accounts for the common difference effect. This paper shows that quasi-hyperbolic discounting emerges in a model of intertemporal choice with one period lag. If consumption in the previous moment of time contributes to utility evaluation of consumption in the current moment of time, we are forced to weaken Koopmans’ stationarity principle: if two consumption streams have the same consumption in the first and the second period then a decision maker’s preference between these streams does not change when all consumption is advanced by one period. Somewhat surprisingly, this weaker version of Koopmans’ stationarity results in essentially a quasi-hyperbolic discounting function. Duesenberry (1952) pioneered the idea of current consumption being affected by “habits” of the past consumption. The models of habit formation (e.g., Pollak, 1970; Ryder and Heal, 1973) assume that habits are captured by the sum of (constantly) discounted consumption in all past moments of time. Becker and Murphy (1988) study the consumption of addictive products in the framework of habit formation. Loewenstein (1987) proposes a reversed model where current utility depends not only on the current consumption but also—on the “anticipatory” future consumption (constantly discounted sum of future consumption). Wathieu (1997, 2004) proposed another model where a decision maker derives utility not only from current consumption but also from consumption in the past moments of time. Wathieu (1997, 2004) postulates that a decision maker derives utility from increment of the current utility over a reference point, which is a weighted sum of all past consumptions and the initial (given) reference point. Wathieu (1997, 2004) aggregates these utilities with constant (exponential) discounting. Blavatskyy (2016) proposed another model where past consumption matters for utility evaluation of the current consumption. In rank-dependent discounted utility model, “ … a decision maker behaves as if maximizing the sum of discounted incremental utilities of future payoffs. A decision maker aggregates the stock of payoffs and subsequent future payoffs are evaluated by their contribution to the overall utility of this stock” (Blavatskyy (2016, p. 788). This model does not generalize Samuelson (1937) discounted utility—two models overlap only if utility function is linear. Blavatskyy (2016, p. 790) allows for non-constant discounting. The remainder of the paper is organized as follows. Section 2 presents our proposed model. Section 3 details behavioral characterization (axiomatization) of this model. Section 4 applies the model to the problem of intertemporal consumption/savings. Section 5 concludes.",
94.0,4.0,Theory and Decision,18 September 2022,https://link.springer.com/article/10.1007/s11238-022-09906-8,Rationality applied: resolving the two envelopes problem,May 2023,Christian Hugo Hoffmann,,,Male,Unknown,Unknown,Male,"Variants of the following problem have acquired a certain renown (e.g. Nalebuff, 1989): You are presented with a choice between two sealed envelopes. Each envelope contains some quantity of money, which can be of any positive real magnitude. You know that one envelope contains twice as much money as the other, but you do not know which contains the larger sum and which is the smaller. You choose one of them—call it “A” and the other “B”. You can keep the money in A (a1), whose numerical value you do not know at this stage, or you can switch A for B (a2). You wish to maximise your money. What should you do? This is the standard formulation of the Two Envelopes Problem, a much studied ‘paradox’ in decision and probability theory, mathematical economics, logic and philosophy. Time and again a new analysis is published in which the author(s) claim(s) finally to explain what actually goes wrong in this puzzle. Each author is eager to emphasize what is new and exceptional in her or his approach and is inclined to conclude that earlier approaches did not get to the root of the matter. This paper addresses the Two Envelopes Problem from a different perspective which will not only enrich the debate but might also resolve the issue. By all means, it will become clear that the presented Two Envelopes Problem does not deserve to be called a paradox (and certainly not an unresolved paradox, as some writers still insist on claiming). Rather, there has only been an easily solvable decision problem—where anything except indifference has no rational ground—serving as one possible starting point for calling basic assumptions of decision theory into question. The structure of this paper, consisting of three parts, is as follows. First, I begin with a short summary of various replies to the Two Envelopes Problem that have been given in the literature by consulting a variety of experts. Their arguments lose cogency, however, when we critically analyse them and depart from the special frameworks they adopted. In Sect. 3, I turn to a more promising approach to tackle the Two Envelopes Problem. This approach is found in a proposal by Priest and Restall (2003). In the final Sect. 4, I will then correct some small flaws and add some new insights, including a clear structuring of the problem, an explanation for where the reasoning of Priest and Restall may go astray, and including a sophisticated conclusion regarding my own proposed solution to the issue.",
94.0,4.0,Theory and Decision,01 October 2022,https://link.springer.com/article/10.1007/s11238-022-09907-7,A Misfit model: irrational deterrence and bounded rationality,May 2023,Karl Sörenson,,,Male,Unknown,Unknown,Male,"Research on deterrence has come a long way; from the early discussions by Brodie into the territory of game theoretic analysis onwards via Schelling (1960, 1966), Kahn (1960) and Selten (1978), to more sophisticated modelling of deterrence games that is strongly linked to theories of deterrence (Powell, 1990; Quackenbush, 2011a; Zagare & Kilgour, 1993, 2000). This later development has meant that game theoretic models have become an inalienable part of contemporary theories of deterrence. While the coherent relationship between model and theory clearly has been an advance for deterrence research, it has also meant a shift concerning how the relationship between theory and model is viewed.Footnote 1 But what do we do with an insight that does not fit with the more advanced accounts of deterrence research? In 1966 Schelling suggested that a way to deter was to behave irrationally (Schelling, 1966, 37). The idea has been criticized since it relies on a notion of a player behaving irrational to deter, but rationality to account for why a player is deterred (Field, 2014; Quackenbush, 2011b; Zagare, 2018; Zagare & Kilgour, 2000). However, in the go-to method for deterrence research, game theory, players are assumed to be rational—not just in a part of their behavior. An account of deterrence that violates a central solutions concept cannot remain a part of such a theory. It is a misfit model. Yet, Schelling may have a point: an agent that behaves in a hazardous manner randomizing between strategies would be a concern for an opponent. So what are we to do with Schelling’s insight regarding irrational behavior for effective deterrence? Should we exclude it and thereby constrain deterrence modelling? Or should we allow it and undercut the core method for the theory? This article argues that there are two issues in play; one has to do with the relation between model and representation of a phenomenon, the other concerns the relationship between model and theory and which scientific ideals we assume. These two issues are dealt with consecutively; first the Mutual Deterrence game, which Schelling departs from, is presented with standard assumptions and solutions. The idea of irrational behavior and deterrence is addressed and a bounded rational model, known as level-k, is suggested as way of methodologically address the phenomenon of irrational deterrence identified by Schelling. With this type of modelling we need not exclude the notion of something like irrational threats on the basis of inconsistent application of solution concepts. The second issue pertains to how the model fits into contemporary deterrence research. This issue is not straightforward as it relates both to guiding scientific ideals as well as the purpose of a given modelling project. Model-theory coherency is a reasonable scientific ideal to assume, the question is what other ideals one exclude if one lets this particular ideal reign sole and supreme. It is argued that if we weigh in both the purpose of a model and open up for other scientific ideals such as problem-solving, then it may be easier to accept a deterrence behavior that does not conform to standard game theoretic assumptions. This might limit a particular model’s contribution to an already well developed theory, but it will expand the possibility to model another part of the deterrence phenomenon. This is the contribution of level-k model.",1
94.0,4.0,Theory and Decision,26 August 2022,https://link.springer.com/article/10.1007/s11238-022-09902-y,Prospect theory in multiple price list experiments: further insights on behaviour in the loss domain,May 2023,Géraldine Bocquého,Julien Jacob,Marielle Brunette,Female,Male,Female,Mix,,
94.0,4.0,Theory and Decision,27 September 2022,https://link.springer.com/article/10.1007/s11238-022-09908-6,Asymmetric guessing games,May 2023,Zafer Akin,,,Male,Unknown,Unknown,Male,"Any investment situation that can be considered a complex game with its potentially high number of players and strategies, requires both deep reasoning and strategic thinking due to the mutual determination of the resulting outcomes. The “guessing” or “p-beauty contest” games (inspired by Keynes, 1936) capture this insight and resemble decision-making in financial markets. In the standard N-player guessing game (\(N\ge 3\)), players simultaneously choose a number from a closed interval, generally [0, 100], and the player whose number is closest to a given fraction (p) of the average of all of the chosen numbers is the winner. The game is dominance solvable and all players choose zero in the unique Nash equilibrium. However, experimental findings are not aligned with this theoretical prediction: in general, participants apply the iterated dominance process (level-k thinking) only up to two-three rounds but convergence is achieved as the game is played out repeatedly.Footnote 1
Grosskopf and Nagel (2008) introduce an even simpler version of this game with \(N=2\) in which iterative reasoning is unnecessary and find a similar pattern.Footnote 2 In this paper, we contribute to this literature by incorporating asymmetric players. We modify the standard guessing game by allowing some players to be relatively more powerful in influencing the target number. It is interesting to study heterogeneity/asymmetry in guessing games because in practice in almost no market are players identical/symmetric in regards to their effect on the market due to differences in wealth, reputation, experience, etc. Moreover, since we have a good understanding of the behavior of symmetric players, it is important to know how robust the equilibrium with symmetric players is with respect to asymmetry, whether and how the players’ behavior change, and whether there is a qualitative change in the structure of equilibrium. In real markets, there is heterogeneity/asymmetry among players (Ken Arrow, in Colander et al., 2004). In various dimensions, asymmetry is the norm rather than being an exception. Asymmetries in information, beliefs and expectations, level of (bounded) rationality, time and risk preferences, strategic intelligence, cognitive ability, reputation, and experience are some examples of observed asymmetries in markets and there is a huge literature that incorporates these into many economics and finance applications.Footnote 3 Industrial organization (IO) literature incorporates many different asymmetries among firms. Standard textbook competition models introduce marginal cost advantage, costly entry, product differentiation, being the first mover, etc. to be able to model firms’ asymmetric market powers. Firm size is one factor for a firm to emerge as the leader in a market in the Stackelberg competition model. Firm size is also a crucial factor for firm growth and sales (Chen, 2008; Hottman et al., 2016; Sutton, 2007). Hortaçsu et al. (2019) find that larger firms are more strategically sophisticated and higher strategic sophistication improves efficiency. Hottman et al. (2016) examines how firm differences in various dimensions such as size explain firm sales. In the Bertrand competition context, Dufwenberg et al. (2007) show that introducing asymmetries in the rationality of players better explains the observed behavior. Levine et al. (2017) show how asymmetries in strategic intelligence and sophistication (measured by players’ choices in the guessing game) fuel superior performance even in intensely competitive markets. Furthermore, a survey study conducted by Einav and Levin (2010) emphasizes how the observed firm heterogeneity plays an important role in international trade (Melitz, 2003; Redding, 2011) and industrial productivity (Olley & Pakes, 1996). Röller and Sinclair-Desgagné (1996) summarize why technological and organizational capability differences among competitors within an industry are persistent.Footnote 4 These and many other studies model asymmetries in various dimensions among firms and they have important implications for the market power of firms and resulting market outcomes. Asymmetry is critical in finance literature as well. This asymmetry is usually represented by a distribution of agents, wealth, or strategies (LeBaron, 2006). Many key debates about market efficiency, rationality, and market anomalies are addressed by assuming heterogeneous agents (Dieci & He, 2018). Behavioral finance also addresses these by allowing for individual differences in terms of beliefs, preferences, information processing capabilities, sophistication, time preferences, etc. (Barberis & Thaler, 2005; Thaler, 1993, 1995). In finance, investors are different in many dimensions but they are usually categorized as small (individual) and large (institutional, e.g., pension funds) investors based on their trade size (Lee & Radhakrishna, 2000). Their investment and trading behavior differ since they have different incentives, experiences, learning opportunities, education, skills, etc. (Malmendier and Shanthikumar, 2007; Mikhail et al., 2007). Thus, it is important to incorporate asymmetries into the models in economics and finance to better understand the behavior of strategically interacting firms and individuals. In the guessing games literature specifically, there are few studies involving asymmetries. Costa-Gomes and Crawford (2006) create asymmetry in two-player continuous payoff guessing games by having different p values and support to characterize which decision rules players employ. Similarly, Güth et al. (2002) introduce heterogeneity with different p values with continuous payoffs and interior equilibrium and find that heterogeneity slows down convergence and reduces earnings. Kovac et al. (2008) replicate Güth et al. (2002) and find conversely that heterogeneous players guess closer and converge faster to the equilibrium.Footnote 5 Another type of asymmetry is introduced within groups based on experience and sophistication. Slonim (2005) finds that experienced players who have a better grasp of how the game is actually played, tend to shift their choices away from the equilibrium as inexperienced players are added to the population, and earn more. With a similar design, Liu (2016) systematically varies the proportion of players who know how the game should be played and finds that choices tend to decrease as the proportion of informed players increases, but exclusively when this proportion is large. Agranov et al. (2012) also change the composition of players by varying the proportion of random-choosing computers and graduate students and find that as the proportion of graduate students increases, players lower their choices. There is another vein of literature focusing on the heterogeneity of subjects in terms of their personal characteristics and cognitive ability.Footnote 6
Alaoui and Penta (2016) show that heterogeneity emerges within the same subject such that more sophisticated players (subjects from more quantitative background) adjust their level-k downwards against less sophisticated players (humanity students) who do not react to a more sophisticated subject pool. This supports the model in Alaoui et al. (2020) showing that the cognitive bound is binding for most subjects. Gill and Prowse (2016) show that more cognitively able subjects choose numbers closer to equilibrium, converge more frequently to equilibrium, and earn more. Kopányi et al. (2019) investigate how price dynamics in a learning to forecast (LtF) asset pricing experiment are influenced by financial advisors who attract more investors by forecasting more accurately and are able to influence market prices asymmetrically. Successful financial advisors attract more money and therefore they have a greater impact on market prices. They show that the asymmetry may reduce price volatility and mispricing unless the competition is not fierce. The focus and structure of the asymmetry in the aforementioned papers are different than in ours. They all treat players symmetrically in terms of their influence on the formation of the target number. We focus on the relative influence of players on the target. In our model, there is only one target and the power of players to influence the target is different, whereas different players have different targets in Costa-Gomes and Crawford (2006) and Güth et al. (2002). In Slonim (2005), Liu (2016), and Agranov et al. (2012), there is only informational asymmetry among players but in our design, heterogeneity is driven solely by the differing strengths of players to influence the target without any informational asymmetry. In the last set of studies (Alaoui and Penta, 2016; Alaoui et al., 2020; Gill and Prowse, 2016), subjects are intrinsically asymmetric in terms of their characteristics that affect their level-k behavior through their cognitive limits and beliefs. However, we create an extrinsic asymmetry among subjects in terms of their ability to affect the target. Kopányi et al. (2019) is closely related to ours in terms of both the types of markets it models and the asymmetric impact of the players on the outcome. Our model can also be motivated by the fact that in financial markets, there are investment advisors who manage different account sizes and their impact on the markets differ accordingly. Kopányi et al. (2019) use experimental LtF asset markets in which the impact of players and prices are endogenously determined. We use guessing games to model these markets where the impact of players is different but fixed. Specifically, we incorporate asymmetry by introducing replicas of the players that render some players relatively more powerful in influencing the target number. In two-player games, this comes down to multiplying the chosen numbers of strong and weak players by \(r_{\text {s}}\ge r_{\text {w}}\ge 1\), respectively. Integer coefficients \(r_{\text {s}}\) and \(r_{\text {w}}\) can be considered as the strengths of the players (r, abbreviation for replicas). The target number is some fraction of the “weighted” average of the two choices. This implies that the strong player has more power to influence the target number in comparison to the weak player. The theoretical solution of our model depends on the relative strength of the players and the value of p. We first characterize the equilibrium for two-player case and then extend our theoretical analysis to N-players. Our theoretical analyses show that for any given \(r_{\text {w}}\) and \(r_{\text {s}}\) values, if p is small enough, the equilibrium is the same as with the standard guessing game. On the other hand, if p is high enough, the payoff structure of the game changes and there is at least one mixed-strategy (no pure strategy) Nash Equilibrium. In this case, the weak player’s winning strategy is imitating the strong player. She can only win if she chooses a smaller (but not too small) number than the strong player’s number. For a general N-player case where each player i has a potentially different \(r_{i}\ge 1\), we characterize a sufficient condition—the strongest player is strong enough with a small p or if the asymmetry among players is not high- for choosing zero for all players to be the unique pure strategy Nash equilibrium (PSNE). We then report the results of an experiment we conducted that adopts commonly used design features in the literature, excluding the asymmetric influence of players on the target. We form two and three-player groups (In many studies, \(N\ge 3\). At one extreme, \(N=1\) in Bosch-Rosa and Meissner (2020), in the other, \(N=7900\) in Bosch-Domenech et al. (2002)). To keep the game as simple as possible, we conduct the experiment by using \(p=\frac{1}{2}\) such that in all games, there is a unique equilibrium in which all players choose zero as in the standard game (\(p=\frac{1}{2}\) in Duffy and Nagel (1997) as well.) Costa-Gomes and Crawford (2006), Güth et al. (2002) and Nagel (1995) use p as a treatment variable including \(p=\frac{1}{2}\), but most frequently, \(p=\frac{2}{3}\) is used in the literature. In our design with two players, the control is the standard two-player guessing game where \(r_{\text {s}}=r_{\text {w}}=1\). In the treatment, the players are asymmetric, \(r_{\text {w}}=1\) and \(r_{\text {s}}=9\). In three-player games, in the control, we have \(r_{\text {w1}}=r_{\text {w2}}=r_{\text {s}}=1\), whereas \(r_{\text {w1}}=r_{\text {w2}}=1\) and \(r_{\text {s}}=8\) in the treatment.Footnote 7 We use (weighted) mean as the order statistics in the calculation of the target. We use the common, bounded, and fixed support for guesses [0, 100] (In Costa-Gomes and Crawford (2006), the support is a treatment variable. Benhabib et al. (2019) have an unbounded guessing interval). We have a tournament structure and there is only one winner unless there is a tie (see Costa-Gomes and Crawford (2006), Güth et al. (2002), Kocher et al. (2006), and Nagel et al. (2017) for continuous payoff structure where each player is paid according to their distance to the target). Regarding the formation of the groups, participants are anonymously and randomly matched. The roles are randomly assigned. Participants play the game individually against each other (see Kocher and Sutter (2005), Kocher et al. (2006), and Sutter (2005) for guessing games played by teams) and there is no communication (see Baethge (2016), Burchardi and Penczynski (2014) and Penczynski (2016) for the effect of communication). Participants play the game for ten rounds in a fixed pair setting (see Burchardi and Penczynski (2014), Fragiadakis et al. (2013), and Grosskopf and Nagel (2009) for repeated play in the two-player guessing games). Finally, at the end of each round, we give full feedback to all players in the group (see Grosskopf and Nagel (2009), Kocher et al. (2014), Sbriglia (2008), and Weber (2003) for the effect of feedback). Our main research questions are: (i) How do the first-period behavior and overall choices differ between the symmetric and the asymmetric cases? (ii) Do the choices in each of the symmetric and asymmetric cases converge to the equilibrium over time? (iii) If there is convergence, are there any differences in terms of the speed of convergence between the symmetric and asymmetric cases? Our results imply that non-equilibrium behavior is more common and overall choices are farther from the equilibrium in two-player asymmetric games than in symmetric games. In all cases, there is convergence towards equilibrium. Introducing asymmetry slows down the convergence to the equilibrium in two, but not in three-player games. Finally, strong players earn slightly more than weak players, and asymmetry increases the discrepancy in choices in both games. The rest of the paper is organized as follows. Section 2 introduces the model and characterization of the equilibrium. Section 3 describes the experimental design in detail. Section 4 presents the experimental results. Finally, Sect. 5 concludes with a discussion of the results.",
94.0,4.0,Theory and Decision,23 August 2022,https://link.springer.com/article/10.1007/s11238-022-09903-x,On stability of economic networks,May 2023,Hamid Beladi,Xiao Luo,Nicholas S. P. Tay,Male,,Male,Mix,,
94.0,4.0,Theory and Decision,25 August 2022,https://link.springer.com/article/10.1007/s11238-022-09904-w,The classification of preordered spaces in terms of monotones: complexity and optimization,May 2023,Pedro Hack,Daniel A. Braun,Sebastian Gottwald,Male,Male,Male,Male,"The question of how well a preorder relation can be captured through real-valued functions is an ongoing research topic since the introduction of utility functions in the early days of mathematical economics. The key observation is that sometimes preferences can not only be measured locally to decide between two elements, but there might be a global real-valued preference function that fully captures the corresponding order relation. That is, in certain situations, one can not only choose a preferable item between any two items in a given set of options, but one can find a single function, or a family of functions, defined on the decision space whose function values quantify the preference relation, so that one can compare function values to decide about the order relation of the corresponding arguments. Since the existence of such functions only depends on the properties of the corresponding preorder, this idea can naturally be applied in many domains of science. In particular, instead of considering preference relations and utility functions on decision spaces, many systems of interest can be thought of as sets of possible states endowed with an order relation encapsulating the intrinsic tendency of the system to transition from one state to another. The fields where these ideas are relevant include thermodynamics (Lieb & Yngvason, 1999; Giles 2016), general relativity (Bombelli et al., 1987; Minguzzi, 2010), quantum physics (Nielsen, 1999; Brandao et al., 2015) and economics (Debreu, 1954; Ok, 2002), among others. The basic property of these real-valued functions f is that they have to be monotones with respect to the corresponding preorder \(\preceq\), that is, \(x\preceq y\) implies \(f(x)\le f(y)\). There are mainly three types of monotones that appear in this context: strict monotones (Alcantud et al., 2016; Peleg, 1970; Richter, 1966), injective monotones (Hack et al., 2022a, b), and utility functions (Debreu, 1954, 1964). In particular, these different types of monotones are used to classify preordered spaces mostly in two different ways: either by whether a given type of monotone exists, or, by whether there exists a family of such monotones, known as a multi-utility, that characterizes the preorder completely (Evren & Ok, 2011; Alcantud et al., 2013, 2016; Hack et al., 2022a, b; Bosi & Herden, 2012, 2016). Equivalently, these spaces can be classified according to either the existence of optimization principles with certain characteristics or the complexity of the preorder, that is, the amount and type of multi-utilities that exist for them. Even though the cardinality of such representing families plays an important role, so far mostly the two cases of countable multi-utilities and multi-utilies consisting of a single element, that is, utilities, have been considered. Moreover, several connections between both types of classifications have been pointed out in the literature (Alcantud et al., 2016; Hack et al., 2022a, b; Alcantud et al., 2013; Bosi et al., 2018), but certain gaps in these connections have prevented the presentation of a general classification of preordered spaces through real-valued monotones. One of the aims of this contribution is to reduce this gap, achieving, thus, a more complete classification (see Fig. 1) and, hence, a better understanding of both complexity and optimization, including how they are related, in preordered spaces. In particular, we take advantage of a characterization of real-valued monotones in terms of families of increasing sets (Alcantud et al., 2013; Hack et al., 2022a, b) that allows to distinguish more classes of preordered spaces than before, both in terms of the cardinality of the multi-utilities and the cardinality of the quotient space of the preorder. Importantly, by providing the corresponding counter examples, we show that certain classes of preorded spaces are, in fact, strictly contained in each other, which, to our knowledge, was not known before. Classification of preordered spaces according to the existence of various real-valued monotones. A distinction between our contributions here and previously known results can be found in the discussion (Sect. 4). Moreover, our contributions can be visualized in Fig. 2",
94.0,4.0,Theory and Decision,25 September 2022,https://link.springer.com/article/10.1007/s11238-022-09905-9,Hold-up induced by demand for fairness: theory and experimental evidence,May 2023,Raghabendra Pratap KC,Dominique Olié Lauga,Vincent Mak,Unknown,,Male,Mix,,
95.0,1.0,Theory and Decision,04 October 2022,https://link.springer.com/article/10.1007/s11238-022-09911-x,Downside risk aversion vs decreasing absolute risk aversion: an intuitive exposition,July 2023,James K. Hammitt,,,Male,Unknown,Unknown,Male,"What is the relationship between downside risk aversion (downside RA) and decreasing absolute risk aversion (DARA)? Under expected utility, both properties imply the harm from risk bearing is smaller when the individual has greater wealth, but they are not equivalent. Downside RA (Menezes et al., 1980) is equivalent to prudence (Kimball, 1990); it is characterized by a positive third derivative of the utility function.Footnote 1 DARA is characterized by the property that the risk premium for a lottery is a decreasing function of baseline wealth (Pratt, 1964). DARA implies downside RA but the converse is not true. The difference between the concepts arises because they measure the harm from risk bearing differently. Downside RA measures the harm using the utility premium, and DARA measures it using the risk premium. To strengthen intuition about the two concepts, I compare downside RA and DARA from several perspectives. My analysis builds on the work of Eeckhoudt and Schlesinger (2006), who showed that downside RA can be described as a preference for adding risk to the better outcome rather than to the worse outcome of a binary, equal-probability lottery. The first perspective is a graphical derivation of the utility and risk premia, clarifying how the change in these premia with wealth depends on the rate of change of the curvature of the utility function, and hence on the third derivative of utility. The second perspective shows how adding risk to the better or worse branch of an equal-probability binary lottery is analogous to a mean-decreasing contraction or a mean-decreasing spread of the initial lottery, respectively. Since risk aversion is equivalent to a preference for mean-preserving contractions over spreads, adding risk to the upper branch will tend to be preferred. The paper is organized as follows. Section 2 defines and graphically derives the utility premium and the risk premium. Section 3 illustrates how the utility and risk premia change with baseline wealth for some simple utility functions that differ in their third derivatives. Section 4 shows how adding risk to the better or worse outcomes of a binary lottery is analogous to contracting or spreading the outcomes of the lottery, and hence downside RA is related to an aversion to mean-preserving spreads. Section 5 applies the probability premium introduced by Jindapon et al. (2021) as a measure of the strength of downside RA. Conclusions are presented in Sect. 6.",
95.0,1.0,Theory and Decision,03 October 2022,https://link.springer.com/article/10.1007/s11238-022-09912-w,Nonlinear risks: a unified framework,July 2023,Pablo Gutiérrez Cubillos,Roberto Pastén,,Male,Male,Unknown,Male,"Agents make decisions under risk. For example, work on fixed or variable payment contracts; invest in cryptocurrencies; work remotely or in person; or invest before an expected tax change. In the literature on risk and uncertainty, these and other decisions are analyzed theoretically by exploring (i) under what conditions the decision-maker selects a higher or lower optimal level of exposure to risk and (ii) what is the interpretation of these conditions. Most of the literature in the comparative statics of stochastic changes has focused on the restrictive case of individual agents maximizing a utility function with a linear budget constraint that links the attributes of the individual’s payoff function (Chiu & Eeckhoudt, 2010; Chiu et al., 2012; Dardanoni, 1988; Eeckhoudt & Schlesinger, 2008; Ekern, 1980; Jean, 1980; Rothschild & Stiglitz, 1970). In contrast, the objective of this paper is to revisit models of decision-making under risk with a nonlinear budget constraint.Footnote 1 In general, the attitude toward risk depends on the shape of the utility function, where a more concave utility function is associated with a higher ""pain from risk"" or higher risk aversion. Importantly, extending the analysis to the case when the budget constraint has its own concavity degree allows us to consider risk not only in preferences but in technology as well due to the random payoff function. Therefore, it is possible to identify restrictions on preferences and technology that yield necessary and sufficient conditions for an increase in risk to increase or decrease the optimal exposure to risk. The role of risk in technology is made clear through the following example. Consider the classical labor supply model, but instead of a fixed wage per hour, the agent production function is concave, where labor and the stochastic factor can have decreasing marginal returns. So, in this case, we can study the agent's optimal response when there is a risk increase in the production structure.Footnote 2 Intuitively, the shape of the budget constraint interacts with the shape of the utility function, where the concavity of the latter is related to risk aversion. Hence, adding nonlinearities to the budget constraint indirectly affects the concavity of the problem, which in turn alters the attitude toward risk of the economic agent. Thus, our work tries to answer the following questions: (i) what effect does nonlinearity in the budget constraint have on the agent's decision under risk? and (ii) can we characterize the comparative static of risk increases in the presence of nonlinearities in the budget set? Therefore, the contribution of this work is to develop an analytical framework to characterize the comparative statics of increases in first- and second-order risk when there are nonlinearities in the budget constraint, where we (i) clarify the intuition of models with a nonlinear budget constraint in the context of risk and uncertainty, (ii) derive technical conditions for understanding the comparative statics under risk and uncertainty, and (iii) develop an intuitive model of working in person under the current health crisis. A few models of nonlinear risks exist, but in general, they impose harsh conditions that are difficult to meet or do not have a clear economic intuition. The reason may be that when both preferences and technology are nonlinear, establishing the sign of higher-order derivatives on both preferences and technology functions becomes an algebraic clutter (Faa Di Bruno, 1857). MacMinn and Holtmann (1983) is one of the early papers dealing with nonlinear technological risks. They investigate the effect that technological uncertainty has on the firm’s optimal production decision and show—similarly to the early work on the theory of the firm under uncertainty—that the firm selects the optimal level of an input at which the expected value of marginal product equals the factor price plus the marginal risk premium. However, unlike the earlier work, they show that the sign of the marginal risk premium is not always positive but depends on the structure of the firm’s technology. Analyzing the effect of a second-order increase in risk (a mean-preserving spread, MPS, Rothschild & Stiglitz, 1970), they arrive at a five terms equation (p. 130) and conclude that a MPS always decreases input demand under strong parameter restrictions, such as a constant factor elasticity of substitution, an elasticity of factor substitution bounded by one, a coefficient of relative risk aversion bounded by one, and an increasing coefficient of relative risk aversion. We establish mild conditions to arrive at the same result; in particular, we do not rely on a constant elasticity of substitution, and the two sufficient conditions on the elasticity and the coefficient of relative risk aversion (RRA) are subsumed in just one sufficient and necessary condition. In a recent paper, Li and Peter (2021) studied the effect of technological uncertainty on self-insurance decisions. They arrive at a similar equation as MacMinn and Holtmann (1983), and impose restrictions (see Li & Peter, 2021, Equation A5) to show that an increase in technological uncertainty raises self-insurance provided that the relative prudence is greater than two, the self-insurance elasticity of loss-state wealth is nonincreasing, and the self-insurance elasticity of technological improvement is non-decreasing in the random variable. To arrive at these results, they transform the payoff function given its nonlinear character and test the effect of a MPS on the payoff function rather than in the fundamental stochastic variable.Footnote 3 In the present paper, we show in Sect. 3 both cases with and without Li and Peters’ transformation. In addition, Vergara (2017) considers a two-period model of saving with a nonlinear stochastic budget function. To sign the effect of a MPS of second-period income on saving, he arrives at a similar equation as in MacMinn and Holtmann (1983) and Li and Peter (2021), but implicitly sets to zero higher-order partial derivatives (see his Eq. 5). This allows him to sign the effect of a MPS on saving, thus arriving at similar results as those in a linear-risk model scaled by an elasticity factor. In Sect. 3, we complete the necessary and sufficient conditions for Vergara’s model. Finally, this paper extends and complements Bonilla and Vergara (2022) comparative statics conditions for precautionary savings under nonlinear risk by stating conditions for unambiguous comparative statics where risk is nonlinear in a general context. The organization of this article is as follows: Sect. 2 introduces the intuition of the problem studied. Section 3 shows the standard model. Section 4 introduces the concepts of first- and second-degree risk increase and presents the main results of this paper with a unified framework for the analysis. Section 5 shows some examples of well-known results that are implicitly contained in our general formulation, plus an additional example pertaining to the current pandemic. Finally, Sect. 6 presents some concluding remarks.",
95.0,1.0,Theory and Decision,01 December 2022,https://link.springer.com/article/10.1007/s11238-022-09916-6,Timescale standard to discriminate between hyperbolic and exponential discounting and construction of a nonadditive discounting model,July 2023,Yutaka Matsushita,,,,Unknown,Unknown,Mix,,
95.0,1.0,Theory and Decision,03 October 2022,https://link.springer.com/article/10.1007/s11238-022-09909-5,Cournot–Bertrand endogenous behavior in a differentiated oligopoly with entry deterrence,July 2023,Duarte Brito,Margarida Catalão-Lopes,,Male,Female,Unknown,Mix,,
95.0,1.0,Theory and Decision,13 October 2022,https://link.springer.com/article/10.1007/s11238-022-09910-y,Resolving ambiguity as a public good: experimental evidence from Guyana,July 2023,Kaywana Raeburn,Sonia Laszlo,Jim Warnick,Unknown,Female,Male,Mix,,
95.0,1.0,Theory and Decision,17 October 2022,https://link.springer.com/article/10.1007/s11238-022-09914-8,Evaluating opportunities when more is less,July 2023,Yukinori Iwata,,,Male,Unknown,Unknown,Male,"In this study, we reconsider how opportunity sets (or menus) available to a decision-maker (DM) should be evaluated in the economics literature on freedom and welfare. One of the most uncontroversial principles in the literature is that adding new opportunities cannot diminish individual well-being. The principle of weak set monotonicity has been accepted by Arrow (1995), Gravel (1998), Puppe (1995, 1996), and Sen (2002).Footnote 1 In contrast, there exists evidence that adding new opportunities is not always desirable in the social psychology literature (Botti & Iyengar, 2006; Iyengar & Lepper, 2000; Schwartz, 2004; Schwartz & Ward, 2004). First, the addition of new opportunities increases decision costs.Footnote 2 A DM must spend various resources, such as time, money, or cognitive effort, to collect alternative-related information, evaluate this information, and make an appropriate judgment. Second, the abundance of alternatives overwhelms a DM’s cognitive abilities. Therefore, he or she does not consider all available alternatives or opts not to choose. Consequently, he or she may select an element in a large menu that is less preferred than that in a small menu. Third, a larger menu may harm the psychological consequences of what a DM actually chooses. For example, he or she becomes anxious about the alternatives that he or she did not choose or regrets choosing the suboptimal option.Footnote 3 While these choice overload phenomena are related to each other, we focus on the more-is-less effect, which was analyzed by Lleras et al. (2017; henceforth, LMNO). The more-is-less effect is a phenomenon in which having more options causes a welfare reduction. LMNO highlight the existence of the more-is-less effect in the model of choice with limited consideration, which is called an overwhelming choice. Thus, this study aims to consider how opportunity sets should be evaluated under the more-is-less effect. In the LMNO overwhelming choice model, a DM considers only a part of all available options, which is referred to as a consideration set, and chooses the most preferred element in the consideration set. An important contribution of LMNO is to develop a method for identifying the DM’s revealed preference from observed choice data under the competition filter property of the consideration set. This property captures the idea that if a consumer considers an item in a highly competitive choice situation (e.g., a large supermarket), then he or she will still consider the item in a relatively less competitive choice situation (e.g., a small grocery store). The clue to identifying a DM’s strictly revealed preferences is to find a seemingly irrational choice reversal caused by contracting a large menu into a small one. Choice reversals play an essential role in exhibiting the existence of the more-is-less effect (these arguments are explained in detail in the next section). Unlike the economics literature on freedom and welfare, weak set monotonicity seems to be controversial under the more-is-less effect. This is because a reduction in the DM’s welfare is caused by an increase in the available options. Thus, a dilemma wherein adding new opportunities may both enhance and diminish individual well-being exists. The first result of this study suggests that weak set monotonicity is incompatible with respecting a DM’s revealed preferences over what he or she actually chooses under a mild assumption, including the existence of the more-is-less effect. This impossibility result is different from those of Gravel (1998) and Puppe (1995) in the economics literature on freedom and welfare. First, they regard weak set monotonicity as a plausible principle, although we do not always agree with this. Second, they assume that a DM is rational in the standard sense, whereas we assume that a DM is boundedly rational in that he or she does not always consider all available alternatives. Considering our impossibility result and the social psychological evidence in terms of choice overload, we propose a hypothesis that “more is always better,” what weak set monotonicity means, is a bias to which moral heuristics leads. According to Sunstein (2005), moral heuristics are “rapid, intuitive judgments” and “make a great deal of sense, but sometimes produce moral mistakes that are replicated in law and policy” (p. 531). Therefore, if our hypothesis is correct, then it will be impossible to rely on weak set monotonicity for evaluating opportunity sets. Based on this hypothesis, we propose a resolution to the dilemma of adding new opportunities that may both enhance and diminish individual well-being. Act-consequentialism over menus is an evaluation of opportunity sets such that a menu is at least as desirable as another in terms of individual well-being if and only if an element chosen from the former is weakly revealed preferred to one chosen from the latter. Thus, the act-consequential evaluation of opportunity sets is based on a DM’s revealed preferences obtained by observing his or her choices. Act-consequentialism over menus has the following characteristics. First, it is obtained by extending act-consequentialism over a set of actions. Second, it distinguishes between “being chosen” and “being preferred.” Third, it is an ex-post and third-party evaluation of opportunity sets. Another insight of act-consequentialism over menus is that “enough is enough” when more is less, which sharply contrasts with “more is always better.” We provide an axiomatic foundation for act-consequentialism over menus. The first axiom is an extension rule that was originally proposed in the literature on ranking the sets of objects under complete uncertainty and was also used by Bossert (1997). The second axiom is instrumentalism, which does not admit the intrinsic value of freedom of choice. This study shows that the act-consequential evaluation of menus is characterized by the extension rule and instrumentalism. The act-consequential evaluation of menus is remarkably simple; however, its applications are interesting. When policymakers provide menus to DMs who follow overwhelming choices, we offer suggestions for their menu-providing policies. The first application is to identify when adding an alternative to a menu strictly enhances individual well-being under the act-consequential evaluation of menus. We demonstrate that the distinction between “being chosen” and “being preferred” plays an important role in this application. The second application is to propose a method for completing the act-consequential evaluation of menus. A DM’s revealed preferences may be extremely incomplete in the LMNO overwhelming choice model, which implies that the act-consequential evaluation of menus will also be incomplete. LMNO show that any ordering extension of the DM’s revealed preferences is consistent with an overwhelming choice. Therefore, multiple weak orders that are consistent with the choice behavior exist. We aggregate all such weak orders into a single ranking over menus and investigate the properties of such a ranking over menus. The remaining sections of this paper are organized as follows. Section 2 provides an example that motivates our study, introduces the LMNO overwhelming choice model, and reviews their results. Section 3 proposes a hypothesis about weak set monotonicity based on an impossibility result and provides an axiomatic foundation for act-consequentialism over menus. Section 4 considers the applications of act-consequentialism over menus. Finally, Sect. 5 concludes the paper.",
95.0,1.0,Theory and Decision,01 October 2022,https://link.springer.com/article/10.1007/s11238-022-09913-9,Generalizing the constrained equal awards rule,July 2023,Bas Dietzenbacher,,,Male,Unknown,Unknown,Male,"A bankruptcy problem is a fundamental allocation problem in which multiple claimants have individual claims on an estate. The question arises which of the possible estate allocations should be selected. For this, bankruptcy theory studies appropriate bankruptcy rules that assign to each bankruptcy problem a feasible allocation, i.e.  an estate allocation such that the individual payoffs are bounded by the corresponding claims. Many axiomatic studies are devoted to bankruptcy problems with transferable utility, or TU-bankruptcy problems (cf. O’Neill, 1982), where the estate and the claims are of a monetary nature. An extensive survey is provided by Thomson (2019). An elementary bankruptcy rule for bankruptcy problems with transferable utility is the constrained equal awards rule, which divides the estate as equally as possible in such a way that the claimants are not allocated more than their claims. This rule originates from old Jewish legal literature and forms the basis of the Talmud rule introduced by Aumann and Maschler (1985). Several axiomatic characterizations of the constrained equal awards rule in the transferable utility context are provided by Dagan (1996), Herrero and Villar (2001, 2002), Yeh (2004), and Yeh (2006). Bankruptcy problems with nontransferable utility, or NTU-bankruptcy problems (cf. Orshan et al., 2003), arise when the claimants have utility functions over their monetary payoffs. Since each monetary allocation corresponds to a utility allocation, the attainable monetary allocations of the estate are expressed in a set of induced utility allocations which are assumed to be normalized in such a way that zero money corresponds to zero utility. The claim vector represents the individual utility claims on the estate. NTU-bankruptcy problems form a natural generalization of TU-bankruptcy problems where the underlying utility functions are not assumed to be linear, and consequently, the estate does not necessarily correspond to a simplex in the payoff space. The constrained equal awards rule for bankruptcy problems with nontransferable utility allocates utility payoffs as equally as possible in such a way that the claimants are not allocated more than their utility claims. We study generalizations of axiomatic characterizations of the constrained equal awards rule for bankruptcy problems with transferable utility. In general, we show that many of the axiomatic results from the transferable utility context carry over to the nontransferable utility context by adequately reformulating the underlying properties on the extended domain. Interestingly, we also reveal new structures in particular cases. We focus on axiomatic characterizations which are based on the properties symmetry, sustainability, and consistency. In a bankruptcy problem with transferable utility, claimants only differ in their claims. In a bankruptcy problem with nontransferable utility, claimants not only differ in their claims, but also in their utility measures. On the one hand, axioms become stronger when they are imposed on a larger domain. On the other hand, uniqueness can be harder to achieve on a larger domain. The strong symmetry property which requires that claimants with equal claims get equal payoffs is occasionally necessary to generalize axiomatic characterizations. The sustainability property, which requires that claimants with small enough claims are fully reimbursed, plays a central role in several other results. An elevator lemma (cf. Thomson, 2011) is exploited to extend several axiomatic characterizations for bankruptcy problems with two claimants to any number of claimants using consistency. Alternatively, bankruptcy problems with nontransferable utility can be interpreted as bargaining problems with claims (cf. Chun & Thomson, 1992). Within this context, the constrained equal awards rule can be considered as a constrained version of the Kalai (1977) solution for bargaining problems without claims. This paper is organized in the following way. Section 2 formally introduces bankruptcy problems with nontransferable utility and the constrained equal awards rule. In Sects. 3, 4, and 5, we derive several axiomatic characterizations based on the properties symmetry, sustainability, and consistency, respectively. Section 6 concludes.",1
95.0,1.0,Theory and Decision,12 October 2022,https://link.springer.com/article/10.1007/s11238-022-09915-7,Conservativeness in jury decision-making,July 2023,Hyoungsik Noh,,,Unknown,Unknown,Unknown,Unknown,,

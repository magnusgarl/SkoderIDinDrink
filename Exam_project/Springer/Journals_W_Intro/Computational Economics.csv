Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
1.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435198,Computer Science in Economics and Management: A New Discipline!,January 1988,Hans M. Amman,,,Male,Unknown,Unknown,Male,,
1.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435199,A decision theoretic approach to file search,January 1988,James C. Moore,William B. Richmond,Andrew B. Whinston,Male,Male,Male,Male,,10
1.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435200,Two- or three-stage least squares?,January 1988,David A. Belsley,,,Male,Unknown,Unknown,Male,,51
1.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435201,Balancing large systems of national accounts,January 1988,F. Van Der Ploeg,,,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435202,Square root kalman algorithms in econometrics,January 1988,Carlo Carraro,,,Male,Unknown,Unknown,Male,,9
1.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435203,A knowledge-based system for production and distribution economics,January 1988,Ramayya Krishnan,David A. Kendrick,Ronald M. Lee,Male,Male,Male,Male,,10
1.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435204,Computer science in economics and management information for authors,January 1988,,,,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435205,Call for papers,January 1988,,,,Unknown,Unknown,Unknown,Unknown,,
1.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00427157,Logical errors in database SQL retrieval queries,January 1988,R. B. Buitendijk,,,Unknown,Unknown,Unknown,Unknown,,
1.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00427158,A logic programming approach to revealed preference theory,January 1988,Kuan-Pin Lin,Stan Perry,,Unknown,Male,Unknown,Male,,
1.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00427159,Expert systems and economic policy analysis: An application to OCS auction leasing,January 1988,Elizabeth Hoffman,Varghese S. Jacob,Andrew Whinston,Female,Unknown,Male,Mix,,
1.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00427160,The impact of collinearity involving the intercept term on the numerical accuracy of regression,January 1988,Stephen D. Simon,James P. Lesage,,Male,Male,Unknown,Male,,10
1.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00427161,A user-friendly interface to Kendrick's DUAL code,January 1988,Cornelia Grischow,Goetz Uebe,,Female,Unknown,Unknown,Female,,
1.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436277,Publisher's announcement,January 1988,,,,Unknown,Unknown,Unknown,Unknown,,
1.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436278,Parallel distributed processing models for economic systems and games,January 1988,Kislaya Prasad,,,Unknown,Unknown,Unknown,Unknown,,
1.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436279,Connecting expert system features to a multiple criteria programming based decision support system,January 1988,Ralf Östermark,Hannu Salmela,,Male,Male,Unknown,Male,,2
1.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436280,GEMPACK: General-purpose software for applied general equilibrium and other economic modellers,January 1988,G. Codsi,K. R. Pearson,,Unknown,Unknown,Unknown,Unknown,,
1.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436281,A general methodology for testing supercomputers in an industrial environment,January 1988,E. Vergison,,,Unknown,Unknown,Unknown,Unknown,,
2.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00454700,Integrated modeling systems: AI in the business and economics context,January 1989,David A. Kendrick,Leon S. Lasdon,Andrew B. Whinston,Male,Male,Male,Male,,
2.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00454701,Integrated Modeling Systems,January 1989,Arthur M. Geoffrion,,,Male,Unknown,Unknown,Male,,29
2.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00454702,A comparison of structured modeling and GAMS,January 1989,David Kendrick,Ramayya Krishnan,,Male,Male,Unknown,Male,,
2.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00454703,Knowledge representation and processing in economics and management,January 1989,Clyde W. Holsapple,Andrew B. Whinston,,Male,Male,Unknown,Male,,
2.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00454704,Logical testing for new approaches to mathematical programming modeling and analysis,January 1989,Fred Glover,Harvey J. Greenberg,,Male,Male,Unknown,Male,,
2.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00454705,An Expert System for Stochastic Control Problems: Automatic Report Generation,January 1989,Jean-Philippe Chancelier,Claude Gomez,Agnès Sulem,Unknown,,Female,Mix,,
2.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00454706,Hercules — A modeling system with knowledge about economics,January 1989,Arne Drud,,,Male,Unknown,Unknown,Male,,4
2.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00454707,Announcement,January 1989,,,,Unknown,Unknown,Unknown,Unknown,,
2.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435828,Vectorization and econometric model simulation,January 1989,Christian E. Petersen,Andrea Cividini,,Male,Female,Unknown,Mix,,
2.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435829,A model of decision-making involving two information processors,January 1989,Varghese S. Jacob,James C. Moore,Andrew Whinston,Unknown,Male,Male,Male,,2
2.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435830,A metalanguage design for sessions with group decision support systems,January 1989,Niv Ahituv,Israel Spiegler,,Male,Male,Unknown,Male,,
2.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435831,Mathematica: A critical appraisal,January 1989,David A. Belsley,,,Male,Unknown,Unknown,Male,,1
2.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436509,Bond pricing in APL2: A study in numerical solution of the Brennan and Schwartz bond pricing model using a vector processor,January 1989,Garth H. Foster,,,Male,Unknown,Unknown,Male,,3
2.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436510,Progressive equilibration algorithms: The case of linear transaction costs,January 1989,Alexander Eydeland,Anna Nagurney,,Male,Female,Unknown,Mix,,
2.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436511,Computational aspects of robust estimators for linear regressions,January 1989,Marilena Furno,Christopher Baum,,Female,Male,Unknown,Mix,,
2.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436512,PRTSM: Pattern recognition-based time series modeler,January 1989,Kun Chang Lee,Sung Joo Park,,,,Unknown,Mix,,
2.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436513,Call for papers,January 1989,,,,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435400,Editorial,March 1990,David A. Kendrick,Leon S. Lasdon,Andrew B. Whinston,Male,Male,Male,Male,,
3.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435401,A survey of reasoning procedures in knowledge based systems for economics and management,March 1990,L. F. Pau,,,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435402,"An intelligent decision support system for supply, distribution, and marketing planning",March 1990,Darwin Klingman,Rema Padman,Nancy Phillips,Male,Female,Female,Mix,,
3.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435403,Industry structure modeling for competitive analysis: An integrated AI and systems approach,March 1990,Edison Tse,Jaffer Syed,,Male,Unknown,Unknown,Male,,
3.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435404,A hierarchical/relational approach to modeling,March 1990,Thomas E. Baker,,,Male,Unknown,Unknown,Male,,2
3.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435405,Toward an environment theory of decision support,March 1990,C. W. Holsapple,A. B. Whinston,,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435406,Logic as an integrated modeling framework,March 1990,Ronald M. Lee,Ramayya Krishnan,,Male,Male,Unknown,Male,,1
3.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436711,Is a GigaLIP Fast Enough?,June 1990,Tom W. Keller,,,Male,Unknown,Unknown,Male,,
3.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436712,Inductive Learning Methods for Knowledge-Based Decision Support: A Comparative Analysis,June 1990,Michael J. Shaw,James A. Gentry,Selwyn Piramuthu,Male,Male,Male,Male,,11
3.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436713,Generic Expert Systems for Management Applications: The Operations Advisor and the Management Advisor,June 1990,Walter Reitman,,,Male,Unknown,Unknown,Male,,4
3.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436714,Comax: An Expert System for Cotton Crop Management,June 1990,Hal Lemmon,,,Male,Unknown,Unknown,Male,,1
3.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436715,ESL — A New Simulation Language for Economic Models,June 1990,G. Ohlendorf,P. Stahlecker,,Unknown,Unknown,Unknown,Unknown,,
3.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436716,How to Strip a Model to Its Essential Elements,June 1990,Giampiero M. Gallo,Manfred H. Gilli,,Male,Male,Unknown,Male,,3
3.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00437064,Publisher's note,September 1990,,,,Unknown,Unknown,Unknown,Unknown,,
3.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00437065,A preference theory approach to decision analysis in resource allocation,September 1990,H. Raghav Rao,James C. Moore,Andrew B. Whinston,Unknown,Male,Male,Male,,2
3.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00437066,MARS: A mergers and acquisitions reasoning system,September 1990,Piero P. Bonissone,Soumitra Dutta,,Male,Female,Unknown,Mix,,
3.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00437067,On square root Kalman filtering: a comment,September 1990,Corrado Corradi,,,Male,Unknown,Unknown,Male,,
3.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00437068,Square root filtering in econometrics: a reply to Corradi's comment,September 1990,Carlo Carraro,,,Male,Unknown,Unknown,Male,,
3.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00437069,A note on the difficulty of comparing iterative processes with differing rates of convergence,September 1990,A. J. Hughes Hallett,,,Unknown,Unknown,Unknown,Unknown,,
3.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00437070,Rates of convergence of B-processes: Some simulation results,September 1990,Venkatesh Bala,,,Male,Unknown,Unknown,Male,,
4.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00426851,Publishers note,February 1991,,,,Unknown,Unknown,Unknown,Unknown,,
4.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00426852,Qualitative economics: An implementation in PROLOG,February 1991,Ron Berndsen,Hennie Daniels,,Male,Female,Unknown,Mix,,
4.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00426853,Modified quadratic hill-climbing with SAS/IML,February 1991,Dennis Patrick Leyden,,,Male,Unknown,Unknown,Male,,
4.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00426854,A Guide to using the collinearity diagnostics,February 1991,David A. Belsley,,,Male,Unknown,Unknown,Male,,
4.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00426855,An interactive application of data envelopment analysis in microcomputers,February 1991,Yih-Long Chang,Toshiyuki Sueyoshi,,Unknown,Male,Unknown,Male,,
4.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00426856,GAUSSX: An integrated environment for GAUSS,February 1991,Jon A. Breslaw,,,Male,Unknown,Unknown,Male,,
4.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436282,Preface: Special issue on qualitative and neural economic analysis,May 1991,L. F. Pau,,,Unknown,Unknown,Unknown,Unknown,,
4.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436283,A stock selection strategy using fuzzy neural networks,May 1991,F. S. Wong,P. Z. Wang,H. H. Teh,Unknown,Unknown,Unknown,Unknown,,
4.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436284,An expert system for balance of payments analysis. A fault diagnostic approach,May 1991,Edward J. Krowitz,,,Male,Unknown,Unknown,Male,,
4.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436285,Application of constraint logic programming to asset and liability management in banks,May 1991,Johan M. Broek,Hennie A. M. Daniels,,Male,Female,Unknown,Mix,,
4.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436286,Qualitative economic reasoning: a disequilibrium perspective,May 1991,Kuan-Pin Lin,Arthur M. Farley,,Unknown,Male,Unknown,Male,,3
4.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436287,Causality and consistency in economic models,May 1991,Claudio Gianotti,,,Male,Unknown,Unknown,Male,,1
4.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00437232,L-CATA: A logic-based expert travel system,August 1991,S. Y. Yan,J. Zeleznikow,,Unknown,Unknown,Unknown,Unknown,,
4.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00437233,A generalization of the Friedlander Algorithm balancing of national accounts matrices,August 1991,Kasper Bartholdy,,,Male,Unknown,Unknown,Male,,2
4.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00437234,Dynamic policy in linear models with rational expectations of future events: A computer package,August 1991,F. Van Der Ploeg,A. J. Markink,,Unknown,Unknown,Unknown,Unknown,,
4.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00437235,Information tradeoffs in model building: A network routing application,August 1991,Anantaram Balakrishnan,James C. Moore,Andrew B. Whinston,Unknown,Male,Male,Male,,2
4.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF02116005,A graphical interface for production and transportation system modeling: PTS,November 1991,David A. Kendrick,,,Male,Unknown,Unknown,Male,,7
4.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF02116006,Obtaining initial parameter estimates for nonlinear systems using multicriteria associative memories,November 1991,Robert Kalaba,Eigh Tesfatsion,,Male,Unknown,Unknown,Male,,5
4.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF02116007,Improving facility planning with decision technology systems,November 1991,Guisseppi A. Forgionne,,,Unknown,Unknown,Unknown,Unknown,,
4.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF02116008,Qualitative solvability in economic models,November 1991,P. Fontaine,M. Garbely,M. Gilli,Unknown,Unknown,Unknown,Unknown,,
4.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF02116009,GAMS: A stylistic approach to economic modelling,November 1991,Anantha K. Duraiappah,,,Unknown,Unknown,Unknown,Unknown,,
5.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435279,Knowledge acquisition from an incomplete domain theory — An application on the Stock Market,February 1992,Robert T.H. Chi,Melody Y. Kiang,,Male,Female,Unknown,Mix,,
5.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435280,A network model and algorithm for the analysis and estimation of financial flow of funds,February 1992,Merritt Hughes,Anna Nagurney,,Male,Female,Unknown,Mix,,
5.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435281,Designing a DSS for the assessment of company performance and viability,February 1992,C. Zopounidis,A. Pouliezos,D. Yannacopoulos,Unknown,Unknown,Unknown,Unknown,,
5.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00435282,General-purpose software for intertemporal economic models,February 1992,George Codsi,K. R. Pearson,Peter J. Wilcoxen,Male,Unknown,Male,Male,,16
5.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436482,Publisher's announcement,May 1992,,,,Unknown,Unknown,Unknown,Unknown,,
5.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436483,Recursive portfolio management: Large-scale evidence from two Scandinavian stock markets,May 1992,Ralf Östermark,Jaana Aaltonen,,Male,Female,Unknown,Mix,,
5.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436484,An application of ESL to a stochastic dynamic market model with free entry and exit,May 1992,G. Ohlendorf,P. Stahlecker,,Unknown,Unknown,Unknown,Unknown,,
5.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436485,A model of explanation for financial knowledge-based systems,May 1992,A. J. Feelders,,,Unknown,Unknown,Unknown,Unknown,,
5.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436486,Simulated annealing: An initial application in econometrics,May 1992,William L. Goffe,Gary D. Ferrier,John Rogers,Male,Male,Male,Male,,8
5.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436487,Equation reordering for iterative processes — a comment,May 1992,M. Gilli,G. Pauletto,M. Garbely,Unknown,Unknown,Unknown,Unknown,,
5.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00426756,A message from the managing editor,August 1992,,,,Unknown,Unknown,Unknown,Unknown,,
5.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00426757,Editor's preface,August 1992,David A. Belsley,,,Male,Unknown,Unknown,Male,,
5.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00426758,Paring 3SLS calculations down to manageable proportions,August 1992,David A. Belsley,,,Male,Unknown,Unknown,Male,,8
5.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00426759,Location of outliers in multiple regression using resampled values,August 1992,Maria Rosaria D'Esposito,Marilena Furno,,Female,Female,Unknown,Female,,1
5.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00426760,SISAM and MIXIN: Two algorithms for the computation of posterior moments and densities using Monte Carlo integration,August 1992,J. Peter Hop,Herman K. Van Dijk,,Unknown,Male,Unknown,Male,,11
5.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00426761,"On the construction of monthly term structures of U.S. interest rates, 1919–1930",August 1992,Christopher F. Baum,Clifford F. Thies,,Male,Male,Unknown,Male,,1
5.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00426762,Understanding macroeconomic models: Structural sensitivity analysis of a medium-sized model,August 1992,Ullrich Heilemann,Heinz Josef Münch,,Male,Male,Unknown,Male,,3
5.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00426763,Analysis of large-scale econometric models using supercomputer techniques,August 1992,Carlo Bianchi,Giuseppe Bruno,Andrea Cividini,Male,Male,Female,Mix,,
5.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436582,Publisher's announcement,November 1992,,,,Unknown,Unknown,Unknown,Unknown,,
5.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436583,Solving a linear multiperiod portfolio problem by interior-point methodology,November 1992,Ralf Östermark,,,Male,Unknown,Unknown,Male,,5
5.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436584,Policy measurement for the dynamic linear model with expectations variables: A multiplier approach,November 1992,Yue Ma,,,,Unknown,Unknown,Mix,,
5.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436585,Nonlocal automated comparative static analysis,November 1992,Leigh Tesfatsion,,,,Unknown,Unknown,Mix,,
5.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00436586,Announcement and call for papers,November 1992,,,,Unknown,Unknown,Unknown,Unknown,,
6.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01885371,Publisher's announcement,February 1993,,,,Unknown,Unknown,Unknown,Unknown,,
6.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01885372,Implementing the single bootstrap: Some computational considerations,February 1993,B. D. McCullough,H. Vinod,,Unknown,Unknown,Unknown,Unknown,,
6.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01885373,User interface aspects of an MRP II planning module,February 1993,M. Nussbaum,G. Garretón,E. Parra,Unknown,Unknown,Unknown,Unknown,,
6.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01885374,Optimal growth and planning in a multi-regional economy: A computer program and application to the Italian case,February 1993,Domenico Campisi,Massimo Gastaldi,Agostino La Bella,Male,Male,Male,Male,,2
6.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299226,Preface: Special issue on research and applications in the Asia-Pacific region,May 1993,Lim Swee Cheang,,,,Unknown,Unknown,Mix,,
6.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299227,A dynamic software release model,May 1993,Süleyman Özekici,Neşe A. Çatkan,,Male,Female,Unknown,Mix,,
6.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299228,The measurement of gender earnings differentials for foreign-trained and local-trained IT professionals: The case of Singapore,May 1993,Rosalind Chew,,,Female,Unknown,Unknown,Female,,
6.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299229,Information technology in a global economy,May 1993,Patricia A. Glenn,,,Female,Unknown,Unknown,Female,,1
6.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299230,Development of a decision support system for service delivery,May 1993,An-Pin Chen,Chien-Hua Hwang,Chien-Yuan Lin,Unknown,Unknown,Unknown,Unknown,,
6.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299231,Construction and modification of judgement matrices in analytic hierarchy process,May 1993,Chen Naidong,Guo Minxue,,,,Unknown,Mix,,
6.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299232,Work Centre management: The manufacturing philosophy,May 1993,John Paynter,,,Male,Unknown,Unknown,Male,,1
6.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299172,Massively parallel implementation of the Splitting Equilibration Algorithm,November 1993,Dae-Shik Kim,Anna Nagurney,,Unknown,Female,Unknown,Female,,4
6.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299173,User modeling for flexible inference control and its relevance to decision-making in economics and management,November 1993,Zhengxin Chen,,,Unknown,Unknown,Unknown,Unknown,,
6.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299174,A synopsis of the smoothing formulae associated with the Kalman filter,November 1993,H. R. Merkus,D. S. G. Pollock,A. F. de Vos,Unknown,Unknown,Unknown,Unknown,,
6.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299175,Economic incentives in software design,November 1993,Hal R. Varian,,,Male,Unknown,Unknown,Male,,1
6.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299176,The design of economic policy under model uncertainty,November 1993,Nicos Christodoulakis,David Kemball-Cook,Paul Levine,Unknown,Male,Male,Male,,5
6.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299177,A constrained optimal control program in APL,November 1993,Han Kang Hong,,,,Unknown,Unknown,Mix,,
6.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299178,A user's guide to the numerical solution of two-point boundary value problems arising in continuous time dynamic economic models,November 1993,William L. Goffe,,,Male,Unknown,Unknown,Male,,4
6.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299179,Research opportunities in Computational Economics,November 1993,David Kendrick,,,Male,Unknown,Unknown,Male,,5
7.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299326,"Computability, complexity and economics",February 1994,Alfred Lorn Norman,,,Male,Unknown,Unknown,Male,,8
7.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299327,Cointegration tests on MARS,February 1994,Peter S. Sephton,,,Male,Unknown,Unknown,Male,,17
7.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299328,Identification environment and robust forecasting for nonlinear time series,February 1994,Berlin Wu,,,Unknown,Unknown,Unknown,Unknown,,
7.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299329,Global optimization using interval arithmetic,February 1994,Max E. Jerrell,,,Male,Unknown,Unknown,Male,,5
7.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299567,Experiences in the pricing of trivariate contingent claims with finite difference methods on a massively parallel computer,June 1994,Niklas Ekvall,,,Male,Unknown,Unknown,Male,,2
7.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299568,Global estimation of feedforward networks with a priori constraints,June 1994,Wayne Joerding,Ying Li,,Male,,Unknown,Mix,,
7.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299569,Tensor methods for full-information maximum likelihood estimation: Unconstrained estimation,June 1994,Seth A. Greenblatt,,,Male,Unknown,Unknown,Male,,1
7.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299570,Computational aspects in applied stochastic control,June 1994,Charles S. Tapiero,Agnès Sulem,,Male,Female,Unknown,Mix,,
7.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299571,Decision modelling with HIPRE 3+,June 1994,J. A. M. Wesseling,A. Gábor,,Unknown,Unknown,Unknown,Unknown,,
7.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299776,Connectionist projection pursuit regression,September 1994,William Verkooijen,Hennie Daniels,,Male,Female,Unknown,Mix,,
7.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299777,Using a genetic algorithm to determine an index of leading economic indicators,September 1994,Arthur M. Farley,Samuel Jones,,Male,Male,Unknown,Male,,13
7.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299778,Recognizing business cycle turning points by means of a neural network,September 1994,Keshav P. Vishwakarma,,,Unknown,Unknown,Unknown,Unknown,,
7.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299779,"Genetic algorithms, teleological conservatism, and the emergence of optimal demand relations: The case of stable preferences",September 1994,Roger A. Mccain,,,Male,Unknown,Unknown,Male,,2
7.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299780,Solving applied general equilibrium models represented as a mixture of linearized and levels equations,September 1994,W. Jill Harrison,K. R. Pearson,E. John Small,Unknown,Unknown,Unknown,Unknown,,
7.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299453,Introduction,December 1994,Anna Nagurney,,,Female,Unknown,Unknown,Female,,
7.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299454,On the use of optimization models for portfolio selection: A review and some computational results,December 1994,Panos M. Pardalos,Mattias Sandström,Costas Zopounidis,Male,Male,Male,Male,,34
7.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299455,Minimax hedging strategy,December 1994,M. A. Howe,B. Rustem,M. J. P. Selby,Unknown,Unknown,Unknown,Unknown,,
7.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299456,A fast algorithm for computing integrals in function spaces: Financial applications,December 1994,A. Eydeland,,,Unknown,Unknown,Unknown,Unknown,,
7.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299457,Numerical schemes for investment models with singular transactions,December 1994,Agnès Tourin,Thaleia Zariphopoulou,,Female,Female,Unknown,Female,,37
7.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299458,Jump-diffusion processes in the foreign exchange markets and the release of macroeconomic news,December 1994,Gordon Johnson,Thomas Schneeweis,,Male,Male,Unknown,Male,,22
8.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01298495,Computational systems for qualitative economics,March 1995,Karl R. Lang,James C. Moore,Andrew B. Whinston,Male,Male,Male,Male,,1
8.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01298496,The dynamics of collective action,March 1995,Bernardo A. Huberman,Natalie S. Glance,,Male,Female,Unknown,Mix,,
8.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01298497,Computing economic equilibria using benefit and surplus functions,March 1995,David G. Luenberger,Robert R. Maxfield,,Male,Male,Unknown,Male,,3
8.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01298498,Ten wishes,March 1995,David A. Kendrick,,,Male,Unknown,Unknown,Male,,2
8.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299711,Modeling the transportation domain,May 1995,K. Fischer,N. Kuhn,J. P. Müller,Unknown,Unknown,Unknown,Unknown,,
8.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299712,On comparative-static analysis in numerical nonlinear economic models,May 1995,Richard Arnott,John Rowse,,Male,Male,Unknown,Male,,1
8.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299713,Optimal sampling-rates and tracking properties of digital LQ and LQG tracking controllers for systems with an exogenous component and costs associated to sampling,May 1995,Jacob Engwerda,Gerard van Willigenburg,,Male,Male,Unknown,Male,,
8.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299714,Solving nonlinear dynamic models by iterative dynamic programming,May 1995,Willi Semmler,,,Male,Unknown,Unknown,Male,,5
8.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01298457,Introduction,August 1995,Chris Birchenhall,,,,Unknown,Unknown,Mix,,
8.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01298458,A distributed parallel genetic algorithm for solving optimal growth models,August 1995,Paul M. Beaumont,Patrick T. Bradshaw,,Male,Male,Unknown,Male,,14
8.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01298459,Coordination via genetic learning,August 1995,Jasmina Arifovic,Curtis Eaton,,Female,Male,Unknown,Mix,,
8.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01298460,Self-organization of markets: An example of a computational approach,August 1995,Nicolaas J. Vriend,,,Male,Unknown,Unknown,Male,,74
8.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01298461,Modular technical change and genetic algorithms,August 1995,Chris Birchenhall,,,,Unknown,Unknown,Mix,,
8.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299733,Estimating simultaneous equations models by a simulation technique,November 1995,Heejoon Kang,,,Unknown,Unknown,Unknown,Unknown,,
8.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299734,Tensor methods of full-information maximum likelihood estimation: Estimation with parameter constraints,November 1995,Seth A. Greenblatt,,,Male,Unknown,Unknown,Male,,
8.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299735,Control metaphors in the modelling of economic learning and decision-making behaviour,November 1995,Scott Moss,,,Male,Unknown,Unknown,Male,,9
8.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF01299736,A distributed block approach to solving near-block-diagonal systems with an application to a large macroeconometric model,November 1995,Jon Faust,Ralph Tryon,,Male,Male,Unknown,Male,,
9.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00115687,Preface,February 1996,,,,Unknown,Unknown,Unknown,Unknown,,
9.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00115688,General financial equilibrium modeling with policy interventions and transaction costs,February 1996,Anna Nagurney,June Dong,,Female,Female,Unknown,Female,,8
9.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00115689,A spectral algorithm for pricing interest rate options,February 1996,Alexander Eydeland,,,Male,Unknown,Unknown,Male,,3
9.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00115690,Neural networks in the capital markets: An application to index forecasting,February 1996,Christian Hæke,Christian Helmenstein,,Male,Male,Unknown,Male,,7
9.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00115691,A neural network approach to long-run exchange rate prediction,February 1996,William Verkooijen,,,Male,Unknown,Unknown,Male,,18
9.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00115692,Linear regression versus back propagation networks to predict quarterly stock market excess returns,February 1996,Ypke Hiemstra,,,Unknown,Unknown,Unknown,Unknown,,
9.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00115693,Neural network for predicting the performance of credit card accounts,February 1996,Ilona Jagielska,Janusz Jaworski,,Female,Male,Unknown,Mix,,
9.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00123638,Computing solutions for large general equilibrium models using GEMPACK,August 1996,W. Jill Harrison,K. R. Pearson,,Unknown,Unknown,Unknown,Unknown,,
9.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00123639,Robust procedures in multiple regression: P-subsets and a computational proposal,August 1996,Maria Rosaria D'esposito,Marilena Furno,,Female,Female,Unknown,Female,,
9.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00123640,Predicting economic time series using a nonlinear deterministic technique,August 1996,Liangyue Cao,Yiguang Hong,Shuhui Deng,Unknown,Unknown,Unknown,Unknown,,
9.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00121632,Editor's preface,August 1996,,,,Unknown,Unknown,Unknown,Unknown,,
9.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00121633,The relative power of zero-padding when testing for serial correlation using artificial regressions,August 1996,David A. Belsley,,,Male,Unknown,Unknown,Male,,
9.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00121634,Using algebraic software to compute the moments of order statistics,August 1996,Corrado Provasi,,,Male,Unknown,Unknown,Male,,1
9.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00121635,Collinearity detection in linear regression models,August 1996,Gianfranco Galmacci,,,Male,Unknown,Unknown,Male,,5
9.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00121636,Clustering problems in optimization models,August 1996,Santosh Kabadi,Katta G. Murty,Cosimo Spera,,Unknown,Male,Mix,,
9.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00121637,Optimal experimental design for combinatorial problems,August 1996,Selden B. Grary,Cosimo Spera,,Unknown,Male,Unknown,Male,,3
9.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00121638,Indirect estimation of stochastic differential equation models: some computational experiments,August 1996,Carlo Bianchi,Eugene M. Cleur,,Male,Male,Unknown,Male,,10
9.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00119476,Functional search in economics using genetic programming,November 1996,Carl P. Schmertmann,,,Male,Unknown,Unknown,Male,,9
9.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00119477,SD-Solver: Towards a “multidirectional” CLP-based simulation tool,November 1996,Christophe Bisière,,,Male,Unknown,Unknown,Male,,
9.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00119478,Checking for saddlepoint stability: An easy test,November 1996,Raouf Boucekkine,Cuong Le Van,,Male,,Unknown,Mix,,
9.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00119479,Features of multiregional and intertemporal AGE modelling with GEMPACK,November 1996,W. Jill Harrison,K. R. Pearson,Alan A. Powell,Unknown,Unknown,Male,Male,,5
9.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00119480,The loss in efficiency from using grouped data to estimate coefficients of group level variables,November 1996,Kathleen M. Lang,Peter Gottschalk,,Female,Male,Unknown,Mix,,
9.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/BF00119481,A variational inequality approach for marketable pollution permits,November 1996,Anna Nagurney,Kathy Dhanda,,Female,Female,Unknown,Female,,12
10.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017990704200,Predicting Index Returns with Morphological Filters,February 1997,Antti J. Kanto,Eero Kasanen,Vesa Puttonen,Male,Male,Female,Mix,,
10.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008699827999,A Dynamic Spatial Cournot-Nash Equilibrium Model and an Algorithm,February 1997,BYUNG-WOOK Wie,ROGER L. Tobin,,Unknown,Unknown,Unknown,Unknown,,
10.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008686107234,A Homotopy Approach to Solving Nonlinear Rational Expectation Problems,February 1997,Mark J. Jensen,,,Male,Unknown,Unknown,Male,,1
10.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008625524072,On Incentives and Updating in Agent Based Models,February 1997,Scott E. Page,,,Male,Unknown,Unknown,Male,,32
10.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017987931447,Interval Arithmetic for Input-output Models with Inexact Data,February 1997,Max E. Jerrell,,,Male,Unknown,Unknown,Male,,12
10.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008661319094,What is Computational Economics?,May 1997,HANS M. AMMAN,,,Unknown,Unknown,Unknown,Unknown,,
10.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008631912069,Visualisation in the Simulation and Control of Economic Models,May 1997,R.D. Herbert,R.D. Bell,,Unknown,Unknown,Unknown,Unknown,,
10.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008683928907,No Arbitrage Between Economies and Correlation Risk Management,May 1997,HÉLYETTE GEMAN,RÉMI SOUVETON,,Unknown,Unknown,Unknown,Unknown,,
10.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008670607684,Algorithms for Finding Repeated Game Equilibria,May 1997,MARK B. CRONSHAW,,,Unknown,Unknown,Unknown,Unknown,,
10.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008634222838,Precision Performances of Terminal Conditions for Short Time Horizons Forward-Looking Systems,May 1997,Raouf Boucekkine,Michel Juillard,Pierre Malgrange,Male,Male,Male,Male,,
10.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008671903509,Analytical Derivatives for Markov Switching Models,May 1997,JEFF GABLE,SIMON VAN NORDEN,ROBERT VIGFUSSON,Unknown,Unknown,Unknown,Unknown,,
10.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008654023721,A Small-Sample Correction for Testing for gth-Order Serial Correlation with Artificial Regressions,August 1997,DAVID A. BELSLEY,,,Unknown,Unknown,Unknown,Unknown,,
10.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008617207791,Computing 3SLS Solutions of Simultaneous Equation Models with a Possible Singular Variance–Convariance Matrix,August 1997,ERRICOS J. KONTOGHIORGHES,ELIAS DINENIS,,Unknown,Unknown,Unknown,Unknown,,
10.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008669208700,Constrained Maximum Likelihood,August 1997,Ronald Schoenberg,,,Male,Unknown,Unknown,Male,,27
10.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008673309609,A Search for Hidden Relationships: Data Mining with Genetic Algorithms,August 1997,GEORGE G. SZPIRO,,,Unknown,Unknown,Unknown,Unknown,,
10.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008677427355,Non-linear Optimization on a Parallel Intel i860 RISC Based Architecture,August 1997,J.F. BALL,R.E. DORSEY,J.D. JOHNSON,Unknown,Unknown,Unknown,Unknown,,
10.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008633613243,Automatic Differentiation and Interval Arithmetic for Estimation of Disequilibrium Models,August 1997,MAX E. JERRELL,,,Unknown,Unknown,Unknown,Unknown,,
10.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008668718837,Hybrid Classifiers for Financial Multicriteria Decision Making: The Case of Bankruptcy Prediction,November 1997,Ignacio Olmeda,Eugenio Fernández,,Male,Male,Unknown,Male,,105
10.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008627722924,Finite-Sample Adjustments for Homogeneity and Symmetry Tests in Systems of Demand Equations: A Monte Carlo Evaluation,November 1997,Francisco Cribari-Neto,Spyros G. Zarkos,,Male,Male,Unknown,Male,,2
10.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008604511209,A Code Archive for Economics and Econometrics,November 1997,Dirk Eddelbüttel,,,Male,Unknown,Unknown,Male,,
10.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008693830990,Derivative Asset Pricing with Transaction Costs: An Extension,November 1997,Stylianos Perrakis,Jean Lefoll,,Male,Male,Unknown,Male,,39
10.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008668206289,Solving Dynamic Economic Models with Nonconvexities Due to Fixed Costs,November 1997,Robert M. Hussey,,,Male,Unknown,Unknown,Male,,2
10.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017190227296,Author Index,November 1997,,,,Unknown,Unknown,Unknown,Unknown,,
10.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017149110458,Subject Index,November 1997,,,,Unknown,Unknown,Unknown,Unknown,,
11.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008606209923,Preface,April 1997,David A. Belsley,,,Male,Unknown,Unknown,Male,,1
11.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008626831304,Numerical Analysis of Strategic Contingent Claims Models,April 1997,Ronald W. Anderson,Cheng Tu,,Male,,Unknown,Mix,,
11.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008618512649,Moving Endpoints and the Internal Consistency of Agents' Ex Ante Forecasts,April 1997,Sharon Kozicki,P.A. Tinsley,,Female,Unknown,Unknown,Female,,15
11.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008674730396,Alternative Approaches to Modeling time Variation in the Case of the U.S. Real Interest Rate,April 1997,Basma Bekdache,,,Female,Unknown,Unknown,Female,,1
11.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008622613557,Modelling Federal Reserve Discount Policy,April 1997,Christopher F. Baum,Meral Karasulu,,Male,Female,Unknown,Mix,,
11.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008610311740,Numerical Strategies for Solving the Nonlinear Rational Expectations Commodity Market Model,April 1997,Mario J. Miranda,,,Male,Unknown,Unknown,Male,,10
11.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008670529487,A Stochastic Nonlinear Regression Estimator Using Wavelets,April 1997,Zuohong Pan,Xiaodi Wang,,Unknown,Unknown,Unknown,Unknown,,
11.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008666428579,Walvelet Analysis of Commodity Price Behavior,April 1997,Russell Davidson,Walter C. Labys,Jean-Baptiste Lesourd,Male,Male,Unknown,Male,,40
11.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008658226761,The Path Integral Approach to Financial Modeling and Options Pricing,April 1997,Vadim Linetsky,,,Male,Unknown,Unknown,Male,,75
11.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017104923369,1997–98 SCE Graduate Student Prize in Computational Economics,April 1997,,,,Unknown,Unknown,Unknown,Unknown,,
12.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008654125853,The WALRAS Algorithm: A Convergent Distributed Implementation of General Equilibrium Outcomes,August 1998,John Q. Cheng,Michael P. Wellman,,Male,Male,Unknown,Male,,83
12.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008602109014,On the Hicksian Laws of Comparative Statics for the Hicksian Case: the Path-Following Approach Using an Alternative Homotopy,August 1998,Takashi Shiomura,,,Male,Unknown,Unknown,Male,,
12.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008625731231,Simulating the Madness of Crowds: Price Bubbles in an Auction-Mediated Robot Market,August 1998,Ken Steiglitz,Daniel Shapiro,,Male,Male,Unknown,Male,,17
12.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008645531911,Computationally Convenient Distributional Assumptions for Common-Value Auctions,August 1998,Michael B. Gordy,,,Male,Unknown,Unknown,Male,,32
12.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008637230094,Implementing the Double Bootstrap,August 1998,B.D. McCullough,H.D. Vinod,,Unknown,Unknown,Unknown,Unknown,,
12.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008693507721,Bubbles and Market Crashes,October 1998,Michael Youssefmir,Bernardo A. Huberman,Tad Hogg,Male,Male,Male,Male,,35
12.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008641414164,Comparative Dynamics in Perfect-Foresight Models,October 1998,Lex Meijdam,Marijn Verhoeven,,Male,,Unknown,Mix,,
12.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008649615981,Teaching Macroeconomics with GAMS,October 1998,P. Ruben Mercado,David A. Kendrick,Hans Amman,Unknown,Male,Male,Male,,7
12.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008685129185,An Introduction to Simulated Annealing Algorithms for the Computation of Economic Equilibrium,October 1998,Lihua Wu,Yuyun Wang,,Unknown,Unknown,Unknown,Unknown,,
12.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008639612347,Nonlinear Versus Linear Learning Devices: A Procedural Perspective,October 1998,Emilio Barucci,Leonardo Landi,,Male,Male,Unknown,Male,,1
12.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008689405084,Running the Economy: A Review of the Internet-Based Fairmodel,October 1998,Mark a. Roscam Abbing,,,Male,Unknown,Unknown,Male,,1
12.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008662309627,Introduction,December 1998,Seth a. Greenblatt,,,Male,Unknown,Unknown,Male,,
12.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008670512353,The Nonconvexities Problem in Adaptive Control Models: A Simple Computational Solution,December 1998,Marco P. Tucci,,,Male,Unknown,Unknown,Male,,9
12.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008691115079,ASPEN: A Microsimulation Model of the Economy,December 1998,N. Basu,R. Pryor,T. Quint,Unknown,Unknown,Unknown,Unknown,,
12.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008682814171,Econometric Estimation of a Continuous Time Macroeconomic Model of the United Kingdom with Segmented Trends,December 1998,K.B. Nowman,,,Unknown,Unknown,Unknown,Unknown,,
12.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008695215988,Front-Tracking Finite Difference Methods for the Valuation of American Options,December 1998,K.N. Pantazopoulos,E.N. Houstis,S. Kortesis,Unknown,Unknown,Unknown,Unknown,,
12.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008699300058,Atomic Decomposition of Financial Data,December 1998,Seth a. Greenblatt,,,Male,Unknown,Unknown,Male,,3
12.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017103118180,Author Index,December 1998,,,,Unknown,Unknown,Unknown,Unknown,,
12.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017188019089,Subject Index,December 1998,,,,Unknown,Unknown,Unknown,Unknown,,
12.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017192203159,Volume Contents,December 1998,,,,Unknown,Unknown,Unknown,Unknown,,
13.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008693431003,Optimal Nonlinear Income Taxation with a Two-Dimensional Population; A Computational Approach,February 1999,Ritva Tarkiainen,Matti Tuomala,,Female,Male,Unknown,Mix,,
13.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008673425186,A Nonrecursive Solution Method for the Linear-Quadratic Optimal Control Problem with a Singular Transition Matrix,February 1999,Jürgen Ehlgen,,,Male,Unknown,Unknown,Male,,1
13.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008600807439,On Optimal Design of Treasury Bonds,February 1999,Rosella Giacometti,,,Female,Unknown,Unknown,Female,,
13.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008610307810,Using Genetic Algorithms to Model the Evolution of Heterogeneous Beliefs,February 1999,James Bullard,John Duffy,,Male,Male,Unknown,Male,,45
13.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008669224277,Symplectic Methods for the Solution to Riccati Matrix Equations Related to Macroeconomic Models,February 1999,Guiomar Martín-Herrán,,,Female,Unknown,Unknown,Female,,1
13.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008649923740,On the Tradeoff Between Computational Simplicity and Asymptotic Properties in Multivariate Probit,February 1999,Ayal Kimhi,,,Male,Unknown,Unknown,Male,,4
13.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008621308348,Solving Irregular Econometric and Mathematical Optimization Problems with a Genetic Hybrid Algorithm,April 1999,Ralf Östermark,,,Male,Unknown,Unknown,Male,,28
13.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008651416896,Optimal Portfolio Hedging with Nonlinear Derivatives and Transaction Costs,April 1999,Jussi Keppo,Samu Peura,,Male,Male,Unknown,Male,,2
13.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008612905284,The Effect of (Mis-Specified) GARCH Filters on the Finite Sample Distribution of the BDS Test,April 1999,Chris Brooks,Saeed M. Heravi,,,Male,Unknown,Mix,,
13.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008666706193,A New Convergence Theorem for Successive Overrelaxation Iterations,April 1999,A. J. Hughes Hallett,Laura Piscitelli,,Unknown,Female,Unknown,Female,,
13.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008679025757,"One Dimensional SDE Models, Low Order Numerical Methods and Simulation Based Estimation: A Comparison of Alternative Estimators",April 1999,Eugene M. Cleur,Piero Manfredi,,Male,Male,Unknown,Male,,2
13.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017161707883,1998–99 SCE Graduate Student Prize in Computational Economics,April 1999,,,,Unknown,Unknown,Unknown,Unknown,,
13.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017122926475,"for the 5th International Conference of the Society for Computational Economics “Computing in Economics and Finance” to be held on the Boston College campus June 24, 25, 26, 1999.",April 1999,,,,Unknown,Unknown,Unknown,Unknown,,
13.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008652106422,Off-Line Computation of Stackelberg Solutions with the Genetic Algorithm,June 1999,Thomas Vallée,Tamer Başar,,Male,Male,Unknown,Male,,22
13.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008670808010,Approximated Distributions of Sampling Inequality Indices,June 1999,Paola Palmitesta,Corrado Provasi,Cosimo Spera,Female,Male,Male,Mix,,
13.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008699809511,Numerical Solution of an Endogenous Growth Model with Threshold Learning,June 1999,Baoline Chen,,,Unknown,Unknown,Unknown,Unknown,,
13.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008666700953,Portmanteau Model Diagnostics and Tests for Nonlinearity: A Comparative Monte Carlo Study of Two Alternative Methods,June 1999,Chris Brooks,,,,Unknown,Unknown,Mix,,
13.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008654116882,A Calibration Procedure of Dynamic CGE Models for Non-Steady State Situations Using GEMPACK,June 1999,Ronald Wendner,,,Male,Unknown,Unknown,Male,,19
13.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017162601861,Call for Papers on Agent-Based Computational Economics (ACE),June 1999,,,,Unknown,Unknown,Unknown,Unknown,,
13.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017173307933,Volume Contents for Volume 13,June 1999,,,,Unknown,Unknown,Unknown,Unknown,,
14.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008655831209,Applied General Equilibrium Modeling with MPSGE as a GAMS Subsystem: An Overview of the Modeling Framework and Syntax,October 1999,Thomas F. Rutherford,,,Male,Unknown,Unknown,Male,,270
14.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008651730300,Dense and Sparse Matrix Classes Using the C++ Standard Template Library,October 1999,Søren S. Nielsen,,,Male,Unknown,Unknown,Male,,
14.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008699629392,Mathematica as an Environment for Doing Economics and Econometrics,October 1999,David A. Belsley,,,Male,Unknown,Unknown,Male,,2
14.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008647612553,"Display and Interactive Languages for the Internet: HTML, PDF, and Java",October 1999,Dirk Eddelbüttel,William L. Goffe,,Male,Male,Unknown,Male,,
14.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008695528483,A C++ Platform for the Evolution of Trade Networks,October 1999,David McFadzean,Leigh Tesfatsion,,Male,,Unknown,Mix,,
14.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008643511645,C for Econometricians,October 1999,Francisco Cribari-Neto,,,Male,Unknown,Unknown,Male,,5
14.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008635327574,Programming Languages in Economics,October 1999,David A. Kendrick,Hans M. Amman,,Male,Male,Unknown,Male,,7
15.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008650907065,Preface,April 2000,David A. Belsley,,,Male,Unknown,Unknown,Male,,
15.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008626508882,Empirical Game Theoretic Models: Computational Issues,April 2000,Olivier Armantier,Jean-François Richard,,Male,Unknown,Unknown,Male,,3
15.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008634709790,Solution of Nonlinear Rational Expectations Models with Applications toFinite-Horizon Life-Cycle Models of Consumption,April 2000,Michael Binder,M. Hashem Pesaran,S. Hossein Samiei,Male,Unknown,Unknown,Male,,2
15.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008638827537,A Test for Strong Hysteresis,April 2000,Laura Piscitelli,Rod Cross,Harbir Lamba,Female,Male,Unknown,Mix,,
15.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008695011608,A Wavelet-Based Nonparametric Estimator ofthe Variance Function,April 2000,Zuohong Pan,Xiaodi Wang,,Unknown,Unknown,Unknown,Unknown,,
15.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008647128446,Parallel Strategies for Solving SURE Models with Variance Inequalities and Positivity of Correlations Constraints,April 2000,Erricos J. Kontoghiorghes,,,Unknown,Unknown,Unknown,Unknown,,
15.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008699112516,Credit Risk Assessment Using Statistical and Machine Learning: Basic Methodology and Risk Modeling Applications,April 2000,J. Galindo,P. Tamayo,,Unknown,Unknown,Unknown,Unknown,,
15.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008651229355,Computing Equilibria in Stochastic Finance Economies,April 2000,Felix Kubler,Karl Schmedders,,Male,Male,Unknown,Male,,9
16.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008720600212,Preface,October 2000,David A. Belsley,,,Male,Unknown,Unknown,Male,,
16.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008741117959,A Small-Sample Correction for Testing for Joint Serial Correlation with Artificial Regressions,October 2000,David A. Belsley,,,Male,Unknown,Unknown,Male,,1
16.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008797318867,Estimation of the Bivariate Stable Spectral Representation by theProjection Method,October 2000,J. Huston McCulloch,,,Unknown,Unknown,Unknown,Unknown,,
16.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008701502938,Inconsistencies in SURE Models: Computational Aspects,October 2000,Erricos J. Kontoghiorghes,,,Unknown,Unknown,Unknown,Unknown,,
16.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008753503846,Recursive Estimation and Testing of Dynamic Models,October 2000,Juan Del Hoyo,J. Guillermo Llorente,,Male,Unknown,Unknown,Male,,1
16.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008757620685,Optimized Multivariate Lag Structure Selection,October 2000,Peter Winker,,,Male,Unknown,Unknown,Male,,24
16.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008709704755,A Computational Approach to Finding Causal Economic Laws,October 2000,I-Lok Chang,P.A.V.B. Swamy,George S. Tavlas,Unknown,Unknown,Male,Male,,30
16.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008761721593,Confidence Interval Estimation for Inequality Indices of the Gini Family,October 2000,Paola Palmitesta,Corrado Provasi,Cosimo Spera,Female,Male,Male,Mix,,
16.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008713823410,Explaining the Persistence of Commodity Prices,October 2000,Serena Ng,Francisco J. Ruge-Murcia,,Female,Male,Unknown,Mix,,
16.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1008722026136,Parallel Krylov Methods for Econometric Model Simulation,October 2000,Giorgio Pauletto,Manfred Gilli,,Male,Male,Unknown,Male,,7
16.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017358027142,2000–2001 SCE Graduate Student Prize in Computational Economics,October 2000,,,,Unknown,Unknown,Unknown,Unknown,,
17.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011244112281,Introduction,February 2001,Anna Nagurney,,,Female,Unknown,Unknown,Female,,
17.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011249812115,Financial Networks and Optimally-Sized Portfolios,February 2001,Anna Nagurney,June Dong,,Female,Female,Unknown,Female,,4
17.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011230712774,Bicriteria Decision Making and Financial Equilibrium: A Variational Inequality Perspective,February 2001,June Dong,Anna Nagurney,,Female,Female,Unknown,Female,,12
17.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011278629862,Numerical Schemes for Variational Inequalities Arising in International Asset Pricing,February 2001,James E. Hodder,Agnès Tourin,Thaleia Zariphopoulou,Male,Female,Female,Mix,,
17.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011261429120,"Time Changes, Laplace Transforms and Path-Dependent Options",February 2001,Hélyette Geman,,,Female,Unknown,Unknown,Female,,8
17.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011274413023,Equilibrium Values in a Competitive Power Exchange Market,February 2001,Chonawee Supatgiat,Rachel Q. Zhang,John R. Birge,Unknown,Female,Male,Mix,,
17.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011638322560,Preface,June 2001,David A. Belsley,,,Male,Unknown,Unknown,Male,,
17.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011624124377,A Higher-Order Taylor Expansion Approach to Simulation of Stochastic Forward-Looking Models with an Application to a Nonlinear Phillips Curve Model,June 2001,Fabrice Collard,Michel Juillard,,Male,Male,Unknown,Male,,24
17.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011619907538,Extending the High Level Architecture Paradigm to Economic Simulation,June 2001,James A. Calpin,Marnie R. Salisbury,David R. Woodward,Male,Female,Male,Mix,,
17.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011676109356,Limited Computational Ability and Approximation of Dynamical Systems,June 2001,Domenico Colucci,,,Male,Unknown,Unknown,Male,,1
17.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011640512990,Robust Estimation of GARMA Model Parameters with an Application to Cointegration among Interest Rates of Industrialized Countries,June 2001,Rajalakshmi Ramachandran,Paul Beaumont,,Unknown,Male,Unknown,Male,,16
17.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011648714807,Estimating Internet Users' Demand Characteristics,June 2001,Alok Gupta,Boris Jukic,Andrew B. Whinston,Male,Male,Male,Male,,7
17.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011667806630,"Asset Pricing Models, Specification Search, and Stability Analysis",June 2001,J. del Hoyo,J. Guillermo Llorente,,Unknown,Unknown,Unknown,Unknown,,
17.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011632311173,Performance of a Hedged Stochastic Portfolio Model in the Presence of Extreme Events,June 2001,Rosella Castellano,Rosella Giacometti,,Female,Female,Unknown,Female,,
17.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011657018442,The Influence of Evolutionary Selection Schemes on the Iterated Prisoner's Dilemma,June 2001,David van Bragt,Cees van Kemenade,Han la Poutré,Male,Male,,Mix,,
17.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1011673403421,Bayesian Analysis of the Stochastic Switching Regression Model Using Markov Chain Monte Carlo Methods,June 2001,Maria Ana E. Odejar,Mark S. McNulty,,Female,Male,Unknown,Mix,,
17.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017175023894,Volume Contents for Volume 17,June 2001,,,,Unknown,Unknown,Unknown,Unknown,,
18.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1013852626172,Introduction,August 2001,Leigh Tesfatsion,,,,Unknown,Unknown,Mix,,
18.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1013810410243,Learning to Be Thoughtless: Social Norms and Individual Computation,August 2001,Joshua M. Epstein,,,Male,Unknown,Unknown,Male,,
18.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1013862427081,Self Organization and Coordination,August 2001,Scott E. Page,,,Male,Unknown,Unknown,Male,,12
18.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1013814511151,Bilateral Trade and ‘Small-World’ Networks,August 2001,Allen Wilhite,,,Male,Unknown,Unknown,Male,,
18.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1013866527989,Leaving the Prison: Permitting Partner Choice and Refusal in Prisoner's Dilemma Games,August 2001,Esther Hauk,,,Female,Unknown,Unknown,Female,,17
18.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1013818611576,Reinforcement Learning Rules in a Repeated Game,August 2001,Ann Maria Bell,,,Female,Unknown,Unknown,Female,,11
18.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1013892610185,Towards a Descriptive Model of Agent Strategy Search,August 2001,Bruce Edmonds,,,Male,Unknown,Unknown,Male,,8
18.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1021078206731,Introduction,October 2001,Anantha Kumar Duraiappah,Richard S.J. Tol,,Unknown,Male,Unknown,Male,,
18.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1021086215235,Estimating a Game Theoretic Model,October 2001,Wietze Lise,,,Male,Unknown,Unknown,Male,,9
18.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1021062232074,Climate Coalitions in an Integrated Assessment Model,October 2001,Richard S.J. Tol,,,Male,Unknown,Unknown,Male,,31
18.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1021014316144,Influence of Economic Constraints on the Shape of Emission Corridors,October 2001,Marian Leimbach,Thomas Bruckner,,Male,Male,Unknown,Male,,7
18.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1021066300214,Formulating and Solving Nonlinear Integrated Ecological-Economic Models Using GAMS,October 2001,Anantha Kumar Duraiappah,,,Unknown,Unknown,Unknown,Unknown,,
18.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1021018417052,Solving Infinite Horizon Growth Models with an Environmental Sector,October 2001,David L. Kelly,Charles D. Kolstad,,Male,Male,Unknown,Male,,9
18.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1014870904813,A Computational Approach to the Fundamental Theorem of Asset Pricing in a Single-Period Market,December 2001,F. Acedo,F. Benito,J. Torres,Unknown,Unknown,Unknown,Unknown,,
18.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1014811703866,"Modeling Instrumental Rationality, Land Tenure and Conflict Resolution",December 2001,Hans M. Amman,Anantha Kumar Duraiappah,,Male,Unknown,Unknown,Male,,5
18.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1014876026892,Two-Stage Budgeting: A Difficult Problem,December 2001,A. Norman,J. Chou,M. Rajan,Unknown,Unknown,Unknown,Unknown,,
18.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1014817821514,A Merit Function for Variational Inequalities Applied to Equilibrium Problems,December 2001,Gianfranco Corradi,,,Male,Unknown,Unknown,Male,,
18.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1014824005585,Digital Portfolio Theory,December 2001,C. Kenneth Jones,,,Unknown,Unknown,Unknown,Unknown,,
18.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017395800464,Author Index to Volumes 17 and 18,December 2001,,,,Unknown,Unknown,Unknown,Unknown,,
18.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017332616394,Subject Index to Volumes 17 and 18,December 2001,,,,Unknown,Unknown,Unknown,Unknown,,
18.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017347917302,Volume Contents for Volumes 17 and 18,December 2001,,,,Unknown,Unknown,Unknown,Unknown,,
19.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1014961502004,Introduction,February 2002,Nicolaas J. Vriend,,,Male,Unknown,Unknown,Male,,
19.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1014988805326,Financial Markets can be at Sub-Optimal Equilibria,February 2002,Shareen Joshi,Jeffrey Parker,Mark A. Bedau,Female,Male,Male,Mix,,
19.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1014936807143,Evolutionary Models of Bargaining: Comparing Agent-based Computational and Analytical Approaches to Understanding Convention Evolution,February 2002,Jeffrey P. Carpenter,,,Male,Unknown,Unknown,Male,,16
19.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1014945023982,Competing R&D Strategies in an Evolutionary Industry Model,February 2002,Murat Yildizoglu,,,Male,Unknown,Unknown,Male,,26
19.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1014997009869,A Behavioural Learning Approach to the Dynamics of Prices,February 2002,Thomas Brenner,,,Male,Unknown,Unknown,Male,,15
19.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1014957310778,"Heterogeneous Beliefs, Risk and Learning in a Simple Asset Pricing Model",February 2002,Carl Chiarella,Xue-Zhong He,,Male,,Unknown,Mix,,
19.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1015001204774,Analytical Score for Multivariate GARCH Models,April 2002,Riccardo Lucchetti,,,Male,Unknown,Unknown,Male,,17
19.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1015021911216,Maximum Likelihood Estimation Using Parallel Computing: An Introduction to MPI,April 2002,Christopher A. Swann,,,Male,Unknown,Unknown,Male,,22
19.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1015051912125,The Dynamics of Palladium and Platinum Prices,April 2002,Bahram Adrangi,Arjun Chatrath,,Male,Male,Unknown,Male,,11
19.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1015056028963,Rational Error Correction,April 2002,P.A. Tinsley,,,Unknown,Unknown,Unknown,Unknown,,
19.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1015008130780,"Procurement Bidding in First-Price and Second-Price, Sealed-Bid Auctions within the Common-Value Paradigm",April 2002,Anders Lunander,,,Male,Unknown,Unknown,Male,,8
19.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1015590616630,Editorial,June 2002,Prof. Constantin Zopounidis,Prof. Panos M. Pardalos,Prof. Jaime Gil-Aluja,Unknown,Unknown,Unknown,Unknown,,
19.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1015596821173,Capturing and Tuning Nonlinear Characteristics of Economic Stabilization Systems by Fuzzy Control Techniques,June 2002,V. Georgescu,,,Unknown,Unknown,Unknown,Unknown,,
19.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1015540718447,Two Fuzzy Approaches for Solving Multiobjective Decision Problems,June 2002,Teresa León,Vicente Liern,Enriqueta Vercher,Female,Male,Female,Mix,,
19.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1015588600700,Rough Sets and Multivariate Statistical Classification: A Simulation Study,June 2002,Michael Doumpos,Constantin Zopounidis,,Male,Male,Unknown,Male,,5
19.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1015551826141,A Non-Parametric Approach to Pricing and Hedging Derivative Securities: With an Application to LIFFE Data,June 2002,J.A. Barria,S.G. Hall,,Unknown,Unknown,Unknown,Unknown,,
19.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1015509225232,Semiparametric Estimation of the Fractional Differencing Parameter of Measures of the U.K. Unemployment,June 2002,Luis A. Gil-Alana,,,Male,Unknown,Unknown,Male,,6
19.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1015568516528,Solution of Multi-Player Linear-Quadratic Alternating-Move Games and Its Application to the Timing Pattern of Wage Adjustment,June 2002,Sau-Him Paul Lau,,,Unknown,Unknown,Unknown,Unknown,,
19.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1017499702155,Volume contents for Volume 19,June 2002,,,,Unknown,Unknown,Unknown,Unknown,,
20.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1020517101123,Solving Linear Rational Expectations Models,October 2002,Christopher A. Sims,,,Male,Unknown,Unknown,Male,,537
20.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1020534927853,Solving Dynamic Equilibrium Models by a Method of Undetermined Coefficients,October 2002,Lawrence J. Christiano,,,Male,Unknown,Unknown,Male,,42
20.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1020576911923,System Reduction and Solution Algorithms for Singular Linear Difference Systems under Rational Expectations,October 2002,Robert G. King,Mark W. Watson,,Male,Male,Unknown,Male,,46
20.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1020529028761,"Production, Growth and Business Cycles: Technical Appendix",October 2002,Robert G. King,Charles I. Plosser,Sergio T. Rebelo,Male,Male,Male,Male,,114
21.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1022250817454,Editor's Preface,February 2003,David A. Belsley,,,Male,Unknown,Unknown,Male,,
21.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1022281319272,Estimation of VAR Models Computational Aspects,February 2003,Paolo Foschi,Erricos J. Kontoghiorghes,,Male,Unknown,Unknown,Male,,9
21.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1022289509702,Time Series Simulation with Quasi Monte Carlo Methods,February 2003,Jenny X. Li,Peter Winker,,Female,Male,Unknown,Mix,,
21.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1022270430175,Higher-Moments in Perturbation Solution of the Linear-Quadratic Exponential Gaussian Optimal Control Problem,February 2003,Baoline Chen,Peter A. Zadrozny,,Unknown,Male,Unknown,Male,,7
21.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1022238914245,"Macroeconomic Effects of Sectoral Shocks in Germany, The U.K. and, The U.S. A VAR-GARCH-M Approach",February 2003,Gianluigi Pelloni,Wolfgang Polasek,,Male,Male,Unknown,Male,,10
21.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1022291016063,Two Models of Information Costs Based on Computational Complexity,February 2003,Mario Eboli,,,Male,Unknown,Unknown,Male,,
21.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1022299201950,Self-Organizing Production and Exchange,February 2003,Allen Wilhite,,,Male,Unknown,Unknown,Male,,3
21.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1022259518788,Optimal Product Lifecycle and Partial Information with Active Learning,February 2003,Arik Sadeh,,,Male,Unknown,Unknown,Male,,3
21.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1022211603767,A Computational Approach to the Collective Action Problem: Assessment of Alternative Learning Rules,February 2003,Juan D. Montoro-Pons,Francisco Garcia-Sobrecases,,Male,Male,Unknown,Male,,1
21.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1022267720606,Computational Tools for the Analysis of Market Risk,February 2003,Alberto Suárez,Santiago Carrillo,,Male,Male,Unknown,Male,,3
21.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1022219804676,Modeling Expectations with GENEFER – an Artificial Intelligence Approach,February 2003,Eric Ringhut,Stefan Kooths,,Male,Male,Unknown,Male,,7
22.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1024571428545,An Information Theoretic Approach to Estimation in the Case of Multicollinearity,August 2003,Marco van Akkeren,,,Male,Unknown,Unknown,Male,,
22.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1024553430101,Testing the Power of a Generalization of the KPSS-Tests against Fractionally Integrated Hypotheses,August 2003,L.A. Gil-Alana,,,Unknown,Unknown,Unknown,Unknown,,
22.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1024592607487,"Multi-Issue Negotiation Processes by Evolutionary Simulation, Validation and Social Extensions",August 2003,Enrico Gerding,David van Bragt,Han La Poutré,Male,Male,,Mix,,
22.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1024561209304,Seasonal Misspecification in the Context of Fractionally Integrated Univariate Time Series,August 2003,Luis A. Gil-Alana,,,Male,Unknown,Unknown,Male,,4
22.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1024569426143,Green Tax Reforms and Computational Economics A Do-it-yourself Approach,August 2003,Christoph Böhringer,Wolfgang Wiegard,Anna Ruocco,Male,Male,Female,Mix,,
22.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1026132128315,Editor's Preface,October 2003,David A. Belsley,,,Male,Unknown,Unknown,Male,,
22.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1026177612385,An Implementation of Bouchouev's Method for a Short Time Calibration of Option Pricing Models,October 2003,Carl Chiarella,Mark Craddock,Nadima El-Hassan,Male,Male,Unknown,Male,,2
22.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1026181713294,A Potential-Field Approach to Financial Time Series Modelling,October 2003,S. Borovkova,H. Dehling,H. Tulleken,Unknown,Unknown,Unknown,Unknown,,
22.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1026133830132,Different Phases in a Supermarket Chain Network: An Application of an Ising Model on Soap Froth,October 2003,Kwok Yip Szeto,Chiwah Kong,,Unknown,Unknown,Unknown,Unknown,,
22.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1026185814203,Numerical Solutions to Some Optimal Control Problems Arising from Innovation Diffusion,October 2003,Luigi De Cesare,Andrea Di Liddo,Stefania Ragni,Male,Female,Female,Mix,,
22.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1026129729224,Bilateral Bootstrap Tests for Long Memory: An Application to the Silver Market,October 2003,Christian de Peretti,,,Male,Unknown,Unknown,Male,,8
22.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1026137931041,Asset Price Dynamics among Heterogeneous Interacting Agents,October 2003,Carl Chiarella,Mauro Gallegati,Antonio Palestrini,Male,Male,Male,Male,,33
22.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1026189916020,Correcting for Omitted-Variable and Measurement-Error Bias in Autoregressive Model Estimation with Panel Data,October 2003,P. A.V. B. Swamy,I-Lok Chang,George S. Tavlas,Unknown,Unknown,Male,Male,,17
22.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1026146100090,Traders' Long-Run Wealth in an Artificial Financial Market,October 2003,Marco Raberto,Silvano Cincotti,Michele Marchesi,Male,Male,Female,Mix,,
22.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1026198216929,Using a Stochastic Complexity Measure to Check the Efficient Market Hypothesis,October 2003,Armin Shmilovici,Yael Alon-Brimer,Shmuel Hauser,Male,Female,Male,Mix,,
22.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1026150300999,A Simulation Framework for Heterogeneous Agents,October 2003,David Meyer,Alexandros Karatzoglou,Kurt Hornik,Male,Male,Male,Male,,10
22.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1026157427528,Author Index to Volumes 21 and 22,October 2003,,,,Unknown,Unknown,Unknown,Unknown,,
22.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1026170301311,Subject Index to Volumes 21 and 22,October 2003,,,,Unknown,Unknown,Unknown,Unknown,,
22.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/A:1026118311599,Volume Contents for Volumes 21 en 22,October 2003,,,,Unknown,Unknown,Unknown,Unknown,,
23.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000007093.33504.9a,Simulating Computable Overlapping Generations Models with TROLL,February 2004,Frédéric Docquier,Philippe Liégeois,,Male,Male,Unknown,Male,,2
23.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000007091.16127.c5,Estimating an Endogenous Growth Model with Public Capital and Government Borrowing: U.S. and Germany 1960–1995,February 2004,Alfred Greiner,Willi Semmler,Gang Gong,Male,Male,,Mix,,
23.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000007188.61993.91,Exact Inference Using Variable Integrating Constant Importance Distributions,February 2004,Charles J. Romeo,Teruo Nakatsuma,,Male,Male,Unknown,Male,,
23.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000007187.30194.2e,Who Should Pay How Much?,February 2004,Christopher Böhringer,Thomas F. Rutherford,,Male,Male,Unknown,Male,,10
23.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000021667.87807.67,General Equilibrium Tax Policy with Hyperbolic Consumers,March 2004,Toke Ward Petersen,,,Male,Unknown,Unknown,Male,,4
23.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000021670.32793.ea,A Simulation Model of the Price Bargaining Rules in Vertical Relationships,March 2004,J. Duvallet,A. Garapin,D. Llerena,Unknown,Unknown,Unknown,Unknown,,
23.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000021673.38534.ef,Computing Equilibria in General Equilibrium Models via Interior-point Methods,March 2004,Mercedes Esteban-Bravo,,,Female,Unknown,Unknown,Female,,15
23.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000021676.64701.12,On the Computational Complexity of Consumer Decision Rules,March 2004,A. Norman,A. Ahmed,I. White,Unknown,Unknown,Unknown,Unknown,,
23.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000021677.46295.60,"Gold Price, Neural Networks and Genetic Algorithm",March 2004,Sam Mirmirani,H.C. Li,,,Unknown,Unknown,Mix,,
23.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000022833.85184.ba,Using the BACC Software for Bayesian Inference,April 2004,William J. McCausland,,,Male,Unknown,Unknown,Male,,8
23.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000022834.86489.e5,Multiscale Analysis of Stock Index Return Volatility,April 2004,Enrico Capobianco,,,Male,Unknown,Unknown,Male,,13
23.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000022831.31888.27,Structural Change and the Order of Integration in Univariate Time Series,April 2004,Luis A. Gil-Alana,,,Male,Unknown,Unknown,Male,,2
23.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000022832.25569.3e,Asset Price Anomalies under Bounded Rationality,April 2004,Emilio Barucci,Roberto Monte,Roberto Renò,Male,Male,Male,Male,,4
23.0,3.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000022836.40541.9f,Spectral Analysis as a Tool for Financial Policy: An Analysis of the Short-End of the British Term Structure,April 2004,Andrew Hughes Hallett,Christian R. Richter,,Male,Male,Unknown,Male,,23
23.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000026787.81469.1f,Computing Economic Chaos,June 2004,Richard H. Day,Oleg V. Pavlov,,Male,Male,Unknown,Male,,4
23.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000026788.19107.15,The Timing of Uncertainty and the Intensity of Policy,June 2004,P. Ruben Mercado,,,Unknown,Unknown,Unknown,Unknown,,
23.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000026794.43145.fc,The Stochastic Permanent Break Model and the Fractional Integration Hypothesis,June 2004,Luis A. Gil-Alana,,,Male,Unknown,Unknown,Male,,
23.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000026795.44450.4a,A Practical Method for Explicitly Modeling Quotas and Other Complementarities,June 2004,W. Jill Harrison,Mark Horridge,Glyn Wittwer,Unknown,Male,,Mix,,
23.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000026797.96647.40,Is It Worth Refining Linear Approximations to Non-Linear Rational Expectations Models?,June 2004,Alfonso Novales,Javier J. Pérez,,Male,,Unknown,Mix,,
23.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000026805.91428.af,The Numerical Performance of Fast Bootstrap Procedures,June 2004,Jean-François Lamarche,,,Unknown,Unknown,Unknown,Unknown,,
23.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000026867.26185.a3,Author Index Volume 23,June 2004,,,,Unknown,Unknown,Unknown,Unknown,,
23.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000026868.85771.4c,Subject Index Volume 23,June 2004,,,,Unknown,Unknown,Unknown,Unknown,,
23.0,4.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000026862.65128.48,Volume Contents Volume 23,June 2004,,,,Unknown,Unknown,Unknown,Unknown,,
24.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000038839.98836.5c,Variations on the Theme of Scarf's Counter-Example,August 2004,Alok Kumar,Martin Shubik,,Male,Male,Unknown,Male,,18
24.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000038838.40320.57,Allocating the Cost of Congestion with the Nucleolus,August 2004,Gilles Reinhardt,,,Male,Unknown,Unknown,Male,,
24.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000038840.58451.c9,Evaluating the Noncentral Chi-Square Distribution for the Cox-Ingersoll-Ross Process,August 2004,S. Dyrting,,,Unknown,Unknown,Unknown,Unknown,,
24.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000038851.72226.76,Analytical Derivates of the APARCH Model,August 2004,Sébastien Laurent,,,Male,Unknown,Unknown,Male,,28
24.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000038893.70411.f5,A Log-Linear Homotopy Approach to Initialize the Parameterized Expectations Algorithm,August 2004,Javier J. Pérez,,,,Unknown,Unknown,Mix,,
24.0,1.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000038894.78378.f1,Analytic Derivatives for Linear Rational Expectations Models,August 2004,Andrew P. Blake,,,Male,Unknown,Unknown,Male,,5
24.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000049438.56302.14,"Uncertainty, Political Preferences, and Stabilization: Stochastic Control Using Dynamic CGE Models",September 2004,Seung-Rae Kim,,,Unknown,Unknown,Unknown,Unknown,,
24.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000049451.49953.28,Equilibrium Prices on a Financial Graph,September 2004,Paolo Falbo,Rosanna Grassi,,Male,Female,Unknown,Mix,,
24.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000049452.14102.bc,On Stochastic Simulation of Forward-Looking Models,September 2004,Dag Kolsrud,,,Male,Unknown,Unknown,Male,,2
24.0,2.0,Computational Economics,,https://link.springer.com/article/10.1023/B:CSEM.0000049491.13935.af,The Conditional Probability Density Function for a Reflected Brownian Motion,September 2004,Dirk Veestraeten,,,Male,Unknown,Unknown,Male,,21
24.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-004-3695-9,Robust Control: A Note on the Timing of Model Uncertainty,October 2004,Arnulfo Rodriguez,,,Unknown,Unknown,Unknown,Unknown,,
24.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-004-4124-9,Robust Control: A Note on the Response of the Control to Changes in the “Free” Parameter Conditional on the Character of Nature,October 2004,Fidel Gonzalez,Arnulfo Rodriguez,,Male,Unknown,Unknown,Male,,4
24.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-004-3544-x,"The Exact Maximum Likelihood-Based Test for Fractional Cointegration: Critical Values, Power and Size",October 2004,Emmanuel Dubois,Sandrine Lardic,Valérie Mignon,Male,Female,Female,Mix,,
24.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-004-4197-5,Can Genetic Algorithms Explain Experimental Anomalies?,October 2004,Marco Casari,,,Male,Unknown,Unknown,Male,,18
24.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-004-4657-y,A Generalized BDS Statistic,October 2004,M. Matilla-GarcÍa,R. Queralt,F. J. VÁzquez,Unknown,Unknown,Unknown,Unknown,,
24.0,3.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-004-2005-x,"Berc Rustem and Melendres Howe, Algorithms for Worst-Case Design and Applications to Risk Management. Princeton, NJ: Princeton University Press, 2002. ISBN 0-691-09154-4",October 2004,Robert J. Tetlow,,,Male,Unknown,Unknown,Male,,
24.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-5169-0,Identifying Volatility Clusters Using the PPM: A Sensitivity Analysis,June 2005,Rosangela H. Loschi,Leonardo S. Bastos,Pilar L. Iglesias,Female,Male,Female,Mix,,
24.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6158-z,Evaluating Market Attractiveness: Individual Incentives Versus Industry Profitability,June 2005,Herbert Dawid,Marc Reimann,,Male,Male,Unknown,Male,,12
24.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6159-y,A Model of Primary and Secondary Waves in Investment Cycles,June 2005,Guido Fioretti,,,Male,Unknown,Unknown,Male,,4
24.0,4.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6160-5,Population Learning in a Model with Random Payoff Landscapes and Endogenous Networks,June 2005,Giorgio Fagiolo,Luigi Marengo,Marco Valente,Male,Male,Male,Male,,3
25.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6244-2,"Editor’s Preface: Computational Economics and Finance, Amsterdam",February 2005,David A. Belsley,,,Male,Unknown,Unknown,Male,,
25.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6245-1,Strategies for the Diffusion of Innovations on Social Networks,February 2005,Floortje Alkemade,Carolina Castaldi,,Female,Female,Unknown,Female,,67
25.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6246-0,The Role of Heterogeneous Agents’ Past and Forward Time Horizons in Formulating Computational Models,February 2005,Serge Hayward,,,Male,Unknown,Unknown,Male,,3
25.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6273-x,"Minority Games, Local Interactions, and Endogenous Networks",February 2005,Giorgio Fagiolo,Marco Valente,,Male,Male,Unknown,Male,,5
25.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6274-9,Learning in a Network Economy,February 2005,Jie-Shin Lin,,,Unknown,Unknown,Unknown,Unknown,,
25.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6276-7,A Frequency Selective Filter for Short-Length Time Series,February 2005,Alessandra Iacobucci,Alain Noullez,,Female,Male,Unknown,Mix,,
25.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6277-6,Tests of Long Memory: A Bootstrap Approach,February 2005,Pilar Grau-Carles,,,Female,Unknown,Unknown,Female,,40
25.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6278-5,Keynesian Dynamics and the Wage–Price Spiral: Identifying Downward Rigidities,February 2005,Pu Chen,Peter Flaschel,,,Male,Unknown,Mix,,
25.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6279-4,Valuation of American Continuous-Installment Options,February 2005,Pierangelo Ciurlia,Ilir Roko,,Male,Male,Unknown,Male,,24
25.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6280-y,Solving SDGE Models: A New Algorithm for the Sylvester Equation,February 2005,OndŘej KamenÍk,,,Unknown,Unknown,Unknown,Unknown,,
25.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6282-9,Aggregation of Dependent Risks Using the Koehler–Symanowski Copula Function,February 2005,Paola Palmitesta,Corrado Provasi,,Female,Male,Unknown,Mix,,
26.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6414-2,Quadrature-Based Methods for Solving Heterogeneous Agent Models with Discontinuous Distributions,August 2005,Robert M. Hussey,,,Male,Unknown,Unknown,Male,,1
26.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6415-1,Estimation of Agent-Based Models: The Case of an Asymmetric Herding Model,August 2005,Simone Alfarano,Thomas Lux,Friedrich Wagner,Female,Male,Male,Mix,,
26.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-7315-0,Precautionary Money Demand in a Cash-in-Advance Economy with Capital,August 2005,Jana Hromcová,,,Female,Unknown,Unknown,Female,,2
26.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-7366-2,Detecting Business Cycle Asymmetries Using Artificial Neural Networks and Time Series Models,August 2005,Khurshid M. Kiani,,,Male,Unknown,Unknown,Male,,14
26.0,1.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-7751-x,Solving Rational-Expectations Models through the Anderson-Moore Algorithm: An Introduction to the Matlab Implementation,August 2005,Paolo Zagaglia,,,Male,Unknown,Unknown,Male,,2
26.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6868-2,User-Friendly Parallel Computations with Econometric Examples,October 2005,Michael Creel,,,Male,Unknown,Unknown,Male,,32
26.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-6869-1,A Possible Conflict between Economic Efficiency and Political Pressure,October 2005,Phil Simmons,Oscar J. Cacho,,Male,Male,Unknown,Male,,1
26.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-0667-7,Computational Issues in the Sequential Probit Model: A Monte Carlo Study,October 2005,Patrick Waelbroeck,,,Male,Unknown,Unknown,Male,,11
26.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-1732-y,Solving the Neoclassical Growth Model with Quasi-Geometric Discounting: A Grid-Based Euler-Equation Method,October 2005,Lilia Maliar,Serguei Maliar,,Female,Male,Unknown,Mix,,
26.0,2.0,Computational Economics,,https://link.springer.com/article/10.1007/s10614-005-1784-z,A MATLAB Solver for Nonlinear Rational Expectations Models,October 2005,Paul L. Fackler,,,Male,Unknown,Unknown,Male,,4
26.0,3.0,Computational Economics,19 January 2006,https://link.springer.com/article/10.1007/s10614-005-9001-7,Discrete Working Time Choice in an Applied General Equilibrium Model,November 2005,Stefan Boeters,Michael Feil,Nicole Gürtzgen,Male,Male,Female,Mix,,
26.0,3.0,Computational Economics,19 January 2006,https://link.springer.com/article/10.1007/s10614-005-9003-5,Individual and Social Learning,November 2005,Nobuyuki Hanaki,,,Male,Unknown,Unknown,Male,,6
26.0,3.0,Computational Economics,17 January 2006,https://link.springer.com/article/10.1007/s10614-005-9004-4,A Computer Algebra Primer and Homework Exercises for use in an Intermediate Macroeconomics Course – A Student/Teacher Collaboration,November 2005,Luke Olson,Max Jerrell,Ryder Delaloye,Male,Male,Male,Male,,
26.0,3.0,Computational Economics,26 January 2006,https://link.springer.com/article/10.1007/s10614-005-9008-0,The KPSS Test with Outliers,November 2005,Jesús Otero,Jeremy Smith,,,Male,Unknown,Mix,,
26.0,3.0,Computational Economics,19 January 2006,https://link.springer.com/article/10.1007/s10614-005-9010-6,Extracting Information from Spot Interest Rates and Credit Ratings using Double Higher-Order Hidden Markov Models,November 2005,Tak-Kuen Siu,Wai-Ki Ching,Michael K. Ng,Unknown,Unknown,Male,Male,,6
26.0,3.0,Computational Economics,07 March 2006,https://link.springer.com/article/10.1007/s10614-005-9011-5,Numerical Inversion Methods for Computing Approximate p-Values,November 2005,Hiroyuki Kawakatsu,,,Male,Unknown,Unknown,Male,,2
27.0,1.0,Computational Economics,03 May 2006,https://link.springer.com/article/10.1007/s10614-005-9013-3,Introduction,February 2006,Roberto Leombruni,Matteo Richiardi,,Male,Male,Unknown,Male,,11
27.0,1.0,Computational Economics,03 May 2006,https://link.springer.com/article/10.1007/s10614-005-9014-2,An Evolutionary Model of Endogenous Business Cycles,February 2006,Giovanni Dosi,Giorgio Fagiolo,Andrea Roventini,Male,Male,Female,Mix,,
27.0,1.0,Computational Economics,19 April 2006,https://link.springer.com/article/10.1007/s10614-005-9015-1,Knowledge-Based Jobs and the Boundaries of Firms Agent-based Simulation of Firms Learning and Workforce Skill Set Dynamics,February 2006,Edoardo Mollona,David Hales,,Male,Male,Unknown,Male,,3
27.0,1.0,Computational Economics,03 May 2006,https://link.springer.com/article/10.1007/s10614-005-9016-0,LABORsim: An Agent-Based Microsimulation of Labour Supply – An Application to Italy,February 2006,Roberto Leombruni,Matteo Richiardi,,Male,Male,Unknown,Male,,12
27.0,1.0,Computational Economics,04 May 2006,https://link.springer.com/article/10.1007/s10614-005-9017-z,Job Search Mechanism and Individual Behaviour,February 2006,Massimo Giannini,,,Male,Unknown,Unknown,Male,,2
27.0,1.0,Computational Economics,03 May 2006,https://link.springer.com/article/10.1007/s10614-005-9018-y,Herding and Clustering in Economics: The Yule-Zipf-Simon Model,February 2006,U. Garibaldi,D. Costantini,P. Viarengo,Unknown,Unknown,Unknown,Unknown,,
27.0,1.0,Computational Economics,04 May 2006,https://link.springer.com/article/10.1007/s10614-005-9019-x,Toward a Non-Equilibrium Unemployment Theory,February 2006,Matteo Richiardi,,,Male,Unknown,Unknown,Male,,12
27.0,2.0,Computational Economics,11 May 2006,https://link.springer.com/article/10.1007/s10614-006-9022-x,The Contributions of Berç Rustem,May 2006,,,,Unknown,Unknown,Unknown,Unknown,,
27.0,2.0,Computational Economics,11 May 2006,https://link.springer.com/article/10.1007/s10614-006-9023-9,The Multifactor Nature of the Volatility of Futures Markets,May 2006,Carl Chiarella,Thuy-Duong Tô,,Male,Unknown,Unknown,Male,,3
27.0,2.0,Computational Economics,26 April 2006,https://link.springer.com/article/10.1007/s10614-006-9024-8,Feedback Approximation of the Stochastic Growth Model by Genetic Neural Networks,May 2006,S. Sirakaya,Stephen Turnovsky,M. Nedim Alemdar,Unknown,Male,Unknown,Male,,2
27.0,2.0,Computational Economics,16 May 2006,https://link.springer.com/article/10.1007/s10614-006-9025-7,An Application of Extreme Value Theory for Measuring Financial Risk,May 2006,Manfred Gilli,Evis këllezi,,Male,Male,Unknown,Male,,193
27.0,2.0,Computational Economics,11 May 2006,https://link.springer.com/article/10.1007/s10614-006-9026-6,Measuring the Degree of Convergence among European Business Cycles,May 2006,Andrew Hughes Hallett,Christian Richter,,Male,Male,Unknown,Male,,13
27.0,2.0,Computational Economics,11 May 2006,https://link.springer.com/article/10.1007/s10614-006-9027-5,Computational Economics: Help for the Underestimated Undergraduate,May 2006,David A. Kendrick,P. Ruben Mercado,Hans M. Amman,Male,Unknown,Male,Male,,4
27.0,2.0,Computational Economics,11 May 2006,https://link.springer.com/article/10.1007/s10614-006-9028-4,An Enhanced Dynamic Slope Scaling Procedure with Tabu Scheme for Fixed Charge Network Flow Problems,May 2006,Dukwon Kim,Xinyan Pan,Panos M. Pardalos,Unknown,Unknown,Male,Male,,11
27.0,2.0,Computational Economics,11 May 2006,https://link.springer.com/article/10.1007/s10614-006-9029-3,Minding the Gap: Central Bank Estimates of the Unemployment Natural Rate,May 2006,Sharon Kozicki,P. A. Tinsley,,Female,Unknown,Unknown,Female,,8
27.0,2.0,Computational Economics,11 May 2006,https://link.springer.com/article/10.1007/s10614-006-9030-x,Robust Artificial Neural Networks for Pricing of European Options,May 2006,Panayiotis C. Andreou,Chris Charalambous,Spiros H. Martzoukos,Male,,Male,Mix,,
27.0,2.0,Computational Economics,11 May 2006,https://link.springer.com/article/10.1007/s10614-006-9031-9,"The Evolution and Emergence of Integrated Social and Financial Networks with Electronic Transactions: A Dynamic Supernetwork Theory for the Modeling, Analysis, and Computation of Financial Flows and Relationship Levels",May 2006,Anna Nagurney,Tina Wakolbinger,Li Zhao,Female,Female,,Mix,,
27.0,2.0,Computational Economics,16 May 2006,https://link.springer.com/article/10.1007/s10614-006-9032-8,Auctioning Bulk Mobile Messages,May 2006,S. Meij,L.-F. Pau,,Unknown,Unknown,Unknown,Unknown,,
28.0,1.0,Computational Economics,28 June 2006,https://link.springer.com/article/10.1007/s10614-006-9021-y,Improving Portfolio Efficiency: A Genetic Algorithm Approach,August 2006,Xiaolou Yang,,,Unknown,Unknown,Unknown,Unknown,,
28.0,1.0,Computational Economics,21 April 2006,https://link.springer.com/article/10.1007/s10614-006-9033-7,Incentives to Cooperate in Network Formation,August 2006,Haydée Lugo,Raúl Jiménez,,Unknown,Male,Unknown,Male,,9
28.0,1.0,Computational Economics,27 April 2006,https://link.springer.com/article/10.1007/s10614-006-9034-6,Introducing Imperfect Competition in CGE Models: Technical Aspects and Implications,August 2006,Roberto Roson,,,Male,Unknown,Unknown,Male,,12
28.0,1.0,Computational Economics,13 July 2006,https://link.springer.com/article/10.1007/s10614-006-9039-1,On the Selection of Adaptive Algorithms in ABM: A Computational-Equivalence Approach,August 2006,Shu-Heng Chen,Chung-Ching Tai,,Unknown,Unknown,Unknown,Unknown,,
28.0,1.0,Computational Economics,05 July 2006,https://link.springer.com/article/10.1007/s10614-006-9041-7,Forecasting Economic Data with Neural Networks,August 2006,Farzan Aminian,E. Dante Suarez,Daniel T. Walz,Unknown,Unknown,Male,Male,,46
28.0,2.0,Computational Economics,29 June 2006,https://link.springer.com/article/10.1007/s10614-006-9045-3,Guest Editorial: Introduction,September 2006,Peter J. Stemp,,,Male,Unknown,Unknown,Male,,
28.0,2.0,Computational Economics,29 June 2006,https://link.springer.com/article/10.1007/s10614-006-9035-5,Controllability in Policy Games: Policy Neutrality and the Theory of Economic Policy Revisited,September 2006,N. Acocella,G. Di Bartolomeo,Andrew Hughes Hallett,Unknown,Unknown,Male,Male,,8
28.0,2.0,Computational Economics,23 August 2006,https://link.springer.com/article/10.1007/s10614-006-9036-4,The Impact of Short-Sale Constraints on Asset Allocation Strategies via the Backward Markov Chain Approximation Method,September 2006,Carl Chiarella,Chih-Ying Hsiao,,Male,Unknown,Unknown,Male,,1
28.0,2.0,Computational Economics,26 September 2006,https://link.springer.com/article/10.1007/s10614-006-9043-5,The Linearisation and Optimal Control of Large Non-Linear Rational Expectations Models by Persistent Excitation,September 2006,Luisa Corrado,Sean Holly,,Female,Male,Unknown,Mix,,
28.0,2.0,Computational Economics,23 September 2006,https://link.springer.com/article/10.1007/s10614-006-9037-3,Central Bank Learning and Taylor Rules with Sticky Import Prices,September 2006,G. C. Lim,Paul D. Mcnelis,,Unknown,Male,Unknown,Male,,
28.0,2.0,Computational Economics,23 August 2006,https://link.springer.com/article/10.1007/s10614-006-9038-2,Congestion and Fiscal Policy in a Two-Sector Economy with Public Capital: A Quantitative Assessment,September 2006,Mihaela Pintea,Stephen J. Turnovsky,,Female,Male,Unknown,Mix,,
28.0,2.0,Computational Economics,04 July 2006,https://link.springer.com/article/10.1007/s10614-006-9042-6,Solving Non-Linear Models with Saddle-Path Instabilities,September 2006,Peter J. Stemp,Ric D Herbert,,Male,Unknown,Unknown,Male,,4
28.0,3.0,Computational Economics,26 September 2006,https://link.springer.com/article/10.1007/s10614-006-9046-2,The Cognitive Origins of Social Stratification,October 2006,Robert Hoffmann,,,Male,Unknown,Unknown,Male,,3
28.0,3.0,Computational Economics,06 July 2006,https://link.springer.com/article/10.1007/s10614-006-9047-1,On the Computation of Stability in Multiple Coalition Formation Games,October 2006,M Elena Sáiz,Eligius M. T Hendrix,Niels J Olieman,Unknown,Unknown,Male,Male,,10
28.0,3.0,Computational Economics,26 September 2006,https://link.springer.com/article/10.1007/s10614-006-9048-0,Optimizing the Garch Model–An Application of Two Global and Two Local Search Methods,October 2006,Kwami Adanu,,,Unknown,Unknown,Unknown,Unknown,,
28.0,3.0,Computational Economics,26 September 2006,https://link.springer.com/article/10.1007/s10614-006-9049-z,Polynomial Algorithms for Pricing Path-Dependent Interest Rate Instruments,October 2006,Ronald Hochreiter,Georg Ch. Pflug,,Male,Male,Unknown,Male,,4
28.0,4.0,Computational Economics,08 November 2006,https://link.springer.com/article/10.1007/s10614-006-9052-4,Fundamental Results on Evolutionary Simulations of Socio-economic Systems: Introduction to the Special Issue,November 2006,Floortje Alkemade,Han La Poutré,,Female,,Unknown,Mix,,
28.0,4.0,Computational Economics,02 December 2006,https://link.springer.com/article/10.1007/s10614-006-9075-x,Republication: On the Selection of Adaptive Algorithms in ABM: A Computational-Equivalence Approach,November 2006,Shu-Heng Chen,Chung-Ching Tai,,Unknown,Unknown,Unknown,Unknown,,
28.0,4.0,Computational Economics,02 November 2006,https://link.springer.com/article/10.1007/s10614-006-9053-3,Revisiting Individual Evolutionary Learning in the Cobweb Model – An Illustration of the Virtual Spite-Effect,November 2006,Jasmina Arifovic,Michael K. Maschek,,Female,Male,Unknown,Mix,,
28.0,4.0,Computational Economics,02 November 2006,https://link.springer.com/article/10.1007/s10614-006-9051-5,Robust Evolutionary Algorithm Design for Socio-economic Simulation,November 2006,Floortje Alkemade,Han La Poutré,Hans M. Amman,Female,,Male,Mix,,
28.0,4.0,Computational Economics,26 September 2006,https://link.springer.com/article/10.1007/s10614-006-9040-8,"How Robust is the Equal Split Norm? Responsive Strategies, Selection Mechanisms and the Need for Economic Interpretation of Simulation Parameters",November 2006,Herbert Dawid,Joern Dermietzel,,Male,Unknown,Unknown,Male,,9
28.0,4.0,Computational Economics,05 July 2006,https://link.springer.com/article/10.1007/s10614-006-9044-4,Evolutionary Dynamics in Public Good Games,November 2006,Christiane Clemens,Thomas Riechmann,,Female,Male,Unknown,Mix,,
28.0,4.0,Computational Economics,22 November 2006,https://link.springer.com/article/10.1007/s10614-006-9050-6,Toward a Non-Equilibrium Unemployment Theory,November 2006,Matteo Richiardi,,,Male,Unknown,Unknown,Male,,4
29.0,1.0,Computational Economics,28 September 2006,https://link.springer.com/article/10.1007/s10614-006-9060-4,Rate of Return Parity with Robot Asset Traders,February 2007,Jason Childs,,,Male,Unknown,Unknown,Male,,
29.0,1.0,Computational Economics,21 October 2006,https://link.springer.com/article/10.1007/s10614-006-9061-3,Approximate CAPM When Preferences are CRRA,February 2007,P. Jean-jacques Herings,Felix Kubler,,Unknown,Male,Unknown,Male,,4
29.0,1.0,Computational Economics,30 January 2007,https://link.springer.com/article/10.1007/s10614-006-9074-y,A Gibbs sampler for mixed logit analysis of differentiated product markets using aggregate data,February 2007,Charles J. Romeo,,,Male,Unknown,Unknown,Male,,13
29.0,1.0,Computational Economics,31 January 2007,https://link.springer.com/article/10.1007/s10614-006-9081-z,Comparative dynamics in an overlapping-generations model: the effects of quasi-rational discrete choice on finding and maintaining Nash equilibrium,February 2007,James A. Sprigg Jr,Mark A. Ehlen,,Male,Male,Unknown,Male,,1
29.0,2.0,Computational Economics,13 December 2006,https://link.springer.com/article/10.1007/s10614-006-9067-x,A new modeling approach investigating the diffusion speed of mobile telecommunication services in EU-15,March 2007,Apostolos N. Giovanis,Christos H. Skiadas,,Male,Male,Unknown,Male,,11
29.0,2.0,Computational Economics,06 January 2007,https://link.springer.com/article/10.1007/s10614-006-9070-2,Solving parametric fuzzy systems of linear equations by a nonlinear programming method,March 2007,S. Muzzioli,H. Reynaerts,,Unknown,Unknown,Unknown,Unknown,,
29.0,2.0,Computational Economics,15 February 2007,https://link.springer.com/article/10.1007/s10614-006-9080-0,Valuing credit default swap in a non-homogeneous semi-Markovian rating based model,March 2007,Guglielmo D’Amico,Jacques Janssen,Raimondo Manca,Male,Male,Male,Male,,18
29.0,2.0,Computational Economics,08 December 2006,https://link.springer.com/article/10.1007/s10614-006-9068-9,Managing value-at-risk for a bond using bond put options,March 2007,Griselda Deelstra,Ahmed Ezzine,Michèle Vanmaele,Female,Male,Female,Mix,,
29.0,2.0,Computational Economics,30 January 2007,https://link.springer.com/article/10.1007/s10614-006-9069-8,Cutting the hedge,March 2007,Giovanni Barone-Adesi,Robert J. Elliott,,Male,Male,Unknown,Male,,12
29.0,2.0,Computational Economics,14 February 2007,https://link.springer.com/article/10.1007/s10614-006-9083-x,What is at stake in the construction and use of credit scores?,March 2007,Mireille Bardos,,,Female,Unknown,Unknown,Female,,6
29.0,2.0,Computational Economics,20 March 2007,https://link.springer.com/article/10.1007/s10614-006-9077-8,Business cycle and corporate failure in France: Is there a link?,March 2007,Eric Bataille,Catherine Bruneau,Frederic Michaud,Male,Female,Male,Mix,,
29.0,2.0,Computational Economics,01 March 2007,https://link.springer.com/article/10.1007/s10614-006-9078-7,Clustering by kernel density,March 2007,Christian Mauceri,Diem Ho,,Male,,Unknown,Mix,,
29.0,2.0,Computational Economics,27 January 2007,https://link.springer.com/article/10.1007/s10614-006-9079-6,Assessment of actions in a multi-actor and multicriteria framework: application to the refunding of microfinance institutions,March 2007,Jean Robert Kala Kamdjoug,Philippe Lenca,Jean-Pierre Barthélemy,Male,Male,Male,Male,,1
29.0,3.0,Computational Economics,30 January 2007,https://link.springer.com/article/10.1007/s10614-006-9082-y,Introduction,May 2007,Willi Semmler,,,Male,Unknown,Unknown,Male,,
29.0,3.0,Computational Economics,06 January 2007,https://link.springer.com/article/10.1007/s10614-006-9063-1,Asset pricing with dynamic programming,May 2007,Lars Grüne,Willi Semmler,,Male,Male,Unknown,Male,,7
29.0,3.0,Computational Economics,15 March 2007,https://link.springer.com/article/10.1007/s10614-006-9062-2,Computational aspects of prospect theory with asset pricing applications,May 2007,Enrico De Giorgi,Thorsten Hens,János Mayer,Male,Male,Male,Male,,29
29.0,3.0,Computational Economics,21 March 2007,https://link.springer.com/article/10.1007/s10614-006-9066-y,Approximation of jump diffusions in finance and economics,May 2007,Nicola Bruti-Liberati,Eckhard Platen,,Female,Male,Unknown,Mix,,
29.0,3.0,Computational Economics,15 March 2007,https://link.springer.com/article/10.1007/s10614-006-9065-z,Prices are macro-observables! Stylized facts from evolutionary finance,May 2007,S. Reimann,A. Tupak,,Unknown,Unknown,Unknown,Unknown,,
29.0,3.0,Computational Economics,06 January 2007,https://link.springer.com/article/10.1007/s10614-006-9071-1,Portfolio optimization when risk factors are conditionally varying and heavy tailed,May 2007,Toker Doganoglu,Christoph Hartz,Stefan Mittnik,Unknown,Male,Male,Male,,19
29.0,3.0,Computational Economics,06 January 2007,https://link.springer.com/article/10.1007/s10614-006-9073-z,Solving dynamic portfolio choice problems by recursing on optimized portfolio weights or on the value function?,May 2007,Jules H. van Binsbergen,Michael W. Brandt,,Male,Male,Unknown,Male,,34
29.0,3.0,Computational Economics,23 March 2007,https://link.springer.com/article/10.1007/s10614-006-9064-0,Strategic asset allocation and market timing: a reinforcement learning approach,May 2007,Thorsten Hens,Peter Wöhrmann,,Male,Male,Unknown,Male,,4
29.0,3.0,Computational Economics,06 January 2007,https://link.springer.com/article/10.1007/s10614-006-9072-0,Intertemporal asset allocation when the underlying factors are unobservable,May 2007,Carl Chiarella,Chih-Ying Hsiao,Willi Semmler,Male,Unknown,Male,Male,,2
29.0,3.0,Computational Economics,19 December 2006,https://link.springer.com/article/10.1007/s10614-006-9054-2,A Computer Algebra Primer and Homework Exercises for use in an Intermediate Macroeconomics Course – A Student/Teacher Collaboration,May 2007,Luke Olson,Max Jerrell,Ryder Deloloye,Male,Male,Male,Male,,
29.0,3.0,Computational Economics,19 December 2006,https://link.springer.com/article/10.1007/s10614-006-9055-1,Individual and Social Learning,May 2007,Nobuyuki Hanaki,,,Male,Unknown,Unknown,Male,,
29.0,3.0,Computational Economics,19 December 2006,https://link.springer.com/article/10.1007/s10614-006-9056-0,The KPSS Test with Outliers,May 2007,Jesús Otero,Jeremy Smith,,,Male,Unknown,Mix,,
29.0,3.0,Computational Economics,24 January 2007,https://link.springer.com/article/10.1007/s10614-006-9057-z,Extracting Information from Spot Interest Rates and Credit Ratings using Double Higher-Order Hidden Markov Models,May 2007,Tak-Kuen Siu,Wai-Ki Ching,Michael K. Ng,Unknown,Unknown,Male,Male,,
29.0,3.0,Computational Economics,19 December 2006,https://link.springer.com/article/10.1007/s10614-006-9058-y,Discrete Working Time Choice in an Applied General Equilibrium Model,May 2007,Stefan Boeters,Michael Feil,Nicole Gürtzgen,Male,Male,Female,Mix,,
29.0,3.0,Computational Economics,19 December 2006,https://link.springer.com/article/10.1007/s10614-006-9059-x,Numerical Inversion Methods for Computing Approximate p-Values,May 2007,Hiroyuki Kawakatsu,,,Male,Unknown,Unknown,Male,,
30.0,1.0,Computational Economics,10 May 2007,https://link.springer.com/article/10.1007/s10614-006-9076-9,Information technology and the welfare cost of anticipated inflation,August 2007,Thomas E. Cone,,,Male,Unknown,Unknown,Male,,
30.0,1.0,Computational Economics,22 March 2007,https://link.springer.com/article/10.1007/s10614-007-9084-4,"Reproducible research in computational economics: guidelines, integrated approaches, and open source software",August 2007,Giovanni Baiocchi,,,Male,Unknown,Unknown,Male,,15
30.0,1.0,Computational Economics,28 March 2007,https://link.springer.com/article/10.1007/s10614-007-9085-3,Computational modeling of city formation,August 2007,Kurt DeMaagd,Scott Moore,,Male,Male,Unknown,Male,,1
30.0,1.0,Computational Economics,13 March 2007,https://link.springer.com/article/10.1007/s10614-007-9086-2,Proving Arrow’s theorem by PROLOG,August 2007,Kenryo Indo,,,Unknown,Unknown,Unknown,Unknown,,
30.0,1.0,Computational Economics,09 March 2007,https://link.springer.com/article/10.1007/s10614-007-9087-1,Comparing accuracy of second-order approximation and dynamic programming,August 2007,Stephanie Becker,Lars Grüne,Willi Semmler,Female,Male,Male,Mix,,
30.0,1.0,Computational Economics,26 April 2007,https://link.springer.com/article/10.1007/s10614-007-9088-0,Business cycle and corporate failure in France: Is there a link?,August 2007,Eric Bataille,Catherine Bruneau,Frédéric Michaud,Male,Female,Male,Mix,,
30.0,2.0,Computational Economics,21 June 2007,https://link.springer.com/article/10.1007/s10614-007-9089-z,The role of intelligence in time series properties,September 2007,Chia-Hsuan Yeh,,,Unknown,Unknown,Unknown,Unknown,,
30.0,2.0,Computational Economics,27 June 2007,https://link.springer.com/article/10.1007/s10614-007-9090-6,A computational approach to modeling commodity markets,September 2007,Karla Atkins,Achla Marathe,Chris Barrett,Female,Unknown,,Mix,,
30.0,2.0,Computational Economics,05 June 2007,https://link.springer.com/article/10.1007/s10614-007-9091-5,Fast and accurate pricing of discretely monitored barrier options by numerical path integration,September 2007,Christian Skaug,Arvid Naess,,Male,Male,Unknown,Male,,9
30.0,2.0,Computational Economics,11 July 2007,https://link.springer.com/article/10.1007/s10614-007-9092-4,Multidimensional Spline Interpolation: Theory and Applications,September 2007,Christian Habermann,Fabian Kindermann,,Male,Male,Unknown,Male,,64
30.0,2.0,Computational Economics,26 June 2007,https://link.springer.com/article/10.1007/s10614-007-9094-2,A Parallel Implementation of the Simplex Function Minimization Routine,September 2007,Donghoon Lee,Matthew Wiswall,,Unknown,Male,Unknown,Male,,45
30.0,3.0,Computational Economics,09 September 2007,https://link.springer.com/article/10.1007/s10614-007-9109-z,Empirical Validation in Agent-based Models: Introduction to the Special Issue,October 2007,G. Fagiolo,C. Birchenhall,P. Windrum,Unknown,Unknown,Unknown,Unknown,,
30.0,3.0,Computational Economics,04 September 2007,https://link.springer.com/article/10.1007/s10614-007-9104-4,"A Critical Guide to Empirical Validation of Agent-Based Models in Economics: Methodologies, Procedures, and Open Problems",October 2007,Giorgio Fagiolo,Alessio Moneta,Paul Windrum,Male,Male,Male,Male,,187
30.0,3.0,Computational Economics,16 August 2007,https://link.springer.com/article/10.1007/s10614-007-9102-6,A Taxonomy of Inference in Simulation Models,October 2007,Thomas Brenner,Claudia Werker,,Male,Female,Unknown,Mix,,
30.0,3.0,Computational Economics,26 July 2007,https://link.springer.com/article/10.1007/s10614-007-9097-z,Validating and Calibrating Agent-Based Models: A Case Study,October 2007,Carlo Bianchi,Pasquale Cirillo,Pietro A. Vagliasindi,Male,Male,Male,Male,,64
30.0,3.0,Computational Economics,03 August 2007,https://link.springer.com/article/10.1007/s10614-007-9101-7,Validating Simulation Models: A General Framework and Four Applied Examples,October 2007,Robert Ernest Marks,,,Male,Unknown,Unknown,Male,,70
30.0,3.0,Computational Economics,23 August 2007,https://link.springer.com/article/10.1007/s10614-007-9095-1,Dynamic Testing of Wholesale Power Market Designs: An Open-Source Agent-Based Framework,October 2007,Junjie Sun,Leigh Tesfatsion,,Unknown,,Unknown,Mix,,
30.0,4.0,Computational Economics,25 May 2006,https://link.springer.com/article/10.1007/s10614-005-9012-4,Stochastic Optimization and Worst-Case Analysis in Monetary Policy Design,November 2007,Stan Žaković,Volker Wieland,Berc Rustem,Male,Male,Unknown,Male,,12
30.0,4.0,Computational Economics,07 July 2007,https://link.springer.com/article/10.1007/s10614-007-9093-3,Grid Enabling Empirical Economics: A Microdata Application,November 2007,Simon Peters,Ken Clark,Stephen Pickles,Male,Male,Male,Male,,3
30.0,4.0,Computational Economics,28 July 2007,https://link.springer.com/article/10.1007/s10614-007-9098-y,Higher-Order Properties of the ‘Exchange Rate Dynamics Redux’ Model,November 2007,Jinill Kim,Yun-kwong Kwok,,Unknown,Unknown,Unknown,Unknown,,
30.0,4.0,Computational Economics,23 August 2007,https://link.springer.com/article/10.1007/s10614-007-9099-x,Teaching Computational Economics to Graduate Students,November 2007,David Andrew Kendrick,,,Male,Unknown,Unknown,Male,,2
30.0,4.0,Computational Economics,03 August 2007,https://link.springer.com/article/10.1007/s10614-007-9100-8,The Robustness of the RESET Test to Non-Normal Error Terms,November 2007,Panagiotis Mantalos,Ghazi Shukur,,Male,Male,Unknown,Male,,4
31.0,1.0,Computational Economics,23 August 2007,https://link.springer.com/article/10.1007/s10614-007-9103-5,Optimal Policy Response with Control Parameter and Intercept Covariance,February 2008,Fidel Gonzalez,,,Male,Unknown,Unknown,Male,,4
31.0,1.0,Computational Economics,25 September 2007,https://link.springer.com/article/10.1007/s10614-007-9105-3,Stochastic Ceteris Paribus Simulations,February 2008,Dag Olaf Kolsrud,,,Male,Unknown,Unknown,Male,,1
31.0,1.0,Computational Economics,19 September 2007,https://link.springer.com/article/10.1007/s10614-007-9106-2,Decentralized Allocation of Human Capital and Nonlinear Growth,February 2008,Orlando Gomes,,,Male,Unknown,Unknown,Male,,
31.0,1.0,Computational Economics,19 September 2007,https://link.springer.com/article/10.1007/s10614-007-9107-1,The Performance of Variance Ratio Unit Root Tests Under Nonlinear Stationary TAR and STAR Processes: Evidence from Monte Carlo Simulations and Applications,February 2008,Daiki Maki,,,Male,Unknown,Unknown,Male,,1
31.0,2.0,Computational Economics,14 September 2007,https://link.springer.com/article/10.1007/s10614-007-9108-0,Solving Linear Rational Expectations Models: A Horse Race,March 2008,Gary S. Anderson,,,Male,Unknown,Unknown,Male,,26
31.0,2.0,Computational Economics,08 September 2007,https://link.springer.com/article/10.1007/s10614-007-9110-6,Analysing DSGE Models with Global Sensitivity Analysis,March 2008,Marco Ratto,,,Male,Unknown,Unknown,Male,,30
31.0,2.0,Computational Economics,19 September 2007,https://link.springer.com/article/10.1007/s10614-007-9111-5,Continuous State Dynamic Programming via Nonexpansive Approximation,March 2008,John Stachurski,,,Male,Unknown,Unknown,Male,,17
31.0,2.0,Computational Economics,14 September 2007,https://link.springer.com/article/10.1007/s10614-007-9112-4,A New Approach for Firm Value and Default Probability Estimation beyond Merton Models,March 2008,Maria Elena De Giuli,Dean Fantazzini,Mario Alessandro Maggi,Female,Male,Male,Mix,,
31.0,2.0,Computational Economics,08 September 2007,https://link.springer.com/article/10.1007/s10614-007-9113-3,Numerical Solution of Optimal Control Problems with Constant Control Delays,March 2008,Ulrich Brandt-Pollmann,Ralph Winkler,Johannes P. Schlöder,Male,Male,Male,Male,,17
31.0,3.0,Computational Economics,19 September 2007,https://link.springer.com/article/10.1007/s10614-007-9114-2,Solution Algorithm to a Class of Monetary Rational Equilibrium Macromodels with Optimal Monetary Policy Design,April 2008,Frank Hespeler,,,Male,Unknown,Unknown,Male,,6
31.0,3.0,Computational Economics,28 September 2007,https://link.springer.com/article/10.1007/s10614-007-9115-1,"A Simple Fractionally Integrated Model with a Time-varying Long Memory Parameter d

t",April 2008,Mohamed Boutahar,Gilles Dufrénot,Anne Péguin-Feissolle,Male,Male,Female,Mix,,
31.0,3.0,Computational Economics,26 October 2007,https://link.springer.com/article/10.1007/s10614-007-9116-0,Seasonal Nonlinear Long Memory Model for the US Inflation Rates,April 2008,Ahdi Noomen Ajmi,Adnen Ben Nasr,Mohamed Boutahar,Unknown,Unknown,Male,Male,,6
31.0,3.0,Computational Economics,04 December 2007,https://link.springer.com/article/10.1007/s10614-007-9117-z,Pricing Risky Debts Under a Markov-modudated Merton Model with Completely Random Measures,April 2008,John W. Lau,Tak Kuen Siu,,Male,,Unknown,Mix,,
31.0,3.0,Computational Economics,04 December 2007,https://link.springer.com/article/10.1007/s10614-007-9118-y,The Impact of Interaction and Social Learning on Aggregate Expectations,April 2008,Mark Bowden,Stuart McDonald,,Male,Male,Unknown,Male,,12
31.0,4.0,Computational Economics,04 December 2007,https://link.springer.com/article/10.1007/s10614-007-9119-x,Economics of Reciprocal Networks: Collaboration in Knowledge and Emergence of Industrial Clusters,May 2008,Haruo H. Horaguchi,,,Male,Unknown,Unknown,Male,,2
31.0,4.0,Computational Economics,04 December 2007,https://link.springer.com/article/10.1007/s10614-007-9120-4,Matching Heterogeneous Traders in Quantity-Regulated Markets,May 2008,Yuya Sasaki,Arthur J. Caplan,,Female,Male,Unknown,Mix,,
31.0,4.0,Computational Economics,05 March 2008,https://link.springer.com/article/10.1007/s10614-007-9121-3,Can Consumer Software Selection Code for Digital Cameras Improve Consumer Performance?,May 2008,A. L. Norman,M. Aberty,F. Zahradnic,Unknown,Unknown,Unknown,Unknown,,
31.0,4.0,Computational Economics,22 February 2008,https://link.springer.com/article/10.1007/s10614-008-9122-x,A Pricing Mechanism for Resource Management in Grid Computing,May 2008,Panos Parpas,Berç Rustem,,Male,Unknown,Unknown,Male,,1
31.0,4.0,Computational Economics,12 March 2008,https://link.springer.com/article/10.1007/s10614-008-9123-9,Two Dimensional Aggregation Procedure: An Alternative to the Matrix Algebraic Algorithm,May 2008,Rodolphe Buda,,,Male,Unknown,Unknown,Male,,2
32.0,1.0,Computational Economics,26 April 2008,https://link.springer.com/article/10.1007/s10614-008-9126-6,New Advances in Financial Economics: Heterogeneity and Simulation,September 2008,Silvano Cincotti,Laura Gardini,Thomas Lux,Male,Female,Male,Mix,,
32.0,1.0,Computational Economics,22 April 2008,https://link.springer.com/article/10.1007/s10614-008-9129-3,Asset Price Dynamics When Behavioural Heterogeneity Varies,September 2008,Domenico Colucci,Vincenzo Valori,,Male,Male,Unknown,Male,,4
32.0,1.0,Computational Economics,24 April 2008,https://link.springer.com/article/10.1007/s10614-008-9132-8,Complex Price Dynamics in a Financial Market with Imitation,September 2008,Ilaria Foroni,Anna Agliari,,Female,Female,Unknown,Female,,9
32.0,1.0,Computational Economics,22 April 2008,https://link.springer.com/article/10.1007/s10614-008-9134-6,Modeling and Simulation of an Artificial Stock Option Market,September 2008,Sabrina Ecca,Michele Marchesi,Alessio Setzu,Female,Female,Male,Mix,,
32.0,1.0,Computational Economics,24 April 2008,https://link.springer.com/article/10.1007/s10614-008-9131-9,A Model of Financial Market Dynamics with Heterogeneous Beliefs and State-Dependent Confidence,September 2008,Carl Chiarella,Roberto Dieci,Lucia Sbragia,Male,Male,Female,Mix,,
32.0,1.0,Computational Economics,26 April 2008,https://link.springer.com/article/10.1007/s10614-008-9127-5,"Learning Agents in an Artificial Power Exchange: Tacit Collusion, Market Power and Efficiency of Two Double-auction Mechanisms",September 2008,Eric Guerci,Stefano Ivaldi,Silvano Cincotti,Male,Male,Male,Male,,13
32.0,1.0,Computational Economics,26 April 2008,https://link.springer.com/article/10.1007/s10614-008-9135-5,The Interplay Between Two Stock Markets and a Related Foreign Exchange Market: A Simulation Approach,September 2008,Erika Corona,Sabrina Ecca,Alessio Setzu,Female,Female,Male,Mix,,
32.0,1.0,Computational Economics,15 May 2008,https://link.springer.com/article/10.1007/s10614-008-9128-4,A Statistical Mechanic View of Macro-dynamics in Economics,September 2008,Simone Landini,Mariacristina Uberti,,Female,Unknown,Unknown,Female,,16
32.0,1.0,Computational Economics,30 April 2008,https://link.springer.com/article/10.1007/s10614-008-9138-2,Integrating Real and Financial Markets in an Agent-Based Economic Model: An Application to Monetary Policy Design,September 2008,Marco Raberto,Andrea Teglio,Silvano Cincotti,Male,Female,Male,Mix,,
32.0,1.0,Computational Economics,26 April 2008,https://link.springer.com/article/10.1007/s10614-008-9137-3,Asset Pricing and Productivity Growth: The Role of Consumption Scenarios,September 2008,Volker Böhm,Tomoo Kikuchi,George Vachadze,Male,Female,Male,Mix,,
32.0,1.0,Computational Economics,27 May 2008,https://link.springer.com/article/10.1007/s10614-008-9136-4,Optimal Monetary Policy and Long-term Interest Rate Dynamics: Taylor Rule Extensions,September 2008,Simone Casellina,Mariacristina Uberti,,Female,Unknown,Unknown,Female,,4
32.0,1.0,Computational Economics,26 April 2008,https://link.springer.com/article/10.1007/s10614-008-9133-7,An R&D Investment Game under Uncertainty in Real Option Analysis,September 2008,Giovanni Villani,,,Male,Unknown,Unknown,Male,,21
32.0,1.0,Computational Economics,22 April 2008,https://link.springer.com/article/10.1007/s10614-008-9130-x,E&F Chaos: A User Friendly Software Package for Nonlinear Economic Dynamics,September 2008,Cees Diks,Cars Hommes,Roy van der Weide,Male,Unknown,Male,Male,,38
33.0,1.0,Computational Economics,28 May 2008,https://link.springer.com/article/10.1007/s10614-008-9146-2,Smart Forward Shooting,February 2009,Manoj Atolia,Edward F. Buffie,,Male,Male,Unknown,Male,,13
33.0,1.0,Computational Economics,29 June 2008,https://link.springer.com/article/10.1007/s10614-008-9149-z,Shocks and Endogenous Institutions: An Agent-based Model of Labor Market Performance in Turbulent Times,February 2009,Christian W. Martin,Michael Neugart,,Male,Male,Unknown,Male,,11
33.0,1.0,Computational Economics,06 July 2008,https://link.springer.com/article/10.1007/s10614-008-9150-6,Learning to Collude Tacitly on Production Levels by Oligopolistic Agents,February 2009,Steven O. Kimbrough,Frederic H. Murphy,,Male,Male,Unknown,Male,,17
33.0,1.0,Computational Economics,13 July 2008,https://link.springer.com/article/10.1007/s10614-008-9151-5,The Two-Period Rational Inattention Model: Accelerations and Analyses,February 2009,Kurt F. Lewis,,,Male,Unknown,Unknown,Male,,5
33.0,1.0,Computational Economics,13 July 2008,https://link.springer.com/article/10.1007/s10614-008-9147-1,Robust Evolutionary Algorithm Design for Socio-Economic Simulation: A Correction,February 2009,Floortje Alkemade,Han La Poutré,Hans Amman,Female,,Male,Mix,,
33.0,1.0,Computational Economics,13 June 2008,https://link.springer.com/article/10.1007/s10614-008-9148-0,Robust Evolutionary Algorithm Design for Socio-Economic Simulation: Some Comments,February 2009,Ludo Waltman,Nees Jan van Eck,,Male,Unknown,Unknown,Male,,7
33.0,2.0,Computational Economics,11 September 2008,https://link.springer.com/article/10.1007/s10614-008-9152-4,A Trade Algorithm for Multi-Region Models Subject to Spillover Externalities,March 2009,Marian Leimbach,Klaus Eisenack,,Male,Male,Unknown,Male,,12
33.0,2.0,Computational Economics,07 September 2008,https://link.springer.com/article/10.1007/s10614-008-9153-3,Measuring the Efficiency of the Intraday Forex Market with a Universal Data Compression Algorithm,March 2009,Armin Shmilovici,Yoav Kahiri,Shmuel Hauser,Male,Male,Male,Male,,19
33.0,2.0,Computational Economics,27 September 2008,https://link.springer.com/article/10.1007/s10614-008-9154-2,Local and Global Interactions in an Evolutionary Resource Game,March 2009,Joëlle Noailly,Jeroen C. J. M. van den Bergh,Cees A. Withagen,Female,Male,Male,Mix,,
33.0,2.0,Computational Economics,25 September 2008,https://link.springer.com/article/10.1007/s10614-008-9155-1,"Economic Policy in a Growth Model with Human Capital, Heterogenous Agents and Unemployment",March 2009,Alfred Greiner,Peter Flaschel,,Male,Male,Unknown,Male,,4
33.0,2.0,Computational Economics,25 September 2008,https://link.springer.com/article/10.1007/s10614-008-9156-0,Numerical Solutions to Dynamic Portfolio Problems: The Case for Value Function Iteration using Taylor Approximation,March 2009,Lorenzo Garlappi,Georgios Skoulakis,,Male,Male,Unknown,Male,,19
33.0,3.0,Computational Economics,28 September 2008,https://link.springer.com/article/10.1007/s10614-008-9157-z,Valuation of R&D Sequential Exchange Options Using Monte Carlo Approach,April 2009,Flavia Cortelezzi,Giovanni Villani,,Female,Male,Unknown,Mix,,
33.0,3.0,Computational Economics,05 October 2008,https://link.springer.com/article/10.1007/s10614-008-9158-y,Models and Simulations for Portfolio Rebalancing,April 2009,Gianfranco Guastaroba,Renata Mansini,M. Grazia Speranza,Male,Female,Unknown,Mix,,
33.0,3.0,Computational Economics,19 October 2008,https://link.springer.com/article/10.1007/s10614-008-9159-x,Impacts of Interval Computing on Stock Market Variability Forecasting,April 2009,Ling T. He,Chenyi Hu,,,Unknown,Unknown,Mix,,
33.0,3.0,Computational Economics,17 October 2008,https://link.springer.com/article/10.1007/s10614-008-9160-4,Block Kalman Filtering for Large-Scale DSGE Models,April 2009,Ingvar Strid,Karl Walentin,,Male,Male,Unknown,Male,,19
33.0,4.0,Computational Economics,04 November 2008,https://link.springer.com/article/10.1007/s10614-008-9161-3,Heterogeneous Labour Markets in a Microsimulation–AGE Model: Application to Welfare Reform in Germany,May 2009,Stefan Boeters,Michael Feil,,Male,Male,Unknown,Male,,8
33.0,4.0,Computational Economics,20 November 2008,https://link.springer.com/article/10.1007/s10614-008-9162-2,"The Neutrality of Money Revisited with a Bottom-Up Approach: Decentralisation, Limited Information and Bounded Rationality",May 2009,Gabriel Galand,,,Male,Unknown,Unknown,Male,,1
33.0,4.0,Computational Economics,18 November 2008,https://link.springer.com/article/10.1007/s10614-008-9163-1,Foreign Ownership Restrictions: A Numerical Approach,May 2009,Bilgehan Karabay,Gernot Pulverer,Ewa Weinmüller,,Male,Female,Mix,,
33.0,4.0,Computational Economics,20 December 2008,https://link.springer.com/article/10.1007/s10614-008-9166-y,Solving House Allocation Problems with Risk-Averse Agents,May 2009,Tommy Andersson,Christer Andersson,,Male,Male,Unknown,Male,,1
34.0,1.0,Computational Economics,12 June 2009,https://link.springer.com/article/10.1007/s10614-009-9183-5,Modeling Default Data Via an Interactive Hidden Markov Model,August 2009,Wai-Ki Ching,Tak Kuen Siu,Wai-Keung Li,Unknown,,Unknown,Mix,,
34.0,1.0,Computational Economics,18 April 2009,https://link.springer.com/article/10.1007/s10614-009-9177-3,Network Topology and Locational Market Power,August 2009,Jiangzhuo Chen,Matthew Macauley,Achla Marathe,Unknown,Male,Unknown,Male,,4
34.0,1.0,Computational Economics,12 March 2009,https://link.springer.com/article/10.1007/s10614-009-9170-x,Simulation Analysis Using Multi-Agent Systems for Social Norms,August 2009,Ichiro Nishizaki,Hideki Katagiri,Toshihisa Oyama,Male,Male,Unknown,Male,,7
34.0,1.0,Computational Economics,19 December 2008,https://link.springer.com/article/10.1007/s10614-008-9165-z,Multiagent System Simulations of Treasury Auctions,August 2009,Alan Mehlenbacher,,,Male,Unknown,Unknown,Male,,1
34.0,2.0,Computational Economics,24 January 2009,https://link.springer.com/article/10.1007/s10614-009-9167-5,Multiagent System Simulations of Signal Averaging in English Auctions with Two-Dimensional Value Signals,September 2009,Alan Mehlenbacher,,,Male,Unknown,Unknown,Male,,
34.0,2.0,Computational Economics,31 January 2009,https://link.springer.com/article/10.1007/s10614-009-9168-4,Which Econometric Specification to Characterize the U.S. Inflation Rate Process?,September 2009,Mohamed Boutahar,David Gbaguidi,,Male,Male,Unknown,Male,,1
34.0,2.0,Computational Economics,05 April 2009,https://link.springer.com/article/10.1007/s10614-009-9176-4,Predicting EU Energy Industry Excess Returns on EU Market Index via a Constrained Genetic Algorithm,September 2009,Massimiliano Kaucic,,,Male,Unknown,Unknown,Male,,3
34.0,2.0,Computational Economics,23 April 2009,https://link.springer.com/article/10.1007/s10614-009-9178-2,Structural Change and Long Memory in the Dynamic of U.S. Inflation Process,September 2009,Mustapha Belkhouja,Mohamed Boutahar,,Male,Male,Unknown,Male,,6
34.0,3.0,Computational Economics,06 December 2008,https://link.springer.com/article/10.1007/s10614-008-9164-0,"Simulating Sequential Search Models with Genetic Algorithms: Analysis of Price Ceilings, Taxes, Advertising and Welfare",October 2009,Ian McCarthy,,,Male,Unknown,Unknown,Male,,1
34.0,3.0,Computational Economics,17 February 2009,https://link.springer.com/article/10.1007/s10614-009-9169-3,Auctions and Differential Pricing: Optimal Seller and Bidder Strategies in Second-chance Offers,October 2009,Yu-An Sun,Poorvi Vora,,Unknown,Unknown,Unknown,Unknown,,
34.0,3.0,Computational Economics,24 March 2009,https://link.springer.com/article/10.1007/s10614-009-9174-6,Reverse Shooting Made Easy: Automating the Search for the Global Nonlinear Saddle Path,October 2009,Manoj Atolia,Edward F. Buffie,,Male,Male,Unknown,Male,,14
34.0,3.0,Computational Economics,02 April 2009,https://link.springer.com/article/10.1007/s10614-009-9175-5,On Deregulating Food Prices,October 2009,S. D. Flåm,I. Gaasland,E. Vårdal,Unknown,Unknown,Unknown,Unknown,,
34.0,4.0,Computational Economics,23 May 2009,https://link.springer.com/article/10.1007/s10614-009-9181-7,Optimal Prediction with Conditionally Heteroskedastic Factor Analysed Hidden Markov Models,November 2009,Mohamed Saidane,Christian Lavergne,,Male,Male,Unknown,Male,,3
34.0,4.0,Computational Economics,28 May 2009,https://link.springer.com/article/10.1007/s10614-009-9180-8,Tests of Random Walk: A Comparison of Bootstrap Approaches,November 2009,Eduardo J. A. Lima,Benjamin M. Tabak,,Male,Male,Unknown,Male,,2
34.0,4.0,Computational Economics,18 March 2009,https://link.springer.com/article/10.1007/s10614-009-9173-7,Network Formation in the Political Blogosphere: An Application of Agent Based Simulation and e-Research Tools,November 2009,Robert Ackland,Jamsheed Shorish,,Male,Unknown,Unknown,Male,,13
34.0,4.0,Computational Economics,20 March 2009,https://link.springer.com/article/10.1007/s10614-009-9171-9,Particle Swarm Optimization Algorithm for Agent-Based Artificial Markets,November 2009,Tong Zhang,B. Wade Brorsen,,,Unknown,Unknown,Mix,,
35.0,1.0,Computational Economics,03 October 2009,https://link.springer.com/article/10.1007/s10614-009-9186-2,"Nonlinear Bivariate Comovements of Asset Prices: Methodology, Tests and Applications",January 2010,Marco Corazza,A. G. Malliaris,Elisa Scalco,Male,Unknown,Female,Mix,,
35.0,1.0,Computational Economics,25 October 2009,https://link.springer.com/article/10.1007/s10614-009-9190-6,Intelligent Mutation Rate Control in an Economic Application of Genetic Algorithms,January 2010,Michael Kurtis Maschek,,,Male,Unknown,Unknown,Male,,11
35.0,1.0,Computational Economics,07 November 2009,https://link.springer.com/article/10.1007/s10614-009-9191-5,Dynamic Innovation Diffusion Modelling,January 2010,Kazunori Shinohara,Hiroshi Okuda,,Male,Male,Unknown,Male,,11
35.0,1.0,Computational Economics,10 May 2009,https://link.springer.com/article/10.1007/s10614-009-9179-1,A Student-t Full Factor Multivariate GARCH Model,January 2010,K. Diamantopoulos,I. D. Vrontos,,Unknown,Unknown,Unknown,Unknown,,
35.0,1.0,Computational Economics,18 September 2009,https://link.springer.com/article/10.1007/s10614-009-9187-1,Dynamics and Structure of the 30 Largest North American Companies,January 2010,Juan Gabriel Brida,Wiston Adrián Risso,,Male,Unknown,Unknown,Male,,35
35.0,2.0,Computational Economics,24 September 2009,https://link.springer.com/article/10.1007/s10614-009-9185-3,Cognitive Bias in the Laboratory Security Market,February 2010,Hidetoshi Yamaji,Masatoshi Gotoh,,Male,Male,Unknown,Male,,1
35.0,2.0,Computational Economics,28 May 2009,https://link.springer.com/article/10.1007/s10614-009-9182-6,How to Maximize the Likelihood Function for a DSGE Model,February 2010,Martin Møller Andreasen,,,Male,Unknown,Unknown,Male,,26
35.0,2.0,Computational Economics,24 September 2009,https://link.springer.com/article/10.1007/s10614-009-9188-0,Modeling Emotions and Reason in Agent-Based Systems,February 2010,Fernando S. Oliveira,,,Male,Unknown,Unknown,Male,,2
35.0,2.0,Computational Economics,29 September 2009,https://link.springer.com/article/10.1007/s10614-009-9189-z,A Dynamic Stochastic Model of Asset Pricing with Heterogeneous Beliefs,February 2010,Serena Brianzoni,Roy Cerqueti,Elisabetta Michetti,Female,Male,Female,Mix,,
35.0,3.0,Computational Economics,04 February 2010,https://link.springer.com/article/10.1007/s10614-010-9199-x,What Format for Multi-Unit Multiple-Bid Auctions?,March 2010,Atakelty Hailu,Sophie Thoyer,,Unknown,Female,Unknown,Female,,18
35.0,3.0,Computational Economics,20 June 2009,https://link.springer.com/article/10.1007/s10614-009-9184-4,Endogenous Neighborhood Selection and the Attainment of Cooperation in a Spatial Prisoner’s Dilemma Game,March 2010,Jason Barr,Troy Tassier,,Male,Male,Unknown,Male,,5
35.0,3.0,Computational Economics,14 March 2009,https://link.springer.com/article/10.1007/s10614-009-9172-8,Using Chebyshev Polynomials to Approximate Partial Differential Equations,March 2010,Guglielmo Maria Caporale,Mario Cerrato,,Male,Male,Unknown,Male,,10
35.0,3.0,Computational Economics,19 December 2009,https://link.springer.com/article/10.1007/s10614-009-9194-2,"Committee, Expert Advice, and the Weighted Majority Algorithm: An Application to the Pricing Decision of a Monopolist",March 2010,Yann Braouezec,,,Male,Unknown,Unknown,Male,,2
35.0,3.0,Computational Economics,05 January 2010,https://link.springer.com/article/10.1007/s10614-009-9193-3,A Framework to Determine the Value of Consumer Consideration Set Information for Firm Pricing Strategies,March 2010,Joseph Pancras,,,Male,Unknown,Unknown,Male,,11
35.0,4.0,Computational Economics,12 February 2010,https://link.springer.com/article/10.1007/s10614-010-9200-8,A Benders Decomposition Method for Solving Stochastic Complementarity Problems with an Application in Energy,April 2010,S. A. Gabriel,J. D. Fuller,,Unknown,Unknown,Unknown,Unknown,,
35.0,4.0,Computational Economics,11 February 2010,https://link.springer.com/article/10.1007/s10614-010-9201-7,A “Nearly Ideal” Solution to Linear Time-Varying Rational Expectations Models,April 2010,Francesco Carravetta,Marco M. Sorge,,Male,Male,Unknown,Male,,10
35.0,4.0,Computational Economics,18 February 2010,https://link.springer.com/article/10.1007/s10614-010-9203-5,Finite Elements in the Presence of Occasionally Binding Constraints,April 2010,José J. Cao-Alvira,,,Male,Unknown,Unknown,Male,,3
35.0,4.0,Computational Economics,05 March 2010,https://link.springer.com/article/10.1007/s10614-010-9204-4,Should Economists Use Open Source Software for Doing Research?,April 2010,A. Talha Yalta,A. Yasemin Yalta,,Unknown,Unknown,Unknown,Unknown,,
35.0,4.0,Computational Economics,18 November 2009,https://link.springer.com/article/10.1007/s10614-009-9192-4,Searching NK Fitness Landscapes: On the Trade Off Between Speed and Quality in Complex Problem Solving,April 2010,Sylvie Geisendorf,,,Female,Unknown,Unknown,Female,,10
36.0,1.0,Computational Economics,19 March 2010,https://link.springer.com/article/10.1007/s10614-010-9208-0,Maximum Likelihood Estimation of the Cox–Ingersoll–Ross Model Using Particle Filters,June 2010,Giuliano De Rossi,,,Male,Unknown,Unknown,Male,,19
36.0,1.0,Computational Economics,09 April 2010,https://link.springer.com/article/10.1007/s10614-010-9202-6,Causal Inference for Structural Equations: With an Application to Wage-Price Spiral,June 2010,Pu Chen,Chih-Ying Hsiao,,,Unknown,Unknown,Mix,,
36.0,1.0,Computational Economics,27 April 2010,https://link.springer.com/article/10.1007/s10614-010-9211-5,Optimal Deterministic and Stochastic Macroeconomic Policies for Slovenia: An Application of the OPTCON Algorithm,June 2010,Reinhard Neck,Gottfried Haber,Klaus Weyerstrass,Male,Male,Male,Male,,
36.0,1.0,Computational Economics,14 March 2010,https://link.springer.com/article/10.1007/s10614-010-9207-1,A Dynamic Model of a Boundedly Rational Consumer with a Simple Least Squared Learning Mechanism,June 2010,Ahmad K. Naimzada,Fabio Tramontana,,Male,Male,Unknown,Male,,6
36.0,1.0,Computational Economics,24 April 2010,https://link.springer.com/article/10.1007/s10614-010-9210-6,Assessing the Quality of Pseudo-Random Number Generators,June 2010,P. C. S. Luizi,F. R. B. Cruz,J. van de Graaf,Unknown,Unknown,Unknown,Unknown,,
36.0,1.0,Computational Economics,04 February 2010,https://link.springer.com/article/10.1007/s10614-010-9197-z,International Environmental Agreements with Asymmetric Countries,June 2010,Marta Biancardi,Giovanni Villani,,Female,Male,Unknown,Mix,,
36.0,2.0,Computational Economics,20 June 2010,https://link.springer.com/article/10.1007/s10614-010-9232-0,The Case of two Self-Enforcing International Agreements for Environmental Protection with Asymmetric Countries,August 2010,Dritan Osmani,Richard S. J. Tol,,Male,Male,Unknown,Male,,30
36.0,2.0,Computational Economics,03 June 2010,https://link.springer.com/article/10.1007/s10614-010-9225-z,An Out-of-Sample Test for Nonlinearity in Financial Time Series: An Empirical Application,August 2010,Theodore Panagiotidis,,,Male,Unknown,Unknown,Male,,
36.0,2.0,Computational Economics,05 May 2010,https://link.springer.com/article/10.1007/s10614-010-9205-3,"Using Boosting for Financial Analysis and Performance Prediction: Application to S&P 500 Companies, Latin American ADRs and Banks",August 2010,Germán Creamer,Yoav Freund,,Male,Male,Unknown,Male,,16
36.0,2.0,Computational Economics,19 June 2010,https://link.springer.com/article/10.1007/s10614-010-9226-y,Partially Adaptive Econometric Methods For Regression and Classification,August 2010,James V. Hansen,James B. McDonald,Brad J. Larsen,Male,Male,Male,Male,,9
36.0,2.0,Computational Economics,10 June 2010,https://link.springer.com/article/10.1007/s10614-010-9229-8,Computation of Equilibria in OLG Models with Many Heterogeneous Households,August 2010,Sebastian Rausch,Thomas F. Rutherford,,Male,Male,Unknown,Male,,9
36.0,3.0,Computational Economics,27 May 2010,https://link.springer.com/article/10.1007/s10614-010-9220-4,Possibilistic Approaches to Portfolio Selection Problem with General Transaction Costs and a CLPSO Algorithm,October 2010,Xi-li Zhang,Wei-Guo Zhang,Wei-Lin Xiao,Unknown,,,Mix,,
36.0,3.0,Computational Economics,17 June 2010,https://link.springer.com/article/10.1007/s10614-010-9227-x,Human and Artificial Agents in a Crash-Prone Financial Market,October 2010,Todd Feldman,Daniel Friedman,,Male,Male,Unknown,Male,,1
36.0,3.0,Computational Economics,21 May 2010,https://link.springer.com/article/10.1007/s10614-010-9216-0,"Numerical Solutions of Asymmetric, First-Price, Independent Private Values Auctions: Comment",October 2010,Junwei Peng,Zhongzhi Yang,,Unknown,Unknown,Unknown,Unknown,,
36.0,3.0,Computational Economics,01 June 2010,https://link.springer.com/article/10.1007/s10614-010-9218-y,Modeling of Asymmetry between Gasoline and Crude Oil Prices: A Monte Carlo Comparison,October 2010,Afshin Honarvar,,,Male,Unknown,Unknown,Male,,4
36.0,3.0,Computational Economics,05 May 2010,https://link.springer.com/article/10.1007/s10614-010-9214-2,Is Price Behavior Scaling and Multiscaling in a Dealer Market? Perspectives from Multi-Agent Based Experiments,October 2010,Ling-Yun He,,,,Unknown,Unknown,Mix,,
36.0,4.0,Computational Economics,05 October 2010,https://link.springer.com/article/10.1007/s10614-010-9240-0,The Role of Additional Information in Option Pricing: Estimation Issues for the State Space Model,December 2010,Ren-Her Wang,John A. D. Aston,Cheng-Der Fuh,Unknown,Male,Unknown,Male,,
36.0,4.0,Computational Economics,14 May 2010,https://link.springer.com/article/10.1007/s10614-010-9215-1,Imposing Curvature and Monotonicity on Flexible Functional Forms: An Efficient Regional Approach,December 2010,Hendrik Wolff,Thomas Heckelei,Ron C. Mittelhammer,Male,Male,Male,Male,,15
36.0,4.0,Computational Economics,02 June 2010,https://link.springer.com/article/10.1007/s10614-010-9221-3,A Technique for Gradual Identification of Labor Market Flows,December 2010,Nissim Ben David,,,Male,Unknown,Unknown,Male,,1
36.0,4.0,Computational Economics,15 July 2010,https://link.springer.com/article/10.1007/s10614-010-9235-x,A New Approach to Unit Root Testing,December 2010,Helmut Herwartz,Florian Siedenburg,,Male,Male,Unknown,Male,,
37.0,1.0,Computational Economics,27 October 2010,https://link.springer.com/article/10.1007/s10614-010-9244-9,Computability of Digital Input Output Models,January 2011,J. R. Guzmán,,,Unknown,Unknown,Unknown,Unknown,,
37.0,1.0,Computational Economics,12 May 2010,https://link.springer.com/article/10.1007/s10614-010-9213-3,Optimization in Non-Standard Problems. An Application to the Provision of Public Inputs,January 2011,A. Jesus Sanchez,Diego Martinez,,Unknown,Male,Unknown,Male,,2
37.0,1.0,Computational Economics,28 January 2010,https://link.springer.com/article/10.1007/s10614-010-9198-y,The Role of Central Bank Operating Procedures in an Economy with Productive Government Spending,January 2011,Jordi Caballé,Jana Hromcová,,Male,Female,Unknown,Mix,,
37.0,1.0,Computational Economics,19 March 2010,https://link.springer.com/article/10.1007/s10614-010-9206-2,New Procedures for Testing Whether Stock Price Processes are Martingales,January 2011,Kei Takeuchi,Akimichi Takemura,Masayuki Kumon,,Unknown,Male,Mix,,
37.0,1.0,Computational Economics,17 September 2010,https://link.springer.com/article/10.1007/s10614-010-9239-6,"Equilibrium Information Acquisition, Prediction Abilities and Asset Prices",January 2011,Wen-Chung Guo,Sy-Ming Guu,Ting-Yun Chang,Unknown,Unknown,Unknown,Unknown,,
37.0,2.0,Computational Economics,10 January 2010,https://link.springer.com/article/10.1007/s10614-009-9195-1,Mean-VaR Portfolio Selection Under Real Constraints,February 2011,J. Samuel Baixauli-Soler,Eva Alfaro-Cid,Matilde O. Fernandez-Blanco,Unknown,Female,Female,Female,,25
37.0,2.0,Computational Economics,21 January 2010,https://link.springer.com/article/10.1007/s10614-009-9196-0,Eliciting Preferences on Multiattribute Societies with a Choquet Integral,February 2011,Patrick Meyer,Grégory Ponthière,,Male,Male,Unknown,Male,,20
37.0,2.0,Computational Economics,17 June 2010,https://link.springer.com/article/10.1007/s10614-010-9230-2,Different Approaches to Forecast Interval Time Series: A Comparison in Finance,February 2011,Javier Arroyo,Rosa Espínola,Carlos Maté,,Female,Male,Mix,,
37.0,2.0,Computational Economics,16 June 2010,https://link.springer.com/article/10.1007/s10614-010-9233-z,Fluctuations in Economic and Activity and Stabilization Policies in the CIS,February 2011,Khurshid M. Kiani,,,Male,Unknown,Unknown,Male,,5
37.0,3.0,Computational Economics,07 January 2011,https://link.springer.com/article/10.1007/s10614-010-9251-x,Bifurcation in Perturbation Analysis:Calvo Pricing Examples,March 2011,Jinill Kim,Andrew T. Levin,Tack Yun,Unknown,Male,Unknown,Male,,
37.0,3.0,Computational Economics,19 January 2011,https://link.springer.com/article/10.1007/s10614-011-9252-4,An Efficient Stochastic Simulation Algorithm for Bayesian Unit Root Testing in Stochastic Volatility Models,March 2011,Yong Li,Zhongxin Ni,Jie Zhang,,Unknown,,Mix,,
37.0,3.0,Computational Economics,11 November 2010,https://link.springer.com/article/10.1007/s10614-010-9246-7,Does Social Welfare Preference Always Promote Cooperation on Barabási and Albert Networks?,March 2011,Bo Xianyu,Ping Chen,,Male,,Unknown,Mix,,
37.0,3.0,Computational Economics,03 February 2011,https://link.springer.com/article/10.1007/s10614-011-9253-3,A Class of Evolutionary Models for Participation Games with Negative Feedback,March 2011,Pietro Dindo,Jan Tuinstra,,Male,Male,Unknown,Male,,16
37.0,3.0,Computational Economics,13 January 2011,https://link.springer.com/article/10.1007/s10614-011-9254-2,Volatility Modeling by Asymmetrical Quadratic Effect with Diminishing Marginal Impact,March 2011,Alex YiHou Huang,,,Male,Unknown,Unknown,Male,,6
37.0,4.0,Computational Economics,12 February 2011,https://link.springer.com/article/10.1007/s10614-011-9259-x,The Performance of German Firms in the Business-Related Service Sectors Revisited: Differential Evolution Markov Chain Estimation of the Multinomial Probit Model,April 2011,W. Erno Kuiper,Anton J. Cozijnsen,,Unknown,Male,Unknown,Male,,1
37.0,4.0,Computational Economics,01 March 2011,https://link.springer.com/article/10.1007/s10614-011-9260-4,An Investigation into the Use of Intelligent Systems for Currency Trading,April 2011,Hannah Thinyane,Jonathan Millin,,Female,Male,Unknown,Mix,,
37.0,4.0,Computational Economics,12 February 2011,https://link.springer.com/article/10.1007/s10614-011-9257-z,A Numerical Toolbox to Solve N-Player Affine LQ Open-Loop Differential Games,April 2011,Tomasz Michalak,Jacob Engwerda,Joseph Plasmans,Male,Male,Male,Male,,7
37.0,4.0,Computational Economics,28 October 2010,https://link.springer.com/article/10.1007/s10614-010-9245-8,The Clock Proxy Auction for Allocating Radio Spectrum Licenses,April 2011,A. Mochon,Y. Saez,P. Isasi,Unknown,Unknown,Unknown,Unknown,,
38.0,1.0,Computational Economics,15 July 2010,https://link.springer.com/article/10.1007/s10614-010-9234-y,Linearization and Higher-Order Approximations: How Good are They?,June 2011,Manoj Atolia,Bassam Awad,Milton Marquis,Male,Male,Male,Male,,6
38.0,1.0,Computational Economics,27 April 2010,https://link.springer.com/article/10.1007/s10614-010-9212-4,Second-Order Sensitivity in Applied General Equilibrium,June 2011,Florian Landis,,,Male,Unknown,Unknown,Male,,3
38.0,1.0,Computational Economics,19 August 2010,https://link.springer.com/article/10.1007/s10614-010-9238-7,Estimation of a Structural Stochastic Volatility Model of Asset Pricing,June 2011,Reiner Franke,Frank Westerhoff,,Male,Male,Unknown,Male,,50
38.0,1.0,Computational Economics,01 July 2010,https://link.springer.com/article/10.1007/s10614-010-9236-9,"Graphical Methods, Inductive Causal Inference, and Econometrics: A Literature Review",June 2011,Dae-Heum Kwon,David A. Bessler,,Unknown,Male,Unknown,Male,,15
38.0,2.0,Computational Economics,01 April 2011,https://link.springer.com/article/10.1007/s10614-011-9265-z,Approximation Errors of Perturbation Methods in Solving a Class of Dynamic Stochastic General Equilibrium Models,August 2011,Xuan Liu,Zhiwei Cui,,,Unknown,Unknown,Mix,,
38.0,2.0,Computational Economics,03 February 2011,https://link.springer.com/article/10.1007/s10614-011-9255-1,The Incompleteness Problem of the APT Model,August 2011,Peter S. Karlsson,,,Male,Unknown,Unknown,Male,,
38.0,2.0,Computational Economics,25 May 2010,https://link.springer.com/article/10.1007/s10614-010-9224-0,The Size and Power of Bootstrap Tests for Spatial Dependence in a Linear Regression Model,August 2011,Kuan-Pin Lin,Zhi-He Long,Bianling Ou,Unknown,,Unknown,Mix,,
38.0,2.0,Computational Economics,10 June 2010,https://link.springer.com/article/10.1007/s10614-010-9228-9,Heuristics for Deciding Collectively Rational Consumption Behavior,August 2011,Fabrice Talla Nobibon,Laurens Cherchye,Frits C. R. Spieksma,Male,Male,Male,Male,,12
38.0,2.0,Computational Economics,20 May 2011,https://link.springer.com/article/10.1007/s10614-011-9267-x,Erratum to: An Investigation into the Use of Intelligent Systems for Currency Trading,August 2011,Hannah Thinyane,Jonathan Millin,,Female,Male,Unknown,Mix,,
38.0,3.0,Computational Economics,18 August 2011,https://link.springer.com/article/10.1007/s10614-011-9287-6,Foreword to the Special Issue of Computational Economics on Complex Dynamics in Economics and Finance,October 2011,Gian Italo Bischi,Carl Chiarella,Laura Gardini,Male,Male,Female,Mix,,
38.0,3.0,Computational Economics,09 September 2011,https://link.springer.com/article/10.1007/s10614-011-9277-8,"Aggregate Demand, Harrod’s Instability and Fluctuations",October 2011,Piero Ferri,Steve Fazzari,Anna Maria Variato,Male,Male,Female,Mix,,
38.0,3.0,Computational Economics,31 August 2011,https://link.springer.com/article/10.1007/s10614-011-9279-6,Credit market dynamics: a cobweb model,October 2011,S. Casellina,S. Landini,M. Uberti,Unknown,Unknown,Unknown,Unknown,,
38.0,3.0,Computational Economics,19 August 2011,https://link.springer.com/article/10.1007/s10614-011-9282-y,Homoclinic and Heteroclinic Bifurcations in an Overlapping Generations Model with Credit Market Imperfection,October 2011,Anna Agliari,George Vachadze,,Female,Male,Unknown,Mix,,
38.0,3.0,Computational Economics,17 August 2011,https://link.springer.com/article/10.1007/s10614-011-9283-x,Nonlinear Dynamics in an OLG Growth Model with Young and Old Age Labour Supply: The Role of Public Health Expenditure,October 2011,Luca Gori,Mauro Sodini,,Male,Male,Unknown,Male,,11
38.0,3.0,Computational Economics,09 September 2011,https://link.springer.com/article/10.1007/s10614-011-9286-7,Local and Global Dynamics in an Overlapping Generations Model with Endogenous Time Discounting,October 2011,Mauro Sodini,,,Male,Unknown,Unknown,Male,,2
38.0,3.0,Computational Economics,28 August 2011,https://link.springer.com/article/10.1007/s10614-011-9280-0,A Nonlinear Duopoly with Efficient Production-Capacity Levels,October 2011,Fabio Lamantia,,,Male,Unknown,Unknown,Male,,11
38.0,3.0,Computational Economics,06 September 2011,https://link.springer.com/article/10.1007/s10614-011-9295-6,Dynamics in Linear Cournot Duopolies with Two Time Delays,October 2011,Akio Matsumoto,Ferenc Szidarovszky,Hiroyuki Yoshida,Male,Male,Male,Male,,13
38.0,3.0,Computational Economics,11 August 2011,https://link.springer.com/article/10.1007/s10614-011-9284-9,Heterogeneous Speculators and Asset Price Dynamics: Further Results from a One-Dimensional Discontinuous Piecewise-Linear Map,October 2011,Fabio Tramontana,Laura Gardini,Frank Westerhoff,Male,Female,Male,Mix,,
38.0,3.0,Computational Economics,28 August 2011,https://link.springer.com/article/10.1007/s10614-011-9293-8,Border Collision Bifurcations in a Footloose Capital Model with First Nature Firms,October 2011,Anna Agliari,Pasquale Commendatore,Ingrid Kubin,Female,Male,Female,Mix,,
38.0,3.0,Computational Economics,09 September 2011,https://link.springer.com/article/10.1007/s10614-011-9278-7,Transition Dynamics in Endogenous Recombinant Growth Models by Means of Projection Methods,October 2011,Fabio Privileggi,,,Male,Unknown,Unknown,Male,,4
38.0,3.0,Computational Economics,30 August 2011,https://link.springer.com/article/10.1007/s10614-011-9294-7,Financial Tools for the Abatement of Traffic Congestion: A Dynamical Analysis,October 2011,Angelo Antoci,Marcello Galeotti,Davide Radi,Male,Male,Male,Male,,6
38.0,3.0,Computational Economics,12 August 2011,https://link.springer.com/article/10.1007/s10614-011-9285-8,Largest Consistent Set in International Environmental Agreements,October 2011,Marta Biancardi,Giovanni Villani,,Female,Male,Unknown,Mix,,
38.0,3.0,Computational Economics,11 August 2011,https://link.springer.com/article/10.1007/s10614-011-9281-z,Can Endogenous Participation Explain Price Volatility? Evidence from an Agent-Based Cobweb Model,October 2011,Domenico Colucci,Vincenzo Valori,,Male,Male,Unknown,Male,,1
38.0,4.0,Computational Economics,03 November 2010,https://link.springer.com/article/10.1007/s10614-010-9242-y,Piecewise Pseudo-Maximum Likelihood Estimation for Risk Aversion Case in First-Price Sealed-Bid Auction,November 2011,Xin An,Shulin Liu,Shuo Xu,,Unknown,,Mix,,
38.0,4.0,Computational Economics,25 May 2010,https://link.springer.com/article/10.1007/s10614-010-9219-x,Discrete Time Non-Homogeneous Semi-Markov Reliability Transition Credit Risk Models and the Default Distribution Functions,November 2011,Guglielmo D’Amico,Jacques Janssen,Raimondo Manca,Male,Male,Male,Male,,14
38.0,4.0,Computational Economics,26 May 2010,https://link.springer.com/article/10.1007/s10614-010-9217-z,"A Computationally Efficient, Consistent Bootstrap for Inference with Non-parametric DEA Estimators",November 2011,Alois Kneip,Léopold Simar,Paul W. Wilson,Male,Male,Male,Male,,43
38.0,4.0,Computational Economics,05 July 2011,https://link.springer.com/article/10.1007/s10614-011-9274-y,A Long Memory Model with Normal Mixture GARCH,November 2011,Yin-Wong Cheung,Sang-Kuck Chung,,Unknown,Unknown,Unknown,Unknown,,
39.0,1.0,Computational Economics,23 March 2010,https://link.springer.com/article/10.1007/s10614-010-9209-z,An Integer Programming Model for Pricing American Contingent Claims under Transaction Costs,January 2012,M. Ç. Pınar,A. Camcı,,Unknown,Unknown,Unknown,Unknown,,
39.0,1.0,Computational Economics,25 May 2010,https://link.springer.com/article/10.1007/s10614-010-9222-2,Using Chebyshev Polynomials to Approximate Partial Differential Equations: A Reply,January 2012,Alejandro Mosiño,,,Male,Unknown,Unknown,Male,,2
39.0,1.0,Computational Economics,03 June 2010,https://link.springer.com/article/10.1007/s10614-010-9223-1,Fuzzy Statistical Analysis of Multiple Regression with Crisp and Fuzzy Covariates and Applications in Analyzing Economic Data of China,January 2012,Jin-Guan Lin,Qing-Yun Zhuang,Chao Huang,Unknown,,,Mix,,
39.0,1.0,Computational Economics,11 June 2010,https://link.springer.com/article/10.1007/s10614-010-9231-1,BRA: An Algorithm for Simulating Bounded Rational Agents,January 2012,Stephan Schuster,,,Male,Unknown,Unknown,Male,,4
39.0,1.0,Computational Economics,02 November 2011,https://link.springer.com/article/10.1007/s10614-011-9298-3,Introduction to the Works of Rodney C. Wingrove: Engineering Approaches to Macroeconomic Modeling,January 2012,Ronald E. Davis,Dallas G. Denery,Raman K. Mehra,Male,,Male,Mix,,
39.0,1.0,Computational Economics,02 November 2011,https://link.springer.com/article/10.1007/s10614-011-9299-2,Classical Linear-Control Analysis Applied to Business-Cycle Dynamics and Stability,January 2012,Rodney C. Wingrove,Ronald E. Davis,,Male,Male,Unknown,Male,,4
39.0,1.0,Computational Economics,02 November 2011,https://link.springer.com/article/10.1007/s10614-011-9301-z,Manual-Control Analysis Applied to the Money Supply Control Task,January 2012,Rodney C. Wingrove,Ronald E. Davis,,Male,Male,Unknown,Male,,
39.0,2.0,Computational Economics,07 August 2010,https://link.springer.com/article/10.1007/s10614-010-9237-8,Properties of the DGS-Auction Algorithm,February 2012,Tommy Andersson,Christer Andersson,,Male,Male,Unknown,Male,,5
39.0,2.0,Computational Economics,02 February 2011,https://link.springer.com/article/10.1007/s10614-011-9258-y,A Flexible Markov Chain Approach for Multivariate Credit Ratings,February 2012,Eric S. Fung,Tak Kuen Siu,,Male,,Unknown,Mix,,
39.0,2.0,Computational Economics,04 March 2011,https://link.springer.com/article/10.1007/s10614-011-9261-3,Modelling the Evolution of National Economies Based on Input–Output Networks,February 2012,Wen-Qi Duan,,,,Unknown,Unknown,Mix,,
39.0,2.0,Computational Economics,18 September 2010,https://link.springer.com/article/10.1007/s10614-010-9241-z,Opinions and Networks: How Do They Effect Each Other,February 2012,Zhengzheng Pan,,,Unknown,Unknown,Unknown,Unknown,,
39.0,2.0,Computational Economics,26 January 2011,https://link.springer.com/article/10.1007/s10614-011-9256-0,Statistical Inferences for Generalized Pareto Distribution Based on Interior Penalty Function Algorithm and Bootstrap Methods and Applications in Analyzing Stock Data,February 2012,Chao Huang,Jin-Guan Lin,Yan-Yan Ren,,Unknown,,Mix,,
39.0,2.0,Computational Economics,11 August 2011,https://link.springer.com/article/10.1007/s10614-011-9289-4,Consumption Utility-Based Pricing and Timing of the Option to Invest with Partial Information,February 2012,Jinqiang Yang,Zhaojun Yang,,Unknown,Unknown,Unknown,Unknown,,
39.0,3.0,Computational Economics,05 January 2011,https://link.springer.com/article/10.1007/s10614-010-9248-5,Propagation of Data Error and Parametric Sensitivity in Computable General Equilibrium Models,March 2012,Joshua Elliott,Meredith Franklin,Margaret Loudermilk,Male,,Female,Mix,,
39.0,3.0,Computational Economics,23 July 2011,https://link.springer.com/article/10.1007/s10614-011-9276-9,Repeated Price Search,March 2012,A. Norman,J. Berman,T. Zeinullayev,Unknown,Unknown,Unknown,Unknown,,
39.0,3.0,Computational Economics,08 December 2010,https://link.springer.com/article/10.1007/s10614-010-9247-6,Two-State Volatility Transition Pricing and Hedging of TXO Options,March 2012,En-Der Su,Feng-Jeng Lin,,Unknown,Unknown,Unknown,Unknown,,
39.0,3.0,Computational Economics,05 July 2011,https://link.springer.com/article/10.1007/s10614-011-9273-z,Valuation of N-stage Investments Under Jump-Diffusion Processes,March 2012,Rainer Andergassen,Luigi Sereno,,Male,Male,Unknown,Male,,8
39.0,3.0,Computational Economics,25 December 2011,https://link.springer.com/article/10.1007/s10614-011-9310-y,What Drives Short Rate Dynamics? A Functional Gradient Descent Approach,March 2012,Francesco Audrino,,,Male,Unknown,Unknown,Male,,2
39.0,4.0,Computational Economics,03 November 2010,https://link.springer.com/article/10.1007/s10614-010-9243-x,Heuristic Optimization Methods for Dynamic Panel Data Model Selection: Application on the Russian Innovative Performance,April 2012,Ivan Savin,Peter Winker,,Male,Male,Unknown,Male,,9
39.0,4.0,Computational Economics,11 January 2011,https://link.springer.com/article/10.1007/s10614-010-9249-4,A Closed-Form Solution to Stollery’s Problem with Damage in Utility,April 2012,Andrei V. Bazhanov,,,Male,Unknown,Unknown,Male,,6
39.0,4.0,Computational Economics,25 December 2010,https://link.springer.com/article/10.1007/s10614-010-9250-y,Transitional Dynamics in Sticky-Information General Equilibrium Models,April 2012,Orlando Gomes,,,Male,Unknown,Unknown,Male,,3
39.0,4.0,Computational Economics,25 March 2011,https://link.springer.com/article/10.1007/s10614-011-9262-2,Exploring US Business Cycles with Bivariate Loops Using Penalized Spline Regression,April 2012,Göran Kauermann,Timo Teuber,Peter Flaschel,Male,Male,Male,Male,,5
39.0,4.0,Computational Economics,25 March 2011,https://link.springer.com/article/10.1007/s10614-011-9263-1,A Numerical Method for Solving Stochastic Optimal Control Problems with Linear Control,April 2012,Walailuck Chavanasporn,Christian-Oliver Ewald,,Unknown,Unknown,Unknown,Unknown,,
40.0,1.0,Computational Economics,24 March 2011,https://link.springer.com/article/10.1007/s10614-011-9264-0,The Hitting Time Density for a Reflected Brownian Motion,June 2012,Qin Hu,Yongjin Wang,Xuewei Yang,,Unknown,Unknown,Mix,,
40.0,1.0,Computational Economics,02 April 2011,https://link.springer.com/article/10.1007/s10614-011-9266-y,Smooth Transition Quantile Capital Asset Pricing Models with Heteroscedasticity,June 2012,Cathy W. S. Chen,Simon Lin,Philip L. H. Yu,Female,Male,Male,Mix,,
40.0,1.0,Computational Economics,18 May 2011,https://link.springer.com/article/10.1007/s10614-011-9268-9,A Second-Order Difference Scheme for the Penalized Black–Scholes Equation Governing American Put Option Pricing,June 2012,Zhongdi Cen,Anbo Le,Aimin Xu,Unknown,Unknown,Unknown,Unknown,,
40.0,1.0,Computational Economics,21 June 2011,https://link.springer.com/article/10.1007/s10614-011-9269-8,Using Pseudo-Parabolic and Fractional Equations for Option Pricing in Jump Diffusion Models,June 2012,Andrey Itkin,Peter Carr,,Male,Male,Unknown,Male,,24
40.0,2.0,Computational Economics,02 June 2012,https://link.springer.com/article/10.1007/s10614-012-9329-8,A Stochastic Chartist–Fundamentalist Model with Time Delays,August 2012,Ghassan Dibeh,Haidar M. Harmanani,,Male,Male,Unknown,Male,,3
40.0,2.0,Computational Economics,08 September 2011,https://link.springer.com/article/10.1007/s10614-011-9290-y,Implied Severity Density Estimation: An Extended Semiparametric Method to Compute Credit Value at Risk,August 2012,J. Samuel Baixauli,Susana Alvarez,,Unknown,Female,Unknown,Female,,2
40.0,2.0,Computational Economics,05 November 2011,https://link.springer.com/article/10.1007/s10614-011-9306-7,"Hiring, Firing and Infighting: A Tale of Two Companies",August 2012,Arnav Sheth,,,Unknown,Unknown,Unknown,Unknown,,
40.0,2.0,Computational Economics,13 October 2011,https://link.springer.com/article/10.1007/s10614-011-9297-4,Massively Parallel Computation Using Graphics Processors with Application to Optimal Experimentation in Dynamic Control,August 2012,Sergei Morozov,Sudhanshu Mathur,,Male,Unknown,Unknown,Male,,12
40.0,2.0,Computational Economics,11 June 2011,https://link.springer.com/article/10.1007/s10614-011-9270-2,Nonparametric Testing for Long-Run Neutrality with Applications to US Money and Output Data,August 2012,Jin Lee,,,Female,Unknown,Unknown,Female,,3
40.0,3.0,Computational Economics,02 November 2011,https://link.springer.com/article/10.1007/s10614-011-9305-8,Bayesian Analysis of Student t Linear Regression with Unknown Change-Point and Application to Stock Data Analysis,October 2012,Jin-Guan Lin,Ji Chen,Yong Li,Unknown,,,Mix,,
40.0,3.0,Computational Economics,28 March 2012,https://link.springer.com/article/10.1007/s10614-012-9323-1,Sequential Action and Beliefs Under Partially Observable DSGE Environments,October 2012,Seong-Hoon Kim,,,Male,Unknown,Unknown,Male,,
40.0,3.0,Computational Economics,13 August 2011,https://link.springer.com/article/10.1007/s10614-011-9288-5,Nonlinearity in Forecasting of High-Frequency Stock Returns,October 2012,Juan C. Reboredo,José M. Matías,Raquel Garcia-Rubio,Male,Male,Female,Mix,,
40.0,3.0,Computational Economics,14 February 2012,https://link.springer.com/article/10.1007/s10614-012-9316-0,On Boundary Conditions Within the Solution of Macroeconomic Dynamic Models with Rational Expectations,October 2012,Frank Hespeler,,,Male,Unknown,Unknown,Male,,
40.0,3.0,Computational Economics,17 August 2011,https://link.springer.com/article/10.1007/s10614-011-9292-9,Velocity Volatility Assessment of Monetary Shocks on Cash-in-Advance Economies,October 2012,José J. Cao-Alvira,,,Male,Unknown,Unknown,Male,,1
40.0,4.0,Computational Economics,24 November 2011,https://link.springer.com/article/10.1007/s10614-011-9300-0,DSGE Modeling on an iPhone/iPad Using SpaceTime,December 2012,Andrew P. Blake,,,Male,Unknown,Unknown,Male,,
40.0,4.0,Computational Economics,19 May 2012,https://link.springer.com/article/10.1007/s10614-012-9326-y,Public Expenditure on Health and Private Old-Age Insurance in an OLG Growth Model with Endogenous Fertility: Chaotic Dynamics Under Perfect Foresight,December 2012,Luciano Fanti,Luca Gori,,Male,Male,Unknown,Male,,1
40.0,4.0,Computational Economics,20 August 2011,https://link.springer.com/article/10.1007/s10614-011-9296-5,The Efficient Frontier for Weakly Correlated Assets,December 2012,Michael J. Best,Xili Zhang,,Male,Unknown,Unknown,Male,,
40.0,4.0,Computational Economics,21 January 2012,https://link.springer.com/article/10.1007/s10614-012-9314-2,Using a Differential Evolutionary Algorithm to Test the Efficient Market Hypothesis,December 2012,Phillip Ray Simmons,,,Male,Unknown,Unknown,Male,,6
40.0,4.0,Computational Economics,11 June 2011,https://link.springer.com/article/10.1007/s10614-011-9272-0,An Economic Model of Oil Exploration and Extraction,December 2012,Alfred Greiner,Willi Semmler,Tobias Mette,Male,Male,Male,Male,,15
40.0,4.0,Computational Economics,15 July 2011,https://link.springer.com/article/10.1007/s10614-011-9275-x,Performance of Some Logistic Ridge Regression Estimators,December 2012,B. M. Golam Kibria,Kristofer Månsson,Ghazi Shukur,Unknown,Male,Male,Male,,64
40.0,4.0,Computational Economics,27 December 2011,https://link.springer.com/article/10.1007/s10614-011-9307-6,Pareto Frontier of a Dynamic Principal–Agent Model with Discrete Actions: An Evolutionary Multi-Objective Approach,December 2012,Itza T. Q. Curiel,Sonia B. Di Giannatale,Katya Rodríguez,Unknown,Female,Female,Female,,1
41.0,1.0,Computational Economics,21 December 2011,https://link.springer.com/article/10.1007/s10614-011-9309-4,Response Surface Estimates of the Cross-Sectionally Augmented IPS Tests for Panel Unit Roots,January 2013,Jesús Otero,Jeremy Smith,,,Male,Unknown,Mix,,
41.0,1.0,Computational Economics,08 November 2011,https://link.springer.com/article/10.1007/s10614-011-9304-9,Norwegian Overnight Interbank Interest Rates,January 2013,Q. Farooq Akram,Casper Christophersen,,Unknown,Male,Unknown,Male,,11
41.0,1.0,Computational Economics,08 February 2012,https://link.springer.com/article/10.1007/s10614-011-9311-x,Motivations for Open Source Project Participation and Decisions of Software Developers,January 2013,Dongryul Lee,Byung Cho Kim,,Unknown,,Unknown,Mix,,
41.0,1.0,Computational Economics,02 November 2011,https://link.springer.com/article/10.1007/s10614-011-9302-y,Testing for Unit Roots in Panel Data Using a Wavelet Ratio Method,January 2013,Yushu Li,Ghazi Shukur,,Unknown,Male,Unknown,Male,,6
41.0,1.0,Computational Economics,23 December 2011,https://link.springer.com/article/10.1007/s10614-011-9308-5,Computing Equilibria in Discounted 2 × 2 Supergames,January 2013,Kimmo Berg,Mitri Kitti,,Male,Unknown,Unknown,Male,,12
41.0,1.0,Computational Economics,08 March 2012,https://link.springer.com/article/10.1007/s10614-012-9319-x,"Unit Root Hypothesis in the Presence of Stochastic Volatility, a Bayesian Analysis",January 2013,Jin-Yu Zhang,Yong Li,Zhu-Ming Chen,,,,Mix,,
41.0,1.0,Computational Economics,17 June 2011,https://link.springer.com/article/10.1007/s10614-011-9271-1,Testing for Structural Breaks at Unknown Time: A Steeplechase,January 2013,Makram El-Shagi,Sebastian Giesen,,Male,Male,Unknown,Male,,4
41.0,1.0,Computational Economics,02 November 2011,https://link.springer.com/article/10.1007/s10614-011-9303-x,Monetary Policy Under Time-Varying Uncertainty Aversion,January 2013,Fidel Gonzalez,Arnulfo Rodriguez,,Male,Unknown,Unknown,Male,,3
41.0,2.0,Computational Economics,30 June 2012,https://link.springer.com/article/10.1007/s10614-012-9334-y,The Interest of Having Loyal Buyers in a Perishable Market,February 2013,Juliette Rouchier,,,Female,Unknown,Unknown,Female,,7
41.0,2.0,Computational Economics,06 January 2012,https://link.springer.com/article/10.1007/s10614-011-9313-8,"Computing Equilibrium Wealth Distributions in Models with Heterogeneous-Agents, Incomplete Markets and Idiosyncratic Risk",February 2013,Muffasir Badshah,Paul Beaumont,Anuj Srivastava,Unknown,Male,Unknown,Male,,
41.0,2.0,Computational Economics,06 November 2012,https://link.springer.com/article/10.1007/s10614-012-9348-5,Simulation Analysis for Choice of Binary Lotteries,February 2013,Ichiro Nishizaki,Tomohiro Hayashida,,Male,Male,Unknown,Male,,
41.0,2.0,Computational Economics,05 June 2012,https://link.springer.com/article/10.1007/s10614-012-9327-x,A Graphical Tool for Describing the Temporal Evolution of Clusters in Financial Stock Markets,February 2013,Argimiro Arratia,Alejandra Cabaña,,Male,Female,Unknown,Mix,,
41.0,2.0,Computational Economics,15 March 2012,https://link.springer.com/article/10.1007/s10614-012-9320-4,Stochastic Evolutionary Game Dynamics and Their Selection Mechanisms,February 2013,Xing Gao,Weijun Zhong,Shue Mei,,Unknown,Unknown,Mix,,
41.0,2.0,Computational Economics,07 June 2012,https://link.springer.com/article/10.1007/s10614-012-9328-9,SEMIFARMA-HYGARCH Modeling of Dow Jones Return Persistence,February 2013,Mohamed Chikhi,Anne Péguin-Feissolle,Michel Terraza,Male,Female,Male,Mix,,
41.0,2.0,Computational Economics,25 February 2012,https://link.springer.com/article/10.1007/s10614-012-9318-y,Comparing Numerical Methods for Solving the Competitive Storage Model,February 2013,Christophe Gouel,,,Male,Unknown,Unknown,Male,,12
41.0,3.0,Computational Economics,28 November 2012,https://link.springer.com/article/10.1007/s10614-012-9353-8,Editorial for the Special Issue: Quantitative Methods in Banking and Finance,March 2013,Chrysovalantis Gaganis,Constantin Zopounidis,Michael Doumpos,Unknown,Male,Male,Male,,
41.0,3.0,Computational Economics,07 October 2012,https://link.springer.com/article/10.1007/s10614-012-9350-y,Wind Derivatives: Modeling and Pricing,March 2013,A. Alexandridis,A. Zapranis,,Unknown,Unknown,Unknown,Unknown,,
41.0,3.0,Computational Economics,22 March 2012,https://link.springer.com/article/10.1007/s10614-012-9322-2,Explanatory Factors and Causality in the Dynamics of Volatility Surfaces Implied from OTC Asian–Pacific Currency Options,March 2013,Georgios Chalamandaris,Andrianos E. Tsekrekos,,Male,Unknown,Unknown,Male,,3
41.0,3.0,Computational Economics,17 October 2012,https://link.springer.com/article/10.1007/s10614-012-9343-x,The Forecasting Performance of Corridor Implied Volatility in the Italian Market,March 2013,Silvia Muzzioli,,,Female,Unknown,Unknown,Female,,18
41.0,3.0,Computational Economics,26 July 2012,https://link.springer.com/article/10.1007/s10614-012-9337-8,Regulations and Audit Opinions: Evidence from EU Banking Institutions,March 2013,Chrysovalantis Gaganis,Fotios Pasiouras,Charalambos Spathis,Unknown,Male,Male,Male,,2
41.0,3.0,Computational Economics,01 August 2012,https://link.springer.com/article/10.1007/s10614-012-9336-9,Portfolio Risk Measures: The Time’s Arrow Matters,March 2013,Alain Ruttiens,,,Male,Unknown,Unknown,Male,,5
41.0,4.0,Computational Economics,18 October 2012,https://link.springer.com/article/10.1007/s10614-012-9341-z,A Generic Framework for a Combined Agent-based Market and Production Model,April 2013,Bas Straatman,Danielle J. Marceau,Roger White,Male,Female,Male,Mix,,
41.0,4.0,Computational Economics,12 June 2012,https://link.springer.com/article/10.1007/s10614-012-9330-2,Optimal Tax Progressivity in Unionised Labour Markets: Simulation Results for Germany,April 2013,Stefan Boeters,,,Male,Unknown,Unknown,Male,,
41.0,4.0,Computational Economics,13 January 2012,https://link.springer.com/article/10.1007/s10614-011-9312-9,Reverse Engineering Financial Markets with Majority and Minority Games Using Genetic Algorithms,April 2013,J. Wiesinger,D. Sornette,J. Satinover,Unknown,Unknown,Unknown,Unknown,,
41.0,4.0,Computational Economics,14 October 2012,https://link.springer.com/article/10.1007/s10614-012-9349-4,Can They Beat the Cournot Equilibrium? Learning with Memory and Convergence to Equilibria in a Cournot Oligopoly,April 2013,Thomas Vallée,Murat Yıldızoğlu,,Male,Male,Unknown,Male,,7
41.0,4.0,Computational Economics,17 August 2011,https://link.springer.com/article/10.1007/s10614-011-9291-x,SIMUL 3.2: An Econometric Tool for Multidimensional Modelling,April 2013,Rodolphe Buda,,,Male,Unknown,Unknown,Male,,1
41.0,4.0,Computational Economics,23 March 2012,https://link.springer.com/article/10.1007/s10614-012-9321-3,Solving Rational Expectations Models with Informational Subperiods: A Perturbation Approach,April 2013,Anna Kormilitsina,,,Female,Unknown,Unknown,Female,,6
41.0,4.0,Computational Economics,08 December 2012,https://link.springer.com/article/10.1007/s10614-012-9356-5,On the Use of the Renormalization Procedure to Estimate the Bifurcation Parameters in Nonlinear Dynamic Models,April 2013,Walter Briec,Laurence Lasselle,,Male,Female,Unknown,Mix,,
42.0,1.0,Computational Economics,30 March 2013,https://link.springer.com/article/10.1007/s10614-013-9376-9,Comparing Strategies of Collaborative Networks for R&D: An Agent-Based Study,June 2013,Pedro Campos,Pavel Brazdil,Isabel Mota,Male,Male,Female,Mix,,
42.0,1.0,Computational Economics,22 January 2012,https://link.springer.com/article/10.1007/s10614-012-9315-1,Network Formation with Heterogeneous Agents and Absolute Friction,June 2013,Joost Vandenbossche,Thomas Demuynck,,Male,Male,Unknown,Male,,15
42.0,1.0,Computational Economics,17 October 2012,https://link.springer.com/article/10.1007/s10614-012-9346-7,The Price and Trading Volume Dynamics Relationship in the EEX Power Market: A Wavelet Modeling,June 2013,Foued Saâdaoui,,,Unknown,Unknown,Unknown,Unknown,,
42.0,1.0,Computational Economics,05 September 2012,https://link.springer.com/article/10.1007/s10614-012-9339-6,A Classical MCMC Approach to the Estimation of Limited Dependent Variable Models of Time Series,June 2013,George Monokroussos,,,Male,Unknown,Unknown,Male,,1
42.0,1.0,Computational Economics,01 November 2012,https://link.springer.com/article/10.1007/s10614-012-9351-x,Stochastic Control of Linear and Nonlinear Econometric Models: Some Computational Aspects,June 2013,D. Blueschke,V. Blueschke-Nikolaeva,R. Neck,Unknown,Unknown,Unknown,Unknown,,
42.0,1.0,Computational Economics,24 April 2012,https://link.springer.com/article/10.1007/s10614-012-9324-0,Partially Adaptive Estimation of Interval Censored Regression Models,June 2013,Jason Cook,James McDonald,,Male,Male,Unknown,Male,,13
42.0,1.0,Computational Economics,21 July 2012,https://link.springer.com/article/10.1007/s10614-012-9335-x,Is the Leading Role Desirable?: A Simulation Analysis of the Stackelberg Behavior in World Petroleum Market,June 2013,Zili Yang,,,Unknown,Unknown,Unknown,Unknown,,
42.0,2.0,Computational Economics,06 May 2012,https://link.springer.com/article/10.1007/s10614-012-9325-z,Using Constrained Optimization for the Identification of Convergence Clubs,August 2013,Paolo Postiglione,M. Simona Andreano,Roberto Benedetti,Male,Unknown,Male,Male,,36
42.0,2.0,Computational Economics,17 June 2012,https://link.springer.com/article/10.1007/s10614-012-9331-1,Tensor Spline Approximation in Economic Dynamics with Uncertainties,August 2013,Moody T. Chu,Chun-Hung Kuo,Matthew M. Lin,Unknown,Unknown,Male,Male,,3
42.0,2.0,Computational Economics,09 June 2012,https://link.springer.com/article/10.1007/s10614-012-9332-0,A Comparison of Various Artificial Intelligence Methods in the Prediction of Bank Failures,August 2013,Halil Ibrahim Erdal,Aykut Ekinci,,Male,Male,Unknown,Male,,15
42.0,2.0,Computational Economics,11 February 2012,https://link.springer.com/article/10.1007/s10614-012-9317-z,Multiple Kernel Learning with Fisher Kernels for High Frequency Currency Prediction,August 2013,Tristan Fletcher,John Shawe-Taylor,,Male,Male,Unknown,Male,,15
42.0,2.0,Computational Economics,20 June 2012,https://link.springer.com/article/10.1007/s10614-012-9333-z,Using Economic Theory to Guide Numerical Analysis: Solving for Equilibria in Models of Asymmetric First-Price Auctions,August 2013,Timothy P. Hubbard,René Kirkegaard,Harry J. Paarsch,Male,Male,Male,Male,,20
42.0,3.0,Computational Economics,27 November 2012,https://link.springer.com/article/10.1007/s10614-012-9352-9,Bubble Formation and Heterogeneity of Traders: A Multi-Agent Perspective,October 2013,Shu-Peng Chen,Ling-Yun He,,,,Unknown,Mix,,
42.0,3.0,Computational Economics,13 December 2012,https://link.springer.com/article/10.1007/s10614-012-9355-6,Estimating the Long-Memory Parameter in Nonstationary Processes Using Wavelets,October 2013,Heni Boubaker,Anne Péguin-Feissolle,,Unknown,Female,Unknown,Female,"The estimation of memory parameter \(d\) in the fractionally integrated process \(I(d)\) has been widely examined in the literature and numerous estimation methods have been devised for when the property of stationarity holds. Among these methods, we find the parametric methods, which are (approximate or exact) likelihood methods in the time domain or frequency domain, and semiparametric estimators which are based on spectral density (see e.g. Doukhan et al. 2003). The case where the fractional integrated process is nonstationary has been studied by different authors who proposed a number of estimation methods.Footnote 1 Velasco (1999a) extends, for non-stationary time series with Gaussian increments, the log-periodogram semiparametric estimate of Robinson (1995a). Velasco (1999b) considers the Gaussian semiparametric estimate analyzed by Robinson (1995b) for stationary processes. Velasco and Robinson (2000) propose a Whittle pseudo-maximum likelihood estimation for nonstationary time series. Shimotsu (2010) develops the Feasible Exact Local Whittle (FELW) estimator; this is an extended version of the Exact Local Whittle (ELW) estimator proposed by Phillips and Shimotsu (2004) and Shimotsu and Phillips (2005, 2006), that is a semiparametric estimator generally giving a good estimation method for the memory parameter in terms of consistency and limit distribution, except in the case where the mean is unknown. Abadir et al. (2007) introduce the fully extended local Whittle estimator, covering stationary and nonstationary regions. Two papers are based on a wavelet estimator. The first one is by Moulines et al. (2008) who propose a wavelet-based semiparametric pseudo-likelihood maximum method estimator; this estimator was introduced in a parametric context in Wornell and Oppenheim (1992) (see also Kaplan and Kuo 1993; McCoy and Walden 1996). The second one is by Lu (2009) who gives a procedure based on discrete wavelet packet transform to estimate the time-varying parameters of a locally stationary k-factor Gegenbauer process. In this article, we propose two new semiparametric estimators in the wavelet domain in order to estimate the parameter of nonstationary long memory models. Compared to the Fourier transform, wavelet approach has a number of advantages to analyze the behavior of nonstationary time series. Wavelets can localize a process simultaneously in time and scale. They allow differencing implicitly and therefore they can be used without problems when \(d>1/2\); they also automatically discount polynomial trends, as noted in Faÿ et al. (2009). Moreover, the wavelet estimator of long-memory parameter has been found fairly robust to model specification in most previous empirical studies. More precisely, we propose the Wavelet Exact Local Whittle (WELW) estimator which constitutes an extension of the ELW estimator of Phillips and Shimotsu (2004) and Shimotsu and Phillips (2005) and Shimotsu and Phillips (2006) in the wavelet domain, and the Wavelet Feasible Exact Local Whittle (WFELW) estimator extending the FELW estimator to the case where the mean is unknown. Our approach is different from the wavelet estimator of Moulines et al. (2008): they too use the local Whittle method but their likelihood function is based on the wavelet coefficients and their estimator is deduced from the asymptotic consistency of these coefficients. Instead, in order to compute the WELW estimator, we decompose the periodogram and not the likelihood function into wavelets, then we determine for each scale the ELW estimator; the WELW estimator has consequently the same distribution as the ELW estimator and is simple to compute. Simulation experiments show that performances of the WELW and WFELW estimators perform better under most cases in the stationary and nonstationary cases. Indeed, the use of wavelet periodogram at each level of resolution allows to generate an estimator of the long-memory parameter \(d\) for each scale which captures better the persistence compared to standard periodogram. The paper is structured as follows. Section 2 presents the main characteristics of wavelet analysis and of the ELW estimator; it then develops the wavelet-based ELW estimators. In Sect. 3, we describe the simulation and empirical results. Section 4 concludes the paper.",6
42.0,3.0,Computational Economics,28 September 2012,https://link.springer.com/article/10.1007/s10614-012-9342-y,Taking Perturbation to the Accuracy Frontier: A Hybrid of Local and Global Solutions,October 2013,Lilia Maliar,Serguei Maliar,Sébastien Villemot,Female,Male,Male,Mix,,
42.0,3.0,Computational Economics,28 August 2012,https://link.springer.com/article/10.1007/s10614-012-9338-7,High-Water Marks and Hedge Fund Management Contracts with Partial Information,October 2013,Dandan Song,Jinqiang Yang,Zhaojun Yang,Unknown,Unknown,Unknown,Unknown,,
42.0,3.0,Computational Economics,18 September 2012,https://link.springer.com/article/10.1007/s10614-012-9340-0,Expected Optimal Feedback with Time-Varying Parameters,October 2013,Marco P. Tucci,David A. Kendrick,Hans M. Amman,Male,Male,Male,Male,,3
42.0,4.0,Computational Economics,04 January 2013,https://link.springer.com/article/10.1007/s10614-012-9358-3,An Evolutionary Model of Price Competition Among Spatially Distributed Firms,December 2013,Ludo Waltman,Nees Jan van Eck,Uzay Kaymak,Male,Unknown,Male,Male,"The phenomenon of cooperative behavior among individuals in social, economic, and biological systems has been fascinating researchers already for a long time. An important topic in the economic and biological literature is the emergence of cooperative behavior among individuals who are pursuing their self-interest. Researchers aim to identify the conditions under which the emergence of cooperative behavior among such individuals is possible. In an economic context, the best-known explanation of cooperative behavior is probably the one based on the idea of reciprocity in repeated encounters. When individuals interact with each other repeatedly, they may choose to behave cooperatively, even though this has a negative effect on their short-term interests. Individuals may choose to behave cooperatively because they realize that if they do not behave this way, others won’t do either. They also realize that in the long run they are better off in a cooperative world than in a non-cooperative one. Hence, although cooperative behavior harms one’s short-term interests, it is likely to be beneficial to one’s interests in the long run. Explaining cooperative behavior in terms of reciprocity assumes that individuals interact with each other repeatedly and that they remember what happened in the past. These assumptions seem reasonable in some contexts but not in others. Because of this, a number of alternative explanations of cooperative behavior have been proposed in the literature. In this paper, we focus on one such explanation. This is the explanation that cooperative behavior is a consequence of the spatial distribution of individuals and the repeated local interaction among them. In the biological literature, this explanation was proposed in a well-known paper by Nowak and May (1992). Many biologists have built on this work, which has resulted in a substantial body of literature. Inspired by the work done in biology, economists have also attempted to explain cooperative behavior in terms of repeated local interaction among spatially distributed individuals. An evolutionary perspective is typically taken, in which individuals are assumed to imitate each other and to randomly experiment with new actions. The first work in the economic literature was done by Bergstrom and Stark (1993) and Eshel et al. (1998). In this work, cooperative behavior was shown to emerge in models in which individuals are organized in a circular structure. A large number of studies have built on this early work.Footnote 1 Studies in the economic literature often focus on rather abstract models. Many studies for example assume that individuals are located in a one-dimensional world. Also, many studies assume a situation similar to a classical prisoners’ dilemma, in which individuals can choose from only two actions (i.e., cooperation and defection). For examples of studies that make these assumptions, we refer to Bergstrom and Stark (1993); Eshel et al. (1998); Jun and Sethi (2007); Mengel (2009) and Stark and Behrens (2010). In this paper, we consider a somewhat less abstract level of modeling. We aim to determine to what extent the findings from earlier studies generalize to models of price competition among spatially distributed firms. In particular, we want to find out whether imitation and experimentation may cause cooperative behavior to emerge in spatial price competition models. Compared with the frequently studied prisoners’ dilemma models, the models that we study are of a more complex nature. There is no simple binary decision between cooperative and non-cooperative behavior in our models. Firms can cooperate with each other by jointly increasing their price, and different price levels correspond with different levels of cooperation. Also, interactions in our models may involve more than two individuals. In one of our models, each consumer has four different firms from which he may choose to buy. Hence, firms in this model always have multiple competitors with which they fight for the same market share. Like in the literature mentioned above, we take an evolutionary perspective in our models. We assume firms’ behavior to be determined by imitation and experimentation. More specifically, we assume that firms change their price either by imitating successful competitors in their neighborhood or by experimenting with small price increases or decreases. We study a variety of conditions under which firms may or may not start to cooperate with each other. We consider both a model in which firms are organized in a one-dimensional space and a model in which firms are organized in a two-dimensional space. Our two-dimensional model has two variants, which differ from each other in the way in which consumers are located. We also look at the effect of the information firms have about competitors in their neighborhood. In doing so, we distinguish between on the one hand the number of competitors about which firms have information and on the other hand the accuracy of the information firms have. Another effect that we look at is the effect of firms’ experimentation probability, that is, the probability with which firms experiment with small price increases or decreases. Due to the complexity of the models that we study, we perform our analyses mainly using computer simulations. This paper is organized as follows. The models that we study are introduced in Sect. 2. The analysis of the models is presented in Sect. 3. The main conclusions of our research are summarized in Sect. 4.",
42.0,4.0,Computational Economics,04 October 2012,https://link.springer.com/article/10.1007/s10614-012-9347-6,Efficiency of Crude Oil Futures Markets: New Evidence from Multifractal Detrending Moving Average Analysis,December 2013,Yudong Wang,Chongfeng Wu,,Unknown,Unknown,Unknown,Unknown,,
42.0,4.0,Computational Economics,10 October 2012,https://link.springer.com/article/10.1007/s10614-012-9345-8,A Genetic Programming Approach for EUR/USD Exchange Rate Forecasting and Trading,December 2013,Georgios A. Vasilakis,Konstantinos A. Theofilatos,Spiros D. Likothanassis,Male,Male,Male,Male,,33
42.0,4.0,Computational Economics,10 October 2012,https://link.springer.com/article/10.1007/s10614-012-9344-9,Error Analysis and Comparison of Two Algorithms Measuring Compensated Income,December 2013,Zhen Sun,Yang Xie,,,,Unknown,Mix,,
42.0,4.0,Computational Economics,09 January 2013,https://link.springer.com/article/10.1007/s10614-012-9357-4,Bacterial Foraging Optimization Approach to Portfolio Optimization,December 2013,Yucheng Kao,Hsiu-Tzu Cheng,,Unknown,Unknown,Unknown,Unknown,,
42.0,4.0,Computational Economics,15 December 2012,https://link.springer.com/article/10.1007/s10614-012-9354-7,Bayesian Unit Root Test in Double Threshold Heteroskedastic Models,December 2013,Cathy W.S. Chen,Shu-Yu Chen,Sangyeol Lee,Female,,Unknown,Mix,,
43.0,1.0,Computational Economics,26 April 2013,https://link.springer.com/article/10.1007/s10614-013-9378-7,Generalization of the Firm’s Profit Maximization Problem: An Algorithm for the Analytical and Nonsmooth Solution,January 2014,R. García-Rubio,L. Bayón,J. M. Grau,Unknown,Unknown,Unknown,Unknown,,
43.0,1.0,Computational Economics,15 May 2013,https://link.springer.com/article/10.1007/s10614-013-9380-0,The Duo-Item Bisection Auction,January 2014,Albin Erlanson,,,Male,Unknown,Unknown,Male,"Recent research in auction theory has produced a number of papers on iterative auctions (Mishra and Parkes 2009; Perry and Reny 2005; Ausubel 2004). In an iterative auction the auctioneer announces a price and bidders submit their bids. A new price based upon the reported bids is announced by the auctioneer. The process is repeated until an allocation is determined. This iterative procedure contrasts with the approach taken in direct mechanisms, where bidders submit their preferences and an allocation is determined. There are several reasons for focusing on iterative auctions. In iterative auctions bidders may not reveal all information regarding their private valuations. This could be a beneficial property. It has been shown that full revelation of preferences can be problematic (Rothkopf et al. 1990; Engelberecht-Wiggans and Kahn 1991). Partial revelation of preferences can lead to less communication and thereby decrease the amount of data required for the computation of an allocation. Another argument for looking closer at iterative auctions is the prevalence of them in real world auctions (e.g. English auction, Dutch auction, etc.). One property of importance when analyzing an auction is efficiency. An efficient assignment maximizes the overall value derived by the bidders. This translates to designing an auction with efficiency as part of the equilibrium in the game induced by the auction. The benchmark for the environment with private valuations is the Vickrey–Clarke–Groves mechanism (Clarke 1971; Groves 1973; Vickrey 1961), henceforth VCG. The VCG mechanism is a direct mechanism with truth-telling as a weakly dominant strategy and the equilibrium outcome is efficient. Another classical mechanism is the English auction. It is an iterative open bid ascending auction for selling one item, and its descending counterpart is the Dutch auction. The English auction is strategically equivalent to the Vickrey auction. Hence, in the English auction it is a weakly dominant strategy to bid truthfully and the resulting equilibrium is efficient. The single item bisection auction presented and analyzed by Grigorieva et al. (2007) is an example of a sealed-bid iterative auction. It elicits a limited amount of information on preferences, but still reaches the VCG outcome. It has also fewer rounds than the English auction. In other words, the single item bisection auction has the correct incentive structure, it is privacy preserving and has a fast convergence rate. For multiple heterogeneous items things get more complicated. It is a complex problem to solve in the most general setting where bidders are allowed to bid on any packages of items. In a seminal paper by Demange et al. (1986) an iterative auction for multiple items with unit-demand bidders was presented. The auction proposed in Demange et al. (1986) results in the VCG outcome. In this paper we propose a multi-item bisection auction. It generalizes the single item bisection auction (Grigorieva et al. 2007). We keep the assumption from the single item bisection auction of unit-demand bidders. Thus, we do not consider the case when bidders bid on packages of items. The auction to be proposed is a multi-item sealed-bid auction for selling heterogeneous items to bidders interested in acquiring at most one item. They all have private valuations. In other words it is a standard assignment problem. To illustrate the idea behind the multi-item bisection auction it is enough to consider the case with two items for sale. We call this the duo-item bisection auction. The duo-item bisection auction is later modified to allow for a more selective elicitation of information on preferences. All our results are given for the environment with two items for sale. However, many of them are straightforward to generalize to a setting with more than two items for sale. The first result establishes an upper limit on the number of iterations for the duo-item bisection auction. Thereafter we proceed by showing that the duo-item bisection auction reaches the VCG outcome under the assumption of truthful bidding. This is not as restrictive as it first may look. There are general results on incentives for dynamic auction mechanisms implementing the VCG outcome. Loosely speaking the results establish truthful bidding as a weakly dominant strategy when bidding strategies are constrained to maximize payoff in each step by taking prices as given (Gul and Staccetti 2000; Parkes 2001). Our last result concerns the modification of the duo-item bisection auction. We prove that the modified duo-item bisection auction attains the VCG outcome, whilst eliciting the minimal amount of information on preferences. The single item bisection auction is straightforward to describe. However, already with two items the generalized bisection auction gets involved and more effort is required to describe it. This is not specific for this auction mechanism. Using auctions for solving an assignment problem with an arbitrary number of items and bidders is a complex problem. There are both computational and theoretical obstacles to overcome. The multi-item bisection auction can also be seen as a computational alternative for solving an assignment problem with known valuations. The rest of the paper is organized as follows. Sect. 2 begins by describing the single item bisection auction. After this the model is presented together with some preliminaries. In the subsequent third section the generalized multi-item bisection auction is described. This is done by considering the case of two items for sale and discuss the auction mechanism for this scenario. Section 3 ends with a short discussion of the multi-item bisection auction. Section 4 contains the main results and Sect. 5 concludes the paper.",
43.0,1.0,Computational Economics,05 December 2013,https://link.springer.com/article/10.1007/s10614-013-9413-8,Implications of a Reserve Price in an Agent-Based Common-Value Auction,January 2014,Christopher N. Boyer,B. Wade Brorsen,,Male,Unknown,Unknown,Male,"Reserve prices (or minimum bids) are widely used in various auction structures and influence bid prices when imposed. Riley and Samuelson (1981) extend Vickrey (1961) to include a reserve price in an independent private values auction, and theoretically illustrate several possible outcomes when a seller imposes a reserve price. They show that reserve prices can increase buyers’ bid prices, decrease buyers’ revenues, increase sellers’ revenues, and decrease the probability of the item being sold. The theoretical predictions in these papers are the foundation for much of the literature on reserve prices in auctions. Several empirical studies have validated Riley and Samuelson’s (1981) conclusions using data from real estate auctions (McAfee et al. 2002), timber auctions (Paarsch 1997), and oil lease auctions (Hendricks et al. 1994). Today, nearly anyone with computer access can sell various items with a reserve price in online auctions (Bajari and Hortacsu 2003; Reiley 2006). Using data from online auctions such as eBay has become a popular approach to analyze the impacts of reserve prices. For example, Reiley (2006) sold Magic: The Gathering game cards on eBay without a reserve price and with various levels of reserve prices. As predicted, Reiley (2006) finds that increasing the reserve price decreases the number of buyers participating in the auction, suggesting that reserve prices can act as a barrier to entry for buyers. Also, buyers’ bid prices increased when a reserve price was imposed by the seller. Reiley (2006) follows Riley and Samuelson’s (1981) independent private values auction model to make these conclusions; even though Reiley (2006 p. 198) acknowledges that common-value auction theory could be argued as more appropriate for the game cards. Reiley (2006) finds support for Riley and Samuelson’s (1981) hypotheses about the impacts of a reserve price in an auction. Little attention, however, has been given to the influence of a reserve price on a common-value auction. A general theoretical framework for a common-value auction with a reserve price is introduced by McAfee and Vincent (1992). They analyzed data from an offshore oil lease auction and found evidence that the reserve price set by the government in oil lease auctions was too low. Levin and Smith (1996) expand on Riley and Samuelson’s (1981) theoretical model and examine the impact of a reserve price in a second-price common-value auction. Their theoretical model is a function of the number of buyers participating in the auction and find the reserve price can benefit the seller in a second-price common-value auction. These theoretical papers give some insight into the effects of a reserve price in a common-value auction, but do not solve for equilibriums of the common-value auction. More recently, Gonçalves (2013) established an online second-price sealed-bid common-value auction, and he found that the seller can increase revenue by setting a reserve price. With a reserve price, the seller is able to increase the bid price, but the quantity sold decreases (Gonçalves 2013). Conversely, setting the reserve price above the highest bid price offered by the buyers in the auction results in a reduction in the seller’s revenues (Gonçalves 2013). Reiley (2006) and Gonçalves (2013) cannot control for all the exogenous factors in their online auctions that might influence the results like in a laboratory experiment (Reiley 2006). The results from these types of studies are recognized by Reiley (2006) and Gonçalves (2013) as being noisy. Solving for the equilibrium strategies of a first-price common-value auction with and without a reserve price analytically and reproducing the solution under controlled laboratory settings could be beneficial to expand the theory. However, the analytical solution of a first-price common-value auction with a reserve price is perhaps infeasible (Gordy 1998), and reproducing this auction in a laboratory experiment is likely expensive. An alternative technique that economists can use to determine the solution to game theory problems is agent-based computational modeling (Arifovic 1996; Bonabeua 2002; Alkemade et al. 2006). Agent-based computational models are artificial markets where interactive agents trade (Tesfatsion 2001). These models can simulate artificial economic markets when data are not available or are expensive to obtain and when there is not an analytical solution. Equilibrium prices in various auction markets have been successfully obtained with agent-based models (Tesfatsion 2001; Guerci et al. 2008; Hailu and Thoyer 2010; Boyer and Brorsen 2013). Using an agent-based auction model to determine the impact of a reserve price in a first-price common-value auction is an appropriate method to find the equilibrium bid price and optimal reserve price. 
Andreoni and Miller (1995) first solved an agent-based common-value auction model but their seller could not impose a reserve price. A genetic algorithm was used to solve their model and they found buyers’ bid prices and average profits to be below the predicted Bertrand-Nash equilibrium or the common value of the item, suggesting buyers are able to shade bids. Recently, Noe et al. (2012) developed an agent-based first-price common-value auction to determine how bid prices change under price uncertainty. Noe et al. (2012) used a genetic algorithm and did not allow the sellers to impose a reserve price. Noe et al. (2012) also found buyers on average shade bids below the Bertrand-Nash equilibrium. Noe et al. (2012) identified several causes of the deviation from the Bertrand-Nash equilibrium, and suggest the learning algorithm may allow natural collusion by the buying agents. Developing an agent-based first-price common-value auction with a reserve price would be an extension of Andreoni and Miller (1995) and Noe et al. (2012). The objective of this study is to establish an agent-based first-price common-value auction to evaluate the impact of a reserve price on the winning bid price and the seller’s expected revenue. We develop an agent-based first-price common-value auction with and without a reserve price for a two-buyer and a three-buyer market structure. Our agent-based auction extends previous agent-based models by allowing the seller to choose a reserve price. Buyers and sellers use a new particle swarm optimization (PSO) learning algorithm, which extends Zhang and Brorsen (2009) to maximize expected revenue. The PSO algorithm uses the buyers’ and sellers’ previous and current best bid/reserve strategies to direct their next bid/reserve strategies to the expected revenue maximizing solution. We follow Kagel and Levin’s (1986) common-value auction theory as the basis for our auction, which assumes bid prices are uniformly distributed. The agent-based model provides an opportunity to extend Kagel and Levin’s (1986) theory by giving one of the buyers a normally distributed bid function.",6
43.0,1.0,Computational Economics,16 March 2013,https://link.springer.com/article/10.1007/s10614-013-9374-y,Simulating the Evolution of Market Shares: The Effects of Customer Learning and Local Network Externalities,January 2014,Liangjie Zhao,Wenqi Duan,,Unknown,Unknown,Unknown,Unknown,,
43.0,1.0,Computational Economics,22 March 2013,https://link.springer.com/article/10.1007/s10614-013-9363-1,DSGE Model Estimation on the Basis of Second-Order Approximation,January 2014,Sergey Ivashchenko,,,Male,Unknown,Unknown,Male,"Estimation of DSGE models is a very important issue. To use DSGE models, one must be knowledgeable about their behaviour, which depends on parameter values. There are different econometric techniques employed for model estimation, but empirical literature has focused on the estimation of first-order linearized DSGE models (Tovar 2008). Computation with linear approximation is much faster than higher order approximation, but its behaviour could differ from the behaviour of more accurate approximations (see Collard and Juillard 2001). Second-order approximation can make the difference between models and approximation behaviour much smaller. Asset pricing is an area where first-order approximation is not applicable because it eliminates all risk premiums (see Tovar 2008), which is why it is important to estimate DSGE models on the basis of second-order (or higher) approximation. Second-order estimation has its advantages. The particle filter serves as a tool for likelihood function construction on the basis of nonlinear DSGE model approximations (An and Schorfheide 2006). There are alternative filters that could be used for likelihood calculation. For example, Andreasen (2008) has shown the advantage of the central difference Kalman filter (CDKF) over some versions of particle filters. There are other nonlinear modifications of the Kalman filter which could be used for the second-order estimation of DSGE. In this article, three versions are used: the CDKF (see Norgaard et al. 2000), the Unscented Kalman filter (UKF, see Julier and Uhlmann 1997), and the Quadratic Kalman filter (QKF). There is no description for the QKF in the econometric literature. The QKF is a special case of the Gaussian assumed density filter (Sarkka 2010). The CDKF and the UKF are other special cases of Gaussian assumed density filter (Ito and Xiong 2000; Sarkka and Hartikainen 2010). The purpose of this article is to compare different versions of second-order estimation techniques for DSGE models. It should be noted that usually filters are compared by RMSE of filtered variables (Sarkka and Hartikainen 2010). Talking about such measure of filters quality leads to smoothing problem which have different goal but the same measure of quality (Sarkka et al. 2007; Sarkka 2008). The purpose of this article concerns estimation where filtering is key element. However, smoothing problem has no relation to estimation. So, RMSE of filtered variables would be presented for different filters.",10
43.0,1.0,Computational Economics,17 March 2013,https://link.springer.com/article/10.1007/s10614-013-9364-0,Paradox Lost: The Evolution of Strategies in Selten’s Chain Store Game,January 2014,William M. Tracy,,,Male,Unknown,Unknown,Male,"The application of analytical decision models to Selten’s Chain Store Paradox has spurred several notable developments in decision theory, including the subgame perfect equilibrium concept (Selten 1978) and the sequential equilibrium concept (Kreps and Wilson 1982a). However, the existing analytical resolutions to the Chain Store Paradox either alter the game to avoid the paradox or fail to organize the existing behavioral data. The inability to explain the behavioral data persists, even when Camerer and Weigelt’s (1988) methods for modeling homegrown priors are applied (Jung et al. 1994). Thus, this paper attempts to partially bridge the divide between theory and data by applying an evolutionary agent-based model (c.f., Holland and Miller 1991) to the paradox. Selten (1978, p. 133) asserted that, “The fact that the logical inescapability of induction theory fails to destroy the plausibility of the deterrent theory is a serious phenomenon, which merits the name of a paradox.” Because of genetic drift over temporally neutral alleles, this paper’s model can produce a system whose long-run behavior approximates an ergodic Markov chain that osculates among the solution space’s various Nash Equilibria. Such a solution enables the coexistence of deterrent behavior and induction behavior, and thus offers a potential resolution to the paradox. The learning path through which the system enters the Markov chain can be calibrated to generate results that are congruent with the limited behavior data on the Chain Store Paradox. These findings are interesting in their own right; many of the analytical solutions to the Chain Store Paradox involve changing the game in ways that might limit its applicability to specific real-world scenarios. For example, the typing approach suggested by Kreps and Wilson (1982b) and Milgram and Roberts (1982) seems more applicable to incumbents that are midsized retail chains than to incumbents with dominate positions in a finite number of product markets (e.g., Apple or Microsoft). In addition to being relevant to a different swath of real-world scenarios, the solution presented in this paper offers specific insights for both practitioners and modelers. Specifically, this paper’s model and solution offer three principal insights. First, the ability of an evolutionary argent-based model to posit a unique resolution into a problem as thoroughly examined as the Chain Store Paradox further bolsters the use of this technique in models of strategic decision making. Second, the extent to which the model fits key aspects of the behavior data depends on the whether the chromosome is represented as a bit-string or a finite state machine; as such, this paper contributes to the discussion of the relationship between the choice of evolutionary mechanisms and the behavior of computational economic modes (c.f., Alkemade et al. 2006). Finally, the limited band of mechanisms and parameters that allow the system to transition between states tentatively suggests a new approach for policy makers looking to promote or prevent a social system’s transition away from a subgame perfect state. The next section reviews the relevant theory. The third section details a class of evolutionary algorithms whose solution to the Chain Store game better explains the observed behavior. The fourth section examines the simulation results and uses concepts from evolutionary theory to examine how genetic drift causes the system to diverge from the game theoretic solution. The fifth section concludes by noting implications for computational modeling and policy research.",1
43.0,1.0,Computational Economics,09 January 2013,https://link.springer.com/article/10.1007/s10614-012-9359-2,OLG Life Cycle Model Transition Paths: Alternate Model Forecast Method,January 2014,Richard W. Evans,Kerk L. Phillips,,Male,Unknown,Unknown,Male,"In 2008, the overlapping generations (OLG) model proposed by Samuelson (1958) turned 50.Footnote 1 OLG models provide a dynamic general equilibrium setting with heterogeneous agents that looks more simple and intuitive on the surface than the more widely used heterogeneous agent models with infinitely lived agents. The OLG framework is invaluable for analyzing any type of question in which age cohorts are affected differently by exogenous shocks. However, the intuitive structure of new generations of finitely lived agents being born in each period comes with at least two costs that are closely related. The fundamental welfare theorems of economics do not hold in OLG models, in general,Footnote 2 and the computation of equilibrium transition paths can require a tremendous computational burden. It is the latter complexity of OLG models—namely, the increased computational burden of computing equilibrium transition paths—that we wish to address in this paper. In particular, we detail a new method for computing an approximation of the rational expectations equilibrium transition path in OLG models with only idiosyncratic uncertainty, which we call the alternate model forecast (AMF) method. Compared to the benchmark rational expectations solution method for this class of models, we find that our AMF method reduces computation times by about 85 percent and the approximation error in terms of mean absolute percent deviation is less than 1 percent. This result is robust to both the degree of heterogeneity in the model and the distance of the initial state from the steady-state equilibrium. A requirement of a rational expectations equilibrium is that agents’ beliefs about the future be correct. The only difference between our AMF solution method and the benchmark rational expectations method is that the AMF method relaxes this requirement of rational expectations in a similar way to Krusell and Smith (1998). We assume that agents’ beliefs about the evolution of the aggregate capital stock—and therefore future interest rates and wages—are a simple function of the current capital stock, which is a summary statistic of the entire distribution of capital. These beliefs need not be correct. They just must be updated every period in a way that is consistent with updated information on the current state and a correct belief that the economy will eventually reach the steady-state. It is this AMF assumption, that agents’ beliefs are a function of only of the aggregate capital stock and not the entire distribution of capital, that provides the tremendous reduction in agents’ informational burden and computational times. The AMF solution method has two equally plausible interpretations. First, the AMF method can be interpreted solely as a computational approximation to a rational expectations equilibrium in which agents’ beliefs are assumed to be correct. The validity of this interpretation relies on the computed AMF transition paths being close to the rational expectations transition paths. We make this comparison in Sect. 3.3 and find that the two computed equilibrium time paths of the economy are very close. A second interpretation of the AMF solution method is to take the alternate model forecasts of agents’ beliefs about the future as their true beliefs. That is, we assume that agents have adaptive expectations rather than rational expectations. A large literature on adaptive, incorrect, or naïve expectations exists. This idea of agents not using all available information is similar to the rational inattention concept of Mankiw and Reis (2002) and Sims (2003). It might be reasonable to assume that it is either too costly for agents to acquire all the information about the entire distribution of capital or that the agents are capacity constrained on information processing. Benhabib and Day (1982) and Michel and de la Croix (2000) showed that when agents have myopic beliefs, complex chaotic dynamics can result in an OLG environment. Chen et al. (2008) build an OLG model with capital accumulation and two-period-lived agents. Their consumers have additively separable preferences with a constant elasticity of intertemporal substitution an no aggregate shocks. They show that in this context when the intertemporal elasticity is small, myopic dynamics can be used to approximate economic behavior under perfect foresight. However, when the elasticity becomes large, myopia produces chaotic cycles. Williams (2003) considers a variety of different learning rules in an otherwise standard real business cycle model and finds that the dynamics are not substantially different from a rational expectations assumption. However, Huang et al. (2009) build an infinitely lived agent model with both neutral and investment-specific technology shocks. They find, in contrast to Williams (2003), that while the steady states for rational expectations and adaptive expectations are identical, the dynamics about this steady state are quite different. They trace this difference to weakening of the wealth effects of shocks and a strengthening of the intertemporal substitution effects. Lastly, Fuster et al. (2010) provide a good survey of the literature on agents’ beliefs. They develop a model of quasi-rational expectations, which they term “natural expectations” and show that this hybrid of myopic and rational expectations can generate the hump shape observed in many macroeconomic time-series. They trace their model’s ability to replicate this to fact that agent’s beliefs do not adequately account for the reversion of the models exogenous driving processes to their mean. With the adaptive expectations interpretation of our AMF solution method, we find that the much less costly informational burden on agents implies equilibrium transition paths that are very close to the rational expectations transition paths. This interpretation and our findings provide support for the rational inattention literature of Mankiw and Reis (2002) and Sims (2003) and for the results of Chen et al. (2008) and Williams (2003) that a weakening of the rationality assumption closely approximates rational behavior. The benchmark conventional solution method for the non-stationary rational expectations equilibrium transition path in OLG models is outlined in (Auerbach and Kotlikoff (1987), chapt. 4) for the perfect foresight case and in (Nishiyama and Smetters (2007), Appendix II) for the case with idiosyncratic uncertainty. We call this benchmark solution method time path iteration (TPI). We will show the conventional TPI method is a rational expectations equilibrium concept in which each agent correctly forecasts the decisions of the other agents, thereby correctly forecasting the future distribution of savings. As the AMF solution method represents an approximation of the benchmark rational expectations time path iteration (TPI) solution method, we compare the AMF method to the TPI method in terms of both speed and accuracy in a model with enough heterogeneity to match the computational issues in many current life cycle models. We limit our comparisons of the AMF solution method to OLG models because this method does not work in an infinitely lived agent model with idiosyncratic uncertainty.Footnote 3
 As a recent example of the constraints exacted by the computational requirements for equilibrium transition paths in OLG models with a significant degree of heterogeneity and idiosyncratic uncertainty, Nishiyama and Smetters (2007) lament that “[t]he more extensive model contained in this paper requires the addition of another state variable, which significantly increases the...required computation time from several hours to typically several days per simulation.” Even a computation time of several hours makes procedures like forecasting prohibitive if the model must be simulated numerous times in order to approximate the distribution of forecasts. This paper is organized as follows. Section 2 describes a model with \(S\)-period lived agents with heterogeneous stochastic ability and defines both the steady-state equilibrium and the non-steady-state transition path equilibrium. Section 3 describes the benchmark equilibrium TPI solution method, the AMF solution method, and compares the two methods in terms of speed and accuracy. Section 4 concludes.",1
43.0,2.0,Computational Economics,28 July 2013,https://link.springer.com/article/10.1007/s10614-013-9392-9,Forecasting Financial Failure of Firms via Genetic Algorithms,February 2014,Eduardo Acosta-González,Fernando Fernández-Rodríguez,,Male,Male,Unknown,Male,"Bankruptcy forecasting is a recurrent theme in financial literature. It is a problem of obvious practical interest and several researchers involved in the financial analysis field have attempted to show the utility of forecasting models based on published annual accounts. Following Dietrich (1984) there exist two reasons for estimating forecasting models on company failure. On the one hand it is a way of showing if the accounting data provides information about the future solvency of the firm. On the other hand it is a mechanism for forecasting financial failure of firms. One of the most statistical techniques used in failure models is the LOGIT (first used in Ohlson 1980), perhaps only surpassed by the multiple discriminant analysis Altman (1968). However, there are several advantages of the LOGIT model that makes it superior. For an overview of the main advantages and drawbacks of these models see Balcane and Ooghe (2004). Other alternative methods for forecasting financial failure of firms have been proposed with different underlying assumptions and computational complexities, among others we have: Expert systems Messier and Hansen (1988), Nonparametric models: Multivariate Adaptive Regression Splines Friedman (1991), Artificial Neural Networks Tam and Kiang (1992), Hybrid Classifiers for combining previous procedures Olmeda and Fernández (1997) and mixed LOGIT models Jones and Hensher (2004). However, there are a number of common problems related to the application on most of these techniques that haven’t been solved yet. In this paper we try to deal with two of them: the selection of regressors and the missing values. One of the problems of financial failure models is that no economic theory about the firm’s solvency exists. So, there is no consensus in the existing empirical studies as to what the explicative variables capable of explaining bankruptcy are and, the first step of any empirical studies about forecasting bankruptcy is the ratios selection previous to the application of any econometric model. Taking into account that groups of ratios often share the same numerator or denominator, an important problem arises in this first step due to redundancy of the information provided by the ratios and the correlation between variables which can produce multicollinearity problems in the estimations. As it is very well known, multicollinearity produces high standard errors in the estimated coefficients of econometric models which will negatively affect the precision of estimations. But, there doesn’t exist any logical procedure capable of identifying the ratios containing the most complete information, avoiding at the same time its duplication. Normally, the researchers start with a large initial battery of financial ratios and a final set is selected based on statistical considerations. A first consequence of this procedure is that the number of all possible submodels that arise from this battery of variables is high. If we call the number of initial financial ratios \(K\), there are \(2^\mathrm{K}\) possible submodels. For K = 40, the number of possible models is 1,099,511,627,776. The explicit computation of each model is prohibitively expensive. In order to resolve this intractable problem, several heuristic methods addressed to restrict attention to a smaller number of potential subsets of regressors are usually employed by practitioners. Some of the most popular are the stepwise procedures, such as forward selection or backward elimination, which sequentially include or exclude variables based on t-ratio statistic considerations (see Miller 2002 as a review of subset selection in regression). However, stepwise tends to over-identify models. The probability of introducing false significant variables in the model is very high Lovell (1983). In this context other procedures have been proposed for improving the selection of regressors as in Sala-i-Martin (1997), Hoover and Perez  (1999), Hendry and Krolzig (2001), Perez-Amaral et al. (2003) and Acosta-González and Fernández-Rodríguez (2007). The main contribution of this paper is to provide a computational search procedure for forecasting financial failure of firms by selecting the variables included in the econometric model employed. More specifically, given a wide amount of possible financial ratios (with redundant information) available for constructing a LOGIT model, this paper applies a algorithmic procedure for selecting models, choosing a parsimonious model with a reduced amount of financial ratios. For this purpose, a recently developed methodology in Acosta-González and Fernández-Rodríguez (2007) is employed. This procedure is based on a Genetic algorithm (GA, henceforth) guided by the Schwarz information criterion (SIC, henceforth) which is designated by the acronym GASIC. The sensitivity of the GASIC procedure towards the multicollinearity presents good behaviour as demonstrated by the authors using simulation. While in Acosta-González and Fernández-Rodríguez (2007) this technique is used for selecting a linear model, which is estimated by least squared, in order to study financial failure we have adapted GASIC for a LOGIT model which is estimated by maximum likelihood. As far as we know, there is only one other approach to the topic of bankruptcy prediction using GA and it is by McKee and Lensberg (2002); these authors construct a model with four explanatory variables previously selected using a rough sets model and the functional form of the model is finally selected by means of genetic programming. On the contrary, in our work we employ a parsimonious LOGIC model whose variables are selected through the GA from a high set of financial ratios. Another problem that the researchers usually have to deal with in the study of financial failure is missing data. Most of the time they have to eliminate a carefully chosen set of observations (companies) and variables (usually ratios) in order to produce a data matrix without missing values. This procedure leads to a sample selection bias. The solution to the problem of missing data that we propose in this paper is to employ a technique of multiple imputation developed by King et al. (2001). The remainder of this paper has been organized as follows. In Sect. 2 the methodology for selecting the optimal set of financial ratios is introduced and insists on the procedure of selecting variables in a LOGIT model via a genetic algorithm. In Sect. 3 the multiple imputation methodology developed by King et al. (2001) is presented as a solution to the missing values problem in the financial failure data bases. In Sect. 4 the empirical results in the Spanish building industry data base are shown. Finally, Sect. 5 presents the conclusions.",37
43.0,2.0,Computational Economics,13 March 2013,https://link.springer.com/article/10.1007/s10614-013-9366-y,The Optimal Economic Uncertainty Index: A Grid Search Application,February 2014,Pei-Tha Gan,,,Unknown,Unknown,Unknown,Unknown,,
43.0,2.0,Computational Economics,17 March 2013,https://link.springer.com/article/10.1007/s10614-013-9371-1,Forecasting Spanish Unemployment Using Near Neighbour and Neural Net Techniques,February 2014,Elena Olmedo,,,Female,Unknown,Unknown,Female,"Increasingly high unemployment is one of the most pressing problems in many European countries—especially Spain, where one-quarter of the labor force currently is jobless. Given its social and political significance, forecasting unemployment rates is especially important to help policy makers in their decision-making. Support for there being a nonlinear deterministic core in labour market time series comes from Diamond (1982), Brock and Sayers (1988), Alogoskoufis and Stengos (1991), Mortensen and Pissarides (1994) and Neugart (2004). And some papers have empirically documented its asymmetric behaviour, such us Neftçi (1984), DeLong and Summers (1986) and Rothman (1991) documented its steepness, while Sichel (1993) pointed out its deepness and McQueen and Thorley (1993) detected its sharpness. But very few authors have investigated the assumption of linearity, to assess whether the use of nonlinear models are justified (see Koller and Fischer 2002; Panagiotidis 2002; Mikhail et al. 2005; Panagiotidis and Pelloni 2007, confirming in almost all cases the nonlinearity in unemployment rates time series). Once the system is confirmed to be nonlinear, very rich dynamic behaviour possibilities can emerge, including sensibility to initial conditions (chaos). This possibility has been exploited recently in some theoretical studies of the labour market, such us Soliman (1996), Mortensen (1999), Neugart (2004), Fanti and Manfredi (2007), Dufourt et al. (2008), Hallegatte et al. (2008) and Tramontana et al. (2010). But empirical papers about the use of nonlinear and complexity tools in labour market are scarce but extensively used in financial time series (for example, by De Lima 1998; Abhyankar 1997; Belaire-Franch et al. 2002; Bask 2002; Wang et al. 2004; Fabretti and Ausloos 2005; Bonilla et al. 2006; Barkoulas 2008; Bigdeli and Afshar 2009; Kyrtsou et al. 2009; Kodera and Tran 2009; Kyrtsou and Terraza 2010; Guhathakurta et al. 2010). In labour market area, the presence of chaos is confirmed in the Spanish case (Olmedo 2011). This could explain the different and unstable behaviour of Spanish unemployment in comparison with other European countries, and the apparent absence of positive effects of economic policies to combat unemployment. All the discussion above has implications in forecasting. Traditional forecasting techniques are based on linear models, but undoubtedly unemployment rates represent a case study in nonlinear dynamics, so many recent empirical papers about unemployment utilize nonlinear parametric models such us Markov-switching type (Hamilton 1989), the threshold autoregresive type (see Tong 1990; Altissimo and Violante 2001; Caner and Hansen 2001) or the smooth-transition autoregressive (STAR) type (see Granger and Teräsvirta 1993; Johnes 1999; Milas and Rothman 2008). These models assume the presence of different regimes, each of them described by a linear model (Hansen 1997; Koop and Potter 1999; Montgomery et al. 1998; Skalin and Teräsvirta 2002, van Dijk et al. 2000, 2002; Rothman 1998; Peel and Speight 2000; Franses et al. 2004). The main problem of the use of nonlinear models is proper model selection and data-dependency. Taking into account that our proposal is to forecast unemployment rates and conditional on confirming the presence of nonlinearity and unstability in data, we propose the use of nonparametric techniques to forecast the evolution of Spanish unemployment (see Golan and Perloff 2004; Moshiri and Brown 2004). The main advantages of these techniques are their simplicity, flexibility and ability to be applied to any kind of data. And we are not only comparing results concerned with the in-sample fitting of linear and nonparametric nonlinear models, but also are going to examine whether out-of-sample nonlinear forecasts dominate linear ones. We are going to compare two different approaches in forecasting Spanish unemployment: The comprehension approach or phase-space reconstruction (PSR), by means of reconstruction theorem, which is suitable when the unknown underlying system has a low dimension because, in this case, the rebuilt surface is simple enough to be fitted quite approximately and hence we will be able to use the reconstruction for the task of prediction. The learning approach, by virtue of artificial neural networks (ANN), which it is more adequate when the system is not simple enough to be reconstructed. Both approaches use nonlinear functions to model the relationships but the reconstruction approach is a local method because the domain is subdivided in subsets, each one identifying an approximation valid only in this subset. The learning approach is a global method because it uses the whole domain to make forecasts. This paper is structured in two sections. In the first section, we briefly explain the methods, and in the second section, we apply the methods to the forecasting of Spanish unemployment.",10
43.0,2.0,Computational Economics,20 March 2013,https://link.springer.com/article/10.1007/s10614-013-9375-x,Integration of Path-Dependency in a Simple Learning Model: The Case of Marine Resources,February 2014,Narine Udumyan,Juliette Rouchier,Dominique Ami,Female,Female,,Mix,,
43.0,2.0,Computational Economics,26 January 2013,https://link.springer.com/article/10.1007/s10614-013-9360-4,Viable Stabilising Non-Taylor Monetary Policies for an Open Economy,February 2014,Jacek B. Krawczyk,Kunhong Kim,,Male,Unknown,Unknown,Male,"This paper uses viability theory (Aubin 1997), to examine basic problems in monetary policy. For specificity, we use and adapt the model studied in Batini and Haldane (1999a). Our results are delivered numerically using a specialized software application (see Krawczyk and Pharo 2011, 2012).Footnote 1
 Much of the literature that focuses on the problem of setting appropriate monetary policy does so by setting up a dynamic economic model, in which the interest rate is used as the control instrument. Together with a loss function that describes the central bank’s preferences over different economic outcomes, the monetary policy problem reduces to one of determining the optimal specification of the interest rate as a function of other model variables. Regardless of the formalism of these models, or the mechanics of their derivation, the fragility of the resulting optimal interest rate rules to changes in the model structure or to different shock specifications is indisputable. To overcome this fragility, a vast body of literature has focussed on the performance of so-called simple rules in dynamic macroeconomic models. The aim remains to minimise the central bank’s loss function, but now the interest rate is a restricted function of only one or two model variables, typically inflation and some measure of the deviation of output from its “natural” level. These simple rules have been found to be more robust to model and parameter uncertainty than their unconstrained counterparts, and are readily portable from one model to another in a variety of settings.Footnote 2 Depending on their precise specification, these rules often imply only marginally higher losses for the central bank than if it were to follow a reaction policy computed via an unrestricted policy rule. Despite these advantages, simple rules for monetary policy cannot account for the comprehensive information set that central banks actually use to make policy decisions.Footnote 3 Central banks’ charters usually require them to maintain inflation within a certain band—sometimes specified as a point target with a tolerance limit—while having regard to other macro variables such as employment, growth, interest and exchange rate volatility and others. Since their targets are subject to complex definitions, it is natural that the loss functions specified in the standard linear-quadratic optimisation frameworks are also capturing those targets imprecisely. Consequently, the resulting simple rules, with their focus on optimalFootnote 4 outcomes, do not fully reflect the complexities of actual decision-making processes at central banks. This is mostly due to the fact that they do not allow for the possibility of off-model judgment by the policymaker. More importantly, the optimal solutions are often contingent on the precise formulation and parameterisation of the model and are consequently vulnerable to changes in model specification and to parameter uncertainty. These facts are the prime motivation for the application of viability theory, see Aubin (1997) and Aubin et al. (2000), to economic dynamics and, in particular, to an open-economy monetary-policy model in this paper. We propose that viability theory, based on the concept of “good-enough” outcomes, provides a closer approximation to the real monetary-policy decision process than the frequently-used linear-quadratic optimisation framework or the one related to the Taylor rule. In this environment, a variety of goals for policy can be readily admitted, which do not have to be precisely defined—either separately or jointly. Furthermore, we do not seek to impose the artificiality of requiring that all outcomes always be (numerically) comparable to one another. In short, viability theory, a relatively young area of continuous mathematics, appears to rigorously capture the essence of good-enough or satisficing control.Footnote 5
 We have previously applied viability theory to the monetary policy problem in a closed-economy setting, see Krawczyk and Kim (2009) and Krawczyk and Sethi (2007). The defining characteristic of these papers was that the model economy was closed to external influences via trade, capital flows or the exchange rate. In this paper, we apply viability theory to the analysis of central bank’s interest rate policies when the model economy features the exchange rate as an extra variable.Footnote 6
 The introduction of the exchange rate to the models previously studied by us obviously opens up a second channel for the transmission of monetary policy to the real economy in addition to interest rates. This duality undoubtedly makes the model more realistic, but in doing so introduces more complications for the determination of monetary policy. In particular, the exchange rate introduces an extra avenue through which shocks can affect the economy. Independent of the type of economy modelled, viability-based policies do not require that explicit weights be placed on the relative importance of the bank’s variables of interest. Furthermore, viability-based policies explicitly constrain the speed at which the controls (e.g.,  interest rates) can be adjusted. These constraints can either be modal ones (imposed by reality), or normative ones. This second possibility means that where a policy-maker wishes to avoid making sudden changes to a policy instrument, a limit can be “baked into” the model specification. This in turn allows the model to produce strategies that are precautionary in that they advise avoidance of “high speed” economic states (like a spiraling inflation). This makes the viability-based policies precautionary in that they advise avoidance of “difficult” economic states. The two features of viability-based strategies: independence of the explicit weights and the precautionary nature, mean that they have the potential to be more robust than their opimization-based counterparts, mainly because they are less likely to do damage if the policy-maker is wrong about some aspects of the environment. Here we say how this paper is organised. In Sect. 2, we provide a brief introduction to viability theory and, in Sect. 3, we explain what kind of policy rules might be obtained using a viability approach. Then, in Sect. 4, we calibrate a simple open-economy monetary policy model.Footnote 7 The model constraints are described in Sect. 5. The main results of the paper, together with an introduction to a specialised piece of software called VIKAASA useful for viability kernel determination are provided and discussed in Sects. 6 and 7. The paper ends with concluding remarks.",2
43.0,2.0,Computational Economics,23 October 2013,https://link.springer.com/article/10.1007/s10614-013-9402-y,Erratum to: Viable Stabilising Non-Taylor Monetary Policies for an Open Economy,February 2014,Jacek B. Krawczyk,Kunhong Kim,,Male,Unknown,Unknown,Male,"Unfortunately in the original publication, the year of reference in the abstract for Krawczyk and Kim (Macroecon Dyn 13(1):46–80, 1999) has been incorrectly stated. The correct publication year should be 2009. The corrected penultimate sentence in the abstract should read as “We also discuss the robustness of such policies to shocks and parameter uncertainty and observe that viability-based policies come from models, which do not require explicit weights on the variables of interest of a central bank.”",
43.0,3.0,Computational Economics,16 March 2013,https://link.springer.com/article/10.1007/s10614-013-9372-0,Utility-based Multi-agent System with Spatial Interactions: The Case of Virtual Estate Development,March 2014,Dominique Prunetti,Alexandre Muzy,Xavier Pieri,,Male,Male,Mix,,
43.0,3.0,Computational Economics,09 March 2013,https://link.springer.com/article/10.1007/s10614-013-9369-8,Building Technical Trading System with Genetic Programming: A New Method to Test the Efficiency of Chinese Stock Markets,March 2014,Hui Qu,Xindan Li,,,Unknown,Unknown,Mix,,
43.0,3.0,Computational Economics,17 March 2013,https://link.springer.com/article/10.1007/s10614-013-9373-z,Symbolic ARMA Model Analysis,March 2014,Keith H. Webb,Lawrence M. Leemis,,Male,Male,Unknown,Male,"Many problems in time series analysis rely on approximate values from Monte Carlo simulations or the central limit theorem rather than exact results. The computer algebra system Maple and the APPL (A Probability Programming Language) package can calculate exact results that would be impractical to compute by hand (Glen et al. 2001). This paper describes our time series extension to APPL which can compute autocorrelation and partial autocorrelation functions of ARMA (autoregressive moving average) models which would otherwise require extremely tedious pencil and paper work or simulation to find, along with several other tools for working with ARMA models. Simulation is not used in any of the results.Footnote 1
 The time series extension to APPL provides procedures for ARMA models to: calculate an autocorrelation (TSCorrelation), including autocorrelations of models with heteroskedastic error terms, calculate a partial autocorrelation (TSPartialCorrelation), calculate a mean (TSMean), calculate a variance or covariance (TSVariance), plot an autocorrelation or partial correlation function (TSPlot), calculate a spectral density function (SDF), perform a unit roots analysis to determine whether or not an AR(\(p\)) model is stationary or an MA(\(q\)) model is invertible (UnitRoots), forecast an AR(\(p\)) model and calculate a confidence interval (TSForecast), generate a realization of a process (TSRealization), perform an exploratory time series analysis (ETSA) which displays several of the previous commands at once. Section 2 describes the data structure for storing ARMA models and explains the ways that the mean, variance, autocorrelation, partial autocorrelation and forecasts are computed. Section 3 illustrates how to use the time series extension through a series of examples. An AR(\(p\)) model has \(p\) autoregressive components, \(0\) moving average components and is defined by where \(Y_{t}\) is the value of the time series at time \(t, c\) is a real-valued constant, \(\phi _{1}, \phi _{2}, \cdots , \phi _{p}\) are real-valued parameters, and \(\epsilon _{0},\epsilon _{1},\epsilon _{2},\cdots ,\epsilon _{t}\) are error terms, typically mutually independent and normally distributed with a mean of \(0\). An MA(\(q\)) model has \(0\) autoregressive components, \(q\) moving average components and is defined by with variables defined similarly. An ARMA(\(p, q\)) model is a combination of (1) and (2): One aspect of analyzing ARMA (and other time series) models is finding autocorrelation functions, that is, the \(j^\mathrm{{th}}\) autocorrelation, denoted by \(\rho _{j}\), is the correlation between the value of a time series \(Y_{t}\) at time \(t\) and its value \(Y_{t-j}\) at time \(t-j\). For models where the \(\epsilon _{t}\) terms are independent and identically distributed (IID), \(\rho _{j}\) is the same at any time \(t\). The \(j^\mathrm{{th}}\) autocorrelation of MA(\(q\)) models with \(q<\infty \) drops off to \(0\) when \(j>q\) because \(Y_{t}\) and \(Y_{t-j}\) do not share terms. On the other hand, the \(j^\mathrm{{th}}\) autocorrelation of stationary AR(\(p\)) models asymptotically approaches \(0\) as \(j\) increases because the value of \(Y_{t}\) affects all future values, but its impact decreases over time. Finding the exact autocorrelation function for an ARMA model can be difficult to do by hand, especially when error terms are not IID. Section 2 explains two ways that this process is automated with Maple and APPL. APPL is essential for working with various distributions of error terms. For example, suppose that \(\epsilon _{t}\sim \mathrm {exponential}(4)\) and we want to find \(E[\epsilon _{t}^{2}]\). APPL can find the exact solution quickly:  The Mean, Variance, and Transform procedures, as well as the data structure for probability distributions in APPL, are all used when working with ARMA models. This enables our software to use an arbitrary probability distribution for the error terms. For more on APPL, see Glen et al. (2001).",5
43.0,3.0,Computational Economics,17 March 2013,https://link.springer.com/article/10.1007/s10614-013-9370-2,Valuation of R&D Investment Opportunities with the Threat of Competitors Entry in Real Option Analysis,March 2014,Giovanni Villani,,,Male,Unknown,Unknown,Male,"In recent years, academic studies have argued that traditional evaluation approaches such as internal rate return (IIR) and net present value (NPV), can not adequately capture the value of an investment project. In fact, these approaches take the viewpoint of passive decision makers who do not dynamically respond to the changing investment environment. Therefore, the real options approach becomes very important both to give explicit valuations of managerial flexibility to grow, delay, scale down or abandon projects and to support management in its decisions (Brandão and Dyer 2005; Smith 2003). The main difference between the NPV rule and real options approach is that while the first considers uncertainty as a risk to be minimized, the second takes uncertainty as an opportunity for maximizing the value of a project. In real options, the options involve real assets as opposed to financial ones. So the project can be valued quantitatively using the analogue option pricing theory as developed in modern finance. In this case, optimal exercise strategies can be derived without consideration of strategic interaction across option holders. However, it is widely acknowledged that a competitive environment may have a considerable impact on the valuation of real options. Under the threat of competition, the exercise of options strategically depends on the trade-off between the benefits and costs of going ahead with an investment against waiting for more information. Waiting can have an informational benefit (McDonald and Siegel 1985; Weeds 2002). However, if a firm chooses to defer exercising its option until better information is received (thus resolving uncertainty), it runs the risk that other firms may preempt it by exercising first as it is shown in Fudenberg and Tirole (1985), Lambrecht and Perraudin (2003), Thijssen (2010), and so on. Such an early exercise by a competitor can erode the profits or even force the option to expire prematurely. Most real-life preemptive situations are often solved by a deterministic timing game approach, as is presented in Grenadier (1996), Huisman (2001) and so on. In these papers the problem is, basically, the following: two competing firms have the option to invest and enter in a duopoly market; the first company to do so (the leader or pioneer) may benefit, after investing, from a temporary or permanent competitive advantage over the other firm (the follower), by securing, for example, a higher market share. In addition, the potential competitors’ actions are modeled as Poisson jumps (see Armada et al. 2011). In our paper a different approach to incorporate the preemption and the competitive dimension in a real options model is taken. In real market terms, dividends may represent several types of opportunity costs caused by holding the real option unexercised, namely the loss of project’s cash flows when an investment opportunity is postponed as it shown in Majd and Pindyck (1987). In particular way, Trigeorgis (1996) incorporates the preemption effect by adding a positive value in the dividend yield parameter. This additional dividend is a “competitive dividend”  which is lost by the owner of real option. The aim of this paper is to propose a real option approach for the valuation of a pioneer R&D investment opportunity and the analysis of the optimal timing to realize the Development cost assuming that potential rivals may enter in the market. Specifically we assume that, if the Development cost is realized in \(t_2\) no firms enters in the market since the rivals’ R&D plan is not yet concluded otherwise, if the Development cost is delayed until time \(t_3\) waiting better market conditions, other rivals may enter in the market and so to erode cash flows. We differentiate from Trigeorgis (1996) for two important reasons. Firstly, we model the R&D projects using exchange options in which investments present uncertainty both in the gross project value and in costs. The most important valuation models of exchange options in continuous time are given in Margrabe (1978), McDonald and Siegel (1985), Carr (1988), Carr (1995) and Armada et al. (2007). In particular way, McDonald and Siegel (1985) and Margrabe (1978) value a simple European exchange option (SEEO), Carr (1988) develops a model to price a compound European exchange option (CEEO) and Carr (1995) and Armada et al. (2007) present the pricing of a simple American exchange option (SAEO). However, if we consider the possibility that the Development investment can be realized in two moments (\(t_2\) and \(t_3\)), the R&D investment opportunity can be priced as pseudo compound American exchange option (PCAEO). In this case, traditional Monte Carlo simulation is a powerful and flexible tool for capital budgeting decisions as it is is witnessed in Cortazar (2001), Cortelezzi and Villani (2009), Gamba (2003) and so no. Secondly, we consider that the pioneer R&D investment generates an information revelation that influences the investment decision of other firms as it is shown in Dias (2004). As it is well know, R&D investments are made often in a phased manner, with the commencement of subsequent phase being dependent on the successful completion of the preceding phase. This is known as sequential investment. Therefore, R&D projects can be considered as compound options in which each stage provides information about the R&D success. The structure of this paper is organized as follows. In Sect. 2, we present the option model to value the R&D investment opportunity of a pioneer firm assuming that other rivals may enter in the market. First of all, we analyze the case with a single and two potential rivals and, after that, we propose a general case with the threat of \(k\) potential competitors. We use a Monte Carlo approach to value the pioneer R&D opportunity. In Sect. 3 we propose some numerical applications showing as the pioneer’s R&D opportunity value depends both on the number of potential rivals that may erode cash-flows and on the information revelation. In Sect. 4 we realize a sensitivity analysis with respect to the other fundamental parameters of our model. Finally, Sect. 5 concludes.",7
43.0,3.0,Computational Economics,03 May 2013,https://link.springer.com/article/10.1007/s10614-013-9379-6,Sticky Information Models in Dynare,March 2014,Fabio Verona,Maik H. Wolters,,Male,Male,Unknown,Male,"This paper illustrates how DynareFootnote 1 can be used to solve and simulate rational expectations models with lagged expectations. In particular, this work might be of interest to economists interested in implementing sticky information models á laMankiw and Reis (2002) in Dynare. Sticky information models have been developed as an alternative to the widely used Calvo sticky price framework. Agents update their information set infrequently and therefore adjust their decisions with a delay to the occurrence of shocks. These models have a number of interesting implications. First, they can generate hump-shaped and delayed inflation reactions in response to monetary policy shocks. Second, in contrast to the standard New Keynesian model announced and credible disinflations require a decrease in real economic activity. Finally, the change in inflation is positively correlated with the level of economic activity. While these models are appealing from an economic point of view, they introduce a number of technical difficulties. Sticky information models include infinite sums of lagged expectations terms. Mankiw and Reis (2007) have developed an algorithm that uses a method of undetermined coefficients and exploits the recursiveness of the model’s dynamics to efficiently solve this class of models. Wang and Wen (2006) transform variables with lagged expectations into their forecast errors and can thus keep the model’s state space at its minimum. The most general algorithm has been developed in Meyer-Gohde (2010). His solution method combines a state-space and an infinite-MA representations with a simple system of linear equations. The method is more general and faster than the original algorithm by Mankiw and Reis (2007). Most closely related to our approach is the strategy pursued in Trabandt (2007). He approximates the infinite state space with a finite state space by truncating the number of lagged expectations. He starts with a minimal number of lagged expectations terms and adds sequentially more terms until the coefficients of the recursive equilibrium law of motion change by less than a prespecified tolerance. We use Dynare, which is probably the most widely used software package for applied work with DSGE models, to solve sticky information models. We approximate the infinite state space with a finite version by truncating the lagged expectation terms. We show that the solution to the models by Mankiw and Reis (2002) and Reis (2009) is reasonably precise for a moderate number of lagged expectation terms. We demonstrate how the Dynare macro language can be used to quickly adjust the truncation point. Dynare requires more time to solve the model than the more specialized algorithms mentioned above. While for example the algorithm in Mankiw and Reis (2007) takes less than a second on a standard computer, the implementation in Dynare takes about 20 seconds due to the increased state space. However, 20 seconds is still a feasible amount of time for simulating models as well as for analysing different policy scenarios or computing optimal policy. Tools to conduct such analysis are already contained in the Dynare software package and can be used directly. Another advantage of using Dynare is that models can be simulated with higher order approximations, which is not possible with the specialized solution algorithms discussed above. Regarding the estimation of sticky information models, for maximum likelihood estimation and Bayesian estimation — where a model needs to be solved many times to simulate the posterior distribution — researchers will need to use a more specialized software package, such as the one developed by Meyer-Gohde (2010). For estimation, it is necessary to compute the ergodic variance to initialize the Kalman filter. This is actually one of the most time consuming parts using Dynare, and it makes estimation of sticky information models with Dynare unfeasible. The remainder of the paper is organized as follows. Section 2 introduces the sticky information Phillips curve from the Reis (2009) sticky information general equilibrium model. Section 3 shows how to implement this equation in Dynare. In Sect. 4 we analyse the accuracy and computational times of simulations computed with Dynare. Section 5 concludes. Appendix A provides a short description of the model equations of Reis (2009). Appendix B lists the Dynare and Matlab files that can be used to reproduce the results in this paper. These files are provided in the SImodels.zip file available on the authors’ and Ricardo Reis’ websites.",2
43.0,3.0,Computational Economics,12 March 2013,https://link.springer.com/article/10.1007/s10614-013-9367-x,Simulation Analysis for Network Formulation,March 2014,Tomohiro Hayashida,Ichiro Nishizaki,Rika Kambara,Male,Male,Female,Mix,,
43.0,4.0,Computational Economics,19 February 2014,https://link.springer.com/article/10.1007/s10614-014-9425-z,Simulation Estimation of Dynamic Panel Discrete Choice Models Using the \(t\) Distributions,April 2014,Sheng-Kai Chang,,,,Unknown,Unknown,Mix,,
43.0,4.0,Computational Economics,27 June 2013,https://link.springer.com/article/10.1007/s10614-013-9390-y,An Abductive-Reasoning Guide for Finance Practitioners,April 2014,Rua-Haun Tsaih,Hsiou-Wei William Lin,Wen-Chyan Ke,Unknown,Unknown,Unknown,Unknown,,
43.0,4.0,Computational Economics,09 March 2013,https://link.springer.com/article/10.1007/s10614-013-9368-9,An Efficient Semi-Analytical Simulation for the Heston Model,April 2014,Xianming Sun,Siqing Gan,,Unknown,Unknown,Unknown,Unknown,,
43.0,4.0,Computational Economics,31 March 2013,https://link.springer.com/article/10.1007/s10614-013-9377-8,Nelder-Mead Simplex Optimization Routine for Large-Scale Problems: A Distributed Memory Implementation,April 2014,Kyle Klein,Julian Neira,,,Male,Unknown,Mix,,
43.0,4.0,Computational Economics,06 March 2013,https://link.springer.com/article/10.1007/s10614-013-9361-3,A Robust Numerical Scheme For Pricing American Options Under Regime Switching Based On Penalty Method,April 2014,K. Zhang,K. L. Teo,M. Swartz,Unknown,Unknown,Unknown,Unknown,,
43.0,4.0,Computational Economics,20 March 2013,https://link.springer.com/article/10.1007/s10614-013-9362-2,Heterogeneous Computing in Economics: A Simplified Approach,April 2014,Matt P. Dziubinski,Stefano Grassi,,Male,Male,Unknown,Male,"This paper shows the potential of heterogeneous computing in solving dynamic equilibrium models in economics. Heterogeneous computing refers to the use of different processing cores (types of computational units) to maximize performance.Footnote 1
 We rely on C++ Accelerated Massive Parallelism (C++ AMP), a new technology introduced by Microsoft, that allows to use accelerators, such as graphics processing units (GPUs), to speed up calculations. GPUs are a standard part of the current personal computers and are designed for data parallel problems, where we assign an individual data element to a separate logical core for processing, applications include graphics, video games, and image processing.Footnote 2 The video card technology has experienced a rapid development mainly driven by 3D rendering (with notable applications in the video game industry), see Boyd (2008). Table 1 reports the GigaFLOPS (i.e., billions of floating point operations per second) for the GPU and the CPU used in this study. In February 2012, Microsoft released C++ AMP as an extension of Visual Studio 2012 which allows to use the massively parallel and heterogeneous hardware available nowadays. Microsoft is not the only entity active in parallel computation, there are at least two other approaches: the Compute Unified Device Architecture (CUDA) of NVIDIA and the Open Computing Language (OpenCL) of the Khronos Group. The CUDA (or, more precisely, C for CUDA) approach is an extension of C and although very fast, as documented in Aldrich et al. (2011), it suffers from vendor lock-in, since it only works with NVIDIA GPUs and cannot be used with, for instance, ATI GPUs.Footnote 3 OpenCL is designed for heterogeneous parallel computing and since it is not tied to any specific video card manufacturer it can work on NVIDIA and ATI GPUs. The main disadvantage thereof is that OpenCL is based on C rather than C++ and, consequently, involves significantly more manual intervention and low-level programming, see Aldrich et al. (2011) for a discussion. C++ AMP, in contrast, simply requires a generic GPU and it abstracts the accelerators for the user. In other words, when using C++ AMP, the user does not need to worry about the specific kind of accelerator available in the system. More importantly, the programmer does not need to know or use manual memory management (different for each GPU), since it is performed automatically. This allows to write more natural and high-level programs. Another big advantage of C++ AMP is its flexibility, boosting developer productivity. It is not necessary to write two different implementations of the same program: one for the NVIDIA GPUs (e.g., using CUDA) and another one for the AMD GPUs (e.g., using OpenCL). It is enough to write one general implementation, since C++ AMP automatically adapts to the specific hardware available on the target machine. In the case of multiple GPUs in the system (e.g., one integrated within the CPU system and another one discrete), they can be used simultaneously, even if they come from different vendors (e.g., an ATI GPU, an Intel GPU, and an NVIDIA GPU). To date, the adoption of GPU computing technology in economics and econometrics has been relatively slow compared to other fields, see Morozov and Mathur (2012) for a literature review. This is somewhat surprising, given the fact that there is a large number of economic and econometric applications where parallel computing can be applied, see Creel and Goffe (2008). The low diffusion of this technology in the economics and econometrics literature, according to Creel (2005), is related to the steep learning curve of a dedicated programming language and expensive hardware. Modern GPUs can easily solve the second problem (hardware costs are relatively low), but the first issue still remains open. We think that C++ AMP is a solution to this problem and that it can help to promote and spread parallel programming in particular and heterogeneous programming in general in the economics and econometrics community. To show the potential of this approach we replicate the exercise of Aldrich et al. (2011), which uses the value function iteration (VFI henceforth), an algorithm easy to express as a data parallel computational problem, to solve a dynamic equilibrium model. We find that using VFI with binary search the (optimized) CUDA program is slower than the naive (unoptimized) implementation using C++ AMP. C++ AMP is also faster in a grid search with a Howard improvement step. In both cases the C++ AMP program is almost 3 times faster than the CUDA one. The rest of the paper is organized as follows. Section 2 provides a simple example that shows the simplicity of the proposed approach compared to CUDA. Section 3 describes the basic idea of parallelization and heterogeneous programming applied to VFI. Section 4 presents the RBC model used in Aldrich et al. (2011). Section 5 reports the time comparison between the two approaches. Section 6 concludes. A complete example of the C++ AMP programming style is presented in the Appendix.",11
43.0,4.0,Computational Economics,10 August 2013,https://link.springer.com/article/10.1007/s10614-013-9394-7,Openness and Technology Diffusion in Payment Systems: The Case of NAFTA,April 2014,Francisco J. Callado-Muñoz,Jana Hromcová,Natalia Utrero-González,Male,Female,Female,Mix,,
44.0,1.0,Computational Economics,17 May 2013,https://link.springer.com/article/10.1007/s10614-013-9382-y,"Utility-Based Pricing, Timing and Hedging of an American Call Option Under an Incomplete Market with Partial Information",June 2014,Dandan Song,Zhaojun Yang,,Unknown,Unknown,Unknown,Unknown,,
44.0,1.0,Computational Economics,24 May 2013,https://link.springer.com/article/10.1007/s10614-013-9384-9,A Neo-institutionalist Model of the Diffusion of IFRS Accounting Standards,June 2014,Dominique Dufour,Pierre Teller,Philippe Luu,,Male,Male,Mix,,
44.0,1.0,Computational Economics,28 July 2013,https://link.springer.com/article/10.1007/s10614-013-9391-x,Loss-Aversion with Kinked Linear Utility Functions,June 2014,Michael J. Best,Robert R. Grauer,Xili Zhang,Male,Male,Unknown,Male,"The traditional finance paradigm, which underlies portfolio theory and asset pricing models, assumes “economic rationality”. Investors are assumed to know their risk attitudes, captured by strictly increasing, smooth and concave utility functions, and beliefs about the probability distribution of returns, wealth or consumption. However, experimental evidence indicates that individuals do not obey the expected utility axioms. Based on this evidence, Kahneman and Tversky (1979) and Tversky and Kahneman (1992) develop prospect theory, a descriptive theory of how individuals make economic choices. The most important aspect of the prospect theory utility function is that it is characterized by a kink (a point of non-differentiability) between “gains” and “losses”, where there is greater sensitivity to losses than to gains. This feature is known as loss aversion. In addition, individuals apply a nonlinear transformation of probabilities of outcomes, overestimating the probabilities of large gains and losses. Based on experimental evidence Tversky and Kahneman (1992) estimate that the loss-aversion coefficient is 2.25. Although this \(S\)-shaped utility function is well known, there is little research on how it affects portfolio choice. Somewhat surprisingly the most promising portfolio selection research focuses on how prospect theory might help explain aggregate economic phenomena. Benartzi and Thaler (1995) calculate the optimal bond-stock portfolio of an investor who is loss averse over changes in financial wealth. They find that the main aspect of prospect theory relevant for portfolio choice is loss aversion. The concavity (convexity) of the value function on the upside (downside) is only of second-order importance. Calculations of an optimal bond-stock portfolio with annual data provide an explanation for the high equity premium and support a loss-aversion coefficient consistent with Tversky and Kahneman’s experimental based estimate. Barberis et al. (2001) extend Benartzi and Thaler’s framework studying asset pricing in a dynamic equilibrium model where investors derive direct utility from consumption (captured by a smooth concave function) and from fluctuations in financial wealth captured by a linear loss-averse function that incorporates changes in risk aversion arising from gains or losses. They find that the model captures a number of aggregate market phenomena, i.e., the high mean, high volatility, and significant predictability of equity returns. However, they consider only one risky asset and base their analysis on special distributional assumptions. Barberis and Huang (2001) examine the case of an additive linear loss-aversion function that incorporates gains or losses for individual investments rather than for an optimized portfolio. Grüne and Semmler (2008) extend the analysis further building on the framework where investors derive direct utility from consumption and a linear loss-averse function defined on wealth. 
Berkelaar et al. (2004) analyze the optimal investment strategy for loss-averse investors, assuming a complete market, general Itô processes for asset prices, as well as a constant interest rate and market price of risk. In this framework, they provide closed-form solutions for the optimal portfolios for the Kahneman–Tversky kinked-utility function and for a kinked power-utility function. Cremers et al. (2005) use full-scaled optimization where optimal allocations are found by means of a grid search method. The advantage of the full-scale optimization approach is that it can solve non-convex programming problems, e.g., problems with \(S\)-shaped utility functions when investors are risk averse to the right of the reference point and risk seeking to the left of the reference point. Its disadvantage is that it can be computationally too cumbersome when more assets and constraints are involved in spite of the immense technological development during the last decades. De Giorgi et al. (2007) solve the problem with \(S\)-shaped utility function by approximating it with a cubic spline around zero (the point of non-differentiability) for smoothing, and then use the MINOS solver for the non-convex programming problem. They also show that the resulting asset allocations are still similar by comparing piecewise linear and the piecewise power \(S\)-shaped function. The primary purpose of this paper is to present an algorithm that efficiently solves the linear version of the prospect theory investment problem for an arbitrary number of assets, discrete return distributions and general linear constraints. The algorithm allows Best and Grauer (2011a, b) to investigate whether the experimental evidence of Tversky and Kahneman (1992) and the promising portfolio selection-asset pricing results of Benartzi and Thaler (1995) and Barberis et al. (2001) carry over to portfolio selection scenarios with more risky assets, discrete-distributions and realistic constraints. The algorithm proves its worth in Best and Grauer (2011a, b) who find that many of the conclusions concerning prospect theory portfolio selection drawn from two asset-problems and specific distributional assumptions do not hold in more general settings and that the characteristics of prospect theory, power-utility and mean-variance of portfolios may be strikingly different. In applied finance, loss-aversion is modeled by downside risk measures like value-at-risk and conditional value-at-risk, where these measures have not been derived from an investor’s utility maximizing problem. However, loss-aversion can also be captured by a utility function displaying a kink at some reference point, where the jump in marginal utility distinguishes “gains” from “losses”. Therefore, we formulate the problem in a general way that allows us to provide a direct link between a piece-wise linear programming (LP) problem and conditional value-at-risk and to view the linear version of prospect theory utility as a special case. Suppose we have \(S\) states (scenarios or time periods) and \(n\) assets. The problem we would like to solve is the followingFootnote 1
 where \(x=(x_1,\ldots ,x_n)^{\prime },\,x_i\) denotes the proportion of wealth invested in asset \(i,\,i=1, \ldots , n,\,r_s\) is the \(n\)-vector of returns for state \(s\) and \(\pi _s\) is the probability of state \(s\) where \(\pi _s \ge 0\) and \(\sum _{s=1}^S \pi _s =1\).Footnote 2 Thus, \(r_s^{\prime }x\) represents the return of portfolio \(x\) at state \(s,\,s=1, \ldots , S.\)
\(A\) is an \((m,n)\) matrix and \(Ax \le b\) are the normal asset constraints including the budget, non-negativity, margin constraints, etc.Footnote 3
 The linear version of the prospect theory utility function is a special case with \(f_s(\cdot )=f(\cdot )\) and with \([t]^+\) denoting the maximum of 0 and \(t,\) and \(y, \hat{y} \in \mathbb R ,\) where \(\hat{y}\) is the (given) reference point. The penalty parameter \(\lambda >0\) is such that higher \(\lambda \) is associated with a higher degree of loss-aversion and thus for \(\lambda >0\) the bilinear utility function is steeper below the reference point than above it. The investor faces a trade-off between return on the one hand and shortfall below the benchmark level \(\hat{y}\) on the other hand. The utility function thus contains an asymmetric or downside risk measure where losses are weighted differently from gains, which reflects the feature that investors are more sensitive to losses than to gains. Sutter (2007) and Kliger and Levit (2009) also employ the bilinear form of the utility function (2) to motivate their experimental investigations of myopic loss aversion. 
Fourer (1985, 1988, 1992) derived a method for solving piecewise-LP problems, which is an extension of the simplex method for LP. In this paper, we develop a method which works with untransformed original variables as does Fourer’s, but in addition, our method efficiently uses the structure of the original problem (1)–(2). The paper proceeds as follows. In Sect. 2 we derive the optimality conditions for the general bilinear utility functions in (1)–(2), transforming the kinked linear-utility problem into a higher dimensional linear program. The advantage of this is that the linear program is differentiable. The disadvantage is the dimension of the linear program is quite large making it computationally expensive to solve. We address this higher dimensional problem in Sect. 3 by formulating an efficient solution algorithm which solves the problem in a smaller dimensional space. In Sect. 4 we employ a numerical example to show that solving the problem with our algorithm is 15 times faster than solving the higher dimensional linear program using the interior point method of Mosek. In Sect. 5 we provide a direct link between piece-wise LP problem introduced in Sect. 2 and the conditional value-at-risk problem. Section 6 concludes.",8
44.0,1.0,Computational Economics,29 November 2013,https://link.springer.com/article/10.1007/s10614-013-9410-y,Minimizing Geographical Basis Risk of Weather Derivatives Using A Multi-Site Rainfall Model,June 2014,M. Ritter,O. Mußhoff,M. Odening,Unknown,Unknown,Unknown,Unknown,,
44.0,1.0,Computational Economics,21 May 2013,https://link.springer.com/article/10.1007/s10614-013-9386-7,A Non-parametric Test for Partial Monotonicity in Multiple Regression,June 2014,Misha van Beek,Hennie A. M. Daniels,,Male,Female,Unknown,Mix,,
44.0,1.0,Computational Economics,28 September 2013,https://link.springer.com/article/10.1007/s10614-013-9400-0,On the Market Selection Hypothesis in a Mean Reverting Environment,June 2014,Emilio Barucci,Marco Casna,,Male,Male,Unknown,Male,"According to the market selection hypothesis, markets reward rational agents rather than irrational agents, see Alchian (1950), Friedman (1953). Rationality stands for two main features: (expected) profit/utility maximization, and correct beliefs. The claim is motivated by a Darwinian argument: if agents do not maximize profits or have incorrect beliefs, then their default probability is high and therefore they will disappear in the long run. In this paper we concentrate on the possibility that agents with incorrect beliefs may survive when the endowment of the economy follows a mean reverting process. Considering different hypotheses on the economy and on agents’ beliefs we show that the market selection hypothesis is a robust result: those with incorrect beliefs do not survive in the long run. We provide a general equilibrium analysis of a continuous time complete market economy with rational and irrational agents. Rational agents exactly know the endowment process, irrational agents instead have a wrong opinion on its level. Both classes of agents maximize the expected utility of the consumption stream over an infinite horizon. Large part of the literature on the survival of irrational traders deals with a lognormal diffusion process, see Kogan et al. (2006), Yan (2008). This assumption is not fully confirmed by the data. As a matter of fact, the empirical literature on asset prices has shown that the expected growth rate of the endowment is not constant over time and that there is evidence of mean reversion, e.g., see Cecchetti et al. (1990), Campbell and Kyle (1993), Ang and Timmermann (2011). Considering an endowment with or without growth, and controlling for the discount factor, we show that irrational agents disagreeing on the level of the mean reverting process, and not on the rate of growth of the economy, cannot survive in the long run. We assume that agents have constant relative risk aversion preferences, but the result is confirmed with a constant absolute risk aversion utility, i.e., when relative risk aversion is unbounded. We consider three different specifications of the endowment process and of beliefs, in all the cases we prove the result. Differently from a lognormal endowment economy, we show that in a mean reverting environment only discount factor heterogeneity allows for the survival of irrational agents. Irrational agents with a low enough discount factor compared to that of rational agents may survive: the higher is their opinion mistake, the lower should their discount factor be. The market selection hypothesis has not been rigorously demonstrated for a long time. De Long et al. (1990, 1991) have provided a negative result showing that overconfident irrational traders may survive and rational traders disappear. Their analysis is based on several assumptions that limit the result: the equilibrium analysis is partial and asset returns are exogenous, i.e., asset prices are not affected by agents’ behavior. Sandroni (2000), Blume and Easley (2006) have provided a formal proof of the market selection hypothesis in a general framework: they develop a general equilibrium analysis with intermediate consumption and endogenously determined asset prices showing that in a complete market model with positive dividends bounded from above, and controlling for the discount factor, only traders with correct beliefs survive in the long run. Agents’ preferences (risk aversion) do not affect the result, only the discount factor matters. Irrational agents with a discount rate lower than that of rational agents may survive: because of incorrect beliefs, they are likely to take bad investment decisions ant to save more, today, but they will consume more in the future and therefore they may survive. This result points out the limits of previous results but does not represent the full picture. Boundedness of the endowment process is a strong assumption. Assuming an economy growing without limit, the market selection hypothesis does not hold in general. Kogan et al. (2006), Yan (2008) have shown this result assuming that the endowment evolves as a geometric continuous time Brownian motion and that irrational traders have incorrect beliefs on the drift of the economy: the expectation of the endowment goes as an exponential function and agents disagree on the exponential coefficient, see also Hirshleifer and Luo (2001), Dumas et al. (2009), Branger et al. (2007) on the survival of overconfident agents. Assuming consumption only at the final date, Kogan et al. (2006) have shown that moderately optimistic irrational agents (they overestimate the rate of growth of the aggregate endowment) may dominate the market in the long run and rational traders may disappear. Yan (2008) also considers intermediate consumption and shows that agents’ preferences matter: survival depends on agents’ risk aversion and on their beliefs about the growth rate of the economy. If all investors have the same time discount rate and the same risk aversion coefficient, then only rational investors survive in the limit. Instead, in the presence of heterogeneous discount factors and/or of heterogeneous risk aversion coefficients with growth, irrational agents may survive: irrational agents with a low enough discount rate or high enough elasticity of intertemporal rate of substitution (inverse of the coefficient of relative risk aversion) save more today and therefore are more likely to survive in the future. Kogan et al. (2009) provide a general result showing that if relative risk aversion is unbounded and forecast errors do not disappear fast enough compared to the growth rate of risk aversion, then agents with less accurate forecasts may survive. On the other hand, if the agents’ relative risk aversion is bounded or the aggregate endowment is bounded and beliefs are significantly different in the long run, then irrational traders will disappear. Our contribution to this literature is twofold. First of all, confirming Kogan et al. (2009), we show that the boundedness assumption is not a necessary condition to establish extinction of irrational traders: we provide examples of economies with an unbounded endowment and extinction of irrational traders. Second, the endowment dynamics and the type of disagreement matter: in a mean reverting economy we show that irrational traders with wrong beliefs on the level of the economy and not on the rate of growth cannot survive. According to the analysis provided in (Kogan et al. 2009, Corollary 4.2), a non correct belief on the mean reverting level induces a significant disagreement in the long run among the agents and therefore irrational agents do not survive. It seems that agents should disagree on the rate of growth of the economy to observe the survival of irrational traders. The paper is organized as follows. In Sect. 2 we present the model for the endowment of the economy and we specify agents’ beliefs. In Sect. 3 we develop the equilibrium analysis of the economy with heterogenous beliefs deriving asset prices and consumption processes. In Sect. 4 we analyze the survival of irrational agents assuming homogeneous discount factors. In Sect. 5 we analyze the survival of irrational agents assuming heterogeneous discount factors. In Sect. 6 we extend our analysis to constant absolute risk aversion utility functions.",3
44.0,2.0,Computational Economics,16 October 2013,https://link.springer.com/article/10.1007/s10614-013-9401-z,Combining Forecasts with Missing Data: Making Use of Portfolio Theory,August 2014,Björn Fastrich,Peter Winker,,Male,Male,Unknown,Male,"Many decisions in the financial sector require knowledge about future realizations of financial quantities such as a stock market index or at least their variances. The quality of the forecasts thus plays an important role for the economic success of financial institutions such as banks, insurance companies, and mutual fund firms. Dependent on the specific financial quantity of interest, various methods are applied to obtain high-quality forecasts, ranging from complex dynamic time series models over attempts that aim to find adequate and observable leading indicators, to the use of expert knowledge. We are concerned with the latter in this paper and provide two main contributions. Firstly, we examine whether the quality of a stock market index forecast can be improved by the application of portfolio optimization techniques to forecasts of individual analysts. Secondly, we provide a flexible method that is able to handle panel datasets that severely suffer from missing data without both having to rigorously exclude entire individuals and points in time or having to somehow fill the missing data. In more detail, the expertise of about 350 financial analysts, collected by the ZEW (Center for European Economic Research) in its Financial Market Survey, is used to examine whether portfolio optimization techniques can improve the quality of predicting the German stock market index DAX 30 six (three and one) months in the future. The underlying idea is to examine whether forecasts of analysts can be thought of as “assets” with specific characteristics that can be used for the construction of forecast-portfolios. For example, while one analyst might tend to be overly optimistic with respect to the future DAX-value, another one might tend to be overly pessimistic, so that cleverly weighting the two forecasts could be beneficial in the sense that both the bias and variance of the forecast might be reduced. We show that weighting several individual forecasts in a forecast-portfolio can indeed improve the forecast-quality. In fact, we show that specific forecast-portfolios consistently outperform a benchmark that is given by the forecast of the most successful analyst with respect to various forecast error measures. Moreover, the forecast-portfolios clearly exhibit a smaller mean error than a no-change prognosis that simply uses the most recent observation and a consensus-forecast that assigns all analysts the same importance. However, improvements in comparison to the naive no-change benchmark could not be obtained for all error measures considered. Similar to portfolio optimization with financial assets, our method of forecast-portfolios is very flexible and allows for the pursuit of various objectives. In this study, we only consider three objective functions which translate ideas from financial management into the forecast-portfolio optimization context. First, the construction of portfolios with a low forecast error-variance and a low mean of errors is inspired by the mean–variance framework of Markowitz (1952). Second, the Value-at-Risk framework (Jorion 2001) motivates the construction of portfolios that aim at minimizing the forecast error, which is not exceeded with a chosen probability, i.e. a quantile of the error distribution is minimized. Thirdly, varyingly severe consequences of over- and underestimation can suggest an asymmetric target with respect to positive and negative estimation errors. Besides these three objectives, other goals can easily be implemented in the proposed methodological framework. An inherent problem of working with surveys which collect replies of individual participants in a panel dataset is their holey structure. Due to the voluntary nature of surveys, it frequently happens that participants fail to submit their replies (on time), thereby causing many missing data. A straightforward way of dealing with this issue is simply to exclude participants with missing data. In fact, a rigorous proceeding like this is a prerequisite for most methodologies, because they cannot handle datasets with holes. However, it is not clear whether the excluded participants are a random selection or a specific clientele. Hence, besides the negative consequence of a potentially dramatic loss of information due to the cross-section reduction, in the forecast-portfolio context an exclusion might also remove analysts with specific characteristics that could be of high relevance. We therefore propose to construct the forecast-portfolios with a stochastic search heuristic (Differential Evolution, Storn and Price 1995, 1997) that is flexible enough to allow for the introduction of a shifting scheme that avoids the exclusion of analysts with missing data. The proposed method to circumvent a rigorous a priori exclusion of observations could be useful in many similar applications and is not restricted to analysts’ forecasts. The remainder of the paper is structured as follows: Sect. 2 provides a selected overview to the literature on combining forecasts. Section 3 details the ZEW Financial Market Survey-dataset used in this study. Section 4 describes how ideas from the construction of portfolios with financial assets are translated into forecast-portfolio optimization problems, and Sect. 5 presents the applied DE. In Sect. 6, the forecast-portfolios are empirically tested and compared to the naive approaches, while Sect. 7 concludes.",6
44.0,2.0,Computational Economics,29 June 2013,https://link.springer.com/article/10.1007/s10614-013-9388-5,A Highly Accurate Finite Element Method to Price Discrete Double Barrier Options,August 2014,A. Golbabai,L. V. Ballestra,D. Ahmadian,Unknown,Unknown,Unknown,Unknown,,
44.0,2.0,Computational Economics,02 June 2013,https://link.springer.com/article/10.1007/s10614-013-9385-8,"Network Externalities, Incumbent’s Competitive Advantage and the Degree of Openness of Software Start-Ups",August 2014,Stefano Colombo,Luca Grilli,Cristina Rossi-Lamastra,Male,Male,Female,Mix,,
44.0,2.0,Computational Economics,29 May 2013,https://link.springer.com/article/10.1007/s10614-013-9387-6,"A Dynamic Network Oligopoly Model with Transportation Costs, Product Differentiation, and Quality Competition",August 2014,Anna Nagurney,Dong Li,,Female,,Unknown,Mix,,
44.0,2.0,Computational Economics,21 May 2013,https://link.springer.com/article/10.1007/s10614-013-9381-z,Timescale Analysis with an Entropy-Based Shift-Invariant Discrete Wavelet Transform,August 2014,Stelios D. Bekiros,,,Male,Unknown,Unknown,Male,"In contrast to the classical Fourier analysis, in which the frequency content of the underlying time series is assumed to be stationary along the time axis, wavelets are defined over a finite domain and unlike the Fourier transform they are localized both in time and in scale. This property, as opposed to the trigonometric or complex exponential functions in Fourier transform, makes wavelets ideal for analysing nonstationary signals especially those incorporating transient phenomena or singularities. Wavelet analysis works with “translates” and “dilates” of a single local function, the so-called “mother wavelet”, which is locally defined with compact support or decays sufficiently fast. However, the existence of wavelets is not a trivial analytical issue and the construction of classes of compactly supported wavelets was first addressed by Daubechies (1988). In that work a reliable methodology for obtaining orthogonal wavelet bases by translating and dilating the mother wavelet was provided. As a mother wavelet with compact support is located in a finite interval, the analysis of a singularity is performed by considering only those translates of the mother wavelet that overlap the singularity. Daubechies (1988) contribution was followed by the development of multiresolution analysis by Mallat (1989) and Coifman and Wickerhauser (1992). Technically, Fourier and wavelet methods involve the projection of a signal onto an orthonormal set of components. Fourier projections are most naturally defined for functions restricted to \(L^{2}({0,2\pi })\) i.e., the set of square integrable functions in the interval \(({0,2\pi })\), because Fourier series have infinite energy but finite power when extended to being defined over the entire real axis. Based on the complex superposition of individual harmonics, the hypothesis is that over any segment of the time series the exact same frequencies hold at the same amplitudes, namely the signal is homogeneous over time. On the contrary, the basis functions in wavelet analysis are defined in \(L^{2}(\mathbb R )\) and are not necessarily homogeneous over time, meaning that they have narrow compact support so that they rapidly converge to zero as time approaches infinity. Such basis functions are called wavelets, in distinction to the trigonometric functions traditionally associated with waves. The flurry of interest in economic applications of wavelets emerged in the mid-90s mostly by Ramsey and his collaborators. Ramsey et al. (1995) pursued a wavelet approach in detecting self-similarity in US stock prices. In addition, Ramsey and Lampart (1998a, b) used a wavelet-based scaling method to investigate the relationship and causality between money, income and expenditure. In other studies Goffe (1994) illustrated the application of wavelets to nonstationary macroeconomic data, in particular for the detection of discontinuities and the occurrence of sharp cusps. In recent works, Almasri and Shukur (2003) address the causal relation between spending and revenue at different timescales, while Gençay et al. (2002) look into dependencies between money, income, expenditure, growth and inflation. Fernandez (2005) deals with the estimation of systematic asset risk. Finally, it is also worth mentioning a stream of papers utilizing wavelet methodology to address theoretical econometric issues and devise new statistical approaches Lee J. and Y. Hong (2001). In natural sciences the utilization of sequences of timescales in the analysis of different modes of behavior, or different relationships between variables, is very common. In economics the notion of timescale is related to time period segmentation and the examined relationships are described as short-run and long-run, or broadly under the term scaling laws (Brock 1999). The scale decomposition often reveals the presence of deterministic regularities or statistical properties of the conditional moments that are seemingly independent of the scale details. However, in wavelet literature the concept of time scaling or “dilation” is quite different from that in economics. The different scales of the wavelet decomposition contain contributions of the signal in different frequencies (although timescale and frequency are not identical concepts as it is further explained below). Based on the selected function space, the time series are analysed into “fine” and “coarse” resolution components extracted from the application of “father” and “mother” wavelets. The former represent the smooth and low-frequency parts of a signal, whereas the latter incorporate the detailed and high-frequency features (Percival and Walden 2000). While frequency analysis (Fourier) results in projecting the entire signal onto ever lower frequencies, time scaling with wavelets is concerned with projecting a localized component of the signal onto an increasingly broader base. Moreover, the basis functions are orthogonal across scales, so that the total variation/energy of the signal at any given point in time is obtained by adding the constituent components extracted at each of the scales. Although at first sight timescale could directly correspond to frequency there is only an indirect connection between these two concepts, as indicated by Priestley (1996). Intuitively, in a naïve interpretation, wide-support wavelets can be associated with low frequencies, while high-frequency analysis requiring high sampling rates can be provided by narrow-support components. The multiresolution features of wavelet decomposition can be useful in econometric analysis. Through wavelet decomposition, the low-frequency content of the data that “captures” the true dynamic relationships can be extracted and the high-frequency fluctuations that might distort the underlying dependencies of the economy can be removed. This paper contributes to the literature by introducing an invariant transform that enables point-to-point comparison among all scales, contains no phase shifts, relaxes the strict assumption of a “dyadic-length” time series, deals effectively with “boundary effects” and is asymptotically efficient. In addition, beyond the existing practice that has utilized subjective judgment in considering the appropriate “depth” of the wavelet analysis, a new entropy-based methodology is introduced to determine the optimal level of decomposition. Finally, an empirical application in financial time series—in particular exchange rates—is pursued. These series are inherently characterized by chaotic patterns, fat tails and long-memory, particularly at high sampling frequencies. The results provide evidence of complex heterogeneous dynamics across and within different scales. The paper develops as follows: Sect. 2 provides an overview of multiresolution methodology and presents a new shift-invariant discrete wavelet transform. Finally, Sect. 3 provides an empirical application and Sect. 4 the concluding remarks.",19
44.0,2.0,Computational Economics,18 May 2013,https://link.springer.com/article/10.1007/s10614-013-9383-x,An Optimal Balanced Economic Growth and Abatement Pathway for China Under the Carbon Emissions Budget,August 2014,Yongbin Zhu,Zheng Wang,,Unknown,,Unknown,Mix,,
44.0,3.0,Computational Economics,05 July 2013,https://link.springer.com/article/10.1007/s10614-013-9389-4,Quarterly Fiscal Policy Experiments with a Multiplier-Accelerator Model,October 2014,David A. Kendrick,George Shoukry,,Male,Male,Unknown,Male,"In a previous paper, i.e. Kendrick and Amman (2010), we conjectured that fiscal policy changes on a quarterly rather than an annual basis might help arrest downturns in the economy before they become really serious and thereby decrease the magnitude of the fluctuations in the economy. This was done in the context of considering a feedback rule for fiscal policy in parallel with the well known feedback rule for monetary policy, i.e. the Taylor Rule (Taylor 1999). The Kendrick and Amman paper discusses the mathematics of the fiscal policy feedback rule in the context of a quarterly model of the economy and thus establishes a framework for the comparison of annual and quarterly fiscal policy scenarios. That paper also discusses the institutional and political changes that might be made to facilitate quarterly changes in fiscal policy through the use of automatic stabilizers and a depoliticized commission. For a recent discussion of more use in macroeconomic policy of automatic stabilizers and depoliticized commissions see Orszag (2011). The present paper extends the work in the previous paper by reporting on the development of a small macroeconometric model that can be used to test the relative performance of annual and quarterly fiscal policy scenarios. This model is simple in that it is confined to fiscal policy and does not include monetary policy. Also, it uses only two behavioral equations, namely a consumption function that is driven by disposable income and an investment equation that includes both the interest rate and an accelerator term. Thus the model’s intellectual roots go back to the development of the multiplier accelerator model by Samuelson (1939) and a similar model by Chow (1967). The model that is developed in this paper from these roots is used to conduct both counterfactual experiments and Monte Carlo experiments that are run over the period of the serious downturn and slow recovery of the economy from the third quarter of 2007 through the third quarter of 2010. These experiments focus on the different outcomes produced by using annual and quarterly fiscal policy scenarios. Not surprisingly, our experiments indicate that quarterly rather than annual fiscal policy changes result in a smoother path for the economy through the use of smaller but more frequent changes in government appropriations. Thus in the quarterly scenario output tracks a desired path more closely than in the annual scenario but is able to do so with a slightly larger increase in government debt over the counterfactual period. This result raised the question of what would happen to the debt path if we increased the weights on the output state variable in the annual scenario to force output results for the two scenarios to be similar. When we tried this we found that while output was roughly the same in the two scenarios, the increase in government debt over the counterfactual period was less with the quarterly policy scenario than with the annual policy scenario. Counterfactual experiments are very interesting but not nearly as general as those which can be obtained with Monte Carlo methods. Thus we report in the second part of this paper on stochastic control experiments using the same model but with a random additive noise in the output equation. The results in this more general framework also point the way to a finding that a relatively simple shift from annual to quarterly fiscal policy could provide either better stabilization results with a slightly larger increase in the debt level or similar stabilization results but with a smaller increase in the debt level over the period covered by the model. We begin here with a discussion of the mathematics of the model and follow this with a discussion of the results first from the counterfactual and then from the Monte Carlo experiments. Both sets of experiments are done with the Duali software developed by Kendrick and Amman and with control theory code developed in MATLAB by Shoukry. We close the paper with a section on “limitations” that explains why we believe that the pattern of results we have found may be robust but the relative size of the effects will most likely change when these kinds of scenarios are run in larger and more complex models.",9
44.0,3.0,Computational Economics,02 August 2013,https://link.springer.com/article/10.1007/s10614-013-9393-8,Generating Random Optimising Choices,October 2014,Jan Heufer,,,Male,Unknown,Unknown,Male,"Afriat’s (1967) Theorem shows that for a set of consumption choices from competitive budgets there exists a continuous, monotonic, and concave utility function if and only if the choices satisfy cyclic consistency. Varian (1982) introduced the Generalised Axiom of Revealed Preference (GARP) which is easy to test and equivalent to cyclic consistency. It is important to know how meaningful this test for utility maximisation is. Therefore, it is helpful to know the probability that random choices violate GARP. This probability can be interpreted as the power of the test for utility maximisation against the alternative hypothesis of random choices. Bronars (1987) suggested a Monte Carlo approach to approximate the power: generate many sets of random choices and tests all of them for GARP. The percentage of sets which violators is the approximate power. Suppose that the researcher is not only interested in testing GARP but also in testing additional assumption for which GARP is a necessary but not sufficient condition. Such assumptions include homotheticity and different forms of separability (cf. Varian 1983), risk aversion (cf. Heufer 2011), and others. If for example a researcher is interested in testing a set of observations for consistency with homotheticity, using Varian’s (1983) Homothetic Axiom of Revealed Preference (HARP), then to approximate the power she can use a variant of Bronars’ Power by testing random choices for consistency with HARP. But GARP is a necessary condition for HARP, and Bronars’ Power for the GARP test might already be close to unity. That means that very few random choice sets satisfy GARP, and therefore cannot satisfy HARP. It would be useful to know the conditional probability that a set of random choices violates HARP given that it satisfies GARP. A simple way to approximate this conditional probability is to use a rejection technique: draw a set of random choices and reject it if it does not satisfy GARP. However, Bronars’ power can be quite literally 100 %, that is, even out of thousands of sets of random choices no set satisfies GARP. This is the case in the 50 budget experiments conducted by Fisman and Kariv (2007) and Choi et al. (2007). The contribution of this paper is to introduce a Monte Carlo approach to generate sets of random choices which satisfy GARP, using a Markovian method introduced by Smith (1984). He showed that a symmetric mixing algorithm which generates a Markov chain on a bounded region will generate a sequence of points asymptotically uniformly distributed within the region. This algorithm is used to generate sets of random choices on budgets which are uniformly distributed such that the sets satisfy GARP. These GARP-consistent random sets can then be tested for consistency with other conditions such as HARP. The algorithms presented here can also be applied to nonparametric approaches to production analysis (cf. Varian 1984).",4
44.0,3.0,Computational Economics,10 August 2013,https://link.springer.com/article/10.1007/s10614-013-9396-5,Capturing the Regime-Switching and Memory Properties of Interest Rates,October 2014,Xiaojing Xi,Rogemar Mamon,,Unknown,Unknown,Unknown,Unknown,,
44.0,3.0,Computational Economics,20 August 2013,https://link.springer.com/article/10.1007/s10614-013-9395-6,Some Pitfalls in Smooth Transition Models Estimation: A Monte Carlo Study,October 2014,Novella Maugeri,,,Female,Unknown,Unknown,Female,"There seems to be a wide acceptance that relationships between macroeconomic variables may often be nonlinear. Nonetheless, the specification and estimation of nonlinear types of models is nothing but straightforward. Among nonlinear models, regime switching (RS) models are becoming increasingly popular in recent applied literature, as they allow capturing state-dependent behaviors which would be otherwise impossible to model. This family of models comprehends Markov Regime Switching models (Hamilton 1989), and smooth transition (ST) models, including smooth transition autoregressive models (STAR) (Granger et al. 1993), and smooth transition error correction models (STECMs) (Dijk and Franses 2000; Kapetanios et al. 2003). In this work we focus on the estimation of ST models, and we put under investigation one of the most routinely employed procedures in this context, namely the ‘concentrating the sum of squares’ procedure (henceforth CSQ), first proposed by Leybourne et al. (1998). We take as a benchmark the first order STECM, nesting in it more simple types of threshold autoregressive (TAR) models, and we propose a Monte Carlo study on the finite sample properties of the nonlinear least squares (NLS) estimator when the CSQ procedure is employed.Footnote 1
 ST models estimation is computationally complex due to at least four order of difficulties. Firstly, ST models estimation, and STECMs estimation in particular, require great data availability and large samples. For this reason, the application of STECMs so far has been confined to problems where large data samples were available, involving for example interest rates (Dijk and Franses 2000), real exchange rates (Bereau et al. 2010), stock returns (Fredj and Yousra 2004), house prices (Balcilar et al. 2011) and only recently to inflation expectations (Maugeri 2010). Second, as Chan and Theodorakis (2011) point out, there is a lack of knowledge of the general statistical properties for the ST family of models, being the restricted two-regimes case almost the only one thoroughly investigated. Third, when practically estimating ST models, choosing the type of nonlinearity implies choosing also a specific modeling cycle. On this aspect, Granger et al. (1993) distinguish two possible strategies: a specific-to-general strategy and a general-to-specific one. The former involves first estimating a simple restricted model, and then proceeding to more sophisticated ones only if diagnostics indicate that the simple model is inadequate. The latter implies that one starts to test linearity against the ST model, and if linearity is rejected one specifies sub hypotheses to see which ST model better suits the data. None of these two approaches has been proved to be strictly superior to the other. Finally, there is a fourth and very relevant order of difficulty, the one we focus on in this paper: while the literature suggests that the best option for estimating ST models is NLS, which in this case should be equivalent to quasi maximum likelihood (QMLE), the practical application of NLS is computationally intensive. For this reason, many authors suggests to reduce NLS’ computational burden by ‘CSQ’. Using the CSQ ‘trick’ to simplify the implementation of NLS is admittedly something that requires caution, nevertheless it has often been advocated as the best solution available to simplify the estimation of ST time series models (Van Dijk et al. 2002; Lundbergh et al. 2003; Shittu and Yaya 2010). CSQ can be summarized as follows: the linear autoregressive part of the model is first estimated by assuming given values of the threshold parameters of the nonlinear part, then the NLS sum of squares function is ‘concentrated’ with respect to the estimated parameters from first step, and then it is evaluated only with respect to the nonlinear parameters. To the best of our knowledge, no other paper has investigated whether performing the CSQ trick costs too much in terms of bias and loss of asymptotic properties of the estimated ST parameters. This paper aims at filling the gap by means of several Monte Carlo experiments, and then it shows the relevance of this issue also in practical situations by providing a simple empirical application. Our Monte Carlo design involves three basic experiments, the first two aim to investigate bias and the third one investigates consistency of the NLS–CSQ estimator. Experiments I and II are are used to investigate the bias property of the two estimators, so each experiment involves generating series of either \(T=180\) or \(T=90\), where we indicate with \(T\) the number of observations, and 10.000 replications. Experiment III finally assesses what happens to these estimators in cases ‘closer to consistency’, and it generates series of \(T=500\) observations reducing the number of simulations to \(100\). We use as a reference point the paper by Van Dijk and Franses (henceforth VDF) (2000) whose data are easily accessible through DATASTREAM, and for which we are able to replicate exactly the parameters estimates. In the first experiment, we use the same parameter values as VDF (2000, Eq. (21), p. 219) to generate the STECM model, and we compare the performance of the NLS estimator when the nonlinear parameters concentrated out of the sum of squares function (NLS–CSQ) and with the one maximum likelihood estimator (MLE). We perform the comparison under different optimization algorithms, and for different values of both the standard deviations of the simulation errors, and for \(T=180\) or \(T=90\). In the second experiment we propose the same type of comparison, but we further simplify our model to a STAR(1) model as in Chan McAleer (2002). The third experiment focuses more on the consistency property of the NLS–CSQ vis-á-vis the ML estimator for both the STECM and the STAR model in a Monte Carlo experiment with a larger sample size of \(500\) observations. The results we obtain seem to confirm our suspects: using NLS–CSQ in the estimation of ST models may yield biased estimates, especially when dealing with small samples. The MLE estimator’s performance is much more precise in this case, although the results are very sensitive to the choice of the starting values. Nevertheless, the NLS–CSQ estimator seems to be consistent when the sample size grows larger. We suspect that the main reason for this lack of robustness has to do with the complex structure of ST models’ objective function, which is not well-behaved. These concerns are precisely addressed in the Sect. 7, where we try to obtain a better picture of the loglikelihood function for the simplest case of our experiments, the STAR(1) model. First, we implement a heuristic procedure based on ML estimation by means of subsequent perturbations of the set of starting values. Then, we employ the global maximization algorithm by Tucci (2002), which allows us to see the exact shape of the empirical loglikelihood in a wide area of the parameter space. Finally, Sect. 8 proposes an empirical application to show that these results do not matter only from the theoretical viewpoint but also in practical circumstances. Section 9 offers some concluding remarks.",4
44.0,3.0,Computational Economics,30 August 2013,https://link.springer.com/article/10.1007/s10614-013-9398-3,Endogenous Movement and Equilibrium Selection in Spatial Coordination Games,October 2014,David Hagmann,Troy Tassier,,Male,Male,Unknown,Male,"There exists a vast literature on the emergence of cooperation and altruistic behavior both in real world situations and in theoretical games. Much of this literature centers around prisoner’s dilemma games either directly or as a metaphor for a real world scenario. A related literature studies the results of coordination games where agents receive payoffs if they are able to coordinate on the same strategy but pay a cost if they do not. For example, it is beneficial to an agent if her friends own computers that use the same operating system as she does. However, it may be inconvenient and costly if they do not (e.g. sharing files may be difficult). Continuing with this example, it may be that one of the potential operating system choices is better than the others. If agents coordinate on this strategy, everyone benefits. However, a tension may exist if the better operating system also has a steeper learning curve. Thus, if an agent chooses the better operating system, but no one else does, it may be very costly to her because she will not have friends to consult when problems occur. To translate this situation to a formal game, consider the following generic two agent, two strategy simultaneous game: Throughout the paper we assume \(a>c,\, d>b\) such that there exist two pure strategy Nash equilibria, \(X,X\) and \(Y,Y\). Agents attempt to coordinate with their game play partners on one of the two Nash equilibria. Thus, our game of interest is a standard \(2 \times 2\) coordination game. Further, we assume that \(a>d\), such that \(X,X\) is the Pareto dominant Nash equilibrium. Harsanyi and Selten (1988) define equilibrium \(Y,Y\) to be a risk dominant Nash equilibrium if \((a-c)(a-c) < (d-b)(d-b)\) which is equivalent to \(a+b < c+d\). Our primary interest in this paper will be with payoffs assigned such that \(Y,Y\) qualifies as risk dominant. We study equilibrium selection in these games using an agent-based model. As in the operating system example, note that there is a tension between agents attempting to coordinate on the Pareto dominant versus the risk dominant Nash equilibrium. All agents would prefer to coordinate on \(X,X\), because each agent receives a larger payoff than in \(Y, Y\). But, should coordination not occur (one agent playing X and the other playing Y), the agent playing X is penalized with a low payoff of b. More importantly, as b decreases, playing X becomes more risky and playing Y becomes more attractive. There exists a large literature on the long run selection of equilibria in these coordination games (without agent movement). As examples, Ellison (1993); Kandori et al. (1993) and Young (1993) study equilibrium selection in an evolutionary framework where agents are randomly matched with game partners. They find that the risk dominant Nash equilibrium is the unique stochastically stable equilibrium when agents have a small probability of making mistakes in strategy selection. Morris (2000) studies the spread of a Pareto dominant Nash equilibrium where agents play a spatial coordination game on various topologies. He finds that a Pareto dominant equilibrium may be favored in some network based coordination games if the number of neighbors in the network expands at an intermediate rate (quickly, but not too quickly). In this paper, we explore how endogenous agent movement (which has not been previously studied in coordination games) affects the equilibrium selection results described above. Specifically, agents are located on a two-dimensional lattice. Agents play a coordination game with each nearest neighbor on the network in each period. Agents choose a best response to last period’s play by their neighbors as their action in the current period. Using agent based modeling, we study the evolution of the agent strategies and the attainment of Pareto versus risk dominant Nash equilibria. Agent movement has previously been studied in spatial prisoner’s dilemma games. In this field, researchers are interested in whether the ability of agents to move favors the invasion of defecting agents into neighborhoods of cooperators, or whether the ability of agents to move allows cooperators to escape defectors. Previous research suggests that the ability of agents to move enhances rates of cooperation on average. For example, Aktipis (2004) studies the behavior of a “walk-away” strategy in a spatial prisoner’s dilemma game where agents cooperate if a rival cooperated in a previous period or move to a new location if the rival defected in the previous period. She finds that walk-away is a successful strategy when placed in an Axelrod (1984) style tournament among commonly studied strategies such as tit-for-tat. Helbing and Yu study the emergence of cooperation in a spatial prisoner’s dilemma where agents both imitate neighbors with high paying strategies and move to nearby locations that yield higher payoffs. They term this strategy “success driven migration” and find it to robustly lead to cooperative outcomes in a variety of situations and noise. Barr and Tassier (2010) study the rates of cooperation and evolution of mixed strategies in a spatial prisoner’s dilemma game where agents are allowed to move. They find that the opportunity to move greatly enhances the probability of agent cooperation across many network structures. Another related literature studies the ability of agents to maintain or sever interactions in prisoner’s dilemma games on evolving heterogeneous networks (Ashlock et al. 1996; Biely et al. 2007; Hanaki et al. 2007; Santos et al. 2006; Van Segbroeck et al. 2008). While fundamentally different, both the movement mechanism studied in our paper and in the literature mentioned above and a severing ties/evolving networks approach both lead to the ability of cooperating agents to avoid defecting agents in a prisoner’s dilemma. Echoing the benefits of movement in prisoner’s dilemma games, we find that adding agent movement into a coordination game also increases the likelihood of good outcomes. Specifically, we find that the Pareto dominant Nash equilibrium is attained much more frequently when agents have the ability to move on the network and choose game play partners, than when agents are not allowed to move. We also study the effects of diverse agent types. We find that diversity in payoffs of the agent types allows for strategies to survive longer in the populations and promotes the attainment of higher payoffs on average.",1
44.0,4.0,Computational Economics,31 August 2013,https://link.springer.com/article/10.1007/s10614-013-9397-4,An Analytic Approach for Stochastic Differential Utility for Endowment and Production Economies,December 2014,Yu Chen,Thomas F. Cosimano,Peter Kelly,,Male,Male,Mix,,
44.0,4.0,Computational Economics,15 September 2013,https://link.springer.com/article/10.1007/s10614-013-9399-2,"Accuracy, Speed and Robustness of Policy Function Iteration",December 2014,Alexander W. Richter,Nathaniel A. Throckmorton,Todd B. Walker,Male,Male,Male,Male,"The Great Recession, the prospect of exponentially rising government debt, interest rates at the zero lower bound, and potential sudden changes to monetary and fiscal policy make clear that nonlinearities are a crucial element to contemporary macroeconomic analysis. Successfully modeling these scenarios requires large and persistent deviations from the non-stochastic equilibrium. Linear approximations around a deterministic steady state poorly capture these equilibrium properties. A nonlinear analysis is needed. Policy function iteration methods for solving and analyzing dynamic stochastic general equilibrium models are powerful from a theoretical and computational perspective. Despite obvious theoretical appeal, significant startup costs and a reliance on grid-based methods have limited the use of policy function iteration as solution algorithm. We reduce these costs by providing a user-friendly suite of MATLAB functions. Within the class of policy function iteration methods, we advocate using time iteration with linear interpolation, since it provides a flexible and accurate way to solve dynamic stochastic general equilibrium models with substantial nonlinearity. We demonstrate the usefulness of our approach by examining several topical examples. We begin with a simple real business cycle model and a standard New Keynesian model. These canonical models are useful starting points because their solutions and properties are well known. They also provide useful benchmarks for speed and accuracy. Section 3 provides step-by-step instructions for how to solve these models using our advocated approach in MATLAB. We introduce multi-core processing using the Parallel Computing Toolbox (PCT) and integrate Fortran through MATLAB executable functions (MEX). Using a 6-core processor (3.47 GHz each) and MEX, our suite reduces computational time by a factor of 12 in the RBC model and by a factor of 24 in the NK model relative to non-parallelized code that does not use MEX. Additional stochastic components further increase the speeds gains associated with MEX and parallelization. Section 4 analyzes the tradeoffs between accuracy and speed using alternative iteration and approximation methods. We demonstrate the flexibility of our advocated approach using the canonical New Keynesian model. Section 5 adds Epstein–Zin preferences to show that our suite can be used to study asset pricing facts. Section 6 introduces regime switching in monetary and fiscal policy parameters with an emphasis on understanding the expectational effects generated by regime switches. Section 7 adds a zero lower bound on the nominal interest rate set by the monetary authority, which introduces a kink in the policy functions. We provide MATLAB code with extensive documentation for each example. Richter and Throckmorton (2012) provide additional supporting code. The primary benefit of our advocated approach over perturbation methods (Judd and Guu 1993, 1997; Gaspar and Judd 1997; Schmitt-Grohe and Uribe 2004) is its ability to easily account for sudden policy changes and other inherent nonlinearities (i.e., zero interest rate bound, default, irreversible investment, etc.). The flexibility and simplicity of the algorithm has led a recent segment of the literature to use this approach to estimate monetary and fiscal policy regimes, quantify expectational effects of policy changes, and study key counterfactual policies.Footnote 1 The suite of programs described in this paper can be easily adapted to handle models with: [i] endogenous regime change, [ii] temporarily nonstationary processes, [iii] binding collateral constraints, [iv] stochastic volatility, [v] news shocks, and [vi] heterogeneous agents. Although this method is grid-based, our suite of programs can solve models with state spaces that contain more than one million nodes and multiple stochastic processes in roughly one hour on a standard 6-core computer. We remind readers that policy function iteration methods are a numerical byproduct of using monotone operators to prove existence and uniqueness of equilibria (Coleman 1991). This provides an additional benefit to using policy function iteration methods, as powerful approximation results and proofs of existence and uniqueness can be employed in conjunction with the numerical algorithm. We briefly review the theory of monotone operators in an online appendix.",42
44.0,4.0,Computational Economics,25 October 2013,https://link.springer.com/article/10.1007/s10614-013-9403-x,A Wavelet-Based Approach to Filter Out Symmetric Macroeconomic Shocks,December 2014,Roman Marsalek,Jitka Pomenkova,Svatopluk Kapounek,Male,Female,Male,Mix,,
44.0,4.0,Computational Economics,19 November 2013,https://link.springer.com/article/10.1007/s10614-013-9409-4,A Modified Least-Squares Simulation Approach to Value American Barrier Options,December 2014,Lihua Zhang,Weiguo Zhang,Xiang Shi,Unknown,Unknown,,Mix,,
44.0,4.0,Computational Economics,22 November 2013,https://link.springer.com/article/10.1007/s10614-013-9406-7,Efficient Sampling and Meta-Modeling for Computational Economic Models,December 2014,Isabelle Salle,Murat Yıldızoğlu,,Female,Male,Unknown,Mix,,
45.0,1.0,Computational Economics,19 November 2013,https://link.springer.com/article/10.1007/s10614-013-9404-9,A Constructive Proof of the Existence of Collateral Equilibrium for a Two-Period Exchange Economy Based on a Smooth Interior-Point Path,January 2015,Wei Ma,,,,Unknown,Unknown,Mix,,
45.0,1.0,Computational Economics,20 November 2013,https://link.springer.com/article/10.1007/s10614-013-9405-8,Efficient High-Order Numerical Methods for Pricing of Options,January 2015,Mojtaba Hajipour,Alaeddin Malek,,Male,Unknown,Unknown,Male,"Research in option pricing theory concerns, among other issues, the computation of the value of an option during the lifetime of an option contract. There are many types of options on the market including European, American and Exotic options. In an idealized financial market, the price of an option \(V (S, t)\) can be computed from the Black–Scholes equation. The Black–Scholes partial differential equation that has been introduced by Black and Scholes (1973) and previously by Robert Merton (1973), is in the following form where \(S\) is the price of the underlying asset at time \(t\) and the real constant \(r > 0\) and \({\sigma }^2\) represent the interest rate and the volatility, respectively. In this work we focus on European and American put options. A European option is the right but not obligation to purchase (sell) an underlying asset at the expiration price \(E\) at the expiration time \(T\) (Wilmott et al. 2002). The value of a European put option \(V(S, t)\) is a function of underlying asset price \(S\) and time \(t\), and satisfies the celebrated Black–Scholes partial differential equation (1) and the following final and boundary conditions where \(E\) is exercise or strike price. The analytical solution of the European call option problem (1–3) is well known (Wilmott et al. 1995). American options are more complicated to price, unlike European options, can be exercised not just at expiry but at any time during the life of the option. The value of an American put option \(V(S, t)\) is also determined from the Black–Scholes equation (1) on \([S_f(t), \infty )\) and following final and boundary conditions where \(S_f(t)\) stands for the free boundary value (the optimal exercise boundary) at the time \(t\) and \(S_f(T)=E\). The minimum value \(S_f(t)\) is the optimal exercise price of the corresponding perpetual contract. For financial engineers, two hedge parameters \(\Delta =V_{S}\) and \(\Gamma =V_{SS}\) are important. \(\Delta \) is a measure of the rate of change in the option price with respect to the price of the underlying share. \(\Gamma \) measures the rate of change in the option’s delta with respect to the share price. Many recent works have focused on the use of Crank-Nicolson (Wilmott et al. 2002; Black and Scholes 1973; Company et al. 2008; Hull 1989) and forth-order compact (Liao and Khaliq 2009; During 2003) methods for the numerical solution of European pricing options. However, They are well known that the kink at the strike price in the payoff function of various options, causes a lower order rate of convergence for high-order schemes. Recently Oosterlee et al. (2005) used a grid stretching transformation in combination with a fourth-order spatial discretization based on a five point stencil and fourth-order backward differencing formula (BDF) for time discretization, to obtain a fourth-order accurate solution for European options. There are few dominant methods such that standard finite difference (SFD) (Zhu 2006; Courtadon 1982; Cen et al. 2012), Compact finite difference (Zhao et al. 2007; Dremkova and Ehrhardt 2011; Tangman et al. 2008), binomial (Jarrow and Rudd 1983; John et al. 1999), Monte Carlo (Boyle 1977), least squares (Longstaff and Schwartz 2001) and analytical approximation (Zhu 2006; Barone-Adesi and Whaley 1987; Blum et al. 1972) methods for dealing with American options. The binomial method is pedagogically appealing, very effective and easy to implement; this is first introduced by Cox et al. (1979). In year 1994, the convergence of binomial methods for American options is proved by Amin and Khanna (1994). Jaillet et al. (1990) show the convergence of the finite difference methods. A comparison of fully implicit finite difference, Crank-Nicolson, modified binomial and Monte Carlo methods for the numerical solution of American and European options can be found in Nwozo and Fadugba (2012). Nwozo and Fadugba (2012) concluded that finite difference methods are the most accurate and converges faster than modified binomial and Monte Carlo methods. In year 2004 a high-order weighted essentially non-oscillatory (WENO) approach is applied for the numerical solution of the American-style Asian option (Oosterlee et al. 2004). Although, their numerical results do not show efficiency of the implemented method. The WENO method was first introduced in 1994 for solving convection dominated partial differential equations (Liu et al. 1994). In year 2012 a modification of the WENO method extended for spatial discretization of degenerate parabolic equations which may contain discontinuous solutions (Hajipour and Malek 2012). Although the solution of the Black–Scholes equation is continuous and the equation is not degenerate, the payoff has a kink. If for simplicity we use the linear SFD methods that involved a wide numerical stencil, these methods will be oscillatory near non-smooth point. Our contribution in this paper is the designing of high-order finite difference schemes based on modified WENO method (Hajipour and Malek 2012) for the pricing of European and American options under the standard Black–Scholes model. The main advantage of such schemes is their capability to achieve arbitrarily high order formal accuracy in smooth regions while maintaining stable, non-oscillatory, and sharp discontinuity transitions. Applying a similar grid stretching transformation as used in Oosterlee et al. (2005) in combination with the high-order WENO discretization leads to an efficient pricing algorithm for the European option pricing. As the discretization in the time direction, we advocate the third-order BDF discretization (Hairer and Wanner 1996). This third-order accurate method has very satisfactory stability properties. To deal with American options, we propose a predictor–corrector scheme based on WENO and BDF techniques. The proposed novel methods prevent the appearance of spurious solutions close to discontinuities and recover the high-order of accuracy around non-smooth point as well as smooth points. The remainder of this paper is organized as follows. In Section 2, we design a hybrid method based on third-order BDF and fifth-order WENO techniques for discretization of a time dependent diffusion-convection-reaction PDE. In Section 3, we first apply the WENO and BDF techniques for the discretization of the Black–Scholes equation. Then to restore the high accuracy of the WENO method for the European put option Black–Scholes equation a grid stretching transformation is used. For the numerical solution of American put options, we also construct a predictor–corrector scheme based on WENO and BDF techniques in Section 3. Section 4 draws conclusions.",9
45.0,1.0,Computational Economics,20 November 2013,https://link.springer.com/article/10.1007/s10614-013-9407-6,Hybrid Method of Multiple Kernel Learning and Genetic Algorithm for Forecasting Short-Term Foreign Exchange Rates,January 2015,Shangkun Deng,Kazuki Yoshiyama,Akito Sakurai,Unknown,Male,,Mix,,
45.0,1.0,Computational Economics,22 November 2013,https://link.springer.com/article/10.1007/s10614-013-9408-5,Analyzing Time–Frequency Based Co-movement in Inflation: Evidence from G-7 Countries,January 2015,Aviral Kumar Tiwari,Niyati Bhanja,Olaolu Richard Olayeni,Unknown,Unknown,Unknown,Unknown,,
45.0,1.0,Computational Economics,07 December 2013,https://link.springer.com/article/10.1007/s10614-013-9411-x,Volatility Forecasting Using Support Vector Regression and a Hybrid Genetic Algorithm,January 2015,Guillermo Santamaría-Bonfil,Juan Frausto-Solís,Ignacio Vázquez-Rodarte,Male,Male,Male,Male,"Stock exchange markets play a fundamental role in modern global economy. These private institutions along with governments establish the principal rules and conditions under which different players could perform transactions over the available financial instruments. Banks, funds, and other investment institutions gather and integrate information related to stock prices, basic and exotic options, volatility, and several data sources in order to profit from their transactions. Thus, financial traders main activities are the modeling of stock returns, portfolio hedging, and risk administration. Models employed for these tasks share parameter inputs, being crucial the dispersion of financial instruments prices. Volatility is used for such endeavor, and sometimes it is naïvely identified as the second mathematical moment of stock returns. Representing volatility in a concrete form, a hard number, or even a clear concept is more complex than it seems. Unlike prices or returns, volatility is not directly observable; thus a proxy needs to be employed. Moreover, the methods used to forecast volatility and the available data strongly influence the proxy choice. Continuous methods use a semi-martingale approach altogether with high frequency data (intraday ticks) to calculate a proxy namely realized volatility (Visser 2009). On the other hand, discrete time series methods usually consider daily closing prices to approximate the dispersion of stock prices in a specific time horizon. In this context volatility proxy is also known as a scale factor (Engle 1982; Visser 2009). The pros and cons for these proxies are well documented, albeit using daily data an unbiased estimator of volatility can be obtained (Visser 2009). Previous research (Pérez-Cruz et al. 2003; Chen et al. 2010; Phichhang and Hengshan 2010) and this work employ the former approach. The first ground-breaking method to forecast the conditional fluctuations in financial time series was Engle’s work (Engle 1982); it is known as the autoregressive conditional heteroskedasticity (ARCH) method. Extending Engle’s idea, the general autoregressive conditional heteroskedasticity (GARCH) method was proposed by Bollerslev (1986); it is able to handle properly three hard properties of financial time series: not correlated returns, fat-tails distributions, and volatility clustering. Consequently, GARCH and its extensions are ones of the most common forecasting models among practitioners, academics and financial engineers. Despite GARCH is able to cope with asymmetric price distributions and non linear data, the model foundations are not enough to represent real-world volatility. As a means to tackle these limitations, several upgrades (EGARCH, APARCH, AGARCH, GJR, etc) have been done in the last 20 years (Phichhang and Hengshan 2010). However, these models require to identify a specific distribution for innovations (Gaussian being the most common) in order to estimate model parameters and volatility future values. Semi parametric models arrived as a means to handle the market dynamism and manage the limitations previously stated. One of the most relevant techniques from this area is neural networks (NN); this mechanism is able to estimate an arbitrary function without any prior specific knowledge as innovations distribution. In 1997, a hybrid approach of NN with GARCH was proposed to forecast volatility by Donaldson and Kamstra (1997). The main characteristics of NN provide a model able to capture the asymmetric effects of volatility, improving the forecasting accuracy. These results suggested that NN was able to model effectively the underlying process of financial time series. Nevertheless, NN techniques are prone to get trapped in local optima and susceptible of overtraining. In 1990s decade, Vapnik and co-workers developed a new learning machine proposal called support vector machines (SVM) (Boser et al. 1992; Cortes and Vapnik 1995; Vapnik 1995) based on the statistical learning theory. While NN induction principle, minimizes the training error, SVM is formulated as a convex optimization problem based on the minimization of the structural risk. Another important feature of SVM is its ability to classify data using a reduced training subset (support vectors), improving the classification time for unknown patterns. Originally, SVM was conceived for the classification problem (SVC), later Drucker et al. (1997) extended this technique to perform regression (SVR). Tay and Cao (2001) showed that SVR is better suited to forecast financial time series than back forward NN. Pérez-Cruz et al. (2003) proposed a hybrid forecasting method (GARCH–SVR) based on GARCH(1,1) tuned by a SVR. Predictive capabilities of GARCH–SVR overcomes those obtained by GARCH(1,1) tuned by maximum likelihood estimation (MLE) as is shown by their experimental results. A recurrent SVR based GARCH(1,1) for dynamically model the volatility process was later proposed (Chen et al. 2010). Experimental results with simulated and real data showed better performance with recurrent SVR than both, the GARCH model based on MLE and NN. Several parametric models (GARCH(1,1), EGARCH and GJR) were combined with least squares SVM (LS-SVM) by Phichhang and Hengshan (2010) obtaining better results than those attained by the parametric models alone. Also, SVR was compared with several GARCH-family methods (GARCH(1,1), EGARCH, FIGARCH) and NN, forecasting the volatility of two chinese markets (Jingfeng and Jian 2011); the results favored SVR forecasting accuracy. Stability of any SVR model and its accuracy strongly depend on the kernel, its parameters, and the penalization term (Chih-Hung et al. 2009). A highly effective and representative model can only be built after these parameters are carefully selected. Particularly, SVR can only perform a nonlinear regression by mapping data from the input space to feature space through a nonlinear function called kernel. In this high dimensional feature space, a linear regression is feasible. Among the most used kernels employed in volatility and other financial time series forecasting are linear, polynomial and gaussian kernels (Pérez-Cruz et al. 2003; Kuan-Yu and Chia-Hui 2005; Ping-Feng et al. 2006; Lung 2006; Chih-Hung et al. 2009; Phichhang and Hengshan 2010; Chen et al. 2010). Commonly, a grid search (GS) method is employed to determine these parameters. Yet, GS brute force approach has severe drawbacks as a priori information requirement and an exponentially computational cost (Shih-Wei et al. 2008). Several techniques have been proposed for tuning SVR parameters (Ha-Nam et al. 2004; Ayat et al. 2005; Lung 2006; Ping-Feng et al. 2006; Shih-Wei et al. 2008; Chih-Hung et al. 2009; Wei-Chiang et al. 2011), where evolutionary approaches have obtained very good results. Specially, genetic algorithms (GA) are able to search for global optimum values in non-linear search space without a priori information and with a modest computational cost. A GA–SVR approach was proposed to forecast Taiwan Exchange Market Index by Kuan-Yu and Chia-Hui (2005); this method tunes SVR parameters using a real-valued genetic algorithm (RGA). Experimental results assert the hypothesis that this approach has better performance than NN and random walk. A hybrid model using linear and non-linear SVR models tuned with GA is applied to forecast exchange rates in Ping-Feng et al. (2006). Their results showed that this hybrid approach is superior to NN and random walk. Another hybrid model called HGA–SVR to forecast daily electricity load is propounded (Chen et al. 2010); it consists in a SVR with a GA tuning process which concurrently tunes the proper kernel and its parameters. Nevertheless, GA success strongly depends on the evolutionary operators implemented. Probabilistic selection rules, neighborhood exploration, and genetic recombination are used to optimize GA searching process (Goldberg 1990). In order to excel GA capacities, Boltzmann tournament selection operator was proposed by Goldberg (1990) to include a thermal equilibrium approach into the GA; however, this method lacks of an explicit cooling scheme. Another aspect which improves genetic algorithms convergence is the usage of proper pseudo-random number generators. The former statement has shown that chaotic processes are a good strategy to escape from local optima, enriching the neighborhood exploration, and improving the GA convergence time (Caponetto et al. 2003). This paper proposes a new algorithm to forecast volatility in financial market based on SVR. The algorithm developed here, namely \(SVR_{GBC}\), uses a genetic algorithm with several genetic operators to tune SVR parameters. Additionally, a new Boltzmann selection method, and chaotic number generators are employed to increase accuracy and stability of the trained model. \(SVR_{GBC}\) is tested with four stock market indexes from ASEAN region and Latin America countries. As in (Phichhang and Hengshan 2010) our experimentation considered two forecast stages (year 2007 and 2008) as a means to test the methods mettle under hectic periods. Model performance over the testing periods is measured by MAPE and DA metrics. We compare the results of \(SVR_{GBC}\) versus SVR tuned by grid search method \((SVR_{GS})\), and a commercial implementation of GARCH tuned by MLE namely \(\textit{GARCH}(1,1)_{MLE}\). The paper is organized as follows. Section 2 briefly reviews SVM and SVR formulation. In Sect. 3 the proposed approach for SVR parameter tuning process is presented. Section 4 describes volatility, the proxy employed, the experimental setup, and our results. Finally, Sect. 5 summarizes the conclusions of our work.",20
45.0,1.0,Computational Economics,30 November 2013,https://link.springer.com/article/10.1007/s10614-013-9412-9,A Model of Stock Manipulation Ramping Tricks,January 2015,Ke Liu,Kin Keung Lai,Qing Zhu,,,,Mix,,
45.0,1.0,Computational Economics,08 December 2013,https://link.springer.com/article/10.1007/s10614-013-9414-7,Hedging International Foreign Exchange Risks via Option Based Portfolio Insurance,January 2015,Libo Yin,Liyan Han,,Unknown,Unknown,Unknown,Unknown,,
45.0,2.0,Computational Economics,10 December 2013,https://link.springer.com/article/10.1007/s10614-013-9416-5,Negishi’s Theorem and Method: Computable and Constructive Considerations,February 2015,K. Vela Velupillai,,,Unknown,Unknown,Unknown,Unknown,,
45.0,2.0,Computational Economics,10 December 2013,https://link.springer.com/article/10.1007/s10614-013-9417-4,Carbon Price Analysis Using Empirical Mode Decomposition,February 2015,Bangzhu Zhu,Ping Wang,Yiming Wei,Unknown,,Unknown,Mix,,
45.0,2.0,Computational Economics,11 December 2013,https://link.springer.com/article/10.1007/s10614-013-9415-6,Identification of Social Interaction Effects in Financial Data,February 2015,Tae-Seok Jang,,,Unknown,Unknown,Unknown,Unknown,,
45.0,2.0,Computational Economics,01 February 2014,https://link.springer.com/article/10.1007/s10614-013-9418-3,Tractable Latent State Filtering for Non-Linear DSGE Models Using a Second-Order Approximation and Pruning,February 2015,Robert Kollmann,,,Male,Unknown,Unknown,Male,"Dynamic Stochastic General Equilibrium (DSGE) models typically feature state variables that cannot directly be measured empirically (such as preference shocks), or for which data include measurement error. A vast literature during the past two decades has taken linearized DSGE models to the data, using likelihood-based methods (e.g., Smets and Wouters 2007; Negro and Schorfheide 2011). Linearity (in state variables) greatly facilitates model estimation, as it allows to use the standard Kalman filter to infer latent variables and to compute sample likelihood functions based on prediction error decompositions. However, linear approximations are inadequate for models with big shocks, and they cannot capture the effect of risk on economic decisions and welfare. Non-linear approximations are thus, for example, needed for studying asset pricing models and for welfare calculations in stochastic models. Recent applied macroeconomic research has begun to take non-linear DSGE models to the data. This work has mainly used particle filters, i.e. filters that infer latent states using Monte Carlo methods.Footnote 1 Particle filters are slow computationally, which limits their use to small models. This paper develops a novel deterministic filter for estimating latent state variables of DSGE models that are solved using a second-order accurate approximation (as derived by Jin and Judd 2000, Sims 2000, Collard and Juillard 2001, Schmitt-Grohé and Uribe 2004, Kollmann 2004 and Lombardo and Sutherland 2007). That approximation provides the most tractable non-linear solution technique for medium-scale models, and has thus widely been used in macroeconomics (see Kollmann 2002 and Kollmann et al. 2011 for detailed references).When simulating second-order approximated models, it is common to use the ‘pruning’ scheme of Kim et al. (2008), under which second-order terms are replaced by products of the linearized solution. Unless the pruning algorithm is used, second-order approximated models often generate exploding simulated time paths. Pruning is therefore crucial for applied work based on second-order approximated models. This paper hence assumes that the pruned second-order approximated model is the true data generating process (DGP). The method presented here exploits the fact that the pruned system is linear in a state vector that consists of variables solved to second- and first-order accuracy, and of products of first-order accurate variables. The pruned system thus allows convenient closed-form determination of the one-period-ahead conditional mean and variance of the state vector. I apply the linear updating rule of the standard Kalman filter to the pruned state equation. The filter here is much faster than particle filters, as it is not based on stochastic simulations. In Monte Carlo experiments, the present filter generates more accurate estimates of latent state variables than the standard particle filter, especially with big shocks or when the model has high curvature. The filter here is also more accurate than a conventional Kalman filter that treats the linearized model as the true DGP.Footnote 2 Due to its high speed, the filter presented here is suited for the estimation of structural model parameters; a quasi-maximum likelihood procedure can be used for that purpose. This paper is complementary to Andreasen (2012) and Ivashchenko (2014) who also develop deterministic filters for second-order approximated DSGE models, and show that those filters can outperform particle filters. These authors too assume linear updating rules. The filter here is closest to Ivashchenko’s (2014) ‘Quadratic Kalman filter’ (QKF) that is also based on closed-form one-step-ahead conditional moments of the state vector—the key difference is that the QKF does not use the pruning scheme.Footnote 3 The present filter (based on pruning) performs well even in models with big shocks and high curvature—for such models the QKF may generate filtered state estimates that diverge explosively from true state variables. Such stability issues never arose for the filter proposed here, in a wide range of numerical experiments. In models with small shocks and weak curvature the filter developed here and the QKF have similar performance. The present paper is also related to Andreasen et al. (2013) who likewise derive a pruned state-space representation for second-order approximated DSGE models and show how to compute moments for pruned models; these authors develop a method of moments estimator for DSGE models, but do not present a filter for latent state variables. (I learnt about Ivashchenko 2014 and Andreasen et al. 2013 after the present research had been completed.)",19
45.0,2.0,Computational Economics,02 February 2014,https://link.springer.com/article/10.1007/s10614-014-9419-x,Solving Dynamic Programming Problems on a Computational Grid,February 2015,Yongyang Cai,Kenneth L. Judd,Stephen J. Wright,Unknown,Male,Male,Male,"Many dynamic optimization problems in economics can be expressed as dynamic programming problems, but solving them numerically would require weeks or months of CPU time on personal computers. This constraint has limited the range of applications of dynamic programming in economics. Fortunately, many dynamic programming problems can be broken down into a large number of smaller problems that can be solved simultaneously. This structure allows one to use the massively parallel architectures that have been developed in the recent past. When applicable, parallelization can drastically reduce the “wall clock time”, the time a user waits for the results. For example, if you have access to one hundred CPUs and can keep each one working on computations necessary for a solution, then the wall clock time can be reduced from 100 h to about 1 h.Footnote 1 Massive parallelism was initially used in supercomputers. Unfortunately, supercomputers are not used extensively in economics due to difficulties in access and the technical demands on users. HTCondor is a form of massive parallelism that avoids the access problems of supercomputers. This paper shows how economists can use HTCondor to solve large dynamic programming problems with their existing software, with only a few basic system commands, and at little cost in terms of money and bureaucratic processes. Dynamic programming (DP) is an essential tool in solving problems of dynamic and stochastic controls in economic analysis, and often has a structure that can exploit parallelism. For example, in value function iteration, the period \(t\) iteration first solves the period \(t\) Bellman optimization problem at many distinct states, where each of these optimization problems use the period \(t+1\) value function. The results are then used to construct an approximation of the period \(t\) value function. If the computational burden of the approximation step is small relative to the collection of state-specific and independent Bellman optimization problems, then there is substantial potential for parallelization to reduce the wall clock time of DP computations. In theory, this is obvious. However, achieving significant gains from parallelism requires the creation of sophisticated software to manage communications among the CPUs being used. Increasing the number of CPUs raises the potential gain from parallelism but increases the amount of communication that needs to be managed by the network software. Furthermore, it is difficult for economists to obtain time on supercomputers.Footnote 2 These barriers have made it impractical for the average economist to use massively parallel architectures. Fortunately for economists, the economic principle of specialization has been at work and computer scientists have developed tools that make it easy for economists to harness the potential power of parallel computing. The first key concept is “high throughput computing”, HTC, where software organizes a cluster so that when a computer is not being used for another purpose, it will be given tasks that are part of someone’s parallel algorithm. In this paper, we use HTCondor.Footnote 3 The second key tool presented in this paper is Master–Worker (MW), a user-friendly parallelization tool deployed on HTCondor. We use these tools to show that dynamic programming problems can fully utilize the potential value of parallelism on existing networks of computers that currently exist in department and college networks. HTCondor acts as a management tool for identifying, allocating and managing available resources to solve large distributed computations. For example, if a workstation on a network is currently unused, HTCondor will detect that fact, and send it a task. HTCondor will continue to use that workstation until a higher-priority user (such as a student sitting at the keyboard) appears, at which time HTCondor ends its use of the workstation. This is called “cycle scavenging” and allows a system to take advantage of otherwise idle CPU time. In this paper, we show that HTCondor plus MW makes it possible to exploit massive parallelism to solve dynamic programming problems. This paper is constructed as follows. Section 2 discusses the potential use of parallelism for dynamic programming. Section 3 gives an introduction of HTCondor-MW system. Section 4 describes numerical algorithms for solving DP problems. Section 5 introduces two types of parallel DP algorithms in the HTCondor-MW system. Sections 6 and 7, respectively, give computational results of the parallel DP algorithms in the HTCondor-MW system for solving multidimensional optimal growth problems and dynamic portfolio optimization problems. Section 8 concludes.",18
45.0,2.0,Computational Economics,07 February 2014,https://link.springer.com/article/10.1007/s10614-014-9421-3,Fiscal and Monetary Policy in a Basic Endogenous Growth Model,February 2015,Alfred Greiner,,,Male,Unknown,Unknown,Male,"In the aftermath of the financial crisis governments had run excessive public deficits to support the financial sector in their economies. Particularly in some euro countries, this brought about extremely high debt to GDP ratios endangering the stability of the whole euro zone. The European Union reacted to that threat by creating the European Stability Mechanism that supports highly indebted countries. Simultaneously, the European Central Bank, just as other central banks in the world, expanded its money supply to assist both the financial sector and governments. Among other things, this raises the question of how the interplay between fiscal and monetary policies affects the performance of market economies. A seminal paper that analysed the effects of monetary policy on the steady-state capital stock is the contribution by Sidrauski (1967) who showed that the rate of money growth does not affect the steady-state capital stock and, consequently, leaves per capita income unchanged. Money is super-neutral in this model implying that inflation does not affect the savings rate in the long-run. Stockman (1981) presented a growth model where money is needed to purchase capital goods. In that model inflation reduces the steady-state capital stock so that monetary policy has real effects. While those contributions focus on monetary policy alone, the interaction between fiscal and monetary policy is at the center of the paper by Leeper (1991). Leeper argues that monetary policy can be confined to inflation targeting as long as fiscal policy guarantees the stabilization of public debt. The paper by Leeper was followed by a great many contributions analyzing under which conditions monetary policy should not only control inflation but also be conducted in a way such that public debt does not become explosive.Footnote 1
 All those contributions have in common that they are exogenous growth models, implying that neither monetary nor fiscal policy can affect the long-run growth rate. But, there also exist studies dealing with money in endogenous growth models. For example, Jones and Manuelli (1995) analyse different types of endogenous growth models and show that in some models inflation has direct effects on the growth rate of the economy by distorting the choice of households between consumption and leisure. That mechanism also charaterises the models presented by Wu and Zhang (1998) and by Gomme (1993). There, a rise in the inflation rate reduces the level of employment, thus, leading to lower growth. Gillman and Kejak (2005) set up a nesting model in order to analyse the theoretical literature on inflation and endogenous growth. They can show that a large array of models can all generate negative effects of inflation on economic growth. Further, they point out that the models can be distinguished whether there is a nonlinear relationship between inflation and growth so that the effect becomes smaller as inflation rises, or whether this relation remains constant over the range of inflation rates. With this contribution we intend to contribute to the research dealing with the question of how the interplay between public debt policy and monetary policy affects growth, inflation and welfare of economies. Among other things, we want to see whether central banks can compensate loose debt policies of governments by a higher growth rate of nominal money supply. It is well known that in endogenous growth models without money, higher debt to GDP ratios go along with lower growth in the long-run.Footnote 2 This holds both for the case of productive public spending (cf. Greiner, 2008, 2012) as well as for the case when government spending is unproductive (see Greiner 2011). Only when imperfect labour markets are allowed for, giving rise to permanent unemployment, public debt does not affect the allocation of resources (see Greiner 2012a) so that raising public deficits to finance productive public investment can lead to a higher balanced growth rate (cf. Greiner and Flaschel 2010). The motivation to analyse fiscal and monetary policy in an endogenous growth setting is that it allows to study the long-run growth effects of policy measures, in contrast to exogenous growth models where fiscal and monetary policy only affect levels and transitional growth rates but not the long-run balanced growth rate. An implication of resorting to an endogenous growth model is that the stylized market economy under consideration is characterized by positive per capita growth in the medium- to long-run. Since the latter is an important stylized fact in growth theory, allowing for an endogenous growth model seems to better fit characteristics of real-world economies. When a monetary sector is to be integrated into models of economic growth, the question of arises how that can be achieved. In economics, usually three approaches can be found (for a survey see Wang and Yip 1992). First, there is the so-called money-in-the-utility-function approach that assumes that real money holdings have a positive effect on the utility of agents. The second way of integrating real money is the cash-in-advance approach that posits that consumption goods and a part of the investment goods must be purchased out of existing real money balances. The third approach, finally, is the transactions-costs approach where money is introduced through a shopping-time technology that states that the time spent for shopping is the smaller the higher the real money holdings are. Resorting to those approaches, the interaction between monetary and fiscal policy can be studied then. In this paper we adopt the money-in-the-utility approach and assume that real money holding yields utility for the representative household. Sustained growth in our model is the result of positive externalities leading to constant returns to capital in the economy. Since we are interested in growth effects of economic policy we use an endogenous growth model. Otherwise, policy measures would only affect the level of variables and the growth rates on the transition path but not growth in the long-run. We can show for our model that a balanced government budget always gives rise to a higher balanced growth rate compared to an economy with permanent public deficits. That also holds with respect to welfare unless governments put a high weight on stabilizing public debt when running permanent deficits. In that case, welfare effects of the balanced budget scenario compared to the scenario with permanent deficits depend on the initial conditions with respect to public debt. When governments run permanent deficits, a stricter debt policy implies higher long-run growth as well as higher welfare. Further, monetary policy is not neutral since it affects labour supply, as in Jones and Manuelli (1995), and, in addition, the debt to GDP ratio and, thus, long-run growth. Therefore, the central bank can compensate a loose debt policy by increasing the nominal money growth rate. Such a policy raises the long-run growth rate but goes a along with higher inflation and less welfare. In addition, that is possible only up to a certain point, meaning that sustained growth is not feasible once the debt policy of the government is too loose in the sense that the reaction of the government to higher public debt falls short of a certain critical value. In the rest of the paper we proceed as follows. In the next section we present the structure of our growth model. Section 3 analyses our model where we study growth and welfare effects of different debt and monetary policies and Sect. 4, finally, summarises the main results and concludes the paper.",2
45.0,2.0,Computational Economics,09 February 2014,https://link.springer.com/article/10.1007/s10614-014-9424-0,ESIS2: Information Value Estimator for Credit Scoring Models,February 2015,Martin Řezáč,,,Male,Unknown,Unknown,Male,"Credit scoring is the use of statistical models and underlying statistical techniques to extract all possibly relevant information from the available data and their transformation into numerical values used in the credit decision making process. It is used to decisions, whether clients are good or bad (their behaviour is not consistent with the conditions of the credit contract). Moreover, it is applied to all areas of credit management, including the measurement of risk, response, revenue, collections or recoveries. Methodology of credit scoring models and some measures of their quality were discussed in works like Hand and Henley (1997) or Crook et al. (2007) and books like Anderson (2007), Siddiqi (2006), Thomas et al. (2002) and Thomas (2009). Further remarks connected to credit scoring issues can be found there as well. To measure the quality of a credit scoring models it is possible to use some from a wide variety of quantitative indices. The most favourite and used in business are Gini index, K–S statistic, Lift or Information value. These indices are used for assessment of several developed models at the moment of development. Likewise, they are used for monitoring the quality of models after the deployment into real business. See Wilkie (2004) or Siddiqi (2006) for more details. The paper deals primarily with the information value, described in Sect. 2. Commonly it is computed by discretisation of data into bins using deciles with requirement on the nonzero number of cases for all bins, see Thomas (2009). As an alternative method to the empirical estimates one can use the kernel smoothing theory, which allows to estimate unknown densities and consequently, using some numerical method for integration, to estimate value of the Information value, see Koláček and Řezáč (2010). However, empirical estimate using deciles of scores, which is the most common way how to compute the information value, may lead to strongly biased results. The empirical estimate with supervised interval selection (ESIS) is defined and discussed in Řezáč (2011). This estimate is based on requirement to have at least \(k\), where \(k\) is a positive integer, observations of scores of both good and bad clients in each considered interval. One can find some articles dealing with comparison of several modeling approaches with respect to some quality criteria. Desai et al. (1996) compared neural networks, linear discriminant analysis and logistic regression according to correct classification rate. Hsieh (2005) dealt with hybrid mining approach employing SOM, K-means and neural networks. The proportion of samples which were correctly classified was used as he performance measure. Hsieh and Hung (2010) compared neural networks, Bayesian network, support vector machines and an ensemble model according to the gain charts. Hand and Henley (1997) discussed this topic and gave some comments on the subject. They concluded that significant improvements in quality of a credit scoring model are more likely to come from including new, more predictive, characteristic or from changing the modeling strategy, rather than from using neural networks instead of logistic regression. In accordance with this statement we are not going to compare any modeling aproaches with respect to any of mentioned quality indices. The main objective of this paper is a description of the modified empirical estimate with supervised interval selection. It is called ESIS2 and leads to further reduction of the bias and the mean square error (MSE), which are crucial for correct assessment of scoring models or variables that enter into these models. Section 3 is devoted to this advanced estimate, which is based on idea of computation of the intervals separately for two regions of given score. By means of Monte Carlo simulations presented in Sect. 5, the performance of the proposed estimator under various distribution parameters settings, discussed in Sect. 4, is compared with that of other standard estimators. The results are impressive and the proposed estimator, almost always, has higher performance than that of the other estimators considered. Section 6 is then devoted to impact on profitability of credit business according to predictive power of credit scoring models.",5
45.0,2.0,Computational Economics,14 February 2014,https://link.springer.com/article/10.1007/s10614-014-9423-1,Finding an Initial Basic Feasible Solution for DEA Models with an Application on Bank Industry,February 2015,Mehdi Toloo,Atefeh Masoumzadeh,Mona Barat,Male,Unknown,Female,Mix,,
45.0,2.0,Computational Economics,14 February 2014,https://link.springer.com/article/10.1007/s10614-014-9422-2,Back to the Future: Economic Self-Organisation and Maximum Entropy Prediction,February 2015,Sylvain Barde,,,Male,Unknown,Unknown,Male,"The presence of self-organisation in economic systems is one of the earliest features identified in economic thought. This phenomenon, referred to initially as the “invisible hand” by Adam Smith, then as “spontaneous order” following the work of Hayek (1945), is now often referred to as “emergence” by the agent-based computational economic literature, for instance in Tesfatsion (2006). Two common elements in all three of these interpretations are the notion that (a) economic mechanisms are not planned ex ante but appear as the result of interactions between agents, and (b) these mechanisms maintain the organisation of economic activity over time. To some extent, this is analogous to the Schrödinger (1967) definition of living organisms as entities able to maintain or increase their internal order, thus maintaining or reducing their internal entropy. This paper argues that a consequence of such self-organisation is that under the right conditions, the evolution over time of economic systems should be predictable by signal restoration algorithms. The intuition for this is simple: signal restoration algorithms are designed to reconstruct signals that have been degraded by noise or by distortion. The standard interpretation of this, in a system where entropy (i.e. disorder) increases with time is that the non-degraded signal existed in the past. However, in a self-organising system, the arrow of time is reversed and the non-degraded signal exists in the future. In practical terms, the paper establishes this result by obtaining a strong formal statement relating to self-organisation in allocation problems, and then illustrates it by using maximum entropy (MaxEnt) signal restoration to successfully predict the evolution of a simple self-organising agent-based model. The rest of the paper is structured as follows. The conceptual structure of the argument and the various strands of literature required to obtain it are first detailed in Sect. 2, and Sect. 3 explains how the use of MaxEnt in signal restoration is related to its existing use in economics. Section. 4 then shows the equivalence of allocation problems and congestion games under reasonable assumptions on preferences, opening up the use of signal restoration algorithms due to the presence of systematic improvement paths. This possibility is then illustrated in Sect. 5 by applying MaxEnt to the Schelling model of segregation. Section. 6 discusses the implications of these findings and concludes.",25
45.0,3.0,Computational Economics,20 February 2014,https://link.springer.com/article/10.1007/s10614-014-9427-x,Core–Periphery Structure in the Overnight Money Market: Evidence from the e-MID Trading Platform,March 2015,Daniel Fricke,Thomas Lux,,Male,Male,Unknown,Male,"Interbank markets allow banks to exchange central bank money in order to share liquidity risks.Footnote 1 At the macro level, however, a high number of bank connections could give rise to systemic risk.Footnote 2 Since it is well known that the structure of a network is important for its resilience,Footnote 3 policymakers need information on the actual topology of the interbank network. The experiences of the last few years have made policymakers aware of the necessity of gathering information on the structure of the financial network in general and the interbank market in particular.Footnote 4 One reason for the previous scarcity of research on the connections between financial institutions is certainly the limitation of available data, the other reason being the neglect of the internal structure of the financial system by the dominating paradigm in macroeconomics during the last quarter of a century, cf. Colander et al. (2009). Recent research in the natural sciences has significantly advanced our understanding of the structure and functioning of complex networks. Network ideas have been applied to very diverse areas and datasets from the internet, epidemiology, ecosystems, scientific collaborations and financial markets, to name a few. Most previous studies on the topology of interbank markets have been conducted by physicists applying measures from the natural sciences to a network formed by interbank liabilities. Examples include Boss et al. (2004) for the Austrian interbank market, Inaoka et al. (2004) for the Japanese BOJ-Net, Soramäki et al. (2007) for the US Fedwire network, Bech and Atalay (2010) for the US Federal funds market, and De Masi et al. (2006) and Iori et al. (2008) for the Italian electronic market for interbank deposit (e-MID). Overall, the most important findings of this literature are: (1) interbank networks are sparse, i.e. their density is relatively low,Footnote 5 (2) degree distributions appear to be scale-free (with coefficients between 2 and 3),Footnote 6 (3) transaction volumes appear to follow scale-free distributions as well, (4) clustering coefficients are usually quite small, (5) interbank networks are close to ‘small world’ structures, and (6) the networks show disassortative mixing, i.e. high-degree nodes tend to trade with low-degree nodes, and vice versa.Footnote 7 This indicates that small banks tend to trade with large banks, but rarely among themselves. Thus, we might expect the interbank network to display some sort of hierarchical community structure. In passing, many authors have indeed mentioned the finding of certain community structures in the interbank network they analyzed. For example, Boss et al. (2004) note that the Austrian interbank network shows a hierarchical community structure that mirrors the regional and sectoral organization of the Austrian banking system. Soramäki et al. (2007) show that the network includes a tightly connected core of money-center banks to which all other banks connect. Thus there is some form of tiering in the interbank market. The empirical findings of Cocco et al. (2009) also show that relationships between banks are important factors to explain differences in interest rates. Community detection is an important aspect in network analysis and in this paper we are concerned with the identification of the set of arguably systemically important (core) banks. In order to do so, we estimate various versions of core–periphery models in the spirit of Borgatti and Everett (2000).Footnote 8 Similar to De Masi et al. (2006) and Iori et al. (2008) we use data from the e-MID trading platform, an electronic trading system for unsecured deposits based in Milan and mainly used by Italian banks for overnight interbank credit. Core–periphery models have been applied in a number of interesting fields before, for example to identify the spreaders of sexually transmitted diseases (see Christley et al. 2005), in protein interaction networks (see Luo et al. 2009), and to identify opinion leaders in economic survey data (see Stolzenburg and Lux 2011). The literature on the structure and importance of financial networks is indeed expanding quickly (see among others Acemoglu et al. 2012; Langfield et al. 2012; Summer 2013). To our knowledge, Craig and von Peter (forthcoming) was the first contribution applying a core–periphery structure to an interbank market. Using this core–periphery framework to a dataset of credit relationships between German banks,Footnote 9 their results speak in favor of a very stable set of core banks. Furthermore, they show that core membership can be predicted using bank-specific features such as balance sheet size.Footnote 10
 In this paper we will apply the (unrestricted) discrete core–periphery model, the (restricted) tiering model due to Craig and von Peter (forthcoming) as well as symmetric and asymmetric versions of a continuous core–periphery model (hitherto not applied to interbank data) to a different set of interbank market data. Using a detailed dataset containing all overnight interbank transactions in the Italian interbank market from January 1999 to December 2010, we find that a core–periphery structure provides a concise characterization of this dataset with many characteristics implied by the core–periphery dichotomy displaying a high degree of persistence and relatively little variability over time. The identified core also shows a high degree of persistence over time, consisting of roughly 28 % of all banks before the global financial crisis and 23 % afterwards. We can classify the majority of core banks as intermediaries, i.e. as banks both borrowing and lending money in the market. Furthermore, allowing for asymmetric ‘coreness’ with respect to lending and borrowing activity considerably improves the fit, and reveals a high level of asymmetry and relatively little correlation between banks’ ‘in-coreness’ and ‘out-coreness’. In particular, overall coreness is mainly driven by the liquidity provision of core members to large parts of the banking system. In contrast, borrowing activity appears to play a less important part in explaining overall coreness. Comparing the empirical identification of the core–periphery structure with artificially generated random and scale-free networks with the same network density, we show that our results could not have been obtained spuriously from a completely random structure of links. Scale-free networks get closer to the empirical results in terms of the size of their (pseudo-) core and the similarity of the simulated networks to a CP structure, but at least simple generating mechanisms for scale-free networks would not be fully consistent with the complete set of our empirical findings. We also shed light on the development during the financial crisis of 2008, finding that the reduction of interbank lending was mainly due to core banks’ reducing their numbers of active outgoing links. Our findings indicate that the core–periphery structure may well be a new ‘stylized fact’ of modern interbank networks. In fact, this finding is surprisingly robust given the substantial differences in the nature of the interbank data from Germany, as investigated by Craig and von Peter (forthcoming), and the Italian data employed here. In addition, we should stress that, at the time of writing, we are aware of similar findings using interbank data from other countries as well, including India, Mexico, the Netherlands, and the UK (see Markose et al. 2010; Martinez-Jaramillo et al. 2012; van Lelyveld and in’t Veld 2012; Langfield et al. 2012, respectively). An open question is how individual bank behavior leads to the observed core–periphery structure. Note that the core–periphery structure implies that banks tend to restrict the set of potential trading partners, thus potentially restricting the amount of search and negotiation costs, cf. Wilhite (2001). Due to the anonymous nature of the dataset, we can only speculate that a key mechanism for the observed structure might be that core banks have a comparative advantage in gathering and distributing information about their counterparties. The remainder of this paper is structured as follows: Sect. 2 gives a brief introduction to necessary terminology for the formalisation of (interbank) networks, Sect. 3 introduces the Italian e-MID interbank data and highlights some of its important properties. Section 4 introduces different variants of the core–periphery model. Section 5 presents the results and different robustness checks. Section 6 discusses the findings and Sect. 7 concludes. A set of Appendices provides more technical details as well as further robustness checks, which can be found in the online appendix to this article.",198
45.0,3.0,Computational Economics,21 February 2014,https://link.springer.com/article/10.1007/s10614-014-9426-y,Finitely Precise Dynamic Programming and Portfolio Choice,March 2015,D. O. Stahl,,,Unknown,Unknown,Unknown,Unknown,,
45.0,3.0,Computational Economics,28 February 2014,https://link.springer.com/article/10.1007/s10614-014-9420-4,Approximating Solutions for Nonlinear Dynamic Tracking Games,March 2015,Doris A. Behrens,Reinhard Neck,,Female,Male,Unknown,Mix,,
45.0,3.0,Computational Economics,27 March 2014,https://link.springer.com/article/10.1007/s10614-014-9433-z,"Supply, Demand and Zero-Intelligence: Shape Matters",March 2015,Jie Yang,Wenjie Zhan,,,Unknown,Unknown,Mix,,
45.0,3.0,Computational Economics,29 March 2014,https://link.springer.com/article/10.1007/s10614-014-9431-1,A Framework for Computational Strategic Analysis: Applications to Iterated Interdependent Security Games,March 2015,Yevgeniy Vorobeychik,Steven Kimbrough,Howard Kunreuther,Male,Male,Male,Male,"Contexts of strategic interaction (games) are ubiquitous. They are often momentous in their consequences. Nations compete with each other in economic markets, for influence on global policy, for prestige that builds “soft power,” and on weapons superiority. Firms compete over consumers, suppliers and access to markets. Individuals interact, competing for salary and promotion within a work environment. Everyone competes over scarce resources. In addition, strategic interactions are often fraught with difficulty for their stakeholders, in part because they are complex and in part because they present challenging tradeoffs between cooperation and competition. Success in war, diplomacy, business, individual advancement, and appropriation of scarce resources may be furthered by well-chosen cooperative alliances. Two problems of fundamental significance arise for those who would study or participate in such strategic interactions (Kimbrough 2012). Strategy selection. What strategy should a player choose? What are characteristics of good strategies? How should a player go about choosing a strategy of play? The strategy selection problem arises for the individual player, who must decide how to play the game as well, of course, for the scholar who seeks to understand strategic interaction. Institutional design. What outcomes can be expected for a given strategic setup? How might the setup be changed in order to achieve preferred policy outcomes? The institutional design problem arises for the policy maker, and all those who would influence policy. Beginning with Axelrod’s landmark studies on iterated prisoner’s dilemma (IPD) (Axelrod 1980a, b, 1984; Axelrod and Hamilton 1981) and continuing since, game tournaments—in which collections of strategies for particular games are played with each other and the outcomes analyzed—have contributed to our understanding of the strategy selection and institutional design problems. The tournaments have been studied both analytically and with computerized simulation. IPD games have been the predominant subject of study, although other games are increasingly drawing scholarly attention (Skyrms 2010). In this paper we report on a series of investigations of Interdependent Security (IDS) games. These games model real-world situations that have important policy significance, where outcomes are stochastic. (See §3 for a detailed discussion.) Moreover, IDS games are unusual in having stochastic payoffs as an essential part of their definition. The stochasticity, by virtue of its magnitude, should not be characterized as mere noise. Recent human subject experiments on IDS games have found that the stochasticity significantly complicates the strategy selection problem for individuals. This in turn leads to challenges for policy formulation. Extending the results of human subject experiments, we undertake an extensive study of IDS games, using computational tournaments. In doing so, we present a broader, more extensive framework for strategic analysis in general games than is normally realized in existing tournament studies. We illustrate this approach in the context of social dilemmas encountered in IDS settings. Our framework is quantitative and computational, allowing one to measure the quality of strategic alternatives across a series of measures, and as a function of relevant game parameters. We focus particularly on performing analysis over a parametric landscape, motivated by public policy considerations, where possible interventions are modeled as affecting particular parameters of the game. To preview the results, our findings (in the IDS context) qualify the oft-touted efficacy of the Tit-for-Tat strategy, demonstrate the importance of monitoring, and exhibit a phase transition in cooperative behavior in response to a manipulation of policy relevant parameters of the game. We begin, in the next section, with a discussion of related work that either employs game tournaments or examines games with stochastic elements.",2
45.0,3.0,Computational Economics,08 April 2014,https://link.springer.com/article/10.1007/s10614-014-9435-x,Stochastic Evolutionary Selection in Heterogeneous Populations for Asymmetric Games,March 2015,Yanfang Zhang,Xing Gao,,Unknown,,Unknown,Mix,,
45.0,3.0,Computational Economics,17 March 2014,https://link.springer.com/article/10.1007/s10614-014-9428-9,Oscillatory Dynamics in a Continuous-Time Delay Asset Price Model with Dynamical Fundamental Price,March 2015,Xunxia Xu,Jia Liu,Zhenyuan Xu,Unknown,,Unknown,Mix,,
45.0,4.0,Computational Economics,09 April 2014,https://link.springer.com/article/10.1007/s10614-014-9434-y,Estimate Long Memory Causality Relationship by Wavelet Method,April 2015,Yushu Li,,,Unknown,Unknown,Unknown,Unknown,,
45.0,4.0,Computational Economics,10 April 2014,https://link.springer.com/article/10.1007/s10614-014-9442-y,Spatial Dynamics of Optimal Management in Bioeconomic Systems,April 2015,David Aadland,Charles Sims,David Finnoff,Male,Male,Male,Male,"Spatial dynamic problems have gained increasing attention in the economics literature, particularly in the areas of international trade policy (Krugman 1991), urban growth (Fujita et al. 1999), labor migration (Tabuchi and Thisse 2002), and environmental policy (Fredriksson and Millimet 2002). Research in the optimal management of ecosystems and natural resources has also started to integrate space into dynamic control problems. Research in this area has focused on metapopulation models with native species migration (e.g., Bhat and Huffaker 2007; Brown and Roughgarden 1997; Bulte and Kooten 1999; Costello and Polasky 2008; Horan et al. 2005; Sanchirico and Wilen 1999; Smith and Wilen 2003), invasive species (Albers et al. 2010; Epanchin-Niell and Wilen 2012; Sanchirico et al. 2010), forest ecosystem management (Albers 1996; Hof et al. 1994; Konoshima et al. 2008; Rosie 1990; Swallow et al. 1997; Swallow and Wear 1993), and general institutional and policy design issues (Brock and Xepapadeas 2010; Kaffine and Costello 2011; Sanchirico and Wilen 2005; Swallow et al. 1990). Combining space and time in optimal control problems creates significant degrees of analytical and computational complexity. To facilitate a solution, researchers have restricted the dimension of the problem by limiting the degree of human behavior, choice variables, number of species, number of patches, number of time periods, migration patterns, or interdependency of spatial patches. For example, Smith et al. (2009) state: “A key feature of virtually all resource models that incorporate both space and time is that spatial characteristics are described by state variables that are indexed by space, but not functionally interdependent over space.” Our research builds on these existing studies by addressing the limitations discussed above. Using standard methods from the macroeconomics literature (Clarida et al. 2000; Farmer 2002), we develop an approximately optimal linear management rule under rational expectations that accounts for temporal and spatial dynamics. Our linearization method is similar to the approximation discussed in Brock and Xepapadeas (2008). To assess the size of the approximation error, we solved for the optimal nonlinear harvesting path on a smaller grid using GAMS. The approximate linear and nonlinear harvesting paths were similar and provide confidence in the method. The management rule is used in conjunction with a nonlinear system of states to simulate optimal paths across space and time. This procedure allows the solution of continuous control problems that are functionally interdependent over a large spatial grid. This approach has two key advantages. First is its ability to accommodate multiple, continuous state variables over a large spatial domain. This is an essential feature in many natural resource and environmental applications where migration or dispersal patterns are governed by species interactions such as invasive species, agricultural pests, fisheries, and epidemiology. Second it allows for spatially decentralized management characterized by a Markov Nash equilibrium strategy for each local manager. To our knowledge, this is the first time optimal management over space has been considered in a system with continuous variables and more than two decision makers. To highlight the power of this approach, we apply it to the case of a pest species (mountain pine beetle) spreading across a heterogeneous landscape in order to prey on a valuable natural resource (trees). The application illustrates several interesting features of managing pest risk and spatial bioeconomic systems in general. First, migration and optimal resource harvesting result in a complex spatial configuration of pest and natural resource densities. This occurs because proximity to a habitat boundary creates differential pest risk across the landscape grid. Second, there exists a fundamental tradeoff between resource harvesting and insect epidemics. While increased resource harvesting mitigates the severity of pest outbreaks (Sims et al. 2010), it has the unintended effect of increasing the rate of pest spread over space (Konoshima et al. 2008). The intuition is straightforward. When pests disperse across the landscape in a density-dependent fashion, increased resource harvesting causes pests to migrate to areas with more abundant resources (prey). Third, local resource harvesting generates two opposing spatial externalities. A reproduction externality causes local producers to under-harvest as they fail to recognize how harvesting decreases neighboring pest populations. A migration externality results in local producers over-harvesting so pests migrate off their management unit. In a calibrated version of our model these two spatial externalities approximately offset.",25
45.0,4.0,Computational Economics,11 April 2014,https://link.springer.com/article/10.1007/s10614-014-9441-z,Option Pricing and Distribution Characteristics,April 2015,David J. Mauler,James B. McDonald,,Male,Male,Unknown,Male,"Both market participants and scholars alike have devoted significant attention to the groundbreaking result by Black and Scholes (1973), in which the theoretical price of a stock option is derived. The subject of extensive examination and research in financial and economic literature, the Black-Scholes model has been extended to appropriately fit a variety of underlying assets, while its limitations have also been identified and corrected for with varying success. One primary shortcoming in the original model is the assumption that the underlying asset follows geometric Brownian motion, implying a lognormal return on the distribution of the asset upon expiration. A wide variety of applications often show the underlying asset to exhibit levels of skewness and kurtosis inconsistent with the lognormal distribution. This inconsistency gives rise to the so-called volatility smile or grin in option pricing. Implied volatility refers to the variance of the asset’s return upon expiration associated with the observed market price via the Black-Scholes model. In essence, knowing the actual price, interest rate, and expected return, one can back out the implied variance by inverting the formula for option prices which assume lognormal returns. In theory, implied volatility should not vary across exercise prices when options share a common underlying asset and expiration date. However, since investors expect more outliers than modeled by the lognormal pdf, a premium is placed on call options farther out-of-the-money (being more likely to be exercised than if returns were indeed lognormal), explaining the volatility smile.Footnote 1 Thus, the smile reflects the need for additional flexibility in the assumed distribution. Using other more flexible distributions is an attractive alternative to merely assuming lognormal returns. In essence, this allows the model to account for the impact of the volatility term structure that the Black-Scholes model ignores. For example, McDonald and Bookstaber (1991) present an option formula based on the generalized beta of the second kind (GB2). Other distributions examined in this context include, among others, the Burr-3 (Sherrick 1996), reciprocal gamma (Milevsky and Posner 1998), Weibull (Savickas 2001), \(g\)-and-\(h\) (Dutta and Babel 2005), and generalized gamma (Fabozzi et al. 2009). Just as observed european-style option prices can be used to estimate implied volatility, they can also be used to estimate the asset’s ex-ante risk-neutral probability distribution, or its implied distribution. A method for doing so is presented at the end of Sect. 2. Bahra (1997) argues that such implied distributions can be useful in gauging market sentiment and conveying forward-looking information relevant in determining monetary policy. Of particular interest is the relative performance of these assumed distributions when estimated accordingly, as well as the tradeoff between increased pricing accuracy and increased computational complexity. In Sect. 2, we present option pricing formulas specific to the lognormal, Weibull, reciprocal gamma, Burr-3, Burr-12, GB2, and \(g\)-and-\(h\). Also included is the inverse hyperbolic sine (hereafter IHS) distribution, which has yet to be used in option pricing, particularly noteworthy in the light of findings by McDonald and Turley (2011) where the IHS was shown to exhibit flexibility in modeling kurtosis and skewness comparable to the \(g\)-and-\(h\) (for \(h\ge 0\)) and superior to many other distributions (see Fig. 1). This suggests IHS options pricing could make a valuable contribution to pricing assets with smiles. Using these results, we then proceed to estimate distributional parameters for each pdf implied from observed prices on S&P 500 Index call options. Section 3 contains a discussion of the data used and Sect. 4 presents the relative performance of each distribution. Concluding thoughts follow in Sect. 5. Moment spaces for the IHS, g-and-h, LN, and GB2. Note that \(h\ge 0\) for the \(g\)-and-\(h\)
",5
45.0,4.0,Computational Economics,10 April 2014,https://link.springer.com/article/10.1007/s10614-014-9436-9,Will the Bail-in Break the Vicious Circle Between Banks and their Sovereign?,April 2015,Clara Galliani,Stefano Zedda,,Female,Male,Unknown,Mix,,
45.0,4.0,Computational Economics,20 March 2014,https://link.springer.com/article/10.1007/s10614-014-9429-8,Multiscale Analysis of the Liquidity Effect in the UK Economy,April 2015,Antonis A. Michis,,,Male,Unknown,Unknown,Male,"The study of interest rate determinants is important to financial institutions because changes in interest rates can greatly influence the profitability of investments. For this reason, many institutions use sophisticated forecasting methods in order to accurately predict changes in equilibrium rates (Mishkin and Eakins 1998, p.128). The liquidity preference theory for the determination of interest rates suggests that changes in equilibrium rates result from changes in the demand for and supply of money. Increases in the money supply by monetary authorities lead to a reduction in interest rates (i.e., the liquidity effect). On the other hand, changes in demand result from income and price effects that are positively related to interest rates. The liquidity effect generally has an immediate influence on interest rates, while income and price effects can involve substantial time lags. This is because increases in the money supply take time before they increase prices and income in the economy. Expectations of inflation also have an important influence on interest rates. It is important for monetary policy to identify the magnitude and speed of adjustment associated with each effect (Mishkin and Eakins 1998, p.123). Following an increase in the money supply, interest rates can rise or fall depending on the size of each effect, and results can vary by time-scale or cycle (e.g., short-term, medium-term and long-term). In order to analyse the liquidity effect, this study estimates regression models at different frequency bands as proposed by Cochrane (1989). However, instead of using a band-pass filter the model’s variables are analysed with a wavelet multiresolution analysis, as suggested by Ramsey and Lampart (1998a, b), in order to examine the liquidity, income and price effects on interest rates in the UK economy by time-scale. Ramsey and Lampart’s methodology is also extended by standardising the time-scale regression models so that all of the variables in a given time-scale are set on a common scale of measurement. This allows for a comparison of the three effects (liquidity, price and income). Results for the UK economy suggest that the magnitude of the liquidity effect varies by time-scale. When only short-term cycles are examined, the liquidity effect is higher than the income and price effects, but in the medium and long-term, the price and income effects become more important. The rest of this article is organised as follows: Section 2 reviews the main empirical evidence for the liquidity effect, and Sect. 3 provides a short introduction to wavelets and presents the standardised time-scale regression models. The data used in the analysis are explained in Sect. 4, and the results for the UK economy are presented in Sect. 5. In Sect. 6, an alternative modelling approach based on the Baxter-King filter is considered and in Sect. 7 the results for the UK economy are compared with France, Germany and Italy. Section 8 summarises the main findings of the article.",7
45.0,4.0,Computational Economics,27 March 2014,https://link.springer.com/article/10.1007/s10614-014-9432-0,Yield Curve and Recession Forecasting in a Machine Learning Framework,April 2015,Periklis Gogas,Theophilos Papadimitriou,Efthymia Chrysanthidou,Male,Unknown,Female,Mix,,
45.0,4.0,Computational Economics,11 April 2014,https://link.springer.com/article/10.1007/s10614-014-9440-0,Evaluating the Default Risk of Bond Portfolios with Extreme Value Theory,April 2015,Yong Ma,Zhengjun Zhang,Weidong Xu,,Unknown,Unknown,Mix,,
45.0,4.0,Computational Economics,11 April 2014,https://link.springer.com/article/10.1007/s10614-014-9437-8,A Behavioral Macroeconomic Model of Exchange Rate Fluctuations with Complex Market Expectations Formation,April 2015,Peter Flaschel,Florian Hartmann,Christian R. Proaño,Male,Male,Male,Male,"As discussed by Kindleberger and Aliber (2005), the evolution of the world economy has been characterized by recurrent financial and exchange rate crises which, due to their often devastating macroeconomic and social effects, have raised important issues for theorists and policy makers. In this context, the ruling paradigm in macroeconomics of Dynamic Stochastic General Equilibrium (henceforth, DSGE) modeling has done a rather non-convincing job in explaining the recurrent character of such financial phenomena and especially the recent global downturn as argued by Colander et al. (2009), and also admitted by some of its proponents such as Chari et al. (2009). Arguably, this unsatisfactory performance has not been the result of a lack of mathematical sophistication. Rather, it derives from the adoption of an equilibrium approach coupled with the assumption of Rational Expectations, which jointly seem questionable both from the methodological and the empirical perspective. In the behavioral finance literature—in contrast to the predominant macroeconomic literature—it is widely acknowledged that the rational expectations assumption is not able to explain basic stylized facts of financial markets—not only concerning crises—, see e.g. Hommes (2006) and Grauwe and Grimaldi (2006), and alternative expectations formation schemes are widely used. This paper proposes a number of departures from the DSGE methodology, which can be seen as the building blocks of a new approach in the Keynesian traditionFootnote 1, by constructing a macrodynamic model along the lines of Dornbusch (1976), which incorporates basic important feedback channels between the real and the financial sector, and in which markets are not assumed to jump to their equilibrium positions, but where dynamic adjustment processes are taking place. One of the key contributions of the paper is the explicit incorporation of market expectations or opinion dynamics in financial markets populated by heterogeneous agents. This allows us to examine the effects of herding and speculative behavior in a simple macrodynamic framework of a small open economy as proposed by Franke (2012). More precisely, we adopt the distinction between chartists and fundamentalists widely used in the literature on exchange rate dynamics, see e.g. Grauwe and Grimaldi (2005), Proaño (2011) and Chiarella et al. (2011). Chartists behave like speculators and can be seen as technical traders who adopt a simple adaptive expectation mechanism. In contrast, fundamentalists focus on basic economic data and expect variables to return to steady state values with a certain adjustment speed. Chartists tend to exert a destabilizing influence on the economy, whereas the presence of fundamentalists is generally stabilizing. Albeit simple, this description of agent heterogeneity on financial markets is consistent with empirical studies that analyze expectational heterogeneity, such as Frankel and Froot (1990) and Menkhoff et al. 2009). This description is also sufficient to examine some of the core features of financial markets that have played a prominent role in the recent global financial crisis. Overall market expectations are here a function of individual fundamentalist and chartist expectations, and depend on the relative weight of each group in the market. Hence, the model economy contains two potential sources of instability: the feedbacks between real and foreign exchange markets via the nominal exchange rate \(e,\) and the endogenous market expectations dynamics produced by the interaction of heterogeneous agents on asset markets. Thus, it allows us to investigate a key question emerging from the current financial crisis, namely whether unfettered, interconnected markets with heterogeneous agents are able to absorb external shocks, or rather tend to amplify them. We prove that the resulting 4D dynamical system describing the evolution of the economy always has either a single steady state characterized by uniformly distributed agents, or three steady states (one with uniformly distributed agents, a chartist and a fundamentalist one). Even though various subdynamics of the model can be stable (at the uniform or fundamentalist steady state), the complete system may be repelling around all of its equilibria. Given the complexity of the 4D nonlinear system, it is difficult to draw more precise conclusions on the overall dynamics (up to the consideration of two supplementing 2D cases). Therefore we adopt numerical methods to further explore the dynamic properties of the model. The numerical simulations show that the 4D system is indeed viable: all trajectories remain in an economically meaningful subset of the state space. In this sense, the model shows the somewhat surprising result that unfettered markets with possibly accelerating real-financial feedback mechanisms have some in-built stabilising mechanism (based on opinion dynamics) that prevent the economy to move on an infeasible path. Moreover, despite the trivial dynamics of the 2D subsystems, the full 4D dynamics can exhibit irregular and even complex motions. Hence, if all of the steady states are repelling, the system can exhibit irregular, though persistent real-financial market fluctuations.Footnote 2 The considered opinion dynamics is therefore capable of ensuring upper and lower turning points in the real-financial market interactions, but the generated persistent fluctuations may still be too large to be acceptable from the societal point of view. The remainder of the paper is organized as follows. In the next section we briefly discuss the Dornbusch (1976) exchange rate dynamics in order to motivate the subsequent framework as well as to highlight its main features. In Sect. 3 we then extend the Dornbusch framework through the incorporation of heterogeneous behavioral expectations and endogenous market expectations formation. We analyze the resulting framework by means of numerical simulations in Sect. 4, focusing on the stability of the system, and in Sect. 5 possible policy measures to tame fluctuations are reviewed. Section 6 draws some concluding remarks.",15
45.0,4.0,Computational Economics,26 April 2014,https://link.springer.com/article/10.1007/s10614-014-9430-2,Using the “Chandrasekhar Recursions” for Likelihood Evaluation of DSGE Models,April 2015,Edward Herbst,,,Male,Unknown,Unknown,Male,"Dynamic Stochastic General Equilibrium (DSGE) are increasingly estimated by Central Banks and academic economists. In estimation, the model equilibrium conditions can be linked to the data using a Linear Gaussian State Space (LGSS) representation. As model complexity increases, so too does computational time. In maximum likelihood and Bayesian contexts, the log likelihood has to be evaluated hundreds of thousands or millions of times, usually in sequential fashion.Footnote 1 In this enviroment, likelihood computation time dominates the running time of the entire algorithm. So it becomes crucial to construct efficient algorithms for likelihood evaluation and state filtering. To wit, considerable effort has been expended constructing elaborate filters tailored to DSGE models—see, for example, Strid and Walentin (2009). The purpose of this note is to report an old and simple algorithm for fast likelihood evaluation outlined in Morf (1974) and Morf et al. (1974) and show that it is ideally suited for DSGE models. The method, which we will call the “Chandrasekhar Recursions” (CR), is simple to implement and can yield considerable speed improvements. This paper is closely related to Strid and Walentin (2009), who develop a “Block Kalman Filter” by exploiting the a priori known structure of the DSGE model to avoid some large matrix operations. The algorithm must be applied on a case-by-case basis. We compare the algorithms in the example section. Moreover, in principle, one could use the CR and also exploit the block structure of the DSGE model. Finally, although the CR has not been used to aid the estimation of DSGE models, they have been employed in a general time series context before; see, for example, Klein et al. (1998). The paper is structured as follows. Section 2 contains background information on the DSGE models and the Kalman Filter, Sect. 3 contains the derivation of the Chandrasekhar Recursions, Sect. 4 contains four examples, and Sect. 5 concludes.",3
46.0,1.0,Computational Economics,03 June 2014,https://link.springer.com/article/10.1007/s10614-014-9447-6,Partially and Wholly Overlapping Networks: The Evolutionary Dynamics of Social Dilemmas on Social Networks,June 2015,Yanlong Zhang,,,Unknown,Unknown,Unknown,Unknown,,
46.0,1.0,Computational Economics,20 July 2014,https://link.springer.com/article/10.1007/s10614-014-9449-4,A Dynamic Discrete/Continuous Choice Model for Forward-Looking Agents Owning One or More Vehicles,June 2015,G. Cernicchiaro,M. de Lapparent,,Unknown,Unknown,Unknown,Unknown,,
46.0,1.0,Computational Economics,20 July 2014,https://link.springer.com/article/10.1007/s10614-014-9450-y,Towards a Holistic Approach for Mutual Fund Performance Appraisal,June 2015,Vassilios Babalos,Michael Doumpos,Constantin Zopounidis,Male,Male,Male,Male,"Mutual funds remain the most popular investment vehicle among individual investors in the US. Despite the decline that followed the outbreak of the global financial crisis of 2008, combined assets of mutual funds in the US at the end of 2010 amounted to $11.8 trillion, while worldwide mutual fund assets stood at the end of 2010 at $24.7 trillion (Investment Company Institute 2011). According to Turtle and Zhang (2012), 90 million individuals hold mutual funds in the United States, and of these individuals, 65 % have more than half of their financial assets invested in mutual funds. A major challenge faced by the mutual fund industry is the proper assessment of the performance of actively managed portfolios. Given the delegated nature of active management industry, the plethora of offered funds (the number of available open-end funds in the US at the end of 2010 was 7,580) and the existing framework with respect to investors’ selection, fund rating agencies have become an integral part of the investment process. Morningstar was the first to introduce the well-known mutual fund rating system in 1986 that has attracted attention from both investors’ and the academic community and has become the key ingredient that investors and financial advisors use when formulating their investment strategies. The issue of whether managers of active portfolios add value still remains controversial. Traditional performance measures compare the return of the examined portfolio with the return of a properly defined unmanaged portfolio (benchmark return) after accounting for all aspects of investment risk. The evolution of financial theory has contributed substantially to the proper definition of systematic risk sources that should be accounted for when evaluating the performance of skilled fund managers. In this context, the single factor evaluation model introduced by Jensen (1968) has been replaced by multi-factor models motivated mainly by the pioneer findings of asset pricing studies such as Fama and French (1993, 1996). In particular, the ability of mutual funds’ managers to achieve superior risk-adjusted returns compared to passively managed portfolios has been extensively studied by a broad range of authors starting with the seminal works of Treynor (1965), Sharpe (1966), and Jensen (1968). Several issues have been examined, such as the performance persistence in mutual fund rankings (Gruber 1996; Carhart 1997), the relation between performance, fund attributes, and economies of scale (Prather et al. 2004; Chen et al. 2004), the role of chance in portfolio performance (Kosowski et al. 2006; Fama and French 2010), and the importance of incorporating economic indicators in predicting future market movements (Jha et al. 2009; Kosowski 2011). However, traditional performance measures namely the Treynor ratio (1965), the Sharpe ratio (1966), and Jensen’s alpha (1968) are plagued with two shortcomings that have spurred the academic and practitioners’ research for the past couple of decades. First, they are rooted in CAPM theory, thus being exposed to standard criticism made against it (Roll 1977, 1978). In addition, they fail to incorporate explicit and implicit costs associated with running and managing a mutual fund (Ippolito 1993). Therefore, apart from the widely employed factor models (Aragon and Ferson 2006; Ferson 2010; Angelidis et al. 2013) another strand of literature has embarked on adopting production measurement techniques such as frontier analysis, and more specifically data envelopment analysis (DEA), as a tool for assessing managed fund performance. Such an approach can tackle effectively the aforementioned limitations of traditional performance measures providing a more holistic approach in the context of portfolio performance assessment. The extensive appeal of DEA-based performance measures is motivated mainly by the simultaneous handling of multiple outputs and input measures, including risk-return attributes, transaction costs, and the cost of obtaining and using information (Ippolito 1993), which have a non-negligible impact on fund performance. In the same vein, as Glawischnig and Sommersguter-Reichmann (2010) pointed out, the derivation of input and output weights endogenously and a single real number performance index provided by DEA appear as the most desirable properties in favor of the method. DEA is a nonparametric approach that has been originally introduced for investigating the productive efficiency of public, not-for profit decision making units. Shortly after, the method has entered into the financial sector with many applications in measuring not only banks’ performance (Berger and Humphrey 1997; Fethi and Pasiouras 2010) but also traditional mutual funds’ performance (Murthi et al. 1997; McMullen and Strong 1998; Galagedera and Silvapulle 2002; Basso and Funari 2001, 2005; Lozano and Gutiérrez 2008; Zhao et al. 2011) and more recently for alternative funds (Gregoriou 2003; Gregoriou et al. 2005). However, despite its usefulness for efficiency evaluation, DEA results do not allow the comparison of all mutual funds in a common setting, as each fund is evaluated under a different weighting of the available input–output variables. Furthermore, in basic DEA models all efficient funds are considered equal, thus not allowing the investor to discriminate between them. The existing approaches to handle these issues include alternative DEA models that could provide a ranking of the funds (Adler et al. 2002), or multi-stage analyses based on linear regression techniques (McDonald 2009; Wang et al. 2011). In this paper, we employ a multicriteria methodology based on a nonlinear additive value function. The resulting model resembles the efficiency estimates of DEA, but it can also be used for performance evaluation and benchmarking purposes. Its additive form makes the model easy to comprehend and implement to any set of funds, independently of the sample used to obtain the DEA efficiency estimates. The multicriteria model provides insights into the relationship of several fund appraisal criteria with the overall performance and efficiency of the funds, including attributes that describe the funds’ investment style. In the above context, this paper adds some important contributions to the relevant literature. Firstly, we contribute to the ongoing research that is concerned with the evaluation of funds’ performance based on nonparametric techniques. In line with recent findings of Lamb and Tee (2012) and Premachandra et al. (2012) who concluded that DEA efficiency estimates of fund performance might be plagued with substantial bias and could be misleading, we opt for an innovative two-stage process of performance evaluation, with the advantages discussed above. Furthermore, through the DEA efficiency analysis and the multicriteria modeling approach, we re-examine the role of risk-return criteria in mutual funds’ performance under different evaluation horizons, investigate the role of the recent crisis, and analyze the relationship between the efficiency/performance of the funds and their investment policies. Finally, the comparative analysis of the obtained multicriteria evaluation of the funds with the ratings of Morningstar provides insights on the connections between the ratings and the efficiency of the funds. The results regarding our integrated performance measure are quite promising in terms of its economic significance. The combination of DEA with the multicriteria methodology is applied to a sample of US no-load equity managed funds. First, we examine their efficiency in the traditional sense using up to date data over the period 2003–2010 and then, following Murthi et al. (1997), we provide updated evidence on the existence of a link between managed fund ratings and efficiency on the basis of the multicriteria evaluation results. Previewing our results, we document a significant negative relationship between funds’ trading activity and funds’ performance and a substantial surge in funds’ portfolio risk which in turn places the ability of fund managers to control investment risk under question especially in the aftermath of the global financial crisis. Moreover, we incorporate into our analysis, for the first time in the relevant literature, the role of funds’ flows. The remainder of the paper is structured as follows. Section 2 describes the techniques used in the paper, including DEA as well as the multicriteria evaluation procedure. Section 3 is involved with the description of the data set, whereas Sect. 4 presents in detail the obtained results. Finally, Sect. 5 concludes the paper and discusses some possible future research directions.",13
46.0,1.0,Computational Economics,25 July 2014,https://link.springer.com/article/10.1007/s10614-014-9455-6,Earnings Per Share Forecast Using Extracted Rules from Trained Neural Network by Genetic Algorithm,June 2015,Hossein Etemadi,Ahmad Ahmadpour,Seyed Mohammad Moshashaei,Male,Male,Male,Male,"Future decisions are uncertain, and the winner in this competition is the one who can forecast the future or at least get enough information about it and decide upon that. Offering forecast patterns is a way that can help the investors to make good decisions. Accounting profit is the main product of accrual accounting, from investors’ perspective, any other value in financial statements is not more important than earnings per share (EPS) Beaver et al. (1989). Profit forecasting is an attractive activity for managers and investors. Investors use this prediction as the basis of selecting a profitable and optimum portfolio Cao and Parry (2009). Also, forecasting EPS helps managers to decide for critical situations like operational budgeting Zhang and Cao (2004) capital budgeting, and optimum resource allocation Cao and Parry (2009). The Securities & Exchange Commission (SEC) in early 1973, for the first time, allowed the listed firms to disclose earnings forecast, and after two years forced them to reveal such information. By the same token, in 2002, the Tehran Stock Exchange (TSE) in Iran mandated EPS forecasting for listed firms based on paragraph G article 5 of Information Disclosure by law Mashayekh and Shahrokhi (2008). Prior research on earnings forecast can be categorized in two orders: Group1- Research using data of past years’ earnings, and time series models for earnings and EPS forecasting, which includes: Richard used time series models to EPS forecasting and then compared the result with analysts prediction about EPS. He collected analysts forecasts on 92 firms listed in New York Stock Exchange in 1972–76 and found that the relative error by analysts was 24.1 %, while the error by time series models was 38.9 %. Thus, analysts prediction was more accurate Richard (1977). Finger checked past earnings capability to forecast future earnings and cash flow. Results confirm that using past earnings data is significantly viable for earnings forecasting in the 88 sampled companies Finger (1994). Morton checked the relationship between future earnings and historical earnings using various forecasting models and found that if a market can fully understand earnings variation; more accurate forecasting will be feasible based on the forecasting model of each company to determine relations between historical and future earnings Morton (1998). Group2-Research using fundamental accounting variables and linear or non-linear models for earnings forecast which includes: Gonedes investigated the information content of six financial ratios and stock earnings. Financial ratios include liquidity ratio, capital structure, work in capital, and two profitability ratios and found there was information content in financial ratios Gonedes (1974). Abarbanell & Bushee used multivariate linear regression for earnings forecasting and found there might be a non-linear relationship between some accounting variables and earnings per share or future earnings Abarbanell and Bushee (1997). In fact, the result of their research was a starting point for using non-linear models in earnings or EPS forecasting. Therefore, Cao & Zhang Zhang and Cao (2004) studied 283 companies in 41 industries, in 2004, and used 4 models for EPS forecasting: linear univariate, linear multivariate, univariate neural network and multivariate neural network. They found that neural networks can better forecast EPS relative to traditional linear models Zhang and Cao (2004). Cao & Parry studied the accuracy of EPS prediction using neural network model and found that a neural network model whose weight is estimated by genetic algorithm is more accurate Cao and Parry (2009). In this research, we examine annual EPS forecast using Multi-layer perceptron neural network (MLP) and extracted rules from MLP neural networks by genetic algorithms based on fundamental accounting variables and comparison their forecast accuracy.",7
46.0,1.0,Computational Economics,12 April 2014,https://link.springer.com/article/10.1007/s10614-014-9438-7,Measuring Risk in Fixed Income Portfolios using Yield Curve Models,June 2015,João F. Caldeira,Guilherme V. Moura,André A. P. Santos,,Male,Male,Mix,,
46.0,1.0,Computational Economics,25 July 2014,https://link.springer.com/article/10.1007/s10614-014-9452-9,A New Methodology for Estimating Internal Credit Risk and Bankruptcy Prediction under Basel II Regime,June 2015,M. Naresh Kumar,V. Sree Hari Rao,,Unknown,Unknown,Unknown,Unknown,,
46.0,1.0,Computational Economics,11 November 2014,https://link.springer.com/article/10.1007/s10614-014-9473-4,Financial Transaction Tax: Policy Analytics Based on Optimal Trading,June 2015,Edward W. Sun,Timm Kruse,Min-Teh Yu,Male,Male,Unknown,Male,"The financial sector is considered to be the major cause of the latest global crisis. The European Commission on 28 September, 2012 put forward a proposal for a financial transaction tax (hereafter, FTT)Footnote 1 attempting to make financial market players take more responsibility for resolving the crisis that they caused and to discourage excessive risk-taking in the future. The European Parliament gave its consent to the proposal on 12 December, 2012.Footnote 2 Having obtained European Parliament’s consent, the European Commission will turn the FTT plans into reality after being authorized by the European Council. There are many examples of financial transaction taxes applied in many countries on spot share trading and derivatives transactions. For example the UK has a stamp duty reserve tax (SDRT) on equities, which is a 0.5 % tax on the value of spot transactions in shares of UK companies. A tax rate (or fee) of between 0.15 and 1 % is executed in Australia, Brazil, China (0.1 %), Hong Kong (0.1 %), India (0.25 %), Indonesia (0.1 %), Ireland (1 %), Switzerland (0.15 %), Taiwan (0.3 %), Singapore (0.2 %), South Africa (0.25 %), South Korea (0.5 %), and the U.S.Footnote 3 Particularly, Taiwan’s transaction tax is rather broad and covers various kinds of securities, including bonds and futures contracts. In addition, France’s amendment to FTT (from 0.1 to 0.2 %) has been approved by the French National Assembly and the French Senate and went into effect on 1 August, 2012. The issue of whether FTT should be levied has been controversial ever since it was suggested to reduce destabilization in equities by Keynes (1936) and destabilization in currency speculation by Tobin (1978). The latest debate has been proposed by Schäfer et al. (2012). Schulmeister et al. (2008) point out that such a debate focuses on the answers to three questions. First, is there excessive trading in financial markets that drives prices to extremely fluctuate over the short run as well as over the long run? Second, could a small tax on financial transactions dampen destabilizing speculation without distorting liquidity beyond the level needed for market efficiency? Third, will the revenues from a general FTT even at a low tax rate be substantial relative to the costs of its implementation? There exists remarkable discrepancy between the answers to each of these three questions based on theoretic and empirical investigations. Based on various perspectives about trading and price dynamics in asset markets and the effects of a transaction tax, FTT proponents state that a transaction tax increases the costs of speculative trades when the frequency of such trades becomes greater. Consequently, it could stabilize asset prices and thereby improve the overall macroeconomic performance. Relying on the perception of trading and price dynamics being fundamentally different from that of FTT proponents, FTT opponents state that a transaction tax decreases liquidity, which in turn increases the short-term volatility of asset prices, consequently reducing market efficiency. In this paper we build a model based on optimal trading to explain there does exist a win-win situation for both government and market participants under FTT. In our model, market liquidity could be preserved and liquidity suppliers can even be better off after conducting an optimal trading strategy after introducing a transaction tax. We assume herein that oligopolistic financial institutions have the obligation to continuously make market (i.e., to maintain liquidity). The objective is then to minimize the transaction cost if financial institutions intend to maintain the same liquidity level as they did before FTT’s introduction. Adhering to the market microstructure theory, market markers will enlarge their quotation spreads to compensate for the profit reduction caused by FTT. The spread is the difference between the best bid (highest price a buyer is willing to pay) and the best ask (lowest price a seller is willing to accept). Previous studies have shown that the bid–ask spread is determined by trading activity, risk of holding inventory, adverse information, and competition. In order to maintain market liquidity, market makers (specialists) are compensated by buying at a low price and selling at a high price—that is, the spread must cover costs incurred by the market makers. As a consequence, the bid–ask spread turns out to be inferred directly from the transaction prices (see, for example Schwartz 1988; McInish and Wood 1992). The literature often assumes the bid–ask spread to be a constant (see, for example Bertsimas and Lo 1998; Almgren and Chriss 2000; Obizhaeva and Wang 2013; Kruse and Sun 2014). Some empirical studies show that the bid–ask spread over a fixed time interval displays a power-law distribution and long-range dependence, and there exists a logarithmic relation between the bid–ask spread and the trade volume (see Plerou et al. 2005; Ponzi et al. 2009). Our model assumes market makers will adopt a linear spread function that contains two parts: a constant and a variation. The constant part covers all common components (e.g., explicit costs and adverse selection) discovered in the literature. The variation part in our model depends on the previous trading volume. The more the market maker trades in the previous trading period, the higher the FTT compensation he requires in the current trading period. Similar to Bertsimas and Lo (1998); Almgren and Chriss (1999, 2000); Obizhaeva and Wang (2013); Sun et al. (2013), and Kruse and Sun (2014), we assume the market maker faces a linear price impact function, in which a linear combination of the permanent and temporary price impacts is characterized by a price impact function. Huberman and Stanzel (2004) indicate that the linear price impact function excludes the quasi-arbitrage (i.e., price manipulation) and supports viable market prices. We also follow other studies (see, for example Huberman and Stanzel 2004; Obizhaeva and Wang 2013) that focus on the discrete-time setting of the model, which helps us to derive the computational solution. In our model, we do not intend to explain the price volatility caused by a transaction tax due to the discrepant perception of FTT upon prices debated by its proponents and opponents of FTT. Instead, we consider the price to be exogenous and decided by the market and assume two different scenarios for the underlying price dynamics: the underlying price change follows the Brownian motion as suggested by Obizhaeva and Wang (2013) and follows the geometric Brownian motion as suggested by Ting et al. (2007) and Sun et al. (2013). In this paper we look at three different trading strategies for the financial institutions that make market by supplying liquidity in oligopolistic market. The first trading strategy is naive trading, which simply splits large orders into equalized small pieces so as to reduce the price impact without considering any optimal adjustment. The second trading strategy is based on the model proposed by Sun et al. (2013), which maintains a constant spread assumption and adopts an optimal trading adjustment to reduce the transaction cost. The third trading strategy is our proposed optimal trading strategy, which takes into account FTT’s influence on the spread (i.e., described by a linear spread function) and optimally chooses a trading strategy to reduce the transaction cost. We show the importance of incorporating the linear spread function in finding the optimal solution under these two price dynamics by presenting the superior performance of our strategy over two other strategies. We take the volume weighted average price (VWAP) as the benchmark to decide the during-trading cost measure of these trading strategies (see, for example Werner 2003; Goldstein et al. 2009). Only the trading strategy with the lowest (highest) VWAP for buying (selling) is preferred. Our numerical results indicate, based on the VWAP benchmark, that the overall performance of our optimal trading strategy dominates the other alternative trading strategies, implying that traders can in fact improve their profitability by applying an optimal trading strategy after the introduction of a financial transaction tax without distorting market liquidity. We organize the paper as follows. Section 2 sets up the fundamental model. Section 3 describes our contributions and introduces the analytical solutions to the optimization problem of minimizing the transaction cost—that is, we consider the linear bid–ask spread function and allow the underlying price dynamics to follow both the Brownian motion and geometric Brownian motion. Section 4 investigates the performance of our model by running large sample simulations and reports our numerical results. Section 5 summarizes our conclusions.",3
46.0,1.0,Computational Economics,25 July 2014,https://link.springer.com/article/10.1007/s10614-014-9454-7,Optimal Investment for the Insurers in Markov-Modulated Jump-Diffusion Models,June 2015,Jinzhi Li,Haiying Liu,,Unknown,Unknown,Unknown,Unknown,,
46.0,1.0,Computational Economics,29 April 2014,https://link.springer.com/article/10.1007/s10614-014-9444-9,Programming Identification Criteria in Simultaneous Equation Models,June 2015,George E. Halkos,Kyriaki D. Tsilika,,Male,Female,Unknown,Mix,,
46.0,2.0,Computational Economics,17 August 2014,https://link.springer.com/article/10.1007/s10614-014-9460-9,Numerical Policy Error Bounds for \(\eta \)-Concave Stochastic Dynamic Programming with Non-interior Solutions,August 2015,Huiyu Li,,,Unknown,Unknown,Unknown,Unknown,,
46.0,2.0,Computational Economics,23 July 2014,https://link.springer.com/article/10.1007/s10614-014-9451-x,Agent Heterogeneity and Facility Congestion,August 2015,Taiyo Maeda,Shigeru Matsumoto,Tadahiko Murata,Male,Male,Male,Male,"The benefits and costs of similar public services are affected by the congestion level. For example, if many people are waiting in line at the postal office, they must wait to obtain the postal service. Irrespective of the amount of time spent queuing, the customers ultimately obtain the same postal service. Another example concerns class size: if a large number of students are assigned to a class, the classroom is crowded. Although the opportunity cost of receiving the lecture is the same, the benefit derived from the educational service decreases as the number of students involved increases. Moreover, public service users typically expect the government to provide the same service quality at all facilities. For example, patients presume that they can access the same health care services at all public clinics or parents hope that the same educational standard is available at all public schools. Hence, the government cannot often differentiate service quality among public facilities. This restriction makes the optimal allocation of facility users an important research topic for the provision of public services characterized by so-called congestion or consumption externalities.Footnote 1
 
Tiebout (1956) argues that individuals sort themselves in a way that provides the most desirable allocation of public goods. If the same service is available at all facilities, a rational agent will move to a less crowded facility. Therefore, the congestion problems of public facilities are resolved through “voluntary sorting” (Tiebout 1956). However, three questions naturally arise about voluntary sorting. First, can we actually achieve the optimal user allocation through voluntary sorting? Second, once the optimal user allocation has been achieved, can it be maintained? Finally, can we find a mechanism that reduces deviation from the optimal user allocation? The existence and nature of the equilibria of voluntary sorting have been studied in the literature that focuses on club economies. Scotchmer and Wooders (1987), for example, examine whether the major conclusion from the theory of competition in private-goods economies applies to club economies with anonymous crowding.Footnote 2 They argue that consumer demand for facility size and crowding must be similar in each club when consumers are grouped into the approximate core. Similarly, Milchtaich (1996) analyzes voluntary sorting as a class of noncooperative crowding games. He assumes that players are differently affected by congestion and shows that this class of crowding game does not possess a pure Nash equilibrium in general. Further, Bogomolnaia and Nicolò (2005) examine the stable assignment of public facilities in the presence of consumption externalities, showing that there is no strategy-proof, efficient, and stable allocation rule if more than two groups have to be formed. Although congestion problems can be resolved through the relocation of public facilities, this action incurs substantial transaction costs and is unfeasible in practice. In this vein, in a laboratory setting, Selten et al. (2007) analyze commuters’ route choice behavior in the face of traffic jams, a significant problem in many countries. They find that subjects keep changing their routes even in a substantially long experiment and conclude that fluctuations around the pure equilibrium are a much better explanation of commuters’ route choice behaviors than the pure equilibrium. This finding implies that the optimal allocation of commuters can be achieved through voluntary sorting but not maintained. The authors further examine whether additional feedback information (the provision of congestion information in the previous period in this case) mitigates these congestion problems and show that commuters change their routes less frequently when such additional feedback information is provided. This paper extends the analysis of Selten et al. (2007) by allowing cost heterogeneity across agents. Although Selten et al. (2007) assume that agents are homogeneous, we assume that they are heterogeneous. Because we assume that subjects are differently affected by congestion levels as in the real world, it is worthwhile examining whether cost heterogeneity influences the facility choices of agents.Footnote 3 Further, because economic agents may reach an equilibrium through an adaptation process (Kandori et al. 1993; Young 1993), we aim to identify the specific adaptation process that characterizes the facility choice behavior of agents. Selten et al. (2007) use a reinforcement learning model in order to simulate commuters’ route choice behaviors and identify the optimal parameter set that simulates the route selections of commuters. In this paper, by contrast, we create state-action tables based on the presented experimental data in order to simulate the facility choices of agents. This approach is commonly used in the operations research literature. Specifically, we consider the following questions. Does cost heterogeneity affect the distribution of facility users? Do high-cost agents change facilities more or less frequently than low-cost agents? As we report below, the theory of congestion games predicts that cost heterogeneity will not influence an agent’s facility choice behavior. To examine the validity of this prediction, we create an original computer network system and conduct laboratory experiments. The structure of the remainder of the paper is as follows. The experimental set-up is described in Sect. 2. In Sect. 3, we summarize the behavior of agents. The experimental results demonstrate that cost heterogeneity affects neither the facility choices of agents nor the congestion levels of facilities, thereby supporting the theoretical prediction. In Sect. 4, we create state-action tables to analyze the adaptation process of the agent. We consider two types of decision rules: (i) an agent chooses the next action according to the current congestion level and (ii) an agent chooses the next action according to a change in the reward condition. We find that the simulation that applies the former decision rule fits the laboratory-derived data better. We summarize our findings and conclude the paper in Sect. 5.",1
46.0,2.0,Computational Economics,12 April 2014,https://link.springer.com/article/10.1007/s10614-014-9439-6,Costly Information in Markets with Heterogeneous Agents: A Model with Genetic Programming,August 2015,Florian Hauser,Jürgen Huber,Bob Kaempff,Male,Male,Male,Male,"Grasping the role of information asymmetries between agents is key to a better understanding of markets. Over the past few decades models with one or two information levels have been developed and explored so we now have a good understanding of these. On this foundation we can move on to explore more complicated, and probably more realistic, settings with more than two information levels. Agent-based simulations are a suitable tool for this exploratory field as they allow for heterogeneous behavior of tradersFootnote 1 as well as for the application of sophisticated learning techniques (see e.g. LeBaron 2006). In this paper, we extend an agent-based model presented by Hauser and Kaempff (2011) to analyze the value of costly asymmetric information in a financial market. Agents in our model apply evolutionary learning techniques to optimize their trading performance. In that optimization process they endogenously choose between various levels of costly information, which allows us to derive clear results on the distinct value of each information level. The theoretical foundation of the paper is found in seminal work by Hayek (1945), Fama (1970), and Grossman and Stiglitz (1980). In his passionate paper in favor of free market economy Hayek (1945) praises the market price’s ability to act as disseminator of information. Private information is aggregated in the market price, which informs everybody about the relative value of an asset. For most agents gathering information in addition to the market price is not necessary, as the price is already the best signal available. In the same spirit, Fama (1970) popularized the efficient market hypothesis (EMH), stating that a market is efficient if prices at all times fully reflect all available information. This, however, raised questions about how efficient markets can become: Grossman (1976) formulated the “information paradox” to demonstrate that the strongest form of the EMH cannot hold: If all information were immediately reflected in prices, no excess profits could be earned even with insider information. As the acquisition and processing of information is costly, no agent would acquire information—but then prices could clearly not reflect all information. In another paper Grossman and Stiglitz (1980) thus postulate that markets require an “equilibrium level of disequilibrium” to function properly. Only then will agents invest time and resources to profit from the small inefficiencies present. Related setting with two information levels have been thoroughly explored [see e.g. models in the tradition of Kyle (1985), or more recently, Goldbaum (2006)]. Major contributions that analyze the value of information in financial markets can be found in the experimental literature, e.g. in Sunder (1992) and Huber et al. (2010). Sunder (1992) staid close to Grossman and Stiglitz (1980) and tested their proposition in a market with two information levels (informed and uninformed), while Huber et al. (2010) explored a model with five distinct information levels. Both studies found evidence mostly supporting Grossman and Stiglitz (1980), especially that some noise was evident in market prices. Considering the value of costly information, Huber et al. (2010) found that net returns do not differ across their five information levels, while gross returns are higher for better informed, as their information costs need to be covered. The paper that probably comes closest to our work is by Hauser and Kaempff (2011). They analyze the emergence of trading strategies in an agent-based simulation covering ten distinct information levels. As a main result, they find that less-informed agents often ignore the subset of information that is available to other agents as well. However, by endowing agents with a fixed allocation of cost-free information they cannot shed light on the exact value of each information level. The present paper tries to fill this gap by extending the model in Hauser and Kaempff (2011) with costly information. To endogenize agents’ information strategies, we allocate nine distinct information levels through an information market (either with fixed costs, or auctioned to the highest bidders) before agents trade in a call auction. Similar to Hauser and Kaempff (2011), we apply genetic programming to optimize trading strategies for the agents in our market.Footnote 2 This method allows us to mimic individual learning and is able to represent strategies of different size and shape, thus allowing the agents to process information in a near-optimal way. In every market, the performance of a trading strategy depends on the behavior of other traders (see LeBaron 2001), thus co-evolutionary learning ensures that agents may adapt to an ever-changing environment.Footnote 3 Applying genetic programming also seems timely, as many hedge funds and high-frequency-traders use these techniques to improve and optimize their trading strategies. We find that in the fixed-costs setting agents first exploit inefficient market prices by choosing high information levels before prices eventually become more efficient and most agents learn to optimize profits by remaining uninformed. In equilibrium most agents choose to buy no information, while some 20 percent of agents buy the highest information level. Market prices show some degree of noise, which compensates the latter agents for buying costly information. In the auction setting prices also start out inefficient, leading to agents submitting high bids for the highest three information levels. As prices become more efficient, agents learn to bid only for the highest information level in equilibrium. In all settings net returns for all agents and all information levels are the same and slightly negative. Market efficiency increases with lower information costs, as more agents choose the highest information level. All these findings support the propositions of Grossman and Stiglitz (1980), but go beyond them: they allow us to conclude that the equilibrium level of disequilibrium, as described by Grossman and Stiglitz (1980), may actually only consist of uninformed and fully informed traders, even if other levels of information are available. The paper is structured as follows: in Sect. 2 we present the market model. Section 3 discusses the optimization process and genetic programming. In Sect. 4 we present results and Sect. 5 concludes.",7
46.0,2.0,Computational Economics,03 May 2014,https://link.springer.com/article/10.1007/s10614-014-9445-8,How to use SETAR models in gretl,August 2015,Federico Lampis,Ignacio Díaz-Emparanza,Anindya Banerjee,Male,Male,Unknown,Male,"The most famous procedure to estimate a Self-Exciting Threshold Autoregressive (SETAR) model is that of Tong (1990), while the most common approach to testing and making inference is due to Chan (1993). A part the main approach of Tong and Chan, the method of Hansen (1996, 1997, 2000) represents the most interesting alternative. Indeed Hansen proposes a confidence interval for the threshold parameter and a bootstrap method to test the linearity of the model. In addition, Hansen states that the standard confidence intervals for the autoregressive coefficients of the SETAR model could be not correct in small samples and proposes another method to calculate them. The purpose of this paper is to make the Hansen’s procedures widely available and contribute to the diffusion of the SETAR models, both in research and teaching. Starting with the original Hansen code we develop a graphical user interface (GUI) in gretl with which estimate a SETAR model and make inference. Our work is another evidence of the use of gretl to disseminate and promote new econometric models; indeed joint to the explanation on how to use the GUI we present some additional functionalities of the program with which make a preliminary analysis previous to the estimation of the SETAR model. 
gretl is a free and open-source software, see Cottrell and Lucchetti (2011) for an overview on the software. But among the various free and open-source software econometrics packages, gretl has the advantage of having an excellent user interface for the average user, similar to other popular commercial programs. As Smith and Mixon (2006) highlight, this GUI is the major reason that this software is very useful for teaching. There are already many econometrics books and courses based on gretl, among them see Adkins (2010). Its easy use makes it advantageous and profitable for academic institutions. Another important characteristic of gretl that makes it suited for research purposes is its numerical accuracy, which is as good as or better than other commercial programs, as shown by Yalta and Yalta (2007) and Baiocchi and Distaso (2003). All these characteristics make gretl a very powerful tool in research, and an increasing number of econometricians have begun to write their own code directly in gretl. For our purposes, the most outstanding feature of gretl is its clear and easy programming language (hansl) that allows us to write many statistical and econometrics operations with few commands. The user can write code and run it in a command-line client (CLI also called gretl console). Moreover, this code can be easily packaged as a function, provided that it uses a GUI and is distributed to the scientific community which is exactly what we do with our code. Our function is freely downloadable from the gretl server (http://gretl.sourceforge.net/). Any user can study the code and search for bugs or, if necessary, modify the code. In this manner, our work can specifically contribute to the use of the SETAR models and the Hansen methodology. In the next section present briefly the SETAR model and the inference under Hansen’s approach. In the fourth section we present a typical case of study and how to use gretl to make a preliminary analysis of the data. Finally we explain the management of the GUI function, and how to estimate a SETAR model and make inferences based on this procedure.",1
46.0,2.0,Computational Economics,07 August 2014,https://link.springer.com/article/10.1007/s10614-014-9457-4,Investigating the Performance of Non-Gaussian Stochastic Intensity Models in the Calibration of Credit Default Swap Spreads,August 2015,Michele Leonardo Bianchi,Frank J. Fabozzi,,Female,Male,Unknown,Mix,,
46.0,2.0,Computational Economics,05 September 2014,https://link.springer.com/article/10.1007/s10614-014-9463-6,An Improved RBF Method for Solving Variational Problems Arising from Dynamic Economic Models,August 2015,A. Golbabai,A. Saeedi,,Unknown,Unknown,Unknown,Unknown,,
46.0,2.0,Computational Economics,05 June 2014,https://link.springer.com/article/10.1007/s10614-014-9448-5,Bootstraps for Meta-Analysis with an Application to the Impact of Climate Change,August 2015,Richard S. J. Tol,,,Male,Unknown,Unknown,Male,"Uncertainty is a key concern about climate change. Whereas the uncertainties about future emissions (Webster et al. 2003), the response of the climate system (Annan and Hargreaves 2011) and the implications for policy (Weitzman 2009) have been widely discussed, the uncertainties about the impact of climate change have received less systematic attention. This paper helps fill that gap. It explores the use of bootstrap and kernel methods for meta-analysis with few observations; and the use of meta-analysis for estimating the uncertainty about a relationship. It applies these methods to the impact of climate change. More precisely, the paper estimates (i) the probability density function of the impact on total economic welfare of a particular amount of climate change, as measured by the global mean surface air temperature; and (ii) how that probability density function changes for more or less severe climate change. Put differently, I estimate the mean impact, its standard deviation, the median, and all other statistical characteristics as a function of climate change. 
Tol (2012a) also explores the uncertainty about the total economic impact of climate change, using the smoothed bootstrap. That paper uses a single functional form for extrapolation. The current paper compares the smoothed bootstrap to the bootstrap and to kernel regression. Kernel regression analysis is extended to restrictions on the functional form. All procedures are readily replicable and can be applied to other data sets. In contrast to the previous paper, the current one tests alternative functional forms, so that we can include both risk and ambiguity aversion in the assessment (Lange and Treich 2008; Millner et al. 2013). The paper is organized as follows. Section 2 presents the data on the impacts of climate change, and Sect. 3 the impact functions used in previous studies. Section 4 discusses the statistical methods. Section 5 reviews the results for the impacts of climate change, and Sect. 6 for its certainty and ambiguity equivalents. Section 7 concludes.",7
46.0,2.0,Computational Economics,04 September 2014,https://link.springer.com/article/10.1007/s10614-014-9462-7,A Complementarity Approach to Solving Computable General Equilibrium Models,August 2015,Sou-Cheng Terrya Choi,,,Unknown,Unknown,Unknown,Unknown,,
46.0,2.0,Computational Economics,13 May 2014,https://link.springer.com/article/10.1007/s10614-014-9446-7,Data Checking and Econometric Software Development: A Technique of Traceability by Fictive Data Encoding,August 2015,Rodolphe Buda,,,Male,Unknown,Unknown,Male,"Econometric modelling consists to represent an economy partially or globally in building a set of different kinds of equations (econometric, account etc.) from a set of time series data, in order to estimate or to forecast this economy at a specific time. According to a data point of view, the econometric modelling work could be viewed as a treatment of a set of input data (a sample of economic variable data) to estimate or to forecast another variable into an output data sample. The consequences of the use of these last data are not insignificantFootnote 1. So the checking of the micro (sales, demand, prices and so on) or macro (GNP, Price-index and so on) data is an important economic challenge that implies to know the quality of the initial sets of data. The quality of such a process depends on the quality of these data—but not only. Since the beginning of the econometric modelling, the size and the complexity of the modelsFootnote 2 have so increased that, the data error risk exists during the management of the models. The problem starts during the gathering of the data, as highlighted by Oskar Morgenstern, in the general context of a poll process, during the communication between the poll agent and the pooled individual. The first and/or the second could mistake or lie. The quality of the data depends on the feed-back—how many filled questionnaires are back?—and on the target sample choice. The questions are sometimes wrong entitled (problems of definition, classification). It could occur error during encoding with a tool. The questions are asked too early or too late. The aggregation of a lot of data could involve a loss of accuracy (Morgenstern 1960). The quality of the data obviously decreases if they are obtained by the computation of a bad quality input data set. Moreover, the more complex is the computation, the more reliable must be the input data (Morgenstern 1960, Chap. 6)—especially during the econometric computation. In an other hand, the data quality depends on institutional problems too—see Volle (1982) and Buda (2003) about the French institutional context of the data gathering organization. 
Clarifier les relations entre la statistique et les pouvoirs, c’est donner au statisticien les moyens de dépasser une représentation étroitement technique de son travail [...] ils (les statisticiens) sont mal armés pour comprendre ce qui se passe en statistique lorsque l’on sort des équations, des modèles et de l’informatique : et pourtant c’est essentiel.
Footnote 3 (Volle 1977). Another problem appears as soon as one wants to manage regional (or other sub-national levels) statistics. Because of their various sources, definitions and collection’s purposes, the regional account has unfortunately not the same good properties than the national account; especially the various regional accounts are seldom coherent each others (Courbis 1983a). The various regional data time series often need some reconciliation treatment before to be used. That is the reason why during their building, one has to keep in mind that any micro-data series are assumed to be used in a national account context too (Garnick 1980; Torene and Goettee 1980). Historically, the econometric modelling work has began during the 1930s when Tinbergen has developed his model (Tinbergen 1939-1940) while the electronic computer was first created in the 1940s but not immediately used (because too expansive and too slow)Footnote 4. 
Both ENIAC, the first (programmable) electronic computer and EDVAC, the first stored-program computer, were created in the 1940s, but the earliest use of any such computer by economists (the EDSAC, a sibling of the EDVAC) did not occur until the early 1950sRenfro (2009b). So the first modeler economists were actually able to use an operational tool, the electromechanical desk calculators, to compute regression parameters or solve econometric models—see Goldberger (2004) and Renfro (2009a), Chap. 2. Neither the Tinbergen model (1936) nor the Klein model I (1954) have been implemented into a computer program (Klein and Goldberger 1955, pp. 70–71)— Klein model was implemented into machine language (Adelman 2004, p. 11). However, Leontief (Leontief 1948) has presented a first use of computer to compute an economic model, in using the Mark I at Harvard (Renfro 2001). The development of the models has depended on information technology progressFootnote 5: the advent of the computer (1940s), the diod (1960s), the personal computer (begin of the 1980s), windows operating system (middle of the 1990s) and Internet (end of 1990s) (Renfro 2009a, Chap. 2), but some large scale models had been very early developed—Leontief (1948) then McCracken (1967a). In the same time, the implementation of the computational work on the computers has progressed. When there were used, the first computers were very rudimentary implemented ; the first scientific computer language (Fortran) has only appeared at 1954 (Backus et al. 1954). During the 1960s, the econometric computational work was implemented into a few softwares, mainly B34S (1964), MOSAIC and TSP (1965), AUTOBOX and TROLL (1966), and MODLER (1969). In France, until the middle of 70th, the models were implemented into Fortran – FIFI (Bussery et al. 1975) (INSEE), REGINA Courbis (1979), ANAIS (Courbis and Sok 1983) (GAMA) etc. Until the middle of the 80th, some institutions [(CEPII, GAMA, OFCE, REXECODE (IPECODE 1983), etc.] has loan the econometric softwares as TSP (Cummins and Hall 1986) and SIMSYS (McCracken and Sonnen 1973) (resp.) to estimate and solve (resp.) the models DMS (Equipe 1987), PROPAGE (Koepp 1982), METRIC (Artus et al. 1981) (INSEE), MOGLI (Courbis et al. 1980) (GAMA) or the OFCE-annual model (Fonteneau 1983) (OFCE). Then TROLL (Eisner 1972) was used to run the HERMES Model (Faubry et al. 1984) (Ecole Centrale) and AREMOS (Boldt and McDonald 1988;Renfro 2004, p.362) (WEFA) has replaced SIMSYS to run the models of INSEE. Then MODULECO (Nepomiastchy and Rechenmann 1983) (INRIA) was developed to run the model MIMOSA (Equipe 1990) (CEPII-OFCE). In the GAMA, two kinds of software were used, especially G-BUILD (Almon 1991) (INFORUM) to run mono-dimensional models [PROTEE (Courbis and Salmon 1986), EDMEE and TAIS]. The econometric software supply has significantly increased with the advent of the personal computer and then, with the advent of the windows operating system (Amman et al. 1996; Renfro 2001, 2003, 2004). From the 1960s to the 1980s, the econometric implementation was often encoded into Fortran instructions. In the middle of the 1980s, has appeared a new kind of language: Econometric Programming Language (Gauss, Ox, AREMOS, MODLER, XSIM among others). The use of econometric softwares has no longer implied any general programming skill (Renfro 2004). The economic and econometric software developers have replaced the “classical” instructions of the usual languages (Fortran, C, Basic, Pascal etc.) by some metalanguageFootnote 6 (GAMS, GAUSS, Stata, Matlab, Mathematica etc.) instructions (Varian 1996; Nielsen 2002; Herbert 2004). The developers have implemented econometric softwares into usual languages (C++, Java, Fortran, Pascal, Basic etc.)Footnote 7 while the econometric users have implemented their models into a metalanguage. More recently, reliability—one can be sure about the results  (McCullough 1998, 1999, 2004)—and readability—one can be able to identify the current algorithm during computation—have appeared as main criteria to choose an econometric programming language. Such characteristics are gathered by the R language (Venables and Ripley 2000), hence its success. Some economists even think that the authors of econometric papers should quote the name and the release of the econometric software they used (McCullough 2009; Renfro 2004) or use open source softwares (Baiocchi 2007; Yalta and Yalta 2010). More and more papers concern econometric softwares presentation, comparison etc—let’s quote (Brillet 1989; Mackie-Mason 1992; Rycroft 1993; Korosi et al. 1993; Silk 1996; Renfro 2004; Ooms 2008; Koenkera and Zeileis 2009) among a lot—including sometimes a reliability analysis according to the Wilkinson’s criteria (Wilkinson 1985, 1994). The data handling is became one of the econometric software quality criteria (Mackie-Mason 1992) too. Tinbergen proposed a separation of the two roles in the econometric modelling work, the economist (who provides model equations from the theories) and the statistician (who provides the validation of the model) (Morgan 1990, p. 109), while in the same time appeared a real macroeconomic data systematization, the national account initiated by the paper of Meade and Stone (Meade and Stone 1941), under the suggestion of Keynes (Vanoli 2002), p. 40). That the reason why most of the econometric modelers—even the pioneers [(Tinbergen 1939, pp. 210–238); (Klein 1950, pp. 135–162); (Klein and Goldberger 1955, pp. 115–133); (Klein and Young 1980, pp. 11–18)]—use data from a government agency or equivalent sources. These who also build some time series data make in the same time national account workFootnote 8. Consequently, it’s interesting to consider here the opinion of Keynes and Tinbergen—see also Tinbergen (1951)—about the potentially failure of the models: Keynes: If the results are not in accordance with theoretical preconceptions, then blame the method and the data but not the theory (Morgan 1990, p. 124). Reply from Tinbergen: if the theory was not confirmed by the results, then the inference was that the theory was wrong or insufficient. (Morgan 1990, p. 124). Since the 1970s, the role played by data in economics has significantly increased. That period has known a wide progress of the data and the modelling tool; a first large scale data bank software was previously developed during the middle of the 1960s (McCracken 1966, 1967b; Renfro 2009a). The suppliers of data progressively have organized the supply in the way of the standardization and the accessibility (Renfro 1980). The data distribution is became an industrial activity, especially with the advent of internet (Renfro 1997). The supply has became various but not necessary easily accessible: the data users have often to change the data format before to input them into his computation system. Such context and other considerationsFootnote 9 has increased the error risks during the data input process (Renfro 2006). In any case the problem of the data quality is not only an empirical one. Some models have been accepted of rejected without any checking of the input data (Wilcox 1998; Renfro 2006). A lot of economists and institutions highlight that an important element of scientific progress consists in the replication requirement. When a paper is published, one could deal again with the data of the author. It’s only possible if the data and the software release are provided by the author of the paper. So that, the econometric activities could be regarded as theoretical one too (Morgan 1990; Qin 1997). During the building of the data bank econometric model, the data error risks are often reaching their highest level. The econometric modelling work needs a storage system to store a set of time series data and a software to manage the data including various facilities—such system has appeared very early with the SIMSYS software (including DATABANK and MASSAGER programs) (McCracken 1966; McCracken and Sonnen 1973). The system has to allow the importation of new time series data and the exportation of some others totally or partially according to various periodicity. It has to allow to replace, update or erase any data, and to manage the revised data in including a documentation management of the time series data. The storage of the time series data can allow to display any kind of data according all of its dimensions (dynamic, individual etc.) in a table or inside a chart. Some basic treatments are necessary: arithmetical one (growth rate, logarithmic transformation, etc.), statistical one (average, etc.), chronological one (periodical change), specifical one (missing data one) (Brillet 1994). The estimation results—especially the coefficients—have to be safety stored to be correctly applied during the resolution of the whole model. The estimation of the equation has to be displayed with its statistics to assess the quality of the equation. The multidimensional econometric modelling involves some more problems. The sizes of the model increases the risk of failure during the storage of input and the output data (or statistics). The data handling have obviously to be minimized for this reason. The data bank can store time series data with different geographic or sectoral levels. But during the working sessions, one could have to use time series data at the same geographic or sectoral level. So a multidimensional data bank software has to allow aggregation of the data—it consists in building a new account from some data at a lower level, e.g. from a departmental level to a regional level. Moreover, at a regional (or other sub-national) level, there are many sources of time series data, so it is often necessary to reconciliate them to obtain a coherent account, especially if we need to use macroeconomic national time series data from governmental sourceFootnote 10. Moreover the effect of data revision is often more important with micro-data than macro one because they are lower than the second one. One can meet such a problem when we collect regional-sectoral data (\(x_{r,s}\)) from a regional source and when we have got national data (\(\tilde{X_{s}}\)) (the sectoral margin) from a national source so that we observe There is a special procedure which is able to reconciliate the microeconomic data with the macroeconomic margin: the RAS procedure (Lahr and de Mesnard, 2004). The data bank has to be checked before and after any econometric treatment. The data checking experience has came from telecommunication industries and management science. The telecommunication industries has studied the condition to minimize the error risk during the sending of a message from the sender to the receiver, based on a specific data encoding (Shannon 1948; Hamming 1950). The technique has been then extended to the software building (Huang 1975, 2009). On the other hand, the management science has systematized the data bank in creating a new concept, the database system (Codd 1970), which consists in developing a specific language of data queryFootnote 11. More recently, the manager scientists have widely systematized the database system into datawarehouse Inmon (1981); Kimball (1996), and lastly the software developers have introduced a concept of traceability in their work which can have some consequences to the data checking Gotel and Finkelstein (1994). We have integrated such experiences (Buda 2008c) in the work we present here. Our paper, is mainly based on the development of the econometric package SIMUL
Footnote 12
Buda (1999, 2001, 2010b, 2011, 2013). We indeed have developed an econometric software initially dedicated to the project REGILINK—a project of regional-national models linked by international trade flows (Courbis 1979, 1981). The first releases of the model (REGINA and then REGIS) have been implemented into some ad hoc Fortran programs. In the middle of the 1990s, the SIMUL project was finally decided. Although a few multi-sectoral softwares were available —mainly IDIOM (Peterson 1987) and DYME (Almon 1991), the previous experience of the SIMSYS use in the GAMA Courbis and Sok (1983) teaches to the Professor Courbis, that some adaptations would be necessary and also it was better for the GAMA Team to built its own software directly according to the REGILINK project needs. Such tool (multi-dimensional one) needs a specific attention to manage the data—see the MODLER experience (Renfro 1998)—so that we have think about some safety processes during the development of SIMUL. The paper is divided into two parts. We’ll firstly present the problem of the data checking in the context of the econometric workFootnote 13, in describing the econometric procedures and their implications in terms of data structure. Then we’ll present the solutions based on the encoding telecommunication’s and datawarehouse results, usually taken to manage the statistics checking problem. Finally we’ll present three encoding procedures we have conceived during the development of our econometric software package SIMUL.",
46.0,3.0,Computational Economics,30 July 2014,https://link.springer.com/article/10.1007/s10614-014-9456-5,Analysis of Carbon Emissions and Their Influence Factors Based on Data from Anhui of China,October 2015,Ma-Lin Song,Yuan-Xiang Zhou,,Unknown,,Unknown,Mix,,
46.0,3.0,Computational Economics,24 August 2014,https://link.springer.com/article/10.1007/s10614-014-9464-5,Measuring Environmental Performance Under Regional Heterogeneity in China: A Metafrontier Efficiency Analysis,October 2015,Yanni Yu,Yongrok Choi,,Unknown,Unknown,Unknown,Unknown,,
46.0,3.0,Computational Economics,28 August 2014,https://link.springer.com/article/10.1007/s10614-014-9467-2,On Modeling Environmental Production Characteristics: A Slacks-Based Measure for China’s Poyang Lake Ecological Economics Zone,October 2015,Ning Zhang,Fanbin Kong,Chih-Chun Kung,,Unknown,Unknown,Mix,,
46.0,3.0,Computational Economics,21 January 2015,https://link.springer.com/article/10.1007/s10614-015-9486-7,The Estimation of Environmental Kuznets Curve in China: Nonparametric Panel Approach,October 2015,Linna Chen,Shiyi Chen,,Unknown,Unknown,Unknown,Unknown,,
46.0,3.0,Computational Economics,10 February 2015,https://link.springer.com/article/10.1007/s10614-015-9487-6,Strategic Adjustment of China’s Power Generation Capacity Structure Under the Constraint of Carbon Emission,October 2015,Yuhong Wang,Xin Yao,Pengfei Yuan,Unknown,,Unknown,Mix,,
46.0,3.0,Computational Economics,22 January 2015,https://link.springer.com/article/10.1007/s10614-015-9488-5,"A Predictive Analysis of Clean Energy Consumption, Economic Growth and Environmental Regulation in China Using an Optimized Grey Dynamic Model",October 2015,Zheng-Xin Wang,,,,Unknown,Unknown,Mix,,
46.0,3.0,Computational Economics,23 April 2015,https://link.springer.com/article/10.1007/s10614-015-9498-3,Two-Stage Network Structures with Undesirable Intermediate Outputs Reused: A DEA Based Approach,October 2015,Jie Wu,Qingyuan Zhu,Liang Liang,,Unknown,,Mix,,
46.0,3.0,Computational Economics,07 July 2015,https://link.springer.com/article/10.1007/s10614-015-9499-2,Measuring Energy Congestion in Chinese Industrial Sectors: A Slacks-Based DEA Approach,October 2015,F. Wu,P. Zhou,D. Q. Zhou,Unknown,Unknown,Unknown,Unknown,,
46.0,4.0,Computational Economics,11 September 2014,https://link.springer.com/article/10.1007/s10614-014-9468-1,Constructing a CGE Database Using GEMPACK for an African Country,December 2015,E. L. Roos,P. D. Adams,J. H. van Heerden,Unknown,Unknown,Unknown,Unknown,,
46.0,4.0,Computational Economics,01 November 2014,https://link.springer.com/article/10.1007/s10614-014-9475-2,Spatial Interaction Model of Credit Risk Contagion in the CRT Market,December 2015,Tingqiang Chen,Xindan Li,Jining Wang,Unknown,Unknown,Unknown,Unknown,,
46.0,4.0,Computational Economics,05 September 2014,https://link.springer.com/article/10.1007/s10614-014-9458-3,Developing Interaction Shrinkage Parameters for the Liu Estimator — with an Application to the Electricity Retail Market,December 2015,Ghazi Shukur,Kristofer Månsson,Pär Sjölander,Male,Male,Male,Male,"Multicollinearity is defined by Frisch (1934) as the situation where the explanatory variables correlate with each other, with the consequence of misleading statistical inference. However, in practice, in small- and medium-sized samples, this variation implies a high risk of misleading coefficient estimates even if this problem disappears asymptotically. To a lesser or greater extent this phenomenon is an issue for every multiple regression model, but the problem is especially severe for interaction-term models. By definition, interaction-term models will automatically induce substantial multicollinearity problems.Footnote 1 Even though this problem generates misleading inference in interaction models, these types of model are the standard models to analyze, for example, event-study effects, structural breaks, seasonality, gender effects and, as in our case, asymmetries in the price-discovery process of the power market. The consequence of the multicollinearity in the multiple linear regression model is wider confidence intervals and an increase in the probability of conducting a Type-II error in any hypothesis testing in terms of the parameters. Moreover, as previously mentioned, in a finite sample the uncertainty of the estimated coefficient is also higher because of an increased coefficient variance which is due to multicollinearity. Therefore, a single coefficient estimate should be interpreted with considerable caution in the presence of the multicollinearity, which is induced by standard interaction-term models. By producing considerably fewer standard errors around the coefficients, we demonstrate that our new Liu estimator is a useful and a recommended remedy for this very common, but often disregarded, problem for econometric practitioners. A common remedy for the problem of multicollinearity is to accept some negligible bias of the estimation in order to decrease the variance of the estimated coefficients. One such remedial procedure is the Liu estimator, which was introduced by Liu (1993). This approach has been considered in many papers since, such as Akdeniz and Kaciranlar (1995), Kaciranlar (2003) and Alheety and Kibria (2009). The advantage of the Liu estimator, compared with the traditional ridge-regression method proposed by Hoerl and Kennard (1970a, b), is that the estimated coefficients are a linear function of the shrinkage parameter \(d\) [see Liu (1993)] instead of a non-linear function as in the case of ridge regression. This leads to a more stable shrinkage of the vector of estimated coefficients. The purpose of this paper is to evaluate some new methods of estimating the shrinkage parameter \(d\), and to demonstrate the gains of using these methods to evaluate the theory of asymmetric price transmission (APT) effects on the Swedish electricity market. The original methods that inspired our new approaches were developed by Hoerl and Kennard (1970a, b), Kibria (2003), Khalaf and Shukur (2005) and Muniz and Kibria (2009). These methods are evaluated by means of Monte Carlo simulations, where several factors such as the sample size and the distribution of the error term are varied. As performance criteria we use the standard evaluation approach of mean-squared errors (MSE). Thus, in order to demonstrate the usefulness of this approach, and to test a policy-relevant economic theory, we apply our new method to empirical data. In this example we analyze whether electricity retailers systematically apply positive APT rates. Using our new Liu method, we analyze whether there is a higher propensity for electricity retailers to swiftly increase their prices to consumers subsequent to an increase in their costs (Nord Pool Spot prices), than to decrease their prices to consumers subsequent to a corresponding decrease in the Nord Pool Spot prices.Footnote 2 In other markets this phenomenon is defined as positive APT effects, and this theory is explained and evaluated in depth in a review article by Meyer and Cramon-Taubadel (2004). In our analysis we find significant evidence of positive APT effects in the Swedish power market. The consequences of this phenomenon are systematic, unjust wealth transfers from consumers to retailers in this inefficient market. The very specification of interaction terms in regression models automatically induces severe multicollinearity problems. However, we demonstrate that our new Liu-estimator approach to a great extent can remedy these problems. Therefore, our results are considerably more reliable and statistically efficient compared with methods which do not take this problem into account.",8
46.0,4.0,Computational Economics,30 October 2014,https://link.springer.com/article/10.1007/s10614-014-9471-6,Wavelet Estimation of Gegenbauer Processes: Simulation and Empirical Application,December 2015,Heni Boubaker,,,Unknown,Unknown,Unknown,Unknown,,
46.0,4.0,Computational Economics,04 September 2014,https://link.springer.com/article/10.1007/s10614-014-9461-8,Tests of Financial Market Contagion: Evolutionary Cospectral Analysis Versus Wavelet Analysis,December 2015,Zied Ftiti,Aviral Tiwari,Khaled Guesmi,Male,Unknown,Male,Male,"The linkages and co-movements of financial markets have been an issue of great interest in a large body of finance literature for more than half a century. Two main reasons may explain this pronounced interest. First, with the expansion of international trade and commerce, financial globalization has been marked by a surge in capital flows across countries. Investors seek to construct international rather than national portfolios. In the meantime, modern financial theory, mainly the capital asset pricing model and arbitrage pricing theory, supports the profound impact of the correlation pattern of risk and return magnitudes and thus of asset allocation and diversification strategies (Sharpe 1964; Lintner 1965; Ross 1976). Investors are thus incited to study and comprehend the potential linkages and correlations between international financial markets in order to construct optimal portfolios (Forbes and Rigobon 2002; Bessler and Yang 2003; Kiviaho et al. 2012). The literature on portfolio selection presumes that an optimal portfolio enables investors to incur low risks and derive high returns (Markowitz 1952; Merton 1972). Second, the wave of financial and economic turmoil and the multiplication of political events have raised widespread concern about the co-movements of financial markets around the world. Since the 1929 Wall Street crash, the latter have been much more significant during turbulent periods and times of crises (King and Wadhwani 1990; Lee and Kim 1993; Bancel and Mittoo 2011; Zaki et al. 2011; Ranta 2013). The need for analyzing and understanding financial markets co-movements and interrelations arises from their potential implications for goods and services markets across countries and over time. This is a consequence of not only the advanced process of globalization and liberalization (Madaleno and Pinho 2012) but also of the high integration of financial and goods markets (Gallegati 2012). Numerous studies have offered several recommendations and recurring suggestions to enhance the modeling of international financial market co-movements. The ultimate objective is to better understand these interrelations and predict their implications for both investors and policy makers. Investors are concerned with their implications for asset allocation, portfolio composition, and investment strategies, while policy makers look for the best policy responses and hedging strategies during crises. Although one can think of a host of issues concerning international stock markets co-movements and linkages, there is no consensus regarding the definition and the measurement of cross-market interrelations (Gallegati 2012; Ranta 2013). Interdependence and contagion are the two main kinds of linkages among various markets: (Gallegati 2012). While interdependence refers to a simple correlation between markets movements, contagion is commonly defined as an increase in the correlation between them after crises (Forbes and Rigobon 2002). Pericoli and Sbracia (2003) present five representative definitions of contagion. According to them, contagion can be defined as a significant increase in either the probability of a crisis in one country or co-movements of prices and quantities across markets, conditional on a crisis occurring elsewhere. The three other classifications provided by these authors are related to the timing of contagion. Contagion can occur either when volatility spills over from the crisis country to the financial markets of other countries, or when the transmission channel differs, after a shock in one market, or even when co-movements cannot be explained by fundamentals. Furthermore, contagion can be either “fundamentals-based” or “pure” (Dornbusch et al. 2000; Kaminsky and Reinhart 2000). Fundamentals-based contagion, often labeled as spillover, refers to transmission mechanisms and shocks propagation among markets that result from real interdependence and global integration in both quiet and turbulent periods (Calvo and Reinhart 1996). Pure contagion stresses excessive co-movements rather than the normal ones that are expected to occur after controlling for fundamental factors (Eichengreen et al. 1996; Bae et al. 2003). Pure contagion is usually induced by investor behavior and bias, including overconfidence, the illusion of control, herding, and so on. Beyond such a classification, not only contagion but also interdependence can induce co-movements during crises. The study focuses on both the interdependence and pure contagion that occurred from 1990–2013 between, on the one hand, the broad-based market return indexes of either the U.S. or Europe, and, on the other hand, 14 OECD countries: Australia, Austria, Belgium, Denmark, France, Finland, Germany, Greece, Italy, Netherland, Norway, Portugal, Spain, and Sweden. Markets indexes are drawn from the Morgan Stanley Capital International indexes (MSCI) and consist of stocks that broadly represent stock composition in the different countries and regions. To provide further evidence, we suggest a new methodology that usefully discriminates between contagion and interdependence, based on frequency domain analysis. We propose two advantageous frequencies approaches, the ESA and the wavelet analysis. The ESA does not impose any restrictions or require pre-treatment of the data (as it is the case of the VAR model, for instance, which requires the series to be stationary or co-integration techniques that can be applied only to time series data integrated in the first order). Second, it does not have an “end-point problem”: no future information is used, implied, or required, as in band-pass or trend projection methods. In addition, the ESA gives a robust frequency representation of non-stationary process. Finally, frequency analysis provides worthwhile information about the time horizon of the interdependence between the two series: the analysis stresses whether the variables under investigation present short, medium, or long-term interdependence. This additional information allows the understanding of any contagion (high frequency) or interdependence (low frequency). Similar to the ESA, the wavelet analysis offers various benefits. First, this filtering method provides an interesting alternative to time series and frequency domain methods because it transforms the time series into different frequency components with a resolution matched to its scale. This is useful when dealing with non-stationary data that exhibit changing frequencies over time, as in the case of financial market data. Second, the wavelet analysis is suitable to macroeconomic and/or financial data in their time-scale components. Third, it provides an alternative representation of the variability and the association structure of certain stochastic processes on a scale-by-scale basis. The multi-resolution decomposition property of the wavelet transform can be used to identify separately contagion and interdependence by associating each to its corresponding frequency component. Our analysis offers many important results. We have proved that the long-term co-movement, related to the interdependence between stock markets indexes, increased during the periods of financial turmoil (the beginning of the 1990s and the subprime crisis in 2007). This interdependence differs among countries. We have also shown that some countries, such as Germany and the Netherlands, were less sensitive to permanent shocks than other countries, including-Greece, Portugal, and Australia. Our results include another important that concerns short-term co-movements. The short-term co-movement assesses the contagion character of various stock markets indexes. We have identified the contagious character of all studied markets with some national differences. Our empirical findings demonstrate that Greece, Spain, Portugal, and Italy were more sensitive to transitory shocks than the U.S. and Europe. This study has significant economic implications. Indeed, the distinction between excessive and normal co-movements is a key issue from a portfolio diversification perspective, especially during periods of high volatility. Investors can derive important information about countries that are less sensitive to permanent shocks and those that are less sensitive to transitory shocks. The study contributes to the research topic through at least three channels. First, most studies mainly focus on contagion, and pay little, if any, attention to interdependence, whereas this study sheds light on both sides of the interrelations in international markets. Second, the research suggests two advantageous frequency approaches, namely, the ESA and the wavelet analysis, in order to escape the limitations of the Fourier transformation and conventional time series in co-movement analysis. Third, the period of the study spans more than two decades, from 1990 to 2013. Such a long period includes numerous economic crises and financial crashes that allow us to draw relevant conclusions regarding co-movements of most dynamic international financial markets. The remainder of this paper is structured as follows. Section 2 briefly reviews the related literature. Section 3 provides data and describes the study’s empirical methodology. The results are summarized and discussed in Sect. 4. A conclusion follows and points to directions for future research.",31
46.0,4.0,Computational Economics,12 October 2014,https://link.springer.com/article/10.1007/s10614-014-9465-4,Minimality of State Space Solutions of DSGE Models and Existence Conditions for Their VAR Representation,December 2015,Massimo Franchi,Paolo Paruolo,,Male,Male,Unknown,Male,"Economic shocks of DSGE models cannot always be recovered from VARs. This situation has been discussed e.g. in Chari et al. (2005), Christiano et al. (2006), Kapetanios et al. (2007), Ravenna (2007), and it is related to the non-fundamentalness of economic models, see Hansen and Sargent (1980), Lippi and Reichlin (1993, 1994) for early treatments of the problem. Solutions of linear (or linearized) DSGE models can be expressed in linear state space form using several solution methods, such as Binder and Pesaran (1997), Uhlig (1999), Klein (2000), Sims (2001). In the following, we refer to the linear state space form as the ABCD form. When the number of shocks is equal to the number of observables and the matrix that loads the former into the latter is non-singular, the ABCD form is called square. We observe that the ABCD form of the solution dynamics need not be minimal, i.e. it may involve a non-minimal number of state variables. This is illustrated for example in Komunjer and Ng (2011), who show that the state space solutions in Christiano et al. (2005), An and Schorfheide (2007), Smets and Wouters (2007), García-Cicco et al. (2010) are all non-minimal. In this paper we investigate population conditions for fundamentalness of the ABCD form and for the existence of a finite VAR representation of the observables.Footnote 1 Fundamentalness is currently checked using the ‘poor man’s invertibility condition’ in Fernández-Villaverde et al. (2007); applications of this approach can be found in Leeper et al. (2013), Schmitt-Grohé (2010), Kurmann and Otrok (2011), Sims (2012) inter alia. The poor man’s invertibility condition consists in the stability of the associate state matrix, \(F\) say, i.e. the state matrix in the system in which the role of inputs and outputs is interchanged, see Bart et al. (2008). Conditions for the VAR to be of finite order are given in Ravenna (2007), where a ‘unimodularity condition’ is proposed as a check. This condition consists in the unimodularity of the matrix polynomial \(I-Fz\), \(z\in \mathbb {C}\),Footnote 2 or equivalently in the nilpotency of \(F\), see Franchi and Vidotto (2013). When the square ABCD form is minimal, we show that the poor man’s invertibility condition and the unimodularity condition are proper checks, i.e. they are necessary and sufficient conditions for fundamentalness. When the ABCD form is non-minimal, we find that the poor man’s invertibility condition and the unimodularity condition are not necessary. In other words \(F\) can be unstable while the square ABCD form is fundamental and \(I-Fz\) can be non-unimodular while the square ABCD form has a finite order VAR representation. Because in current economic practice systems are not always in minimal state space form, in this paper we present alternative orthogonality conditions that are necessary and sufficient and which hold in possibly non-minimal square systems. This provides a check that does not require to transform the system to minimal ABCD form. These orthogonality conditions involve the eigenvectors associated with eigenvalues of the associate state matrix \(F\), and correspond to the case where there exist states that are cancelled in the VAR representation of the observables. For fundamentalness, the orthogonality conditions must apply to all the unstable eigenvalues, while for the existence of a finite VAR representation, the orthogonality conditions must hold for every nonzero eigenvalue. When the orthogonality conditions hold, we find that the eigenvalues of the associate state matrix \(F\) are also eigenvalues of the state matrix, \(A\) say. This implies that when the eigenvalues of \(A\) and \(F\) are different, the orthogonality conditions do not hold. This provides a simplified strategy based solely on eigenvalues of \(A\) and \(F\). Finally we find that if the ABCD form is minimal, the orthogonality conditions coincide with the poor man’s invertibility condition and the unimodularity condition. This shows that the present orthogonality conditions are an extension of the conditions in the literature that apply to general (i.e. not necessarily minimal) linear state space forms. Therefore, invertibility of linearized DSGE models should be checked either using the present orthogonality conditions or by first transforming the state space form to a minimal representation and then by applying the poor man’s invertibility condition to the minimal state space form. Similarly, existence of a finite VAR representation should be checked either using the present orthogonality conditions or by first transforming the state space form to a minimal representation and then by applying the unimodularity condition to the minimal state space form, see Franchi (2013) for an illustration on the Smets and Wouters (2007) model. The results of the paper employ well-known concepts from systems theory, see e.g. Kailath (1980), Antsaklis and Michel (2007). The possibility to have cancellations of the type discussed in this paper in non-minimal state space systems was noticed by Rosenbrock (1970) inter alia, see his remark after Theorem 1.2, page 38 and Corollary 3, page 115. Here we give explicit algebraic condition on the system matrices that characterizes this situation. These tools are meant to be used by macro-economists, who deal with non-minimal state space representations of linearized DGSE models in practice and who are interested in determining if the observables in their model admit a VAR representation. The results in this paper are also related to cointegration.Footnote 3 The possibility to have unstable states within a non-minimal state space gives in fact one more way to incorporate cointegration in DGSE models, by allowing unit-root stochastic trends in the states that can be canceled in the stable VAR representation of observables. The rest of the paper is organized as follows. Section 2 introduces the square case. Section 3 shows that fundamentalness and the existence of a finite order VAR can be characterized in terms of an appropriate transfer function. Section 4 presents the orthogonality conditions. Section 5 specializes results to minimal square ABCD forms, showing that for minimal state spaces the orthogonality conditions coincide with the poor man’s invertibility condition and the unimodularity condition. Section 6 concludes. All proofs are collected in the Appendix.",13
46.0,4.0,Computational Economics,31 July 2014,https://link.springer.com/article/10.1007/s10614-014-9453-8,Word-of-Mouth Communication and Demand for Products with Different Quality Levels,December 2015,Bharat Bhole,Bríd G. Hanna,,,Female,Unknown,Mix,,
46.0,4.0,Computational Economics,30 October 2014,https://link.springer.com/article/10.1007/s10614-014-9470-7,The Krusell–Smith Algorithm: Are Self-Fulfilling Equilibria Likely?,December 2015,Marco Cozzi,,,Male,Unknown,Unknown,Male,"This paper investigates whether the popular Krusell and Smith (KS) algorithm, used to solve heterogeneous-agent economies with aggregate uncertainty and incomplete markets, is likely to be subject to multiple self-fulfilling equilibria (SFE). This possibility arises because the equilibrium Aggregate Law of Motion (ALM) is unknown and needs to be computed through a guess-and-verify iterative procedure. Crucially, the agents’ optimal decision rules have to be calculated at each step of this fixed-point problem, but they in turn depend on the ALM being tried. In principle, this process can lead to a complementarity between the guess related to the agents’ perception of the evolution of future prices and their implied choices. This method was first proposed by Krusell and Smith (1998), and it has been successfully applied to a wide variety of problems. Notable examples include the pricing and allocation of risky and safe assets (Krusell and Smith (1997), Pijoan-Mas (2007) and Storesletten et al. (2007)), the magnitude of welfare costs due to business cycles (Castaneda et al. (1998), Mukoyama and Sahin (2006), and Krusell et al. (2009)), fluctuations in frictional labor markets (Gomes et al. (2001) and Nakajima (2012)), the determinants of fiscal policy (Heathcote (2005)), and the analysis of rising wage inequality in a political economy framework (Corbae et al. (2009)). Where does the possibility of multiple SFE stem from? In the simplest environment, households only choice concerns their savings. If postulating an ALM for capital that is above (below) the equilibrium one leads to more (less) resources being saved by the households in the aggregate, we would be in a situation displaying complementarity between the guessed aggregate dynamics of capital and the resulting saving behavior. This instance could lead to multiple SFE, as acknowledged by Krusell and Smith (2006), who argued for the absence of multiple SFE by analyzing a simple two-period model. Since the uniqueness of the equilibrium is impossible to prove analytically, this paper undertakes a systematic study on the subject with a numerically intensive procedure, considering the full-blown version of their model, with both infinitely-lived agents and preference heterogeneity. My investigation is complementary to the analysis performed in Young (2005), who assesses the robustness of the KS algorithm along several dimensions. In particular, he argues for the absence of SFE by working with a version of the model with heterogeneous beliefs. Unlike him, I rely on a Monte Carlo analysis of the KS economy. Through appropriately designed perturbation experiments, I do not find evidence supporting the existence of SFE such that the parameters representing the ALM converge to different values, depending on the initial guess. Most replications tend to cluster around two different values of the ALM, and converge to ALM parameters that differ from the equilibrium one, but the discrepancy is always quantitatively negligible. Furthermore, increasing the accuracy of the numerical procedure until hitting a feasibility boundary due to the computational time reduces this difference by an order of magnitude.Footnote 1 Although it is difficult to disentangle the gap from the numerical error induced by the discretization of the state space, the sampling variability arising from the simulations and the convergence criteria, this finding makes the numerical error the most likely culprit behind the differences between the postulated equilibrium ALM and the sequence of converged ALM’s found in the replications. In terms of substance, even if one is willing to consider these alternative ALM’s as different equilibria, the implied differences are always quantitatively minimal. The findings show that, although the economy features a wealth distribution with a fat right tail, the share of agents increasing their savings in response to ALM’s that overpredict the future aggregate capital is well below \(0.5\,\%\). In particular, agents need to have accumulated no less than 100 and up to 300 times the average income for the negative income effect to start prevailing, leading them to increase their savings. It follows that, in the aggregate, this change is dominated by the reduction in savings of the vast majority of agents, and multiple SFE are unlikely to arise. The rest of the paper is organized as follows. Section 2 briefly presents the model and the calibration. Section 3 discusses the role of the ALM in the KS model. Section 4 outlines the perturbation experiments. Section 5 describes the main results, while Sect. 6 concludes. Three Appendices report the complete calibration, discuss in more detail the numerical methods used, and present some additional results together with a set of robustness exercises.",2
46.0,4.0,Computational Economics,02 September 2014,https://link.springer.com/article/10.1007/s10614-014-9466-3,A Dynamic Interface for Trade Pattern Formation in Multi-regional Multi-sectoral Input-output Modeling,December 2015,George E. Halkos,Kyriaki D. Tsilika,,Male,Female,Unknown,Mix,,
47.0,1.0,Computational Economics,15 February 2015,https://link.springer.com/article/10.1007/s10614-015-9495-6,Network Approaches to Interbank Markets: Foreword,January 2016,Simone Alfarano,Daniel Fricke,Matthias Raddant,Female,Male,Male,Mix,,
47.0,1.0,Computational Economics,03 June 2014,https://link.springer.com/article/10.1007/s10614-014-9443-x,Interbank Exposure Networks,January 2016,Sam Langfield,Kimmo Soramäki,,,Male,Unknown,Mix,,
47.0,1.0,Computational Economics,08 November 2014,https://link.springer.com/article/10.1007/s10614-014-9477-0,Centrality Measurement of the Mexican Large Value Payments System from the Perspective of Multiplex Networks,January 2016,Bernardo Bravo-Benitez,Biliana Alexandrova-Kabadjova,Serafin Martinez-Jaramillo,Male,Unknown,Male,Male,"In the last few years, the concepts of network theory and financial systems have become highly related. This is because in the last decade network theory has become a widely used method to model social relationships, whereas financial systems, in particular after the crisis are among the most studied complex social structures. It is remarkable how in a very short period of time network theory has attracted financial authorities’ attention as the growing literature has allowed us to gain better understanding of the structure of the financial entities’ relationships. Characterized by its multidimensional nature, the complexity in the financial world arises not only by the diversity of business relations among participants, but also is due to the difficulty to identify the relevant institutions and the implications that the failure of such institutions would have on the system. Certainly, in many jurisdictions (e.g. Canada, Mexico, Russia and Brazil only to mention some) knowing who is the most important player, could be relatively easy to distinguish. Nevertheless the task to find out who are among the most important 10 or 15 banks (beyond size) could turn out to be significantly more complex. This is the reason why the systemic view is important as single players cannot see beyond their immediate relationships, it is then left to the supervisors and regulators to study and analyze the system as a whole. In this sense, how we measure the relevance of the participants becomes an important aspect of the analysis. If financial stability is our main concern, previous studies have shown that banks’ ranking according to assets size is not perfectly correlated with the ranking of institutions considering the contagion impact they have in the interbank market: Martinez-Jaramillo et al. (2014); also Krause and Giansante (2012) have shown the importance of network properties compared to aggregated balance sheet variables to assess the spread of contagion in a bank crisis. In the broader context of payment systems, we could expect that the size of the financial institutions is not perfectly correlated with the participants’ volume and value of payments they send. It is important to stress that in the present study we want to identify relevant players beyond the obvious size and volume measures, for which there is no need for network modeling. In that way, finding adequate metrics to rank financial institutions is a crucial aspect in the assessment of the whole structure. For that reason, modeling the bilateral relationships in the financial systems has tied us down to find new ways to interpret the network’s topology measures. However, it is important to use the network paradigm with economic sense as it has been proposed in Battiston et al. (2012), a study in which a “meaningful” centrality measure is proposed: the DebtRank. Nevertheless, the DebRank measure is tied to the concept of exposures and cascade defaults; a more appropriate centrality measure for payment systems is needed, a centrality measure which can be related with one of the main drivers in payment systems: intra-day liquidity management. In this line, the centrality measures applied to the network’s topology allow us to have a better understanding of what role different participants perform in the complex environment of evolving relations. Furthermore, as we said, the financial structure has multidimensional nature, i.e. exist different links between the same institutions that represent different business activities. In addition this structure is subject to changes through out time. Thus in such complex landscape, having the ability to rank institutions under specific period of time and business activity’s framework according to the degree of their interactions, as well as the interactions of their counterparties with others, enable us to evaluate the relevance of each institution at a systemic level in a particular time period. Today, modeling relationships among financial institutions through a network is widely adopted and some studies have applied specific kind of network structures according to the different type of financial participants’ interactions, e.g. the interbank exposures and the payment systems. Nevertheless, it is difficult to incorporate into the determination of its relevance the multiple business activities a financial institution has. For this reason, it has been recently proposed to study the financial system as a multilayer system of networks, in which each layer is a network and it is built on a specific business relationship among financial institutions. For example, the interbank exposures network, the payment system network, the repos network, etc. From the individual perspective, the strategic decisions, which shape the network structure are taken sometimes simultaneously and even from the same decision body. This is the reason why we argue that instead of analyzing each case separately the multilayer system of networks is the appropriate way to represent the business activities we are referring to. This kind of close examination of the financial system, will eventually allow us to elaborate measures to distinguish between different operational conditions observed in the system. Nowadays among the main challenges being faced by the authorities is to be able to clearly discriminate between normal and stressed operational conditions, in particular in the context of payment systems. Furthermore, they also need to learn how to identify fragile elements in the structure which potentially could turn out to be risky; this task is of greater relevance. We define the degree of “fragility” as the extent to which the entities of the network are exposed to external shocks, i.e. how easily unfavorable changes in the environment could create stressed conditions in the system. Regarding studies that focus on the impact in the interbank money markets under stressful conditions, some examples are: the paper by Afonso et al. (2011) in which the authors try to identify unsecured interbank loans in the transactions sent between participants through the Fedwire payment system, to identify the importance of liquidity hoarding and counterparty risk in events around Lehmann’s failure; in order to do so, the authors extract such information from the payment systems data via variations of an algorithm proposed by Furfine (1999). Also, the paper by Abbassi et al. (2013) presents a similar study for the European interbank money market obtained from the TARGET2 data, also using a Furfine-type algorithm to get the information, and using network based measures to describe and find evidence of a market freeze. On the other hand, the paper by Arciero et al. (2013) provides a refinement and a methodology for the validation of data provided by Furfine algorithms. Nevertheless, such studies focus on the impact in the interbank money market, instead of focusing on the consequences of shocks in the payment systems per se. On this regard, payment systems have been successfully modeled under the network paradigm by many central banks, some of the most relevant studies are: Soramäki et al. (2006), Bech and Atalay (2008), Becher et al. (2008), Bech et al. (2010), Rordam and Bech (2008), Pröpper et al. (2008), Wetherilt et al. (2010); also, the influence of liquidity costs in the emergence of tiering relationships among participants in the UK Large Value Payment System have been studied: Adams et al. (2010). In the present paper, we built on the existing literature and go further as we are the first to have a closer look at the different layers forming the payment system landscape. Currently, SPEI has 95 direct participants classified in general in two subsets—a. credit institutions, which in turn could be divided in multi-purpose commercial banks and public development banks; and b. non bank financial institutions, which also could be split into brokerages and other non bank financial institutions. In the present study we are aimed to analyze the bilateral relationships among multi-purpose commercial banks, resulting from the different type of payments institutions sent to each other through the Mexican Large Value Payment System, SPEI, classified as: participant to participant, participant to third party and third party to third party payments. We defined a layer for each payment type mentioned above, such that the overall structure of the network is characterized by three different network layers and can be referred as a multi-layer (or multiplex) network. It is important to note that the evolving network topology observed at the different layers depends on the relationships among those layers, more specifically on the cost and on the interdependency on the level of liquidity required for the different type of payments. Given that the interdependency on the level of liquidity is determined by the participants’ behavior, in this study we analyze the topology of the multiplex network, whereas the behavior related dependency is aimed for future research. In this sense, there is a large literature in statistical physics, however, that also uses the terminology “multi-layer networks” (Buldyrev et al. 2010; Li et al. 2012), but those studies incorporate more behavior related features in their analysis, so their conclusions present a higher degree of interdependency among the network layers. The network evolution is analyzed through a time window between January 2005 and December 2012 with data obtained from the SPEI. Our proposal is to describe stylized facts observed in the three types of the payment system layers and calculate centrality measures in order to have a better understanding of the structure of interbank relationships observed in SPEI. In addition to the centrality measures, in our study we applied topological measures such as degree, strength, clustering coefficient and flow. As in the case of the DebtRank for exposures networks, in this paper a “meaningful” centrality measure proposed for payment system networks, the Sink Rank (Soramäki and Cook 2013), will be contrasted with other centrality measures, already studied in Martinez-Jaramillo et al. (2014), and its added value is examined. The rest of the paper is organized as follows—in the next section we briefly describe the modelling framework, in Sect. 3 we present the stylized facts describing the SPEI’s network, whereas in Sect. 4 we explain the centrality measures used in the context of payment system. In the last section we conclude with our main findings and suggestions for further research.",10
47.0,1.0,Computational Economics,05 November 2014,https://link.springer.com/article/10.1007/s10614-014-9478-z,Cascades in Real Interbank Markets,January 2016,Fariba Karimi,Matthias Raddant,,Female,Male,Unknown,Mix,,
47.0,1.0,Computational Economics,14 February 2015,https://link.springer.com/article/10.1007/s10614-015-9493-8,Bank Capital Shock Propagation via Syndicated Interconnectedness,January 2016,Makoto Nirei,Vladyslav Sushko,Julián Caballero,,Male,Male,Mix,,
47.0,2.0,Computational Economics,01 November 2014,https://link.springer.com/article/10.1007/s10614-014-9474-3,Convergence of European Business Cycles: A Complex Networks Approach,February 2016,Theophilos Papadimitriou,Periklis Gogas,Georgios Antonios Sarantitis,Unknown,Male,Male,Male,"On February 7, 1992, the MaastrichtFootnote 1 treaty was signed by the 12 leaders of the members of the European Community. This established the European Union and set the foundations for the introduction of a common currency, the euro. The political and monetary integration of Europe was partly motivated by the Optimum Currency Area (OCA) theory of Mundell (1961). In this work, Mundell described an OCA as a region where the advantages of the circulation of a single currency among two or more nations are greater than the disadvantages of abolishing the sovereign currencies. Some of the advantages from the adoption of a single currency include: (a) elimination of the exchange rate risk, (b) reduced transaction costs, (c) increased price transparency and comparability since all commodities are priced in a single currency and (d) efficient allocation of labor and capital within the monetary union. Nonetheless, the adoption of a single currency does not come without associated costs and tradeoffs (Frankel 1999; Kenen 2000). One of the main disadvantages of abdicating the national currency is that monetary policy can no longer be used at the country level: the mechanism that allows national economies to absorb macroeconomic shocks by adjusting the money supply seizes to exist. The other significant cost is often referred to as the “one size does not fit all” problem: for a country that is in the contractionary phase of the economic cycle a loose monetary policy is preferable to a tight one in order to stimulate its economy. The opposite is true for an economy that operates within an inflationary gap. Thus, the synchronization of the participating countries’ business cycles is important for the implementation of an overall efficient monetary policy. In the opposite case, the common monetary policy is inefficient and may even be destabilizing. Consequently, business cycle synchronization should be considered as a prerequisite for a successful and efficient monetary union. The Eurozone is the first and only international example of the creation of such a common currency area. For this reason, there is a wide (and ever growing) literature that deals with the issue of business cycle convergence in Europe. However, the empirical results are not conclusive and thus a consensus has not been reached. Some representative studies include Artis and Zhang (1997) that examine the evolution of correlations between the cyclical component of 15 countries before and after the launch of the exchange rate mechanism (ERM) in Europe. The authors provide evidence in favor of business cycle convergence of ERM countries with Germany and decoupling from the U.S. business cycle for the period following the ERM. Massmann and Mitchell (2004) report an overall convergence between a set of 12 European countries during the period 1960–2001, despite some divergence patterns in the beginning of the 1990s that were mainly induced by the unification of Germany in 1989. Altavilla (2004) applies a Markov switching model on the EMU members and concludes that the establishment of a common currency in Europe has led to increased business cycle synchronization between the EMU economies. Montoya and de Haan (2008) use data on an alternative economic aggregation level in Europe, namely, the 53 NUTS1Footnote 2 areas and conclude that despite small signs of divergence during the mid-1980s and early 1990s, the areas under consideration present increased convergence at the full period examined i.e. 1975–2005. Cancelo (2012) applies a rolling window correlation coefficient for the time period 2004-2010 to uncover the evolution of convergence within a selected set of 14 European plus 3 external countries after the 2007 crisis. He concludes that during the crisis the Eurozone countries have in general increased the alignment of their business cycles with the non-Eurozone countries while Greece, Ireland, Spain and Portugal present more idiosyncratic cycles in the aftermath of the crisis. Gogas (2013) using three alternative methodologies examines the business cycle co-movement of 12 European countries before and after the establishment of a common currency in 1999 and provides evidence in favor of business cycle convergence after the introduction of the euro. Inklaar and de Haan (2001) build on the work of Artis and Zhang (1997) using the same dataset and methodology. However, instead of considering two time periods (pre- and post-ERM), they divide the original sample in four sub-periods and apply the correlation coefficient. Their results contradict those of Artis and Zhang (1997) since the coefficient is found to fluctuate throughout the four periods under consideration. Silva (2009) examines the status of convergence between 26 European economies before and after the emergence of the EMU and argues that no uniform outcome can be derived regarding business cycle synchronization within Europe as some economies tend to move closer through time while others seem to diverge. In this paper we work within a Graph Theory context, to empirically examine the evolution of business cycle synchronization of 22 European countries throughout a time period of 26 years (namely 1986–2011), using as a mid-point the year of the introduction of the euro in 1999. We construct the complex network that is associated to the GDP growth rate similarity of the selected economies and then analyze its topology with the use of standard network metrics and the threshold-minimum dominating set (T-MDS). We perform our empirical analysis within a rolling window framework which allows us to obtain a dynamic view of the evolution of the GDP growth rates network of the 22 European countries and provide empirical evidence on whether their business cycles seem to converge after the introduction of the common currency. The foundations of Graph Theory were first established by Euler (1741) when he tried to solve the famous “seven bridges of Konigsberg” problem in mathematics. He was trying to find a unique path that drives through the seven bridges that connected the individual parts of the town of Konigsberg. Since this “introductory” study, Graph Theory was popularized and applied in diverse scientific fields including path-rooting problems (Plotkin 1995), metabolic-biological networks (Weng et al. 1999; Schuster et al. 2000), social network analysis (Milgram 1967; Freeman 1979), technological networks (Watts and Strogatz 1998; Amaral et al. 2000), etc. Lately, Graph Theory has been integrated in the analysis of complex economic systems (Garlaschelli et al. 2007; Savva et al. 2010; Schiavo et al. 2010) and more specifically financial networks (Nagurney and Siokos 1997; Bonanno et al. 2004; Boginski et al. 2006; Tumminello et al. 2010; Bowden 2012), stock market networks (Vandewalle et al. 2001; Tse et al. 2010), the banking sector (Minoiu and Reyes 2013; Papadimitriou et al. 2013), etc. The first contribution of our work is that it attempts to implement a new methodological context for the analysis of business cycle synchronization that departs from classical econometric models and utilizes network analysis tools to investigate possible convergence patterns. In the relevant literature we find only two more studies that use network analysis for the examination of business cycle convergence; namely, Gomez et al. (2012) and Caraiani (2013). However, Caraiani (2013) presents a static image based on the correlations between GDP growth rates for the full period under consideration. He does not attempt to show the evolution of the business cycle through time as we do in this study, so that no inference on whether the cycles converge or diverge after the monetary union can be drawn. Gomez et al. (2012) present a dynamic analysis based on the application of a rolling window correlation coefficient, nonetheless, they use the minimum spanning tree (MST) methodology that possesses inherent weaknesses when applied to economics networks: the corresponding algorithmic calculation imposes unnecessary restrictions in the optimization procedure that may result in sub-optimal solutions. The second innovation is the introduction of the threshold-minimum dominating set (T-MDS) methodology. The T-MDS is an essential improvement of the MDS optimization tool (MDS has so far been successfully applied in wireless computer networks; see Cheng et al. 2003 and Wu et al. 2006). We enhance the MDS methodology by introducing a thresholding step to remove all the uninformative edges of the network. The resulting network and the identified T-MDS nodes are used to describe the evolution of business cycle convergence patterns in Europe. The rest of this study is organized in the following way: in Sect. 2 we present the selected data set. In Sect. 3 we describe the methodological context. In Sect. 4 we deliver our empirical analysis while in Sect. 5 we briefly summarize the paper and conclude.",15
47.0,2.0,Computational Economics,31 October 2014,https://link.springer.com/article/10.1007/s10614-014-9472-5,Entering H\(^{\infty }\)-Optimal Control Robustness into a Macroeconomic LQ-Tracking Model,February 2016,David Hudgins,Joon Na,,Male,,Unknown,Mix,,
47.0,2.0,Computational Economics,04 November 2014,https://link.springer.com/article/10.1007/s10614-014-9469-0,On Modeling Economic Default Time: A Reduced-Form Model Approach,February 2016,Jia-Wen Gu,Bo Jiang,Harry Zheng,,Male,Male,Mix,,
47.0,2.0,Computational Economics,15 November 2014,https://link.springer.com/article/10.1007/s10614-014-9476-1,Balancing Social Accounting Matrices with Artificial Polymorphus Ants,February 2016,Rolando Gonzales Martínez,,,Male,Unknown,Unknown,Male,"A social accounting matrix (SAM) records the transactions between the sectors of an economy. During its construction a SAM is not a «balanced matrix», in the sense that the row-sums and the column-sums of the matrix are not equal. In this paper, an ant colony (henceforth, ACO) algorithm is proposed for balancing a SAM. An ACO algorithm is a metaheuristic optimization based on the swarm behavior of ants, which choose and optimal path between their colony and the source of food by leaving a trace of pheromones. In the ACO algorithm of this paper, the «distance» between the unbalanced and the balanced matrix is the Euclidean norm between the row-sums and the column-sums of a matrix. Artificial ants randomly explore modifications of the elements of the matrix. An improvement in balancing the matrix—i.e. a reduction of the «distance» between the unbalanced and the balanced matrix—increases the amount of pheromones, thus increasing the probability of choosing the path towards the equilibrium of the matrix. Polymorphism—variations in size and morphology—of the artificial ants facilitates the allocation and partitioning of the optimization task. Section 2 briefly explains the biological inspiration behind ACO algorithms. Section 3 describes the artificial ACO algorithm. Section 4 contains a simulation exercise, a comparison with RAS and an empirical application to a real SAM. Section 5 concludes. MatLab codes to replicate the results can be found at the end of the study.",
47.0,2.0,Computational Economics,15 November 2014,https://link.springer.com/article/10.1007/s10614-014-9480-5,The Diablo 3 Economy: An Agent Based Approach,February 2016,Makram El-Shagi,Gregor von Schweinitz,,Male,Male,Unknown,Male,"Over the past decade virtual gaming worlds—so called massive multiplayer online games (MMO)—gained immense popularity. The more successful games such as World of Warcraft and Diablo 3 attracted more than 10 million players. However, what makes those games so interesting from the perspective of an economist, is not (only) the commercial success, but that these game environments host complete virtual economies. These economies face problems very similar to those encountered in the real world such as inflation, distributional issues etc.Footnote 1 The similarity between specific aspects of the real world and the virtual realities of MMOs generated a new branch of literature, that essentially exploits MMOs as natural experiments (Szell and Thurner 2010). Several MMOs were sufficiently successful to generate quite vivid real-world markets (mostly through ebay) for the in-game commodities and—more importantly—in-game money.Footnote 2
 Economic aspects became so important in those games that updates (so called patches) often primarily tackle economic issues, such as inflation. Essentially these patches can be interpreted as monetary policy.Footnote 3
 The similarity of policy relevant economic issues in the real world and MMO economies inspired us to develop a multi-agent model, reproducing economic features of one of the most successful current MMOs, Diablo 3. In economic terms, we analyze an economy with heterogeneous commodities that are produced and traded by heterogeneously endowed agents. Heterogeneous agent models have long been used to model price movements on a variety of markets including commodity markets (see e.g. Kirman and Vriend 2001), financial markets (Palmer et al. 1994; de Long et al. 1990) and monetary policy (Raberto et al. 2008). What distinguishes our model from most agent based models is first that we do not consider an economy fluctuating around a constant equilibrium but a rapidly growing economy (with stochastic growth). Second, our model considers commodities where the consumer’s choice is not about setting the quantity of the commodity he or she consumes, but consumers can choose between different quality levels for similar products. Many existing commodities such as automobiles share this feature. Rather than buying several cars, the typical consumer who wishes to spend more for transportation would go for a better car. The same holds true (to a certain extent) for other commodities, such as real estate and food, that represent a substantial part of spending. However, maximization of commodity quality is rarely included in macroeconomic models. In this paper we focus on reproducing and analyzing some stylized facts of the Diablo 3 economy: First, high quality commodities exhibit strongly increasing prices (that are often perceived as an expression of a general inflation).Footnote 4 Second, goods of heterogeneous quality show a highly different price development, something we call heterogeneous price development. Third, although the average quality of newly found items remains constant, the average quality of traded items increases over time. Fourth, the price for each given quality level of commodities follows a hump-shaped curve, where the price decline starts once better items become widely available through trading. Fifth, we find that heterogeneous price development as observed in Diablo 3 causes a highly unequal wealth distribution with very low economic mobility. And finally, we find that loose monetary policy can cause a breakdown of the quantity relation. This paper is meant as a stepping stone to further research where our evidence is used to deepen our understanding of the real world counterparts of economic problems we discuss. The advantage of our model is that it combines the simplicity that is inherent to model economies with an observable but similarly simple counterpart. By matching the dynamics of the game economy—where real agents interact—we can thus easily verify that our behavioral assumptions are good approximations to reality. The remainder of the paper is structured as follows. Section 2 motivates the relevance of our model. Section 3 explains our model economy. Section 4 presents our results on price developments, as well as the wealth and income distribution. It also contains results from a monetary policy experiment and robustness checks for all important elements of the model. Section 5 concludes.",2
47.0,2.0,Computational Economics,13 January 2015,https://link.springer.com/article/10.1007/s10614-015-9482-y,Financial Time Series Modeling and Prediction Using Postfix-GP,February 2016,Vipul K. Dabhi,Sanjay Chaudhary,,Male,Male,Unknown,Male,"There are two challenging problems in financial time series prediction: (i) stock price prediction and (ii) stock trend prediction. Both problems are difficult to solve due to inherent nonlinear and non-stationary characteristics of financial time series. Moreover, there are many external factors like political events, economic conditions, international financial markets, and human operations which can affect the data values of the series. Thus, it is difficult to design a system that can predict the exact future values of these series with high accuracy. However, many investors are more interested in the prediction of stock trend than the exact stock price as the decision to invest is initially influenced by the trend and then by the price. Investors can take trading strategies based on the trend persisting in the stock price or stock market (Vasanthi et al. 2011; Hong Tan 1995). Moreover, investors can develop profitable trading strategies based on prediction of stock trend (Trading based on Trend Prediction) (Vasanthi et al. 2011). Therefore, prediction of stock price and stock market trends has become very important in financial time series prediction. In this article, we have applied combination of wavelet analysis and postfix genetic programming for the prediction of stock price and stock market trends. This article has been motivated by the challenge to accurately predict the short term trend of stock price and stock market index using historical time series. The underlying methodology is based on the following steps: (i) applying discrete wavelet transform for extracting the trend of the given financial time series, (ii) phase space reconstruction of the extracted time series using time-delay embedding approach, (iii) developing a model for the trend time series using Postfix genetic programming. The developed model is then used for one-step ahead and multi-step ahead predictions. Different approaches used for financial time series modeling and prediction can be classified into two categories: (i) conventional approaches and (ii) heuristic approaches. In the past, conventional approaches like Box–Jenkins models, auto regressive moving average (ARMA) and auto regressive integrated moving average (ARIMA) have been used for developing a model for the given financial time series (Chatfield 2003). The developed model is then used for predicting the future values of the series. However, these approaches were developed based on the assumption of stationarity of the given time series and the independence of the residuals. Moreover, these approaches were able to capture only linear behavior of data and lack the ability to identify non-linear patterns and irregularity in the time series. In recent years, heuristic approaches like artificial neural network (ANN), support vector machine (SVM) (Vapnik 2000) and genetic programming (GP) (Koza 1992) have been used by practitioners for modeling and prediction of non-linear and irregular time series. Neural networks (NN) has been used for modeling and prediction of financial time series (Zhang and Hu 1998). Past stock values and technical indicators are used as inputs to NN and the target values as the output. Hansen and Nelson (1997) used NN for tax revenue forecasting. They concluded, from experimental results, that ANN outperforms conventional statistical forecasting approaches. In recent years, practitioners started to use SVM for financial time series prediction (Kim 2003). Kim (2003) found that SVM models gave better prediction accuracy on out-of-sample dataset compared to NN models. The higher accuracy of SVM models was due to the fact that SVM applies the structural risk minimization principle which is superior (because gives higher generalization ability) compared to empirical risk minimization principle, exploited by NN. Moreover, SVM guarantees to reach the global optimum whereas NN may trap into local optimum. However, both ANN and SVM produce black box models, which are difficult to interpret and implement. Moreover, the main problem with ANN and SVM approaches is the determination of values for optimal structure (number of layers, number of neurons and activation functions, kernel functions) and parameters (learning rate, hyperparameters), which requires an expertise (Yeh et al. 2011). The symbolic regression technique can be used for developing a model for the given time series (determining a mathematical model in a symbolic form that approximates the given time series data). The advantage of the symbolic regression technique over traditional regression techniques is that it searches simultaneously for both the structure and the appropriate numeric coefficients. Symbolic regression can be performed by means of GP (Koza 1992). GP approach is preferred over other approaches for symbolic regression because the approach produces an explicit mathematical expression as a solution. The objective of the present work is to explore applicability of a postfix notation based GP system, Postfix-GP, for trend modeling and prediction of financial time series. Moreover, the article also presents a comparison of two GP techniques (Postfix-GP and ECJ-GP) for prediction of financial time series. The Postfix-GP uses linear individual representation and stack based evaluation. This helps Postfix-GP to minimize both the memory requirement and evaluation time compared to conventional tree based representation. The Postfix-GP is applied to evolve models for two stock price series (Intel and Microsoft) and two stock market indexes series (NASDAQ Composite and S&P CNX Nifty) problems. The evolved Postfix-GP models are then used for out-of-sample predictions for these series. The performance of the evolved Postfix-GP models on all the time series problems are measured using mean absolute error (MAE), mean absolute percentage error (MAPE), mean squared error (MSE), normalized mean squared error (NMSE) and correlation coefficient (CC). The performance of the evolved Postfix-GP solutions is compared with those obtained using ECJ (Luke et al. 2007), a Java based framework for evolutionary computation, for all time series problems. The rest of the paper is organized as follows: The next section presents the related work. Section 3 presents the design of the Postfix-GP based financial time series prediction system. Section 4 presents experimental settings. The results for stock price series are presented in Sect. 5. Section 6 presents results for stock indexes series. It also compares predictive performance of solutions obtained using Postfix-GP and ECJ. Conclusions are presented in Sect. 7.",10
47.0,2.0,Computational Economics,13 January 2015,https://link.springer.com/article/10.1007/s10614-015-9483-x,The Stability Analysis of Predictor–Corrector Method in Solving American Option Pricing Model,February 2016,R. Kalantari,S. Shahmorad,D. Ahmadian,Unknown,Unknown,Unknown,Unknown,,
47.0,2.0,Computational Economics,15 January 2015,https://link.springer.com/article/10.1007/s10614-015-9484-9,Intraday Anomalies and Market Efficiency: A Trading Robot Analysis,February 2016,Guglielmo Maria Caporale,Luis Gil-Alana,Inna Makarenko,Male,Male,Female,Mix,,
47.0,2.0,Computational Economics,22 January 2015,https://link.springer.com/article/10.1007/s10614-015-9485-8,Economic Modeling Using Evolutionary Algorithms: The Influence of Mutation on the Premature Convergence Effect,February 2016,Michael K. Maschek,,,Male,Unknown,Unknown,Male,"
Genetic Algorithms are the best known representation of a class of direct random search methods called evolutionary algorithms. Though widely used to solve complex optimization problems, their use within economics is grounded on their ability to represent adaptation of individuals to the underlying parameters of their economic environment. Their use facilitates modeling bounded rationality; a departure from the rational expectations hypothesis which requires a model of learning employed in its place. When genetic algorithms are utilized in this manner, a binary encoding of strategies is typically used. That is, strategies are represented by strings of zeros and ones over which the evolutionary operators act in order to introduce diversity (mutation and crossover). Each strategy is associated with a fitness level based on past success which determines the likelihood of replication in the future population of rules. Rules whose application has been more successful are more likely to become represented in the population. This occurs according to a classical probabilistic proportion selection operator that uses the relative fitness to serve as selection probability. Recently, the effect of this encoding choice has been questioned. Specifically, Alkemade et al. (2006, 2007, 2009) have shown that under certain conditions when an EA is employed for modeling purposes the results may exhibit premature convergence: different individual simulations, or runs, of the algorithm can lead to very different results. They argue that premature convergence results from a parameterization of population size that is too small. However, in their recent investigation, Waltman et al. (2011) point to the binary encoding of strategies as the culprit; arguing real-valued encoding of strategies does not exhibit the same observation even at the very low population level utilized in the Alkemade et al. consideration of the phenomenon. They argue the phenomenon has a straightforward explanation: binary encoding of strategies is associated with evolutionary stable equilibria where the inversion of a single bit is unlikely to upset the state for more than a few iterations. That is, when the population of rules is homogenous and associated with one of a small sub-set of strategies, no single-bit mutation of any individual rule is associated with a higher fitness level. However, recent work directed towards introducing two-level learning (or self-adaptation) genetic algorithms in an economic environment has illustrated the important relationship between the mutation operator and premature convergence. Two-level learning incorporates certain strategy parameters into the representation of each individual. Specifically, Maschek (2010) has demonstrated that, even in economic environments where the fitness of potential rules is complementary across individuals, the strategy parameters providing the likelihood of mutation for the individual can evolve according to the genetic algorithm alongside other binary encoded strategies. Lower mutation rates evolve as the potential evolutionary advantage from introduction of heterogeneity falls. This occurs alongside the convergence to economic equilibria. However, it was demonstrated that the progression towards a significant autocorrelation relationship in outcomes inherent with adaptive mutation may be at the expense of lower convergence reliability. Mutation rates may evolve to levels insufficient for introducing sufficient diversity too quickly, causing what was referred to as low-mutation non-convergence traps. It was demonstrated that a lower bound on the mutation rate is required to avoid premature convergence. Alternatively, these lower bounds may not be required if the fitness function is specified in a manner that yields a high degree of selective pressure. Importantly, these traps look very similar to the premature convergence phenomenon outlined in other work. Avoiding the premature convergence phenomenon may, therefore, be attained by taking similar measures. In this work, the same framework and basic parameterization as Waltman et al. (2011) is utilized. Through alternative specifications of the EA, the cause of premature convergence is re-investigated. First, the previous explanation for the cause of premature convergence is examined. The likelihood of the argument is examined by extending the length of the binary string used to encode strategies. Outcomes of these longer strings are then normalized in a manner that yields the same range of outcomes. This lengthening of the binary string allows for single-bit mutations that do achieve higher levels of fitness, meaning such outcomes are no longer evolutionary stable. Second, low-mutation non-convergence traps can be avoided with fitness transformations yielding larger evolutionary pressure. The ability of such transformations to prevent premature convergence is investigated. Finally, it is shown that for alternative specifications of the mutation operator premature convergence is not a characteristic the application of the EA in this environment.Footnote 1
",3
47.0,3.0,Computational Economics,30 January 2015,https://link.springer.com/article/10.1007/s10614-015-9489-4,Causality in Continuous Wavelet Transform Without Spectral Matrix Factorization: Theory and Application,March 2016,Olaolu Richard Olayeni,,,Unknown,Unknown,Unknown,Unknown,,
47.0,3.0,Computational Economics,03 February 2015,https://link.springer.com/article/10.1007/s10614-015-9491-x,Detecting Causality in Non-stationary Time Series Using Partial Symbolic Transfer Entropy: Evidence in Financial Data,March 2016,Angeliki Papana,Catherine Kyrtsou,Cees Diks,Female,Female,Male,Mix,,
47.0,3.0,Computational Economics,10 February 2015,https://link.springer.com/article/10.1007/s10614-015-9494-7,Estimation of Panel Model with Spatial Autoregressive Error and Common Factors,March 2016,J.  B. Qian,,,Unknown,Unknown,Unknown,Unknown,,
47.0,3.0,Computational Economics,18 February 2015,https://link.springer.com/article/10.1007/s10614-015-9496-5,Winner Determination Algorithms for Combinatorial Auctions with Sub-cardinality Constraints,March 2016,Christopher Garcia,,,Male,Unknown,Unknown,Male,"There has been a significant interest in combinatorial auctions over the past two decades, with many publications on this topic appearing in the literature during this time. Combinatorial auctions involve a set of items to be sold simultaneously. Prospective buyers then place bids which consist of packages, where a package may contain any combination of items. This type of auction can thus provide enhanced economic efficiency. However, it is often a difficult task to determine the winners in this type of auction. Winner determination in combinatorial auctions is a form of the weighted set packing problem. Set packing has long been known to be strongly NP-hard and consequently, is often computationally intractable in practice (Garey and Johnson 1979). In this research we discuss a particular type of generalization to the standard combinatorial auction. This problem involves a set of items \(R\) to be sold from which buyers can bid. Each bidder \(i\) submits a bid \(b_i \subseteq R\) at price \(p_i \). The bidder also submits a fixed number of items \(d_i\) desired out of \(b_i \), where \(d_i \le |b_i |\). The bidder is thus willing to accept any items in \(b_i\) but does not necessarily want all items in \(b_i \). We refer to this type of constraint as a sub-cardinality constraint, and to \(d_i\) as a sub-cardinality number. It is readily observed that this problem becomes a standard combinatorial auction when all \(d_i =|b_i |\). When \(d_i <|b_i |\) is possible the problem of winner determination requires not only selecting which bidders win, but also which items each winner will ultimately get. This type of auction is motivated by cases where buyers are interested in multiple potentially substitutable items in close proximity to one other. Such a concern may be present in many real-world applications of combinatorial auctions such as real estate (Quan 1994), purchasing radio spectrum frequencies (Mochon et al. 2011; Ausubel et al. 1997; Plott and Cason 1996), advertising (Dulluri and Raghavan 2005), and potentially many others. Commercial real estate developers, for example, may be interested in buying several properties within a specified area. Often the precise locations of the properties are not critically important, so long as they are all within the desired area. Purchasers of advertisements may face similar concerns, for instance, when they look to purchase advertising slots during “hotspot” times such as major sporting events. The exact times are not as critical, so long as they get the designated number of slots during the events. In cases such as these, sub-cardinality constraints are easier and more appropriate to use than Boolean expressions. Sub-cardinality constraints were first mentioned in the context of bidding languages as “k-of” expressions (Boutilier and Hoos 2001). Since then, however, combinatorial auctions involving this concern have had little attention in the literature. In this paper we examine the winner determination problem for this form of combinatorial auction, referred to hereafter as WDP-SC. We propose and evaluate three solution methods: one based on integer programming, a second based on simulated annealing, and a third based on the Metaheuristic for Randomized Priority Search (Meta-RaPS). We test these methods using data sets generated from four distributions: one that models general proximity-based interest, another that models “hotspot” interest, and two other distributions of interest used in previous literature. We then discuss the results and their implications.",2
47.0,3.0,Computational Economics,24 June 2015,https://link.springer.com/article/10.1007/s10614-015-9505-8,Credit Risk Scoring with Bayesian Network Models,March 2016,Chee Kian Leong,,,,Unknown,Unknown,Mix,,
47.0,3.0,Computational Economics,10 June 2015,https://link.springer.com/article/10.1007/s10614-015-9502-y,A Non-stationary Model of Dividend Distribution in a Stochastic Interest-Rate Setting,March 2016,Andrea Barth,Santiago Moreno–Bromberg,Oleg Reichmann,Female,Male,Male,Mix,,
47.0,3.0,Computational Economics,10 June 2015,https://link.springer.com/article/10.1007/s10614-015-9501-z,LU Decomposition in DEA with an Application to Hospitals,March 2016,Mehdi Toloo,Rahele Jalili,,Male,Unknown,Unknown,Male,"Linear programming (LP) is a mathematical technique for the optimization of a linear objective function, \(\mathbf{cx}\), subject to linear equality and linear inequality constraints, \(\mathbf{Ax}\lessgtr \mathbf{b}\), where \(\lessgtr \in \left\{ \le , =, \ge \right\} \) and it is assumed that the constraint matrix, \(\mathbf{A}\), is of size \(m\times n\). For simplicity of exposition, we consider linear equality constraints, \(i.e.\ \mathbf{Ax}=\mathbf{b}\), which is required to be solved in several research applications and many scientific studies. LP plays a major role in various fields of study like business, economics, engineering, management and applied mathematics. The simplex method, proposed by G. B. Dantzig for solving LP problems in 1947, is one of the top algorithm in recent century. In order to initiate the simplex method, an identity matrix must be available as an initial feasible basis; if such basis is unavailable, then we shall resort to artificial variables. These variables help us to find a feasible basis for the modified problem; however to determine a feasible basis for the main problem, they have to be vanished (if at all possible). The two-phase method and the big-M method are two main approaches that can be used to eliminate the artificial variables. The big-M method can theoretically help us to find a feasible basis for the main problem; however, determining its parameter M is an open issue. Therefore, the two-phase method is practically used in modern-age computer and software. Indeed, this method is more common in commercial software and consequently in this paper we are going to focus on the two-phase method. Data envelopment analysis (DEA), is a non-parametric approach based on LP to measure the relative efficiency of some homogeneous decision making units (DMUs). Each DMU consumes multiple inputs to produce multiple outputs. Charnes et al. (1978) introduced the first DEA model (CCR) under constant returns to scale (CRS) assumption, then Banker et al. (1984) extended the CCR model, called BCC, using variable returns to scale (VRS) assumption.There are two main forms of these models, envelopment and multiplier, which are mutually dual and consequently are closely related together. In the envelopment form, an important issue is to consume less possible inputs, while keeping at least the current output level. The DMU under evaluation is determined to be efficient if such reduction of inputs level be unavailable, otherwise it is inefficient. Whereas in the multiplier form the optimal weights (multipliers) of inputs and outputs is desirable. Hence, in this form the relative efficiency of a DMU is obtained by maximizing the ratio of weighted sum of outputs to the weighted sum of inputs, subject to the condition that the same ratio for all DMUs must be less than or equal to one. If the ratio be equal to one, then the under evaluated DMU is considered as an efficient one. There are other variant DEA models which are designed based on the slacks of input excesses and output shortfalls: additive model (see Charnes et al. 1985) and slacks-based measure model which was introduced by Tone (2001). In some research papers the problem of evaluating the performance of hospitals or health care centers are concerned. Leleu et al. (2014) utilized a weighted DEA approach to measure the inefficiency of inputs to the production process on a real data set of 138 hospitals operating in Florida during 2005. The authors demonstrated how their findings contribute to current policy debates both on the federal US and the state of Florida level. Karska and Dursun (2014) proposed a fuzzy multi-criteria group decision making framework for supplier selection integrating quality function deployment and DEA with imprecise data and then utilized a case study in a private hospital in Istanbul to illustrate the application of their approach. Wu et al. (2015) utilized DEA Malmquist methodology to examine the changing trends in total factor productivity and the impact of policy on the efficiency of China’s health care system at the provincial level between 2003 and 2011. Mitropoulos et al. (2015) proposed a method which enhances statistical inference in DEA and then verified their approach by a real data set involving 177 Greek hospitals in various sizes according to the hierarchical structure of the Greek health system (primary, secondary and tertiary care). Millington et al. (2015) studied 30 sites along two highly urbanised streams in Brisbane (Australia) and provided a DEA-based framework for guiding the management of urban stream health. There are several different algorithms to solve the linear system \(\mathbf{Ax}=\mathbf{b}\). In addition, in numerical analysis, different decompositions are used to solve the system of equations; LU decomposition (or LU factorization) is one of the most famous techniques in this area of research. Al-Kurdi and Kincaid (2006) used LU decomposition with iterative refinement to solve a system of linear algebraic equations with a large sparse coefficient matrix and then compared it with direct LU decomposition. Ford (2015) designed an algorithm for Gussian elimination and introduced the idea of partial pivoting. In some cases, the sparse LU decomposition is assumed in the factorization of the sparse coefficient matrix which is described in Mittal and Al-Kurdi (2002). Howevere, according to Gill et al. (1991) the type of decomposition involves production of two matrices, one lower triangular matrix, \(\mathbf{L}\), and one upper triangular matrix, \(\mathbf{U}\). This product sometimes includes a permutation matrix as well. obviously the LU decomposition is particularly well suited for sparse and large-scale problems and can be computed using the Gaussian elimination approach. This algorithm provides the most efficient and accurate way of solving a general linear system of the form \(\mathbf{Bx}=\mathbf{b}\), where \(\mathbf{B}\) is an invertible matrix. Indeed, \(\hbox {rank} (\mathbf{B}),\det \mathbf{B}\), and \(\mathbf{B}^{-1}\) can be calculated using the Gaussian elimination methodology. The Gaussian elimination method runs in \(O(m^{3})\) time where m is the size of \(\mathbf{B}\) (see Stoer and Bulirsh 2002). The majority of the operations in this algorithm is spent on the matrix \(\mathbf{B}\) itself, which means that, if this matrix involves a special structure, such as upper-triangularity or lower-triangularity, then the order of the algorithm can be reduced. 
Bartels and Golub (1969) introduced the simplex method using the LU-factorization of the basis and proposed to employ the “elimination form of the inverse.” Hu and Pan (2008) suggested an approach to update the simplex multiplier vector from one iteration to the next based on updates to the LU factors. For more details we refer the reader to Gill et al. (1974, 1987), Saunders (1976a, b) and Ping-Qi (2014). This paper is organized as follows: Sect. 2 reviews the approach of Toloo et al. (2015) to obtain an initial basic feasible solution (IBFS) for the CCR model. In Sect. 3, LU decomposition on the basis of CCR model is implmented. Applicability of the developed approach is illustrated by a numerical example in Sect. 4. Finally, in the last section we present some conclusion and remarks.",9
47.0,3.0,Computational Economics,21 February 2015,https://link.springer.com/article/10.1007/s10614-015-9497-4,Dynamic Input–Output Models in Environmental Problems: A Computational Approach with CAS Software,March 2016,George Halkos,Kyriaki Tsilika,,Male,Female,Unknown,Mix,,
47.0,3.0,Computational Economics,30 September 2015,https://link.springer.com/article/10.1007/s10614-015-9527-2,Erratum to: Measuring Environmental Performance Under Regional Heterogeneity in China: A Metafrontier Efficiency Analysis,March 2016,Yanni Yu,Yongrok Choi,,Unknown,Unknown,Unknown,Unknown,,
47.0,4.0,Computational Economics,23 January 2015,https://link.springer.com/article/10.1007/s10614-015-9481-z,Analysis of Correlation Based Networks Representing DAX 30 Stock Price Returns,April 2016,Jenna Birch,Athanasios A. Pantelous,Kimmo Soramäki,Female,Male,Male,Mix,,
47.0,4.0,Computational Economics,10 June 2015,https://link.springer.com/article/10.1007/s10614-015-9500-0,Optimal Prediction Periods for New and Old Volatility Indexes in USA and German Markets,April 2016,Javier Giner,Sandra Morini,Rafael Rosillo,,Female,Male,Mix,,
47.0,4.0,Computational Economics,04 February 2015,https://link.springer.com/article/10.1007/s10614-015-9490-y,Using a Genetic Algorithm to Improve Recurrent Reinforcement Learning for Equity Trading,April 2016,Jin Zhang,Dietmar Maringer,,Female,Male,Unknown,Mix,,
47.0,4.0,Computational Economics,20 November 2014,https://link.springer.com/article/10.1007/s10614-014-9479-y,"Forecasting US Unemployment with Radial Basis Neural Networks, Kalman Filters and Support Vector Regressions",April 2016,Charalampos Stasinakis,Georgios Sermpinis,Andreas Karathanasopoulos,Male,Male,Male,Male,"The voluminous macroeconomic literature includes a variety of forecasting competitions of linear and non-linear architectures. Through these studies researchers attempt to shed light on time series, such as inflation or unemployment, that are relevant to monetary and policy decisions worldwide. Several techniques have been applied to such forecasting tasks with ambiguous results. Therefore, statisticians and econometricians turn to highly computational, time-varying and adaptive in nature techniques. Neural networks (NNs) are one such class of models that can assist their quest for improved forecast accuracy. Especially in periods of extreme structural instabilities, NNs’ data-adaptive learning and clustering ability can prove to be very useful in forecasting applications (Zhang et al. 1998). It is, thus, not surprising that NNs continue to receive a great deal of attention in the literature (Huang et al. 2013; Özkan 2013; Fernandes et al. 2014; Olmedo 2014). Forecasting unemployment rates, especially, is a very well documented case study (Szpiro 1997; Montgomery et al. 1998; Rothman 1998; Koop and Potter 1999). Skalin and Teräsvirta (2002) use multivariate STAR models to forecast unemployment rates. Moshiri and Brown (2004) apply a back-propagation model and a generalized regression NN model to estimate post-war aggregate unemployment rates in the USA, Canada, UK, France and Japan. The out-of-sample results confirm the forecasting superiority of the NN approaches against traditional linear and non-linear autoregressive models. Bayesian NNs are applied in the case study of forecasting unemployment in West Germany by Liang (2005). The empirical evidence indicate that the NNs present significantly better forecasts than traditional autoregressive models. Milas and Rothman (2008) use smooth transition vector error-correction models to predict unemployment rates in the non-Euro G7 countries. The proposed model outperforms the linear autoregressive benchmark and improves significantly the forecasts of the US and UK unemployment rate during business cycle expansions. Olmedo (2014) performs a competition between non-linear models, including NNs and nearest neighbour algorithms, to forecast different European unemployment rate time series. The best results are provided by a vector autoregressive and baricentric predictor. As the forecasting horizon lengthens the performance deteriorates and in some cases NNs. The idea of combining forecasts to improve forecast accuracy is not new (Bates and Granger 1969; Newbold and Granger 1974; Deutsch et al. 1994). Swanson and Zeng (2001) perform forecast combinations based on a model-selection approach and suggest that a SIC-based approach to combine forecasts can be a useful alternative to combination methods such as simple averaging or mean square error minimization. Teräsvirta et al. (2005) examine the forecast accuracy of linear autoregressive, smooth transition autoregressive and NN models for 47 monthly macroeconomic variables, including unemployment rates, of the G7 economies. The empirical results prove that their forecasting ability is much improved when they are combined with autoregressive models. Kapetanios et al. (2008) report that combinations of statistical forecasts from several models (random walks, STARs, ARs, VARs etc.) generate good forecasts of inflation and growth. They also note that such forecast combinations can serve as an unbiased benchmark, which could be compared with conditional and judgemental policymaker’s expectations. Finally, Vasnev et al. (2013) combine forecasts of models incorporating monthly and quarterly macroeconomic time series to predict the monetary operations of the Reserve Bank of Australia. Their findings confirm the benefits of forecast combination models and present alternative methods of forecasting monetary decisions. Given the previous framework, the rational of this paper is twofold. Firstly, we investigate the efficiency of the radial basis function neural networks (RBFNNs) in forecasting the US unemployment. Secondly, we explore the utility of Kalman filter and support vector regression (SVR) as forecast combination techniques. On one hand, an autoregressive moving average model (ARMA), a smooth transition autoregressive model (STAR) and three different neural networks architectures, namely a multi-layer perceptron (MLP), recurrent neural network (RNN) and a psi sigma network (PSN) are used as benchmarks for our RBFNN. On the other hand, our forecast combination methods are benchmarked with a simple average and a least absolute shrinkage and selection operator (LASSO). The statistical performance of our models is estimated throughout the period of 1972–2012, using the last 7 years for out-of-sample testing. The empirical evidence of this application is further validated by the use of the modified Diebold–Mariano test. With this study, we intend to extend the growing literature of using RBFNNs and NNs in general in financial and macroeconomic forecasting task. In addition, the evaluation of the Kalman Filter and SVR adds validity to the evidence of previous studies that report the benefits of combining forecasts. Finally, the performance of those non-linear and time-varying combination methods evaluate if there is a need to experiment beyond traditional linear equivalents. The rest of the paper is organized as follows. Section 2 presents the description of the dataset used in this application. Sections 3 and 4 give an overview of the forecasting models and the forecast combination methods implemented respectively. The statistical performance of our models is presented in Sect. 5. Finally, some concluding remarks are summarized in Sect. 6.",14
47.0,4.0,Computational Economics,25 February 2015,https://link.springer.com/article/10.1007/s10614-015-9492-9,Exploiting Financial News and Social Media Opinions for Stock Market Analysis using MCMC Bayesian Inference,April 2016,Manolis Maragoudakis,Dimitrios Serpanos,,Male,Male,Unknown,Male,"Stock market prediction has always gained certain attention from researchers. There is a controversy as regards to whether there is a method for accurate prediction of stock market movement, mainly due to the fact that modeling market dynamics is a complex and volatile domain. Stock market research encapsulates two main philosophical attitudes, i.e. fundamental and technical approaches Technical-Analysis (2005). The former states that stock market movement of prices derives from a security’s relative data. In a fundamentalist trading philosophy, the price of a security can be determined through the nuts and bolts of financial numbers. These numbers are derived from the overall economy, the particular industry’s sector, or most typically, from the company itself. Figures such as inflation, joblessness, return on equity (ROE), debt levels, and individual price to earnings (PE) ratios can all play a part in determining the price of a stock. In technical analysis, it is believed that market timing is the key concept. Technicians utilize charts and modeling techniques from past data to identify trends in price and volume. These strategists believe that market timing is critical and opportunities can be found through the careful averaging of historical price and volume movements and comparing them against current prices. Technicians also believe that there are certain high/low psychological price barriers such as support and resistance levels where opportunities may exist. They further reason that price movements are not totally random. Nevertheless, according to several researchers, the goal is not to question the predictability of financial time series but to discover a good model that is capable of describing the dynamics of stock market. Towards the latter direction, stock market analysis by utilizing Information Technology methods is a dynamic and challenging domain. Over the past years, there has been an increasing focus on the development of modeling systems, especially when the expected outcomes appear to yield significant profits to the investors’ portfolios. In alignment with modern globalized economy, the available resources are becoming gradually more plentiful, thus difficult to be analyzed by standard statistical tools. Therefore, technical analysis experts judge that stock market is an excellent representative field of application with strong dynamics in research through data mining, mainly due to the quantity and the increase rate that data is being produced. Thus far, there have been a number of research papers that emphasize solely in past data from stock bond prices and other technical indicators. Nevertheless, throughout recent studies, prediction is also based on textual records, based on the logical assumption that the course of a stock price can be influenced by news articles, ranging from companies releases and local politics to news of superpower economy Ng and Fu (2003). However, unrestricted access to news information was not possible until the early 1990’s. Nowadays, news are easily accessible, access to important data such as inside company information is relatively cheap and estimations emerge from a vast pool of economists, statisticians, journalists, etc., through the World Wide Web. Despite the large amount of data, advances in Natural Language Processing and text mining allow for effective computerized representation of unstructured document collections, analysis for pattern extraction and discovery of relationships between document terms and time-stamped data streams of stock market quotes. Despite the fact that news play an important role towards influencing stock market trends, public mood states or sentiment, as expressed through various means that promote inter-connectivity, such as Web 2.0 platforms, may also play a similarly important role. Targeted research in the domain of psychology has proven that emotions in addition to information have a direct impact in human decision-making Liu et al. (2007). Therefore, a logical assumption would be for someone to consider public opinion as a factor that could also affect stock market values. In the present study, the main goal is to study the impact of technical analysis, news articles and public opinions to the task of predicting stock market value. The importance of this study lies to the fact that technical analysis contains the event and not the cause of the change, while textual data may interpret that cause. Following recent trends in the task at hand, this paper takes into account a large number of technical indices, accompanied with features that are extracted by a textual analysis phase from financial news articles and public opinions from financial information portals and Twitter. We incorporate Machine Learning algorithms, that have been adjusted to match the characteristics of the collected dataset, which is characterized by a plethora of attributes. Our proposed methodology is based on a novel Markov Chain Monte Carlo (MCMC) Bayesian Inference approach, which estimates the conditional probability distributions of network structures that are obtained by a Tree-Augmented Naïve Bayes (TAN) algorithm. Experimental results, including a virtual trading experiment, are promising. Certainly, as it is tedious for a human investor to read the plethora of available daily news and public reactions concerning a company as well as other financial information, a prediction system that could analyze such textual resources and find relationships with price movement at future time windows is beneficial. The paper is structured as follows: Section 2 provides an overview of literature concerning stock market prediction, in an attempt to link previous works with the article and also to provide a clear motivation for the proposed study. Section 3 presents the methodology overview, introducing Bayesian networks and their use towards modeling and reasoning under conditions of uncertainty. Since standard Bayesian networks face certain issues when dealing with classification problems in high-dimensional domains, in Section 4, we provide the theoretical framework of MCMC inference which suits our needs. Section 5 deals with the proposed method, which utilizes the main characteristics of MCMC but also introduces a novel approach on estimating conditional probability distributions from networks that favor classification tasks. Section 6 presents the input data characteristics as well as the processing phase of textual information and Section 7 describes the experimental evaluation process. Concluding remarks and future directions are found in Section 8.",10
47.0,4.0,Computational Economics,14 March 2016,https://link.springer.com/article/10.1007/s10614-016-9563-6,Adaptive Radial Basis Function Methods for Pricing Options Under Jump-Diffusion Models,April 2016,Ron Tat Lung Chan,,,Male,Unknown,Unknown,Male,"In this paper we show how to compute European and American option prices in the Merton jump-diffusion model using radial basis function (RBF) interpolation techniques. RBF methods have recently been proposed for numerically solving initial value and free boundary problems for the classical Black and Scholes equation, both in the one and in the multiple asset case Fasshauer et al. (2004a, b), Hon and Mao (to appear). The new feature of the present paper is that in the Merton model (and comparable jump-diffusion models, as in general Lévy type models), the Black and Scholes PDE is replaced by a partial integro-differential operator or PIDE, involving a non-local term in the form of an integral operator. Our main contribution is to show how to numerically solve these in an efficient way using RBFs, both for initial value and free boundary problems (as for American options), and including when singularities in the initial value (identified with the option’s pay-off) are present. We have chosen the Merton jump-diffusion model as a typical case on which to test the present RBF methodology. Comparing with our previous studies Chan and Hubbert (2014) and Brummelhuis and Chan (2014), we mainly focus on improving the interpolation accuracy of the option pay-off in order to achieve a higher accuracy of option prices. To acheive this goal, we adopt an adaptive scheme proposed by Driscoll and Heryudono (2007) and inverse multiquadric radial basis function (IMQ). Currently, PIDEs such as the Merton one have mostly been treated by a traditional finite difference method (FDM), or by a finite element method (FEM). The idea is to simply fully discretize the PIDE on an equidistant grid, after having (artificially) localized the equations to some bounded interval/domain in \(\mathbb {R } \). The non-local integral term can be computed by numerical quadrature or by using the fast fourier transform (FFT). In general, there are a number of problems which arise with these current approaches: The option price behaviour outside the solution domain must be assumed; see e.g. Cont and Voltchkova (2005), d’Halluin et al. (2004, 2005). Some of the literature, e.g. Andersen and Andreasen (2000), Briani et al. (2007), Cont and Voltchkova (2005), Matache et al. (2005), has played down the importance of pricing American and European vanilla option values when time to maturity is less than 6 months. The reason is that for short times-to-maturity the numerical methods used price the option incorrectly around the strike price where a singularity (kink) exists. A singularity is defined as a point at which the function, or its derivative, is discontinuous. The payoff functions of vanilla call and put options have such a singularity. As a result, standard numerical methods like FDM and FEM cannot give accurate precision and suffer a reduced rate of convergence when one uses them to price options at a very short time to maturity. Foysth et al. shed light on addressing this kind of problem (d’Halluin et al. 2005) by suggesting Rannacher’s time stepping method (Rannacher 1984). This is a mixture of implicit and Crank-Nicolson methods. They demonstrate this technique by approximating an option price whose maturity is a quarter of a year. This method gives second order rates of convergence when pricing European options but not for American ones. By using the same idea and combining it with a penalty method and a modified form of a timestep selector suggested in Johnson (1987), Forysth et al. in their other paper (d’Halluin et al. 2004) show how to achieve second order convergence for pricing American options. Although their methods can yield second order convergence, the necessary calculations can be quite complex. In Andersen and Andreasen (2000), Briani et al. (2007), d’Halluin et al. (2005), d’Halluin et al. (2004), etc, the fast fourier transform is applied to calculate the non-local jump integral term in the PIDE, and the diffusion and integral terms are treated separately. This therefore requires that function values are interpolated and extrapolated between the diffusion and integral grids so as to approximate the convolution term. Andersen and Andreasen’s approach of combining an operator splitting approach with the fast fourier transform (FFT) approximation of a convolution integral to price European options with jump diffusion Andersen and Andreasen (2000) cannot deal easily with American options. The papers (Briani et al. 2007; Cont and Voltchkova 2005; d’Halluin et al. 2004, 2005) implement an implicit–explicit numerical scheme to price European or American options under the Merton jump-diffusion model. These papers treat the convection (hyperbolic) term of the PIDE explicitly by implementing the upwind scheme and the diffusion (elliptic) term of the PIDE implicitly. As a result, restrictive stability conditions are necessary for the convection term when the upwind scheme is implemented. A final but fundamental problem with both FDM and FEM is that these are, in practice, restricted to problems of two or three space dimensions; however, most applications easily need many more, e.g. when pricing basket options. Our RBF-method will circumvent many of these disadvantages. In particular, differential and integral terms will be treated on an equal footing, and the use of an adaptive RBF-scheme will allow us to deal with the singularity in the option pay-off. This paper is divided into five sections, including this introduction. Section 2 is a brief review of Metron’s jump-diffusion model. In Sect. 3 we first explain adaptive residual subsampling method and then define our RBF algorithm for solving PIDEs, which we implement the Merton PIDE. Section 4 contains our numerical results for interpolation of an initial put payoff function and for both European and American put options, including an analysis of the max error, the root-mean-square error and the relative error. Section 5 concludes.",14
48.0,1.0,Computational Economics,18 August 2015,https://link.springer.com/article/10.1007/s10614-015-9518-3,Time-Frequency Adapted Market Integration Measure Based on Hough Transformed Multiscale Decompositions,June 2016,George Tzagkarakis,Juliana Caicedo-Llano,Thomas Dionysopoulos,Male,Female,Male,Mix,,
48.0,1.0,Computational Economics,14 August 2015,https://link.springer.com/article/10.1007/s10614-015-9512-9,Conditions Sufficient to Infer Causal Relationships Using Instrumental Variables and Observational Data,June 2016,Henry L. Bryant,David A. Bessler,,Male,Male,Unknown,Male,"While some authors will be very careful, application of IV methods is very commonly accompanied by causal interpretation lacking rigor. The terminology used in many expositions on IV encourages causal interpretation—one is said to estimate the “causal effects” of one variable on another. As we will show, this interpretation requires maintained assumptions about causality that are almost always implicit, and seem to often be completely unappreciated. The Wikipedia entry on on instrumental variables provides a convenient example of this phenomenon: “If tobacco taxes affect health only because they affect smoking (holding other variables in the model fixed), correlation between tobacco taxes and health is evidence that smoking causes changes in health.” (emphasis present in the original material Wikipedia 2013). This statement contains an implicit assumption regarding the direction of possible causality between tobacco taxes and rates of tobacco smoking, namely that if the two are correlated, that the former would be a cause of the latter. Yet high rates of smoking might entice policy makers to implement or raise tobacco taxes to exploit a large potential source of revenue, or may compel policy makers to raise tobacco taxes to discourage unhealthy behavior. Alternatively, one might support the conclusion by assuming that if there is any correspondence between smoking and health outcomes, that the former caused the latter. This specific judgment in this specific problem may be perfectly reasonable in light of accumulated scientific evidence, but blind assumptions regarding the direction of causal flow in all IV applications is certainly unwise. Critically, note that observationally equivalent covariance properties will result from data generated by the causal structure \({\textit{Taxes}}\rightarrow {\textit{Smoking}}\rightarrow {\textit{Health}}\) and the alternative causal structure \({\textit{Taxes}}\leftarrow {\textit{Smoking}}\leftarrow {\textit{Health}}\). All three variables will be mutually correlated in either case. If \({\textit{Taxes}}\leftarrow {\textit{Smoking}}\leftarrow {\textit{Health}}\) was the true causal structure, a researcher could nonetheless predict \({\textit{Smoking}}\) using values of \({\textit{Taxes}}\) (call the predictions \({\textit{Smoking}}^{*}\)), and could find a statistically significant correspondence between \({\textit{Smoking}}^{*}\) and \({\textit{Health}}\). This strategy clearly cannot be used to infer that smoking affects health outcomes unless additional assumptions are invoked. A burgeoning literature describes the inference of causal relationships among observed random variables when controlled experiments are not conducted (Glymour and Cooper 1999; Hoover 2005; Pearl 2000; Spirtes et al. 2000). This literature generally documents the development of algorithmic approaches to inferring causality among a possibly large number of variables. Such algorithms facilitate an exploratory approach to causal inference, wherein the researcher does not explicitly form or test specific causal hypotheses. A nascent offshoot from this literature, however, applies the logic that underlies such algorithms to the problem of investigating specific causal hypotheses. Bryant et al. (BBH 2009) demonstrate how, under certain conditions, one can use observational data on three variables, say A, B and C, to conclude, in the presence of latent variables or not, that A does not cause B. They use Monte Carlo methods to demonstrate reliable small sample properties of such inference. The key idea is that a non-zero correlation between A and B can be interpreted as causal and not just associational under certain reasonable assumptions, and that causality cannot run from A to B if there exists a third variable C (call this an instrument) such that A and B are correlated, A and C are correlated, and C and B are not correlated. Abstracting from the possibility of causally related latent variables, the associated causal structure is \(C\rightarrow A\leftarrow B\). Thus, an “inverted fork” in the causal structure is the revealing feature that facilitates inference of the direction of causal flow. 
Scheines (2005) describes the similarities between causal inference in experimental and observational studies, and the conditions under which one can conclude that A does cause B even when A is not directly manipulated. As described below, two instrumental variables are required for such a conclusion, rather than just one—it is easier to disprove a causal relationship using observational data (as in BBH) than to infer one. These two instruments must be related to A and B, and to one another, in very specific ways. A causal inverted fork (involving A and the test instruments) is again crucial to the inference of the direction of causal flow, and distinguishes the methods described here from standard instrumental variables methods. If suitable instruments are available, then under some reasonable assumptions positive causal relationships can be formally inferred. The present paper provides two contributions. First, we review the relevant causal inference literature, demonstrate that standard IV methods are not sufficient to prove causality between two variables in the absence of maintained assumptions, and we clarify the conditions required to conclude that A does cause B, where A, B, and related instruments are only observed and not actively manipulated. Couched in the philosophy of falsification and the framework of classical statistical hypothesis testing, we clarify the conditions sufficient to reject a null hypothesis that A does not cause B, which we write as \(H_{0}\) : \(A\not \rightarrow B\), in favor of an alternative hypothesis \(H_{A}\): \(A\rightarrow B\). Second, we characterize the confidence that we can place on such judgments for linearly related, jointly normal random variables. That is, we conduct an analysis like that of BBH for the causal inference problem discussed in Scheines (2005).",2
48.0,1.0,Computational Economics,28 November 2015,https://link.springer.com/article/10.1007/s10614-015-9534-3,Reducing Overreliance on Sovereign Credit Ratings: Which Model Serves Better?,June 2016,Huseyin Ozturk,Ersin Namli,Halil Ibrahim Erdal,Unknown,Male,Male,Male,"The internationalization of financial markets in last decades has dramatically increased and differentiated investment opportunities across the world by creating new challenges. Measuring credit risk has been so important that credit rating agencies (CRAs) became the indispensable institutions in current financial architecture. Yet, conventional credit risk measurement methods can mislead and contribute to systemic crises by dislocating actual risk in various investment decisions. Hence, measuring credit risk is an unbearably important issue that calls for thorough credit scoring systems. Accurate measurement of credit risk is an extremely difficult task, since an attempt to measure it suffers from the weaknesses of measurement tools and wrong value judgements. CRAs have been regularly under harsh criticism, the crux of which turned out to be inaccuracies in credit ratings, since the 2008 financial crisis. Although credit ratings of banks and structured financial instruments were on the top agenda in the early days of global financial crisis, sovereign credit ratings have also been under fire especially when Eurozone fiscal problems become preeminent. CRAs were blamed once again for the tumult after deep downgrades of many Eurozone countries. A successful credit scoring will certainly benefit both international lenders and borrowers since repayment difficulties arising from false rating alarms can lead to tremendous burden on financial system. Correct measurement of sovereign credit risk can help avoid huge losses in financial transactions. Yet, “who will measure?” and “how will be measured?” are the important questions to be addressed. From a regulatory perspective, improvement in the measurement of credit risk is highly encouraged by international financial organizations or joint regulatory initiatives. For instance, the recommendations of the Financial Stability Board (FSB) emphasize the details of individual credit scoring systems (FSB 2010). Among the policy recommendations of a report by the FSB, the idea of individual credit scoring was intensely stressed in order to overcome the overreliance on CRAs’ credit ratings which were blamed to be inaccurate. Therefore, it is not hard to anticipate that devising individual credit scoring systems will be among the top priorities of financial institutions who suffer from credit ratings. The FSB partially answers the question of “who will measure?”, yet “how will be measured?” is still pending to be answered. Linear discriminant analysis, principal component analysis, linear regressions, ordered response models etc. are the widely used statistical tools in modelling sovereign credit ratings. However as Wang et al. (2011) argue, statistical techniques assume strong prerequisites that many of them are frequently violated. For instance, multivariate normality assumptions and normality assumptions for each and every independent variable are not warranted. In addition, the prediction power of statistical models is frequently low and the validity of models is relatively poor. Recent studies suggest that unlike statistical methods, artificial intelligence (AI) models that do not have strong prerequisites outperform the statistical models in predicting credit ratings. Yet, these studies mainly examine corporate credit ratings and evidences on sovereign credit ratings are scant. Our purpose in this paper is to identify the determinants of sovereign credit ratings, and identify the methods that predict the sovereign credit ratings the best. For this purpose, we examine various AI models in the prediction of credit ratings through a comparison with a conventional statistical model. We also distinguish investment grade countries and speculative grade countries, whether the predictive power of the models considerable differ for these two groups of countries. There is no overall best AI technique applicable in credit scoring models. The best powerful prediction depends on the details of the data structure, the data characteristics, the classification of credit ratings etc. In our analyses, we employ five different techniques that are relatively new in this literature that were not employed in the prediction of sovereign credit ratings to the best of our knowledge. These techniques are classification and regression trees (CART), multilayer perceptron (MP), support vector machines (SVM), Bayes net, and naïve Bayes. Based on the findings, we compare the predictive power of these techniques with statistical models in predicting sovereign credit ratings. The dataset of this study encompasses 1023 country-year observation owing to 92 countries for the period of 1999–2010. The results suggest that AI techniques outperform the conventional statistical technique in predicting sovereign credit ratings. Although four of the techniques employed in this study have over 90 % prediction power, the CART and MP techniques outperform the others with the accuracy ratios very close to 100 %. Moreover, even the least performer in AI methods clearly outperforms the statistical technique. These findings have many implications for practitioners in credit scoring industry, the most important of it is the reliability of AI techniques for individual credit scoring systems. The remainder of the paper is organized as follows: the next section will focus on the sovereign ratings and the literature. The third section will briefly introduce the techniques in our analyses. The fourth section will discuss the data selection process and the results. The last section will conclude with policy implications ant future research themes.",8
48.0,1.0,Computational Economics,12 October 2015,https://link.springer.com/article/10.1007/s10614-015-9531-6,Optimal Estimation Strategies for Bivariate Fractional Cointegration Systems and the Co-persistence Analysis of Stock Market Realized Volatilities,June 2016,Marcel Aloy,Gilles de Truchis,,Male,Male,Unknown,Male,"In recent years, fractional cointegration has attracted considerable attention. Indeed, theoretical and empirical studies have primarily investigated the rigid case where observables possess unit roots and the cointegrating errors are weakly dependent (see e.g. Engle and Granger 1987; Johansen 1991; Phillips 1991).Footnote 1 However, the seminal definition proposed by Granger (1981) is more flexible and introduces the possibility that the explanatory variables and errors cointegration may have fractional orders of integration, respectively denoted \(\delta \) and \(\gamma \). In a pioneering work, Cheung and Lai (1993) suggest a two-step procedure to estimate a bivariate fractional cointegration system: the first step is to estimate the long-run coefficient \(\beta \); the second step is to estimate \(\gamma \), the integration order of collected residuals. Subsequently, numerous studies addressed the issue of estimating \(\beta \) in presence of cointegration with unknown integration orders,Footnote 2 leading to identify three sub-cases of cointegration: the strong fractional cointegration (\(\delta -\gamma >1/2\) and \(\delta >1/2\)); the weak fractional cointegration (\(\delta -\gamma <1/2\) and \(\delta \lessgtr 1/2\)); the stationary fractional cointegration (\(\delta -\gamma <1/2\) and \(\delta < 1/2\)). A more recent strand of the literature focuses on estimating \(\delta , \gamma \) and \(\beta \) jointly in order to achieve greater efficiency (see Lobato 1999; Velasco 2003). For instance, Velasco (2003) and Hualde and Robinson (2007) suggest different methods to jointly estimate \(\delta \) and \(\gamma \), while Nielsen (2007), Robinson (2008), and Shimotsu (2012) propose a local Whittle estimators of \(\delta , \gamma \) and \(\beta \).Footnote 3
 Therefore, there are several strategies for estimating fractional cointegration, implying estimators with non-equivalent asymptotic and finite sample properties. Accordingly, determining an optimal strategy of estimation is not so simple for practitioners. Panopoulou and Pittis (2004), Kurozumi and Hayakawa (2009), and Hualde and Iacone (2012) deal with this issue providing theoretical and/or numerical comparisons but essentially focus on \(\beta \) estimates. In this article, we propose a Monte Carlo study that covers the three cases of cointegration and deals with two-step and one-step procedures. Our panel of estimators includes popular as well as recent techniques. The results of our finite sample analysis provide useful guidance to practitioners. The rest of the paper is laid out as follows. The Sect. 2 introduces the data generating process and the different estimators considered in our simulation study, distinguishing between both the two-step and the one-step procedures. Section 3 presents the results of the Monte Carlo experiment. Section 4 provides an empirical illustration and a feasible methodology to estimate and test for stationary cointegration. Section 5 concludes.",
48.0,1.0,Computational Economics,08 July 2015,https://link.springer.com/article/10.1007/s10614-015-9508-5,The Effect of a Credit Crunch on Equilibrium Market Structure,June 2016,Martin Watzinger,,,Male,Unknown,Unknown,Male,"In 2007 and 2008 there existed a widespread fear that several OECD countries were suffering from a credit crunch. Loan losses and lower asset prices ate significantly into the equity of the banking sector, a fact which many believed would cause banks to ration credit. According to standard macroeconomic models, a lesser amount of available credit leads to a reduction in investment, resulting in turn in lower production and lower welfare in the following periods (e.g. Bernanke et al. 1999). However, these models do not take into account the change in market structure which might result from the financial frictions. For a welfare analysis, market structure is important because it directly influences prices and the available choices for consumers. For example, if in a duopoly a smaller competitor is unable to replace its broken machinery because of a lack of credit, he might exit and leave the consumer with a monopoly supplier. Including financial frictions in any oligopoly model is challenging because investment, financing decisions, and market competition are inherently interdependent and dynamic: past investment decisions determine today’s market structure which in turn influences current investment decisions. But a firm can only invest if enough means are available. Investment funds can either come from current profits (determined by today’s market structure), retained cash (determined by past financing decisions), or its debt capacity (determined by future profits). Additionally, a firm cannot retain more cash or pay back more debt than its current available funds. To complicate matters further, firms rationally anticipate future investment needs and shortages in funding. This article contributes to the literature on financial constraints by proposing a computationally feasible model which takes all these factors into account for the case of a dynamic duopoly. To integrate financing and investment decisions, we introduce firms with an endogenous capital structure and an optimizing bank into an Ericson–Pakes framework. However, including an endogenous capital structure for each firm gives rise to a large state space which makes a calculation of the equilibrium intractable with conventional Gauss–Seidel and Gauss–Jacobi algorithms. Therefore, we use a variant of the novel algorithm based on the experience based Markov equilibrium (EBE) framework introduced by Fershtman and Pakes (2012) to solve our model. Each firm is characterized by three state variables: capacity, debt level, and cash reserves. Firms accumulate capacity over time and compete repeatedly in the product market to earn profits. In every period, they aim to maximize the net present value of dividend payments. For this purpose, they optimally choose production, investment, the amount of cash to retain, and the size of debt repayments. Whatever is left of the profits after subtracting all incurred costs is distributed as a dividend to the shareholders. Firms can apply for a loan if the current cash flow is insufficient to cover expenses. The loan is provided by a risk-neutral bank given that its expected return exceeds an exogenously set minimum threshold. This threshold parametrizes the amount of credit rationing prevalent in the market. Using this model, we show that credit rationing serves as a propagation mechanism which amplifies small idiosyncratic shocks to capacity. This mechanism can lead to the monopolization of the market. If a firm loses productive capacity through a depreciation shock, lower current profits are available for financing investment. With well-functioning credit markets, the firm can compensate for this loss in its cash flow by increasing the amount of credit financing. But if credit is rationed, firms might be unable to cover the cost of capacity addition. Without investment, the firm remains at the lower capacity level which is associated with less funds. If the firm is hit by another depreciation shock which further tightens credit constraints, its ability to react by an increase in investment is reduced even more. Eventually, this process can lead to the exit of the firm, even if it has the same total factor productivity as the remaining incumbent. The monopolization of the market is made permanent by two other effects: first, with credit rationing, entrants face financing constraints, too. Therefore new firms cannot enter because they do not obtain sufficient credit to finance initial outlays. Therefore the monopolization due to the credit constraints is not quickly reversed by market entry. Second, the competing firm can expand its own capacity and market share. Increased capacity translates into higher profits which eases credit rationing in the competitor’s investment process. In the following plays, the monopolist can then finance itself through cash retainment, increase capacity faster, and gain a dominant position in the market. Given these theoretical results, a recession which is accompanied by a credit crunch might not be only “cleansing,” i.e., destroy unproductive firms (as in Caballero and Hammour 1994), but also force viable competitors out of the market. The welfare of the consumer is reduced by higher prices caused by the ensuing monopolization. This observation gives a rationale for government interventions which aim to increase the credit volume available to companies. According to our model, such programs should seek in particular to support small firms to prevent their exit or facilitate their entry. The reason is that small companies (in contrast to large companies) do not have sufficient free cashflow to finance investment and therefore have to rely on a functioning credit market to fund start-up costs or growth plans. If they cannot finance investment with credit they exit or fail to enter the market, which reduces competition and consumer welfare. This article contributes to the growing literature on modeling imperfect competition with heterogeneous firms. It is the first model to introduce financial frictions in an Ericson–Pakes framework.Footnote 1 We extend the dynamic duopoly model outlined in Besanko and Doraszelski (2004) and Besanko et al. (2010) by firms with an endogenous capital structure and an optimizing bank. The larger state space resulting from the endogenous capital structure makes it is necessary to use the new stochastic algorithm of Fershtman and Pakes (2012) to numerically compute the equilibrium. With this algorithm, we can solve the game much faster than with the commonly used Pakes and McGuire (1994) or Pakes and McGuire (2001) algorithms. To the best of our knowledge, the only other application of the Ericson–Pakes framework to finance is Kadyrzhanova (2009), which models the effect of corporate control imperfections on industry structure. However, others have worked on financial frictions in dynamic firm models using the alternative framework of Hopenhayn (1992). In contrast to Ericson–Pakes type models, this framework considers only aggregate firm dynamics by assuming an infinite number of firms with an infinitely small market share. Therefore, it is impossible to consider oligopoly behavior in this framework. In addition, in the model of Hopenhayn (1992), all dynamics are driven by permanent firm specific shocks, because temporary shocks average out. In our model, in contrast to that, temporary idiosyncratic shocks are amplified through the capital structure and competitive behavior. The number of applications of the modeling framework of Hopenhayn (1992) in the finance literature is huge: for instance, Cooley and Quadrini (2001) investigate the effect of financial frictions on firm growth. Gomes (2001) explains the effect of financial frictions on investment. Hennessy and Whited (2005) consider a dynamic trade-off model of leverage, corporate saving, and real investment to explain debt dynamics.Footnote 2
 Our results are qualitatively similar to the effects described in Kiyotaki and Moore (1997), which characterizes the emergence of credit cycles. Their main idea is that in downturns, both earnings and the liquidation value of collateral are low because potential buyers are cash-strapped. Due to the lower collateral value credit constrained firms cannot borrow for investment, which in turn further reduces their future earnings. As the liquidation value of the collateral is again reduced by this reduction in expected profits, a reinforcing cycle ensues. In contrast, in our article, the firms cannot borrow further money because banks are cash strapped and the effect is transmitted via the expectations of the banking sector and oligopoly behavior. The remainder of this article is organized as follows. We set up the model in Sect. 2. Sections 3 and 4 present the results and robustness checks. Section 5 concludes.",
48.0,1.0,Computational Economics,08 July 2015,https://link.springer.com/article/10.1007/s10614-015-9506-7,A Numerical Method for Discrete Single Barrier Option Pricing with Time-Dependent Parameters,June 2016,Rahman Farnoosh,Hamidreza Rezazadeh,M. Hossein Beheshti,Male,Unknown,Unknown,Male,"Option pricing is one of the most common problems in quantitative finance and a great number of researchers are involved in it Black and Scholes (1973). (see for example Fusai and Recchioni 2008; Heynen and Kat 1995). Barrier options are among the most applicable and popular types of exotic options which are desirable in financial markets. Barrier options are traded in different features. As a description, down-and-out barrier option is that option which terminate (knock-out) if the price of underlying asset descends and hits the pre determined barrier. In practice and with attention to academic literature, barrier options have been studied under two discrete and continuous monitoring assumption. In the first case, the price of underlying asset has been checked at fixed times and monitoring dates, (for instance, weekly or monthly). Some pricing approaches, especially for discrete monitoring case, could be found in Mutenga and Staikouras (2004) and also its references. On the other hand, some other studies Geman and Yor (1996), Hui et al. (2000), Kunitomo and Ikeda (1992), Pelsser (2000) and Sü han et al. (2013) investigated pricing barrier option under continuous monitoring assumption. There are several drastic discrepancies between option prices under these two mentioned assumptions (see Fusai and Recchioni 2008). In the majority of the related studies, the price of underlying assets is modeled as geometric Brownian motion process where the model parameters are assumed constant. The assumption of time-dependent parameters provides a more flexible model to embed the possible events (politically or economically) that may be arisen. In the present paper, we try to price a down-and-out discrete barrier option on an underlying asset (for instance, stock) which is modeled as geometric Brownian motion with time-dependent parameters. In this regard, a set of transformations are employed to correspond time-dependent partial differential equations (PDEs) for option price (see Marianito and Rogemar 2006) with time independent ones. Afterwards, the obtained time independent PDEs are simply converted to familiar heat equations whose answers are written as multiple integral forms. Finally, a new method is proposed to accurately computerize the mentioned multiple integral. This article is organized as follows. In Sect. 2, the model structure for pricing discrete down-and-out barrier options is discussed and a recursive method is obtained. In Sect. 3, a numerical algorithm is proposed to evaluate the multiple integral in Sect. 2. In Sect. 4, a comparison of some available methods is given via some numerical results. And finally, remarks and conclusions are expressed in Sect. 5.",15
48.0,1.0,Computational Economics,15 July 2015,https://link.springer.com/article/10.1007/s10614-015-9503-x,Global Exponential Stability of Cournot Duopolies with Delays,June 2016,Wei Chen,Wentao Wang,,,Unknown,Unknown,Mix,,
48.0,1.0,Computational Economics,19 August 2015,https://link.springer.com/article/10.1007/s10614-015-9514-7,Belief Aggregation with Automated Market Makers,June 2016,Rajiv Sethi,Jennifer Wortman Vaughan,,Male,Female,Unknown,Mix,,
48.0,1.0,Computational Economics,05 June 2015,https://link.springer.com/article/10.1007/s10614-015-9504-9,Notes on a ‘Constructive Proof of the Existence of a Collateral Equilibrium’,June 2016,Venkatachalam Ragupathy,K. Vela Velupillai,,Unknown,Unknown,Unknown,Unknown,,
48.0,1.0,Computational Economics,11 July 2015,https://link.springer.com/article/10.1007/s10614-015-9507-6,A Guide to the StatFact EViews Add-in,June 2016,Charles Rahal,,,Male,Unknown,Unknown,Male,"This work is a response to the increasing acceptance of factor models in time series econometrics as a viable method of dimension reduction in the ‘big data’ paradigm. In particular, a noticeable demand exists in the econometric community for a front-end interface which makes available some of the routines discussed within prominent papers in the factor literature from recent years. This add-in aims to fill that gap by providing options on how to transform data and subsequently extract stationary factors in the style of Stock and Watson (2002). The add-in then calculates six information criteria (PC1, PC2, PC3, IPC1, IPC2, IPC3, AIC3, BIC3) based on the seminal paper of Bai and Ng (2002), which guide the choice of factors typically used in the second stage of empirical applications. Such applications are predicated on the work (in a forecasting context for instance) not only of Stock and Watson (2002), but Bernanke et al. (2005) in a vector autoregressive framework, which is then generalized to an error correction context in Banerjee et al. (2014). Following this, the routine outputs a series of (optional) graphics to allow further analysis of the extracted factors. It is hoped that this add-in will contribute to the field by allowing an easily accessible method of computing the factors, made possible by the increasing availability of ‘macro-panel’ style datasets, especially within a developing economy context.",
48.0,2.0,Computational Economics,05 September 2015,https://link.springer.com/article/10.1007/s10614-015-9520-9,Exploring Price Fluctuations in a Double Auction Market,August 2016,Mingjie Ji,Honggang Li,,Unknown,Unknown,Unknown,Unknown,,
48.0,2.0,Computational Economics,22 September 2015,https://link.springer.com/article/10.1007/s10614-015-9525-4,Multi-dimensional Nondiscretionary Factors in Data Envelopment Analysis: A Slack-Based Measure,August 2016,Alireza Amirteimoori,Mahnaz Maghbouli,Sohrab Kordrostami,Unknown,Female,Male,Mix,,
48.0,2.0,Computational Economics,04 September 2015,https://link.springer.com/article/10.1007/s10614-015-9517-4,A Stochastic Model of Dynamic Consumption and Portfolio Decisions,August 2016,Willi Semmler,Maik Mueller,,Male,Male,Unknown,Male,"Dynamic portfolio decisions are concerned with simultaneous decisions on savings and asset allocation. Research has moved away from the assumption that asset returns are constant over time and it allows now for time varying asset returns. A standard reference of stochastic dynamic portfolio theory is Campbell and Viceira (2002) who present model variants of constant consumtion—wealth ratio and constant risk premia. They study the effect of risk aversion and time horizon on asset allocation. They use an approximate asset allocation equation (Campbell and Viceira 2002, Chap. 2) that depends on a static part, where the asset allocation solely responds inversely to the risk aversion parameter and an intertemporal hedging term, capturing the asset covariance with reductions in expected future interest rates. Their assumption of constant variance and expected premia deliver a constant fraction of assets allocated to risky assets and to the risk free assets. To avoid the complications of a stochastic version, the use of low frequency movements in dynamic portfolio decisions is proposed in Gruene et al. (2007) and Semmler and Hsiao (2011) and dynamic programming (DP) algorithm is used for solving for dynamic consumption and portfolio decisions. In contrast to that approach this paper sets out a basic framework for a stochastic portfolio model that uses DP. Here now asset returns are stochastic as in Campbell and Viceira (2002).Footnote 1
 In the above papers as well as in this paper—and in contrast to CV (2002)—local approximation method to solve the stochastic model are avoided and the use of a global solution procedure such as DP is suggested. Whereas CV (2002), assume a constant consumption—wealth ratio and a constant equity premium, DP can allow both to be time varying. The impact of different variances of equity and bond returns on consumption, asset allocation and the value function can be explored. Alternative scenarios are studied. We allow for a stochastic influence on equity returns, on bond returns and on both. Some results may be of general interest: (1) the mean consumption rate is falling—the mean saving rate is rising—if both equity and bond returns are stochastic, (2) the mean fraction of investment in risky assets falls if both returns are stochastic, (3) the optimal value function becomes less wave-like and flatter when both variances are present, (4) the assets accumulated fluctuate less with both returns being stochastic, (5) the accumulation in risky assets falls with rising variance of the returns on risky assets, and (6) the amplitude of the wave-like behavior of consumption, asset allocation and value function exist but it decreases with higher variance of the equity return. In terms of our method we want to stress that the stochastic dynamic portfolio decision method presented here allows for online decisions as data on asset returns are available in real time. The method is set up in a way such that it helps to make fund decisions online for various types of investment opportunities. The paper is organized as follows. Section 2 presents the stochastic dynamic optimization algorithm. Section 3 introduces the dynamic portfolio model, solves the model and presents the results. Section 4 concludes the paper.",1
48.0,2.0,Computational Economics,09 August 2015,https://link.springer.com/article/10.1007/s10614-015-9509-4,Adapting and Optimizing the Systemic Model of Banking Originated Losses (SYMBOL) Tool to the Multi-core Architecture,August 2016,Ronal Muresano,Andrea Pagano,,Unknown,Female,Unknown,Female,"Nowadays scientific applications are called to provide more and more accurate results which are going to be used in very different fields: engineering, economic and environmental studies, policy making, etc. However, such accuracy requires high computational power to be executed. Additionally, the current trend in computer system is to use multi-core architectures with a large number of cores. This opens new possibilities and, at the same time, new challenges, especially when we wish to fine tune an application to get the most out of the available IT infrastructure. Moreover, multi-core architecture have opened the road to include more parallelism within nodes. Lately, GPUs technology has collected an increase interest as computational efficient processors. The first goal is to correctly identify the parallelizable sections in the code in order to take advantage of this architecture. Whenever an application is developed in serial and we would parallelize it, we have to consider diverse key points such as: core communications, data locality, dependencies, memory size, etc., in order to improve the performance metrics. Then, to obtain the most out of multi-core capacities so to improve the performance metrics of specific applications, it is important to develop a set of best practices in order to manage the inefficiencies generated by the overhead added by the parallel library (Michailidis and Margaritis 2012). This paper describes a set of adaptation techniques and the optimizations processes done on the SYMBOL model original code, aiming to improve its performance in two directions: execution time and scalability. SYMBOL is a statistical tool estimating losses deriving from bank defaults, explicitly linking Basel capital requirements to the other key tools of the banking safety net, i.e. Deposit Guarantee Schemes, and bank Resolution Funds (De Lisa et al. 2011). This tool has been used by Commission Services to prepare various Impact Assessments of European Commission (EC) regulatory proposals to enhance financial stability and prevent future crises (Capital Requirement Directive Proposal, Bank Recovery and Resolution Directive and Financial Transactions Tax). Moreover, SYMBOL is used to analyse the contributions of individual banks to total losses originated in the banking sector. This is an area of particular interest to policy-makers, as information on the factors determining risk contributions could be used in areas such as taxation of financial institutions (i.e. risk levies) and structural reform (e.g. too big-to-fail paradigma). The original version of SYMBOL has been developed in serial, and it presents some computational weakness, leading to a dramatic increase in execution time, when the model is run on a very large input data, lasting from several hours to days depending on the number of default scenarios in the simulations.Footnote 1
 Hence, we have designed a procedure to adapt SYMBOL to a multi-core environment taking advantage of the computational power benefits of this architecture. Then, we are improving the execution time of SYMBOL with two goals: executing with large data sets and scaling the number or default scenarios, by using in an efficient manner the computational resources. The paper is structured as follows: a brief description of the SYMBOL model in Sect. 2. Section 3 describes how to do an efficient execution of SYMBOL on multi-core architecture. Section 4 presents the optimization results for the SYMBOL model. Conclusions are discussed in Sect. 5.",3
48.0,2.0,Computational Economics,25 August 2015,https://link.springer.com/article/10.1007/s10614-015-9519-2,Age-Specific Labour Market Effects of Employment Protection: A Numerical Approach,August 2016,Stefan Boeters,,,Male,Unknown,Unknown,Male,"The effect of employment protection measures on the total number of jobs is ambiguous in general. The positive effect of lower job destruction is counteracted by the negative effect of lower job creation, which leaves the net effect ambiguous. This is the result both of analytical studies of theoretical models (e.g., Bertola 1992) and of empirical studies, whose parameter estimates are often inconclusive (see Addison and Teixeira 2003, for an overview). This ambiguity carries over to the more specific case of age-related labour market effects. Age-related effects enter the picture via two types of research questions. First, we can ask which age group will be affected the most by a uniform employment protection measure. Second, we can ask how an age-specific employment protection measure will affect both the target age group and other groups to which it does not apply. Of particular interest in age-specific labour market analysis are the first years of activity (youth employment) and the years before retirement (old-age employment, pre-retirement). In this paper, I present a numerical model for simulating these age-specific effects based on the theoretical set-up of Chéron et al. (2011; “CHL” in the following). Its working mechanisms are illustrated by simulations of stylised, both general and age-related policy measures. The appropriate conceptual set-up for analysing employment protection measures is using matching models of the labour market with endogenous job creation and job destruction (see the canonical formulation in Mortensen and Pissarides 1994). The set-up must take account of transitions into and out of employment because it is precisely these transitions that employment protection policy facilitates or hinders. In the canonical formulation of the matching model, agents are assumed to live infinitely. As a consequence, this formulation cannot capture age-specific effects. In the past few years, however, several authors have presented models that extend the matching approach by using a lifecycle dimension (Bettendorf and Broer 2005; Saint-Paul 2009; CHL 2011, 2012). Workers can then be followed from their start on the labour market until retirement, with the focus particularly on the phase towards the end of labour market activity. Here, an “end-game effect” (Saint-Paul 2009) can be identified, namely a worsening of the labour market position of older workers that purely results from the shrinking distance to mandatory retirement. In the analysis of employment protection measures, as always in situations with countervailing effects, the usefulness of a purely analytical approach is limited. The overall effect can only be determined if the partial effects are quantified and netted out against one another. In this respect, there is no difference between the model with the lifecycle dimension and the model with infinitely lived agents. CHL (2011) stretch the analytical approach to the limit. They derive the optimal employment protection policy that corrects for externalities that arise in a matching setting where workers differ by age and search is undirected. In the derivation, they exploit the fact that their model contains only a single labour market distortion, which can be corrected by the proposed policy. However, even under these favourable conditions, analytical results can only be derived for the transition rates whereas the effects of the policy measures on employment and unemployment stocks remain ambiguous. At this point, the numerical approach of the present paper is a step forward in the following respects: (1) It shows that age-independent labour market policies can have age-specific effects. (2) It reveals direct and indirect effects of age-specific policies, e.g. the difference between workers in the target group and those below the eligibility threshold. (3) It illustrates the interaction of flow effects (at the hiring and firing margins) with stocks, which produces differences in the effects of hiring subsidies versus firing taxes. At the level of the numerical implementation, one issue needs discussing. The theoretical CHL model is based on a continuous distribution of worker productivity. A necessary step in the calculation of the transition probabilities as well as the state values of employment and unemployment is solving the integral over the densities of these distributions. However, an explicit expression for the integral can be derived only for very simple distribution functions. Numerical integration would be time-consuming and practically infeasible with a large number of age classes. Therefore, in this paper I take the approach of discretising the productivity distribution. This creates a follow-up problem. When analysing small policy shocks (which is particularly relevant for marginal analysis), reactions can become non-smooth once the reservation productivity switches between neighbouring bins of the distribution. In Appendix 2, I present a refined version of the model, which allows for the endogenous adjustment of bin boundaries. This approach eliminates the problems of non-smooth reactions and turns out to be a reasonable compromise between quantitative precision and computational feasibility. The body of the present paper is organised as follows. After a quick informal overview of the model in Sect. 2, its numerical design is laid out in detail in Sect. 3. The working mechanisms are then illustrated with both age-independent (Sect. 4) and age-specific (Sect. 5) employment protection policies. Section 6 discusses a number of possible extensions and Sect. 7 concludes.",
48.0,2.0,Computational Economics,27 August 2015,https://link.springer.com/article/10.1007/s10614-015-9521-8,Is It Possible to Visualise Any Stock Flow Consistent Model as a Directed Acyclic Graph?,August 2016,Peter G. Fennell,David J. P. O’Sullivan,Stephen Kinsella,Male,Male,Male,Male,"An open problem in the emerging stock flow consistent (SFC) macroeconomic literature concerns how best to represent these models graphically and causally. When we speak of causality in this paper, we refer not to phenomenological causality, but to the logical structure of a given SFC model in a given modeller’s mind. The goal of this paper is to solve both the representational problem and the specification of causal structure problem using directed acyclic graphs (DAGs). We prove that for any SFC models there is a corresponding DAG. Using a newly developed software package, we show this correspondence in action. Our approach simplifies the process of making any SFC model, as well as visualising and inferring causality once the model has been built. Every SFC macroeconomic model is built to mimic the flow of funds data for an individual economy (Godley and Lavoie 2007). Sectoral interlinkages are explicitly modelled to ensure consistency of stock and flows, so that every quantity comes from somewhere, and goes somewhere (Caverzasi and Godin 2015). These models often feature precision regarding time, several financial assets and rates of return, budget constraints and adding up constraints (Tobin 1982). At the representational level, stocks and flows are tracked using balance sheet and transaction matrices. Several hundred behavioural and identity equations build and balance the model. These models are unwieldy as a result. Once the model is defined, it can be calibrated or estimatedFootnote 1 in order to determine the parameter values. The model is then numerically simulated and eventually shocked in order to compute out-of sample values for the endogenous variables (Godley 1999).Footnote 2 For models estimated from real-world data, it is generally possible to compute confidence intervals around the predicted values of the model using exogenous innovation terms (Dos Santos and Zezza 2008).Footnote 3 However, even for models developed purely for the purpose of simulation, causal identification can be difficult. An example may aid intuition. Typically SFC models use linear consumption functions to understand the household sectors consumption decisions from current income and past wealth. In this linear consumption function, causality runs from right to left, so, when disposable income increases, consumption increases. But why should this be? At times it must be the case that increases in consumption cause increases in disposable income as people work more to afford to consume. There is no justification within the model for such a causal choice, other than an appeal to convention and the literature. Using DAGs, it is possible to infer the most likely causal structure, especially for behavioural equations which are not as straightforward as consumption functions. DAGs were developed in the late 1990s, and later extended by Pearl (2009), Morgan and Winship (2007), Lauritzen (2001), Moneta et al. (2013) and others to capture the insight that in an interacting system, the expression of one variable can cause an effect in other variables. This effect is generally unknown when the behaviour of the system is observed as a whole. In controlled experimental environments variables can of course be isolated and their causality inferred, but in macroeconomic models such isolation is rarely possible. The aim of most DAG modelling is to reverse engineer causality by inferring variable interactions from observations of the entire system. In economics Bessler et al. (2003) and Hoover (2001) have papers applying the directed graph concept to economic issues, while Moneta et al. (2013) applies component analysis to recover the causal structure of VAR models. In this paper we make three contributions to the literature. First, we show that every SFC model has a corresponding DAG. Second, we demonstrate how to infer causality directly from the graphical representation of the model. Third, we have developed software to simulate any SFC model and show its DAG. The rest of the paper is organized as follows. The equivalence between any SFC model and its DAG is derived in Sect. 2, while an example is given in Sect. 3 of the bank-money world (BMW) model, developed by Godley and Lavoie (2007). Finally, we conclude and give directions for future research.",1
48.0,2.0,Computational Economics,24 September 2015,https://link.springer.com/article/10.1007/s10614-015-9526-3,Lost in Translation: Explicitly Solving Nonlinear Stochastic Optimal Control Problems Using the Median Objective Value,August 2016,Ivan Savin,Dmitri Blueschke,,Male,Male,Unknown,Male,"Over the last few decades, a large number of studies have tried to extend or rather to replace classical research methods, which impose strong restrictions on the model at hand, by using different computer-based simulation techniques. One of the methods which is becoming increasingly popular is heuristic optimization. It imposes few, if any, restrictions on models, at the price of being more computationally demanding [for a concise discussion of the matter see Gilli and Schumann (2014)]. The present study follows this line of research and analyses the use of an evolutionary, heuristic approach in comparison to a more ‘traditional’ algorithm for optimal control of nonlinear stochastic problems. The baseline framework in the field of optimal control problems is the linear-quadratic (LQ) optimization technique, which is included in nearly all existent solution algorithms. There are many extensions of these methods for more sophisticated scenarios like considering nonlinear models and stochastic problems [e.g. Chow (1975, 1981); Kendrick 1981)]. One of the algorithms which deals with these types of problems is OPTCON as described in Matulka and Neck (1992) and Blueschke-Nikolaeva et al. (2012). However, the OPTCON algorithm still relies on the LQ optimization technique and, therefore, has some limitations typical for this framework. One very important limitation which is common when applying the LQ optimization framework to nonlinear problems is the need to linearize the problem. Especially in the case of stochastic problems, this necessity requires some simplification assumptions and causes a loss of information. Furthermore, the non-linear system is usually required to be convex to allow the solution to be obtained by (pure) gradient methods. In contrast, when using heuristic evolutionary optimization methods there is no need to require the convexity of the model at hand, since heuristics are specifically designed to handle badly behaved but realistic models. In a recent paper by Blueschke et al. (2013b) a new way of handling optimal control problems is analyzed. The authors test an evolutionary approach for this purpose, namely differential evolution [DE, Storn and Price (1997)], which does not rely on the LQ framework. The authors apply DE to optimal control problems in nonlinear dynamic economic systems with an asymmetric objective function, where the ‘classical’ OPTCON algorithm does not work. Applying the DE method increases the computational time substantially but allows us to get new insights into optimal control problems. In particular, some better approximations of the targets stated by policy makers are achieved. However, the work by Blueschke et al. (2013b) is designed to deal with deterministic problems only. The present study extends this methodology and analyses the application of DE for stochastic problems. Two alternative ways of dealing with the stochastic nature of the problem are considered. On the one hand, applying DE to stochastic problems allows us to run an ‘extreme event analysis’, where the outcomes of the best and worst scenarios (in terms of the objective function value achieved) can be investigated. On the other hand or rather in addition, DE allows us to minimize the expected objective value using a different selection criterion. When using ‘classical’ optimization algorithms for stochastic problems, policy makers obtain only very limited information as a solution: usually only the value of an optimal strategy (sometimes only a local optimum) and an ex-post objective value. Neither is there any information about the reliability of the stochastic solution reported, nor any discussion of model evolution in certain extreme scenarios. In contrast, the present paper delivers an algorithm that combines two alternative stochastic solutions and broadens the range of decision support information for policy makers when choosing the optimal strategy. Here we use the notation ‘ex-post objective value’ to highlight the following limitation of classical stochastic optimal control algorithms. An ex-post objective value means that the problem is solved as if it were stochastic, which leads to a set of optimal controls \((u_t^{*})\). In order to evaluate this stochastic solution (i.e. to calculate the final objective value), the deterministic model is applied in classical algorithms, which means that a certain set of parameters is used to calculate the respective states \((x_t^{*})\). The final objective value (an ‘ex-post objective value’, as denoted in our paper) is then calculated for this resulting solution \((x_t^{*}, u_t^{*})\). In contrast, using the DE algorithm allows us to calculate both the ‘ex-post objective value’ for the best solution and an expected objective value, which takes the stochastic nature explicitly into account. Summarizing the arguments presented above, one can see that traditional methods perform several ‘translation steps’ from the original nonlinear stochastic problem to a model which can be solved using well known optimization techniques. In contrast, we propose to solve the original problem explicitly by adopting differential evolution. The present study makes the following contributions. First, the possibility of DE to calculate both the ex-post objective value and an expected objective value constitutes an important advantage because, as we demonstrate, the ex-post objective function and the actual expected objective function (represented by the least median, see Sect. 3.2) are not ‘associated’: if one function decreases monotonously on a certain sequence of solutions, the other one will not necessarily do the same on this set of solutions. As a consequence, a solution preferred by the ex-post function does not necessarily have the lowest expected value and the other way around. This inconsistency between the objective functions hinders a comparison of different candidate solutions. Hence, the question is raised as to whether the classical approaches are well specified for stochastic nonlinear problems and whether their solutions are not merely suboptimal ones. Second, as we show below, DE clearly and consistently outperforms OPTCON in minimizing the expected objective function for the two different problems tested (at the same time supporting our criticism with regard to the optimality of ex-post solutions). Third, additionally addressing the ‘extreme events’, DE broadens the range of information available to determine an optimal strategy for the problem at hand. This is briefly illustrated by comparing the differences between states and controls achieved for one of the problems considered. The paper proceeds as follows. In Sect. 2 we define the class of problems to be tackled by the algorithms. Sect. 3.1 briefly reviews the OPTCON algorithm and describes its limitations when Sect. 3.2 introduces DE as an alternative strategy for solving nonlinear stochastic optimal control problems. In Sect. 4 the results obtained for these two approaches based on two different models (MacRae and SLOVNL) are stated. Section 5 concludes.",7
48.0,2.0,Computational Economics,23 October 2015,https://link.springer.com/article/10.1007/s10614-015-9530-7,Bootstrap Inference of Level Relationships in the Presence of Serially Correlated Errors: A Large Scale Simulation Study and an Application in Energy Demand,August 2016,A. Talha Yalta,,,Unknown,Unknown,Unknown,Unknown,,
48.0,2.0,Computational Economics,14 September 2015,https://link.springer.com/article/10.1007/s10614-015-9524-5,ABATE: A New Tool to Produce Marginal Abatement Cost Curves,August 2016,Oswald Marinoni,Martijn van Grieken,,Male,Male,Unknown,Male,"Marginal abatement cost curves (MACC) analyse the pollutant mitigation options available to a region or country (Senatla et al. 2013). By providing a link between the emission reduction potential of abatement options and the costs associated to each option it is possible to provide a measure of cost of eliminating an additional unit of emissions (Morris et al. 2008). MACC are therefore useful to compare the cost-effectiveness of different mitigation, ultimately investment, options and facilitate a prioritisation of these investments. The earliest cost curves were developed after the oil price shocks in the 1970s but were, at the time, not called Marginal abatement cost curves (MACC) curves, but rather saving curves or conservation supply curves (Kesicki 2010). More recently, MACC were brought to attention by Enkvist et al. (2007) who wanted to provide ‘an understanding of the significance and cost of each possible method of reducing (greenhouse gas) emissions and of the relative importance of different regions and sectors. There has been an increasing amount of applications of MACCs ever since for specific regions, countries, industry sectors and/or pollutants. Akimoto et al. (2012) and Wagner et al. (2012) for example analysed MACC of global greenhouse gas mitigation measures for the years 2020 and 2030 by country and sector. Specific sectors for which greenhouse gas mitigation measures were analysed include, to name a few, the Thailand cement industry (Hasanbeigi et al. 2010), the UK transport sector (Kesicki 2012), the UK agricultural sector (MacLeod et al. 2010; Moran et al. 2011) or Australian and New Zealand steel businesses (Freislich et al. 2008). Abatement cost curves for pollutants other than greenhouse gases were, for example, developed for the mitigation of aerosols (Sarofim et al. 2010) and the reduction of phosphorus, nitrogen and suspended solids in a water quality related context (Hall 2012). To produce MACC, Kesicki and Ekins (2012) outline two methods. Method one includes assessments of individual abatement measures with the cost and pollution mitigation potential assessed separately (Fig. 1, left) whereas the alternative approach includes the use of an integrated model which, through many model runs under different boundary conditions, for example different \(\hbox {CO}_{2}\) taxes (provided Carbon is the pollutant), build a MACC (Fig. 1, right). The tool presented in this paper and the subsequent discussion refer to MACC that are produced by approach #1 (Fig. 1, left). Examples of MACCs. Left expert based MACC, based on the assessment of individual abatement measures. Right MACC based upon an integrated energy model (Kesicki and Ekins 2012) MACCs are known to have methodological shortcomings such as intertemporal issues and questions of uncertainty. Nevertheless, MACC have turned into a commonly used policy tool (Kesicki and Strachan 2011). More recently, Taylor (2012) identified a flaw affecting the negative cost reduction measures which affects the ranking of abatement options which would require a range of organisations to revise their abatement curves. This paper however does not aim at providing a discussion of potential methodological or analytical shortcomings of MACC but recognises the increasing popularity of MACC as an instrument to inform policies. Surprisingly though a research revealed that there are not many publicly available tools to produce and visualise MACC. And while reading a MACC is considered straightforward, constructing one is not (van Tilburg et al. 2010). Some examples of MACCs found in the literature The research showed that the most common tool which researchers seem to be using is the use of a spreadsheet solution (Fig. 2). Although there are workarounds to produce column charts with individual box widths (Peltier Technical Services Inc. 2014) it is not straightforward to do so and the construction and subsequent changes to charts is time consuming. With no straightforward option to change the width of individual columns in an Excel column chart, a line series chart is frequently used (e.g. Ackerman and Bueno 2011; Hasanbeigi et al. 2010; Sarofim et al. 2010). One solution to produce column like charts though is the MACC generator (Bull 2012). The MACC generator is a Microsoft Excel based spreadsheet solution and free for non-commercial use. Curves can be accurately built and labelled in a good quality. The problem of having to plot columns in varying widths is solved by computing the corner coordinates of individual columns that represent abatement measures and to draw a line series through these coordinates. This produces a rectangular outline which resembles a column chart however these columns are not closed boxes. Consequently, there is no option within Excel to fill these ‘pseudo’ columns with a colour which could reflect different categories of abatement measures or represent other characteristics such as uncertainty for instance. Another potential solution is the MACTool (ESMAP 2013) which is characterised as a tool to support policy decisions to reduce greenhouse gas emissions and improve energy security. The graphical outputs of the MACTool are also Excel based (see for example Knight 2012). At the time of writing this paper, a fully functional version of the tool was not yet available. Although effort was dedicated to a research of available tools to produce MACC, it is not claimed that the research was exhaustive. Other tools to create MACC might be available but if there are they are either hard to find, not published or inaccessible to the public. The summary of the review is that the production of column based MACCs is not a straightforward task, especially if a greater number of MACCs are to be produced for a variety of spatial entities as in the context of a subsequently briefly outlined research project for which ABATE was developed.",2
48.0,3.0,Computational Economics,28 November 2015,https://link.springer.com/article/10.1007/s10614-015-9535-2,Evolving Fuzzy-GARCH Approach for Financial Volatility Modeling and Forecasting,October 2016,Leandro Maciel,Fernando Gomide,Rosangela Ballini,Male,Male,Female,Mix,,
48.0,3.0,Computational Economics,12 December 2015,https://link.springer.com/article/10.1007/s10614-015-9536-1,An Intelligent Computing Approach to Evaluating the Contribution Rate of Talent on Economic Growth,October 2016,Yong He,Siwei Gao,Nuo Liao,,Unknown,,Mix,,
48.0,3.0,Computational Economics,14 October 2015,https://link.springer.com/article/10.1007/s10614-015-9532-5,Additional Information Increases Uncertainty in the Securities Market: Using both Laboratory and fMRI Experiments,October 2016,Hidetoshi Yamaji,Masatoshi Gotoh,Yoshinori Yamakawa,Male,Male,Male,Male,"Providing investors with a significant amount of securities investment-related information in the form of accounting data-which represent public information by nature-is a basic securities market policy tool employed in developed economies. Its aim is to make securities investment-related data common knowledge among investors in order to improve transparency and fairness in the market. Needless to say, the macroeconomic objective of this measure is to lure investors’ savings into the securities market to finance private sector investment in the form of direct finance. From a microeconomic perspective, it is aimed at supporting utility maximization behavior among investors in the securities market, because investors will use the information they acquire to make appropriate and timely estimates and engage in securities investment to earn a profit, thereby maximizing the utilities derived from current-period consumption and future consumption. The Efficient Market Hypothesis (hereinafter referred to as the “EMH”) is well known as a theory that supports government public policy for the securities market. First formulated by Muth (1961) and Samuelson (1965) during the 1960s, the EMH was applied to some economics topics by Lucas and Sargent (1981) and to finance research in the 1970s by Fama (1970).Footnote 1 Initial research findings showed favorable empirical corroboration of the EMH. However, the late 1970s to the 1980s saw experimental studies being used to verify the EMH (see Plott and Sunder 1982, 1988), and the results of empirical studies questioning the validity of the EMH were published,Footnote 2 leading to widespread debate on the pros and cons of the theory. During the 1990s, more studies questioned the validity of the EMH in both the experimental and empirical fields.Footnote 3 Moreover, the results of psychology investigations negatively affected the process of verifying the EMH (see Hirshleifer 2001). The existence of cognitive bias in securities market participants was pointed out, and findings cast doubt on the EMH. In an experimental paper, it is pointed out that “the market pricing process could be strongly affected by information held by many participants, and such distortion will be even more significant if such information is favorable. However, if the widely shared information is unfavorable, the adjustment speed is not as fast” (see Yamaji and Gotoh 2010).Footnote 4
 Bringing together results on validation of the EMH obtained through studies conducted in the 1980s and the questions raised about the EMH by the results of psychology studies and our experiments, one possible interpretation is that pricing phenomena in the securities market that demonstrate the informational efficiency of the market according to the EMH should be considered specific cases that only appear in markets with a special information holding structure and rationally behaving market participants. In reality, investors are capable of making decisions with a wider range of characteristics, underlining the need to recognize this possibility through experimental and empirical studies. Some recent studies have referred to one such class of investment decision making with a wider range of characteristics as the psychological class. In the field of behavioral economics or behavioral finance, efforts have been made to explain investment activities and market pricing phenomena that appear irrational under the existing economics framework with the assistance of results from psychology research.Footnote 5
 Research in this area has become more complex. Neuro-economics and neuroscience have also affected research on the decision-making processes of human beings. In particular, the technology of functional magnetic resonance imaging (later referred to as fMRI) can gradually clarify which parts of the brain are activated during economic decision making (see Glimcher et al. 2009). This paper builds on past research and, using findings from laboratory and fMRI experiments, points out the possibility that depending on the level of information investors acquire in the securities market, they may not buy and sell securities based on a generally consistent and rational decision-making model, but may modify their decision-making rules in certain circumstances according to psychological factors and trade securities based on such rules.",1
48.0,3.0,Computational Economics,23 September 2015,https://link.springer.com/article/10.1007/s10614-015-9528-1,Economic Growth Prediction Using Optimized Support Vector Machines,October 2016,Elmira Emsia,Cagay Coskuner,,Female,Unknown,Unknown,Female,"To understand the economic situation and as well as economic growth in a contrary, in recent years numerous number of researchers have focused on gross domestic production (GDP) and gross national product (GNP). The GDP itself is not generated and there are other parameters which overshadow on the behavior of that. Some of them are included here. Labor, Gross Capital Formation, Trade, Inflation GDP deflator, Real Effective Exchange Rate index, General government final consumption expenditure, Gross savings. Understanding the effectiveness of these related factors on the GDP has always been controversial question among the researchers. Over the past decades, there have been done many studies dealing with modeling of GDP and also understanding its relevant parameters. To date, most of the developed techniques have been basically linear and consequently they could not be able to capture all the characteristics of the macroeconomic variables such as GDP during modeling process. To cope with this deficiency, artificial intelligence (AI) approaches such as artificial neural networks (ANNs), adaptive neuro-fuzzy inference system (ANFIS), support vector machines (SVMs) and evolutionary algorithms (EAs) have been successfully employed by the researchers, which seem to be more adequate to explain the behavior of the economic time series (Hoptro et al. 1991; Weigend et al. 1991; William et al. 2002). In the present paper, SVR formalism is integrated with genetic algorithm to automatically determine the optimal parameters of SVR with the highest predictive accuracy and generalization ability simultaneously. The strategy (henceforth SVR–GA) uses an SVR as the nonlinear process modeling paradigm, and the GA for optimizing the parameters of the SVR model to improve the accuracy of prediction of SVR predictor. With the best of our knowledge, the hybrid model called SVR-GA is being used for the first time for economic studies in terms of modeling and optimization. Time series of GDP of Turkey",8
48.0,3.0,Computational Economics,10 October 2015,https://link.springer.com/article/10.1007/s10614-015-9533-4,On the Historical Exchange Rates Euro/US Dollar,October 2016,Fernando Vadillo,,,Male,Unknown,Unknown,Male,"With day 0 defined to be 3 January 2000, let \(R_n, n=0,\ldots ,3701\) denote the exchange rate between the Euro (EUR) and the US Dollar (USD), the last day is the 19 September 2014. The values of \(R_n\) are plotted in Fig. 1, this picture seem to suggest that the points lie on a continuous jagged curve, remembering the Brownian motions of the literature Ross (1999), Allen (2007), Klebaner (2005), Øksendal (2010), Higham (2000), Roberts (2009). Historical Exchange Rates EUR/USD This paper seeks to explain this exchange rate dynamic. We do not know many studies on this subject in recent years, perhaps the most interesting and original paper is Federici and Gandolfo (2012), where the authors reject the possibility of chaotic dynamics, while for example Rabanal and Tuesta (2010) tries to adapt the exchange rate between two countries using a newer generation of models known as the new open economy macroeconomics (NOEM). This paper is organized as two parts: the first in Sect. 2 describes a brief review of the GBM, and we study if the sequence \(R_n\) follow this type of model testing the contingency table for the successive differences of the logarithms \(D_n= \log \left[ R_n /R_{n-1} \right] \); because our results are consistent with this assumption, we computer the historial volatility. In second part in Sect. 3 we present the discrete Fourier transform and computer the periodigram for the sequence \(R_n\) in order detecting possible periods; the bad results suggest use wavelet analysis in Sect. 3.2, despite the wavelets still remain largely unfamiliar to students of economics and finance, in the past decade considerable progress has been made in finance (see for example Gallegati and Semmler 2014), in particular the techniques used is that used in Gallegati et al. (2014) to study the effect of increased productivity on unemployment for US or more recently Shahbaza et al. (2015) to analyze the time-frequency relationship between oil price and exchange rate in Pakistan. The final section summarizes the results that we consider most important. Our numerical methods were implemented in Matlab© (see for example Higham and Higham 2000; Moler 2004 for a simple introduction); the experiments were carried out in an Intel(R) Core(TM)2 Duo CPU E6850 @ 3.00GHz. The codes for the numerical tests and for this example are available on request.",1
48.0,3.0,Computational Economics,25 August 2015,https://link.springer.com/article/10.1007/s10614-015-9522-7,On the Choice of a Genetic Algorithm for Estimating GARCH Models,October 2016,Manuel Rizzo,Francesco Battaglia,,Male,Male,Unknown,Male,"The class of generalized autoregressive conditional heteroscedastic (GARCH) models, introduced by Bollerslev (1986), has received great attention in the literature devoted to the analysis of financial time series, because of its importance in reproducing the so-called volatility clustering, along with ARCH models (Engle 1982), of which the GARCH models represent, indeed, a generalization. Building models (precisely estimating their parameters) with this features is not trivial: in fact, since the earliest related works, the inference from GARCH models has always been based on the Maximum Likelihood Estimation principle, that, under some Gaussian assumptions, leads to the Quasi Maximum Likelihood Estimator (QMLE), which is the most used in this field. The QML function, even for the simplest form of the model, the GARCH(1,1), has been found numerically difficult to optimize using classical methods, like Newton’s for example, because of drawbacks like multimodality (see Zumbach 2000 for an account of these problems), so different kind of optimization methods have been proposed. The growing importance in literature of metaheuristics and evolutionary procedures has encouraged many researchers to try using these methods in statistical applications (for some comprehensive accounts see Baragona et al. 2011 and Winker and Gilli 2004). Several metaheuristics and evolutionary algorithms have already been proposed in literature for the estimating problem in exam. As observed by Winker and Maringer (2009) the theoretical ML estimator for the GARCH model cannot be observed in practice. Deterministic algorithms approximations very often provide high quality solutions, but they fail to do so from time to time due to the inherent complexity of the estimation problem. This is true even for the simplest form of the model, the GARCH(1,1). For such a reason stochastic algorithms may be more efficient. Winker and Maringer proposed the Threshold Accepting, but many more options can be found in literature. Adanu (2006) showed that the performance of the genetic algorithm (GA) and the Differential Evolution in optimizing the GARCH model, compared with two local search methods, is competitive, especially when the problem complexity is high. Wang and Li (2001) observed that the GA based GARCH optimization outperforms the conventional numerical methods on the aspect of computational robustness and accuracy. Furthermore, the GAs have been successfully employed in several applications where modifications of the basic GARCH model are involved, for example Fuzzy-GARCH models (Hung 2009) or Grey GARCH-Type models (Geng and Zhang 2015). Santamaría-Bonfil et al. (2015), proposed a Support Vector Machine model and used a hybrid genetic algorithm for estimating the parameters. In this paper we examine the behaviour of GAs when used to estimate the parameters of a GARCH(1,1) when a fixed computational time, measured in number of fitness function evaluations, is allocated in number of generations G, number of algorithm restarts R and number of chromosomes in the population N, in a range of different ways. Thus we can study and analyze the effects of each of these parameters on the estimates.",2
48.0,3.0,Computational Economics,15 August 2015,https://link.springer.com/article/10.1007/s10614-015-9510-y,On Modelling and Forecasting Predictable Components in European Stock Markets,October 2016,Khurshid M. Kiani,,,Male,Unknown,Unknown,Male,"A wide body of empirical research appears to be devoted towards predictability in stock returns (Bekaert 1995; Harvey 1995; Haque et al. 2001, 2004; Claessens et al. 1995; and Buckberg 1995). This is mainly because of large economic gains that could be realized due to suitable trading strategies (Xu 2004). While empirical literature verifies that stock returns predictability does existFootnote 1 the knowledge concerning predictability of stock returns depends on the development of new econometric methodsFootnote 2 that allow us to accurately assess this evidence. The present contribution employs non-Gaussian state space models for modeling and detecting possible existence of predictable components in stock excess returns in all the series employed. These models incorporate the features that have been documented in the empirical literature, however, because of the complexity and the techniques required to estimate these models, empirical researchers abstain using them. The assumption that stock returns are normally distributed is widely used in theoretical finance; however, empirical evidence that stock returns are normally distributed is questionable due to studies by Mandelbrot (1963), Fama (1965), and Clark (1973) wherein Paretian stable distributions were considered better than the normal distributions because of existence of non-normality in stock returns. Later, researchers including Akgiray and Booth (1988), Jensen and de Vries (1991), De Vries (1991), Buckle (1995), Mantegna and Stanley (1995), and McCulloch (1997) also found evidence of non-normality in stock returns. Similarly, a number of researchers explored time varying volatility that is the other aspect of empirical distribution of stock excess returns and concluded that volatility persistence does exist in stock returns (Akgiray 1989; Nelson 1991; Pagan and Schwert 1990; and Goose and Kroner 1995). Watson (1986), Conrad and Kaul (1988), and Harvey (1989) including others employed state space models with the assumptions that the underlying errors were independently and identically distributed (i.i.d.) normal and that stock returns were evolving from first order autoregressive process. However, McCulloch (1996) as well as Bidarkota and McCulloch (2004) concluded that stock returns are typically non-Gaussian that encompasses fat tails. Exclusion of such features from a forecasting models that are employed for forecasting predictable components in stock returns may result in inefficient estimation. A number of researcher have studied stock return predictability,Footnote 3 however, studies pertaining to possible existence of predictable component in European Stock Excess return series using non-Gaussian space state models that account for non-normality and GARCH-like effects are sparse. Therefore, in this study, I investigate possible existence of persistent predictable signals (if any) in monthly stock price indexes over their respective risk free rates from eighteen European economies.Footnote 4
 It would be of interest to study possible existence of predictable component in European stock excess returns for a number of reasons. For instance, the data comes from a multitude of European stock markets which is often divided into regions based on geographical, cultural or historical criteria although for the purpose of this study, I follow directional distribution of Europe that divides it into Central Europe, Eastern Europe, Northern Europe, Southern Europe, and Western Europe. Therefore, data on stock price indexes for counties in these European regions are expected to have variations in the results, which might have been due to the underlying country-specific institutional factors governing these countries’ stock markets. In addition, the time series models used in the present study encompass features to account for non-normality and conditional heteroskedasticity as was pointed out by various stock price empirics. Finally, no one in my knowledge has yet explored possible existence of predictable components in excess return series using forecasting models as well as the time series data on excess stock returns from a variety of European countries like I did. Therefore, present research seeks to forecast possible existence of predictable components in all the series using non-Gaussian state space models that encompass all the relevant features which is a contribution to the literature. As envisaged from the study results, even in the presence of statistically significant evidence of non-normality and volatility persistence in excess returns series for the most countries studied, statistically significant persistent predictable component does exist in Austria, France, Germany, Iceland, Ireland, Italy, Netherlands, Norway, Spain, Sweden, and Switzerland stock excess returns. However, results show that statistically significant persistent predictable component does not prevail in one Northern European country (Denmark), one Southern European country (Greece), one Western European country (United Kingdom), one Northern European country (Denmark), one Southern European country (Greece), two Central European countries (Croatia, and Hungary), and two Eastern European countries (Russia and Slovakia). In order to account for non-Gaussian data, the return series are modeled within the framework of Paretian stable distributions that were also employed by Mantegna and Stanley (1995), Buckle (1995), and McCulloch (1997). Therefore, as in Oh (1994) and Bidarkota and McCulloch (1998), the normality assumption is relaxed in favor of stable distributions, therefore, the Kalman filter is not operable efficiently with stable distributions where innovations are assumed to be non-normal. Likewise, the present study explicitly accounts for volatility persistence in all the return series using GARCH-like process. The remaining study is organized as follows. Section 2 outlines the most general state space or unobserved component model employed. In Sect. 3 data sources, empirical results, and hypotheses tests are discussed, and finally Sect. 4 includes conclusion.",1
48.0,3.0,Computational Economics,09 August 2015,https://link.springer.com/article/10.1007/s10614-015-9511-x,Economic Study of Problems of Depletion of Several Interrelated Non-renewable Resources,October 2016,R. García-Rubio,L. Bayón,C. Tasis,Unknown,Unknown,Unknown,Unknown,,
48.0,3.0,Computational Economics,21 August 2015,https://link.springer.com/article/10.1007/s10614-015-9515-6,Trading Structures for Regional Economies in CAS Software,October 2016,George E. Halkos,Kyriaki D. Tsilika,,Male,Female,Unknown,Mix,,
48.0,3.0,Computational Economics,22 August 2015,https://link.springer.com/article/10.1007/s10614-015-9516-5,"A Note on Julia and MPI, with Code Examples",October 2016,Michael Creel,,,Male,Unknown,Unknown,Male,,1
48.0,3.0,Computational Economics,20 August 2015,https://link.springer.com/article/10.1007/s10614-015-9513-8,Version Control Systems to Facilitate Research Collaboration in Economics,October 2016,Rodrigues Bruno,,,Unknown,Unknown,Unknown,Unknown,,
48.0,4.0,Computational Economics,30 November 2015,https://link.springer.com/article/10.1007/s10614-015-9546-z,Fractional Order Financial Models for Awareness and Trial Advertising Decisions,December 2016,Benito Chen-Charpentier,Gilberto González-Parra,Abraham J. Arenas,Male,Male,Male,Male,"Advertising is a common communication form that can be used to induce consumers to take some actions with respect to products or services. There are many ways to attempt to change consumer behavior such as advertising messages using mass media such as newspapers, magazines, television and radio, or other media such as outdoor advertising, websites, text messages, etc (Huang et al. 2012). Advertising tries to convince consumers to buy a certain product by first creating a “need” for the product in general and then by creating a differentiation among products to get consumers to buy a particular brand. The study of advertising strategies is important in order to boost the sales and improve the firm’s profit. Thus, it is important to construct an appropriate dynamic advertising model to characterize the time-dependent sales which depend on consumer population. There are many proposed models for the advertising issue that tackle this problem from the economic, marketing, and operations management point’s of view (Muller 1983; Huang et al. 2012; Kamrad et al. 2005; Wang et al. 2013; Ip et al. 2012). Dynamical equation models have appeared in the past five decades that analyze advertising policies over time (Muller 1983; Huang et al. 2012). In these dynamical models, the critical state variables, such as sales, market share, subpopulations, brand goodwill, etc., are assumed to change continuously with respect to time, as described by differential equations. The scenario or assumptions of each advertising model are quite different. For instance, some are associated with the advertising competition between two, or among three or more brands. Another scenario is to consider advertising policies for the introduction to the market of a new product. Most economics models depicting advertising in a dynamic context regard advertising as a single variable which directly or indirectly affects the revenues of the firm (Muller 1983; Huang et al. 2012; Kamrad et al. 2005). Marketers, however, do not regard advertising as a single variable with a single objective. The objectives are typically stated in terms of a communications effect. The objectives will be stated in terms of increasing awareness, changing attitude, changing predisposition to buy, or some combination of the three (Muller 1983). Generally, the differential form models consider that the change of a state variable depends instantaneously on the values of the states variables. However, in many cases the changes do not depend on the actual values of the state variables exclusively. Instead they usually depend on the value of the states variables at different previous times. There is a cumulative effect on the results of the advertising. Also the effect of advertising is not instantaneous. There usually is a delay between the time of the advertisement and the time it takes effect. Thus there is a need to incorporate memory, that is the dependence on previous times, in advertising models. Fractional derivatives, since they are define in terms of an integral over all the history of the project (see Eqs. 1 and 2). Therefore in this paper we present a advertising fractional order model that includes the effect of previous values of the state variables. This model is studied under different scenarios in order to understand in a better way the effect of two different type of advertising on the population dynamics. This model is based on a diffusion process and the advertising is divided on increasing awareness and changing predisposition to buy. For simplicity these types will be denoted throughout the paper as awareness and trial advertising (Muller 1983). Lately, fractional order models have been presented in several areas of science such as engineering, applied mathematics, economics, and bioengineering and consequently considerable attention has been given to the solutions of fractional differential equations (Diethelm 2012; Skovranek et al. 2012; Danca et al. 2013). For instance, in (Hu and Chen 2013) authors developed a nonlinear economic system with fractional derivative that considers the gross domestic production, inflation, and unemployment rate. They apply the model to concrete macroeconomic data of USA. For certain applications the use of fractional derivatives is justified since they provide a better model than integer order derivative models do since they provide a powerful instrument for incorporation of memory and hereditary properties of the systems as opposed to the integer order models, where such effects are neglected or difficult to incorporate. As mentioned above, the memory effect is due to the fact that fractional derivatives are non-local as opposed to the local behavior of integer derivatives. The next state of a fractional system depends not only upon its current state but also upon all of its historical states. In addition, fractional order models have one more degree of freedom than the counterpart integer order model. For some applications involving epidemic models, for example (Diethelm 2012), fractional order models have been shown to produce results that fit real data better than integer order derivative models. Thus, under the assumption that the advertising process has a memory effect on the consumers we propose an advertising fractional order model to explain and understand the effect of two components of the advertising: awareness and trial advertising. In order to deal with fractional derivatives we rely on the Caputo operator and on an accurate predictor–corrector to approximate numerically the fractional derivatives (Diethelm et al. 2002). However, it is important to remark that other numerical approaches to different types of fractional diffusion models appears in the literature (Deng 2007; Rida and Arafa 2011; Liu 2012; Al-Rabtah et al. 2012; Scherer et al. 2011; Erturk et al. 2012). This paper is organized as follows. In Sect. 2 we present the basic definitions of fractional derivative. Sect. 3 is devoted to the presentation of the financial advertising model including the fractional-order one. In Sect. 4, numerical simulations of the advertising fractional order model under different scenarios are presented in order to better understand the effect of different type of advertising on the population dynamics. The last Section is devoted to the conclusions.",7
48.0,4.0,Computational Economics,01 December 2015,https://link.springer.com/article/10.1007/s10614-015-9537-0,Solving the Incomplete Markets Model in Parallel Using GPU Computing and the Krusell–Smith Algorithm,December 2016,Michael C. Hatcher,Eric M. Scheffel,,Male,Male,Unknown,Male,"Models with incomplete markets and heterogeneous agents are used widely in macroeconomics. It is therefore important that researchers be able to solve these models quickly. This is a non-trivial problem since the set of state variables includes the cross-sectional distribution of wealth—an infinite-dimensional object. Krusell and Smith (1998) show that this problem can be circumvented by approximating the cross-sectional distribution of wealth with a small number of moments. This reduces the dimension of the state vector dramatically, making numerical simulations of incomplete market models tractable. Nevertheless, the Krusell–Smith algorithm is quite time-consuming, especially if the optimal decisions of a large number of agents are computed sequentially or using only a few processing cores in parallel. In this paper, we demonstrate the potential of graphics processing units (GPUs) in solving incomplete market models with heterogeneous agents and aggregate uncertainty using the Krusell–Smith algorithm. We rely on the compute unified device architecture (CUDA) of NVIDIA and show that using the GPU delivers a speed gain over the central processing unit (CPU) which rises sharply as the number of agents is increased. In particular, we document speed gains in the panel simulation stage of the Krusell–Smith algorithm of between 40 and 4000 times as the number of agents is increased from a relatively small number such as 10,000 to very large, but plausible, numbers such as 10 million agents.Footnote 1
 As discussed by Aldrich (2014), GPUs are relatively inexpensive pieces of hardware comprised of large numbers of individual processing cores capable of parallelization. This makes them ideal for computational work that has a high arithmetic intensity, that is, work which requires large numbers of computational operations which are almost identical and can be computed independently of one another. The Krusell–Smith algorithm fits this description because the optimal capital choice must be computed for each agent conditional on the mean of the wealth distribution, current capital holdings, the aggregate state of the economy, and idiosyncratic shocks. As this algorithm has been used widely in the heterogeneous-agent literature, our findings should be of use to other researchers. Our paper also contributes to a recent strand of literature that documents the potential of GPU computing in solving dynamic general equilibrium models in economics. The seminal paper in that literature is Aldrich et al. (2011). They show that improvements in speed of up to 200 times are possible when solving a simple real business cycle model using value function iteration and CUDA architecture. Subsequently, Morozov and Mathur (2012) tackled a more complicated optimal control problem using CUDA. They consider imperfect information dynamic programming with a learning versus experimentation trade-off, so that the value function need not be convex and the policy function need not be continuous. For this problem, speed gains are 15–26 times are reported. Since GPU hardware has developed rapidly over the past few years, even larger gains should be well within reach. A detailed survey of the current state of GPU computing in economics can be found in Aldrich (2014). He simulates an exchange economy with complete markets and agents with heterogeneous beliefs and documents speed gains of more than 1000 times. Our paper goes beyond this because we consider an economy with incomplete markets and idiosyncratic shocks. In addition, our analysis differs from the early GPU literature on optimal control because we apply CUDA to the panel simulation stage of the Krusell–Smith algorithm.Footnote 2 Consequently, we isolate the gains from CUDA in a context in which it has not previously been applied. We also investigate the computation time versus accuracy trade-off that arises with GPUs due to the choice between single- and double-precision arithmetic. Here, we find that single-precision arithmetic is roughly twice as fast as double-precision arithmetic but produces similar numerical results. Given that substantial speed gains are available in both cases, these results demonstrate the potential of GPU computing to make the trade-off between speed and accuracy somewhat less severe. Table 3 reports the performance of GPU and CPU used in this study. The gains in computation time that we document are important for three reasons. First, solving even simple incomplete market models is quite time-consuming. Since this is likely to be a barrier to researchers entering the literature, it is important to lower solution times where possible. Second, non-trivial reductions in computation time would make it feasible to simulate richer economic models that include a larger state space, a task which might otherwise be considered prohibitively time-consuming by many researchers. Finally, improvements in computation speed would enable researchers to focus more effort on improving accuracy safe in the knowledge that total computation time could be kept relatively low. As pointed out by Den Haan (2010, p. 5), it is desirable to improve the accuracy of current algorithms for solving heterogeneous agent models by at least an order of magnitude. Our paper is related to several computational papers in the heterogeneous-agent literature. Most directly, we solve the model in Krusell and Smith (1998) using their simulation-based methodology. They consider an incomplete markets version of the neo-classical growth model with a lower bound on capital holdings, aggregate productivity shocks, and idiosyncratic employment shocks. Their algorithm has the advantage that it is simple and relatively easy to program. In recent years, several alternative solution algorithms have been developed. These solution methods and their performance are documented in a 2010 special issue of the Journal of Economic Dynamics and Control. In that project entitled ‘Computational Suite of Models with Heterogeneous Agents’ a version of the Krusell–Smith model with unemployment benefits and 10,000 agents was solved using several different algorithms. As discussed by Den Haan (2010), the Krusell–Smith algorithm does relatively well in terms of accuracy, but it is not one of the fastest solution methods. For example, the backward induction algorithm of Reiter (2010) solves the model in less than 1 h, while the explicit aggregation Algorithm in Den Haan and Rendahl (2010) takes less than 10 min.Footnote 3 By comparison, the Krusell–Smith algorithm takes over 5 h. It would therefore be desirable to speed up this method. We show how this can be done using CUDA, and we document very large speed gains as the cross-section of agents is increased to large but plausible numbers as 10 million. Our work is also closely related to the Maliar et al. (2010) paper in the Computational Suite Project. They solve the model using the original Krusell–Smith algorithm by employing a consumption Euler equation method that iterates on a grid of pre-specified points. In a second stage they compute the aggregate law of motion as in Krusell and Smith (1998), that is, by simulating a panel for capital holdings and running regressions on the simulated data. Here, we follow the same approach, except that we use CUDA architecture to speed up the panel simulation stage.Footnote 4
 The paper proceeds as follows. Sect. 2 briefly sets out the model, introduces parallel computation on GPUs and describes our approach of exploiting this hardware in the context of the Krusell–Smith model. Section 3 reports a time comparison between GPU and CPU and discusses solution accuracy. Finally, Sect. 4 concludes.",1
48.0,4.0,Computational Economics,30 November 2015,https://link.springer.com/article/10.1007/s10614-015-9538-z,Simulation Studies Comparing Dagum and Singh–Maddala Income Distributions,December 2016,Kazuhiko Kakamu,,,Male,Unknown,Unknown,Male,"
Kleiber (1996) shows that Dagum and Singh–Maddala income distributions are closely related, although he states that the Dagum distribution should provide a better fit than the Singh–Maddala distribution. Moreover, Tadikamalla (1980) shows that the shape of the Dagum distribution is considerably more flexible than that of the Singh–Maddala distribution. Therefore, the Dagum distribution more often is assumed as the income distribution in empirical analyses. Accordingly, Kleiber (2008) reviews studies that examine income distributions of the Dagum type. In empirical analyses, grouped data have been utilized widely and many works have examined several distributions from grouped data (e.g., McDonald and Ransom 1979a). Although McDonald and Mantrala (1995) show that the estimates are different if the number of groups is different, even if the same data source is used,Footnote 1 the properties of the estimators have been examined rarely. As far as we know, the properties of the estimators have been examined only by McDonald and Ransom (1979b), who assume a gamma distribution. However, their focus is on comparing several estimation methods. In addition, the properties of the model selection criteria have not been examined, in spite of the fact that several model selection criteria, including the akaike information criterion (AIC), have been utilized widely in empirical analyses. Therefore, we consider that the properties of the model selection criteria, which take into account the effect of group numbers, become important when we use these distributions in empirical analyses. Moreover, the parameters are used not only for their own inferences, but also for the inference of the Gini coefficients (e.g., Atoda et al. 1988; Chotikapanich and Griffiths 2000; Nishino and Kakamu 2011). However, it is well known that the Gini coefficients are highly nonlinear functions of the parameters. As shown by Gelfand et al. (1990), the MCMC method can yield valid inferences on nonlinear functions of the parameters, such as the Gini coefficients. Therefore, it would be worthwhile to examine the properties of the Gini coefficients in a Bayesian framework.Footnote 2
 Recently, researchers of inequality have become concerned with not only the Gini coefficient but also the top income share. For example, Atkinson et al. (2011) examine the history of the top income shares and Brzezinski (2013) examines the properties of the top income share. While the Gini coefficient measures inequality among whole households or individuals, top income share is constructed as ratios (portions) of upper tails to total income of households or individuals. Therefore, top income share might be sensitive to the choice of the underlying hypothetical distributions. Thus, we analyze top income share as an alternative inequality measure to the Gini coefficient. This study has two objectives. One is to explore the reason why Dagum distribution is preferred to Singh–Maddala distribution in terms of AICs through Monte Carlo experiments, in which the data-generating processes (DGPs) are the generalized beta distribution of the second kind (GB2 distribution). This involves considering the Dagum and Singh–Maddala distributions as special cases. The other objective is to examine the properties of the Gini coefficients and top income shares for these two distributions by means of root mean square errors (RMSEs). From the experiments, we confirm that the fit of the distributions depends on the relationships and magnitudes of the parameters and that the RMSEs of the Gini coefficients and top income shares also depend on the relationships of the parameters. The rest of this paper is organized as follows. In Sect. 2, we briefly discuss the relationship between the Dagum and Singh–Maddala distributions and set forth the framework for Bayesian inference. Section 3 presents the results obtained from the Monte Carlo simulations. Finally, brief conclusions and remaining issues are presented in Sect. 4.",9
48.0,4.0,Computational Economics,27 November 2015,https://link.springer.com/article/10.1007/s10614-015-9539-y,Volatility Analysis of Financial Agent-Based Market Dynamics from Stochastic Contact System,December 2016,Di Xiao,Jun Wang,Hongli Niu,Female,,Unknown,Mix,,
48.0,4.0,Computational Economics,12 January 2016,https://link.springer.com/article/10.1007/s10614-015-9552-1,The Portfolio Heuristic Optimisation System (PHOS),December 2016,N. Loukeris,I. Eleftheriadis,E. Livanis,Unknown,Unknown,Unknown,Unknown,,
48.0,4.0,Computational Economics,16 October 2015,https://link.springer.com/article/10.1007/s10614-015-9529-0,Hybrid Perturbation-Projection Method for Solving DSGE Asset Pricing Models,December 2016,Yuanyuan Chen,Stuart Fowler,,Unknown,Male,Unknown,Male,"The use of dynamic stochastic general equilibrium (DSGE) asset pricing models has gained increasing popularity among financial economists because they describe agents’ optimizing behavior (e.g., Chen 2013; Guvenen 2009; Danthine and Donaldson 2002; Boldrin et al. 2001). However, the first-order perturbation solution method typically found in the DSGE literature have long been recognized to be inappropriate in asset pricing models where uncertainty is a key determinate for difference in asset returns (Mehra and Prescott 1985). Essentially, the certainty equivalence implied by first-order perturbation washes out differences in asset returns from risk. Additionally, recent asset pricing models involve extreme non-linear parts (e.g., Chen 2013; Guvenen 2009; Danthine and Donaldson 2002; Boldrin et al. 2001; Fernández-Villaverde et al. 2010; Caldara et al. 2012; Ruge-Murcia 2012) that are not easily solvable by log-linearization. Studies of DSGE asset pricing models have relied on value function iteration (VFI) methods defined over discrete grids (e.g., Guvenen 2009; Danthine and Donaldson 2002; Caldara et al. 2012). Unfortunately, this method is inefficient in a large state-space; the curse of dimensionality means that the time it takes to find a solution on a grid increases exponentially with the state space. In this paper, we explore the applicability of a hybrid perturbation-projection method (HPP) in solving of recent complex DSGE asset pricing models. The HPP method, first introduced by Judd (1996) for regular DSGE models, combines an imprecise N ’th-order perturbation solution and a projection algorithm (via change of variables) to improve the solution’s accuracy. This method, as demonstrated in Judd (1996, 2002) and Fernández-Villaverde and Rubio-Ramirez (2006), has been shown to greatly improve the precision of simple DSGE models. This paper researches HPP’s solution properties when a specific kind of complexity is introduced into a DSGE model. More specifically, we consider a DSGE asset pricing model. This model has standard non-linearities in both household preferences and firm production technologies. Additionally, the asset pricing markets introduces their own type of non-linearities; leverage can force a nonlinear wedge between asset rates. The asset pricing model is in the same spirit as the DSGE asset pricing model of Chen (2013) albeit without loss aversion/narrow framing (LANF) preferences. With these extreme non-linearities, it may be the case that HPP fails to correctly and efficiently describe the economies’ equilibrium. Therefore, our research is to evaluate effectiveness of solving the same DSGE asset pricing model with identical calibrations between via the old VFI method supported by many economists and via our new proposed HPP method. To conduct this experiment, we solve the DSGE asset pricing model using both HPP and VFI methods. we then compare the methods along two dimensions. In the first dimension, we evaluate how accurate the solution methods are in describing the equilibrium allocations and prices. This is achieved by computing the implied distribution of the euler equation errors (EERs) along the simulated path. Comparing the EERs is natural since they are on average zeros in equilibrium. In addition, the errors are unit-free, which facilitates comparisons across different solution methods. The second dimension focuses on the speed (as opposed to accuracy) of the competing solution methods. Theoretically, the HPP method should be speedier since the solution method does not involve computation of the equilibrium in a large number of points. Though computational speed is not unit-free (it is in seconds), it is a standard method for evaluation algorithm efficiency in computer science. Applying this HPP method to DSGE asset pricing models, if successful, opens new fansion of analyses for both macroeconomists and financial economists. In terms of results, we find that VFI is superior in accuracy to HPP only around the steady state. However, as the one moves to the tails of the state space, HPP is more accurate than VFI. In other words, the HPP method is more stable (lower error variance). Additionally, the VFI method is computationally slow; it takes about 520 times longer on an Intel Celeron(R) 2.53 GHz Microsoft Windows XP system. By reducing the size of the grid, the VFI method can approach an equal speed of the HPP method. Unfortunately, the accuracy of VFI method in this environment deteriorates and therefore it is no longer superior to HPP at any state. To generalize our results, we include another DSGE model by Caldara et al. (2012) that compares HPP not just with VFI, but also with third-order perturbation and chebyshev polynomial. Their results are similar to ours: HPP is as accuracte as other three methods; VFI is much slower than HPP. In all, the results of my study show that HPP method is suitable for both DSGE and DSGE asset pricing models since it is: (i) as accuracy as VFI, and (ii) the computation time is relatively small. The remaining part of the paper is organized as follows. Section 2 details the (HPP). Section 3 describes the DSGE asset pricing model with two different cases. Section 4 compares the results from the (HPP) and from discrete grid search and VFI for our base case and another DSGE asset pricing model. Section 5 concludes the paper.",
48.0,4.0,Computational Economics,28 November 2015,https://link.springer.com/article/10.1007/s10614-015-9543-2,Econometric Filters,December 2016,D. S. G. Pollock,,,Unknown,Unknown,Unknown,Unknown,,
48.0,4.0,Computational Economics,30 November 2015,https://link.springer.com/article/10.1007/s10614-015-9541-4,A Comparative Study of the Performance of Estimating Long-Memory Parameter Using Wavelet-Based Entropies,December 2016,Heni Boubaker,,,Unknown,Unknown,Unknown,Unknown,,
49.0,1.0,Computational Economics,30 November 2015,https://link.springer.com/article/10.1007/s10614-015-9544-1,A Toolkit for Value Function Iteration,January 2017,Robert Kirkby,,,Male,Unknown,Unknown,Male,"This article introduces a ‘Value Function Iteration (VFI) Toolkit’. The toolkit makes it easy for users to solve infinite horizon value function problems; loosely, inputting the return function, the toolkit solves for and outputs the value function and optimal policy function. Many further commands for creating time series simulations, agents’ stationary distributions, and moments of those distributions are also included. The toolkit is implemented in MatlabFootnote 1 and automatically switches between using the GPU and parallel CPUs based on which is typically faster in practice. Allowing the user access to the power of parallelization, especially on the GPU, without the user having to understand the programming concepts of parallelization is one of the main contributions of the toolkit. Those who would like to get straight into trying out the toolkit and learn about it that way are directed to vfitoolkit.com where explanatory material and basic examples can be found, and the toolkit downloaded. This article focuses instead on describing the main uses envisaged for the VFI Toolkit and on explaining some of the design philosophy and major decisions involved in how the toolkit works as well as explanation of one of the main commands, namely that for infinite horizon value function iteration. The Toolkit algorithms are based on full-discretization of the state-space. This choice reflects that full-discretization is robust to the many difficult cases—non-concave return functions, non-convex choice sets, non-differentiable value functions—that often lie behind the decision to use value function iteration methods.Footnote 2 Due to its robustness full-discretization also provides a good benchmark against which other algorithms can be compared (Aruoba et al. 2006) and has the advantage of being easily implemented for large state-spaces and parallelized, including on the graphics processor. The weaknesses of full-discretization are that it tends to be less accurate and slower than more sophisticated but less robust algorithms; although GPU parallelization substantially ameliorates the speed disadvantage. Four main uses for the VFI Toolkit are envisaged: Teaching, Testing Algorithms, Replication, and Research. These are now addressed in turn. For Teaching, the VFI Toolkit allows students after writing code to solve a basic value function problem, to easily look at questions like how do the Value function, optimal policy function, and model statistics (such as the Standard Business Cycle Statistics) respond to changes in the model. The VFI Toolkit allows the user both to evaluate the kinds changes that are typically easy to implement regardless of the algorithm used—such as changing parameter values, or changing the utility function—as well as difficult to implement with some algorithms—such as whether tax revenues are rebated lump-sum or introduced as a government spending term in the utility function. Testing algorithms is likely to be the main appeal of the VFI Toolkit for more advanced users. Say you have implemented an algorithm using a two-dimensional endogenous grid method, with triangular interpolation to return to grid points, or are using shape-preserving splines to fit the value function. Obviously your code will do better on the speed-accuracy trade-off.Footnote 3 The VFI Toolkit may still be useful as a quick double-check that your codes do not contain any substantial coding errors (we all make them!). While the VFI Toolkit will have a longer run-time than your advanced hand-crafted codes it will solve your problem (thanks to its robustness) and is easy to implement. Thus, with just a little of your own time and a bit of run-time the toolkit provides an easy way to double-check your own codes for errors. 
Bona and Santos (1997) argue for a conception of numerical simulation of models as laboratory experiments. A natural implication of this is that like laboratory experiments they should be subject to replication. While the importance of replication has been widely accepted in other quantitative areas of Economics such as applied Econometrics (Coffman and Niederle 2015), laboratory experiments, and field experiments it is not so well recognised in quantitative computational Economics. Replication is a use to which the VFI Toolkit is especially suited as it is comparatively easy to implement almost any kind of Macroeconomic model. Since the VFI Toolkit uses global solution methods and is built around robust algorithms it can handle a very wide range of problems. Another advantage of the toolkit in replication is that the main commands will be known not to contain bugs. We can thus be more confident of their outputs although issues of numerical errors will remain. Use of the VFI Toolkit for replication is illustrated later in this article with the replication of Hansen (1985), an early paper from the real business cycle literature. Hansen (1985) represents a convenient example to illustrate the robustness of the VFI Toolkit (due to the atypical assumptions used for the exogenous productivity shock process) and of the potential importance of nonlinear global solution methods like those of the toolkit. The potential use of the VFI Toolkit for Research is largely self-evident. It solves the value function iteration problems that are used in practice for research in Economics: from investigating the properties of models by simulation, to quantitative models in Macroeconomics, to the structural estimation methods that have become increasingly popular in recent years. The rest of this article first discusses the design philosophy underlying how the VFI Toolkit is designed and some examples of what this means in terms of the codes. Then a more detailed description of one of the main functions, that for solving infinite horizon value function problems, and the algorithm implementing it, is given. A replication of Hansen (1985) using the toolkit is then given which also provides opportunity for a brief discussion of how the common use of non-global solution methods has had substantial effects on certain aspects of Macroeconomic modeling.",6
49.0,1.0,Computational Economics,01 December 2015,https://link.springer.com/article/10.1007/s10614-015-9547-y,Pessimistic Optimal Choice for Risk-Averse Agents: The Continuous-Time Limit,January 2017,Paolo Vitale,,,Male,Unknown,Unknown,Male,"Risk-aversion is an important aspect of agents’ preferences, which heavily influences their actions, in particular when the economic environment is complex and uncertain, and agents need to consider the future implications of their decisions. Investigating the nexus between risk-aversion and agents’ behavior is a challenge, in that optimal control problems are typically difficult to solve under risk-aversion. Researchers have usually relied on linear-quadratic formulations as they can be easily solved employing well-established results. Thus, within a linear-quadratic set-up straightforward recursive formulae immediately yield the optimal policy, while according to the certainty equivalence principle unknown variables can be replaced by their maximum likelihood estimates. Linear-quadratic formulations are however problematic, as the convexity of the quadratic cost function represents risk-aversion in an unsatisfactory manner. In fact, the prescribed optimal policy does not change when the environmental uncertainty varies, while risk-averse agents ought to care for the degree of uncertainty they face. In addition, in many economic problems, such as the production problem we investigate in Sect. 3, a quadratic objective function corresponds to the assumption of risk-neutrality on the part of the optimizing agent. To correct for these shortcomings, Whittle (1990) introduces a risk-adjustment in the objective function of the standard linear-quadratic set-up moving from a linear-quadratic to a linear-exponential-quadratic formulation. In doing so he augments the convexity of the objective function and the degree of risk-aversion of the optimizing agent. Within his formulation, he shows that: (i) the optimal policy is identified via a pessimistic (or worst-case) choice mechanism; (ii) risk-sensitive (or modified) certainty equivalence and separation principles hold; and (iii) recursive formulae describe the optimal policy. Despite its versatility, few researchers have employed Whittle’s methodology in economics (exceptions are Mamaysky and Spiegel 2002; van der Ploeg 2009, 2010; Vitale 1995, 2012; Zhang 2004). This is because an important limitation of his methodology is that it does not allow to consider time-discounting in a satisfactory way. 
Hansen and Sargent (1994, 1995, 2013) have introduced time-discounting into risk-sensitive optimal control problems by formulating a recursive optimization criterion à la Epstein and Zin. We extend their contribution in two ways. Firstly, we manipulate their optimization criterion in order to exploit a number of results developed by Whittle. Secondly, we consider the continuous-time limit of their discrete-time formulation. This allows to establish that sufficient conditions for the existence of optimal solutions coincide with those which apply under risk-neutrality. Indeed, an important feature of Hansen and Sargent’s linear–exponential–quadratic formulation in discrete-time is that for a sufficiently high degree of risk-aversion no optimal policy exists. On the contrary, in the continuous time limit an optimal solution always exists if some regularity conditions are met, irrespective of the degree of risk-aversion. This paper is organized as follows. In Sect. 2 we reformulate Hansen and Sargent’s recursive optimization criterion for the class of Markovian discounted linear exponential quadratic Gaussian (DLEQG) problems so as to apply, with simple adjustments, results originally derived by Whittle for the class of linear exponential quadratic Gaussian (LEQG) problems. In this Section we see how Whittle’s risk-sensitive certainty equivalence principle (risk-sensitive CEP) is maintained while the corresponding Riccati equation, which yields the optimal policy for the class of Markovian LEQG problems, is modified. In Sect. 3 we investigate the continuous-time limit of the DLEQG problem, showing that in the limit the optimal policy is characterized by a modified version of the differential Riccati equation which applies to the corresponding LEQG formulation. We are then able to show that in the continuous-time limit if the quadratic cost function in the DLEQG formulation is positive definite an optimal solution always exists irrespective of the degree of risk-aversion of the optimizing agent. In this Section we also discuss, as an illustrative example of the Markovian DLEQG problem in continuous-time, the optimal production policy of a risk-averse entrepreneur which runs a monopolist firm facing a demand schedule subject to stochastic shocks for the commodity it produces. Interestingly, risk-aversion makes the entrepreneur more aggressive. Given her preferences, the entrepreneur finds it optimal to systematically produce a larger quantity than that selected by her risk-neutral counterpart. This is because, despite a larger supply of the commodity depresses its price and jeopardizes future profits, it also reduces their variability. Consequently, a risk-averse entrepreneur is willing to gain smaller profits (in order to reduce their variability) and to produce a larger quantity of the commodity than a risk-neutral counter-part. This may appear counter-intuitive as it contradicts results typically obtained within static formulations. However, such a feature of the impact of risk-aversion on the behavior of economic agents in dynamic optimization exercises appears elsewhere. For instance, Holden and Subrahmanyam (1994) and Vitale (1995, 2012) find that risk-aversion makes a privately informed strategic agent trade more aggressively in a sequential call auction market, while the same trader would be more cautions in a one-shot call auction market. In other words, a point which is worth emphasizing is that the Markovian DLEQG formulation allows to derive implications of risk-aversion which are both general and stark. Indeed, not only risk-averse agents are pessimistic, but also bold. In Sect. 4 we investigate the class of DLEQG problems under imperfect state observation. Here, Whittle’s risk-sensitive separation principle (risk-sensitive SP), which allows to separate control and estimation, is reformulated for the class of DLEQG problems and it is applied to the example discussed in Sect. 3. In Sect. 5 we extend our analysis to the formulation in which the state vector is subject to pre-determined disturbances. A final Section offers some concluding remarks.",6
49.0,1.0,Computational Economics,01 December 2015,https://link.springer.com/article/10.1007/s10614-015-9540-5,"Superstars Power, Mining the Paths to Stars’ Persuasion",January 2017,Ana Suarez-Vazquez,Elena Montañés-Roces,,Female,Female,Unknown,Female,"Research on the influence of superstars on box office revenue has yielded somewhat mixed results. The lack of unanimity in the interpretation of the relationship between stars and success has been attributed to four different reasons: (1) the problem that studies address; (2) the data collection approach; (3) the estimation procedures; (4) the concept of “superstar”. In the context of movie stars worth, these dimensions typically take the following form: (1) the studies analyzes the relationship between the presence of superstars and the box office revenue; (2) data collection involves using some of the cinema bases available; (3) standard ordinary least square approaches dominate cinema revenues’ modeling; (4) studies apply a specific measure of star power. Traditionally, star power literature has focused on answering whether the presence of superstars increase cinema revenues or returns to investment (Ravid 1999). A complementary approach is to address the mechanism by what the presence of a superstar can increase spectators’ interest in a given film. The degree to which a person is motivated to thoughtfully consider the merits of a particular object has received a great deal of attention in marketing literature. The attitude and behavior change resulting from the power of persons is known as persuasion (Cacioppo et al. 1991). This idea suggests the possibility of improving the knowledge about star power applying current work of persuasion. In comparison with the majority of the sectors, the cinema industry has the privilege of the availability of enormous and rich data bases (e.g. www.imdb.com, www.boxofficemojo.com). Instead of studying collective movie attendance decisions, gathering the information of individual spectators can provide a psychological explanation about why individuals choose particular movies (Eliashberg et al. 2006). In the past few years, remarkable progress in cinema revenues’ distribution modeling have been possible due to the application of non-standard models to modeling motion-picture profit (Walls 2009). In environments in which there are complex relationships between predictor and target variables support vector machine (SVM) suits well. The SVM is a semiparametric technique with origins in the machine-learning literature of engineering and computer science (Cui and Curry 2005). Machine learning is slowly making its way in marketing science (Abernethy et al. 2008). In the context of the cinema market it has been used for recommender systems of movies. However, in these applications datasets were formed using aggregated publicly available databases (see Cheung et al. 2003). Previous studies have proposed different measures of star power. Some of them are related with the consistency of stars’ past box office success, that is the bankability of the stars. Alternatively, star power can also be approximated by star buzz measured by the intensity of internet searches about a given star (Karniouchina 2010). This paper analyzes whether superstar ability to persuade spectators is affected by the facet of superstar power considered (bankability/star buzz). The rest of the paper is organized as follows. Section 2 discusses the major theoretical frameworks of persuasion, with particular emphasis on the theoretical approaches to explaining the underlying processes of persuasive communication. Section 3 describes the different alternative gauges of star power, the empirical study and the data mining process. Section 4 presents the main results and the last section gives some concluding remarks.",2
49.0,1.0,Computational Economics,17 December 2015,https://link.springer.com/article/10.1007/s10614-015-9548-x,Forecasting Home Sales in the Four Census Regions and the Aggregate US Economy Using Singular Spectrum Analysis,January 2017,Hossein Hassani,Zara Ghodsi,Mawuli Segnon,Male,Female,Unknown,Mix,,
49.0,1.0,Computational Economics,17 December 2015,https://link.springer.com/article/10.1007/s10614-015-9549-9,"Applying the Hybrid Model of EMD, PSR, and ELM to Exchange Rates Forecasting",January 2017,Heng-Li Yang,Han-Chou Lin,,,Unknown,Unknown,Mix,,
49.0,1.0,Computational Economics,17 December 2015,https://link.springer.com/article/10.1007/s10614-015-9545-0,Convergence of Discretized Value Function Iteration,January 2017,Robert Kirkby,,,Male,Unknown,Unknown,Male,"We provide uniform convergence results on the computational solution of both the value function and the optimal policy function that depend only on monotonicity of the return function. These results are based on value function iteration using pure discretization of state-space. The assumptions underlying the results are less strict than in existing results, and are easy to check in practice. The results on the optimal policy function in particular are more generally applicable than those in the existing literature and are the main contribution of this paper. These results on uniform convergence for computational solution of both the value function and the optimal policy function fill an important gap in the literature. Existing results either depend on differentiability (Santos and Vigo-Aguiar 1998) and so cannot be applied to certain models of interest, are only applicable to finite horizon models (Bertsekas 1976), or only cover convergence of the value function and not the optimal policy function (Whitt 1978; Stachurski 2008). Further, all existing results on the convergence of the optimal policy function depend on strict (or strong) concavity of the return function and strict convexity of the feasible choice set. In models in which the value function and return function are differentiable, the return function is strictly concave, and feasible choice sets are convex the first-order conditions of the optimization problem are both necessary and sufficient. In these cases numerical methods based on directly solving the first-order conditions (often rearranged into the Euler equations) such as perturbation methods and projection methods are typically used as they give a better speed-accuracy trade-off than value function iteration (Aruoba et al. 2006). So existing results for convergence of the optimal policy function using discretized value function iteration are based on assumptions which in practice would lead to numerical methods other than value function iteration being used. The results on the convergence of the optimal policy function and value function using discretized value function iteration presented in this paper instead allow for the kinds of situations—non-differentiability, non-concavities in the return functions, non-interior optimal choices, and non-convexities in the choice sets—in which value function iteration becomes the preferred method. In these situations the first-order conditions (and hence also the Euler equations) are no longer necessary and sufficient conditions, and so most alternative numerical methods simply cannot be used. There are many Economic applications in which such situations arise. For example models with a mixture of discrete and continuous variables or occasionally binding constraints which cause certain non-differentiabilities in the value function (Clausen and Strub 2014); examples include models with discrete choices of housing or durable goods (Díaz-Giménez et al. 1992; Rust 1987; Fella 2014) and models with periodically binding constraints (Huggett 1993; Rao Aiyagari 1994). Models in which there are two possible production technologies and firms must decide whether to adopt the ’alternative’ technology lead to non-differentiablilities and a non-concave return function (Santos and Peralta-Alva 2005; Alpanda and Peralta-Alva 2010). Models in which the payroll tax is explicitly modelled as a fixed rate up to a certain maximum income (as under current US law) generate non-convex choice sets (Díaz-Giménez and Pijoan-Mas 2011). In all of these situations previous results could not be applied but the results of this paper can.Footnote 1
 In many Macroeconomic models, such as those with heterogeneous agents (Ríos-Rull 1995, 2001; Heathcote et al. 2009), solving the value function problem is just a step to getting the optimal policy function which can then be used to find, by simulation and fixed-point arguments, the solution to the model itself. Knowing that the computational solution to the optimal policy will converge is a first step to establishing that simulations of the model will also do converge (Santos and Peralta-Alva 2005); these simulations in turn are needed both for calibration exercises, and as already mentioned are part of solving many Macroeconomic models such as those with heterogeneous agents. Likewise, estimation of Macroeconomic models is typically dependent on the optimal policy function (Fernandez-Villaverde et al. 2006). For these reasons uniform convergence of the optimal policy function when using value function iteration is arguably of greater interest than the convergence of the value function itself. As part of the derivation of the results on uniform convergence this paper also provides numerical error bounds, however the bounds are too loose to be useful in practice.Footnote 2 However, since the bounds go to zero as the distance between the grid points goes to zero these bounds can be used to prove that numerical errors will go to zero asymptotically. It is to exactly this purpose of proving that numerical errors go to zero asymptotically that these bounds are used in the body of this article. While too loose to be practical the numerical error bounds are still of some direct interest. They illustrate that using denser grids in regions where the return function and value function are steepest will tend to help minimize numerical errors. They also show that concentrating grids in the regions around the optimal policy choices helps minimize errors in the optimal policies. 
Value Function Iteration While having an inferior speed-accuracy trade-off than other numerical methods such as perturbation methods or projection methods (Aruoba et al. 2006) value function iteration has certain advantages in terms of being more widely applicable. Many of these alternative numerical methods are based on the first-order conditions (FOCs; often rearranged into the Euler equations) of the optimization problem and so are valid when the FOCs are both necessary and sufficient conditions; eg. the FOCs are a sufficient condition if the return function is continuous, differentiable, and concave; and the choice set is a convex set. Existing theory bounding the numerical errors from value function iteration assume properties—such as interiority, and differentiability—that make the FOCs necessary and sufficient; and so in practice other numerical methods are likely to be used in those situations. Examples of common alternative numerical methods based on solving the FOCs (or Euler equations) include perturbation methods (whether first-order, second-order, or third-order perturbations) and projection methods. The uniform convergence results derived in this paper allow for situations in which the FOCs are not sufficient conditions, and so most other solution methods are invalid and value function iteration becomes a standard choice. The results of this paper are derived based on discretized value function iteration. They exactly mimic the discretized value function iteration algorithm as it it commonly implemented.Footnote 3 Namely by discretizing the state variables, discretizing the control variables (and hence the maximization step), and discretizing the numerical integration (eg. by quadrature methods such as the Tauchen method).Footnote 4
 Discretized value function iteration is chosen as the basis for the convergence results on the grounds that it both commonly used and is robust to the kind of situations—non-differentiability, non-interior choices, non-concavities in return function, and non-convexities in feasible choice sets—which are often of interest to us in the application of these convergence results. Discretized value function iteration has also recently been given a new lease on life in practical applications thanks to parallelization on graphics processors (Aldrich et al. 2011). The VFI Toolkit for Matlab (vfitoolkit.com) provides an implementation of the discretized value function iteration that can be used to solve a wide variety of value function problems taking advantage of the graphics processor (Kirkby 2015). Useful references on the convergence of value function iteration include Bertsekas (1976) who takes a similar approach to discrete state space approximations in value function iteration, but only for the finite-horizon case with a discrete iid exogenous shock process, and also derives bounds for the optimal policy function, but which depend on strong concavity of the return function. Closest to the results provided here, Whitt (1978) gives numerical error bounds for the infinite horizon value function problem with general shock processes.Footnote 5 However he does not go on to provide numerical error bounds for the optimal policy; essential for our purposes. The results we provide for the value function itself (but not those for the optimal policy function) are largely a combination and extension of these two earlier results; with the addition of explicitly showing how the numerical errors in the value function vary between different parts of the state space. Other existing results on convergence of the value function tend to be based on ‘partial discretization’: considering the errors from approximating the state space, but not the choice variables, nor the the numerical integration. This has the advantage that they are able to derive tighter bounds on numerical errors, and also results relating to the speed at which numerical errors go to zero asymptotically, and on using fitted value function methods. The disadvantage is that they also mostly require a degree of differentiability of the value function, interiority of optimal policies, convexity of choice sets, and concavity of the return function. Santos and Vigo-Aguiar (1998) work with the infinite horizon case looking at partial discretization and using a fitted value function (specifically the value function is modeled using finite elements methods, rather than being a single number for each point as in the discretized state space). They also provide results on the speed of convergence to the true value function; namely that it is quadratic in the grid size; and that convergence to the policy function is linear in the grid size. Their numerical error bounds are tighter than those derived here, tight enough to be useful in practice. Stachurski (2008) provides further results on numerical error bounds for partial discretization using a variety of more sophisticated fitted value function methods such as approximating the value function by certain types of splines or polynomials; he shows that the numerical errors resulting from a number of fitted value function methods popular in the literature go to zero asymptotically as the grids get finer. Of general interest are Stachurski’s results on shape-preserving fitted approximation methods, a methodology advocated for smooth problems by Cai and Judd (2014). Pál and Stachurski (2013) show how these results can allow for periodically-binding constraints and non-differentiable return function and value function, but do not consider the errors from approximating the choice variables; their results are based on fitted value function iteration, with Monte-Carlo integration. While Stachurski (2008) in particular provides results that could be used in place of those given here for bounding errors in the value function arising from pure discretization he does not provide results on the optimal policy function; the results provided by this paper on the value function also allow us to consider how grids should be constructed in terms of where to place points to minimize errors, an issue discussed later. I now discuss two possible situations for which the error bounds are not applicable: if the slope of the return or value function goes to infinity (say due to Inada conditions) and if next periods state cannot be chosen directly. These are addressed in turn: (i) the slope of a return or value function going to infinity would occour when Inada conditions are present. At first glance this seems problematic given the prevalence of Inada conditions, but it is actually not likely to be any problem at all. The presence of Inada conditions is generally used to prove theoretically that there exists, eg., a minimum level of consumption, and thus one can work with a bounded space defined using that minimum level of consumption, this ensures all the functions are bounded and thus that we can apply the standard theorems for bounded value functions, having redefined the problem on this new space, the problem of the slope of a return or value function going to infinity would no longer occour, and one could apply the error bounds derived here as usual. (ii) if next period’s state cannot be chosen directly the error bounds derived here are invalid,Footnote 6 but extensions to allow for this might be possible; Whitt (1978) provides some results of this nature. Section 2 provide the convergence results, the conditions they are valid under, a comparison to other results in the literature, and a Proposition that brings together the main results. The rest of the paper consists of the full derivation of these results. The Appendices provide some needed background mathematical theory.",3
49.0,1.0,Computational Economics,18 December 2015,https://link.springer.com/article/10.1007/s10614-015-9550-3,On Asymmetric Market Model with Heteroskedasticity and Quantile Regression,January 2017,Cathy W. S. Chen,Muyi Li,Songsak Sriboonchitta,Female,Unknown,Unknown,Female,"Following the seminal work of Markowitz (1952), the capital asset pricing model (CAPM) that was independently proposed by Sharpe (1964) and Lintner (1965) has become the standard tool to explore the relationship between expected returns and market risk premiums. Despite its empirical flaws and the existence of modern approaches to asset pricing and portfolio selection, CAPM still remains popular due to its simplicity and utility in a variety of situations. The original CAPM is: where \(E(R_i )\) and \(E(R_m )\) are respectively the expected returns of the capital asset and the market portfolio, \(R_f \) is the risk-free rate, \(E(R_m )-R_f \) is the so-called market risk premium, and \(\beta _i \) is the sensitivity of the expected asset returns to the expected market returns and can be estimated by: \(\frac{Cov\left( {R_i -R_f ,R_m -R_f } \right) }{Var\left( {R_m -R_f } \right) }\). Although CAPM has gained overwhelming success for asset pricing and portfolio selection, it possesses some empirical flaws as seen from the finance industry. We note that CAPM is not convincing from the empirical viewpoint (not precisely accurate), since many of its assumptions are not exactly satisfied. Many of the constructs of the market model are widely used in investment. Traditionally, market beta is conceived to be invariant over time, but there is considerable evidence rejecting the constancy of market beta. Banz (1981) and Fama and French (1993) find no linear relationship between expected return and market beta under the unconditional market model. Moreover, the constant beta coefficient and time invariant variance in the original market model make it less convincing to capture the dynamics of real financial markets [see Engle and Rodrigues (1989), Ferson and Harvey (1993), Jagganathan and Wang (1996), and Silvapulle and Granger (2001)]. Hansen and Richard (1987) show that a conditional CAPM might succeed in explaining the stock returns from varying market betas even if its corresponding unconditional asset pricing model fails to explain the stock returns from a constant market beta. Jagganathan and Wang (1996) use time-varying betas to explain quite well the cross-section average returns in the New York Stock Exchange (NYSE) and American Stock Exchange (AMEX). The conditional market model with time-varying betas also possesses other advantages. For instance, Menzly et al. (2004) propose a general equilibrium model in which betas and dividend growth expectations are time variant. Under this time-varying setting, they show that the forecasting ability of dividend yields to return is reduced and the forecasting ability to dividend growth is eliminated. Ang and Chen (2007) show strong evidences of time-varying betas and find that such betas could be treated as latent variables to capture the book-to-market effect. Adrian and Franzoni (2009) present a conditional CAPM so that investors could learn and realize the unobservable risk loading by observing the realization of excess returns. 
Ghysels (1998) argues that betas might actually vary much more slowly and discretely than what we think. In that case, betas with discrete variation should be considered. The literature also discusses the asymmetry of \(\beta \) related to good and bad news, which is the so-called leverage effect. To overcome the invalid assumptions and applications of the model, many modern approaches to asset pricing and portfolio selection have been developed. Among them, Braun et al. (1995) and Cho and Engle (1999) study the leverage effect of \(\beta \); Granger and Silvapulle (2002) present CAPM under three market scenarios, i.e. bear, steady, and bull, and show that betas are much higher in a bear market. Akdeniz et al. (2003) establish a simple two-regime homoscedastic threshold nonlinear CAPM with only two discrete betas and compare it with the traditional and time-varying CAPM. Some special empirical characteristics concerning financial time series are dynamic and clustering volatility and fat tails. The family of autoregressive conditional heteroskedastic (ARCH) and generalized ARCH (GARCH) models, coined by Engle (1982) and Bollerslev (1986), has been commonly adopted in the literature to estimate time-varying variance. Bollerslev et al. (1988) first model a dynamic market beta in terms of time-varying variances and covariance, via a multivariate GARCH model. Chen et al. (2011) introduce a multiple-regime threshold nonlinear CAPM with the GARCH process for volatility specification. They confirm the time-varying nature of market risk, in response to changes in the market, and that this discrete time variation can vary across assets. In light of the above issues in modeling beta, this present paper develops an asymmetric market model embedding both the leverage effect of market news and the previous return to express the instability of beta and the error with heteroskedasticity to capture the time-varying conditional variance. In the current financial environment, extreme market conditions are occurring more frequently. Compared to conditional mean regressions, quantile regressions (Koenker and Bassett 1978; Koenker and Hallock 2001) have attracted greater attention due to their robustness to these extremes and have been widely used in many fields of applied econometrics. In the framework of market behavior, Chen et al. (2009) and Chuang et al. (2009) investigate various causal relations for the international stock markets over different quantile levels, while Chen et al. (2012) and Chen and Gerlach (2013) propose more general quantile market models with smooth transition (or threshold transition) and heteroskedasticity. The advantage of quantile analysis is that it allows one to estimate the relationship between an individual stock’s excess return and a market index at extreme quantiles, capturing the impact of extreme market conditions through the distributions. Its second contribution comes from the computation of asymmetric betas and market news over various quantile levels, representing various return positions and risk scenarios. For example, in extreme low quantiles, we are interested in estimating betas in order to understand the sensitivity of value at risk (VaR) with respect to market return. The proposed model captures asymmetric risk through the autoregressive effect, market beta, and negative news. This model allows us to investigate parametric relations from the perspective of conditional quantiles by controlling some risk factors. This paper advances the market model twofold. First, we propose an asymmetric market model with heteroskedasticity to capture both the leverage effect and the time-varying variation. Second, we consider the asymmetric market model under a quantile regression since it is more robust to understanding the risk of financial investments under extreme market conditions versus normal ones. These models also contain the lagged value of the excess return, which helps to identify whether the return series exhibits mean reversion or market efficiency. The computation for the estimation/inference of the proposed models is fast and efficient, each being completed within 10 seconds on a standard PC using R. This paper is organized as follows. Section 2 introduces the asymmetric market model with heteroskedasticity and the asymmetric quantile market model. Section 3 applies the proposed models and methods to analyze 15 stocks, which are heavily traded in the Dow Jones Industrial Average, and to illustrate the flexibility of our model and estimation methods. Section 4 provides some concluding remarks.",6
49.0,1.0,Computational Economics,22 December 2015,https://link.springer.com/article/10.1007/s10614-015-9551-2,A Rejoinder to Notes on a ‘Constructive Proof of the Existence of a Collateral Equilibrium’,January 2017,Wei Ma,,,,Unknown,Unknown,Mix,,
49.0,2.0,Computational Economics,28 December 2015,https://link.springer.com/article/10.1007/s10614-015-9542-3,Game Theoretic Modeling of Economic Systems and the European Debt Crisis,February 2017,Jonathan William Welburn,Kjell Hausken,,Male,Male,Unknown,Male,"A game theoretic model is developed comprising six types of players, i.e. countries, central banks, banks, firms, households, and financial intergovernmental organizations (FIGOs). Countries produce, consume, trade, invest, borrow, and lend. Central banks lend, borrow, set interest rates, may default, penalize default, and set inflation targets. Banks borrow, lend, set interest rates, default, and penalize default. Firms produce, demand labor, invest, import, export, borrow, lend, and default. Households consume, invest, borrow, lend, and default. FIGOs borrow, lend, set interest rates, default, and penalize default. In this work, we present a general model, characterize the players, and define Nash equilibria. The players’ strategy sets are large, making the model extensively endogenous. However, the numerous strategic-choice variables in the model make solutions of the full model extremely complicated. The model can be used with all strategic variables as strategic variables, or used interpreting some of the variables as exogenous parameters. We consider several simplifications of the model. For example, single-country numerical analyses and sensitivity analysis show how a country can respond to a temporary negative shock with increased borrowing to finance public consumption. We present a dynamic model that guides readers to understand the strategies of all stakeholders in economic systems. We present findings from relevant numerical examples. Our conclusions explain how crises can plausibly spread and how each player’s strategy can contribute to the spread. We also put forth ideas for how players may adjust their strategy to stem the spread of contagion. Strategic interaction implies contagion. For example, we illustrate in this paper that a shock to one country may impact another country through contagion. The literature on contagion builds on the literature of sovereign default. We highlight several methods for modeling sovereign default including financial models and sovereign default models. Two-country numerical analyses demonstrate contagion through the credit channel since a shock that causes default in a borrower country can propagate to a lending country. The trade channel is similar, as a negative shock in one country can diminish demand for imported goods, transmitting the adverse shock to another country through reduced trade. Finally, two-country numerical analyses are used to demonstrate how common macroeconomic conditions can create adverse shocks without contagion, since a joint interest-rate shock can negatively affect both countries. Our results demonstrate that each of the three causes discussed above (contagion through credit channels, contagion through trade channels, or common macroeconomic conditions with no contagion) can lead to crises even if all players in the model behave rationally. Most of the literature applying game theoretic approaches to economic contagion is limited in either its use of game theoretic and decision theoretic tools, or its ability to address the multiple-channels problems. Morris and Shin (2012, 2000) model contagion in a coordination game of investors to currency crises, bank runs, and debt pricing to assess adverse selection. The importance of behavior is demonstrated by Chari and Kehoe (2004) who argue that models of herd behavior can explain financial crises. Broner (2008) argues that the private information in models of currency crises can create multiple equilibria and unpredictable currency devaluations. Furthermore, many have argued that crises can be self-fulfilling. Obstfeld (1984) argues that speculative attacks can make balance-of-payments crises self-fulfilling. Lorenzoni and Werning (2013) create a model where self-fulfilling debt crises follow from investor expectations on default. They argue that a crisis follows from a shift from a good equilibrium to a bad one. Hausken and Plumper (2002) propose a contagion game for how a crisis spreads and can be contained through intervention by the IMF and collective action. Baral (2013) proposes a network formulation game between United States borrower banks, lender banks, and the Federal Reserve to model contagion. Similarly, Acemoglu et al. (2013) compute Nash equilibria in a network formulation of the interbank market to describe contagion and counterparty risk. Most similar in spirit to this paper, Welburn and Hausken (2015) present a model consisting of many of the players in this paper except FIGOs. Whereas they present results from a numerical analysis using data for the United States, we present results from numerical analyses using data for Greece and Germany in order to give insight into the Eurozone debt crisis. The discussion of contagion builds on the sovereign debt literature which seeks to explain debt and default for small open economies through single country models. Reinhart and Rogoff (2009) consider debt crises through history. Eaton and Gersovitz (1981) and Eaton et al. (1986), describe the unique problems of sovereign lending presented by a lack of enforcement. While domestic lending can be legally enforced in default through seizure, foreclosure, as well as other methods, foreign lending is more challenging to enforce. Eaton et al. (1986) further elaborate that given the lack of enforcement, payment on sovereign debt is mostly voluntary. 
Eaton and Gersovitz (1981) explain that the potential for repudiation drives repayment. They explain that default has a cost by harming the reputation of the borrower. In their framework, the decision to default follows the path that gives the highest utility; default or no default. Bulow and Rogoff (1988) continue the sovereign debt discussion. They argue that reputation alone is not enough to ensure repayment. Bulow and Rogoff (1988) develop a model to explain that sanctions are required for payment. While the type of penalty varies, the literature on sovereign default often uses some penalty (e.g., reduced output, loss in utility, autarky) to enforce repayment. Default risk can, in fact, be handled in several ways. It is popular in the finance literature to model default exogenously. Duffie and Singleton (1999) develop a reduced-form model to price bonds with the risk of default. In this approach, default follows an exogenous stochastic process. Andritzky (2006) presents an extensive assessment of sovereign lending and associated risks within the framework of financial models such as the Duffie and Singleton (1999) model. He presents a detailed history of sovereign lending and the numerous default crises that have occurred in recent history. Andritzky (2006) study of debt crises focuses on what the results of past crises has been, what their impact to investors has been, and how default risk can be modeled. He highlights how default risk can be handled by financial models (e.g., structural vs. reduced-form models). Additionally, sovereign debt risk can be evaluated using credit ratings from major ratings agencies. Gaillard (2011) discusses credit ratings for sovereign debt. In light of criticisms of credit ratings following the 1997 East Asian Crisis and 2010 Greek Debt Crisis, Gaillard (2011) explains how ratings are made & assigned and the methodology used to rate debt. Ratings are demonstrated to be procyclical and offer limited value. Furthermore, Gaillard (2014) demonstrates flaws in ratings following the Eurozone crisis where downgrades occurred too slowly. As a result, Gaillard (2014) argues that policymakers and investors should decrease their reliance on credit ratings and improve in-house modeling of default risk. Finally, Das et al. (2007) seek to understand common cause failures as correlated corporate defaults. We address the need for improved modeling of default risk by modeling default endogenously and through time. In their seminal default model, Eaton and Gersovitz (1981) consider debt with repudiation. Aguiar and Gopinath (2006) explain how defaults can occur in a quantitative model of small open economies. Arellano (2008) estimates default risk using a stochastic recursive equilibrium model applied to emerging economies. Mendoza and Yue (2011) explain defaults in the context of business cycle dynamics using a general equilibrium model. Default is often handled as a one-time event. Those following the Eaton and Gersovitz (1981) setting often model default as a single decision. In contrast, those following the Andritzky (2006) settings model default as a single random event. Yue (2010) and Arellano and Bai (2013) offer another approach by including Nash bargaining for debt renegotiation in a model of linkages in sovereign debt markets accounting for default. The outcome of a failed negotiation is autarky for the borrower and a total loss for the lender. The sovereign debt literature (in the previous subsection) discusses crises in a single country. Crises, however, are often not isolated to a single country. In many cases crises spread across countries and regions through a contagion process. In 1989, a Brazilian crisis contributed to a Latin American crisis. In 1994, a Mexican crisis contributed to another Latin American crisis. The 1990s led to further notable examples. In 1994, a crisis in Thailand spread across Southeastern Asia driving the Asian financial crisis. This crisis, in turn, contributed to a Russian area crisis in 1998. The most recent crisis, the 2009–2012 Eurozone crisis which began in Greece, serves as a primary motivation for this paper. We seek to quantitatively explain key elements of the Eurozone crisis, specifically the crisis in Greece, applying our game theoretic model. Contagion follows from globalization, trade, and interconnectedness. This problem is not modern; Reinhart and Rogoff (2009) document crises dating back to the fourteenth century. The consensus for modelling contagion risks is incomplete. Forbes and Rigobon (2002, 2001) argue that so-called contagion episodes actually follow from increased interdependence following an adverse shock and that the effect can be viewed through increased co-movements. Hernández and Valdés (2001) and Kaminsky et al. (2003) clarify the possible mechanisms of contagion, that it can be driven by a debt channel, a trade channel, or that the process that appears to be contagion may be driven by a common macroeconomic cause. Much of the contagion literature focuses on the debt channel which has many ties to the interbank market. Kiyotaki and Moore (1995) present a model of cross-sector contagion following a vicious cycle of declining asset prices and investment. Allen and Gale (2000) address the ability for shocks to spread across regions of the economy due to network structure. Furthermore, Giesecke and Weber (2006) extend the reduced form models of financial default risk to assess contagion between firms. Acharya and Yorulmazer (2008) add to the literature on contagion specifying the importance of information on contagion through the banking sector. They find that information (e.g. bad news) can lead banks to act as a group driving contagion. These sources provide insight into contagion, however do not specifically address the risk of sovereigns with contagion. Many others extend the sovereign debt literature to discuss contagion. Arellano and Bai (2013) extend Yue (2010) to present a multi-country model with debt renegotiation where all defaulting countries renegotiate simultaneously. They calibrate their model to the Eurozone crisis predicting commoving interest rates and finding that a default in one country increases the probability of default in another. Park (2012) presents a sequential equilibrium model of contagion where the mechanism of contagion is the investor (i.e. lender) in sovereign debt. Following the several notable debt crises including the recent Eurozone crisis, much attention in the sovereign debt literature has been given to developing macroprudential policies. Galati and Moessner (2013) describe that the recent development of literature towards macroprudential policy shares the objective of shaping macro-focused policy that improves financial stability. Of course, we argue that our approach using game theory offers a strong framework for which macro policy can be developed. Here, we acknowledge that several approaches can be used for managing crises which we either do not model, or model indirectly. Austerity is often presented as a solution for managing crises. Austerity describes the sharp reduction in spending used by government to reduce deficits avoiding further crisis. An ardent critic of austerity is Krugman (2013), see also Eggertsson and Krugman (2012). While the austerity approach offers certain advantages in winding down budget deficits, it sharply reduces spending, consumption, and investment in a potentially harmful manner. We explore and endogenize these effects in this paper. An additional approach for managing crises is the use of capital controls. Capital controls refer to policies enforced by the government which restrict capital flows out of the country. This approach has mostly been used by developing countries to boost domestic assets. Edwards (1999) argues that the use of capital controls is often ineffective and causes distortions and corruption. Financial repression represents another form of governmental policy used to stem debt crises. The term financial repression refers to the governmental policy of setting interest rates on government debt at or below the inflation rate. Financial repression, thereby, works as a tax of the saving of households who purchases government bonds providing governments with a cash flow to repay debts. This approach has some similarities to austerity and the imposition of capital controls in both its desired effect and its implementation. Reinhart et al. (2011) argue that financial repression can be used as an alternative to debt restructuring and default. That is, a government could generate income through financial repression to pay down debts avoiding debt crises. Others argue that this approach comes with significant drawbacks that reduce the policy’s ability to be an effective tool for managing debt crises. Roubini and Sala-i-Martin (1992) argue that the use of financial repression inhibits growth making it less desirable. Financial repression, consequently suffers from a similar argument as do capital controls in their limited ability to serve as macroprudential policy. In this paper we focus on strategic behavior in a highly interconnected system. We provide a framework for discussing how crises can be managed through strategic interaction without the use of capital controls or financial repression. We account for degrees of austerity which are relevant to discussions of policy. Our paper advocates the use of game theory in constructing macroprudential policy. Section 2 presents a model with six types of players; households, firms, banks, central banks, countries and FIGOs. We characterize the strategy sets of each player and present a description of variables used in the model. In this section we present a model with similarities to a small open economy framework with endogenous default. We describe markets for debt and markets for goods. We explain the basis for endogenous default and define the penalty for defaulting. We conclude this section with a definition of the Nash Equilibrium. In Sect. 3, we numerically analyze a single country with households and firms in equilibrium. We illustrate the effect of public consumption, lump sum transfers, austerity, a positive interest rate shock, and a negative productivity shock in a ten period analysis. In Sect. 4, we numerically analyze two countries, determining the effect of a negative productivity shock and default. In Sect. 5, we use 2005–2011 empirical data for Greece and numerically analyze the effect of public consumption, negative productivity shocks, austerity, public investment, and default, assuming one period, two periods, and seven periods. In Sect. 6, we use 2007–2008 empirical data for Greece and Germany and numerically analyze the effect of public consumption, negative productivity shocks, public investment, and default, assuming two periods. Section 7 presents policy recommendations from the findings and intuition of our framework. Section 8 concludes.",6
49.0,2.0,Computational Economics,02 January 2016,https://link.springer.com/article/10.1007/s10614-015-9556-x,Global Banking on the Financial Network Modelling: Sectorial Analysis,February 2017,Fathin Faizah Said,,,Unknown,Unknown,Unknown,Unknown,,
49.0,2.0,Computational Economics,06 January 2016,https://link.springer.com/article/10.1007/s10614-015-9554-z,Permanent Breaks and Temporary Shocks in a Time Series,February 2017,Yoonsuk Lee,B. Wade Brorsen,,Unknown,Unknown,Unknown,Unknown,,
49.0,2.0,Computational Economics,09 January 2016,https://link.springer.com/article/10.1007/s10614-016-9561-8,A New Stable Local Radial Basis Function Approach for Option Pricing,February 2017,A. Golbabai,E. Mohebianfar,,Unknown,Unknown,Unknown,Unknown,,
49.0,2.0,Computational Economics,12 January 2016,https://link.springer.com/article/10.1007/s10614-015-9555-y,Debt Portfolio Management for an Oil Company Under Oil Price Uncertainty,February 2017,Vladimir Korotin,Arseniy Ulchenkov,Rustam Islamov,Male,Male,Male,Male,"The events of the second quarter of the year 2014, which had place in the world commodity and currency markets, namely sharp fall of hydrocarbons value and subsequent depreciation of the national currency, as well as similar events of the years 2008, 2009, all these events reaffirmed the need to fully integrate all the uncertainties of various nature whether for financial economic models or different mathematical models, depending of external environment. Having been realized a stochastic nature of the crises, many companies began to refuse deterministic approaches in activity planning in the mid 2000-s. Recently, we can observe a period of transition: they use a set of scenarios instead of point estimations. The most widespread method implemented in the number of companies is formation of so-called scenarios (trajectories) of sequences of events. As a rule, such scenarios are called “worst-case”, “baseline” and “optimistic”. At that, more often they forget that it’s impossible to construct a function of stochastic variable probability distribution by three points. A special and very important task is to assess intervals of possible values of the total random variable and the final assessment of predetermined criterion exceedance probability (see Korotin et al. 2014) (for example, the ability of the considered company to service a debt), as well as understanding of the way in which a distribution structure of incoming stochastic values has an effect on the final result, and this task is absolutely unsolvable within the frames of Scenario Analyses. Very often, because of the complexity of the mathematical models, the solution of this problem in an analytical form is hardly possible; accordingly, the solution of the problem is possible only by numerical modeling of how the model parameters uncertainty effects on the results. The present article is concerned with stochastic and deterministic models’ analysis and the later use of the uncertainty analysis methods.",6
49.0,2.0,Computational Economics,16 January 2016,https://link.springer.com/article/10.1007/s10614-015-9557-9,Endogenous Demand and Demanding Consumers: A Computational Approach,February 2017,Carlos M. Fernández-Márquez,Francisco Fatás-Villafranca,Francisco J. Vázquez,Male,Male,Male,Male,"In recent times there has been a growing interest in consumer behavior research, mainly motivated by empirical evidence from a wide range of disciplines such as Sociology, Economics, Psychology or Marketing. Some studies suggest that in order to analyze phenomena such as innovation, it is necessary to consider a wide range of aspects including the heterogeneity of consumers, their taste for new things and their degree of tolerance when products change their characteristics (Bianchi 1998; Malerba et al. 2007). Other studies affirm that consumer desires are satisfied by social interpretation of the characteristics of goods and not by the goods in themselves (Witt 2005; Loasby 2001; Leiss 1983; Ironmonger 1972; Lancaster 1966). Both desires and social interpretation of characteristics often arise from social emulation (Aversi et al. 1999; Cowan et al. 1997; Veblen 1899). This phenomenon is very important in the case of discretionary consumption goods,Footnote 1 where their valuation does not necessarily come from the material satisfaction provided, but rather from the social image associated to some of their characteristics (Becker 1996; Baudrillard 1981; Stigler and Becker 1977). Furthermore, evidence suggests that consumers are not fully rational as Rational Choice Theory assumes (Nelson and Consoli 2010). That is, they do not maximize their utility function on known environments, but rather they employ heuristics to estimate if a given product (often new and rather complex) is better or worse than another for satisfying their desires (Valente 2003). Emulating the behavior of other consumers that are considered popular, socially prestigious or well-connected is a way of making consumption decisions in uncertain and turbulent environments. This sub-optimal behavior is not unique to consumption activities; in general, it is becoming increasingly clear that economic agents, either consumers or producers, are not optimizers, but rather they learn by means of adaptation (Witt 2001; Nelson and Winter 1982). Although all these questions have been studied empirically, their theoretical implications on demand behavior and, in general, on market processes still remain hidden. Demand has been of little interest for mainstream approaches that usually describe it in a very simple way by means of the typical aggregate demand curve. However, evidence shows that demand is not a passive subsystem; conversely, it is transformed by mechanisms that have influence and shape the supply. For this reason it is necessary to continue to deepen our understanding of demand (Valente 2012). This paper aims to make a small contribution in that sense, by analyzing how some particular market properties (such as the number of producers or the industrial concentration), that are traditionally explained by supply, can also be explained by demand (Mowery and Nelson 1999) in the case of discretionary consumption markets such as consumer electronics, the wine industry, high quality food, leisure (tourist destinations, restaurants, night clubs, etc.), designer clothing, fashion accessories or fragrances. In particular, we introduce an agent-based model in which social emulation and adaptation of producers to the progressive transformation of demand play an important role. Agent-based modelling (ABM) is a multidisciplinary methodology that deals with the study of socio-economic systems whose aggregate properties at the macro-level can be properly explained as if they emerge from a set of decentralized relationships between heterogeneous agents at the micro-level (Pyka and Fagiolo 2007; Rixen and Weigand 2014). Despite its youth, this methodology has a growing interest as a theoretical tool for studying complex systems. In our case, at the micro-level there are consumers that set and update their tastes and desires mainly via social interaction (by means of social emulation), and producers that aim to attend to the needs of market. The industry level (macro) consists of some aggregate variables of interest (that emerge from the micro-level) such as the adoption rate of the goods, the number of producers in the market and the Herfindahl index to measure industrial concentration. The structure of the paper is the following. In Sect. 2 we describe all of the assumptions of our model about how consumers behave and how producers make decisions. Since there are many parameters in the model, in Sect. 3 we study the relevance of each of them, concluding that the parameter that represents the strictness of consumer requirements plays the most important role in the dynamics of the system. Section 4 is devoted to presenting the main dynamic properties of the model; one of them, especially important for its novelty, is the effect that the degree of consumer requirements has on the industrial structure of the market. Empirical evidence of that theoretical relationship is shown in Sect. 5. Finally, in Sect. 6 we present the main conclusions of our study.",4
49.0,2.0,Computational Economics,21 January 2016,https://link.springer.com/article/10.1007/s10614-016-9562-7,An Effective Computational Model for Bankruptcy Prediction Using Kernel Extreme Learning Machine Approach,February 2017,Dong Zhao,Chunyu Huang,Huiling Chen,,Unknown,Unknown,Mix,,
49.0,3.0,Computational Economics,01 February 2016,https://link.springer.com/article/10.1007/s10614-015-9559-7,Algorithmic Representations of Managerial Search Behavior,March 2017,William M. Tracy,Dmitri G. Markovitch,Deepu Philip,Male,Male,Unknown,Male,"Economists increasingly use search algorithms to help model relevant phenomena. Topics addressed using this approach include: government policy and industrial structure (e.g., Kollman et al. 2000; Tracy et al. 2013); the relationship between industrial dynamics and competition (Lenox et al. 2007); firm innovation policy (e.g., Auerswald et al. 2000; Kauffman et al. 2000); organizational structure (e.g., Fang et al. 2010; Siggelkow and Rivkin 2006); and managerial knowledge and learning (e.g., Gavetti and Levinthal 2000; Gavetti et al. 2005). Although there are many different types of search tasks performed by economic agents, the papers cited above utilize models of backward-looking search. Backward-looking search is defined as search conducted without a cognitive map of the underlying topology. Hence these models focus on situations in which decision-making is based on trial-and-error, rather than theory. The majority of work in this area implicitly assumes that search processes are greedy, or non-decreasing in performance; if performance drops after a change to the status quo, that change is rejected (see for example Gavetti and Levinthal 2000; Rivkin 2000). This assumption is at odds with a rich literature on cognitive biases that suggests management teams and decision makers in general do not like to reverse their decisions (Audia et al. 2000; Lowe and Ziedonis 2006; Staw 1976). This extant literature suggests that manager’s cognitive biases could cause firms to preserve changes that decrease performance relative to the status quo. Whether or not agents preserve changes that lower performance can have a significant impact on the behavior of computational models of backward-looking search. The behavior of many of these models depends on agents or organizations “getting stuck” on local optima or on other topological sticking points (Rivkin and Siggelkow 2002). Non-greedy algorithms are less susceptible to this type of outcome. In one of the few computational search studies that test a non-greedy search algorithm, Knudsen and Levinthal (2007) identified a number of organizational structures that performed identically when department-level backward-looking search was modeled using a standard greedy search algorithm. However, they observed that the performance of those organizational structures varied significantly when they tested a non-greedy search algorithm that occasionally preserved performance-decreasing changes. In this paper, we examine experimental data on managerial search behavior. We observe a non-trivial propensity to preserve changes that slightly decrease performance during backward-looking search. However, the likelihood of preservation is inversely related to the magnitude of the performance decrease. While we recognize that the context of the search task impacts the nature of search behavior, our findings suggest that computational researchers should explore alternatives to the search algorithms commonly used. Based on our findings, we propose an alternative search algorithm in which the likelihood of preserving a performance decreasing change is inversely proportional to the magnitude of the performance decrease. Depending on the phenomena being studied, modelers may which to either adopt this modified algorithm outright, or use as in a robustness check.",7
49.0,3.0,Computational Economics,24 February 2016,https://link.springer.com/article/10.1007/s10614-016-9566-3,A note on the Estimation of a Gamma-Variance Process: Learning from a Failure,March 2017,Gian P. Cervellera,Marco P. Tucci,,Male,Male,Unknown,Male,"Since Black and Scholes (1973) seminal article on option pricing, there has been a (n exponentially) growing literature on modeling both asset returns (log price increments) and option prices. Black and Scholes model implies that log price returns are identically and independently distributed as a normal random variable. When used for option pricing, given the assumption of constant parameters in the model, “a minimal prediction of the Black–Scholes formula is that all options on the same underlying asset with the same time-to-expiration but with different striking prices should have the same implied volatility” (Rubinstein 1994, p. 4). Both implications appear to be contradicted by the empirical evidence. Daily log-returns are typically characterized by distributions with kurtosis greater than three. In other words, higher peaks above the mean and thicker tails, or more frequent extreme events, than a normal distribution. Usually, there is little or no autocorrelation in returns, at least for one or two lags, but it is present a long range dependence structure in squared and absolute returns, violating the independence assumption. Then, returns often show periods of low variation followed by periods of higher variation. Namely, they display varying volatility, or heteroscedasticity.Footnote 1 In the options market, at the end of the 1980s the graph of implied volatility as a function of striking price, for otherwise identical options, begins to depart from a horizontal line, maybe as a consequence of the stock market crash of October 1987.Footnote 2 Since then volatility smiles and / or volatility smirks (either forward skew or reverse skew) are common across option markets as remarked in a countless number of studies. To overcome these problems, many models have been proposed. The Variance-Gamma (VG) process is clearly one of the most popular. Introduced as a model for asset returns in Madan and Seneta (1990), it has then been generalized to a non-symmetric process in Madan et al. (1998) and its use extended to option pricing. It belongs to the family of Lévy processes of infinite activity and “like the Poisson process, the VG process is pure jump ... and ... it can be expressed in terms of its Lévy density” (Fu 2007, p. 23).Footnote 3 It is obtained by evaluating a Brownian motion at a random time given by a gamma process. Random time that allows to model the flow of “economically relevant time”, or “market activity time”, reflecting the random speedups and slowdowns in real-time economic and business activity. In other words, “the more share trades that occur, or the more information released to the market on a given day, the faster ‘time’ progresses” (Finlay 2009, pp. 10–11).Footnote 4 “Under this process, the unit period continuously compounded return is normally distributed, conditional on the realization of a random time” (Madan et al. 1998, p. 80), with conditional variance given by a gamma random variable, from here the name VG.Footnote 5 The resulting stochastic process has three (four when the location parameter is included) parameters. In addition to the volatility parameter characterizing the Brownian motion, there is a parameter that determines the percentage excess kurtosis in the log return distribution (i.e. a measure of the symmetric increase in the left and right tail probabilities of the distribution compared to the normal distribution) and one for skewness that allows for asymmetry of the left and right tails of that density. Very conveniently, for testing purposes, it nests the lognormal density and the Black–Scholes formula as a parametric special case. The VG model has been reported to perform better than the Black-Scholes, or geometric Brownian motion (GBM), model in a number of empirical studies. Madan et al. (1998) show that, once calibrated to the market prices, it captures volatility smile and fat-tailness of the asset return distribution. Among the others, Daal and Madan (2005) use it in pricing foreign currency options, Fiorani (2003) in pricing European and American options on S&P 500, Fiorani and Luciano (2006) and Hurd (2007) in credit risk modeling. Given that analytical solutions are available only for European-style options, a lot of attention has been devoted to implementing efficient numerical methods to evaluate other kinds of options.Footnote 6 The progress in this direction and the nice properties of the VG model have led to its implementation in the Bloomberg system through the function SKEW as described in Stein et al. (2007). Recently, VG correlated models have been introduced in Madan and Khanna (2009) and Eberlein and Madan (2010) and versions of the multivariate VG model applied in a large scale application (Wallmeier and Diethelm 2012). New applications of the VG model include its use to study the magnitude of the overpricing of the so-called reverse convertibles in Deng et al. (2013) and, in a regime-switching framework, to value a new type of insurance contract for near to retirement population in Fard and Rong (2014). So far, the estimation problem has not received much attention. Some papers present only few estimated parameters for a small, selected empirical database. Others, see e.g. Finlay and Seneta (2008), use simulated data. Only recently, a few works report estimates based on a broad data set. Rathgeber et al. (2013) consider daily data on the US companies listed in the Dow Jones for the period 01/01/1991-12/31/2011. After applying a regime switching model in order to identify normal and turbulent times within the data set at hand, they estimate the VG model for log returns for the various periods with six alternative estimation methods. In another study, Le Courtois and Walter (2014) estimate the VG parameters on the returns of top French companies and of the French CAC40 Index, over the period 01/03/01-04/15/09, by maximum likelihood (ML). In both cases the authors, consistent with most of the literature on this topic, do not report any computational problems. Does this mean that usually, apart from the occasional need to distinguish between normal and turbulent periods, the estimation of parameters in a VG model for daily log returns is computationally straightforward? And that, as reported in Figueroa-López et al. (2012, p. 19), some problems may be encountered only in finding ML estimates for a VG model applied to very high frequency data (i.e. observations distanced 30 min or less)? Can practitioners use a generally available, well documented econometric software, instead of having to write their own code to estimate their VG model? Some hints of caution about the ‘computational straightforwardness’ can be found in Seneta (2004, pp. 180–183). First, he points out that care has to be taken in computing ML estimates with packages such as Matlab\(^{\textregistered }\) which use implicit functions. Then, in discussing a couple of empirical examples dealing with log daily price differences, he states that “when the original data... were used ... attempts to find ... (the ML estimates of the parameters) were unsuccessful.” Moreover, since April 2012, it is possible to estimate the parameters of a VG model by using the, generally available, ‘VarianceGamma’ R package documented in Scott and Dong (2012). The interested users can compute the parameters of a VG model simply calling the vgFit function incorporated in this package for their data set. The goal of this paper is to shed some more light on the computational complexity of estimating a VG model for daily log returns.Footnote 7 This is done by reporting the author’s experience in trying to replicate the ML estimates presented in Madan et al. (1998, p. 90, Table 1). Therefore the problem of forecasting and the comparison of VG forecasts with forecasts from other models is beyond the scope of the present work. The discussion is organized as follows. After a brief introduction of the VG model (Sect. 2), the problems encountered in replicating Madan et al. (1998) results are described in Sect. 3. At this stage both standard econometric software, such as the vgFit command available in the R package, low level programming languages, such as Matlab\(^{\textregistered }\),Footnote 8 and non-standard optimization software, such as Ezgrad described in Tucci (2002), are used to find the optimum of the log-likelihood function. As in the original reference, the increments in prices are assumed independent so classical statistical procedures for estimation and hypothesis testing are readily applicable. Section 4 describes how sensitive the parameter estimates can be to very small changes of the estimation sample. Then the performances of several procedures using R and Matlab\(^{\textregistered }\) implicit functions are compared on a big data set (Sect. 5). An intuitive procedure which works nicely both when implemented in R and Matlab\(^{\textregistered }\) is presented and applied to Madan et al. (1998) sample in Sect. 6. The main conclusions are summarized in Sect. 7.",2
49.0,3.0,Computational Economics,26 February 2016,https://link.springer.com/article/10.1007/s10614-016-9565-4,Extremal Pure Strategies and Monotonicity in Repeated Games,March 2017,Kimmo Berg,,,Male,Unknown,Unknown,Male,"Repeated games have been studied extensively but it is still an open problem how exactly the set of subgame-perfect equilibria behaves when the players’ time preferences change. Intuitively, it should be easier to support equilibria when the players are more patient, since patience puts more weight to the future payoffs. This makes the deviations less profitable since the continuation payoff is always higher on the equilibrium path compared to the punishment payoff that the player receives if he deviates from the equilibrium strategy. This intuition may, however, lead to wrong conclusions as it ignores one important factor: the punishment payoff may increase when the players become more patient, which may change the player’s incentives and produce a profitable deviation. This paper examines the punishment strategies and the monotonicity of equilibria with respect to the discount factor. Many papers have focused on the properties of the equilibrium payoffs but here we pay special attention to the sequences of actions, i.e., the equilibrium paths. The theory of infinitely repeated games has been developed in Abreu (1988) and Abreu et al. (1986, 1990). These papers characterize the set of subgame-perfect equilibria and show that the monotonicity of payoffs is related to the convexity of the payoff set; e.g., the equilibrium payoffs are convex and monotone when the players can use a public correlating device. Recently, Yamamoto (2010) has shown that without correlated strategies the payoff set is not in general convex nor monotone no matter how patient the players are, which is in sharp contrast to the folk theorem (Fudenberg and Maskin 1986). The non-monotonicity of payoffs is also observed in Mailath et al. (2002), see also Mailath and Samuelson (2006), where they show that the maximum payoff in a prisoner’s dilemma is decreasing for a certain range of discount factors. This property can be easily seen in Fig. 1, which shows the equilibrium payoffs in a prisoner’s dilemma for two different discount factor values: \(\delta =0.4\) is shown by the plus signs and \(\delta =0.45\) (more patient players) is given by the smaller dots. We can see that most of the payoff points move a little when the players become more patient. For example, the maximum equilibrium payoff of player 1 inside the circle is given by the path where player 1 defects in the first stage giving payoff (4, 0) and after that the players keep on cooperating, which gives them payoff (3, 3). The average payoff of player 1 decreases as discounting shifts weight from the first stage payoff of 4 to the later payoffs of 3. The monotonicity of payoffs is related to the discreteness of the payoff set and the properties of average discounted payoffs but not the players’ incentives nor the punishment strategies. Equilibrium payoffs in a prisoner’s dilemma for two discount factors This paper shows that the monotonicity of paths is a more robust property of equilibria. The equilibrium paths are monotone in the discount factor if the punishment payoffs do not increase. This holds, e.g., in any prisoner’s dilemma where the punishment payoffs remain the same for all discount factors. The monotonicity means that the players may design a sequence of actions and if this designed path is an equilibrium for a given level of patience, then it is also an equilibrium when the players become more patient. Moreover, this implies that the set of equilibrium paths may only enlarge. However, it is shown in this paper that the monotonicity of paths does not hold in general. A numerical example is constructed so that the punishment payoff increases when the players become more patient. This result raises a little warning to what may happen in a class of games and emphasizes the importance of the punishment strategies. It should be noted that the non-monotonicity of paths is not totally new observation since Mailath and Samuelson (2006) notice it under imperfect monitoring in Sect. 7.2.2. The relationship between the punishment payoffs and the stage game’s minimax payoffs has received much attention in the literature, primarily related to the folk theorems (Fudenberg and Maskin 1986; Abreu et al. 1994; Wen 1994). The recent work has focused on the case of unequal discount factors (Lehrer and Pauzner 1999; Salonen and Vartiainen 2008; Houba and Wen 2011; Guéron et al. 2011; Chen and Takahashi 2012). Moreover, Gossner and Hörner (2010) examine the lowest equilibrium payoffs when the players cannot perfectly observe the opponents’ actions. In these games, the lowest equilibrium payoff can be strictly lower than the minimax payoff; see Ex. 5.10 in Fudenberg and Tirole (1991). In this paper, we assume perfect monitoring and hence the punishment payoff is never below the minimax value. Many computational methods for repeated games (Cronshaw and Luenberger 1994; Cronshaw 1997; Judd et al. 2003; Burkov and Chaib-draa 2010; Salcedo and Sultanum 2012; Abreu and Sannikov 2014) are based on the set-valued fixed-point characterization of Abreu et al. (1986, 1990). Recently, Berg and Kitti (2015) have shown that the equilibrium paths consist of repeating fragments called elementary subpaths. This has provided a new methodology for analyzing the set of equilibria, computing the pure-strategy payoffs (Berg and Kitti 2013) and identifying the equilibrium payoffs as particular fractals (Berg and Kitti 2014). These papers assume that the punishment payoffs are known but it has turned out that the punishment paths may be very complicated and difficult to find in some games. This paper offers a simple solution for finding the punishment paths, when the set of equilibrium paths is small enough, i.e., when the discount factors are small. A better algorithm using the idea of branch and bound is presented in Berg and Kärki (2014a), but also it has problems finding the punishment paths in certain games. Solving this problem is left for future research and here we focus on examining the properties of the punishment paths. The paper is structured as follows. In Sect. 2, the repeated game model is formulated and the notion of subgame-perfect equilibrium is defined. Section 3 examines the monotonicity of equilibrium paths and payoffs. A numerical method for finding the punishment paths is presented in Sect. 4. Three examples are given in Sect. 5; they demonstrate the punishment payoffs in an oligopoly game, the non-monotonicity of equilibrium paths, and the case of unequal discount factors. Section 6 concludes.",4
49.0,3.0,Computational Economics,27 February 2016,https://link.springer.com/article/10.1007/s10614-016-9567-2,Searching for Inefficiencies in Exchange Rate Dynamics,March 2017,Guglielmo Maria Caporale,Luis Gil-Alana,Alex Plastun,Male,Male,Male,Male,"Pair trading is a technique often used by practitioners to predict short-term price movements and detect arbitrage opportunities. It searches for statistically linked asset pairs and any mis-pricings that can be exploited through arbitrage trading until the divergence in prices disappears. This paper develops a new pair trading method to detect inefficiencies in exchange rate dynamics and arbitrage opportunities that is based on a convergence/divergence indicator (CDI) belonging to the oscillatory class. The proposed technique is applied to 11 exchange rates over the period 2010–2015, and trading rules based on CDI signals are obtained. The suggested approach is of general interest and can be applied to different financial markets and assets. The basic idea is as follows: the degree of correlation between financial assets varies over time, and can be very high in certain periods. For example, the average correlation between EURUSD and AUDUSD in 2015 has been higher than 0.9 at the daily frequency, and in the range [0.8–0.9] if considering hourly intraday data, but at times the hourly correlation has dropped below 0 and even below \(-\)0.5 before reverting to “normal” values. We investigate the reasons for such abnormal situations in the case of the FOREX market using a convergence/divergence indicator (CDI) and show its efficiency in comparison to other popular methods. The layout of the paper is as follows. Section 2 briefly reviews the literature on technical analysis. Section 3 describes the data and outlines the methodology. Section 4 presents the empirical results, while Sect. 5 offers some concluding remarks.",6
49.0,3.0,Computational Economics,03 March 2016,https://link.springer.com/article/10.1007/s10614-016-9569-0,Accurate and Robust Numerical Methods for the Dynamic Portfolio Management Problem,March 2017,Fei Cong,Cornelis W. Oosterlee,,,Male,Unknown,Mix,,
49.0,3.0,Computational Economics,02 April 2016,https://link.springer.com/article/10.1007/s10614-016-9571-6,How Would Bilateral Trade Retaliation Affect China?,March 2017,Chunding Li,,,Unknown,Unknown,Unknown,Unknown,,
49.0,3.0,Computational Economics,04 April 2016,https://link.springer.com/article/10.1007/s10614-016-9578-z,Robust Monte Carlo Method for R&D Real Options Valuation,March 2017,Marta Biancardi,Giovanni Villani,,Female,Male,Unknown,Mix,,
49.0,3.0,Computational Economics,09 April 2016,https://link.springer.com/article/10.1007/s10614-016-9576-1,Numerical Modeling of Dependent Credit Rating Transitions with Asynchronously Moving Industries,March 2017,D. V. Boreiko,Y. M. Kaniovski,G. Ch. Pflug,Unknown,Unknown,Unknown,Unknown,,
49.0,4.0,Computational Economics,03 March 2016,https://link.springer.com/article/10.1007/s10614-016-9568-1,Optimal Influence Strategies in Social Networks,April 2017,Christos Bilanakos,Dionisios N. Sotiropoulos,George M. Giaglis,Male,Unknown,Male,Male,"The rapid growth of online social media (such as Facebook and Twitter) has raised the importance of social interactions for the analysis of consumer behavior. More specifically, consumers often communicate in online social networking platforms to exchange information about the quality of new goods introduced in the market. Indeed, the process of word-of-mouth communication has been identified by recent marketing research as an important informational channel affecting consumers’ purchasing decisions (Iyengar et al. 2011). At the same time, the impact of traditional marketing techniques has been constantly decreasing (Schmitt et al. 2011). In this context, a natural question is to study the various strategies developed by firms to manipulate the flow of information in social networks. A common strategy recently being followed by many firms involves exploiting certain agents’ disproportionate influence on others to accelerate the diffusion of a new product (Hinz et al. 2014). For instance, Ford started a buzz marketing campaign in 2009 to spread information about a new brand (Ford Fiesta) in social media. The company selected a set of influential bloggers, gave a Fiesta to each of them and encouraged them to spread information about the new brand through the use of videos, tweets and blog posts. The campaign was successful, since it had a significantly positive impact both on brand awareness and on the volume of sales. Another prominent example is the online fashion store ASOS, which has recently launched a campaign on Twitter by using the “BestNightEver” hashtag and relying on the popularity of certain youth idols. This enabled the company to achieve a high level of consumer engagement and considerably increase its sales, thus registering its highest return on investment up to date. Such results can provide guidelines to firms for selecting influential agents in online social media and conducting buzz marketing campaigns to build brand loyalty and promote their profitability. However, there is a body of literature which indicates that targeting the most influential agents might not always be the best practice for the diffusion of a product. For example, it has been argued that highly connected nodes receive a great amount of information because of their central position in the network. As a result, these influential agents normally need more time to decide which bits of information to transmit through the network (Dodds and Watts 2004). In this framework, Watts and Dodds (2007) use computer simulations to show that diffusion cascades are mainly driven by easily influenced individuals rather than by influential agents. The conditions under which a monopolist should target the more or less influential nodes of a network have been studied by Galeotti and Goyal (2009) in a theoretical model which studies the optimal advertising strategy of a firm intending to raise consumers’ awareness or perceptions about the level of brand quality. Their analysis shows that the monopolist’s profit-maximizing strategy depends crucially on the content of social interaction. In particular, the firm should seed highly influential agents in the case of proportional adoption externalities but should target marginalized consumers when information about the product disseminates through a word-of-mouth communication process. The model proposed in this paper also focuses on investigating the structural conditions under which the monopolist should target the more or less influential consumers. However, we suggest an entirely different modeling framework—as explained below—and treat the firm as a node participating in the underlying network rather than as an external agent. Influential agents have been generally defined as “individuals who are likely to influence other persons in their immediate environment” (Katz and Lazarsfeld 1955). The main properties of such an “opinion leader” include her competence, her strategic social location and the personification of certain values. Similarly, the concept of a maven refers to someone who has disproportionate influence on others due to her recognized expertise in a particular field (Gladwell 2006). A recent branch of the literature suggests various procedures to identify key players in a social network (Borgatti 2006). The alternative measures representing the centrality of individual agents are thoroughly described in Jackson (2008). In this context, the degree centrality of a node—i.e., the number of her direct neighbors—has often been suggested both as an empirical measure of influence (Hinz et al. 2011) and as an appropriate theoretical tool to define influential agents (Galeotti and Goyal 2009). However, the degree centrality is a simple measure of popularity which does not provide enough information about the strategic location of a node in the network. In order to capture this aspect, the notion of betweenness centrality has been used to characterize influential agents as those who act like bridges and facilitate the flow of information between different groups of nodes (Hinz and Spann 2008). Alternatively, the influence and the strategic position of a node in the network can be measured by her eigenvector centrality, which depends on how much central or important her neighbors are. According to this interpretation, an agent is influential to the extent that she has influential friends (Richardson and Domingos 2002). This measure of influence has been used by Golub and Jacksonm (2010) in the context of a social learning model. A closely related concept is that of Katz–Bonacich centrality applied by Ballester et al. (2006) to identify the key player in a noncooperative network game with local payoff complementarities. Our paper follows (Golub and Jacksonm 2010) by identifying the most influential agent of the network in terms of eigenvector centrality, which corresponds better to iterated influence relationships involved in the process of opinion formation. In general, there are two main approaches to modeling the formation of agents’ beliefs in a social network. The first approach considers a set of fully rational agents using Bayes’ rule to update their beliefs over time (Acemoglu et al. 2011). The second approach assumes that agents are boundedly rational and the process of updating in each period occurs according to an exogenously specified rule of thumb—e.g., by taking weighted averages of neighbors’ beliefs from the previous period (DeGroot 1974; Golub and Jacksonm 2010). This article adopts the relatively more realistic perspective of bounded rationality to model a network of agents who form their beliefs about product quality according to the rule specified in the DeGroot model. We consider a weighted and strongly connected network which includes a monopolistic firm and two consumers (representing two groups or types of customers) buying the good produced by the monopolist. The firm can spend an amount of resources to increase its influence on consumers’ beliefs about the brand quality. This activity can be understood as a costly investment that raises the valuation of the good, thus also increasing the associated market demand. Agents repeatedly communicate in the network to form their beliefs about quality and consumers ultimately make their purchasing decisions given the monopolistic price set by the firm. The proposed formulation results in a linearly constrained, nonlinear optimization problem with a rational objective function, which cannot be analytically tracked unless particular assumptions are made on the exogenous parameters of the model. Therefore, optimization has been computationally conducted through the utilization of the sequential quadratic programming (SQP) approach which constitutes a state of the art method for nonlinear optimization problems. For the benchmark case where all consumers have the same initial beliefs, we analytically and numerically show that the monopolist always invests relatively more in affecting the most influential consumer’s opinion. We also characterize the optimal overall amount of resources that should be allocated by the firm to the activity of manipulating the network. When consumers have different initial beliefs, we rely on the appropriate numerical methods to show that the firm might optimally target the least influential consumer if the latter’s initial valuation of the good is low enough (relative to the other’s valuation). In this context, we conduct a series of experiments showing that the monopolist’s incentives to target the agent with the relatively lower influence increase both with the distance between consumers’ initial beliefs and with the degree of trust attributed on the set of consumers by the firm. In both cases of uniform and non-uniform initial beliefs, the equilibrium valuation of the good and the monopolist’s profit are minimized when consumers’ influences become equal, implying that the firm benefits from the presence of consumers with divergent strategic locations in the network. The above results hold either when the firm faces a binding constraint on total investment or not. In the latter case, however, the underlying forces driving the U-shaped behavior of the equilibrium profit curve are qualitatively different as shown by a decomposition of profit into its two main components (net revenue and investment cost). The study of comparative statics implies that an increase in consumers’ initial beliefs weakens the monopolist’s incentives to manipulate the network and a higher level of trust placed on consumers by the firm has an ambiguous (initially positive but eventually negative) impact on the latter’s equilibrium investment strategy. Finally, we extend our baseline model by considering the monopolist’s strategic motivation to communicate persistently high beliefs throughout the opinion formation process with the aim of increasing the valuation of the good reached in equilibrium. The experiments conducted in this enriched setting verify the main results of our analysis and show that the firm’s incentives to behave in such a strategic manner decrease with consumers’ initial beliefs but increase with the market size, with the available budget and with the direct influence exerted by the most influential consumer on the other. The rest of this article is organized as follows. Section 2 introduces the basic modeling framework and Sect. 3 derives the equilibrium outcome both analytically and computationally. Section 4 studies the implications of equilibrium when the firm faces a binding constraint on total investment and consumers have either uniform or different initial beliefs about brand quality. Section 5 deals with the case of a non-binding constraint on total investment. Section 6 investigates the firm’s strategic incentives to communicate persistently high beliefs in the underlying network. Finally, Sect. 7 concludes and provides directions for future research.",2
49.0,4.0,Computational Economics,07 April 2016,https://link.springer.com/article/10.1007/s10614-016-9579-y,Parallel Optimization of Sparse Portfolios with AR-HMMs,April 2017,I. Róbert Sipos,Attila Ceffer,János Levendovszky,Unknown,Male,Male,Male,"Portfolio optimization was first investigated by Markowitz (1952) in the context of diversification to minimize the associated risk and maximize predictability. Mean reversion is a good indicator of predictability, as a result, identifying mean reverting portfolios has become a key research area (D’Aspremont 2011; Fogarasi and Levendovszky 2013; Sipos and Levendovszky 2013). However, portfolio optimization becomes very complex when introducing cardinality constraints in order to minimize the transaction costs. In this case, optimizing sparse portfolios is proven to be NP hard (Natarajan 1995). The stochastic nature of a mean reverting portfolio is described by the so-called Ornstein–Uhlenbeck process (the solution of the OU stochastic differential equation). Since, this model is rather limited, we need an extension of the stochastic modeling of portfolio values by using the Auto-Regressive Hidden Markov Model (AR-HMM). AR-HMMs are widely used for predictions (Durbin 1998; Hassan and Nath 2005; Jurafsky and Martin 2006; Mamon 2007), furthermore, a multi-state AR-HMM can be considered as a generalization of the traditional OU modeling (Sipos and Levendovszky 2015), which enables us to perform prediction based algorithmic trading on general financial time series. So far, optimization of mean reverting portfolios has been performed subject to maximizing the so-called predictability parameter. In this paper we introduce a new objective function, namely, we maximize the return which can be achieved with a predefined probability. This new objective function gives rise to more profitable portfolios. One of the major changes in the computer software industry made towards high performance simulations has been the move from serial programming to parallel programming. Driven by the insatiable market demand for real-time, high-definition 3D graphics, the Graphics Processing Unit (GPU) has evolved into a highly parallel, multithreaded computational engine (NVIDIA 2014). The GPU is primarily designed for high-speed graphics, which are inherently parallel. The General Purpose GPU (GPGPU) model let the programmer to use this huge computational power for any kind of problems without need for the graphics primitives (Cook 2013). Using the GPU shows growing interest in the field of computational finance also (Pagès and Wilbertz 2011). There are several applications that have clearly proven the success of highly parallel programming, such as option pricing, risk calculation and time series prediction. In this paper, we will use this computational power for time series modeling, identification and portfolio optimization. As a result, the main contribution of the paper are given as follows: (i) we maximize the return achieved with a predefined probability based on the corresponding p.d.f.-s instead of optimizing the predictability parameter (which provides us with a higher profit); (ii) we use the AR-HMM approach to model the underlying time series, which is more general than the OU modeling; (iii) the sparse portfolio optimization is carried out by stochastic search yielding a global and valid optimum; (iv) portfolio optimization and model identification algorithms are tailored to parallel architectures such as GPGPU which paves the way towards a more sophisticated HFT. Finally, trading is perceived as a “walk” in the “buy/sell” action space, which is then tested numerically on randomly generated time series and FOREX rates. The results exhibit good average returns. The structure of the paper is as follows: In Sect. 2, the model and the notations are introduced and the concept of AR-HMM modeling is briefly summarized together with its connection to the mean reverting processes; In Sect. 3, we optimize the portfolio according to the new objective function; In Sect. 4, the computational model is mapped out; In Sect. 5, the parallel implementation details are described; In Sect. 6, a detailed performance analysis is given based on historical data; In Sect. 7, some conclusions are drawn.",3
49.0,4.0,Computational Economics,08 April 2016,https://link.springer.com/article/10.1007/s10614-016-9580-5,A Non-iterative Bayesian Sampling Algorithm for Linear Regression Models with Scale Mixtures of Normal Distributions,April 2017,Fengkai Yang,Haijing Yuan,,Unknown,Unknown,Unknown,Unknown,,
49.0,4.0,Computational Economics,13 April 2016,https://link.springer.com/article/10.1007/s10614-016-9573-4,Adaptive Quadrature for Maximum Likelihood Estimation of a Class of Dynamic Latent Variable Models,April 2017,Silvia Cagnone,Francesco Bartolucci,,Female,Male,Unknown,Mix,,
49.0,4.0,Computational Economics,13 April 2016,https://link.springer.com/article/10.1007/s10614-016-9574-3,The Comparison of Power and Optimization Algorithms on Unit Root Testing with Smooth Transition,April 2017,Tolga Omay,Furkan Emirmahmutoğlu,,Male,Male,Unknown,Male,"Testing stationarity properties of economic variables has attracted a great deal of attention among economists in the last three decades; and as a result, following the seminal work of Dickey and Fuller (1979) many unit root tests have been proposed. These unit root tests can be generally classified under three classes—standard linear unit root tests, unit root tests using a nonlinear framework, and unit root tests that allow for a break in mean and/or trend. This study will concentrate on the third class, namely the unit root tests that allow for break in mean and/or trend. Several researchers including Perron (1989, 1990), Rappaport and Reichlin (1989), Zivot and Andrews (1992), Lumsdaine and Papell (1997), and Bai and Perron (1998) have recognized alternative trend specifications in testing for the unit root hypothesis. This strand of literature has focused on models with segmented line trends; and single or multiple breaks (Vougas 2006). Yet, another strand of literature has developed unit root tests where the alternative hypothesis is that of stationarity around a smoothly changing trend. Leybourne et al. (1998) (LNV, hereafter) and Sollis (2004) used logistic trend functionsFootnote 1 that allow for a smooth break in the deterministic trend of the data. Bierens (1997) modeled nonlinear trend using Chebyshev polynomials, while Becker et al. (2006) used trigonometric functions (via means of Fourier transformations) to model possible gradual breaks in the data generating process. Omay (2015) extends this integer frequency flexible Fourier form (IFFFF) unit root test to fractional frequency, thereby proposing a new test that is a generalization of the IFFFF type of unit root tests. The use of either Chebyshev polynomials or trigonometric functions might be problematic, because there is no unique way of choosing the order of polynomials or the frequency components for the trigonometric functions. However, in the case of logistic trend functions the parameters of interest in the gradually changing trend function may be estimated using a convenient nonlinear estimation algorithm. By the same token, smooth transition regression (STR) models have also been proved to capture gradual structural breaks quite well (e.g., Granger and Teräsvirta 1993; Lin and Teräsvirta 1994; Greenaway et al. 1997). Moreover, the STR type trend modeling can incorporate broken or unbroken trend lines, thereby allowing for gradual as well as abrupt break (Vougas 2006). Along these lines, the STR type of trend modeling can also be seen as a generalization of the first strand of trend modeling. Due to all these reasons, researchers should focus on STR type of trend modeling more seriously. Fortunately, Vougas (2006) has investigated the neglected numerical issues that necessitate the re-calculation of critical values for the LNV tests. The comparison of alternative estimation algorithms is yet another important issue within the context of LNV type unit root tests. Chan and McAleer (2002)Footnote 2 and Maugeri (2014) have compared the nonlinear optimization algorithms in the STAR-GARCH models and in non-linear co-integration frameworks, respectively. According to these authors, concentrating the sum of squares method and some other popular nonlinear optimization algorithms were not found as efficient as they were expected to be. Therefore, the comparison of alternative nonlinear optimization algorithms for unit root estimation serves as an important research question that has only been partially answered so far in the literature. Nonlinear estimation inherits various problems such as convergence and good starting values. These problems may represent an obstacle for the estimation process like the inability to find global optima. Consequently, these kinds of problems lead to biased and inefficient estimates of the true data generating process. In this study, our main focus is on the LNV type unit root tests which naturally encounter these kinds of problems due to the STR type nonlinearity inherent in the deterministic components of the testing process. For this purpose, we organize a simulation study by generating data under the alternative hypothesis of the LNV unit root tests and compare the estimation results of different optimization algorithms by employing statistical tools such as mean square error (MSE) and root mean square error (RMSE). Comparing the estimation results of the nonlinear data generation processes by using different optimization algorithms is crucial to measure the losses and gains (by means of MSE and RMSE) of the specific type of optimization algorithm employed. Such an analysis is therefore needed to obtain better critical values and to carry out power and size analysis at a minimum cost where the minimum cost principle forms the basis of virtually every statistical analysis of data.Footnote 3
 The purpose of this paper is to investigate the performance of the various optimization algorithms that can be applied to the LNV type of unit root testing. The six methods analyzed in this study are BFGS, Gauss–Jordan, Simplex, Genetic, sequential quadratic programming (SQP) and extensive grid-search (EGS). We have especially included in this study the algorithms that are commonly used within the context of the LNV type of nonlinear unit root tests, and found out that derivative free methods have advantage over hill climbing methods while obtaining the accurate critical values. On the other hand, increasing the complexity of data generating process simple algorithms has advantages over more complex algorithms in computing the parameter estimates. In order not to restrict the analysis to the logistic function, we prefer to include other types of trend functions as well. In the smooth transition autoregressive literature there are two popular choices of the transition function, \(F\left( {s_t ;\gamma ,c} \right) \), namely the logistic function and the exponential function.Footnote 4 To our knowledge, no unit root test has been developed so far in the literature that uses an exponential smooth transition (EST) type of trend function. Therefore, to extend our analysis we propose a new unit root test that includes EST type of trend function in its testing process, following the LNV methodology. In this type of smooth transition trend, SQP has advantages for obtaining unbiased estimates over all other optimization methods. The second best methodology for this type of trend function is found to be again the Genetic algorithm. Therefore, the conclusion drawn for the LNV type of unit root testing seems to be holding true for the unit root tests that include EST type of trend function as well. In the LNV type of unit root tests, Genetic algorithm produces the best results with respect to unbiased estimators and the SQP is the second best algorithm. However, the most important finding of this paper is that the simple EGS methodology, which we propose in this study, can produce the best result with respect to accuracy and the biasedness when the grid area is increased by decreasing the magnitudes of increments (or grid intervals). For the EGS method we obtained the RMSE by fixing the computation time. However, we had to reorganize the grid search intervals while fixing the computation time. When we decreased the increments for the grid search intervals we obtained the best results with respect to RMSE, but this took too much computation time. From these experiments, we can feel free to say that the best method with respect to accuracy is the extensive grid search (EGS). For the other methods we increased the iteration number, but this intervention did not change the RMSE. In addition, we used 0.0001 convergence criteria for all the other optimization algorithms, and increasing or decreasing these criteria did not change the RMSE computations in the acceptable region [0.01, 0.0000001]. Moreover, we realize that there is a gap in the unit root studies that the newly proposed tests are not analyzed between each other’s data generating process (DGP). By using these new findings, we employ Genetic algorithm to obtain the power analysis. Hence, we investigate the power comparison of different nonlinear unit root test under various DGP such as state dependent nonlinearity namely Kapetanios, Shin and Snell (KSS) and Enders and Granger (EG) test.Footnote 5 We have obtained interesting results such as LNV type unit root test can manage to capture state dependent nonlinearity when the transition speed is high. On the other hand, KSS test is found to be very sensitive to structural breaks due to the reason that it is an approximated test from a nonlinear model. Besides, increasing the structural break parameter further deteriorates the power features of the KSS test. Another interesting result is obtained when we use the newly proposed unit root test, which includes the EST type of trend function. In the entire parameter region ADF test is the most powerful test with respect to all other nonlinear tests. Therefore, this type of unit root testing is not powerful and this is probably why it is not used in the previous literature. Furthermore, as stated in Leybourne et al. (1998) the NLS estimation of the parameters \(\gamma \) and \(\tau \) does not allow for closed-form solutions, it would be extremely hard to find any analytical relationship between the \(\hat{{\varepsilon }}_t \) and \(y_t \). Therefore this issue leads the determination of the null asymptotic distribution of the test statistics by analytical means more or less intractable (Leybourne et al. 1998). Leybourne et al. (1998) also stated that the linearity property in the intercept and trend terms ensures that the residual \(\hat{{\varepsilon }}_t \) from all the models are invariant to the choice of the starting values (i.e., \(\mu _0 =\psi )\) and that Model A, B and C are invariant to both starting values and the drift term \(\kappa \). Therefore, the obtained critical values are invariant to the parameters \(\gamma \) and \(\tau \), and as a result invariant to \(\alpha \) and \(\beta \). Finally, we have analyzed this issue in the Appendix 1 part by using a simulation study and conclude that the claims of Leybourne et al. (1998) is true by using the Genetic algorithm. Finally, we have used the Australian real interest rate parity hypothesis (RIPH) to empirically verify the results that we have obtained in the simulation studies. The unit root test results obtained varied a lot depending upon the type of optimization algorithm used, but the RIPH was found to hold only when the Genetic algorithm was utilized. Thus, the results of the empirical study have also corroborated the findings of the simulation studies carried out in the paper. The rest of the paper is structured as follows. Section 2 introduces and discusses the various optimization algorithms employed in the study. Section 3 investigates the performance of the alternative optimization algorithms through Monte Carlo simulation studies. It reports and compares the critical values of the LNV, Sollis (2004) and newly proposed unit root tests obtained by employing each optimization algorithm along with the biases of the estimated parameters for each alternative optimization method under the alternative hypotheses of these tests and employ power analysis. Section 4 provides an empirical application of the aforementioned optimization methods to the Australian real interest rate parity hypothesis (RIPH). Section 5 is reserved for concluding remarks.",15
49.0,4.0,Computational Economics,11 May 2016,https://link.springer.com/article/10.1007/s10614-016-9582-3,"Simple Agents, Intelligent Markets",April 2017,Karim Jamal,Michael Maier,Shyam Sunder,Male,Male,Male,Male,"A central feature of economic theory is derivation of equilibrium in economies populated by agents who optimize some well-ordered function such as profit or utility. Although it is recognized that actions of economic agents are subject to institutional constraints and feedback (North 1990), exploration of the extent to which equilibrium arises from characteristics of the institutional environment, as opposed to the behavior of individuals, has been limited; Becker’s (1962) derivation of downward slope of demand functions is a notable exception. The normal modeling technique is to ascribe sophisticated computational abilities to a representative agent to solve for equilibrium (Muth 1961). Plott and Sunder (1982, henceforth PS) have shown that markets with uncertainty and asymmetrically distributed information (with two or three states of the world) disseminate information and converge near rational expectations equilibria when populated with profit-motivated human traders. The present paper asks if the PS results can also be achieved by minimally intelligent traders (Gode and Sunder 1993) using the means-end heuristic and reports an affirmative answer. Simon (1969, Chapter 3) questioned the plausibility of human agents, with their limited cognitive abilities, forming rational expectations by intuition. Accumulated observational evidence on these cognitive limits of individuals shifted the burden of proof and led to calls for evidence that markets can overcome such behavioral limitations (Thaler 1986; Tversky and Kahneman 1986). Laboratory studies of markets populated by asymmetrically-informed profit-motivated human subjects reveal that their aggregate level outcomes tend to converge near the predictions of rational expectations theory (Forsythe and Lundholm 1990; Forsythe et al. 1982; Plott and Sunder 1988). However, since complex patterns of human behavior can only be inferred, not observed directly, it is difficult to know from human experiments which elements of trader behavior and faculties are necessary or sufficient for various markets to attain their theoretical equilibria.Footnote 1 This difficulty has led to claims that the inability of humans to optimize by intuition implies that economic theories based on optimization assumptions are prima facie invalid [for example, Tversky and Kahneman (1986)]. Such doubts about the achievability of mathematically derived equilibria, when individual agents are not able to perform complex optimization calculations are understandable. From a constructivist point of view (Smith 2008), rational expectations equilibria place heavy demands on individual cognition to learn others’ preferences or strategies, and to arrive at unbiased estimates of underlying parameters of the economy by observing markets. In theory, disseminating and detecting information in markets calls for bootstrapping—rational assessments are necessary to arrive in equilibrium and such assessments require observation of equilibrium outcomes. Cognitive and computational demands on individuals to arrive at economic equilibria, especially rational-expectations equilibria, are high, raising doubts about the plausibility of equilibrium models (Simon 1969). Replacing humans with algorithms allows us to examine whether the use of certain simple heuristics by individual traders is sufficient for attaining rational expectations equilibria (as a proof of concept). Without claiming that human traders actually use such heuristics, it is possible and useful to determine if heuristics making low computation demands on human reasoning might be sufficient for attaining equilibria in a given market environment. Combining Newell and Simon’s (1972) means-end heuristic with Gode and Sunder’s (1993, 1997) zero-intelligence (ZI) approach, we find and report that markets with uncertainty and asymmetric information attain outcomes approximating rational expectations equilibria, even when they are populated by simple minimally-intelligent adaptive algorithmic traders. Since the statistical distribution of these outcomes is centered near the PS observations of markets with human traders, the convergence of their outcomes to equilibrium can be attributed to the combination of the market structure and the minimal levels of intelligence and adaptive ability built into the trading algorithms. Since these trader faculties are far less demanding than what is assumed in deriving the equilibria, and certainly within the known human capabilities, we infer that the convergence of markets to rational expectations equilibria emerge mainly from the properties of the market and simple and plausible decision heuristics, rather than from complex and sophisticated optimization (Becker 1962; Gode and Sunder 1993; Gigerenzer and Todd 1999; Smith 2008).",2
49.0,4.0,Computational Economics,12 September 2016,https://link.springer.com/article/10.1007/s10614-016-9623-y,"Forecasting Bank Failure: Base Learners, Ensembles and Hybrid Ensembles",April 2017,Aykut Ekinci,Halil İbrahim Erdal,,Male,Male,Unknown,Male,"There is a strong relationship between the bank failures and economic growth, especially in developing countries (see Ramirez and Shively 2012). Turkish economy has faced deep financial crises in 1994, 1999 and 2001. Banks were increasingly investing in government bonds, also taking huge currency mismatches and interest rate risk in 1990s. Bank Ekspres, Egebank, Esbank, Interbank, Sumerbank, Yasarbank and Yurtbank were transferred to the savings deposit insurance fund (SDIF) because of 1999 economic crisis and Bank Kapital Turk, Etibank, Demirbank, Iktisat Bank, Tarisbank, Bayındırbank, EGSbank, Kentbank, Sitebank and Toprakbank were transferred to the SDIF because of the 2001 financial crisis. The total cost of banking operations, i.e. government bonds issued by the Turkish Treasury, advances granted by the Central Bank of the Republic of Turkey (CBRT) and the SDIF’s own resources, have reached to $16.9 billion (see Canbas et al. 2005). It can be seen that the forecasting bank failure could decrease the total cost of bailout operations and mitigate the harmful effects of bank default. Firstly, statistical models such as linear, multivariate or quadratic discriminant analysis, factor analysis and logistic regression methods were used in the literature (Meyer and Pifer 1970; Sinkey 1975; Martin 1977; West 1985; Kolari et al. 2002). As a next step, intelligent models such as artificial neural networks, operations research, evolutionary approaches, hybrid intelligent methods, fuzzy logic and support vector machines have been preferred over statistical models due to their better predictive success of bank failures (Tam 1991; Tam and Kiang 1992; Olmeda and Fernandez 1997; Bell 1997; Swicegood and Clark 2001; Canbas et al. 2005; Ravi and Promodh 2008; Ravi et al. 2008; Boyacıoglu et al. 2009; Ekinci and Erdal 2011; Erdal and Ekinci 2013). As a last step, researchers focus on ensemble models because of their capabilities of integrating multiple predictions (see Shin et al. 2006; Ramu and Ravi 2009; Verikas et al. 2010; Ravi and Promodh 2010; Kima et al. 2010; Paramjeet et al. 2012). In this paper, we use base learner, ensemble and hybrid ensemble models to predict bank default in Turkish banking system with 37 privately owned commercial banks (17 failed, 20 non-failed) for the period of 1997–2001. This paper is organized as follows: the second section discusses the methods; the third section presents data, empirical settings and empirical findings. The paper ends with some brief concluding remarks.",27
50.0,1.0,Computational Economics,16 April 2016,https://link.springer.com/article/10.1007/s10614-016-9575-2,Detection of Mispricing in the Black–Scholes PDE Using the Derivative-Free Nonlinear Kalman Filter,June 2017,G. Rigatos,N. Zervos,,Unknown,Unknown,Unknown,Unknown,,
50.0,1.0,Computational Economics,06 July 2016,https://link.springer.com/article/10.1007/s10614-016-9577-0,WorkSim: A Calibrated Agent-Based Model of the Labor Market Accounting for Workers’ Stocks and Gross Flows,June 2017,Olivier Goudet,Jean-Daniel Kant,Gérard Ballot,Male,Unknown,Male,Male,"The model WorkSim is a novel tool of analysis for labor markets. The first objective of the model is to reproduce the gross flows between the important states: employment (distinguishing fixed term contracts and open ended contracts), unemployment and inactivity, and the ratios of individuals in these states. The novelty of the model is that it simulates the gross flows on the basis of the rational decisions of individual heterogeneous agents. The gross flow concept is crucial because each flow unit is caused by a decision that involves comparing idiosyncratic expected benefits and costs for the agent, and a flow unit will yield idiosyncratic benefits and costs for the agent and possibly other agents. It is not the case for transition based models that imply some time aggregation. Once the model is calibrated, the second objective is to characterize the nature of the labor market under study. This is done, first by examining the patterns of flows and stocks at the aggregate level and at the levels of different categories of labor, second by sensitivity experiments, modifying some exogenous parameters and variables such as the demand for the good. Finally the model once calibrated is a tool for experimenting labor market policies, including changes in the labor law. The multi-agent methodology is the perfect tool for such a research program, since it can model institutions precisely, and account for heterogeneity and individual interactions. Simulation results enable us to compute aggregate variables such as the flows and the stocks, and finally the individual careers and the main types of trajectories. However, the labor market is complex and this means that the modeling progresses only by steps. The present version is consistent as a stock-flow model and more detailed than other existing stock-flow models of the labor market, analytic, econometric, or multi-agent. The model builds on the experience of model ARTEMIS proposed by Ballot (1981, (1988, (2002) and a preliminary version of WorkSim by Lewkovicz and Kant (2008). ARTEMIS is the first multi-agent model to have modeled the gross flows between the three main states of the individuals, with the addition of on-the-job search as a state. This was also done within an institutional framework, notably with a temporary help firm, and firing costs. The accounting framework of stocks and flows allowed for a rigorous analysis of the competition between the different categories of labor. It threw some light on the effects of aggregate shocks or institutional change on the displacement or integration in open ended contracts of such categories as the young workers, female workers, low educated workers. The underlying hypothesis, that results confirm, is that these effects on the gross flows and stocks are highly non linear, or even non monotonic, and difficult to obtain through available econometric methods. For instance, a negative demand shock could possibly lower the unemployment rate of young non educated workers who would abandon participation, but raise unemployment for the other workers. The version of WorkSim presented in this artical aims to analyze the French labor market in 2011. However the methodology we have developed will enable researchers to use it for other countries as well. WorkSim puts emphasis on one of the most important features of the French labor market that is the major role of the fixed term contracts, about 80 % of the hires in 2011. The present version is mainly devoted to the reproduction of the flows on the basis of our modeling of rational decisions. It then provides a first characterization of the patterns of flows of the different categories of workers, which is key for understanding the nature of a labor market, letting policy design for future work. Due to lack of space, we mainly restrict our economic analysis to the observation of a segmentation, and then throw a first light on the fundamental question: is the segmentation of a temporary or permanent nature for a generation of individuals? This paper is organized as follows. Section 2 will present the theoretical framework and related models, Sect. 3 will develop the model. Section 4 will deal with the calibration procedure, and Sect. 5 the first characterization of the French labor market on the basis of the results. Section 6 concludes.",5
50.0,1.0,Computational Economics,21 April 2016,https://link.springer.com/article/10.1007/s10614-016-9581-4,AdaBoost Models for Corporate Bankruptcy Prediction with Missing Data,June 2017,Ligang Zhou,Kin Keung Lai,,Unknown,,Unknown,Mix,,
50.0,1.0,Computational Economics,12 May 2016,https://link.springer.com/article/10.1007/s10614-016-9583-2,A Recursive Method for Solving a Climate–Economy Model: Value Function Iterations with Logarithmic Approximations,June 2017,In Chang Hwang,,,,Unknown,Unknown,Mix,,
50.0,1.0,Computational Economics,03 June 2016,https://link.springer.com/article/10.1007/s10614-016-9584-1,Wavelets Analysis on Structural Model for Default Prediction,June 2017,Lu Han,Ruihuan Ge,,,Unknown,Unknown,Mix,,
50.0,1.0,Computational Economics,25 May 2016,https://link.springer.com/article/10.1007/s10614-016-9585-0,Online Portfolio Selection Strategy Based on Combining Experts’ Advice,June 2017,Yong Zhang,Xingyu Yang,,,Unknown,Unknown,Mix,,
50.0,1.0,Computational Economics,26 May 2016,https://link.springer.com/article/10.1007/s10614-016-9586-z,Finite Sample Critical Values of the Generalized KPSS Stationarity Test,June 2017,Peter Sephton,,,Male,Unknown,Unknown,Male,"It has been over twenty years since Kwiatkowski et al. (1992) provided a test of whether a series is stationary (henceforth the KPSS test), and as is the case with unit root tests, while the asymptotic properties of the test are well defined, its behavior in small samples is less-well understood. Sephton (1995) estimated response surfaces to construct small sample critical values of the test, following MacKinnon (1991, (1994) seminal work on the critical values of popular unit root and cointegration tests. The KPSS test has been studied extensively.Footnote 1 It has been extended to panel data (see Hadri 2000), among others); its performance has been studied in the presence of structural change and breaks (Harvey and Mills 2004, Carrion-i-Silvestre and Sanso 2007, Sephton 2008a); its potential size distortion has been examined (Choi 2009), Cappuccio and Lubian 2010); and it has been used in joint unit root-stationarity tests to guide inference (Charemza and Syczewska 1998,Carrion-i-Silvestre et al. 2001, Keblowski and Welfe 2004). While the asymptotic critical values of the KPSS test statistics are simple to construct, finite sample critical values are almost always necessary in applied work. In an important paper, Hobijn et al. (2004) extended the KPSS test to the case of zero-mean stationarity and they demonstrated the relative merits of alternative approaches to estimating the long-run variance of the series. The purpose of this paper is to update the response surface estimates of the original KPSS test contained in Sephton (1995), and to provide new response surfaces of the generalized KPSS (henceforth the GKPSS test) test of Hobijn et al. (2004) for use in applied work. While applied researchers may prefer to bootstrap the distribution of the test statistic, in practice, finite sample critical values are an attractive first-step to performing inference. For example, the popular econometrics package GRETL reports finite sample critical values drawn from response surfaces in Sephton (1995), but it currently does not include an option to bootstrap the test statistics. Moreover, it is only recently that researchers have examined the performance of the bootstrapped KPSS test. Popular bootstrapping methods include the sieve bootstrapFootnote 2 and the stationary bootstrap.Footnote 3 Lee and Lee (2012) argue that the sieve bootstrap can refine the finite sample performance of the KPSS test and in empirical applications, demonstrate that bootstrapped critical values are larger in value than the asymptotic critical values, providing stronger evidence of stationarity. In an exhaustive simulation study, Gulesserian and Kejriwal (2014) examine the performance of both the KPSS and GKPSS tests under a variety of data generating processes, and find that the stationary bootstrap outperforms the sieve bootstrap. They show that the stationary bootstrap has power functions that increase with the sample size, unlike the sieve bootstrap, likely due to the fact that the sieve bootstrap does not impose the null hypothesis when generating the bootstrap samples. Given the widespread use of the KPSS test and the relatively specialized knowledge one requires to implement a stationary bootstrap (with or without the choice of data-dependent block size due to Patton et al. 2009), response surface estimates of the finite sample critical values of the test are still of interest to the applied researcher. The next section briefly reviews the construction of the test statistics, and is followed by a discussion of the response surface regressions derived from a series of Monte Carlo simulations. An application to the 10 year Treasury Constant Maturity rate demonstrates the merits to this approach.",10
50.0,2.0,Computational Economics,16 June 2016,https://link.springer.com/article/10.1007/s10614-016-9598-8,LSM Algorithm for Pricing American Option Under Heston–Hull–White’s Stochastic Volatility Model,August 2017,O. Samimi,Z. Mardani,F. Mehrdoust,Unknown,Unknown,Unknown,Unknown,,
50.0,2.0,Computational Economics,30 August 2016,https://link.springer.com/article/10.1007/s10614-016-9605-0,A Numerical Method to Approximate Multi-Asset Option Pricing Under Exponential Lévy Model,August 2017,Leila Khodayari,Mojtaba Ranjbar,,Female,Male,Unknown,Mix,,
50.0,2.0,Computational Economics,11 August 2016,https://link.springer.com/article/10.1007/s10614-016-9606-z,Dynamic and Asymmetric Contagion Reactions of Financial Markets During the Last Subprime Crisis,August 2017,Wei Zhou,,,,Unknown,Unknown,Mix,,
50.0,2.0,Computational Economics,03 September 2016,https://link.springer.com/article/10.1007/s10614-016-9607-y,"Contrarian Behavior, Information Networks and Heterogeneous Expectations in an Asset Pricing Model",August 2017,Tomasz Makarewicz,,,Male,Unknown,Unknown,Male,"In this paper, I study the effect of information networks on learning in a non-linear asset pricing model. The main question of this work is how agents react to optimism or pessimism of their friends: do they learn herding, or rather contrarian behavior? And how does this affect the emerging market dynamics and stability of the fundamental equilibrium? In order to address this issue, I consider a model, in which the agents apply genetic algorithm (GA) to optimize a simple linear price forecasting rule. The agents learn whether to follow the observed price trend, but also whether to trust the past trading decisions of their friends, which allows for an explicit learning of either herding or contrarian behavior. One of the fundamental debates of contemporary economics is whether economic agents can learn rational expectations (RE), that is model-consistent predictions of future market prices (Muth 1961). In the context of financial markets, RE require the agents to form a self-fulfilling infinite sequence of forecasts (Blanchard and Watson 1982). Even under a linear relation between the expectations and realized prices, this task is far from being trivial (Sims 2002; Lubik and Schorfheide 2003; Anderson 2008). Furthermore, agents may face additional information constraints (Kormilitsina 2013) or lack the knowledge about the underlying law of motion of the market (Carravetta and Sorge 2010), which further complicates finding the RE solution. Therefore, real economic actors may be unable to bear the cognitive load that is required to form RE. In this case, they would have to learn to predict prices, which does not necessarily lead to ‘as if’ rational expectations equilibria (Hommes and Zhu 2014; Anufriev et al. 2015). Among other evidence, experiments suggest that people use simple forecasting heuristics (Hommes 2011; Heemeijer et al. 2009; Hommes et al. 2005). In the case of asset markets, this leads to price oscillations that repeatedly over- and undershoot the fundamental (RE) equilibrium. Nevertheless, many economists question the empirical validity of such experiments, as these are based on economies with no or limited information flows between the agents. An informal (yet popular) belief is that in real financial markets the agents can share knowledge about efficient and inefficient trading strategies, and so an information network facilitates convergence towards RE. Informal information sharing is indeed common among financial traders (Nofsinger 2005; Bollen et al. 2011). Being closer to the core of an information network leads to higher profits (Cohen et al. 2008), but some researches have also argued that networks are a cause of herding (Shiller and Pound 1989; Acemoglu and Ozdaglar 2011). The latter argument became popular after the 2007 crisis in the non-academic discussionFootnote 1 and in behavioral economics (for example Akerlof and Shiller 2010, refer to animal spirits as the driving force of financial bubbles). This leads back to the two conflicting views on the rationality of financial agents, which can be associated with Keynes (Akerlof and Shiller 2010) and Muth (1961). How important is therefore herding for market stability, and is herding propagated by information networks? There is no clear answer neither from theoretical nor from empirical work. Herding is understood as behavior such that individuals, facing strategic uncertainty, disregard their individual information and instead follow the ‘view of the others’, such as the mood of their friends or general market sentiment (see also Sect. 2.6 for a detailed discussion). From the RE perspective, market price should contain all necessary information about the stocks, hence perfect rationality rarely leaves room for herding or networking. The exception would be the case of significant private information (Park and Sabourian 2011) or sequential trading (with the famous example of information cascades, see Anderson and Holt (1997), for a discussion), but neither approach has a clear empirical motivation. Alternatively, models that depart from rational expectations often investigate some form of social learning. The seminal paper by Kirman (1993) stands as the benchmark for the studies of economic herding (see Alfarano et al. 2005, for a more recent example and a literature review). In this ‘ant-model’, agents are paired at random and imitate each others choices with some exogenous probability, which leads to interesting herding dynamics. The problem with this approach, however, is that individual imitation is assumed as given, instead of being learned by the agents. Another approach comes with the classical Brock-Hommes heuristic switching model (HSM; Brock and Hommes 1997), in which agents coordinate on price prediction heuristics that have a better past forecasting performance. A more general, agent-based counterpart of HSM comes with genetic algorithm (GA) based models of social learning (see Arifovic et al. 2013, for an example and a literature overview). This approach can explicitly account for social learning (agents switch to better strategies), but does not fit our intuition of herding, which is understood as following the beliefs of others, instead of using similar trading or forecasting strategies. Empirical studies give ambiguous results on the existence or importance of herding, with the main issue being that such behavior cannot be directly observed in market data. Chiang and Zheng (2010) show that the stock indices between industries are sometimes more correlated than the fundamentals would imply, which can be understood as a sign of herding. However, this effect is absent in Latin America and US data, and its interpretation is subject to debate. An alternative approach is to use experiments, where the information structure is controlled by the researcher. This leads to a surprising result, however: experiments suggest that contrarian (anti-herding) behavior is more common than herding. Two such experiments, both including professional traders as subjects, were reported by Drehmann et al. (2005) and Cipriani and Guarino (2009). As a result, existence of herding or contrarian behavior in financial markets is not a clear-cut fact. Furthermore, it is not clear whether herding would bring economic agents closer to the rational outcome, or rather to volatile price dynamics (Shiller and Pound 1989). In order to understand the empirical evidence, we require a theoretical inquiry into how herding or contrarian behavior may be learned. Furthermore, such a learning may depend on the structure of ‘information flows’, and thus we need to study it in a context of different information networks. Recent years have seen a rising popularity of studies devoted to the agent interaction within networks. Probably the most famous example comes with the Siena model (Snijders et al. 2010), which offers a valuable insight into peer effects on the individual decision making. In the financial literature, researchers became interested in the balance sheet interconnections of financial agents such as banks (in’t Veld and van Lelyveld 2014; Fricke and Lux 2014). Nevertheless, theoretical studies of networks in the economic literature typically focus on static environments (such as the mentioned balance sheet connections). Models with explicit learning and expectations formation are scarce, and tend to rely on simple behavioral rules of imitation, since adding realistic learning features into such models easily makes them analytically intractable (see Jadbabaie et al. 2012, for a discussion). To the best of my knowledge, Panchenko et al. (2013) (henceforth PGP13) are the only authors who conduct a full-fledged theoretical study of the effect of the information network on expectation formation in an asset pricing model. The authors use a HSM model to show that price oscillations are not tamed by the presence of the network. Their interesting paper is, however, subject to some limitations from the perspective of the research problem of this paper. First, in PGP13 the agents can only choose between two predefined forecasting heuristics, leaving little space for any real learning. Second, these two heuristics (chartist and fundamental rules) do not depend on the decisions or beliefs of agent’s friends, but only on realized aggregate market conditions (prices). Third, in PGP13 the network provides information about the realized profitability of the two heuristics, but no insight into the beliefs of one’s friends. The agents are then assumed to fully incorporate this information into their heuristic choice, instead of learning whether to trust it or not. As a result, PGP13 offer a valuable insight into spread of chartist strategies, but not into learning of herding or contrarian behavior itself, which is the topic of this paper. Finally, PGP13 limit their attention to random networks, whereas information networks in many real markets may be much more complicated. The goal of my paper is to investigate a much more involved learning model. I will use the GA model proposed by Anufriev et al. (2015) (henceforth AHM15), which explains well the individual forecasting heterogeneity of Learning to Forecast experiments. This approach has two advantages: (1) I will work with a realistic, experimentally tested model and (2) I will obtain further insight into the original experiments: to what extent their results (such as the price bubbles) depend on the lack of information networks. The GA model by Anufriev et al. (2015) is an agent-based model (ABM) based on the work of Hommes and Lux (2011). Its idea is that agents, who are asked to predict a price, follow a simple linear forecasting rule, which is a mixture of adaptive and trend extrapolation expectations. This rule requires specific parametrization, and each agent is endowed with a list of possible specifications of the general heuristic. The agents then observe the market prices and update the list of rules with the use of the GA stochastic evolutionary operators. For instance, if the market generates persistent price oscillations, the agents will experiment with higher trend extrapolation coefficients. Since the agents use the GA procedure independently, the model allows for explicit individual learning. Anufriev et al. (2015) show that the model replicates well the experimental degree of individual heterogeneity, as well as aggregate price dynamics. In this paper I extend the GA model of Anufriev et al. (2015), to include an information network in such a way that agents may learn herding or contrarian strategies. Agents observe the past trading behavior of their friends and can learn whether to trust it, just as they learn whether to extrapolate the price trend. The model by its ABM structure can evaluate the effects of different, also asymmetric, information networks on price dynamics. Furthermore, the model explicitly accounts for individual learning, and so I can also study the formation of herding/contrarian behavior at the individual level. The paper is organized in the following way. Section 2 will introduce the theoretical agent-based model, describing a two-period ahead non-linear asset pricing model with GA agents and robotic trader. The third section will present the parametrization of the model, including the investigated network structures, and the setup of the Monte Carlo numerical study of the model. Section 5 will be devoted to small networks of six agents, with which I will highlight the emerging properties of individual learning and resulting price dynamics. The fifth section will move to large networks of up to 1000 agents. Finally, the last section will sum up my results and indicate potential extensions.",4
50.0,2.0,Computational Economics,03 September 2016,https://link.springer.com/article/10.1007/s10614-016-9617-9,"A Practical, Accurate, Information Criterion for Nth Order Markov Processes",August 2017,Sylvain Barde,,,Male,Unknown,Unknown,Male,"The rapid growth in computing power over the last couple of decades, combined with the development of user-friendly programming languages and an improvement of fundamental statistical and algorithmic knowledge have lead to a widening of the range of the computational methods available to researchers, from formal modelling to estimation, calibration or simulation methodologies. While this multiplication of available methods has offered a greater modelling flexibility, allowing for the investigation of richer dynamics, complex systems, model switching, time varying parameters, etc., it has come at the cost of complicating the problem of comparing the predictions or performance of models from radically different methodological classes. Two recent examples of this, which are by no means exclusive, are the development of the dynamic stochastic general equilibrium (DSGE) approach in economics, and the increase in the popularity of what is generally referred to as agent-based modelling (ABM), which uses agent-level simulations as a method of modelling complex systems, and for which even the issue of bringing models to the empirical data can prove to be a problem. Within the DSGE literature on model validation and comparison, one of the first to identify and address this problem in a direct and systematic manner is Schorfheide (2000), who introduces a loss function-based method for evaluating DSGE models. This is then complemented by the DSGE-VAR procedure of Del Negro and Schorfheide (2006) and Negro et al. (2007), which explicitly sets out to answer the question ‘How good is my DSGE model?’ (p. 28). The procedures gradually developed over time in this literature are summarised in the section on DSGE model evaluation of Negro and Schorfheide (2011), which outlines several methods for evaluating DSGE performance, such as posterior odds ratios, predictive checks and the use of VAR benchmarking. Similar concerns relating to model evaluation and comparison also exist in the ABM literature, to the extent that two special journal issues have been published in order to identify and address them. Fagiolo et al. (2007), as part of the special issue on empirical validation in ABM of Computational Economics, provide a very good review of the existing practices and provide advice as to how to approach the problem of validating an agent-based simulation model. The most obvious example of the need for validation methodologies is the recent development of several large-scale agent-based frameworks that allow the investigation of key macroeconomic questions, such as Keynes/Schumpeter model developed in Dosi et al. (2010, (2013, (2015), which allows for analysis of fiscal and monetary policies, or the European-wide EURACE collaboration of Deissenberg et al. (2008), Van Der Hoog et al. (2008) and Holcombe et al. (2013), which aims to allow large-scale modelling of economic systems. As pointed out by Fagiolo and Roventini (2012), these would greatly benefit from being compared to the standard DSGE macroeconomic workhorse models. Nevertheless, as outlined by Dawid and Fagiolo (2008) in the introduction of the special issue of the Journal of Economic Behavior and Organization on adapting ABM for policy design, finding effective procedures for empirical testing, validation and comparison of such models is still very much an active field of research. Some of the recent developments in this regard are related to the approach suggested here, for example the state similarity measure of Marks (2010, (2013), which aims to measure the distance between two time-series vectors, as is the case here. Similarly, ongoing work by Lamperti (2015) also explores the possibility of comparing models on the basis of simulated data alone with an information based measure, using however a very different measurement approach. Finally, another approach of note is Fabretti (2014), which treats the simulated data from an implementation of the Kirman (1993) model as a Markov chain in order to estimate its parameters. While again the objective and methodology used are different from what is proposed here, the idea of mapping the simulated data to a Markov process is very similar in spirit. The paper aims to contribute to this general issue of comparing different lineages of models by providing a proof-of-concept for a Markovian information criterion (MIC) that generalises the Akaike (1974) information criterion (AIC) to any class of model able to generate simulated data. Like the AIC, the proposed criterion is fundamentally an estimate of the Kullback and Leibler (1951) (KL) distance between two sets of probability densities. The AIC uses the maximised value of the likelihood function as an indirect estimate of the KL distance, however, this obviously requires the model to have a parametric likelihood function, which is no longer straightforward for many classes of modelling methodologies. The proposed criterion overcomes this problem by relying instead on the original data compression interpretation of the KL distance as the inefficiency resulting from compressing a data series using conditional probabilities that are an estimate or approximation of the true data generating process. This fundamental equivalence between data compression and information criteria has led to the emergence of what is known as the minimum description length (MDL) principle, which relies on the efficiency of data compression as a measure of the accuracy of a model’s prediction. Grünewald (2007) provides a good introduction to the MDL principle and its general relation to more traditional information criteria, while Hansen and Yu (2001) explore the use of MDL within a model selection framework, concluding that “MDL provides an objective umbrella under which rather disparate approaches to statistical modelling can coexist and be compared” (Hansen and Yu 2001, p. 772). The proposed methodology provides three key contributions compared to existing methods. The first is to provide a standardised measurement, as the procedure places all models on an equal footing, regardless of their numerical methodology or structure, by treating the simulated data they produce as the result of a Nth order Markov process, where the number of lags is chosen to capture the time dependency of the data. As pointed out by Rissanen (1986), Markov processes of arbitrary order form a large subclass (denoted FSMX) of finite-state machines (FSMs), i.e., systems where transitions are governed by a fixed, finite transition table. By mapping every model to be compared to its FSM representation and scoring these transition probabilities on the empirical data, the MIC is able to overcome differences in modelling methodologies and produce a standardised criterion for any model reducible to a Markov process. This is designed to take advantage of the fact, pointed out by Tauchen (1986a, (1986b) and Kopecky and Suen (2010), that many economic variables and modelling approaches can in fact be approximated by a Markov process.Footnote 1 A second contribution is that the algorithm used to obtain the transition table possess a guaranteed optimal performance over Markov processes of arbitrary order, which as will be shown below, allows an accurate measurement for the MIC. Finally, the algorithm measures cross-entropy at the observation level, producing a vector which sums up to the MIC which enables the reliability of a measurement to be tested statistically. Because purpose of the approach is to use the MIC to compare a set of models \(\{M_{1},\,M_{2},\ldots ,M_{m} \}\) against a fixed-size data set, it is important to also highlight the data snooping problem identified by White (2000) and the reality check procedures that must be carried out to avoid it. Essentially, because statistical tests always have a probability of type I error, repeated testing of a large (and possibly increasing) set of models on a fixed amount of data creates the risk of incorrectly selecting a model that is not truly the best model in the set. White (2000) therefore proposes a procedure that takes into account the size of the model comparison set \(\{M_{1},\,M_{2},\ldots ,M_{m} \}\) when testing for performance against a benchmark model \(M_0.\) A recent development in this literature is the model confidence set (MCS) methodology of Hansen et al. (2011), which differs from White’s reality check in that it does not test against a benchmark model, but instead identifies the subset \(\hat{\mathcal {M}}_{1-\alpha }\) of models in the set which cannot be distinguished from each other at significance level \(\alpha .\) This is well suited to the model-specific vectors of scores produced by the MIC, therefore the MCS is included in the Monte Carlo analyses presented below. The remainder of the paper is organised as follows. Section 2 first discusses the use of universal data compression as an empirical tool for evaluating prediction accuracy and details the theoretical properties of the MIC. A Monte Carlo analysis is then performed in Sect. 3 in order to compare the MIC against the AIC benchmark in an ARMA-ARCH setting and evaluate the criterion’s performance. Section 4 discusses the use of the methodology in practical settings and Sect. 5 concludes.",35
50.0,2.0,Computational Economics,24 May 2016,https://link.springer.com/article/10.1007/s10614-016-9587-y,Measuring and Testing Tail Dependence and Contagion Risk Between Major Stock Markets,August 2017,EnDer Su,,,Unknown,Unknown,Unknown,Unknown,,
50.0,3.0,Computational Economics,28 May 2016,https://link.springer.com/article/10.1007/s10614-016-9588-x,Bayesian Analysis of Power-Transformed and Threshold GARCH Models: A Griddy-Gibbs Sampler Approach,October 2017,Qiang Xia,Heung Wong,Rubing Liang,,,Unknown,Mix,,
50.0,3.0,Computational Economics,01 June 2016,https://link.springer.com/article/10.1007/s10614-016-9589-9,A New Method For Dynamic Stock Clustering Based On Spectral Analysis,October 2017,Zhaoyuan Li,Maozai Tian,,Unknown,Unknown,Unknown,Unknown,,
50.0,3.0,Computational Economics,02 June 2016,https://link.springer.com/article/10.1007/s10614-016-9591-2,Cowboying Stock Market Herds with Robot Traders,October 2017,Jaqueson K. Galimberti,Nicolas Suhadolnik,Sergio Da Silva,Unknown,Male,Male,Male,"Stock markets are complex dynamic systems where the interactions between their composing agents have a crucial role in determining aggregate outcomes. By fostering the emergence of collective conformity, such as fads and social manias, these interactions can propagate small deviations of individual behavior from the fundamental valuations to the overall market. The occurrence of large fluctuations in stock prices can therefore be attributed to, at least to some extent, the emergence of herd behavior. From this point of view, this paper offers two main contributions. First, we propose an agent-based model that accounts for the role of imitation in individuals’ decisions and is capable of generating the large swings observed in actual stock markets. Second, we evaluate the effectiveness of policy mechanisms aimed at preventing the occurrence of large fluctuations caused by herd behavior. Stock market crashes can have harmful effects on economic activity. Large losses of wealth can induce lower levels of consumption, and abrupt changes in the cost of capital can lead to severe distortions in investment decisions. Macroeconomic policy authorities should therefore be interested in the prevention of such stock market collapses. But recognizing the complexity of stock markets poses daunting challenges for policy making. Attempts to stabilize stock markets with monetary policy and financial regulation raise several impracticalities. First, bubbles are hard to spot and, until very recently, central banks were not inclined to respond to developments in asset prices (see Blanchard et al. 2012, for an assessment of how this view might be changing). Second, the experience of the recent crisis suggests that financial markets tend to innovate around regulations and the nature of risk-taking changes as the financial system gets more sophisticated (Edey 2009). Another example is given by the Chinese authorities role and response to the 2014–2015 bubble and crash in the equity market, where “broad-ranging interventions [...] appear to have increased investor uncertainty about financial sector policies” (IMF 2015). In this paper, we argue that these aspects are typical of large, dynamic and complex systems that can self-organize into a critical state where minor perturbations may give rise to instabilities of macroscopic scales (see, e.g., Scheinkman and Woodford 1994; Bak and Paczuski 1995). One key issue is that such extreme events cannot be predicted and therefore require a different paradigm for the design of effective interventions. We set up an artificial stock market with autonomous agents that interact in a two-dimensional lattice using simple decision rules based on imitative and fundamentalist behavior. Imitation is a key component for the emergence of herding in our model, and it can be motivated from different theoretical reasons: Private information can generate incentives for a rational decision maker to follow others’ actions in sequential environments (e.g., Banerjee 1992; Bikhchandani et al. 1992; Devenow and Welch 1996; Bikhchandani and Sharma 2001, for reviews of this literature). Imitation also can arise from social (Bernheim 1994) and psychological factors that cause behavior to deviate from fully rational considerations (e.g., Kirman 1993; Lux 1995). Nevertheless, empirically, there is scarce evidence that distinguishes between these motives in real markets [see Cipriani and Guarino (2014) for a discussion on the disconnect between the empirical and theoretical literature]. To circumvent this debate, imitative behavior is introduced in our model by the explicit assumption of a simple local interaction rule, rather in the spirit of Herbert Simon’s bounded rationality (Simon 1982). As usual, prices in our model are determined at the market level in response to imbalances between aggregate demand and supply for the stock. Hence, the ultimate cause of extreme returns in our model is oscillations of market liquidity, which is consistent with recent accounts of how the 2008–2009 financial market turmoil propagated (see Brunnermeier 2009, for a review). Nevertheless, the emergence of large liquidity imbalances is not implicit in our model behavioral assumptions, nor by its market micro-structure. It is the local interactions architecture of our model that amplify clustered shortages of liquidity and have a significant impact at the overall market level. The inherent complexity that these interactions prompt often restrict the feasibility of standard analytical tools for realistic inferences. One solution is the use of the agent-based computational approach (see Tesfatsion et al. 2006), where dynamic systems of interacting agents are computationally modeled to facilitate generative explanations (Epstein 2007). Because the estimation of agent-based models is complicated by the lack of simple analytical solutions (see Grazzini and Richiardi 2015, and references therein), we developed an empirical strategy based on a goodness-of-fit measure for the whole distribution of stock returns generated by our model. Specifically, we calibrate our model to replicate actual stock markets data, which present distribution of returns characterized by heavy tails, i.e., extreme returns are more likely to occur than Gaussianity would imply. Using simulations, we then show that our model is capable of matching the empirical distribution of daily returns of the Dow Jones Industrial Average (DJIA) index from 1996 to 2012. We then turn our focus to the design and evaluation of policy schemes aimed at the prevention of sudden liquidity dry-ups. To that end, we conduct several counter-factual exercises using our calibrated model. First, we show that an “aggregate market-maker” type of liquidity provision policy is only partially effective for the stabilization of our artificial stock market. More generally, we argue that a policy design that neglects the interconnections between individual decisions and their scaling up to aggregate outcomes can be misleading and costly. To account for the complex nature of stock markets, we propose the use of a system of trading algorithms, or robot traders, as described in Suhadolnik et al. (2010). Robot traders have been around for years, but used only for private gain. More recently, their use for high-frequency trading has been the cause of intense debate on whether their effects are beneficial or harmful to the functioning of financial markets, and how regulation should cope with the rapid pace of their technological innovation (see the reviews by Foucault 2012; Kirilenko and Lo 2013; Farmer and Skouras 2013, for an ecological perspective). Here, instead, we propose their systematic use for the benefit of public policy making. In contrast to the practice of responding to aggregate observations, our robot traders are triggered locally to follow a contrarian rule in order to prevent stampede reactions caused by herd behavior. To prevent financial imbalances we also introduce a self-regulatory mechanism to decrease the robot’s contrarian behavior in response to its individual financial position. Hence, every robot has the autonomy to trade on the basis of its local information, but is also bound by its own track of transactions. Also, addressing one of the key criticisms to asset price targeting, the robots’ intervention does not depend on assessments of the stock’s fundamental value. The only requirement is the introduction of a parallel system of autonomous trading algorithms that will gather information in real-time at key junctures of the market structure. There is an interesting parallel between our approach and the role of independent assessments in collective decision making. According to what is known as “Condorcet’s jury theorem,” named after its proponent, the 18th century French intellectual Marquis de Condorcet, the pooling of independent information held by multiple individuals can lead to better decisions than those relying on particular dictatorial assessments (see, e.g., Grofman et al. 1983; Young 1988; Boland 1989). In other terms, and nitpicking the popular belief that stock markets are sometimes driven by the madness of crowds, we devise a “crowd of robot traders” to restore the wisdom often associated with collective decisions (see Surowiecki 2005; Landemore et al. 2012, for many examples). The existence of interdependencies between the individual decisions, however, may lead to violations to the Condorcet’s principle. When decisions are correlated, the effectiveness of information pooling through the majority rule tends to decrease (Ladha 1992; Berg 1993; Ladha 1995). Clearly, this is the case in our crowd of robot traders—even though the robot traders are devised to operate autonomously, the interconnectedness of agents in our artificial market can give rise to correlated contrarian responses. To circumvent this issue, we further developed a coordination mechanism that splits the robots’ action into two stages: First, the local information is collected and pooled by a financial policy authority. Next, the decisions of the robot traders are coordinated to take into account the general assessment of the market condition. We find that the robot traders are capable of stabilizing the stock market and reshaping the distribution of returns towards a Gaussian distribution, while the self-regulatory mechanism guarantees its financial sustainability. Furthermore, with the aid of the coordination mechanism, the number of robots required to mitigate extreme events is substantially reduced, which means that our approach requires only tiny perturbations to the usual functioning of the stock market. The calibration of our model also evidenced some uncertainty regarding agent’s sensitivity to the observation of a quorum in the local neighborhoods, and our results indicate the relevance of such a specification for the design of effective stabilization policies. The remainder of this paper proceeds as follows: In Sect. 2, we present our artificial stock market model and describe the calibration approach we adopted to match statistical properties observed in the data. Section 3 describes the liquidity provision policies we considered in the counter-factual exercises of Sect. 4. Section 5 concludes the paper with some final remarks.",1
50.0,3.0,Computational Economics,01 June 2016,https://link.springer.com/article/10.1007/s10614-016-9592-1,Can Minorities Escape Wage Discrimination by Forming Firms?,October 2017,James Fain,,,Male,Unknown,Unknown,Male,"Wage discrimination against some demographic groups is well-documented, pervasive and persistent. This topic has been the subject of much economic research. Theoretical models of discrimination often start with a group of discriminating firms and two types of workers, one favored and one disfavored. In many of these models the set of discriminating firms never changes, as the authors assume a fixed number of discriminating firms and proceed from there. This assumption exists to make the model tractable, but it a very harmful assumption, for the fluid nature of markets includes the entry and exit of firms. If no new firms enter the market, then new, non-discriminating firms can’t or won’t enter the market. To the best of my knowledge there are no models in which the disfavored workers seek to avoid working for a discriminating firm by forming their own firm. In this paper I rectify that omission and examine the degree to which minority workers can escape discrimination by forming firms. In his original work on discrimination, Becker (1957) posited that if a sufficient number of jobs exist at non-discriminating firms, minority workers will be able to escape wage discrimination by taking jobs at the non-discriminating firms. The implication of this is that a large-scale change in social attitudes is not necessary for minority workers to be free from wage discrimination: measured wage discrimination will be eliminated if even a relatively small group of employers do not discriminate. In this situation the minority workers are likely to experience longer periods of unemployment as they search for a non-discriminating employer, but there should be no wage discrimination that could be detected by observing the actual matches between firms and workers. In a frictionless economic world, minority workers would be able to locate jobs at the non-discriminating firms. However, in real world labor markets there are numerous frictions and constraints that can hinder workers and firms as they seek an optimal match. In addition to exploring minority firm formation, below I examine the degree to which minority workers are able to locate non-discriminating employers in a labor market with frictions. In the model below initially all firm are majority-owned; the possibility of minority-owned firms arises at a later date. All majority-owned firms are assumed to discriminate and the average level of wage discrimination from these firms is constant. Only minority-owned firms offer jobs to minority workers at non-discriminating wages. I am therefore testing the following hypothesis: if there is no change in the majority population regarding discrimination, to what degree are minority agents able to escape wage discrimination by forming their own firms? Note that this model provides minorities with two paths to escape wage discrimination: forming a firm or finding employment at a non-discriminating minority-owned firm. To test this hypothesis I need to be able to compare the state of the labor market when minorities are not able to form firms to its state when they are able to form firms. Agent-based simulations are an ideal way to make this comparison. There may be a number of obstacles that make forming firms unappealing or even untenable for minorities. Minority firms may experience discrimination by customers that would make these firms difficult to sustain. Another substantial problem may be a lack of access to credit. Starting a new business often requires the owner to borrow money initially, and many businesses require access to revolving credit, so a lack of access to credit may effectively preclude many minorities from starting their own firms. There is recent evidence in the U.S. of disparities in access to credit. There is evidence of “redlining” in the mortgage market (Munnell et al. 1996), whereby banks deny mortgages to African-Americans that they would approve for whites with the same employment and credit characteristics. In addition, in the Pigford vs. Glickman settlement African-American farmers were awarded large judgments against the U.S. Department of Agriculture for discrimination in the awarding of agriculture loans from 1981 to 1996 (Cowan and Feder 2008). The discrimination in mortgage lending and agricultural lending suggests that African-Americans may have more limited access to business credit than do other Americans. The lack of access to credit may also extend to other minority groups. Following Richiardi (2006), below I establish a labor market with endogenous firm formation. Each period relatively dissatisfied individuals choose whether to search for a new job or form a new firm. New firms are risky and often fail. I extend Richiardi’s model by allowing individual agents to be either a member of the majority or a minority. Only majority agents may form firms initially. All majority firms practice employer wage discrimination, paying minority workers less than similarly productive majority workers. Under the initial regime, minority workers have no means to escape this discrimination. I allow the simulations to proceed until the model reaches an equilibrium stage, and record the state of the labor market at that stage. I then change the regime and allow minority workers to form firms too. This simulates a sudden change in the legal environment or a suddenly improved access to credit. This change disrupts the market and changes the composition of the firms. I allow the simulations to establish a new equilibrium, and then I compare the new equilibrium with minority firms to the old equilibrium that lacked minority firms. The outline of this papers is as follows. I first review the relevant literature and explain how my model differs from Richardi’s. In Sect. 3 I examine the results associated with the base model, and in Sect. 4 I consider what happens when agent locations are added to the model. I also consider the impact of having the minority agents concentrated in a densely populated area, which allows one to explore some spatial mismatch issues. Section 5 reports the results of using different parameter values to examine how sensitive the results are to the specific parameters chosen, and Sect. 6 offers a summary and conclusions.",
50.0,3.0,Computational Economics,17 June 2016,https://link.springer.com/article/10.1007/s10614-016-9593-0,Performance of Tail Hedged Portfolio with Third Moment Variation Swap,October 2017,Kyungsub Lee,Byoung Ki Seo,,Unknown,,Unknown,Mix,,
50.0,3.0,Computational Economics,02 June 2016,https://link.springer.com/article/10.1007/s10614-016-9594-z,An Econometric Analysis of Insurance Markets with Separate Identification for Moral Hazard and Selection Problems,October 2017,Shinya Sugawara,Yasuhiro Omori,,Male,Male,Unknown,Male,"During the course of the previous decade, empirical studies have been rapidly catching up with highly developed economic theories of asymmetric information. This paper proposes a new microeconometric framework to analyze the information problem in insurance markets. Our main contribution to the extant literature is that we can separately identify moral hazard and selection problems. This is a clear advantage over a traditional method that represents information asymmetry as a single parameter. In particular, the conventional methodology uses a bivariate probit model for consumer data, as discussed by Chiappori and Salanié (2000). The two dependent variables of this model are the purchase of insurance and the occurrence of an accident. In this approach, the standard asymmetric information problem can be detected as a positive correlation between these two dependent variables. Specifically, moral hazard implies that consumers increase their riskiness after they have purchased insurance, while adverse selection implies that riskier consumers have a stronger demand for insurance. In addition to computational simplicity, this bivariate probit model has a nice property in data availability. This model does not require detailed contract information, which may be hard to obtain, but instead relies on variables that are commonly obtained from general household surveys. Thus, bivariate probit analysis has rapidly become a popular technique that has been applied to various insurance markets. However, most empirical investigations have failed to detect a significantly positive correlation between two dependent variables; in fact, some studies have even found a negative correlation. In response to these unexpected results, De Meza and Webb (2001) proposed an important alternative theory to adverse selection. This theory, which is known as advantageous selection, states that less risky individuals have a stronger demand for insurance, due to their risk aversive preference. In contrast to adverse selection, advantageous selection produces a negative effect of risk on an insurance purchase. Thus, the conventional bivariate probit approach may not be applicable if both moral hazard and advantageous selection are present because the distinct and conflicting effects of these two factors cannot be captured by a single correlation parameter. To assess the validity of such a theory, one must separately identify the moral hazard and the selection problems(which mean either adverse selection or advantageous selection). This task is beyond the scope of the bivariate probit model. In this paper, we present a new approach that permits the separate identification of moral hazard and selection problems. We begin our econometric modeling with adding two terms to the bivariate probit model. These terms allow for moral hazard and selection problems to be measured as two distinct coefficient parameters rather than as a correlation between two dependent variables. Despite the simple appearance of our approach, our model specification encounters statistical difficulty due to the existence of mutual dependencies between the dependent variables. It is shown that we cannot separately identify two coefficients without an additional assumption. Thus, to overcome this identification problem, we assume the simultaneous determination of the two dependent variables. This simultaneity assumption introduces an econometric problem that has been analyzed in the literature of nonlinear simultaneous equation models with limited dependent variables, such as Amemiya (1975), Heckman (1978) and Gouriéroux and Monfort (1979). These works yielded a statistical problem that is referred to as incoherency by Gouriéroux et al. (1980); this problem involves the fact that probabilistic models are not well-defined without a restrictive assumption on parameter values. To handle the incoherency problem, we adopt an approach that was proposed by Tamer (2003) and Ciliberto and Tamer (2009) in the literature on empirical analyses of entry games. These studies introduced a latent variable that formulates a well-defined probabilistic model. This variable is called a selection rule, because it characterizes players’ choices among multiple Nash equilibria. However, this approach creates the incidental parameter problem (Lancaster 2000), because it is difficult to construct a consistent estimator for parameters that does not depend on the sample-specific selection rule. To overcome the incidental parameter problem, Ciliberto and Tamer (2009) constructed a moment inequality using boundary conditions of the selection rule. Because the resulting econometric model is partially identified, they employed a set estimation method of Chernozhukov et al. (2007). This paper adopts a Bayesian approach that we introduced in a prior study (Sugawara and Omori 2012). A clear distinction between our Bayesian approach and previous classical methods is that we explicitly estimate the sample-specific variable. Because the Bayesian estimation approach can work with finite samples, the sample-specific selection rule is estimable. The lack of the incidental parameter problem enables us to construct a standard likelihood function rather than a moment inequality. Based on the well-defined likelihood function, we can conduct standard inference techniques. To demonstrate the advantage of our Bayesian methodology, we propose a way to answer the empirically important question of what type of an information structure consumers may face. We show that distinct information structures correspond to non-nested statistical models. This finding indicates that statistical model selection is an appropriate method to answer this question. However, there has not yet been invented a classical model selection procedure for our model.Footnote 1 On the other hand, our Bayesian estimation is accompanied with standard model selection techniques. To provide an empirical application of our approach, we analyze the US dental insurance market. Our model selection results indicate that moral hazard and advantageous selection are both present in this market. These findings are intuitively interpreted as an indication that for dental care, early preventive concerns both reduce risk and stimulate insurance demand. The detection of advantageous selection reveals an advantage of our methodology because this result cannot be derived from conventional bivariate probit analysis. Our study relates to three literatures; the econometrics of insurance markets, Bayesian statistics and empirical analyses of dental care. First, with respect to econometric analyses of insurance markets, this paper supplements two recent streams of research. One of these research streams discusses the structural estimation approach, such as Cardon and Hendel (2001) and Einav et al. (2010b), and was summarized by Einav et al. (2010a). In contrast to the reduced form approach of the bivariate probit model, the structural approach explicitly models moral hazard and selection problems to achieve the separate identification. This body of literature continues to grow because many variations of structural models can exist. Another growing insurance-related literature is reduced form analyses that are specialized in the examination of advantageous selection. These studies generally involve a binary choice analysis in which advantageous selection is detected as an effect of individual’s risk aversion on the insurance purchase dummy. To measure this risk aversion, researchers have employed a variety of explanatory variables, such as subjective mortality rate by Cawley and Philipson (1999), seat belt use by Finkelstein and McGarry (2006), and health status and schooling levels, which serves as proxies for cognitive ability and financial numeracy, by Fang et al. (2008). No consensus has yet been reached with respect to the appropriate choice of an explanatory variable among the many candidates. We believe that our study can complement these recent literatures. Our methodology proposes a simple model using commonly available data, while the structural approaches analyze more sophisticated models using detailed data. Furthermore, our model requires only data regarding the occurrence of an accident, which is commonly available information, as an explanatory variable to characterize the selection problem. Therefore, one can employ a preliminary analysis using our simple method before structural estimation, to make an appropriate choice among enormous candidates of models and explanatory variables. Second, with respect to Bayesian statistics, there is a growing literature on partially identified models. Theoretically, Moon and Schorfheide (2012) and Kitagawa (2012) provided theoretical comparisons of the classical and Bayesian estimators. For the technical concern, Liao and Jiang (2010) proposed an estimation procedure that applies the Bayesian version of the method of moments in Kim (2002) and Chernozhukov and Hong (2003). Unlike the quasi-Bayesian approach of Liao and Jiang (2010), our methodology is based on a standard likelihood function that is available at the cost of a distributional assumption. Third, there have been several empirical studies about the information problem in the US dental insurance market. Previous studies have consistently detected moral hazard in various datasets from this market, such as Mueller and Monheit (1988) for a surveyed dataset and Manning et al. (1985) for an experimental dataset. On the other hand, selection problems have been regarded as a non-negligible but a difficult concept to be identified, due to the incoherency problem (Sintonen and Linnosmaa 2000). An exceptional study is Munkin and Trivedi (2008), which we deeply owe in our construction of an empirical study. The organization of this paper is as follows. In Sect. 2, we describe relevant econometric models. Section 3 considers the corresponding inferential frameworks. The proposed method is applied to the US dental care insurance market in Sect. 4. Section 5 concludes the paper.",1
50.0,3.0,Computational Economics,09 June 2016,https://link.springer.com/article/10.1007/s10614-016-9595-y,A Generalized Singular Value Decomposition Strategy for Estimating the Block Recursive Simultaneous Equations Model,October 2017,Mircea I. Cosbuc,Cristian Gatu,Erricos John Kontoghiorghes,Male,Male,Unknown,Male,"The estimation of the simultaneous equations model (SEM) is of great importance in econometrics (Ando and Zellner 2010; Angristet al. 2000; Greene 2008; Imbens and Newey 2009; Kontoghiorghes 2000; Matzkin 2008). The most commonly used estimation procedure for the SEM if the three-stage least-squares (3SLS) procedure. It involves Kronecker products and direct sum of matrices that make the solution computational expensive even for modest sized models (Belsley 1988; Zellner and Theil 1962). Therefore, there is a need in exploiting the SEM structure in order to reduce the estimation computational burden. The SEM comprising \(G \ge 1\) structural equations is given by: Here \(y_i \in \mathbb {R}^T\) is the dependent variable vector, \(X_i \in \mathbb {R}^{T \times k_i}\) is a full column rank matrix of predetermined variables, \(Y_i \in \mathbb {R}^{T \times g_i}\) is a matrix of endogenous variables with \(1\le g_i \le G\), \(\beta _i\in \mathbb {R}^{k_i}\) and \(\gamma _i\in \mathbb {R}^{g_i}\) are the unknown structural parameters, \(\delta _i^T = (\beta _i^T \quad \gamma _i^T)\), \(W_i = (X_i \quad Y_i) \in \mathbb {R}^{T \times h_i}\) and \(h_i=k_i + g_i\). The disturbances \(\epsilon _i\) have zero mean and \(\text {E}(\epsilon _i\epsilon _j^T)=\sigma _{i,j}I_T\) for \(i,j=1,\dots ,G\). The stacked system can be expressed as: where \(W=(X \quad Y)\), \(Y=(y_1,\dots ,y_G)\), \(X \in \mathbb {R}^{T \times K}\) is the matrix of all the distinct exogenous variables, \(WS_i=W_i\) and \(S_i \in \mathbb {R}^{(K+G)\times h_i}\) are selection matrices with \(S = \oplus _{i=1}^GS_i\), \(\delta ^T=(\delta _1^T, \dots , \delta _G^T)\) and \(E=(\epsilon _1, \dots , \epsilon _G)\). The dispersion of E is given by \(\text {Var}(E)=\Sigma \otimes I_T\), where \(\Sigma =\left[ \sigma _{i,j}\right] \in \mathbb {R}^{G \times G}\) (Andrews and Kane 1970). Note that \(\oplus _{i=1}^G\) is the direct sum which yields a block diagonal matrix and will be abbreviated to \(\oplus ^G\). For the structural equations to be identifiable it must hold that \(h_i \le K\) for any \( i=1, \dots , G \) (Hausman 1983). Furthermore, often some variables may appear in more than one equation, and thus, \(K\ne \sum _{i=1}^Gk_i\) and \(G \ne \sum _{i=1}^G g_i\). The special case of the block recursive SEM (BR-SEM) represents a particular SEM in which endogenous variables can be grouped in independent blocks (Dhrymes 1978; Hausman 1983; Lloyd and Lee 1976). Furthermore the endogenous variables in each block are dependent only on other endogenous variables in the same block or in previous blocks. Thus, for the BR-SEM, let the \(G=G_1+G_2+\dots +G_p\) endogenous variables be partitioned in p groups: \(y^{(1)}=\{y_1,\dots , y_{G_1}\}\), \(y^{(2)}=\{y_{{G_1}+1},\dots , y_{{G_1} + {G_2}}\}, \dots \), \(y^{(p)}=\{y_{G_1 + G_2 + \dots + G_{p-1}+1},\dots , y_{G}\}\). For notation convenience, hereafter, let \(\widetilde{G}_0=0\) and \(\widetilde{G}_i=\sum _{j=1}^i G_j\). Thus, the BR-SEM can be written as where \(W^i=\oplus _{j=\widetilde{G}_{i-1}+1}^{\widetilde{G}_i}(X_j\quad Y_j)\) and \(\epsilon ^i= \oplus _{j=\widetilde{G}_{i-1}+1}^{\widetilde{G}_i} \epsilon _{j}\). The assumption that the errors from different blocks are uncorrelated yields a block-diagonal error covariance matrix, that is, \(\Sigma = \oplus ^{p}\Sigma _i\), where \(\Sigma _i\in \mathbb {R}^{G_i\times G_i}\) (Hausman 1983). The endogeneity of the SEM is eliminated by premultiplying the model by \(X^T\) which yields the transformed SEM: The three-stage least squares (3SLS) estimator of the SEM is the generalized least squares estimator of the latter transformed model and is given by: where \(\widehat{\Sigma }\) is a consistent estimate of the true error covariance matrix \(\Sigma \) (Srivastava and Tiwari 1978; Zellner and Theil 1962). In the case of the BR-SEM the 3SLS estimator is simplified by obtaining the estimators of each block in 3 separately (Hausman 1983): where \(X^i=\oplus _{j=\widetilde{G}_{i-1}+1}^{\widetilde{G}_i} X_j \) and \(i = 1,\dots ,p\). For non-small models, the 3SLS estimator is computationally expensive (Belsley 1988; Zellner and Theil 1962). In the special case of block-recursive structure SEM, the matrices involved in the computations display a block diagonal and banded structure. Here, this is exploited while using the generalized singular value decomposition (GSVD) as main estimation tool in order to reduce the computational burden (Paige 1985; Paige and Saunders 1981; Van Loan 1976). Specifically the problem is reduced to a number of independent smaller estimation problems corresponding to each block of the recursive SEM. Section 2 considers the estimation of the SEM using the GSVD. In Sect. 3, an efficient estimation procedure for the BR-SEM is proposed, which exploits the block structure of the estimated error covariance matrix \(\widehat{\Sigma }\). Computational results are presented in Sects. 4 and 5 concludes.",
50.0,3.0,Computational Economics,26 July 2016,https://link.springer.com/article/10.1007/s10614-016-9601-4,Uncertain Potential Output and Simple Rules in Small Open Economy,October 2017,Guido Traficante,,,Male,Unknown,Unknown,Male,"This paper analyzes the implications of incomplete information on potential output for the conduct of monetary policy in a small-open economy theoretical model. I assume that monetary policy is conducted through simple rules, including the exchange rate peg. Moreover, by comparing the outcomes under different simple policy rules, I will assess if uncertain potential output makes a fixed exchange rate regime preferable. The existing literature on uncertain potential output deals with closed economy models.Footnote 1 Orphanides (2001, (2003) argues that a significant (real time) overestimation of potential output in the US during the oil shocks of the 1970s leads to a monetary policy stance which turned out to be, with the benefit of hindsight, excessively loose. This is shown by comparing the Taylor rule with real time data and with revised data. In a theoretical paper, Cukierman and Lippi (2005) prove that, even if the policymakers efficiently estimate potential output, this does not avoid persistent retrospective policy errors. Ehrmann and Smets (2003) use a calibrated model of the euro area to investigate the implications of incomplete information about potential output for the conduct of monetary policy. Their main result is that, under unobservable potential output, there are persistent deviations between the actual and perceived output gap in response to cost-push shocks. Since the central bank cannot distinguish between a cost-push shock and a shock to potential output, after a negative cost-push shock, it assigns some probability that a positive supply shock hits the economy. As a consequence, the central bank will lower the interest rate by more than it would otherwise have done, leading to a larger response in the output gap and a smaller fall in inflation. More recently, incomplete information has been analyzed also in terms of a time-varying inflation target and it has been related to monetary policy transparency. Melecký et al. (2009) investigate the effects of monetary policy transparency on macroeconomic volatility in an estimated model of the Eurozone where private agents are unable to distinguish between temporary shocks to the central bank’s monetary policy rule and persistent shifts in the inflation target. They find that announcing the inflation target improves social welfare, because it counteracts private agents’ overestimation of the inflation target volatility. In a model with parameter uncertainty, Dennis and Ravenna (2008) show that an unanticipated change in the inflation target deteriorates social welfare while the new inflation target is being learned. Using a theoretical DSGE model with incomplete information about policy targets, Di Giorgio and Traficante (2013) show that transparency about output and inflation target increases social welfare. Moreover, uncertainty about the inflation target entails a higher welfare loss than uncertainty about potential output. Among the simple rules available in open economy, there is the possibility of pegging the exchange rate. Economic history provides several examples of floating regimes, managed floating regimes, exchange rate pegs and monetary unions. Countries can officially declare an exchange rate regime and subsequently decide to renege on it, determining a discrepancy between the official—also called de jure—exchange rate regime and the actual de facto regime. Calvo and Reinhart (2002) argue that many countries demonstrate a “fear of floating”: some policymakers limit exchange rate movements even when the currency is de jure floating and, on the other hand, other policymakers declare fixed exchange rate regime and they then allow their currencies to fluctuate. As of 2015, nineteen European countries are in the euro currency area, while since 1981 the Monetary Authority of Singapore aims at keeping the trade-weighted exchange rate within a reference band.Footnote 2
 What are the incentives to join a monetary union or to peg the exchange rate? A strong argument for a fixed exchange rate regime was suggested by Giavazzi and Pagano (1988), who show that fixed exchange rate dominates the other alternative policies because an inflation-prone country can borrow credibility from a central bank who credibly aims at stabilizing prices. To that extent, “tying central bank’s hands” allows to reach a stable inflation. Monacelli (2004) shows that with high elasticity of substitution between domestic and foreign goods and for high weight to the stabilization of the output gap variability, a regime of fixed exchange rates is more desirable than the discretionary optimal policy. Soffritti and Zanetti (2008) find mixed evidence about the desirability of fixing the exchange rate: time-consistent monetary policy leads to a lower loss than a policy that fixes the exchange rate, but the exchange rate peg is preferable when the policymaker maximizes the agent’s utility. Galí and Monacelli (2005) prove that an exchange rate peg is worse in welfare terms than a floating regime because it limits the possibility of influencing the terms of trade in a way beneficial to domestic consumers. In Berger (2006), even if monetary policy is very volatile, importing a more stable monetary policy through a fixed exchange rate is not welfare improving if prices are sticky in the producer’s currency. Ravenna (2012) shows that a fixed exchange rate can be an optimal choice even if a policy-maker could commit to the first-best monetary policy whenever the private sector’s beliefs are not consistent with the central bank’s dependability. In this paper I evaluate the performance of simple rules and its relationship with the exchange rate regime under incomplete information. More in detail, I consider a theoretical small-open economy DSGE model where I introduce incomplete information by assuming that the policy-maker and the private sector cannot observe potential output and ex-post perception errors of the state of the economy affect private sector’s choices and monetary policy. Using output and domestic inflation as observable variables, they learn gradually the state of the economy through Kalman filtering. The main results of the paper are two. First of all, the simple rule that performs better does not include a term in the exchange rate. Second, both under complete and incomplete information, there exists a parameter combination in the policy coefficients such that the exchange rate peg performs equally or even better than an independent monetary policy. The paper is organized as follows. Section 2 describes the small-open economy model and monetary policy. The signal-extraction problem is analyzed in Sect. 3, while Sect. 4 provides a numerical evaluation, by means of simulations, of monetary policy rules under complete and incomplete information. Section 5 concludes.",1
50.0,4.0,Computational Economics,24 August 2016,https://link.springer.com/article/10.1007/s10614-016-9616-x,An Agent-Based Simulation of the Stolper–Samuelson Effect,December 2017,Luzius Meisser,C. Friedrich Kreuser,,Male,Unknown,Unknown,Male,"Agent-based simulations are complex, often chaotic systems. As such, they exhibit rich dynamics that are hard to achieve with traditional means. However, these rich dynamics can be a curse rather than a blessing as they can lead to arbitrary, unverifiable results. This insight lead us to build a stable, verifiable agent-based model instead, with equilibria that are in line with classic theory. In particular, we implemented a minimal agent-based simulation of the Stolper–Samuelson effect (Stolper and Samuelson 1941) in an Arrow–Debreu economy with profit-maximizing firms (Arrow and Debreu 1954). The choice of the Stolper–Samuelson effect is rather random and of secondary importance. The prime achievement of this paper lies in the introduction of methods to enable the simulation of a classic result with unprecedented accuracy. There are a number of agent-based projects that aim at comprehensively modeling a large-scale economy, examples being the Eurace project by Deissenberg et al. (2008), the family of models by Gatti et al. (2011), and the Jamel framework by Seppecher (2012). Due to their complexity, it is non-trivial to rigorously test them. In contrast, our simulation focuses on a single, well-defined effect, allowing for exact quantitative verification. In this regard, we follow the footsteps of authors such as Brock and Hommes (1998), Gintis (2007), or LeBaron (2001), who encourages benchmarking agent-based models with classic equilibrium results. Implementation-wise, our model most closely resembles that of Wolffgang (2015) with its emphasis on applying best practices from software-engineering. To our knowledge, Wolffgang’s model is the first to apply the exponential search algorithm discovered by Bentley and Yao (1976) to price finding, an idea we adopt and adjust. While exponential search helps to achieve faster convergence and better accuracy, it does not address input synchronization. Input synchronization being hard to achieve in price-driven markets has also been observed by firm theorists Milgrom and Roberts (1994). For firms depending on multiple perishable input goods (in our model different types of man-hours), it is essential that all their bids succeed. With Cobb–Douglas production, failing to acquire one of the input goods already leads to a total loss of production. We improve input synchronization by introducing sensor prices. Furthermore, causal loop diagrams—a tool from system dynamics—are used to identify the method of normalizing prices indirectly by fixing dividends as a more stable choice than the usual method of normalizing a randomly chosen price directly. While irrelevant in static equilibrium theory, the choice of how to normalize prices decidedly impacts the dynamics of the simulation. Together, these three measures enable the emergence of the theoretically expected equilibrium with high accuracy. Section 2 specifies the rather unspectactular general-equilibrium version of our model. In Sect. 3, the fundamental mechanisms of the agent-based model are described and analyzed. Results are presented in Sect. 4. It turns out that—within a certain parameter space—our agent-based simulation is stable, accurate, and fast. Finally, we conclude with Sect. 5.",
50.0,4.0,Computational Economics,09 March 2017,https://link.springer.com/article/10.1007/s10614-017-9671-y,Influence of Inefficiency in Government Expenditure on the Multiplier of Public Investment,December 2017,Shigeaki Ogibayashi,Kosei Takashima,,Male,Unknown,Unknown,Male,"There has been widespread interest in the economic influences of fiscal policy, especially since the great recession of 2007–2009 (Whalen and Reichling 2015). In promoting an economy while reducing government deficits, much attention has been paid to understanding how fiscal policy affects the economy; this attention has been reflected in the ongoing debate over the size of the fiscal multiplier (Whalen and Reichling 2015). According to traditional economic theory, the multiplier of public investment has been defined as the inverse of the marginal propensity to save (Krugman and Wells 2009). It is also well known, however, that the actual multiplier of public investment is much smaller than that which theory predicts (Baum et al. 2012; Bruckner and Tuladhar 2010; Murata et al. 2005; OECD 2009; Perroti 2004; Sugimoto 2008). The multiplier of an actual system is generally estimated through the use of three methods: the use of macroeconometric forecasting models, time-series models, and dynamic stochastic general equilibrium models (Whalen and Reichling 2015). Although estimated values vary greatly, it is widely recognized that the multiplier can be <1, or even negative in some cases (Baum et al. 2012; Bruckner and Tuladhar 2010; OECD 2009; Perroti 2004; Sugimoto 2008). For example, Sugimoto (2008) estimated the multiplier of public investment in the Japanese economy and reported that the average multiplier for the 1955–2011 period ranged from 0.76 to 1.38. Perroti (2004) estimated multipliers of public investment for five advanced countries and reported that the estimated multipliers ranged between −0.88 and 5.46, thus indicating that the multiplier can be not only <1 but also negative. This discrepancy has been partially explained in the literature as stemming from the influences of other factors, such as taxation and transfer payments to households (Krugman and Wells 2009). In addition, Morishima (1984) derived an equation with which to obtain the multiplier of public investment, based on reasonable assumptions under which the multiplier can be as low as 1.45, thus indicating that the multiplier is also affected by the ratio of imports and firms’ ways of distributing earnings surpluses, among other things. However, these factors cannot on their own explain why the multiplier can be <1. Bruckner et al. (Bruckner and Tuladhar 2010) studied the multiplier of the Japanese economy and discussed the influences of such factors that contribute to the low multipliers as the inefficient allocation of public investment, overinvestment and high capital stock that leads to low marginal productivity, and the influence of crowding out. However, the most responsible factor among them has not been specified because of a lack of evidence. Although there have been many arguments on this matter (Baum et al. 2012; Bruckner and Tuladhar 2010; OECD 2009; Perroti 2004; Whalen and Reichling 2015), there is still no conclusive consensus as to why the multiplier might be <1. In this sense, there has been a dearth of research that quantitatively explains why actual multipliers may sometimes be <1, or even negative. One interesting study is that of Perroti (2004), who pointed out that public investment might be particularly prone to political pressure and loaded with so-called pork-barrel projects that have no economic rationale. In more extreme cases, this can foster downright corruption and rent-seeking activities (Perroti 2004). Although this explanation seems reasonable, his idea is not supported by the results of any quantitative analysis. Meanwhile, agent-based modeling (ABM) is a sound approach for studying why the multiplier of public investment might be <1, because it is a bottom-up approach in the sense that macro phenomena emerge in an artificial system modeled on a computer as a result of interactions between agents in a way similar to that of an actual system, and therefore, ABM can deal with heterogeneity, individual agents’ bounded rationality, and non-equilibrium dynamics in social systems. One criticism of ABM relates to the validation of ABM and points out that the macro phenomenon that emerges in ABM is insensitive to parameter values and it would therefore be difficult to derive the necessary conditions for the model to exhibit specific macro behaviors as pointed out in the literature (Marks 2007). According to the authors’ previous study (Takashma and Ogibayashi 2014), however, input conditions in ABM can be divided into parameter values and the system structure of the model and the macro phenomenon is quite sensitive to the system structure of the model, which is characterized by such factors as the types of agents, the behavioral rules of agents, and the fields where agents develop their activities. In addition, there is one-to-one correspondence between the macro phenomenon, its underlined mechanism, and the model structure, the latter of which is indispensable in reproducing the phenomenon. By studying this model structure that is indispensable in reproducing the desired phenomena, in terms of a series of computer experiments, we can elucidate the underlying mechanism of the occurrence of the macro phenomenon (Takashma and Ogibayashi 2014) In relation to the model structure that is required to reproduce the influence of public policies, we have developed an agent-based model of a macroeconomic system and analyzed the model structure that is indispensable in reproducing the positive influence of a corporate tax reduction and that includes inefficiency in public expenditure, executive compensation, the use of internal funds for investment, and not-too-severe credit creation. The strongest factor among them is inefficiency in government expenditure, where said inefficiency is defined as the ratio of a firm subsidy to the total value of government expenditure (Ogibayashi and Takashima 2013, 2014). Here, the inefficiency in government expenditure in our previous and present studies corresponds to the degree to which the government pays an excessive amount of money to firms compared with its economic value. This payment of an excessive amount of money is a kind of transfer payment or firm subsidy that is made in the real economy in various ways behind the mask of economic policy packages such as industrial promotion and the protection of domestic industry. With respect to previous research on the ABM approach on the fiscal multiplier, there is a very recent paper that analyzed the influence of the credit market on the fiscal multiplier (Napoletano et al. 2015). However, that research focused on the influence of government spending on the impact of and recovery from the exogenously given bankruptcy shock of a household in a specifically designed artificial society that does not seem to be an imitation of the real economic society in the sense that, in the real system, every macro phenomenon emerges as a result of interactions between decision-making agents. Moreover, it does not analyze the influence of the way of government spending. The present study introduces the idea of inefficiency in government expenditure and conducts a series of computer experiments, based on the ABM of artificial economic systems developed by the authors (Ogibayashi and Takashima 2009, 2013, 2014), to analyze the influences of inefficient public expenditure on the multiplier of public investment and the gross domestic product (GDP). The system structure of the model in the present study is essentially the same as that of the model that reproduced the influence of a corporate tax cut. In addition to the ABM approach, mathematical equations for the multiplier of public investment are derived according to our revision of Morishima’s economic linkage table; also discussed are the mechanism for the influence of inefficiency in government expenditure on the multiplier of public investment, and the reason why the multiplier is sometimes <1.",2
50.0,4.0,Computational Economics,24 August 2016,https://link.springer.com/article/10.1007/s10614-016-9612-1,Computational Experiments Successfully Predict the Emergence of Autocorrelations in Ultra-High-Frequency Stock Returns,December 2017,Jian Zhou,Gao-Feng Gu,Wei-Xing Zhou,,,,Mix,,
50.0,4.0,Computational Economics,06 September 2016,https://link.springer.com/article/10.1007/s10614-016-9613-0,Is the Extension of Trading Hours Always Beneficial? An Artificial Agent-Based Analysis,December 2017,Kotaro Miwa,Kazuhiro Ueda,,Male,Male,Unknown,Male,"Recently, the extension of trading hours for stocks has increasingly been discussed. In several markets (e.g., NYSE and NASDAQ), both pre-market and after-hours trading sessions have already been introduced, and the Tokyo Stock Exchange is considering the extension of trading hours by introducing extended-hours sessions and/or shortening the midday recess. The extension of trading hours is intended to provide more trading opportunities and improve price efficiency (Osaki 2014). Periodical market closures could have significant negative effects on price efficiency and trading opportunity. First, periodical market closures might impede stock prices from incorporating public and private information. Kyle (1985), Glosten and Milgrom (1985), Foster and Viswanathan (1990), and Easlay and O’Hara (1992) show that public and private information accumulates overnight, while information asymmetry declines over the course of trading periods. These studies suggest that market closures may lead to a delay in the incorporation of information into asset prices, which could widen the divergence between asset prices and their fundamental values. Second, periodical market closure may cause excessive price fluctuations, especially at the beginning and end of the trading session on an intraday basis. Wood et al. (1985) and Harris (1986) find that a standard deviation in returns is especially high at the open and close of the regular-hours session; this U-shaped pattern of return volatility is also found in non-U.S. markets (Hamao and Hasbrouck 1995; Abhyankar et al. 1997). Third, periodical market closures could cause skewed trading activity, i.e., trading could be concentrated at the open and close of the trading session. Jain and Joh (1988) document a U-shaped intraday pattern of trading volume; trading volume is especially high at the open and close of the regular trading session. Finally, periodical market closures could reduce investors’ trading opportunities. Therefore, the extension of trading hours is likely to mitigate the market inefficiencies caused by market closures, i.e., extending trading hours could lower the divergence between market prices and fundamental values, lower return volatility (especially at the open and close of the regular-hours session), increase daily trading volume, and ease concentration of trading activity at the open and close. However, limited investor participation during extended hours raises concern about the effect of extending trading hours. The trading per time unit in the after-hours sessions is less than 5 % of the trading per time unit in the regular-hours sessions in the U.S. stock market. (Barclay and Hendershott 2004). Although trading during the extended-hours session allows investors to react quickly to after-market news, market prices are less efficient during the extended-hours session compared to those during the regular-hours session because of reduced liquidity (Barclay and Hendershott 2003). It is quite uncertain whether extending trading hours would mitigate the market inefficiencies if there were only a few market participants during the extended-hours session. In this study, we uncover the effect of extending trading hours assuming limited market participation during the extended-hours sessions. A few prior studies empirically analyze the effect of extending trading hours. For example, Houstion and Ryngaert (1992) find that reductions in NYSE trading hours had little effect on return volatility and trading volume during the week that the reductions occurred; however, they did have an effect on the distribution of return volatility and trading volume during the week. Fan and Lai (2006) report that a significant change in the intraday pattern of return volatility and trading volume does not be observed after extending the trading session of the Taiwan Stock Exchange by 1.5 h. Although these studies might indicate that the market inefficiencies are not easily mitigated by a change in trading hours, the result might be due to an insufficient change in trading hours. The drawback of empirical analyses on the extension of trading hours is that there is no perfect sample with which to compare prices and trading behavior between a market with the extended-hour session and one without the session. Thus, there is an obvious limitation in showing the effect of the extended-hours session via empirical analyses. In contrast, model-based analysis allows for the direct comparison of price behavior and trading activity between a market with extended hours and one without extended hours. Additionally, we can easily understand the underlying reasons as to why the extension of trading hours is effective or ineffective when there are a limited number of market participants during the extended-hours sessions. Thus, in this study, we perform a model-based analysis to analyze the effect of extending trading hours. We specifically designed our simulation model based on that of Brock and Hommes (1998) instead of using existing models that are built for analyzing the effect of market closure on price and trading volume. Brock and Kleidon (1992) extend Merton’s (1971) model to analyze the effect of market closure on the trading concentration at the open and close of the regular-hours session. To analyze the intraday pattern of price and trading behavior, Hong and Wang (2000) develop a competitive market model with periodic closures, where investors trade for allocation and informational reasons. Brock and Kleidon’s (1992) model is in a partial equilibrium setting and cannot show the equilibrium dynamics between returns and trading volume. More importantly, to be analytically tractable, both models assumed that there are always enough market participants; in other words, the models cannot analyze how the number of market participants affects market prices and trading activity. Therefore, these models do not allow the analysis of the effect of extended-hours sessions with a limited number of market participants. In contrast, the model proposed by Brock and Hommes (1998) is a simple simulation-based model with evolutionary dynamics; the model’s strengths are its simplicity and high flexibility (including high scalability). Therefore, in this study, we extend their model by incorporating extended-trading hours with limited investor participation. First, we show that the extended model can reproduce the U-shaped intraday pattern of return volatility and volume, the gradual incorporation of fundamental information during the regular-hours session, and two important stylized facts: fat-tailed returns and clustered volatility. Subsequently, the effect of illiquid extended-hours sessions is analyzed on the basis of following three factors: the deviation between asset prices and fundamental values, volatility of asset returns (especially at the open and close of the regular-hours session), and trading volume (especially at the open and close). If illiquid extended-hours sessions mitigate the negative effect induced by market closure, the deviation between market price and fundamental value, return volatility, and trading concentration at the open and close should be reduced, and daily trading volume should be increased by the extension. The rest of this paper is constructed as follows. Section 2 derives the market model with limited extended-hours trading. Section 3 presents the simulation results for whether the extension of trading hours is beneficial, even if there are limited market participants during the extended-hours session. Section 4 concludes the paper with a summary of the main findings.",9
50.0,4.0,Computational Economics,19 November 2016,https://link.springer.com/article/10.1007/s10614-016-9631-y,Endogenous Fundamental and Stock Cycles,December 2017,Weihong Huang,Yu Zhang,,Unknown,,Unknown,Mix,,
50.0,4.0,Computational Economics,22 August 2016,https://link.springer.com/article/10.1007/s10614-016-9614-z,The Psychological Force Model for Lowest Unique Bid Auction,December 2017,Rui Hu,Jinzhong Guo,Tao Zheng,Male,Unknown,,Mix,,
50.0,4.0,Computational Economics,24 May 2017,https://link.springer.com/article/10.1007/s10614-017-9694-4,Can Sentiment Analysis and Options Volume Anticipate Future Returns?,December 2017,Patrick Houlihan,Germán G. Creamer,,Male,Male,Unknown,Male,"In today’s society, much human interaction takes place online through blogs, emails and chat boards, to name a few. Blogging websites like Twitter, have gained mass popularity and serve as a medium for communicating through a few sentences, embodying the low social presence and high self-disclosure classification of Social Media as defined by Kaplan and Haenlein (2010). The nature of microblogs, being more to the point on a topic and less verbose (140-character limit for Twitter posts), make them prime candidates to extract sentiment for use in predictive analytics (Bermingham and Smeaton 2010; Ghiassi et al. 2013; Martínez-Cámara et al. 2014; Aisopos et al. 2016; Saif et al. 2016). 
Gruhl et al. (2005) showed that blogs and other on-line social media websites are predecessors to ‘real-world’ behavior and the volumes of posts related to various products on Amazon’s website are highly correlated with actual purchase decisions. Pang and Lee (2004) provided further support for social media data as a viable source to use in predictive analytics, which is validated by the fact that people are more inclined to share their opinions on social media websites to mere strangers. Extracting features from social media messages have proven to be a robust method for a variety of different labels. Hennig-Thurau et al. (2015) and Asur and Huberman (2010) leveraged Twitter messages and tweets related to a specific movie before its release date and showed a positive correlation between message volume and movie ticket sales. Wu and Brynjolfsson (2014) created an index of Google search queries related to housing prices and sales, which was shown to be a forward-looking indicator of the housing market trends. Choi and Varian’s (2012) research showed that Google query search volume is a strong predictor of future economic activity in various industries. Also, Google trends data was leveraged to forecast weekly volatility by Hamid and Heiden (2015). These studies further validated the internet as a source for robust predictive data and behavior patterns. Several studies related to capital markets suggested the volume of stock chatter messages were a predictor of volatility and next day returns (Wysocki 1998; Tumarkin and Whitelaw 2001; Antweiler and Frank 2004; Da et al. 2011; Zhang et al. 2013, 2014; Shen et al. 2016). Bollen et al. (2011) extracted the mood state and sentiment of many users on a stock blogging site and presented highly predictive directional moves in the Dow Jones Industrial Average, two days out, with an 87.6% accuracy. Also, Houlihan and Creamer (2015) leveraged volume and sentiment as features from StockTwit messages and showed how they help explain continuation and reversal effects. Sentiment will be one of the main features used in this research. Another way to capture the market sentiment is through the options market. Anthony (1988) has shown that increased trading in call options leads to next day gains in various underlying stocks that experienced a spike in call volume the day prior. The latter research would warrant using call option volume as a feature for a model to predict a label, such as future directional moves. Chen and Lu (2017) identified stocks with large decreases in option implied volatility experienced abnormal gains. Cao et al. (2003) find that option volume imbalances, specifically, short-term out of the money call option volumes, are predictors of pending takeovers. This finding points to a somewhat inefficient market, one where only informed traders have access to insider information before an announcement. However, this inefficiency can be leveraged as an indicator for a model that attempts to predict a label such as the next day directional move. Billingsley and Chance (1988) showed one such indicator, the put-call ratio, to yield abnormal gains when used in a trading strategy. The put-call ratio, PCR, is simply the total daily put volume divided by the daily call volume for a particular equity. Intuitively, a ratio below 1.0 would point to a bullish indicator, whereas a ratio greater than 1.0 points to a bearish indicator. However, Billingsley and Chance (1988) show that a ratio of 0.7 is a better threshold. Additionally, not only is PCR suggestive of being a short-term indicator for near-term directional moves of stocks or indexes, but the PCR also seems to be more of a contrarian indicator than a conformist indicator. In fact, several other indicators are contrarian in nature, including short-term interest and VIX. Hu (2014) shows that imbalances between option volume and underlying volume predict future stock returns. Pan and Poteshman (2006) also show that volume for specific traders contained information about future prices. This latter study had access to a unique data set that showed new buyer volume that was broken out by various traders. Unique put-call ratios were derived using each particular trader. The data (1990–2001) was analyzed using a univariate regression, where the independent variables are the corresponding put-call ratios and the dependent variable is the next day risk-adjusted return. The results showed stocks with low put-call ratios derived from a particular trader (full-service) outperformed stocks with high put-call ratios by \(+\)40 basis points on the next day and 1% over the following week. The premise here is that informed, full-service investors trading the underlying stock instead of index options have firm, specifically related information rather than market-wide news. Also, stocks that went through periods of higher breadth (advancing issues relative to declining issues) rewarded investors with abnormal returns of 2.92% in 6 months and 4.95% in a 12-month period as shown by Chen et al. (2002). Also, Houlihan and Creamer (2014) formulated trader specific call-put ratios based on option contract volume and determined that specific traders have superior information over other traders as they showed higher Sharpe ratios with specific trader call-put ratios. The contribution of this research suggests that sentiment extracted from social media messages and market data based call-put ratios contain information to forecast asset returns. In addition, we leveraged a unique dictionary which captures measurable mood states of authors. Sentiment is a crowd-sourced measure from the general investing community and behavior is in the form of overreactive and especially underreactive effects observed by investors. Additionally, the call-put ratios represent traders whose sentiment and behavior can be captured through option volume data. Leveraging all features together yielded the highest monthly cumulative returns and annualized Sharpe ratios, suggesting the additional information generated by combining both sentiment and behavior from social media and market data improved asset return direction. Lastly, we validate several risk factors that help explain asset price returns.",7
50.0,4.0,Computational Economics,24 November 2016,https://link.springer.com/article/10.1007/s10614-016-9637-5,Emergent Heterogeneity in Keyword Valuation in Sponsored Search Markets: A Closer-to-Practice Perspective,December 2017,Agam Gupta,Biswatosh Saha,Uttam K. Sarkar,Unknown,Unknown,Male,Male,"The use of search engines in navigating the Internet in an expanding digital world makes the search engine an integral part of corporate marketing plans. Keyword-based advertising on a search engine, also referred to as sponsored search advertising (SSA), is one of the important forms of online advertisement which accounts for nearly half of the total online advertising spends of about 50 billion US$ (IAB 2014). These advertisements are sold through a ‘second price auction’ (Edelman et al. 2007) process wherein an advertiser bids the maximum amount she is willing to pay for a user click, but the advertiser pays only when her ad is clicked. The initial body of work on SSA focused on the problem of allocating advertisement slots available on the search result page of ‘each keyword’ to advertisers who compete for a higher (or favorable) slot that commands better user attention and clicks [for a detailed review of the work refer to Maillé et al. (2012)]. The problem of allocating multiple advertising slots on a search query necessitated a shift from the truthful Vickrey auction (Vickrey 1961) which focused on determining a single winner for the article being auctioned. Search engines solved this problem through the use of the Generalized Second Price (GSP) auction mechanism to price the ordered listing of ads (Edelman et al. 2007). Extant literature builds an understanding of these search engine markets based on the notion of an equilibrium (Katona and Sarvary 2010), where no advertiser has incentives to exchange her position with another advertiser (Maillé et al. 2012, p. 273). Thus, the market would eventually converge to stable bids from advertisers who would be ordered according to the decreasing intrinsic valuation of the keyword (assuming no change in valuation in the meantime). The aforementioned understanding is based on a class of models that assume a ‘known value per click’ of an individual keyword by an advertiser. However, in the absence of this assumption the existence of an equilibrium cannot be proved (Abrams et al. 2007) and we believe that this assumption is rather strong and may not represent reality well. Consider the following illustrative example. Figure 1 shows dataFootnote 1 on the keyword ‘automated mobile testing’—the position at which an advertiser’s ad was placed, and the bid of the advertiser for this keyword over fifteen weeks on a popular search engine. We observe significant fluctuations in the bid and the ad position. In other words, an advertiser’s bid on a keyword undergoes frequent revisions and is not temporally stable. Bid changes affect ad position, and the advertiser does not seem to have a fixed ‘known value’ for the keyword. This illustrative instance motivates our enquiry and points to significant temporal dynamics in the market that have hitherto remained unexplored in literature. Average bid and average position of the advertiser for the keyword ‘automated mobile testing’ If there is no known click value, how do advertisers bid on keywords? In addition, advertisers only have a vague idea about various market parameters like the click through rate at different advertising slots unlike the assumed perfect knowledge required for equilibrium results in extant models. Feldman et al. (2007) showed that budget optimization in sponsored search campaigns is an NP-Hard problem. The imperfect knowledge of the advertiser, along with the inherent complexity in managing sponsored search ad campaigns comprising thousands of keywords, calls for taking into account the actual bidding practices of advertisers. Advertisers often engage the help of advertising agencies specializing in managing sponsored search campaigns. Most often these agencies employ simple ‘satisficing’ heuristics to manage the bids of an advertiser’s account (Maillé et al. 2012, p. 281). The literature on advertiser bidding strategies suggests that the non-truthful nature of GSP auctions induces strategic bidding behavior (Edelman and Ostrovsky 2007). This may give rise to a plethora of Nash-equilibria (Edelman et al. 2007) and it is not known a priori if any of these equilibria are actually reached in real keyword auctions (Maillé et al. 2012, p. 287). While this stream of literature does look at few bidding strategies,Footnote 2 these bidding strategies have been analyzed for markets where ads are ranked based on their bid, often referred to as rank-by-bid, and have not considered the more widespread case (in Google or Yahoo search engines) of ranking ads based jointly on advertiser bid and the past click-through rate (CTR) of the advertiser on that keyword, referred to as rank-by-revenue (since revenue is generated only on actual clicks). To understand the bidding decisions used by advertisers in real life, we studied the practices followed in an advertising agency. We conducted several interviews with the account management executives of the advertising agency to understand their practices and routines in detail. Further, the researchers also gained access to internal process documents which were used by the advertising agency to train new recruits. In addition to the interviews one of the researchers spent several hours in informal interaction with the employees of the advertising agency which allowed observing the practices of the account managers and executives from close quarters. Through these interviews and interactions we found that advertiser’s bids and costs were managed as a portfolio rather than as independent, individual keywords. All decisions with respect to bid management were taken on the aggregate performance of the keyword portfolio. This was contrary to literature which assumes ‘independent’ valuation of a keyword in arriving at equilibrium bids by advertisers. Further, the agency often had a target of delivering a pre-defined number of clicks at a given cost. This target cost was not specific to an individual keyword but acted as a constraint at the aggregate level of a portfolio of keywords. In this research, we explore the dynamics of the market when advertisers do not have an ex-ante differential valuation for keywords, rather, follow simple cost-cap heuristics to manage their sponsored search ad campaigns in a rank-by-revenue auction system. In particular, we enquire if embracing a closer-to-practice approach in modeling the market with advertisers with simple cost-cap heuristics yields qualitatively interesting insights. Ranking of ads based on CTR and bid (expected revenue) induces a temporal connectedness between successive auctions of a keyword making sponsored search auctions path dependent. The portfolio based bid management practice spatially connects the auctions on different keywords as the bids are determined based on aggregate performance. Thus, auctions in sponsored search market could be both temporally and spatially connected. The spatial and temporal connectivity among auctions, coupled with the start conditionsFootnote 3 (which question necessary conditions for equilibrium) make any mathematical closed form analysis untenable. In this research, we simulate an artificial sponsored search market and bring out the dynamics of the different bid management heuristics, one derived from industry practice where keywords are managed as a portfolio, and the other as a comparative case, where keywords are managed individually. The results of the simulations contribute to literature in the following two ways. Firstly, we find that different levels of the market exhibit different dynamics. While at an individual keyword level advertiser bids are constantly in flux, at an aggregate level (of all keywords) the search engine revenue converges to a value slightly above the target cost imposed by the advertiser. In contrast to the bids that are undergoing frequent changes, an advertiser’s click share on a particular keyword is rather stable exhibiting path dependence and increasing return dynamics taking place in the market. Secondly, we find that the distribution of keyword bids and cost per clicks are emergent in nature, that is, they exhibit a novel and coherent structure occurring at the macro level, in contrast to the micro-level components and processes out of which they arise (Goldstein 1999). While the literature treats valuation of a keyword as an intrinsic characteristic of the keyword, specific to an advertiser, and thus known ex-ante, we show that keyword valuation distribution can also be seen as an emergent outcome of complex market interactions which becomes known only in an evolutionary sense through observed bid or cost per click information. Experiments with two alternate bid management heuristics show that the bid management heuristic affects both the observed absolute valuation of a keyword (by altering the cost per click of the keyword) and its observed relative valuation with respect to other keywords. The rest of the paper is organized as follows. Section 2 describes the setup of the simulation model. The results of the simulation are presented in Sects. 3 and 4 discusses the key implications of the results. Section 5 concludes the paper.",2
51.0,1.0,Computational Economics,11 March 2017,https://link.springer.com/article/10.1007/s10614-017-9670-z,Comparing Solution Methods for DSGE Models with Labor Market Search,January 2018,Hong Lan,,,,Unknown,Unknown,Mix,,
51.0,1.0,Computational Economics,05 November 2016,https://link.springer.com/article/10.1007/s10614-016-9629-5,Where has All the Education Gone? Everywhere But into Growth,January 2018,Hongchun Zhao,Yanjie Liu,,Unknown,Unknown,Unknown,Unknown,,
51.0,1.0,Computational Economics,28 October 2016,https://link.springer.com/article/10.1007/s10614-016-9630-z,Terms of Trade Shocks and Monetary Policy in India,January 2018,Chetan Ghate,Sargam Gupta,Debdulal Mallick,Unknown,Unknown,Unknown,Unknown,,
51.0,1.0,Computational Economics,09 June 2016,https://link.springer.com/article/10.1007/s10614-016-9597-9,R&D-based Calibrated Growth Models with Finite-Length Patents: A Novel Relaxation Algorithm for Solving an Autonomous FDE System of Mixed Type,January 2018,Hwan C. Lin,L. F. Shampine,,,Unknown,Unknown,Mix,,
51.0,1.0,Computational Economics,02 September 2016,https://link.springer.com/article/10.1007/s10614-016-9618-8,Investment Index Construction from Information Propagation Based on Transfer Entropy,January 2018,Fujio Toriumi,Kazuki Komura,,Male,Male,Unknown,Male,"In recent years, the number of individual investors has increased in financial markets. However, a large information gap exists between individual and institutional investors, unduly impairing individual investors, which may have a bad influence on the market. There are especially large information gaps on short-term risks. For example, sudden fluctuations in market consensuses caused by rapid political/economical changes strongly affect short-term price changes. In such cases, to build a profitable position, institutional investors exploit their advantages in information collection, creating information gaps. The gap causes big problems for individual investors. In this study, we propose a new investment index that focuses on the relationships among stocks to help manage the risk of individual investors. In such situations, individual investors must understand the influences to constructing a portfolio to realize risk management. Thus, technology for understanding the relationships among each stock is required to provide support systems for individual investments. For instance, if the short-term influential spread among securities can be captured, individual investors are helped estimate the risk of holding stocks. In this study, we propose a method that quantifies the relationships among securities that are difficult to predict qualitatively. To analyze such relationships among securities, a cross-correlation matrix (Plerou et al. 1999) is often used as a general method. However, it suffers from the following limitations: It is affected by the strong influence of drawdowns; It is unable to consider influence from plural securities; It is required to expand its method to be able to analyze causality. In this study, we employ Schreiberfs transfer entropy (Schreiber 2000) to detect the relationships among securities. Transfer entropy is a method of time-series analysis that effectively detects causality. It uses discrete values that allow the influence of drawdowns to be ignored. Moreover, because transfer entropy has an additivity when the time-series data have stochastic independence, it can also consider the influence from plural securities. With the above characteristics, we develop a new investment index that considers information propagation among securities by transfer entropy. We also evaluate our proposed index using a virtual transaction simulation.",5
51.0,2.0,Computational Economics,04 October 2016,https://link.springer.com/article/10.1007/s10614-016-9625-9,A Network Analysis of the United Kingdom’s Consumer Price Index,February 2018,Georgios Antonios Sarantitis,Theophilos Papadimitriou,Periklis Gogas,Male,Unknown,Male,Male,"The Inflation Targeting (IT) regime was introduced by the central bank of New Zealand in 1990. Since then, several central banks from advanced economies have adopted this monetary policy framework, including the FED, the ECB and the Bank of England. For an efficient implementation of the IT regime, the precise measurement of inflation is crucial. However, the headline inflation measures include components that can display highly volatile behavior, causing the estimated inflation to deviate temporarily from its true underlying trend. This problem is tackled by measuring the core inflation. The measures of core inflation attempt to dampen the temporary deviations that are caused by highly volatile components, in order to unveil the true outlook of inflation dynamics. This task has been undertaken so far by using classic econometric analysis. An indicative list of papers that suggest new or compare existing measures of core inflation includes the ones of Bryan and Cecchetti (1994), Cecchetti (1996), Scott (1997), Clark (2001), Cogley (2002), Mankikar and Paisley (2002), Marques et al. (2003), Kapetanios (2004) and Armour (2006). However, the dynamic complexity of modern financial and economics systems is such, that the sum of the parts is often greater than the system as a whole. Complex Networks are able to capture the inter-dependencies of the elements of the studied system and to highlight their emerging behaviors, providing a detailed analysis from the aggregate network level down to the agent-specific one. Thus, Network Theory can be an alternative and complementary framework for Consumer Price Indices’ analysis. The central goal of this paper is to construct a new measure of core inflation for the United Kingdom, taking advantage of the latest advancements in Complex Network analysis and optimization techniques. This work is, to our knowledge, the first attempt to study the United Kingdom’s Consumer Price Index (CPI) as a complex network. Our work is policy oriented, since we seek to provide the Bank of England with an additional measure of the underlying inflation. Nevertheless, we take advantage of the complex network framework and we perform a multi-stage analysis of the U.K. CPI network, offering new insights to U.K. policy makers in each step. The history of network analysis (or Graph Theory) goes back to the eighteenth century. It was originally used in path routing problems (Euler 1741) and later in many other scientific fields and applications such as biology (Guimera and Amaral 2005), social sciences (Eubank et al. 2004), the Internet (Barabási et al. 2000) and economics (Mantegna 1999; Allen and Gale 2000; Acemoglu et al. 2012). In this study we focus on the interactions of the 85 U.K. CPI classes, depicting them as a complex network. In this context, the nodes of the network pertain to the 85 U.K. CPI classes of goods and services, while the edges that connect them are calculated as the cross-correlations between each pair of the classes’ time series. A central contribution of our paper lies within the use of an optimization technique called the Threshold-Minimum Dominating Set (T-MDS), through which we are able to identify a minimum set of representative classes that can adequately describe the entire network of CPI classes. The classic Minimum Dominating Set (MDS) is an optimization process which identifies a minimum subset of nodes such that every node of the network belongs to that subset or is directly connected to one or more of its components. The MDS is well-known for data mining applications (Shen and Li 2010), and metabolic networks’ analysis (Taghipour et al. 2012). The first time that the classic MDS was used for the analysis of complex economics networks was in Papadimitriou et al. (2013). We provide a three-stage empirical analysis of the U.K. CPI network, using different tools and techniques from Graph Theory in each stage. First, we calculate standard network analysis metrics to study the network’s topology and to provide inference regarding its evolution through time. Second, using clustering techniques, we identify non-overlapping groups (communities) of classes to infer on the network segmentation. In the last stage of our analysis we apply the T-MDS optimization technique to identify a minimum subset of network representative nodes. Several interesting new insights are provided in each stage of this analysis. First, the results from the basic network metrics’ analysis indicate that the U.K. CPI class network is rather sparse in normal times (low correlations between the CPI classes) but the latest crisis induced a considerable shock to the network topology, leading to an increased average correlation between the CPI classes and consequently to a denser network. Second, we find that the identified CPI class communities differentiate substantially from the actual U.K. CPI categories that these classes belong to. This finding suggests that stronger correlations exist between classes of different CPI categories rather than between the classes of same categories and that diverse (macroeconomic) factors impact the classes in each CPI category. Finally, we employ the T-MDS technique to identify the most representative of the 85 U.K. CPI classes and we construct a new, reduced version of the official CPI which satisfies the criteria of being an appropriate core inflation measure for the U.K. Using econometric analysis, we compare our proposed measure with the core inflation measures currently used by the Bank of England. We find that the proposed T-MDS measure presents lower volatility than two of the Bank of England core inflation measures and moreover it is the only measure that qualifies as an unbiased attractor of the CPI inflation vis-à-vis the seven core inflation measures used by BoE. This new measure can be conveniently added to the official Bank of England’s set of core inflation measures as it requires no further resources for its calculation. The rest of the paper is organized as follows: the next section presents the data set and the network construction process. Section 3 describes the methodological context. In Sect. 4 we provide and discuss the empirical results and in Sect. 5 we conclude the paper with a brief summary.",9
51.0,2.0,Computational Economics,07 March 2017,https://link.springer.com/article/10.1007/s10614-017-9672-x,State and Network Structures of Stock Markets Around the Global Financial Crisis,February 2018,Jae Woo Lee,Ashadun Nobi,,,Unknown,Unknown,Mix,,
51.0,2.0,Computational Economics,18 November 2016,https://link.springer.com/article/10.1007/s10614-016-9632-x,Systemic Risk on Trade Credit Systems: with the Tangible Interconnectedness,February 2018,Jisang Lee,Duk Hee Lee,Sung-Guan Yun,Unknown,,Unknown,Mix,,
51.0,2.0,Computational Economics,13 June 2016,https://link.springer.com/article/10.1007/s10614-016-9596-x,Another Look at Large-Cap Stock Return Comovement: A Semi-Markov-Switching Approach,February 2018,Kaihua Deng,,,Unknown,Unknown,Unknown,Unknown,,
51.0,2.0,Computational Economics,01 June 2017,https://link.springer.com/article/10.1007/s10614-017-9699-z,"Artificial Momentum, Native Contrarian, and Transparency in China",February 2018,Hung-Wen Lin,Mao-Wei Hung,Jing-Bo Huang,Unknown,Unknown,,Mix,,
51.0,2.0,Computational Economics,16 September 2017,https://link.springer.com/article/10.1007/s10614-017-9692-6,Estimation of Dynamic Mixed Hitting Time Model Using Characteristic Function Based Moments,February 2018,Yogo Purwono,Irwan Adi Ekaputra,Zaäfri Ananto Husodo,Unknown,Unknown,Unknown,Unknown,,
51.0,2.0,Computational Economics,06 September 2017,https://link.springer.com/article/10.1007/s10614-017-9741-1,Sparse Bayesian Variable Selection with Correlation Prior for Forecasting Macroeconomic Variable using Highly Correlated Predictors,February 2018,Aijun Yang,Ju Xiang,Hongqiang Yang,Unknown,,Unknown,Mix,,
51.0,3.0,Computational Economics,23 June 2016,https://link.springer.com/article/10.1007/s10614-016-9599-7,Calibrating the Italian Smile with Time-Varying Volatility and Heavy-Tailed Models,March 2018,Michele Leonardo Bianchi,Svetlozar T. Rachev,Frank J. Fabozzi,Female,Male,Male,Mix,,
51.0,3.0,Computational Economics,13 August 2016,https://link.springer.com/article/10.1007/s10614-016-9608-x,Pricing Credit Default Swaps Under Multifactor Reduced-Form Models: A Differential Quadrature Approach,March 2018,Alessandro Andreoli,Luca Vincenzo Ballestra,Graziella Pacelli,Male,Male,Female,Mix,,
51.0,3.0,Computational Economics,16 August 2016,https://link.springer.com/article/10.1007/s10614-016-9609-9,A Discontinuity Model of Technological Change: Catastrophe Theory and Network Structure,March 2018,Torsten Heinrich,,,Male,Unknown,Unknown,Male,"Path dependence and discontinuity have been central issues for institutional and evolutionary economics since at least Gunnar Myrdal’s introduction of the idea of Circular Cumulative Causation (see, e.g., Myrdal 1974), possibly even since Thorstein Veblen’s (1898, 2008) earlier writings on Cumulative Causation and path dependence in history, institutions, and fashion goods. A formal framework to capture these concepts presented itself with René Thom’s (1975) analysis of catastrophe theory. It has since been applied to different aspects of economic systems (Woodcock and Davis 1978; Herbig 1991; Lange et al. 2004; Rosser 2000; Dou and Ghose 2006 both with and without the framework of institutionalist theory and evolutionary economics. Catastrophe theory models in economics do, however, generally operate at an aggregate level only; in fact, they became controversial in the 1990s (for an overview, see, e.g., Rosser 2007). Consideration of, for instance, group or network structures would transform the model into a set of distinct but overlapping models, thereby increasing the system’s complexity by orders of magnitude. The current paper presents a simple catastrophe theory model of technological change with network externalities. The model is then complemented by adding an agent-based micro layer. This allows studying the impact of different network structures among the agents but also provides some justification for the initial aggregated-level model, as it confirms the dynamics found in the aggregated model. The technological-institutional layer of the model is very simple; a base technology is contrasted with a new innovative technology that the agents may adopt. This follows the spirit of earlier dynamic models of technological change, including the tradition of generalized urn schemes (Arthur et al. 1987) and the few earlier catastrophe models (Lange et al. 2004; Dou and Ghose 2006). In this system, the adoption dynamics follow a replicator equation with capacity boundary, superlinear acceleration of adoption as a result of the network externality, and a constant drag interpreted as an infrastructure cost. This yields a polynomial that can be reduced to the canonical equation of a cusp catastrophe (for overviews on cusp catastrophes, see, e.g., Sewell 1977; Saunders 1980; Rosser 2007). While the aggregated-level version of the model thus produces a classical cusp catastrophe—a result that is preserved in the agent-based form—it is found that the behavior of the model changes locally with the network structure. The network structure affects the agent’s neighborhood sizes (number of directly connected agents) and their distribution. In the agent-based version, the adoption decisions are governed by the number or share of direct neighbors adopting the technology, not the number or share of adopters in the general population. It is therefore not surprising that the network as such has an effect on the results at the aggregated level. Depending on the network structure, different local neighborhoods may persist in different adoption states (specifically for small world networks) and the theoretical equilibria of the underlying aggregated level equation may exhibit different degrees of stability and sensitivity depending on the network structure. In turn, the network structure affects the general catastrophic or non-catastrophic outcomes as well when the slow variables (the catastrophe parameters, namely the cost and the capacity boundary) are included in the simulation, i.e. when the catastrophe is allowed to happen. While the present work investigates merely an aspect out of a large possibility space, it encourages further research using agent-based catastrophe theory models especially of economic aspects to which catastrophe theory has previously successfully been applied; aspects such as technological and institutional change, economic crises, or industry structure. In Sect. 2, a more detailed overview of the literature on both network externalities and catastrophe theory in economic systems will be given. Section 3 proceeds with a simple theoretical model of an industry subject to network externality that results in a classical cusp catastrophe. This model will then be extended to an agent-based version and the role of the network structure and other aspects on the behavior of the model will be studied in Sect. 4. Section 5 concludes.",7
51.0,3.0,Computational Economics,17 August 2016,https://link.springer.com/article/10.1007/s10614-016-9610-3,A Dynamic Model of Unemployment with Migration and Delayed Policy Intervention,March 2018,Liliana Harding,Mihaela Neamţu,,Female,Female,Unknown,Female,"Creating new job opportunities is a priority for any vibrant economy. In the aftermath of the financial crisis government efforts have primarily focused on containing instability and reducing public debt, yet active labour market policies might be needed to promote employment. New vacancies reduce the number of those registered as unemployed, but they conceivably attract as well new migrant workers. International migration can add to the uncertainty in labour market prospects for natives, and easily translates into a sensitive issue for policy makers to review in times of economic instability (Facchini and Mayda 2008; Blanchflower and Shadforth 2009; Hatton 2014). Arguably, a successful policy addressing unemployment and that benefits the economy over time creates employment opportunities for natives, as well as supporting migrants. We hence propose to explore the viability of a labour market policy supporting vacancy creation, where the government acknowledges the simultaneous search for jobs by natives and new immigrants. We consider unemployment in migrants destinations as a factor in the latters’ decision to move. Unemployment levels influence the potential to match migrants with employment opportunities abroad and it can act as a deterrent to further immigration. The empirical literature on labour mobility finds a weak and ambiguous relationship between unemployment and migration (see for a survey of relevant studies Bauer and Zimmermann 1999; Jean and Jimenez 2011; Angrist and Kugler 2003; Blanchflower and Shadforth 2009; Damette and Fromentin 2013). Yet, Hatton (2014) concludes that during an economic slump the increase in unemployment is typically associated with a decline in immigration. As such, we will accept that migration is declining where unemployment is on the rise, further reflecting the experience during the recent economic crisis. The complex interaction between migration and unemployment over time motivated a dynamic model capturing developments in aggregate unemployment for an open economy. We build on insights from job search and matching models of unemployment [e.g. Pissarides (2011)] but focus on aggregate flows to and out of unemployment, along with migration and its labour market impact in an open economy. We analyse the fluctuation in unemployment in a continuous time framework, as previously proposed by Shimer (2011) when undertaking empirical tests of US unemployment factors. We explore more closely the dynamic market interaction between unemployment, vacancies, employment and migration, by adding policy intervention to the framework. Earlier research considering the economic dynamics of migration is rare, but Camacho (2013) models the decisions of inter-regionally mobile skilled workers and analyses the steady state stability of a system informed by the new economic geography framework. The paper provided numerical simulations presenting stability trajectories in the context of different technologies. While also building a dynamic system and undertaking simulation for an open economy, we depart in this paper from the individual choices of migrants and focus instead on the aggregate outcomes in terms of unemployment, with migration a contributing factor. Our approach is to build a system modelling the dynamics of unemployment and to question the extent to which the policy framework leads to locally stable outcomes. The local stability of our system is analysed theoretically under particular conditions, on the basis of a non-linear dynamic mathematical model with distributed delays. Earlier economic literature modelling delays in a different decision making context observes that lack of lags in information, while conventionally a source of stability, it can also destroy a stable equilibrium (Huang 2008). To test the impact of delays in policy reaction we simulate our system under alternative specifications. We observe a variation in outcomes where migration is built into the policy reaction function and enhances labour market intervention beyond the usual response to local unemployment. Intervention in reaction to particular events has been previously considered by Nikolopoulos and Tzanetis (2003) who developed a model that looks at housing allocation of homeless families due to a natural disaster. Using concepts from this paper, Misra and Singh (2011) constructed and analysed nonlinear mathematical models for the reduction of unemployment. In Misra and Singh (2013), the model is further described by a nonlinear dynamic system with delay. Their system includes: the number of unemployed, the number of employed, and the number of newly created vacancies through government intervention. There are separations from existing jobs and posts are being occupied by a proportion of those unemployed, who also benefit from policy induced vacancies creation. A time delay is introduced in the rate of creation of new vacancies through policy action and a detailed stability analysis is provided. Our study is primarily in line with that of Misra and Singh (2011, (2013). The latter started from a macroeconomic perspective in a developing country and observed aggregate matching processes and the interaction between unemployment and policy supported vacancies. We propose to extend the application of this framework to the case where migrants arrive to look for jobs, and can become unemployed or take up employment at destination. Our analysis reflects the circumstances in developed economies that are open to at least some internationally mobile workers. Under conditions similar to the single market operating in the EU there is no direct policy control over migrant numbers and governments just observe the stock of migrants on their territory along with the number of the unemployed. Consequently, any vacancies created through policy intervention will depend on the number of the unemployed observed simultaneously with new migrant labour. In this paper we consider initially a new state variable to supplement the system described in Misra and Singh (2013), the total number of jobs on the market. This variable captures market responses to unemployment and the implicit downward pressures on wages. Due to the fact that “continuously distributed” delay models are more realistic (Ruan 1996), we also propose to use a distributed time delay in relation to both unemployment and migration. We further focus on the number of immigrants as a variable along with market and policy induced job creation. For the resulting nonlinear mathematical model we use stability theory of differential equations. We prove analytically that under some conditions there is a unique positive equilibrium point. Then, for this equilibrium point, we study the local stability behaviour and we analyse the influence of the distributed time delay on the stability properties. In this way, two different kernels are introduced and a detailed analysis is done with respect to these. For both Dirac and weak kernels we prove that Hopf bifurcation occurs and a family of periodic solutions bifurcates from the equilibrium when the bifurcation parameter passes through a critical value. This critical value of delay is obtained analytically. The paper is organised as follows. In Sect. 2 the model for unemployment reduction with migration and two kernels is described. An equilibrium analysis is presented in Sect. 3. For different types of kernels a stability analysis is done in Sect. 4. Numerical simulations are carried out in Sect. 5. Finally, concluding remarks are given in Sect. 6 and the Appendix provides technical details.",13
51.0,3.0,Computational Economics,26 August 2016,https://link.springer.com/article/10.1007/s10614-016-9611-2,Endogenous Grids in Higher Dimensions: Delaunay Interpolation and Hybrid Methods,March 2018,Alexander Ludwig,Matthias Schön,,Male,Male,Unknown,Male,"Dynamic models in discrete time are workhorse models in Economics. However, most of these models do not have an analytic closed form solution and therefore have to be solved numerically. To this purpose, numerous procedures have been developed in the literature, cf. Judd (1998), Miranda and Fackler (2004). If the problem is differentiable, a popular approach is to use first-order methods, i.e., to iterate on first-order conditions based on an exogenous grid of state variables (EXOGM). An important contribution to this literature is Carroll (2006) who introduces the method of endogenous gridpoints (ENDGM). In comparison to EXOGM, ENDGM greatly enhances computational speed because part of the problem can be computed in closed form. This paper investigates extensions of Carroll’s ENDGM to dynamic problems with more than one continuous endogenous state variable. We present and evaluate two alternatives to EXOGM by use of a specific economic model with two endogenous state variables. The first alternative is a full-blown ENDGM which avoids rootfinding procedures throughout but requires a rather complex interpolation method. As a second method we investigate a hybrid method (HYBGM) which stands in between EXOGM and ENDGM by using rootfinding procedures in one dimension combined with standard fast interpolation methods. To understand how the tradeoff between these alternatives arises in higher dimensions, first focus on a simple consumption-savings problem in on one dimension as in Carroll (2006). In a standard exogenous grid method (EXOGM) one solves in each iteration for each grid point on grid \(\mathcal {G}^a\) of today’s state variable a (\(=\)assets) some non-linear problem. The solution is given by the associated control variable c (\(=\)consumption) and next period’s endogenous state variable assets, \(a^\prime \). Solution of this equation also requires interpolation on some function(s) f on \(a^\prime \) because generally \(a^\prime \notin \mathcal {G}^a\)—e.g., f could be the policy function. To summarize, the mapping in EXOGM is \(a \rightarrow c \rightarrow a^\prime \) whereby this mapping requires, among other numerical operations, solving a non-linear equation and interpolation. Also observe that, for some regular grid \(\mathcal {G}^a\), the “endogenous” grid of \(a^\prime \) is generally irregular because the spacing between grid points is a result of the entire mathematical operation. The trick of ENDGM is to reverse the mapping, i.e., \(a^\prime \rightarrow c \rightarrow a\). Instead of working on an exogenous grid for a, this is achieved by defining a grid on next period’s assets, \(\mathcal {G}^{a^\prime }\). Depending on the nature of the problem it is then possible to solve for c analytically. This is the crucial step: The speed advantage of ENDGM relative to EXOGM is achieved because the mapping \(a^\prime \rightarrow c\) has a closed form solution. For given contemporaneous variables c and next period’s \(a^\prime \) one can endogenously compute today’s endogenous state a. Given the regular grid \(\mathcal {G}^{a^\prime }\), the “endogenous” grid of a is generally irregular. In subsequent iterations, it is necessary to interpolate on such an irregular grid. In one dimension this does not cause any specific problems. In this paper we highlight, however, that this irregularity of endogenous grids is the source of a problem specific to ENDGM in higher dimensions. We emphasize that this drawback is not related to the solution of the system of equations per se but results from the endogenously computed states. The resulting state grid is generally not rectangular because gridpoints are irregularly distributed in the space. In consequence, even linear interpolation is much more costly than for conventional rectangular grids. This is easiest to understand again by example. Consider two endogenous state variables a and h, where h is human capital, as in our application. Accordingly, \((a^\prime ,h^\prime )\) are next period’s endogenous state variables. Control variables are consumption c, as before, as well as investment in human capital, i. Corresponding to the one-dimensional problem the mapping in EXOGM is \((a,h) \rightarrow (c,i) \rightarrow (a^\prime ,h^\prime ) \) which requires solution of a system of two non-linear equations. In ENDGM, the mapping is reversed, i.e., \((a^\prime ,h^\prime ) \rightarrow (c,i) \rightarrow (a,h)\). As for the one dimensional problem, this mapping may have a closed form solution but the endogenous grid formed of a, h is irregular. In subsequent iterations one has to interpolate on such an irregular grid. This irregularity severely complicates location of points for interpolation in higher dimensions. Hence, there exists a fundamental trade-off between EXOGM and ENDGM in higher dimensions. On the one hand, EXOGM requires the use of numerical routines throughout whereas ENDGM computes solutions to first-order conditions in closed form. On the other hand, interpolation in EXOGM is on regular grids and therefore simple. Interpolation in ENDGM on irregular grids is much more complex. We solve this complex interpolation by Delaunay triangulation Delaunay (1934) which originates from the field of Geometry and was only recently introduced to Economics by Brumm and Grill (2014).Footnote 1 Our contribution is to investigate its performance in combination with ENDGM. Our in-between method HYBGM uses exogenous gridpoints in one dimension and endogenous gridpoints in the other.Footnote 2 Consequently, the endogenously computed grid is only irregular in one dimension whereas it is regular in the other, a so-called rectilinear grid. Interpolation on a rectilinear grid is easy, just as in the one-dimensional problem. The trade-off between HYBGM and ENDGM is therefore between numerically more costly routines in some dimensions vis-à-vis analytical solutions in all dimensions but a more complex interpolation. Another aspect is that HYBGM is easier to implement. To analyze and to compare these methods we develop a simple consumption savings model with endogenous human capital. Our specification is such that the model features two endogenous state variables, financial assets and human capital. Evaluation of methods in this two dimensional setup is done by comparing speed and accuracy of the different approaches. Our main finding is that HYBGM and ENDGM both dominate EXOGM: they are substantially faster. The relative speed advantage of ENDGM decreases in the number of grid points because the complex interpolation becomes increasingly costly whereby the speed of solvers used in EXOGM and HYBGM improves when the density of the grid increases due to improved initializations. In light of the positive speed results of ENDGM reported for one-dimensional problems in Jørgensen (2013) and several others, the finding that HYBGM dominates EXOGM is not surprising because it preserves the complexity of EXOGM in the one dimension and the speed advantage of the endogenous grid method in the other. That ENDGM dominates EXOGM is a quantitative finding because the resolution of the trade-off between the more complex interpolation method and the simpler solution of the system of non-linear equations is a priori not clear. Furthermore, we also find that ENDGM dominates HYBGM in our infinite horizon application. There we use an “Approximate Delaunay” method which spares out a large part of the computational burden of the interpolation in ENDGM. In our finite horizon application, which uses “Pure Delaunay”, the choice between HYBGM and ENDGM depends on the number of gridpoints in each dimension. For a relatively low number of gridpoints, ENDGM is advantageous and vice versa for HYBGM.Footnote 3
 Related work by Krueger and Ludwig (2007) in heterogenous agent models and Barillas and Fernandez-Villaverde (2007) in the neoclassical growth model extends ENDGM to problems with two control variables but just one endogenous state variable. Hintermaier and Koeniger (2010) numerically solve a durable goods model with two endogenous state variables by applying a hybrid method similar to our HYBGM. The key difference to our version of HYBGM is that we solve the non-linear equation with a univariate solver whereas Hintermaier and Koeniger (2010) use interpolation techniques that are generically less accurate. Other related literature evaluates the performance of ENDGM in the context of estimating structural models Jørgensen (2013) and/or extends ENDGM to a class of dynamic programming problems with both discrete and continuous choices in which the value function is non-smooth and non-concave, cf. Fella (2014) and Iskhakov et al. (2015). Most closely related to our contribution is White (2015) who builds on the earlier working paper version of our paper Ludwig and Schön (2013). White (2015) applies a slight modification of our economic model to evaluate the performance of an alternative interpolation method to Delaunay interpolation which superimposes more structure. To understand this, notice that Delaunay interpolation comes in three steps. The first is a triangulation of the state space. The second is the location of the triangle in which a specific interpolation point is located which is a simple search. The third is the interpolation itself. The costly step in the overall procedure is the triangulation. White (2015)’s method circumvents this step by preserving the ordering of the exogenous and endogenous grid points so that no new objects have to be created for the purpose of interpolation. The state space is subdivided into irregular quadrilateral sectors on which standard bilinear interpolation is possible. An important question is how the Delaunay method compares with White (2015)’s approach. Given that White (2015) avoids the costly triangulation step, the method is faster than Delaunay interpolation for a given number of gridpoints. However, because triangles cover smaller areas in the state space than quadrilateral sectors, we conjecture that accuracy is lower and that accuracy decreases more strongly in the irregularity of the endogenously constructed grid. For strong irregularity standard methods to locate a grid point may also not work.Footnote 4 We leave an investigation of these aspects for future research. Our analysis proceeds as follows. Section 2 presents the simple life-cycle model with endogenous assets and human capital on which we base the evaluation of methods. Section 3 introduces the main features of the methods under evaluation, the method of exogenous gridpoints, the pure method of endogenous gridpoints and the hybrid method. Section 4 presents results according to speed and accuracy of all three methods. Section 5 concludes. Additional material is contained in an Appendix.",7
51.0,3.0,Computational Economics,22 August 2016,https://link.springer.com/article/10.1007/s10614-016-9615-y,Agent-Based Simulation and Microstructure Modeling of Immature Stock Markets,March 2018,Hazem Krichene,Mhamed-Ali El-Aroui,,Male,Unknown,Unknown,Male,"Artificial stock markets (ASM) based on multi-agent simulation emerged to explain complex macro-features observed on financial markets, through a bottom-up approach based on modeling microscopic patterns. One of the most challenging tasks of ASM is the reproduction of price returns stylized facts (fat tails on returns distribution, absence of returns autocorrelation, volatility clustering\(\ldots \)) considered as universal statistical properties of stock markets (see for eg. Cont 2001). These stylized facts are related to markets’ anomalies induced by the rejection of the rational expectation hypothesis (REH) and the efficient market hypothesis (EMH) (see Hommes 2006). These anomalies differ across markets, mainly between mature and immature stock markets, as underlined by several empirical works. Indeed, immature markets exhibit high risk and inefficiency reflected by the presence of heavier tails and over-predictability on price returns with a more persistent volatility (Bekaert et al. 1998; Alagidede 2011). The important work of Bekaert and Harvey (Bekaert and Harvey 2003) on the characteristics of emerging markets, stated that immature markets are mainly characterized by important information asymmetry due to the influence of local irregular channels (as the poor level of information disclosure, the high presence of corruption, the poor legal environment\(\ldots \) see Yartey 2008) on information transmission. They explained the high predictability of returns by the high correlation to local information and the low informational efficiency. In the absence of works trying to explain artificially the immature stock markets’ properties (probably due to the complex and multiple features of these markets), this paper aims at modeling information asymmetry and herd behavior in order to reproduce, through simulation experiments, the main stylized facts of immature markets. We consider that these markets suffer from poor information quality which causes the emergence of informed and uninformed investors, where these latter are more likely to follow collective behavior to improve their trading strategies. Some previous works tried to model herd behavior and private information spread on stock markets (see for e.g. Chiarella et al. 2009; Pastore et al. 2010). However, these works could not simulate markets with different degrees of information asymmetry and herd behavior to reproduce stock markets with different maturity levels. This will constitute the major contribution of this work. To this end, we will consider trading between informed and uninformed investors. As in Pastore et al. (2010), information asymmetry is modeled by private information spread. Information flows by exchanging sentiments among investors who share connections on a social network. The different degrees of information asymmetry will be modeled by the introduction of a new behavioral network inspired by the works of Bollobás et al. (2003) and Guo et al. (2006). By combining ideas from these previous works we construct a directed weighted scale-free network populated by informed and uninformed agents. The agent information level is derived from its behavior. Like Brock and Hommes (1998) and Chiarella et al. (2009) investors are modeled by agents having mixed behaviors (fundamentalist, chartist and noise trading); they are supposed to analyze the market information based on their previous performance measures. Fundamentalist and chartist agents allow the modeling of informed and uninformed investors, respectively. Based on the behavioral network, different assortative topologies will be obtained allowing the simulation of different preferential attachments between investors depending on their behaviors. On immature markets, informed investors are more likely to be linked together to exchange private information among them, which promotes the information asymmetry and the herd behavior of uninformed investors. To model these features, we employed a social network to model private information spread, and we aimed at reproducing different assortative topologies related to the emergence of communities on markets with high information asymmetry. This paper will be structured as follows: Sect. 2 will present how different trading behaviors are modeled and how traders interact on a social network according to their beliefs. In Sect. 3, simulation protocol will be presented and the network topology will be studied to identify the way of reproducing different degrees of information asymmetry. In Sect. 4, based on the network topologies and agents’ behaviors, we will determine the way of emergence of immature markets stylized facts, analyzed through the returns distribution properties, the market predictability and the forecast performance of ARFIMA models. In Sect. 5, concluding remarks will be presented by underlining future research lines on artificial immature stock markets.",5
51.0,3.0,Computational Economics,29 August 2016,https://link.springer.com/article/10.1007/s10614-016-9619-7,Advantages of an Ellipse when Modeling Leisure Utility,March 2018,Richard W. Evans,Kerk L. Phillips,,Male,Unknown,Unknown,Male,"Constrained optimization problems are the hallmark of many economic models. It is well known that these models become difficult to compute when they have occasionally binding inequality constraints.Footnote 1 This computational difficulty is multiplied when the model has high dimensions of heterogeneity among the agents with occasionally binding constraints. Judd et al. (2003) document this issue in the context of non-adaptive grid methods. One of the key household decisions that includes important upper-bound and lower-bound occasionally binding constraints is the labor supply decision. In this paper, we characterize a specification for the utility of leisure that is based on the general equation for an ellipse. We show that this functional form has multiple benefits. Computationally, our proposed elliptical utility function provides Inada conditions at both the upper-bound and lower-bound constraints on labor supply.Footnote 2 The two main functional forms for the utility of leisure—namely, constant relative risk aversion (CRRA) and constant Frisch elasticity (CFE)—each only have a single Inada condition on either the upper bound or lower bound of labor supply. We find that the presence of these two Inada conditions in the elliptical utility of leisure specification speeds up the computation by a factor between three and six times. Further, we use a number of metrics to show that the elliptical utility of leisure specification is a close approximation to the common CRRA and CFE specifications. We show that the fitted marginal utilities, the microeconomic household outcomes in a life cycle model, and the macroeconomic outcomes in a simple real business cycle (RBC) model are all very similar. Lastly, the elliptical utility function has the empirically attractive property that the Frisch elasticity of labor supply is decreasing as labor supply increases. This is also true of CRRA utility of leisure. However, the elliptical utility Frisch elasticity approaches a finite level as labor supply goes to zero, whereas the Frisch elasticity approaches infinity as labor goes to zero for the CRRA specification. It is true that corner solutions of labor supply are important occurrences empirically in household decisions. However, even in macroeconomic models with large degrees of heterogeneity, we rarely see bins of individuals fine enough that their empirical labor supply averages are either zero or the maximum.Footnote 3 This suggests that, at least in macroeconomic models, the use of the elliptical utility of leisure function that bounds solutions away from the corners in labor supply does not cost the modeler much realism. Further, even in microeconomic models, the lack of corner solutions may be worth the speedup in computation time. Section 2 presents the general household constrained optimization with a focus on the labor supply decision. We then present the two main specifications for the utility of leisure (CRRA and CFE) followed by the specification of our proposed elliptical utility functional form. Section 3 compares the economic outcomes from these three specifications in terms of marginal utilities of leisure, Frisch elasticities of labor supply, microeconomic outcomes from a life cycle model, and macroeconomic outcomes from a simple RBC model.",2
51.0,3.0,Computational Economics,31 August 2016,https://link.springer.com/article/10.1007/s10614-016-9620-1,Estimating Dynamic Binary Panel Data Model with Random Effects: A Computational Note,March 2018,Gang Yu,Wei Gao,Shaoping Wang,,,Unknown,Mix,,
51.0,3.0,Computational Economics,22 September 2016,https://link.springer.com/article/10.1007/s10614-016-9622-z,Pollution Control with Time-Varying Model Mistrust of the Stock Dynamics,March 2018,Fidel Gonzalez,,,Male,Unknown,Unknown,Male,"In this paper I develop a framework to study the regulation of a stock pollutant where, in contrast to previous literature, the degree of trust that the environmental authority has in its estimated model is allowed to change over time. A time-varying degree of model uncertainty can arise when changes in key environmental, technological, and economic variables affect the trust that the environmental authority has on the accuracy of its estimated model. For example, Lang (2014) and Herrnstadt and Muehlegger (2014) show that unusual changes in weather increases the concerns about climate change. Herrnstadt and Muehlegger (2014) show that these concerns extend to pro-environment congressional votes. Moreover, previous literature has shown that environmental and economic systems can experience shifts to different dynamic systems. Polasky et al. (2011), Crepin et al. (2012), de Zeeuw and Zemel (2012) mention how these dynamic system shifts can take place in ecological, terrestrial, global climate, consumer choice, financial markets, cultural, and political systems. Thus, an environmental authority aware that the dynamics of economic and environmental system can change may become more mistrustful about the accuracy of its own estimated model after observing changes in key variables. The degree of mistrust may go back to its original level once these changes and the memory these events have faded away. The main contribution of this paper is the introduction of a time-varying degree of uncertainty aversion of the environmental authority. To the best of my knowledge, this is the first paper to do this in a dynamic framework in environmental economics. The general results of this paper show that accounting for the possibility of a time-varying degree of uncertainty aversion produces different emission taxes and abatement compared to those from the traditional time-fixed uncertainty aversion models used in the previous literature. For example, if the environmental authority believes that the model uncertainty may be reduced in the future then current emissions taxes should also decrease. In particular, I model an environmental authority’s time-varying degree of trust in its own model of the pollution stock dynamics by setting up a Markov regime-switching model with an optimistic and a pessimistic regime, where robust control is introduced in both regimes but the degree of model mistrust is higher in the pessimistic regime. The theoretical framework I develop is illustrated using Hoel and Karp’s (2001) functional forms and data because of its simplicity and parsimony, but this setup can be modified and implemented in more complex linear quadratic models. This framework is used for normative purposes. This implies that in the decision-making process the environmental authority anticipates the possibility of alternating to a regime with different degree of mistrust using the transition probabilities in the Markov chain. An emergent literature has adapted robust control to environmental and resource economics and use this new set of tools to obtain optimal policies under Knightian uncertainty. This paper fits neatly into this nascent literature. In one of the first applications, Roseta-Palma and Xepapadeas (2004) implements robust control to resource management decision, particularly water management. Gonzalez (2008) analyzes optimal emissions taxes and welfare under model uncertainty about the evolution of the pollution stock. Vardas and Xepapadeas (2010) analyzes the effect of implementing k-ignorance and robust control to biodiversity management. Funke and Paetz (2011) uses robust control to obtain optimal levels of CO\(_2\) mitigation. Athanassoglou and Xepapadeas (2012) analyzes robust policies for a stock pollutant in the presence of damage control and mitigation. Anderson et al. (2014) introduce robust control in a dynamic integrated framework that allows them to consider model uncertainty with respect to climate and economic dynamics. A common thread in Gonzalez (2008), Vardas and Xepapadeas (2010), and Athanassoglou and Xepapadeas (2012) is the use of robust control to analyze its relationship with the Precautionary Principle. The Precautionary Principles is a vague decision rule that in simple terms proposes that in the face of scientific uncertainty environmental policy should take action to avoid environmental degradation. In general, these studies have found that the use of robust control implies an implementation of the Precautionary Principle in that Knightian model uncertainty leads to a more active environmental policy. The main contribution of this paper is the introduction of a time-variant degree of mistrust in the environmental authority’s own model of the pollution stock dynamics. The model I present in this paper adapts and improves on the Macroeconomic model in Gonzalez and Rodriguez (2013) using the Hoel and Karp (2001) model as well as the work in Zampolli (2006) and Hansen and Sargent (2008). I obtain four main results. First, introducing the possibility that the environmental authority may switch to a regime where it has a different degree of trust in its estimated model of the pollution stock dynamics, produces changes in the current level of emissions taxes. This result holds even for a small probability to switch to the other regime and for a short expected duration of the other regime. In general, if the environmental authority believes that there is a possibility to switch to a regime with higher model uncertainty, then an active approach compatible with the Precautionary Principle is optimal by increasing emissions taxes in the current regime. Alternatively, if the environmental authority believes that there is a possibility to switch to a regime with lower model uncertainty, then emissions taxes in the current regime should decrease. These results also hold when the switch to the other regime is permanent. This suggests that if the environmental authority considers the possibility that the model uncertainty may permanently decrease at some point in the future, then current emission taxes should decrease. Second, increases in the mistrust of the environmental authority’s model of the pollution stock dynamics lead to higher emissions taxes. This is also compatible with the Precautionary Principle and confirms previous results in the literature. However, this result also indicates that decreases in the mistrust in the environmental authority’s model of the pollution stock dynamics lead to lower emissions taxes. Third, there is an interesting asymmetry in the response of the emissions taxes in each regime to changes in the transition probabilities. Emission taxes in the optimistic regime are more sensitive than those in the pessimistic regime to changes in the transition probabilities. Fourth, in the presence of a range of possible transition probabilities, an environmental authority in the optimistic regime is better off by assuming the highest transition probability to the pessimistic regime in that range. In general, the environmental authority obtains higher welfare by overestimating rather than underestimating the probability of transiting to the pessimistic regime. The paper broadly consists of two parts. The first part (Sects. 2, 3) is the theoretical approach. The second part (Sects. 4, 5) is a numerical illustration of the theoretical model. Specifically, the paper is divided into six sections. Section 2 develops the theoretical model. Section 3 shows the analytical solution to the new environmental authority problem. Section 4 presents the data used to calibrate the model. Section 5 presents and analyzes the numerical results. Section 6 concludes and suggests paths for future research.",
51.0,3.0,Computational Economics,13 September 2016,https://link.springer.com/article/10.1007/s10614-016-9624-x,A New Vision of Classical Multi-regional Input–Output Models,March 2018,George E. Halkos,Kyriaki D. Tsilika,,Male,Female,Unknown,Mix,,
51.0,3.0,Computational Economics,19 October 2016,https://link.springer.com/article/10.1007/s10614-016-9626-8,An Integrated Matching-Immunization Model for Bond Portfolio Optimization,March 2018,P. Xidonas,C. Hassapis,C. Staikouras,Unknown,Unknown,Unknown,Unknown,,
51.0,3.0,Computational Economics,19 October 2016,https://link.springer.com/article/10.1007/s10614-016-9627-7,Correlation Structure and Evolution of World Stock Markets: Evidence from Pearson and Partial Correlation-Based Networks,March 2018,Gang-Jin Wang,Chi Xie,H. Eugene Stanley,Unknown,,Unknown,Mix,,
51.0,3.0,Computational Economics,02 November 2016,https://link.springer.com/article/10.1007/s10614-016-9628-6,A Semi-Parametric Non-linear Neural Network Filter: Theory and Empirical Evidence,March 2018,Panayotis G. Michaelides,Efthymios G. Tsionas,Panagiotis Patrinos,Unknown,Male,Male,Male,"Ever, since the seminal work of Burns and Mitchell (1946), the primary objective in a business cycles framework is to study the fluctuations of a time series around a trend. Despite the fact that throughout the last decades a number of—often contradicting—quantitative techniques have been proposed in order to extract the cyclical component of a time series, what seems to remain elusive in the literature, is an appropriate universal and global technique to be used in order to assess business cycles. Probably the most popular approach in the literature regards business cycles as fluctuations around a trend, the so-called “deviation cycles” (Lucas 1977). In this context, trend estimation is of outmost importance, because it is necessary for the extraction of the cyclical component and for the propagation of shocks (Nelson and Plosser 1982). In fact, the more accurate the trend estimation, the more reliable the business cycle series extracted. Therefore, reliable trend estimates of a time series are very crucial because they can assist in addressing relevant issues and constitute, therefore, a very important task for researchers. Thus far, for the extraction of the cyclical component of a time series, researchers assume that the trend specification of the time series follows a certain pattern i.e. linear, exponential etc. Nevertheless, this is an ad-hoc assumption which totally ignores the inherent characteristics of the time series at hand i.e. the existence of fat tails, long memory, etc. To this end, in what follows we formally establish a novel methodological framework that takes into consideration the inherent non-linearities of the time series. More specifically, in this work, we decompose a time series into trend and secular component by introducing a novel de-trending approach based on a family of artificial neural networks (ANNs). So far, ANNs have found limited applications in Economics. However, they have very important advantages, such as increased flexibility, excellent approximation properties, and instead of fitting the data with a pre-specified model, they let the dataset itself serve as evidence to support the model’s approximation of the underlying model (Santin et al. 2004). Thus, ANNs are quite flexible and attractive when the theoretical trend specification is not known a priori (Zhang and Berardi 2001). In this work, instead of fitting the time series data with a pre-specified trend equation, we utilize an ANN specification and let the dataset itself serve as evidence to support the model’s approximation of the underlying trend. Also, by exploiting the excellent approximation properties of Neural Networks we prove formally that the proposed trend specification is a global approximation to any arbitrary trend. So far, a famous claim by Hodrick and Prescott (1981, 1997) states that the “conceptual framework is that over long time periods, their average is near zero”. In this work, we prove formally (mathematically and statistically) that the produced cyclical component, by means of our proposed approach, does indeed disappear in the long run, or in other words, its mean value is equal to zero. Next, using a number of relevant data generating processes (DGPs) we investigate: (a) the ability of the proposed neural network filter (NNF) to extract the cyclical component of an artificially generated time series that exhibits cycles in a wide range of frequency domains; (b) the ability of NNF to extract cycles that incorporate changes in volatility, amplitudes and phase shifts; (c) the distortionary effect of the cycles produced by NNF with regard to the artificially generated cycles. As a next step, the results of the aforementioned simulations are compared with Baxter–King (BK) and Hodrick–Prescott (HP) filters, and the Monte Carlo results suggest that the performance of NNF is superior in all cases. Lastly, our proposed technique is confronted with real-world data to assess its ability to model satisfactorily various situations of interest. In this context, from an economic viewpoint, we provide the estimation and visualization of business cycles fluctuations for output in EU15 using fuzzy clustering to study the creation of groups of countries with similar characteristics. Given that there has been a growing interest lately in the approaches for de-trending non-stationary times series and for representing their underlying trends, we will show that our proposed technique has the following advantages when compared to the widely adopted filtering methods of Hodrick–Prescott (HP) (Hodrick and Prescott 1997) and Baxter–King (BK) (Baxter and King 1999): First, it avoids the problem of a pre-specified functional form of trend, since it lets the dataset itself serve as evidence to support the model’s approximation of the underlying trend. Second, it does not require a priori assumptions for the smoothing parameter. Third, it is able to capture the non-linear characteristics that business cycles exhibit. Fourth, it is capable of capturing all frequency ranges and all spectrum peak locations. Fifth, the distortionary effects it creates are very limited even in the near unit root case and, sixth, using Monte Carlo techniques it is clearly superior when compared to the HP and BK using various DGP processes, including the near unit root case.Footnote 1
 The paper is structured as follows: Sect. 2 provides a literature review, Sect. 3 introduces the NNF; Sect. 4 derives the proposed filtering method and provides some helpful results; Sect. 5 investigates NNF’s ability to capture the cycles generated by a number of DGPs; Sect. 6 sets out the proposed econometric implementation; Sect. 7 presents the empirical results; finally, Sect. 8 concludes.",7
51.0,3.0,Computational Economics,17 November 2016,https://link.springer.com/article/10.1007/s10614-016-9633-9,Computing Transitional Cycles for a Deterministic Time-to-Build Growth Model,March 2018,Hwan C. Lin,,,,Unknown,Unknown,Mix,,
51.0,3.0,Computational Economics,20 June 2017,https://link.springer.com/article/10.1007/s10614-017-9710-8,Erratum to: Computing Transitional Cycles for a Deterministic Time-to-Build Growth Model,March 2018,Hwan C. Lin,,,,Unknown,Unknown,Mix,,
51.0,3.0,Computational Economics,25 November 2016,https://link.springer.com/article/10.1007/s10614-016-9634-8,On the Stochastic Sensitivity and Noise-Induced Transitions of a Kaldor-Type Business Cycle Model,March 2018,Irina Bashkirtseva,Davide Radi,Tatyana Ryazanova,Female,Male,Female,Mix,,
51.0,3.0,Computational Economics,24 November 2016,https://link.springer.com/article/10.1007/s10614-016-9635-7,Conditional Versus Unconditional Utility as Welfare Criterion: Two Examples,March 2018,Jinill Kim,Sunghyun Kim,,Unknown,Unknown,Unknown,Unknown,,
51.0,3.0,Computational Economics,30 November 2016,https://link.springer.com/article/10.1007/s10614-016-9636-6,Mean-Extended Gini Portfolios: A 3D Efficient Frontier,March 2018,Frank Hespeler,Haim Shalit,,Male,Male,Unknown,Male,"The mean-extended Gini (MEG) investment model offers an alternative to the standard mean-variance (MV) model by measuring risk using Gini’s mean difference instead of the standard deviation. MEG was developed by Yitzhaki (1983) to allow for the specific introduction of risk aversion differentiation into the risk decision process.The MEG approach was first used in finance by Shalit and Yitzhaki (1984) to price risky assets and construct efficient portfolios which are second-degree stochastic dominant (SSD). Later, Shalit and Yitzhaki (2005) provided superior alternative optimal allocations to the MV efficient frontier, in particular when risky assets are not normally distributed. We present a numerical optimization algorithm in Mathematica in order to construct mean-extended Gini efficient portfolios for large sets of assets with and without short-sales positions. The purpose is to familiarize the investment practitioner with the MEG model as a substitute to the MV model in portfolio selection. Solving for MEG portfolios enables investors to construct efficient portfolios that are tailored to their specific risk requirements. Indeed, when investors desire to hold riskier or less risky assets, MEG has the advantage of incorporating individual risk aversion. However, because the extended Gini is calculated by weighing the ranking function as a proxy for the cumulative distribution, analytical optimization techniques are unavailable. Hence, there is a need to develop numerical optimization techniques that would make MEG a superior tool for choosing optimal portfolios.",
51.0,3.0,Computational Economics,08 August 2016,https://link.springer.com/article/10.1007/s10614-016-9602-3,Erratum to: ABATE: A New Tool to Produce Marginal Abatement Cost Curves,March 2018,Oswald Marinoni,Martijn van Grieken,,Male,Male,Unknown,Male,"Unfortunately, in the first page of the original publication, software availability link to download the tool that is described in the paper is not included. The link is provided in this erratum. 
Software availability ABATE is available for download from http://doi.org/10.4225/08/578DA7F5877FB.",
51.0,4.0,Computational Economics,30 November 2016,https://link.springer.com/article/10.1007/s10614-016-9639-3,Dynamics Evolution of Trading Strategies of Investors in Financial Market,April 2018,Binghui Wu,Tingting Duan,Jianmin He,Unknown,Unknown,Unknown,Unknown,,
51.0,4.0,Computational Economics,02 December 2016,https://link.springer.com/article/10.1007/s10614-016-9640-x,"Profitability Edge by Dynamic Back Testing Optimal Period Selection for Technical Parameters Optimization, in Trading Systems with Forecasting",April 2018,D. Th. Vezeris,C. J. Schinas,G. Papaschinopoulos,Unknown,Unknown,Unknown,Unknown,,
51.0,4.0,Computational Economics,07 December 2016,https://link.springer.com/article/10.1007/s10614-016-9642-8,DEA-Based Piecewise Linear Discriminant Analysis,April 2018,Ai-bing Ji,Ye Ji,Yanhua Qiao,Unknown,,Unknown,Mix,,
51.0,4.0,Computational Economics,09 January 2017,https://link.springer.com/article/10.1007/s10614-016-9643-7,Discovering Traders’ Heterogeneous Behavior in High-Frequency Financial Data,April 2018,Ya-Chi Huang,Chueh-Yung Tsao,,Unknown,Unknown,Unknown,Unknown,,
51.0,4.0,Computational Economics,21 January 2017,https://link.springer.com/article/10.1007/s10614-017-9648-x,Network Topology and Systemically Important Firms in the Interfirm Credit Network,April 2018,Ohsung Kwon,Sung-guan Yun,Duk Hee Lee,Unknown,Unknown,,Mix,,
51.0,4.0,Computational Economics,27 January 2017,https://link.springer.com/article/10.1007/s10614-017-9649-9,The Impact of the Tobin Tax in a Heterogeneous Agent Model of the Foreign Exchange Market,April 2018,Filip Stanek,Jiri Kukacka,,Male,Unknown,Unknown,Male,"In 1972, James Tobin proposed a small, uniform tax on all foreign exchange transactions. Tobin argued that the absence of any consensus on fundamentals in foreign exchange markets in combination with low transaction costs and the limited rationality of market participants transforms the price discovery process into a “game of guessing what other traders are going to think” (Tobin 1978). However, if investors form their expectations at least partially based on the perceived expectations of other market participants, it would create positive feedback that may cause price misalignments and excessive volatility. Because most speculative transactions are made on a very short-term basis, Tobin believed that a small tax imposed on every transaction could dissuade most short-term speculators and consequently stabilize the market. Many participants in the global economy find the promise of more predictable exchange rates appealing. Therefore, it is unsurprising that since 1972, a transaction tax levied on foreign exchange markets (henceforth referred to as the Tobin tax) has been frequently discussed. The most recent example is the Central Bank of China, which is considering adopting the Tobin tax to protect the Yuan against speculative capital flows (Li 2015, via Bloomberg). Despite the vivid public debate over the possible effects of the tax, academic scrutiny has remained relatively scant. The most straightforward way to assess the impact of the Tobin tax is to examine some real world impositions of such a tax. Unfortunately, the time series necessary to do so are not available, as the Tobin tax has never been implemented. Aliber et al. (2003) nevertheless developed an innovative method for estimating transaction costs using foreign exchange futures and concluded that transaction costs (such as the Tobin tax) are positively associated with exchange rate volatility. In addition to the work of Aliber et al. (2003), several authors have also conducted studies addressing transaction costs on stock markets and obtained rather conflicting results (e.g., Umlauf 1993; Hau 2006; Liu and Zhu 2009). Interpreting these results is even more difficult because stock markets differ substantially from foreign exchange markets in terms of trading volumes and their microstructure. It is therefore not apparent whether these studies have any bearing on the Tobin tax. Because the empirical evidence is sparse, researchers have employed various theoretical models to address the question of how the Tobin tax might alter exchange rate dynamics. Heterogeneous agent models have proven particularly fruitful in this regard, as such models are able to generate time series that are qualitatively similar to those observed in real markets. Ultimately, it would be non-sensical to examine the impact of the Tobin tax on trading volume or endogenous volatility using a model that is incapable of generating either. In this paper, we extend existing agent-based research regarding the Tobin tax by exploring the possible impacts of the tax in a market cleared by a Walrasian auctioneer—settings that, to the best of our knowledge, have yet to be examined in any extant study and that might be, as we will argue below, more realistic than other frequently adopted clearing mechanisms. This modification is motivated by recent research demonstrating the substantial importance of the market-clearing mechanism and liquidity provision when evaluating the impact of the Tobin tax. There is broad consensus that the Tobin tax would reduce market depth, which may in turn increase volatility, as the price impact of a single order will be larger (e.g., Farmer et al. 2004). Proponents of the Tobin tax typically argue that because foreign exchange markets have relatively high depth, this effect will be negligible compared with the change in volatility originating from the change in the composition of the population of traders. While most studies explicitly model the structure of the trader population, few account for the taxs effect on volatility through market liquidity. This omission occurs because most studies explore the impact of the tax in a dealership market in which the market price is determined using a price impact function that is constant with respect to total trading volume. This danger of systematically overestimating the positive effect of the Tobin tax was noted by Ehrenstein et al. (2005), who demonstrated that under a more realistic price impact functionFootnote 1 that decreases with respect to total trading volume,Footnote 2 the Tobin tax can have either negative or positive effects on volatility depending on how sensitive the price impact function is with respect to total trading volume. These results seriously challenge conclusions drawn from previous models. 
Pellizzari and Westerhoff (2009) further supports the findings of Ehrenstein et al. (2005) by exploring the impact of the Tobin tax on a population of agents interacting in either a continuous double auction or in a dealership market. In a dealership market in which the market maker provides abundant liquidity, the Tobin tax reduces volatility. In a continuous double auction with endogenous liquidity provision, the otherwise stabilizing effect of the Tobin tax is offset by the reduction in market depth. The results of Pellizzari and Westerhoff (2009) were replicated in a laboratory experiment using human subjects by Kirchler et al. (2011), rendering the conclusions particularly sound. In addition to studies focusing solely on the link between market microstructure and the impact of the Tobin tax, a variety of other research papers also indirectly support the claim that the models clearing mechanism and liquidity provision might be the key determinants of whether the Tobin tax can successfully stabilize the market. Because different authors naturally employ slightly different agent-based models, we can investigate the relationship between the structure of each model and its results. A majority of researchers have explored the impact of the Tobin tax using a dealership market framework with a price impact function that is constant with respect to total trading volume and, consistent with our expectations, found that the tax reduces volatility. The Tobin taxs ability to stabilize the market was reported, e.g., in Ehrenstein (2002), Westerhoff (2003, 2004b, 2008), Westerhoff and Dieci (2006), Demary (2006),Footnote 3 Bianconi et al. (2009), and Demary (2010). Most recently, Flaschel et al. (2015) confirmed the ability of the Tobin-like tax to stabilize the market in a broader macroeconomic setting. While the impact of the Tobin tax appears to be clear when only considering dealership markets, we observe a notably different picture when examining agent-based models exploring the impact of the Tobin tax in the continuous double auction framework, which allows for endogenous liquidity provision. Mannaro et al. (2005) and Mannaro et al. (2008) demonstrated that under a double auction-like mechanism, the Tobin tax increases price volatility. Further, considering a similar setting, Lavička et al. (2013) reported that the tax increases volatility while simultaneously reducing the kurtosis of returns. Results more in line with the expectations of Tobin tax proponents were obtained by Hein et al. (2006). Motivated by the evident importance of the market microstructure, we chose to explore the impact of the tax imposed on a market that is cleared by a Walrasian auctioneer instead of a double auction or the simple price impact function. The reason that we chose to explore the Tobin tax in this particular setting is twofold. First, actual foreign exchange markets are not organized as pure continuous double auctions or dealership markets. Dealers continuously update two-way quotes for their customers and thus act as market makers. Moreover, dealers trade with one another either via brokers (in a continuous double auction fashion) or directly using systems such as EBS or Reuters. Intra-dealer trading accounted for approximately 39% of total trading volume in 2013 (BIS 2013). While this relatively large figureFootnote 4 was the basis of criticism regarding extensive speculation on foreign exchange markets, it is not entirely true that such trading is primarily of a speculative nature. Instead, intra-dealer trading represents the tedious task of passing undesired inventories (originated by a single, possibly speculative, customer-dealer trade) along until they encounter a dealer with the opposite undesired position, such that they neutralize one another (Flood 1994). This phenomenon is conveniently termed “hot potato trading” in the literature. In addition, dealers generally change their positions dramatically throughout the trading day, but at the end of the day, a majority of them have a zero net position, just as they began the day (Cheung et al. 2004). Considering these two points, one could easily gain the impression that the intra-day trading among dealers and the adjustment of quotes for customers is some sort of intricate tâtonnement process of searching for the price under which all customers would hold desired positions with respect to their expectations and, simultaneously, none of the dealers, which together represent “the auctioneer”, would be exposed to exchange rate risk. A second, subtler point is that by using Walrasian clearing, we bypass the choice regarding the precise form for the price impact function and its relationship with liquidity. This approach is convenient because such a function is often difficult to estimate (e.g., Westerhoff 2004a) and may affect the measured impact of the Tobin tax substantially (Ehrenstein et al. 2005). Admittedly, Walrasian clearing is not a panacea, and hence this circumvention of the problematic decision regarding the price impact function comes at its own cost. In particular, the uncertainty regarding the price impact function is transferred to the utility functions of individual agents, which serve an analogous role in equilibrium pricing models—they determine how sensitive the current exchange rate is with respect to the expectations of individual agents. Thus, this approach is only as good as our knowledge of these functions. Nonetheless, it might still be considered an improvement, as there exists a relatively broad consensus regarding the utility functions used in asset-pricing models, with the majority of studies utilizing a mean-variance framework originally introduced by Sharpe (1964).Footnote 5 The research regarding price impact functions is, on the other hand, a rather novel area, and some definite conclusions regarding their form are likely still to come. For many references as well as an analysis of the possible pitfalls of such estimations, see Weber and Rosenow (2005). Overall, to study the effects of a Tobin tax imposed on the retail market, it seems appropriate as well as convenient to model the interbank market as a Walrasian auctioneer. Considering the complexity of the interbank foreign exchange market, this approximation is somewhat crude. Nonetheless, by assessing the impact of the Tobin tax in this particular setting and by contributing to the variety of the existing research, we hope to help guide policies in markets whose complexity is beyond the reach of any single study. The remainder of the paper is structured as follows. Section 2 introduces the model and extends it by incorporating transaction costs in the optimization problem faced by retail traders. In Sect. 3, the calibration method is described, and multiple simulation runs are performed to assess the impact of the Tobin tax on selected statistics. Finally, Sect. 4 concludes.",6
51.0,4.0,Computational Economics,25 January 2017,https://link.springer.com/article/10.1007/s10614-017-9650-3,Fiscal Policy Design in Greece in the Aftermath of the Crisis: An Algorithmic Approach,April 2018,Ilias Kostarakos,Stelios Kotsios,,Male,Male,Unknown,Male,"For more than 20 years, monetary policy was considered as being potent enough to manage the fluctuations of the business cycle, using the interest rate as the only policy instrument and having a low, stable level of inflation as the sole policy target. This type of policy, essentially based on the Taylor rule (see Taylor 1993) led to what has been termed as the “Great Moderation” era (see Bernanke 2004), since the volatility of the cycle was greatly reduced. During this period fiscal policy was tasked with ensuring debt sustainability, mainly via using automatic stabilizers, since any discretionary action was deemed as potentially destabilizing. Among the reasons cited regarding the inferiority of fiscal relative to monetary policy are the long lags in the recognition, design and implementation of fiscal policy measures, the short length of recessions and the political constraints entailed (see Blanchard et al. 2010 for a detailed analysis). However, the global financial crisis of 2008 and the ensuing debt crisis that hit countries in the southern periphery of the EU have led to a resurgence of interest in the design of fiscal policy, partly because of the binding zero lower bound constraint on interest rates which renders monetary policy, to a large extent, ineffective. A highly illustrating example of fiscal policy design comes from the case of Greece. The Greek economy suffered from persistent deficits combined with increasing debt-to-GDP ratios during the period up to 2009, thus leaving no fiscal space for exercising expansionary policy when the crisis hit. In May of 2010, under heavy pressures from international markets, the Greek government agreed to implement an adjustment program designed to maximize credibility and ensure that public finances are sound, so that public debt is back on a sustainable path. The program was based on frontloaded implementation of fiscal consolidation measures; in particular, certain targets—the so-called conditionality targets—were set for the main policy variables (primary deficit etc.) and the Greek government had to design the fiscal measures necessary (i.e. the appropriate changes in government expenditures and revenues) to ensure that the targets would be reached within the specified time-frame. Due to the austerity nature of the program, the aforementioned measures included cuts in (nominal) public sector wages, layoffs in the public sector etc. What is important to note is that the program was designed on the basis of the feedback methodology: once the conditionality targets were set, the relevant measures were designed and implemented. The program was evaluated over certain intervals, e.g. quarterly, and depending on the assessment the policy measures were in many cases re-designed—this is a key element of the feedback approach. The Greek program exhibited elements of ‘positive’ feedback, meaning that if a target was missed, then the measures were intensified in the same direction; for example, if the target for the primary surplus was missed, then further decreases in government expenditures combined with tax hikes were implemented. Our aim in this paper is to propose a control-theoretic computational approach for the design of fiscal policy based on the algorithmic linear feedback methodology, utilizing a technique known as model matching (which, to the extent of our knowledge has not been applied before in the literature regarding the theory of economic policy design). Our approach is based on examining whether appropriate linear fiscal policy rules exist and, if they do exist, how they should be designed so that desired values for the policy targets are reached. Specifically, we develop appropriate symbolic algorithms and, once the (fixed) policy targets have been set, we use these algorithms in order to design linear fiscal policy rules (or, feedback laws) for the instrument at hand (government expenditures), so that the policy objectives are exactly met. These policy rules are ‘responsive’ (following Taylor 1993), in the sense that the parameters of the resulting algebraic expressions of the policy rules are not fixed. This is in contrast, for example, to the well-known Friedman k% rule, which stipulated that the money supply should be increased by a constant k percentage in every period, regardless of the state of the economy. Moreover, the value of the instrument in period t depends upon lagged values of the instrument and the target variables. Thus, the policy rules take into account the state of the economy when deciding on the size of the policy instruments and, overall, they represent a more discretionary approach to fiscal policy. We note here that, following the classification system in Kendrick (2005) and Kendrick and Amman (2006), the policy rules presented in this paper are ‘handcrafted’ i.e. they are not the result of some optimization process. Also, following Kendrick and Amman (2010, 2014), we assume that fiscal policy adjustments will be made on a quarterly basis, in order to examine whether the increased frequency of policy interventions will allow for a smoother path for the economy following a severe economic downturn. Finally, we consider different specifications regarding the disbursement of government expenditures in order to examine whether a frontloaded or a backloaded approach is better suited for the problem at hand. The analysis is conducted within a linear, deterministic variant of the standard multiplier–accelerator model proposed by Samuelson (1939). The reason for choosing such a simple model is its tractability; it can be manipulated analytically and will allow us to understand the workings of the system once the proposed methodology is applied. Thus, it serves as a first step in understanding the workings of the system’s behavior before extending the methodology to more complex (nonlinear and/or stochastic) models. The main advantage of using the feedback methodology for policy design is that it will help in shortening policy lags, via shortening the design and implementation lags, since these can be explicitly incorporated into the equations of the model. Another advantage of our approach is that the solution technique ensures that the predetermined sequence of policy targets will be exactly followed (or, tracked), without any deviations. Moreover, the solution technique is parameterized and thus it allows for proper symbolic algorithms to be developed. However, the most important advantage is that based on these algorithms, a whole class of fiscal policy rules for solving the policy problems at hand can be designed enabling the policymaker to choose the rules that are the most appropriate based on different criteria (eg. the costs incurred by the implementation of a particular policy rule, so that the aim would be to choose the rule that causes the lower cost). This allows us to simulate the model under different policy rules, develop criteria for choosing policy rules and obtain important insights as to which rule is more appropriate depending on the particular case at hand. Finally, this method ensures that not only is complete tracking achieved i.e. the policy targets are exactly met but, moreover, the time path of the instruments is such that we have an immediate adjustment of the system; that is, if the policy rule is implemented in period t, the targets are met in period \(t+1\) (and all subsequent periods) i.e. the system immediately settles on the desired trajectory. Obviously, the lags associated with fiscal policy design and implementation make the instant adjustment of the system seem unrealistic. Nonetheless, this approach serves as a guideline of what the ‘optimal’ path for the policy instruments should be (optimal in the sense that complete tracking, without delay, of the target sequence is achieved). The results presented in this paper are, to a large extent, contingent on the linear and deterministic nature of the model, which admittedly is quite restrictive. Nonlinear models exhibit much more complex dynamic behavior which closely resembles the actual workings of the economy (e.g. multiple equilibria). However, since the bulk of the analysis regarding nonlinear systems is conducted based on linearizations in the vicinity of the equilibrium points, a thorough examination of the linear case is necessary in order to obtain a benchmark for the analysis. We must note here that another important dimension that should be examined is that of the effects of stochastic elements that could potentially alter the time-path and the structure of the model. The introduction of such elements excludes the possibility of complete tracking of the desired target-values and gives rise to a discussion regarding alternative policies that will allow for a better response from the policymaker. Such considerations are addressed in latter sections. Our results indicate that an economy like Greece in early 2010, facing a combination of high debt-to-GDP ratios and large declines in GDP, should implement short-term expansionary fiscal policy plans to ensure positive GDP growth rates. Of course, the results are model specific and due to the simple nature of the model, we did not expect any counter-intuitive results. However, the proposed methodology allows us to quantify the results, since we obtain the exact sequence of policy instruments necessary for reaching the policy targets. Moreover, because policy design in this case is essentially rule-based, it provides a clear, contingent policy plan for the short-term period. Thus, the implications of the counterfactual experiments set a useful benchmark regarding the design of fiscal policy following the feedback (rule-based) approach. The paper is organized as follows. In Sect. 2 we present the model. In Sect. 3 we state the problem and the solution technique. Section 4 presents the relevant algorithms. In Sect. 5 we provide the counterfactual policy experiments. Section 6 concludes.",3
51.0,4.0,Computational Economics,07 February 2017,https://link.springer.com/article/10.1007/s10614-017-9651-2,"Short-Term Price Overreactions: Identification, Testing, Exploitation",April 2018,Guglielmo Maria Caporale,Luis Gil-Alana,Alex Plastun,Male,Male,Male,Male,"The efficient market hypothesis (EMH) is one of the cornerstones of financial economics (Fama 1965). Its implication is that there should not be any exploitable profit opportunities in financial markets. However, the empirical literature has documented the presence of a number of so-called “market anomalies”, i.e. price behaviour that appears to create abnormal profit opportunities. One of the most famous stock market anomalies is the so-called overreaction hypothesis detected by De Bondt and Thaler (1985), who showed that investors tend to give excessive weight to recent relative to past information when making their portfolio choices. A special case of the overreaction hypothesis is short-term price reactions after one-day abnormal price changes. Empirical studies on various financial markets show that after such price changes there are bigger contrarian price movements than after normal (typical) daily fluctuations (Atkins and Dyl 1990; Bremer and Sweeney 1991; Bremer et al. 1997; Cox and Peterson 1994; Choi and Jayaraman 2009; etc). This paper provides new evidence on the overreaction anomaly by analysing both price counter-movements and movements in the direction of the overreaction and comparing them to those after normal days. First, we carry out t tests to establish whether the data generation process of prices is the same after days of overreaction and typical days. We show that short-term overreactions cause the emergence of patterns in price behaviour, i.e. temporary market inefficiencies that could result in extra profit opportunities. Then we use a trading robot method to examine whether or not trading strategies based on the detected statistical anomalies are profitable, i.e. whether price overreactions are simply statistical phenomena or can also be seen as evidence against the EMH. The analysis is carried out for various financial markets: the US stock market (the Dow Jones index and two companies included in this index), FOREX (EURUSD, USDJPY, GBPCHF, AUDUSD) and commodity markets (Gold, Oil). The remainder of this paper is organised as follows. Section 2 briefly reviews the existing literature on the overreaction hypothesis. Section 3 outlines the methodology followed in this study. Section 4 discusses the empirical results. Section 5 offers some concluding remarks.",22
51.0,4.0,Computational Economics,28 January 2017,https://link.springer.com/article/10.1007/s10614-017-9652-1,A New Predictive Measure Using Agent-Based Behavioral Finance,April 2018,Todd Feldman,Shuming Liu,,Male,Unknown,Unknown,Male,"This paper contributes to the literature in two aspects. First, we showcase how to calibrate the Friedman and Abragam’s (2009) (FA) agent-based model using actual financial data, thereby expanding the empirical literature that tests the validity of behavioral and agent-based models. Second, we create a new proxy of heterogeneous beliefs estimated from the FA model and test how heterogeneity in beliefs impacts future stock returns and volatility. The FA model is an agent-based financial model with endogenous pricing that helps explain the emergence of bubbles and crashes in simulations. We briefly describe this model here and leave the detailed discussion in Sect. 3. The simulation model works in the following manner. First, every agent or portfolio manager is randomly assigned a portfolio size and an allocation to a risky asset at the start of the simulation. Second, the portfolio managers update their allocations to the risky asset based on the historical market return, their current idiosyncratic alpha, and a measure of risk from behavioral biases. A positive (negative) alpha gives the managers more (less) confidence, so they allocate more (less) to the risky asset. The measure of risk is based on aggregate losses of all managers. More aggregate losses make the managers more skittish. Third, the simulation program averages each manager’s allocation to the risky asset across all managers weighting by the portfolio size. This measure forms the risky asset’s average market demand. The price for the risky asset is determined by fundamental information and the average market demand. Therefore, a change in the average market demand leads to a change in the risky asset’s price and a change in the price leads to a change in the average market demand. This feedback loop between demand and price is essential to the FA pricing process, which continues until the end of the simulation. The result of the model is that we see bubbles and crashes in the simulated stock prices using this pricing process. In this paper, we empirically test the FA model by inputting data on thousands of US equity mutual fund managers into the FA pricing process. We estimated the stock price series from the FA model and compare its dynamics against that of the S&P 500. Our results show that the FA estimated stock price series and the actual S&P 500 series are similar in their peaks and valleys. Although the FA series average annual stock return is not exactly same as that of the S&P 500, the second and higher order return moments are very similar. Moreover, the returns of the estimated FA price series and the actual S&P 500 series fit to the same distribution. Next, we adopt a new proxy of heterogeneous beliefs based on the FA model. Specifically, we first estimate how much each manager invests in the US stock market versus cash, i.e., position size, on a weekly basis. Then we measure heterogeneity in beliefs by calculating the standard deviation of position sizes across all managers. We believe that the actions of fund managers, i.e., how they allocate their assets (position sizes), truly reflect their beliefs. Higher variability in estimated position sizes to the US stock market indicates greater heterogeneity in beliefs. The reason that we estimate fund manager position sizes from the FA model instead of calculating it directly from the data is because not all data on US stock exposure is available. The advantage of our empirical proxy is that we use fund manager estimated position sizes instead of survey or analyst data to create the proxy because actions speak louder than words. Investors may say one thing and do another. So proxies based on survey or analyst data may not reflect investors true beliefs. We then use this proxy to test how heterogeneity in beliefs affects stock market. Prior theoretical models and empirical evidence suggest that heterogeneity in beliefs about stocks can lead to bubbles and crashes. We find statistically significant evidence that greater heterogeneity in stock positions among managers leads to future stock market declines. Graphical evidence shows that the height of the variability in position sizes corresponds with the height of the S&P 500 series in 2000 and 2007. Regression results suggest that as variability in position sizes increases by 1%, the US stock market is predicted to fall by 1.35% in the next 12 months. We also find a positive relationship between the variably in position sizes and changes in stock market volatility. In addition, the probability of recession also increases when there is a higher level of heterogeneity in beliefs.",2
51.0,4.0,Computational Economics,25 January 2017,https://link.springer.com/article/10.1007/s10614-017-9653-0,Finite Difference Method for the Black–Scholes Equation Without Boundary Conditions,April 2018,Darae Jeong,Minhyun Yoo,Junseok Kim,Unknown,Unknown,Unknown,Unknown,,
51.0,4.0,Computational Economics,17 February 2017,https://link.springer.com/article/10.1007/s10614-017-9654-z,Efficient Simulation of Value-at-Risk Under a Jump Diffusion Model: A New Method for Moderate Deviation Events,April 2018,Cheng-Der Fuh,Huei-Wen Teng,Ren-Her Wang,Unknown,Unknown,Unknown,Unknown,,
51.0,4.0,Computational Economics,06 February 2017,https://link.springer.com/article/10.1007/s10614-017-9655-y,Trading Volume and Price Distortion: An Agent-Based Model with Heterogenous Knowledge of Fundamentals,April 2018,Vivien Lespagnol,Juliette Rouchier,,,Female,Unknown,Mix,,
51.0,4.0,Computational Economics,06 February 2017,https://link.springer.com/article/10.1007/s10614-017-9656-x,A Linear Stochastic Programming Model for Optimal Leveraged Portfolio Selection,April 2018,Davi Michel Valladão,Álvaro Veiga,Alexandre Street,Unknown,Male,Male,Male,"The portfolio optimization literature is extensive and explores different aspects of the problem such as objective function, constraints, transaction costs and price dynamics, see Kolm et al. (2014). The main goal is to represent in a reliable manner the risk-return trade-off first presented by Markowitz (1952) to support investment decisions. In particular, advances in optimization under uncertainty have enhanced robustness and flexibility of portfolio selection models to obtain more reliable investment strategies in practice. On one hand, robust optimization mitigate effects of estimation errors on asset allocation. A robust portfolio optimization model considers, for instance, expected returns in a given uncertainty set ensuring solution feasibility even for the worst case scenario, see Kim et al. (2014) and references therein. More recently, Fernandes et al. (2016) propose a data-driven approach to adaptively define the uncertainty set and solve the portfolio optimization considering a worst case loss constraint. On the other hand, stochastic programming provides a framework that allows for a more flexible representation of various aspects of the problem. For instance, a stochastic programming model efficiently handles the coherent risk measure, see Artzner et al. (1999), Conditional Value-at-Risk in a static, (Rockafellar and Uryasev 2000, 2002; Street 2009), or even dynamicFootnote 1 setting (Shapiro 2009; Rudloff et al. 2014). However, little attention is given to loans modeling, in particular to borrowing costs and credit limits. In static models, it is common to assume a unique fixed borrowing rate such as in the classical mean-variance approach. Indeed, Markowitz (1952) assumes an unbounded credit limit and risk-free borrowing rate allowing for short selling position in the risk free asset. A more complex, but still unrealistic, assumption is to define a linear borrowing cost function based on the risk free interest rate plus a fixed positive risk premium, as we see in many dynamic models for asset and liability management (see Ziemba and Mulvey 1998; Carino and Ziemba 1998; Kouwenberg 2001; Mulvey and Shetty 2004; Hilli et al. 2007; Birge and Louveaux 1997) and debt management (see Balibek and Murat 2009; Consiglio and Staino 2010; Date 2011; Valladão et al. 2014). In practice, there is a finite number of lenders offering a limited amount of money for a fixed interest rate. In this work, we propose a portfolio and leverage selection optimization model with a piecewise linear borrowing cost function by virtue of representing multiple lenders. Our main contributions are: Propose a convex piecewise linear borrowing cost function that reflects the minimum borrowing cost for a given debt size. Incorporate the convex piecewise linear borrowing cost function into a CVaR-based linear stochastic programming problem for optimal asset allocation and leverage selection. Provide numerical results showing that the proposed model is computationally tractable and outperforms selected benchmarks. This paper is organized as follows: In Sect. 2, we develop a two-stage stochastic programming model with the proposed cost function. In Sect. 3, we motivate our modeling choice with a numerical example showing the practical consequences of sub-optimal decisions obtained using the usual linear borrowing cost approximations. Moreover, we show that proportional credit limits are equivalent to upper bounds, imposed by each lender, on the incremental leverage ratio of the borrower. Finally, in Sect. 4 we conclude summarizing the contributions of our work.",4
51.0,4.0,Computational Economics,09 February 2017,https://link.springer.com/article/10.1007/s10614-017-9657-9,Evaluation of a DSGE Model of Energy in the United Kingdom Using Stationary Data,April 2018,Nasir Aminu,,,Male,Unknown,Unknown,Male,"This aim of this paper is to investigate the effects of energy prices shocks on economic activities in the United Kingdom (UK) with filtered data. The decline in the energy prices has positive significant impacts in reducing costs in energy intensive sectors such as transportation and manufacturing. Declining energy prices are also favourable to economies that are importers and net-importers of oil, such as the UK, China, India and Japan. However, it is bad news for oil dependent economies, such as Nigeria, Venezuela, and Kuwait. The second quarter, the months of March to June, of 2015 UK CPI report showed that inflation in the energy intensive sectors fell by 1.8%. However, energy prices have fluctuated significantly in the past decade. A good example is that it took only 5 months, from July 2014 to December 2014, for the price of crude oil to fall from about $100 a barrel to $52 a barrel. Crude oil prices also fell from about $150 a barrel in 2008Q1 to under $40 a barrel in 2009Q1. Conversely, oil prices quickly reversed course, climbed steadily and reached more than $75 a barrel in 2009. Such sequence of phenomena is what pushes for empirical research and application of theoretical work. 
Millard (2011) estimated an energy model in the UK using the Bayesian method. However, he found that energy price shocks (oil prices and gas prices) have little effect on the variability of output and inflation. His findings are consistent with Harrison et al. (2011). Few other authors used dynamic stochastic general equilibrium (DSGE) model to study the UK economy, such as Harrison and Oomen (2010) and Faccini et al. (2011). They develop models of inflation, built around the ‘New Keynesian Phillips Curve’ (NKPC), to imply how inflation depends on lagged inflation, expected future inflation and the real marginal cost. In these models, real marginal cost will also be equivalent to real unit labour costs, although, as shown by Faccini et al. (2011) and Kamber and Millard (2010), since energy and labour are complementary inputs to production, the real marginal cost is affected by changes in energy prices. Therefore, movements in energy prices will be significant for inflation. Since consumers are also users of energy, any shift in energy prices will have a direct impact on CPI inflation, which is not impacted by the NKPC. As oil prices rise, central banks expect to tighten monetary policy. Borrowing rate expect to increase since investors demand higher interest rates, with an expectation of higher inflation. However, I did not find empirical evidence of Bank of England, like the Federal Reserve, responding to rising energy prices in the past. In the past 30 years, a large body of research tried to examine the effects that oil prices shocks have had on the macroeconomy. Studies, on oil prices shock, (Bernanke et al. 1997; Kilian 2008; Hamilton 2009) found that these shocks seem to have a lesser effect on output, interest rates and inflation during the great moderation period. As Nordhaus (1980) puts it, the fundamental logic of energy policy is to lower demand of energy, in order, to have lower terms of trade losses and, thereby, higher real income in the economy. 
Kim and Loungani (1992) and Finn (1995) study the significance of energy price shocks using closed economy real business cycle (RBC) models, with an emphasis on the United States. They find that energy prices shock can provide little significance in explaining the real macroeconomic aggregate fluctuations in the economy. Conversely, the study of Miguel et al. (2003) finds that where they proposed a small open economy RBC model, the oil price shocks are highly significant in explaining aggregate fluctuations. Their results show that oil prices shocks can explain a significant percentage of output fluctuations in many southern European countries. Their models also replicate the cyclical path of the periods of oil crisis in the European economies. The rise in the relative price of oil had a negative impact on welfare, mostly in the southern European countries, which historical data relates to a lax monetary policy in oil crisis periods. In this paper, I present how a dynamic stochastic general equilibrium (DSGE) model, that unusually incorporates energy sector, is set up. The model, like many macroeconomic models, aims to capture some aspects of reality of macroeconomic aggregates in the economy from optimizing behaviour at individual level. These aspects of reality include how monetary policy changes feed through the economy and what drives inflation in the economy. This is because no single model, yet, has been able to capture all aspects of reality in an economy. I explain the estimation technique and how it is carried out, and assessed the fit of the model using indirect inference testing. I also show how the model works by impulse response functionsFootnote 1 (IRF), vector autoregressive impulse response functions (VAR-IRF). I then discuss the estimation result by variance decomposition and analyse what the model says about energy price shocks by giving a timeline of the crisis period.",4
51.0,4.0,Computational Economics,04 February 2017,https://link.springer.com/article/10.1007/s10614-017-9658-8,Information and Efficiency in Thin Buyer–Seller Markets over Random Networks,April 2018,Michiel van de Leur,,,Male,Unknown,Unknown,Male,"In this paper we consider buyer–seller markets in which traders are not anonymous, but where transactions can only occur between connected traders. In the cigar market for instance Canada buys goods from countries such as Cuba and Nicaragua, but due to an embargo the United States cannot. Another example is the job market, in which applicants compete for similar jobs at different companies. A related market in the field of finance is the spot foreign exchange market, which is studied by Gould et al. (2013) and is an example of a market in which trade occurs through Bilateral Trading Agreements. Traders provide a block list containing trading partners with whom they prefer not to trade, to protect themselves against adverse selection and to control counterparty risk. In such a market a transaction between two traders can only take place if both are not part of the other’s block list. In such examples the network is either fully known, or traders solely observe their own counterparties. Corominas-Bosch (2004) describes these markets over networks, both with sequential and simultaneous bargaining procedures. As shown above a network of connections between traders can exist for many reasons, such as embargoes, previous relationships, trading agreements, the need for a shared language, or because it is physically impossible to visit all traders. This study focuses on the effect that the amount of available information about the network structure has on expected efficiency, independently of the reason why links are formed. Hence, we show whether it is optimal in markets over networks to reveal the entire network structure to traders. Uncertainty about the network structure is a new feature since in related literature it is assumed that the network structure is common knowledge. Markets over networks have been studied in various settings, albeit that either the network structure is assumed to be known to all traders, or the valuations and costs of others. These assumptions greatly simplify the decision process of traders and allow the market to be fully efficient. Corominas-Bosch (2004) and Chatterjee and Dutta (1998) consider a market in which traders submit an offer side by side which the other traders accept or reject. In Corominas-Bosch all buyers have the same valuation and sellers the same cost; this allows the network to be split into different subgraphs. In every subgraph the short side extracts all the possible surplus. We show that this result solely holds when the entire network structure is known; under partial information about the network structure, or under incomplete information about valuations and costs, not all the surplus is necessarily extracted. Spulber (2006) and Kranton and Minehart (2001) study simultaneously ascending-bid auctions in which sellers jointly raise their ask until supply equals demand, and then trade occurs. The former shows that full efficiency is only guaranteed when every pair can trade. Easley and Kleinberg (2010) and Blume et al. (2009) introduce intermediaries who act strategically and profit from trade. These intermediaries ensure, similarly to Corominas-Bosch (2004), that full efficiency is reached in every equilibrium. Calvó-Armengol (2001) formalises the power of a trader in a network considering the number of linked traders and their links. A higher market power is achieved when a trader is linked to more traders and when linked traders have fewer links themselves. In our study traders with higher market power exert this power and post more aggressive offers. 
Myerson and Satterthwaite (1983) and Chatterjee and Samuelson (1983) study Nash equilibrium strategies for bilateral trading and exhibit an equilibrium in which they are essentially linear. We restrict attention to similar linear markup and markdown strategies where the intensity of the markup or markdown depends on the information set that is available to the trader. These linear markup strategies have been introduced by Zhan and Friedman (2007), Cervone et al. (2009) discuss a version that is symmetric between buyers and sellers. This paper extends the literature of markets over networks, relaxing the assumption that the entire realisation of the network is known to traders. We introduce uncertainty about the network structure, and study the effect that the quantity of available information about the network structure has on allocative efficiency. This allows for conclusions to be drawn about the most efficient way in setting up exchanges in terms of the available information. We show that the effect of the quantity of information on efficiency is non-monotonic. Furthermore, switching from complete to incomplete information about traders’ valuations flips the shape of this non-monotonicity. We consider thin buyer–seller markets which have few traders, who trade only over existing links in a bipartite graph. A buyer–seller market, for example where physical goods or jobs are concerned, naturally constructs a bipartite network, in which links solely occur between the set of buyers and the set of sellers. We assume that these links are realised with the same probability and independently of each other, constituting a bipartite extension of the random graphs introduced by Erdős and Rényi (1960, 1961). Traders behave strategically, and we derive equilibrium configurations depending on the information about the network structure that is available to traders. Three nested information sets about the realisation of the network are compared. Under no information, traders place orders without knowing which links materialise, but simply the probability p that each link may exist. With partial information, traders know their own links and the probability p that links may exist between other market participants. Under full information, the entire structure of the network is common knowledge. Under complete information about traders’ valuations, we show that for any value of p both no information and full information lead to full allocative efficiency, while the partial information regime is weakly dominated. However, under a more realistic assumption of incomplete information about traders’ valuations, this ranking is reversed. If traders use linear markup strategies, partial information strongly dominates full and no information for any value of p. Hence, under incomplete information about valuations and costs it is optimal when traders solely have knowledge about their own connections. Roughly speaking, we could say that the market power of an agent depends both on his own strength and on the weaknesses of his partners. Our results suggest that, in the markets mentioned before, it is beneficial to inform traders about their strengths, while keeping them in the dark about their partners’ (potential) weaknesses. The organisation of this paper is as follows. The model and the trading mechanism are described in Sect. 2, together with the markup and markdown strategies and the information sets. Efficiency under complete information about traders’ valuations is studied in Sect. 3, followed by incomplete information in Sect. 4. Robustness of the results is discussed in Sect. 5. Finally, Sect. 6 concludes.",
51.0,4.0,Computational Economics,02 February 2017,https://link.springer.com/article/10.1007/s10614-017-9659-7,Visual Economic Modelling System (VEMS) for Computable General Equilibrium Models,April 2018,Nico Vellinga,,,Male,Unknown,Unknown,Male,"Computable general equilibrium (CGE) models are widely used for policy analysis. There are many CGE models available, but IFPRI came up with a so-called Standard CGE model as discussed in Löfgren et al (2002). Building a computable general equilibrium (CGE) model can be a tedious and time consuming task. One first has to set up a nesting structure for the production functions, introduce agents, stocks and flows, and derive the equations for each production function, derive budget constraints and the like. It also requires a lot of book-keeping. One has to make sure all flows are accounted for. No leakage should occur [see for instance Vellinga (2008)]. To calibrate the model, a social accounting matrix (SAM) has to be set up and the calibration has to be carried out based on the equations of the model. Many authors rely on presenting the nesting structure graphically in their paper, like Figures 2 and 3 in Wang et al (2009). Another example in which a graphical representation of the production structure is used is Figure 1 in Ignaciuk et al (2006) and the nesting in Figures 2, 3, 4 and 5 of that same paper. Most economic modellers set up models writing down the underlying equations of this graphical representation. A graphical representation of the model is clearer than the resulting set of equations. It is then logical to first set up the nesting graphically and move on from there. This is the starting point of the software tool VEMS, which stands for Visual Economic Modelling System. In a next step, various stocks and agents are added to this graphical production structure. The flows of goods and services provided by stocks are represented by lines. The software tool VEMS is a prototype tool that helps the modeller in each step of the process of setting up an economic model, from the calibration until running simulations. In each step it is possible to show help texts that will aid the modeller in understanding what is shown, how equations have been derived, etc.Footnote 1 As such, VEMS is a practical implementation of the ideas of Wang and Slagle (1996). They propose an object-oriented, knowledge-based approach to formulate applied general equilibrium models. VEMS generates the GAMS code to run the model. The models are available in four formats. The model can be specified as a non-linear programming (NLP) problem, as a mixed-complementarity problem (MCP), as an enhanced MCP model, denoted by MCP\(^{+}\), and in the format written with the mathematical programming system for general equilibrium (MPSGE)Footnote 2. More on these different formats will be presented later in this paper. It is beneficial to have these different formats of models and to have the model represented in a graphical manner. As noted by Greenberg and Murphy (1995), different people need different views of a model. For example, a person implementing a model looks for the equations, while a policy maker finds it sufficient to look at the graphical representation of it. Static and dynamic models can be set up with VEMS. Dynamic models can either be a backward-looking model or a forward-looking model. In the latter it is assumed that there is perfect foresight. The VEMS tool is designed using Microsoft Visual Basic 2008. An object oriented approach (OO) has been chosen as OO lends itself very good for designing such a tool. VEMS is built around two other tools, first, the general algebraic modelling system, or GAMSFootnote 3, and secondly, the Prolog (Bratko 2001) implementation of the tool SWI-PrologFootnote 4. Prolog is used to calibrate the model. GAMS can also be used for the calibration and it can be used to run the simulations. One would need a GAMS license that includes the PATH and CONOPT solvers to be able to use VEMS. If one also want to run the models in MPSGE format, the MPSGE subsystem should be part of the GAMS license. In theory the VEMS application could generate the code for running the model in other algebraic languages for mathematical programming, like AIMMS (Bisschop and Entriken 1993) or GEMPACK (Codsi and Pearson 1988). But these are left as possible future extensions. In the past, other (software) tools or descriptions of tools have been developed to assist modellers when setting up models. Phillips (1950) discusses a mechanical model that helps to understand how an economic model works. Winschel and Krätzig (2008) describe an object-oriented software framework for specifying, solving and estimating dynamic general equilibrium models. Constanza (1987), Constanza and Gottlieb (1998) and Constanza and Voinov (2001) discuss the modelling language STELLA. For a comparison between GAMS, MPSGE and GEMPACK, see Horridge and Pearson (2011). Another interesting system is the Hercules system described in Drud (1989) which is helpful to improve the model representation and error detection in models. It uses a model representation in which all economic assumptions are directly available. There is also the tool MAGNETFootnote 5 for the GTAP model (see Hertel 1997). This tool is helpful once a model has already been developed. VEMS is used for the steps before that when the individual items of the model, like equations, still have to be developed. MAGNET allows the user to include certain parts of the GTAP model, or exclude them, run simulations, etc. To our knowledge, none of these tools have all the elements that have been implemented in the VEMS tool. But some ideas of these other tools are potential very beneficial to an economic modeller and could also be implemented in VEMS in a later stage. The background for designing economic model in a visual manner is discussed in Sect. 2. The underlying ideas on why it is feasible and ultimately beneficial to automate the model building process is discussed in Sect. 3. The actual implementation of VEMS in Microsoft Visual Basic and using Object Oriented Programming is in Sect. 4. Various variations of models are generated. They are discussed in Sect. 5. The calibration of the model is the subject of Sect. 6, while the base run and other simulations are in Sect. 7. Sect. 8 concludes.",
51.0,4.0,Computational Economics,02 February 2017,https://link.springer.com/article/10.1007/s10614-017-9660-1,Sparse Bayesian Variable Selection in Probit Model for Forecasting U.S. Recessions Using a Large Set of Predictors,April 2018,Yang Aijun,Xiang Ju,Lin Jinguan,,,Female,Mix,,
52.0,1.0,Computational Economics,25 January 2017,https://link.springer.com/article/10.1007/s10614-017-9646-z,A Unique and Stable \(\hbox {Se}{\mathcal {C}}\hbox {ure}\) Reversion Protocol Improving Efficiency: A Computational Bayesian Approach for Empirical Analysis,June 2018,Cédric Wanko,,,Male,Unknown,Unknown,Male,"In addition to the traditional problems of “irreversibility” (Myerson 1979) and of interim rationality (Jackson 2003), Bayesian collective decision processes deal with difficulties to choose one process among many others as well as the complexity of implementing such a process. To solve these problems we develop an approach to the problem of inefficiency of the Nash equilibrium with a unique and stable process in a non-cooperative Bayesian universe (Zamir 2008) that descends from the mechanism theory.Footnote 1 In our approach, we recall that \(N=\left\{ 1,2\right\} \) is the set of players. \(\left( \Theta _{i}\right) _{i\in N}\) is a finite, non-empty type set of player i, \(\forall i\in N\) and \(\Theta =\Theta _{1}\times \Theta _{2}\). \(\ g\) is player’s joint probability linking the players to the type set \(\Theta \). \(\ S_{i}\) is a finite, non-empty strategy set belonging to player i, \(\forall i\in N\), and \(S=S_{1}\times S_{2}\). C is a finite, non-empty choice set of players, and \(C=\left\{ c_{1},...,c_{\left| c\right| }\right\} \). \( u_{i}:\Theta \times C\rightarrow \mathbb {R} \) is a von Neumann and Morgenstern payoff function of player i, \(\forall i\in N\).\(\ \ N(.)\) (or NP(.)) is the set of Nash equilibria (or set of) payoffs. \(\ W^{\alpha ,r}\) (or \(W^{\alpha ,\tilde{r}}\)) is the incentive feasible set with the lottery l(.) predetermined (or variable). 
Primo, in search of efficiency, our interest naturally looks toward correlation device. So, we assume a probability space \(\Gamma =\left( \Omega ,\mathcal {A},\mathcal {P} \right) \), in which \(\Omega \) is the set of states of the world with \(\omega \in \Omega \) being a particular state of \(\Omega \). \(\ A\) is the set of actions associated with \(\Omega \), and \(\mathcal {P}\) the set of probabilities resulting from these events. \(\ \gamma =\left( \left( \Omega , \mathcal {A},\mathcal {P}\right) ,\left( \lambda _{i}\right) _{i\in N}\right) \) is a “correlation device”in which \(\gamma \in \Gamma \). \(\lambda _{i}\) is a measurable function from \(\left( \Omega ,\mathcal {A},\mathcal {P}\right) \). \(\ \lambda _{i}\) results from a finite set of signals from player i. The information structure of a correlation device (Aumann 1974, 1987) is a guaranty of informational robustness and can greatly improve the players’ payoffs. However, the players may not want, or are not always able, to commit themselves to cooperate or to coordinate their actions in search of a maximum outcome, even when a mediator exists. This behaviour is mainly due to the problem of confidentiality of privte information. So, in order to maintain independence between players (in particular concerning confidentiality of private information), we focus on mechanisms avoiding correlation. Forges and Minelli (1998) introduce self-fulfilling mechanisms showing that no irrelevant information is transmitted to the players. (Self-fulfilling mechanisms) A mechanism is self-fulfilling if the following strategies constitute a Nash equilibrium of the game using the mechanism: first, the players report to the mechanism their true type; second, they play out the action corresponding to the signal received. This definition is retained here. We accept that a pure strategy of player is separated into two steps: the first consisting of choosing his message according to his type, and the second consisting of choosing his strategy according to his type and according to the message sent to the Bayesian mechanism and the signal received. De facto, assume that a social choice rule f
\(:\Theta \twoheadrightarrow C\) is a correspondence, and let \(k:S\rightarrow C\) be an outcome function. f is a singletonFootnote 2 \(\forall \theta \in \Theta \) and \(\forall c\in \) f
\(\left( \theta \right) \). Assume also a Bayesian mechanism \(\alpha =\left( S,k\right) \) , translating this procedure and generating a distribution \(\alpha \left( c\mid s\right) \in \mathbb {R} ^{+}\) such that \(\alpha :C\times S_{1}\times S_{2}\rightarrow \mathbb {R} ^{+}\) for \(\sum _{c^{\prime }\in C} \alpha \left( c^{\prime }\mid s\right) =1\) and \(\alpha \left( c\mid s\right) \ge 0,\) \(\forall c\in C\). \(\ S_{1}\) and \(S_{2}\) are two collections of signalsFootnote 3 with \(s_{i}\in S_{i},\forall i\in N\). 
Secundo, to allow an improvement of players’ payoffs, at least as those generated through correlation device when maintaining independence, we look toward the works of Gossner (1998, 2000) and Lehrer and Sorin (1997). They show that, under certain conditions, all public “mediated-talk” mechanisms may be able to mimic a correlation device distribution. Therefore, these mechanisms make recourse to the correlation device unnecessary for players. All Lehrer and Sorin’s mediated-talk mechanisms immunize against unilateral defection. This property of immunity against unilateral defection is a straightforward result of the safety (or security or robustness) of the communication protocol: (Secure protocol) Following Gossner (1998), a protocol is secure if, and only if, two conditions are fulfilled: No player can change the distribution of the translated signals of other players by changing his messages. No player can gain information about the translated signals of the others, either by considering his original signal instead of the one he translated, or by changing his messages. A protocol is secure when no player has the possibility either to mislead other players, or to intercept their private information. The property of circular permutation enables us to give proof of the independence of the signals. This property of circular permutation is emphasised in building the \(en\mathcal {C}oding matrix\). (Secure protocol) We say that a protocol is secure when for every game extended by a mechanism with an information structure, the following procedure is a Nash equilibrium: Follow the protocol, therefore generating signals that could have been issued by the information structure of the correlation device, Play in constituent game according to Nash equilibrium as if the obtained signals had been sent by the information structure of the correlation device. Their results show that there exists equivalence between: first, the information structure of a communication mechanism and a correlation device, and second, the information-wise improvement and the payoff-wise improvement, according to the Blackwell equivalence theorem (Blackwell 1951, 1953). This improvement is equivalent to a correlated equilibrium payoff. From this point of view, many works as Bassan et al. (2003), Lehrer et al. (2006, 2010, 2013) and Taneva (2015) characterize cases when one information structure induces a higher best payoff than another one. Gossner (2010) highlights the dual role of information in a different analytic setting. Recently, other works also showed that under specific conditions more information shrinks the set of Bayes correlated equilibria (Liu 2015; Bergemann and Morris 2016). 
Tertio, we use a voluntarily implementable mechanism in the sense of Maskin and Moore (1999) and Jackson and Palfrey (2001). The principle of a voluntarily implementable mechanism is to allow players the possibility of replaying—or, in the language of Maskin and Moore (1999), to renegotiate. The goal of the mediator is to avoid that the players wish to replay after his recommendation. Keeping in mind this possibility, the mediator makes an announcement that takes into account the fact that players are able to replay, such that no one actually has an incentive to deviate from it. We translate this process by means of a “reversion” function and a lottery that allows players to weight their probability of fully participating in the mechanism. Let us suppose the reversion process \(r:C\times \Theta \rightarrow C \) in which \(k\left( s_{1}\left( \theta _{1}\right) ,s_{2}\left( \theta _{2}\right) \right) =c\) is the outcome (or the recommendation), and \(r\left( c,\theta \right) \) is the equilibrium resulting from the process when \(\alpha \) announces c in \(\theta \). We say that \(\alpha \) implements a social choice rule f
\(\left( .\right) \) with \(r\left( .,.\right) \) accordingFootnote 4 to the distribution \(\alpha \left( r\left( c,\theta \right) \mid s\right) \). \(\ \ \alpha \left( r\left( {\widetilde{c}} ,\theta \right) \mid s\right) \) is the probability that the mechanism announces c, knowing l(.) and \(\tilde{r}(.,.)\), showing the reversion when l(.) is variable. In general, the form of expected payoffs of players is such that, for all \({\widetilde{c}}\left( \theta _{1},\theta _{2}\right) :\Theta \times \Theta \rightarrow \widetilde{C}\): in which \(\widetilde{C}\) is the set of announcements with lottery and is the conditionally expected payoff for player i, who knows his type is \( \theta _{i}\), when he asserts he is of type \(\theta _{i}^{\prime }\), and all the other players j are honest. 
In fine, the result of a voluntarily implementable mechanism that may be able to mimic a correlation device distribution allow us to define the reversion protocol of a voluntarily implementable Bayesian mechanism in which risk-averse players have no incentive to cheat or to deviate from the mediator’s recommendation and that can greatly improve their equilibrium expected payoffs as compared to those generated through coordination applied to the results of an unsatisfactory mediation (Wanko 2011). In this paper, we start from our previous work (Wanko 2009, 2011) in which we showed the existence and the selection of a secure reversion protocol and we move to the next step in order to examine unicity and stability of this secure reversion protocol from a theoretical point of view. First, we can verify unicity, showing the resulting equivalence between an antagonistic mixed extension and a correlated equilibrium distribution. Second, we can verify stability, showing that Myerson’s revelation principle
Footnote 5 is corroborated. We have the reasoning process that is the inverse of Myerson’s revelation principle of breaking down the protocol of the mechanism and of making more complex the method determining the frequency of the announcement signals (the recommendations). We start from a mechanism that is not necessarily a direct revelation,Footnote 6 reducing it to one that is a direct revelation with a protocol that simulates a correlation device (when that is possible) and enabling it to translate the true willingness of players. In addition, we propose a more empirical point of view than our previous work in the numerical simulation. Actually, instead of looking for a correlated equilibrium distribution that matches the outcome of an unsatisfactory mediation, we look for the values of some missing data in the unsatisfactory mediation that matches the outcome of a correlated equilibrium distribution. Our contribution essentially focuses on the following proof for risk-averse players: when there exists an information structure of a voluntarily implementable mechanism that simulates the information structure of a correlation device, the expected equilibrium payoffs are easier to compute in the sense that the predetermined secure reversion protocol is unique—more predictive,Footnote 7 and stable—there is no loss of generality in considering only mechanisms with protocol predetermined by correlation device. The modus operandi appears in the following way: Firstly, we determine the socially optimal values coming from an unsatisfactory mediation through a Karush–Khun–Tucker program in a Bayesian universe for risk averse players. Secondly, we want to generate a correlation device allowing us to know with which probability players decide to follow recommendations. To reach our objective, we work out the correlation device that we want to obtain then we replace missing data with the appropriate values that allow us to implement the correlated equilibrium distribution generated by this correlation device. Thirdly, and in fine, we should be able to mimic the unique correlation device that encourages players to reveal truthfully their private information in the Bayesian mechanism with reversion process.Footnote 8 In order to give proof of this simulation, we recall the implementation of the \(en\mathcal {C}oding matrix\) simulating the correlated equilibrium distribution in this third step. After some preliminaries concepts in Sect. 2, we state the main results of this work in Sect. 3 followed by a numerical simulation showing the implementation of statistical data for future empirical analysis in Sect. 4. In Sect. 5 we conclude about limits and future research.",
52.0,1.0,Computational Economics,27 January 2017,https://link.springer.com/article/10.1007/s10614-017-9647-y,Can Efficiency of Returns Be Considered as a Pricing Factor?,June 2018,J. Francisco Rubio,Neal Maroney,M. Kabir Hassan,Unknown,Male,Unknown,Male,"According to Tobin (1958); Hanoch and Levy (1969); Arditti and Levy (1975); Leland (1999), and Joro and Na (2006) the mean-variance frontier is only consistent with traditional utility theory if either stock returns are normally distributed or the utility function is quadratic in nature. In fact, since both assumptions are violated if returns are skewed, normality of returns is regarded as the only sufficient condition. Moreover, Ang et al. (2006) further this discussion by estimating the downside risk of equities; equities which are positively skewed should be regarded as more risky thereby providing higher returns. However, they estimate downside risk as market driven rather than asset specific. Given that stock returns are not necessarily normally distributed (Lau et al. 1990; Turner and Weigel 1992; Campbell and Hentschel 1992; Arditti 1975; Mandelbrot 1963; Fama 1965; Press 1967; Praetz 1972; Blattberg and Gonedes 1974; Simkowitz and Beedles 1980; Smith 1981; Ball and Torous 1983; Kon 1984; So 1987; Gray and French 1990), investors need to be compensated for further moments (Kraus and Litzenberger 1976; Kane 1982; Ho and Cheung 1991). More specifically, the risk-return paradigm needs to be re-evaluated to account for skewness when pricing stocks and mutual funds. We contribute to the literature by expanding the discussion on downside risk; that is, the extra risk associated with positively skewed returns. Unlike Ang et al. (2006), we develop a measurement for said downside risk as an asset specific measurement which is independent of overall market conditions. In this regard, we look for a measurement that can (1) depend on positive skewness which signals high downside risk, (2) be fund-specific in nature, and (3) be easily quantifiable; Data envelopment analysis meets all this requirements. Using DEA, we estimate an asset specific relative efficiency score which we expect can improve on current empirical asset pricing models. Our contribution works twofold: (1) we include efficiency scores in traditional empirical asset pricing model to test the hypothesis that investors do think about efficiency when investing, and (2) efficiency can help signal differences between investment assets, as it is the case of Socially and Responsible Investing assets. The use of DEA allows for multiple inputs and outputs to be incorporated into a production framework while still offering a single ex-post performance index, thus allowing easy comparison between funds and even groups of funds. We use this to compare constrained and unconstrained investments. DEA estimates a production possibility frontier which envelops a production possibility set as tightly as possible. One can estimate a performance index based on the distance between a specific fund and the aforementioned frontier. Funds on said frontier are deemed 100% efficient. Several studies have ranked the performance of mutual funds using DEA scores, but to our knowledge, such studies have been limited to only comparing efficiency scores among different assets [see for example Murthi et al. (1997), McMullen and Strong (1998), Morey and Morey (1999), Choi and Murthi (2001), Basso and Funari (2001, 2003, 2005), Galagedera and Silvapulle (2002), Gregoriou (2003, 2006), Haslem and Scheraga (2003), Chang (2004), Darling et al. (2004), Gregoriou and McCarthy (2005), Gregoriou et al. (2005), Gregoriou and Chen (2006), Eling (2006), among others]. Our study extends these findings by including efficiency scores in factor regressions. We establish an efficiency score as a subjective characteristic of a fund at a given month. The efficiency value is defined as \(0<1-P^{\textit{BCC}}<1,\) where 1 represents a fund which is 100% inefficient. Overall, our results suggest that the use of our efficiency score can help decrease average mispricing errors. We find that including the efficiency score in time-series regressions decreases average alphas from 18 basis points to 13 basis points in absolute values. Furthermore, this study updates the literature by measuring the performance of smaller asset sub-samples that impose limiting rules on the available asset universe, as is the case of Socially Responsible Investing,Footnote 1 also known as Sustainable and Responsible Investing (SRI Investments are described in more depth in “Appendix”). This set of assets is self-restricted to investing only in specific types of companies, thus effectively influencing portfolio performance based on the mean-variance frontier. Hence, we expect that SRI holds more diversifiable risk. But the performance of these constrained assets has been debated in the literature without consensus (Markowitz 1952, 1959). Yet investment in such funds has increased during the last decades according to the Social Investing Forum.Footnote 2 We expand the literature by not only using the efficiency measurement to rank the funds, but also to measure the level of over- or under-performance. SRI accounts for a small section of the overall American market, and as such, direct comparison with the asset universe will result in a size bias. To overcome this problem, we compare SRI with unconstrained investments as follows: first, given an efficiency score and a fund type, we rank funds in quintiles. Cross-tabulationsFootnote 3 of efficiency score quintiles for SRI and traditional funds show that there are more ethical funds within the low efficiency quintiles. There are 4.27% more socially responsible mutual funds in Q1 and Q2 compared to traditional investing, while there are 3.75% more orthodox funds in Q4 and Q5 compared to ethical funds. Second, we include an ethical dummy in a cross-section regression, which includes the efficiency score, to test if efficiency has an effect on such asset sub-samples. We find that ethical funds receive a discount between 7 and 12 basis points in next period returns. Controlled for efficiency, this is taken as evidence that ethical mutual funds underperform traditional investment assets. In addition, we provide evidence of a significant difference in performance pre- and post- crises. Based on a Chow test, we define 2 structural breaks in the data corresponding to the dot-com bubble and the real state bubble. Our findings show that alphas have increased from pre-crisis levels suggesting that mutual funds have enjoyed an extend period of abnormal returns following the structural breaks. Moreover, we provide evidence that although investors do seem consider efficiency when investing, this consideration was stronger for the pre-crisis period in comparison with the post-crisis periods. Finally, it should be noted that our findings not only expand on the traditional empirical estimation of asset pricing models, they also provide the means of comparison between different asset classes, as exemplified by the use of SRI investing. The remainder of this paper is structured as follows. Section 3 develops the estimation of an efficiency score and its use under an investment strategy paradigm as well as describing the data. Finally, Sect. 4 provides concluding remarks.",5
52.0,1.0,Computational Economics,04 February 2017,https://link.springer.com/article/10.1007/s10614-017-9661-0,Time Series Simulation with Randomized Quasi-Monte Carlo Methods: An Application to Value at Risk and Expected Shortfall,June 2018,Yu-Ying Tzeng,Paul M. Beaumont,Giray Ökten,,Male,Male,Mix,,
52.0,1.0,Computational Economics,06 February 2017,https://link.springer.com/article/10.1007/s10614-017-9662-z,Evolutionary Frequency and Forecasting Accuracy: Simulations Based on an Agent-Based Artificial Stock Market,June 2018,Ya-Chi Huang,Chueh-Yung Tsao,,Unknown,Unknown,Unknown,Unknown,,
52.0,1.0,Computational Economics,23 February 2017,https://link.springer.com/article/10.1007/s10614-017-9664-x,Hilbert Spectra and Empirical Mode Decomposition: A Multiscale Event Analysis Method to Detect the Impact of Economic Crises on the European Carbon Market,June 2018,Bangzhu Zhu,Shujiao Ma,Yi-Ming Wei,Unknown,Unknown,,Mix,,
52.0,1.0,Computational Economics,31 March 2017,https://link.springer.com/article/10.1007/s10614-017-9679-3,Erratum to: Hilbert Spectra and Empirical Mode Decomposition: A Multiscale Event Analysis Method to Detect the Impact of Economic Crises on the European Carbon Market,June 2018,Bangzhu Zhu,Shujiao Ma,Yi-Ming Wei,Unknown,Unknown,,Mix,,
52.0,1.0,Computational Economics,02 March 2017,https://link.springer.com/article/10.1007/s10614-017-9665-9,An Automated Investing Method for Stock Market Based on Multiobjective Genetic Programming,June 2018,Alexandre Pimenta,Ciniro A. L. Nametala,Eduardo G. Carrano,Male,Unknown,Male,Male,"The efficient market hypothesis (Fama 1970), assumed by some economists, states that a stock price behaves as a random walk. It suggests that any price forecasting attempt consists on a futile effort because if returns were forecastable, market participants would use them to generate unlimited profits. The profit seeking behavior of market agents would therefore destroy any predictability pattern that may arise in the series for some time. This characteristic makes market time series non stationary and harder to forecast, nevertheless, temporary predictable patterns may arise in financial series and can be exploited. Instead of being the end of the story for forecasting methods in stock markets, the efficient market hypothesis is on the contrary a motivation for the development of innovative adaptive financial forecasting methods, since any conventional method would quickly become unsuccessful. Despite the difficulties in forecasting financial time series, many efforts have been spent to better understand the stock market. Atsalakis (2009) an extensive study of conventional prediction methods is performed. In this comparison ARMA (autoregressive moving average models), ARIMA (autoregressive integrated moving average), ARCH (autoregressive conditional heteroscedasticity), and GARCH (generalized autoregressive conditional heteroscedasticity) models leaded to the best results (Atsalakis 2009). However, these standard methods assume stationarity of the time series and are deemed to be unsuccessful in practice under the efficient market hypothesis. Recently, several computational intelligence (CI) based systems have been proposed for supporting stock market investments (Atsalakis and Valavanis 2009), such as neural networks and/or fuzzy logic. Atsalakis and Valavanis (2009) present a review of 150 publications that use those techniques. In this review, the authors collect evidence that CI based systems usually obtain better results than conventional methods. Genetic programming (GP) (Poli et al. 2008) has also been used for this purpose. Computational intelligence and hybrid fuzzy methods have been also applied to forecasting financial time series (Sadaei et al. 2016; Talarposhti et al. 2016) with promising results. This work proposes a computational system for automated investment in stock markets. The system is a combination of multiobjective optimization (Kalyanmoy et al. 2002), genetic programming (Poli et al. 2008), and technical trading rules (Murphy 1999), which are commonly employed in financial time series evaluation. Additional procedures are used to improve the quality of results, such as automatic outlier detection/removal and feature selection. Finally, the decision is taken based on an ensemble. To best of the authors knowledge, such a combination is an innovation in the literature. The proposed method is an evolutionary method combining technical trading rules in a more sophisticated way with an adaptive process. Different classifiers in the ensemble could potentially learn different temporary patterns in the data and combine this into a single decision with a majority rule, giving more reliability to the action. The proposed algorithm was applied to identify ideal moments for sending buying and selling orders for six shares of the Brazilian stock market BOVESPA. The experiments were performed in an evaluation window of two years, between February 2013 and February 2015. The results were compared with Buy and Hold (Murphy 1999), which estimates the variation of the asset on the period, and two other automated techniques, based on the technical analysis. In addition, a second evaluation window (July 2015 to July 2016) was also considered in order to estimate how the proposed method would work in a very critical moment of Brazilian economics and politics. This text is structured as follows: a brief conceptual background of some techniques employed in this manuscript is presented in Sect. 2, jointly with some related works; the proposed automated investment system is described in Sect. 3; results obtained by the proposed algorithm and three benchmark methods on six assets of BOVESPA are described in Sect. 4; finally, some concluding remarks are drawn in Sect. 5.",21
52.0,1.0,Computational Economics,10 March 2017,https://link.springer.com/article/10.1007/s10614-017-9666-8,A Hybrid Metaheuristic for the Efficient Solution of GARCH with Trend Models,June 2018,Lourdes Uribe,Benjamin Perea,Oliver Schütze,Female,Male,Male,Mix,,
52.0,1.0,Computational Economics,07 March 2017,https://link.springer.com/article/10.1007/s10614-017-9667-7,Testing for Unit Roots in Dynamic Panels with Smooth Breaks and Cross-Sectionally Dependent Errors,June 2018,Tolga Omay,Mübariz Hasanov,Yongcheol Shin,Male,Unknown,Unknown,Male,"Testing stationarity of the variable observed over a long time period across a larger number of cross-section units has attracted a great attention (e.g., Quah 1994; Maddala and Wu 1999; Hadri 2000; Choi 2001; Im et al. 2003, 2005; Smith et al. 2004; Pesaran 2007; Pesaran et al. 2013).Footnote 1 The primary reason behind the widespread development of the panel data technique is to improve the power performance of the conventional unit root tests by exploiting both time and cross-section dimensions (Breitung and Pesaran 2005). Indeed, most widely used panel unit root tests proposed by Levin et al. (2002) and Im et al. (1995, 2003, henceforth, IPS) are a generalisation of the Dickey and Fuller (1979) test to a panel setting. After the seminal contribution by Perron (1989), it has been well-established that failure to account for the presence of structural breaks may lead to misleading inference on the stationarity property of time series. Conventional unit root tests that ignore structural changes suffer from substantial size distortions even if the series under consideration are stationary around changing trends (e.g., Perron 1989, 1990; Perron and Vogelsang 1992; Leybourne et al. 1998). The number of studies have developed panel unit root tests, which can accommodate structural breaks in the data generating process (e.g., Im et al. 2005; Carrion-i-Silvestre et al. 2005; Hadri and Rao 2008; Bai and Carrion-i-Silvestre 2009). 
Im et al. (2005) generalise the Lagrangian multiplier (LM) unit root tests advanced by Schmidt and Phillips (1992) and Amsler and Lee (1995), and develop the panel LM unit root tests in the presence of structural changes. Carrion-i-Silvestre et al. (2005) modify the panel-based KPSS (Kwiatkowski et al. 1992) stationarity test advanced by Hadri (2000) by allowing for multiple structural breaks. Hadri and Rao (2008) and Hadri et al. (2012) generalise the test procedure by Carrion-i-Silvestre et al. (2005). Karavias and Tzavalis (2012) propose panel unit root tests that allow for a common structural break in the individual effects or trends for a short time period and the large cross-section units. Bai and Carrion-i-Silvestre (2009) propose a panel unit root test, which can control for both multiple structural breaks and cross-section dependence. A common feature of the above panel unit root tests is that structural breaks are assumed to occur instantaneously. As pointed out by Granger and Teräsvirta (1993), Lin and Teräsvirta (1994) and Lundbergh et al. (2003), however, gradual (rather than instantaneous) change might be more appropriate for characterising state or regime-dependent economic and finance variables. In terms of aggregate data, an instantaneous change is plausible only if all of economic agents react simultaneously in the same direction to the shocks. In practice, with many heterogeneous agents faced with agency conflicts and information asymmetries (e.g., Healy and Palepu 2001), however, the time path of structural change in economic and finance variables is likely to be smooth, which we call slowly moving trends (SMT). Another shortcoming of the existing test procedures lies in that individual units in panels are assumed to be cross-sectionally independent with the only exception being Bai and Carrion-i-Silvestre (2009). Breitung and Pesaran (2005) highlight that the cross-section independence assumption is too restrictive, especially in the context of cross-country studies. The cross-section dependence (hereafter, CSD) may arise due to spatial correlations, spillovers, omitted global/multilateral variables, common unobserved shocks or general residual interdependence. The presence of cross-sectionally correlated errors makes the conventional panel unit root and cointegration testing procedure invalid. Maddala and Wu (1999) and Banerjee et al. (2004, 2005) demonstrate that the finite sample performance of the first generation of panel unit root tests, ignoring CSD, perform very poorly for the cross-sectionally correlated panels. CSD is usually modelled through embedding unobserved common factors within error components. Phillips and Sul (2003), Bai and Ng (2004), Moon and Perron (2004) and Bai and Carrion-i-Silvestre (2009) apply the principal component analysis to deal with CSD. On the other hand, Pesaran (2007) and Pesaran et al. (2013) propose to extend the IPS tests augmented with cross-sectional averages of both dependent variable and regressors, that are designed to proxy unobserved common factors. Alternatively, the bootstrap-based panel unit root tests have been advanced to deal with the general structure of cross sectional correlations (e.g., Maddala and Wu 1999; Chang 2004; Smith et al. 2004; Ucar and Omay 2009). We aim to develop the unit root tests in dynamic panels, which are likely to be characterized jointly by SMT and CSD. In this case, inference based on existing tests would be biased and invalid. In particular, we extend the test procedure proposed by Leybourne et al. (1998) (henceforth, LNV) to a panel setting by modelling SMT through the logarithmic smooth transition function, which has been successful in capturing gradual structural changes (Granger and Teräsvirta 1993; Lin and Teräsvirta 1994).Footnote 2
 Next, to deal with CSD, we consider two alternative approaches, the common correlated effects (CCE) estimator proposed by Pesaran (2006) and the bootstrap-based approach. The CCE approach is valid for panels with N and T of the same order of magnitude (Pesaran 2007), which is more general than the principal component (PC) approach that requires the condition, \(N{/}T\rightarrow 0\).Footnote 3 Alternatively, we follow the Sieve bootstrap re-sampling approach (Smith et al. 2004; Chang 2004; Ucar and Omay 2009), and derive the bootstrap-based test which can approximate the (general but unknown structure of) distribution of the test statistic in a robust manner. Via the comprehensive simulation exercises, we investigate the finite sample performance of our proposed tests with the cross-sectionally augmented ADF \(({\overline{{ CADF}}})\) test proposed by Pesaran (2007). Overall simulation results suggest that the performance of our proposed tests is quite satisfactory, by achieving correct sizes and good powers even when cross-section unit and time period are rather modest. Surprisingly, we find that the \({\overline{{ CADF}}}\) test is reasonably powerful even in the presence of SMT, which occurs only if the break parameters are homogeneous across cross-section units. When the break parameters become heterogeneous, however, the power of the \({\overline{{ CADF}}}\) test declines rapidly. On the contrary, our proposed tests perform quite well irrespective of whether break parameters are homogeneous or heterogeneous. As an empirical illustration, we apply our proposed tests to testing stationarity of real interest rates of 17 OECD economies, together with bootstrap-based IPS and \({\overline{{ CADF}}}\) tests. In particular, we employ the sequential panel selection method proposed by Chortareas and Kapetanios (2009). We find that our proposed tests are able to reject the null hypothesis of unit root for 15 countries whereas conventional bootstrap IPS and CADF tests can reject it for only six countries. We observe that different monetary policy regimes have been implemented during the sample period. In particular, most countries have shifted to adopting the inflation target regime during 1990s. Also, the inflation has been more stable while the policy rate has been more smoothed especially after the Great Moderation in mid 1980s. Furthermore, the gradually moving real interest rates have also been affected by technology and productivity shocks as well as changes in households’ preferences. In this context, our findings suggest that allowing for slow moving breaks together with CSD will lead to more rejections of the unit root hypothesis, which provides strong support for the Fisher hypothesis. The rest of the paper is organized as follows. Section 2 develops the unit root test statistic for dynamic panels subject to SMT without CSD among error terms. In Sect. 3, we propose the two unit root test statistic valid for dynamic panels subject to both SMT and CSD, based on the CCE estimation and the Sieve bootstrap, respectively. In Sect. 4, we examine small sample properties of the proposed tests via extensive Monte Carlo simulations. Section 5 provides an empirical application. Section 6 contains concluding remarks. “Appendix 1” describes the Sieve bootstrap method while “Appendix 2” contains the additional simulation results. We also relegate all other technical discussions to the supplementary online Technical Annex.",18
52.0,1.0,Computational Economics,20 March 2017,https://link.springer.com/article/10.1007/s10614-017-9668-6,Decision Theory Matters for Financial Advice,June 2018,Thorsten Hens,János Mayer,,Male,Male,Unknown,Male,"Financial advice is one of the most important services offered in the financial industry. A financial advisor needs to know which trade-off markets deliver and which risk tolerance a client has so that he can recommend to him a portfolio of assets that is optimally placed on the financial markets’ trade-off. Thus the role of a financial advisor is to build a bridge between the clients and the markets. Using a decision theory is a huge simplification in this task. A decision theory structures the market data along simple key characteristics (e.g. means and variances of returns) and it suggests a way to model the risk tolerance of a client (e.g. by variance aversion). Without a decision theory the financial advisor would have to explain to the client all characteristics of the markets for all possible portfolios he can recommend and then infer the client’s risk tolerance in all those dimensions. It is clear that such an approach is impossible both because the time and the capabilities of the client (and the advisor) are limited. In this paper we show that the choice of the decision theory is not innocuous for the financial advice given. For the same data on the markets and the same clients the recommended portfolios differ substantially with the decision theory chosen. The decision theories we compare are mean–variance theory, expected utility theory and cumulative prospect theory. Thus we cover a broad range of decision theories used by practitioners, academics preferring rational and those preferring behavioral decision theories. Depending on the market data the median differences (measured in terms of certainty equivalents) can be as high as 6.5% p.a.! That is to say, if the advisor assumes the client is best described by one of the three decision models but the client is actually a decision maker who follows a different decision model then the recommended portfolio can have a return that is 6.5% lower p.a. than the portfolio that is optimal for the client. To substantiate our claim we combine market data that is standard for asset allocation purposes data with a collection of agents assessed by Abdellaoui et al. (2007) according to Cumulative Prospect Theory. Then we map the clients into other decision models by figuring out the answer to a standard risk tolerance question that a CPT-client with the characteristics found in Abdellaoui et al. gives. This answer is then interpreted as an answer of the client if he were of a different decision model type (mean–variance, expected utility). Notice that a financial advisor always sees the same answer given by the client to this standard risk tolerance question. Whether the financial advisor interprets this answer according to CPT, mean–variance or expected utility depends on the decision theory he uses. For these theories we then compute the optimal portfolios on the market data. Our approach is novel but it should be compared with Levy and Levy (2004) who compare optimal CPT-portfolios computed in two different ways: CPT restricted to the mean–variance efficient frontier and CPT without such restriction. In Levy and Levy (2004) the same decision model (CPT) is assumed and two different solution techniques for finding the optimal CPT-portfolio are considered. The finding of Levy and Levy (2004) is that these different solution techniques do not matter much. De Giorgi and Hens (2009) for original prospect theory (PT) and Hens and Mayer (2014) for CPT extend Levy and Levy (2004) to realistic data sets in which case the differences become larger. In our paper we find even larger differences because we do not only change the optimization technique for a given decision model but we also change the decision theory with which we model the decision maker. The rest of the paper is organized as follows: In Sect. 2 we first briefly discuss the objective function of CPT, followed by the formulation of the four different portfolio selection models considered in this paper. Subsequently we discuss the numerical solution aspects and present the proximity measure utilized for comparing the different optimal portfolios. Section 3 first describes and discusses the data sets that are utilized in our numerical experiments, followed by the presentation of our method for determining the risk-aversion coefficients for the mean–variance, quadratic utility and power utility (CRRA) portfolio selection models. In Sect. 4 we present the numerical results that we obtained for the comparison of the portfolios computed by the different portfolio selection approaches. In Sect. 5 we investigate the robustness of our results. Section 6 concludes the paper.",
52.0,1.0,Computational Economics,06 March 2017,https://link.springer.com/article/10.1007/s10614-017-9669-5,An Efficient Adaptive Real Coded Genetic Algorithm to Solve the Portfolio Choice Problem Under Cumulative Prospect Theory,June 2018,Chao Gong,Chunhui Xu,Ji Wang,,Unknown,,Mix,,
52.0,1.0,Computational Economics,16 March 2017,https://link.springer.com/article/10.1007/s10614-017-9673-9,On the Allocation of Multiple Divisible Assets to Players with Different Utilities,June 2018,Ephraim Zehavi,Amir Leshem,,Male,Male,Unknown,Male,"The study of bargaining between players who can benefit from cooperation dates back to the beginnings of game theory (Nash 1950). Over the years many different solutions to the bargaining problem have been proposed. A good overview of bargaining solutions and models can be found in a volume by Osborne and Rubinstein (1990) and the references therein. In contrast the age-old problem of adjudication of conflicting claims has been dealt with by all societies probably since the dawn of civilization. The contemporary study of the mathematical problem of resolving conflicting claims can be attributed to O’Neill (1982), where he formulated the problem in a game theoretic manner and solved it using cooperative game theoretic techniques. In the last thirty years there has been extensive exploration of the axiomatic bases of bargaining solutions and ways to resolve conflicting claims in bankruptcy cases. An excellent recent overview can be found in Thomson (2013a, b), which extends Thomson’s overview of older results on the relationship between bargaining and the adjudication of conflicting claims (Thomson 2003, 2012). There are several alternative approaches to analyzing collaborative solutions. One approach is based on building an axiomatic structure that leads to a single solution. This approach began with the Nash bargaining solution (Nash 1950, 1953) and includes the analysis of many other solutions; e.g., the Raiffa solution (Raiffa 1953), the Kalai–Smorodinsky solution (Kalai and Smorodinsky 1975), and the family of generalized Nash solutions (Kalai 1977). Other approaches emphasize the negotiation process to reach a final agreement. Salonen (1988) was the first to establish a step-by-step axiomatic definition of the discrete Raiffa solution for the N-player bargaining problem, based on four axioms. Livne (1989), as well as Peters and Damme (1991) presented characterizations of the continuous Raiffa solution. Recently, Trockel (2009) suggested viewing the discrete Raiffa solution as a repetition of a process based on three standard axioms; namely (a) Pareto optimality (b) invariance to affine transformation, and (c) symmetry (Diskin et al. 2011) generalized the Raiffa solution to the case of multi players achieving interim settlements step-by-step. They defined a family of discrete solutions for N-person bargaining problems which approaches the continuous Raiffa solution as the step size gradually becomes smaller. Anbarci and Sun (2013) proposed a unified framework for characterizations of different axioms that lead to different bargaining solutions. Their solution was simplified by Trockel (2014b) who also filled in a gap in the proof. Recently, Trockel also proposed an alternative formulation for the discrete Raiffa solution based on non-transferable utility games (Trockel 2014a). Another approach is to define a bargaining process that leads to a specific bargaining solution. Myerson (1991), Tanimura and Thoron (2008) and Trockel (2011) proposed a mechanism for reaching bargaining solutions in which two players are allowed to make a sequence of simultaneous propositions and to converge to the discrete Raiffa solution. The Aumann and Maschler (1985) bankruptcy solution is based on an interpretation of two claim resolution scenarios discussed in the Talmud. The first case is the Contested Garment (CG) problem where two men disagree on the ownership of a garment.Footnote 1 The second caseFootnote 2 addresses the estate division problem among three women. Aumann and Maschler (1985) constructed two rules that generalize the Talmud rules and can be applied to resolve the bankruptcy problem. Later these rules were generalized by Moreno-Ternero and Villar (2006) who defined a family of rules termed TAL. Thomson (2008) extended the Talmud rules even further by considering a wider family of rules which he termed ICI and CIC. Another line of research was taken by Dagan and Volij (1993). They represented bankruptcy problems as bargaining problems. This enabled them to study the Nash bargaining and the Kalai Smorodinsky solutions as means of solving the bankruptcy problems. Specifically, they proved that the Nash bargaining solution induces the constrained equal award rule and the Kalai-Smorodinski induces the proportional rule. In the other direction, Quant et al. (2006) suggested that certain games; i.e., the class of compromise admissible games with transferable utility, can be considered as coalitional games and their solution was related to the run-to-the-bank (RTB) rule, showing that in certain cases bankruptcy solutions can be the basis for the solution to cooperative games. This leads directly to the query of whether the Raiffa solution can be described as an iterative application of a bankruptcy problem. This is indeed one of the goals of this paper. An interesting generalization of the bankruptcy problem to multiple issues was first proposed by Calleja et al. (2005). In their formulation each agent has multiple claims regarding the total assets, and each claim is related to a different issue. They defined a division problem with multiple issues as follows: Let the set of agents be \(I=\left\{ 1,\ldots ,N\right\} \). Each agent has a vector of claims \(\mathbf{c}_n=(c_{n1},\ldots ,c_{nK})\), regarding the issues \(1,\ldots ,K\) and for all n, k \(c_{nk}\ge 0\). A multiple issue bankruptcy problem is given by the pair \(\left( C,E\right) \) where \(C=\left[ \mathbf{c}_1,\ldots ,\mathbf{c}_N\right] ^T\), \(E>0\) is the total value of the assets that should be divided among the agents and \(c=\sum _{n=1}^N \sum _{k=1}^K c_{nk}>E\). A vector \(\mathbf{x}=(x_1,\ldots ,x_N )\) is efficient if \(\sum _{n=1}^N x_n=E\). A division rule for a multi-issue claim problem is a function that assigns to each multi-dimensional claim problem (C, E) an efficient vector \(\mathbf{x}=f(C,E)\). They proposed a multi-dimensional extension of the run-to-the-bank rule in O’Neill (1982) and showed that it coincides with the Shapley value for the generated coalitional game. Based on the work of Calleja et al. (2005), González-Alcón et al. (2007) showed that the multi-dimensional run-to-the-bank rule always yields a core element, and that it satisfies self-duality. Ju et al. (2007) provides a good discussion of the different problems that can be represented as a multiple issue bankruptcy problem. An extended discussion of the problem of multi-issue bankruptcy is presented in Hinojosa et al. (2012) where it is shown that the theory of cooperative games provides an allocation rule consistent with the Talmud rule (Aumann and Maschler 1985) in the case of two agents. It is worth noting that the literature on multiple issue claim problems has only been concerned with allocating vectors on the face of the N dimensional simplex (the face containing the efficient allocation vectors). However, this does not cover the most general case of claim problems. When there are multiple assets, these are replaced by the total worth of all the assets. In many bankruptcy problems this is indeed desirable for operational simplicity of the bankruptcy problem. However, different types of assets can definitely have different value for different agents. For example, the agents may be subject to different taxation laws, in which case they might prefer to have larger share of one type of asset rather than others (for example, the taxation of property, equity in companies and cash differ significantly depending on the countries. In this sense, we assume that utility is not transferable between agents. This generalization is the main focus of this paper. In this case the differences between bankruptcy and general bargaining tend to decrease, since in both cases different agents have different utilities for each division of the assets. The non-transferable utility claim problem can now be formulated as follows: Assume that we have N agents and a vector of assets \(\mathbf {E}=\left( E_1,\ldots ,E_K\right) \). We assume that each of the assets is divisible such that any agent can get a part of each asset. Agent \(n, \ \ 1 \le n \le N\) has claims \(c_{n1},\ldots ,c_{nk}\) for each of the assets. Furthermore, each agent has a utility associated with each asset \(u_{nk}\). A generalized claim problem is given by a triple \(\left( \mathbf {E},\mathbf {C},\text{ U }\right) \). The allocation matrix \(\mathbf {A}=(\alpha _{nk}).\) The total utility for player n is given by \(\sum _{k=1}^K \alpha _{nk} u_{nk}\). The allocation rule for a generalized claim problem is a function \(f\left( \mathbf {E},\mathbf {C},\text{ U }\right) =\mathbf {A}\) where \(\mathbf {A}\) is a stochastic allocation matrix, i.e., \(\sum _{n=1}^N \alpha _{nk}=1\) for every k. This generalizes Haake’s model (Haake 2009), who studied the Perles–Maschler solution and the discrete Raiffa solution for two agents sharing K divisible commodities with utilities that are linear in the share in each commodity. Haake derived a procedure based on pricing. In an article on frequency allocation problems, Leshem and Zehavi (2006, 2008) discussed a more general resource allocation problem where the utilities are convex functions of the weighted linear sum of the assets (a slightly more general model than the generalized claim problem). They provided an efficient algorithm for computing the NBS and showed that in the two agents case only a single commodity is shared and the computational complexity is \(K\log (K)\). They also discussed the NBS in the general case of N players and \(K\ge 2\) commodities (Leshem and Zehavi 2008) and showed that there is always a solution where at most \({N}\atopwithdelims (){2}\) commodities are shared and any other commodity is allocated to one of the agents. This was extended and similar results were shown to hold for the Generalized Nash solutions and the Kalai–Smorodinsky solutions (KSS) in Zehavi and Leshem (2009). A related problem was considered by Ponsati and Watson (1997) and by Mármol and Ponsati (2008) who addressed the problem of resolving global bargaining problems over a finite number of different issues. They defined max-min and leximin global bargaining solutions. However, their rules allowed the agents to back away from agreements on previous issues if required. This was done through a comprehensive extension of the sum of the previously agreed points and the new point. Interestingly, under quite general conditions, a recent result reported in Zehavi et al. (2013) showed that the optimal solution for any Pareto optimal solution can always be achieved by allocating all the utility related to each issue to one of the agents in all but \(N-1\) issues. By extending the analysis of the pareto boundary by Mjelde (1983) they showed that this is also valid for the (generalized) NBS and the KS solutions. Since under any issue-by-issue negotiation it is unlikely that for any given issue all the value related to an issue will be allocated to a single agent, issue-by-issue bargaining with comprehensive extension is limited. In fact the comprehensive extension (Mármol and Ponsati 2008) conceals a renegotiation of agreements in the previous stages. In this paper we extend both the discrete Raiffa bargaining solution (Raiffa 1953) and the Talmud rule (TR) (Aumann and Maschler 1985) for resolving the allocation of K assets to N agents when the utility of each player is a convex function of the linear sum of its utility for each asset. In the discrete Raiffa solution the players reach an agreement step by step on an intermediate partition of the utility. However, if some utility is left over, all the players continue to solve the problem until Pareto optimality is achieved. The Talmud rule bankruptcy solution is based on an extension of a Talmudic approach involving two individuals claiming a single garment to resolve a dispute between heirs. The structure of the paper is as follows: In Sect. 2 we describe the model of the bargaining game and define a unified notation. In Sect. 3 we discuss the generalizations of the Raiffa and the Talmud rule to K assets. In Sect. 4 we discuss the properties of the solutions and provide some detailed examples. We conclude that global bargaining solutions can be obtained by solving a sequence of linear programming problems.",
52.0,1.0,Computational Economics,17 March 2017,https://link.springer.com/article/10.1007/s10614-017-9676-6,Financial Soundness Prediction Using a Multi-classification Model: Evidence from Current Financial Crisis in OECD Banks,June 2018,D. Fernández-Arias,M. López-Martín,F. Fernández-Navarro,Unknown,Unknown,Unknown,Unknown,,
52.0,1.0,Computational Economics,17 August 2016,https://link.springer.com/article/10.1007/s10614-016-9604-1,Programming Correlation Criteria with free CAS Software,June 2018,George E. Halkos,Kyriaki D. Tsilika,,Male,Female,Unknown,Mix,,
52.0,2.0,Computational Economics,27 March 2017,https://link.springer.com/article/10.1007/s10614-017-9678-4,Robust Monetary Policy in a Model of the Polish Economy: Is the Uncertainty Responsible for the Interest Rate Smoothing Effect?,August 2018,Mariusz Górajski,,,Male,Unknown,Unknown,Male,"The fundamental point of the analysis presented in this paper is based on the assumption that policy makers act in an optimal manner. This hypothesis is consistent with a generally accepted principle of economics which states that any economic behaviour can be understood as a problem of constrained optimization (see Tinbergen 1952; Theil 1961). It is believed that this principle should apply to central banks (CBs) (see Friedman 1969; Svensson 1997; Galí 2009) as strictly as to the representative firm or household. However, standard optimal inflation targeting rules obtained in linear–quadratic models are inconsistent with data and produce a too aggressive policy. Moreover, the majority of central bank short-term interest rate paths are smooth and only gradual changes can be observed. This gradualism has been considered as evidence that monetary policy makers follow the interest-rate smoothing incentive and it can be explained using the optimal monetary policy models by adding the interest-rate smoothing term to the CBs objective function (Goodfriend 1987). But this heuristic procedure has not much substantiation in central bank’s targets and raises the question: What are the rational reasons for the gradual movements in the monetary policy instrument? This paper examines whether gradual movements of optimal interest rates can be explained by incorporating a structure of parameter uncertainty for an optimal central bank with the sole aim of price and output stability. More precisely, we investigate the effects of different forms of uncertainty in the linear framework on the optimal central bank policy. In models with parameter uncertainty we minimize the expected value of central bank’s objective function which is calculated also with respect to the random model’s parameters and as a result we obtain a so called optimal policy with multiplicative uncertainty (or in short robust monetary policy). The Brainard conservatism principleFootnote 1 not always turns out to be fulfilled in dynamic models. In existing literature this principle is confirmed for a few dynamic models of monetary economy, but still under the assumption that there is no correlation between the risk and the parameter uncertainty. An unambiguous answer to the question of whether the correlated uncertainty about parameters affect optimal monetary policy is not known. We examine the Brainard principle in the presence of correlation between random parameters and exogenous shocks. This paper proposes a general method based on the dynamic programming principle to derive optimal monetary policy rules with multiplicative uncertainty (see “Appendix 1”). These rules are those that are the best amongst those that yield an acceptable performance in a specified range of models described by parameter uncertainty of the structural model. In this paper we propose a new and simple approach to uncertainty-management with no active learning process, where estimation and control are separated. We apply dynamic programming methods for general linear systems to derive exact solutions. Moreover, we assume that the model parameters follow a serially uncorrelated process with an estimated mean and variance at the beginning of the decision period. This framework helps us to obtain an analytical solution of optimal monetary policy and makes the counter-factual model simulations feasible in reasonable time. Furthermore, we do not need to impose any prior assumptions on the parameters distribution. In addition to the main contribution, we show empirical application for Poland. As the true model of the economy is unknown, we estimate a VAR model of monetary transmission mechanism in Poland with parameter uncertainty. We do not impose any restriction on random model parameters, hence this approach can to some extent handle model uncertainty. On the basis of the estimated model the optimal paths of macroeconomic variables are found and the analysis of impulse response functions (IRFs) with different stochastic structures of parameters is conducted. Analysing not only these structures of parameter uncertainty but also controlling the level of multiplicative incertitude we compare volatility of macroeconomic variables and IRFs from optimal monetary policy models with the empirical model counterparts to find the uncertainty structure which matches closer the optimal policy to data. We compare the individual influence of two factors: structural uncertainty in macroeconomic dynamics and smoothing term in the central bank objective function on the optimal unrestricted policy rule. We show that the optimal paths of interest rates in the model which has certainty and an interest rate smoothing term in the objective function can be approximated by the optimal interest rates derived from the model without a smoothing term, but with appropriately chosen uncertainty parameters. The paper is organized as follows: in the next section we briefly review the existing literature on uncertainty in monetary policy. Section 3 introduces the linear model of monetary transmission mechanism with parameter uncertainty. In Sect. 4 we derive the solution to the optimal monetary policy problem with multiplicative uncertainty. Section 5 contains the empirical results where we compare the optimal monetary policy rules with different structures of model uncertainty. In Sect. 6 we conclude our findings.",3
52.0,2.0,Computational Economics,10 April 2017,https://link.springer.com/article/10.1007/s10614-017-9677-5,Making Decisions in a Sustainable Development Context: A State-of-the-Art Survey and Proposal of a Multi-period Single Synthesizing Criterion Approach,August 2018,Anissa Frini,Sarah Benamor,,Female,Female,Unknown,Female,"In 2005, Quebec’s provincial government passed a law on sustainable development that established a new common management framework for all government department and agency decisions, especially decisions related to public project selection. This law emphasizes the need to simultaneously consider the environmental, social and economic dimensions of decisions and the need for a long-term sustainable development vision. Government decisions must now take into consideration not only the immediate impact but also future consequences, which may be uncertain, imprecise or ambiguous. Consequently, government departments and agencies are seeking to develop best practices and innovative methods that will enable them to make sustainable decisions in areas such as energy, healthcare, transportation, forest management, construction, electricity generation and so on. Making decisions in sustainable development contexts is now a major concern for various levels of government. To comply with this legislation, government decisions must assess the long-term compromise between maintaining environmental integrity, social equity and economic efficiency while promoting stakeholder involvement. Decision-making problems in this context are complex because of conflicting decision-making criteria, diverse stakeholder views and values, long-term consequences that must be assessed in an uncertain future, and inaccurate data (imprecision, ambiguity, fuzziness, incompleteness, etc.). More specifically: The economic, social and environmental impact of alternatives must be considered concurrently in the decision-making process. Each alternative must be evaluated based on a set of conflicting economic, social and environmental decision-making criteria, which can be quantitative (evaluated on a ratio or interval scale) or qualitative (evaluated on an ordinal scale or using linguistic variables). Since sustainable development is development that meets the needs of the present without compromising the ability of future generations to meet their own needs, each alternative must be evaluated over short, medium and long-term planning horizons. Assessments of long-term consequences of alternatives should account for unforeseen events, which can occur over time and affect future evaluations of the alternatives. Evaluation of alternatives could be flawed (inaccurate, ambiguous, fuzzy or incomplete). A variety of stakeholders with different and ultimately conflicting viewpoints could be involved in the decision. This article is concerned with the decision-making process in a sustainable development context, keeping these factors in mind. More specifically, this article focuses on how to make sustainable decisions in these complex situations. Our objective is twofold. The first objective aims to provide an extensive survey of the literature in order to identify research avenues that have not yet been addressed. The second objective aims to develop a multi-criteria, multi-period approach that will allow for aggregation of the long-term assessment of alternatives. 
Objective 1 aims to provide an extensive survey of the literature on decision-making in sustainable development contexts. The results focus on the main methods that were used, application areas, criteria and indicators that were considered, uncertainty modeling (if any occurred), stakeholder involvement, and any considerations of long-term assessment. This survey aims to provide researchers interested in developing novel methods and approaches for sustainable decision-making with the most valuable data from current research and also provides professionals and decision-makers with a set of significant information on dealing with real decision-making situations in sustainable development contexts. 
Objective 2 aims to propose a multi-criteria, temporal (multi-period) approach that attempts to accommodate the requirements of sustainable development under uncertain conditions. Such an approach shows how the multi-criteria decision aid (MCDA) paradigm can be of use in processing the multi-period aspect of the decision-making process in sustainable development contexts. This novel approach is then used to select the best compromise from among several sustainable forest management options while considering economic benefits, environmental impact and decision-maker preferences. Sustainable decisions are very complex to achieve, and our research provides a practical shift from business as usual toward sustainability by including environmental, social and temporal dimensions in the decision-making process. This article is organized as follows. Section 2 introduces sustainable development. Section 3 presents the main results and a summary of the literature survey on decision-making in sustainable development contexts, with particular emphasis on application areas, methods and uncertainty modeling. Future research avenues are then identified. Section 4 proposes the steps of our novel multi-criteria, multi-period single synthesizing criterion approach, and Sect. 5 illustrates this approach in the context of sustainable forest management.",17
52.0,2.0,Computational Economics,25 April 2017,https://link.springer.com/article/10.1007/s10614-017-9685-5,"Brownian Signals: Information Quality, Quantity and Timing in Repeated Games",August 2018,António Osório,,,Male,Unknown,Unknown,Male,"Coordination and cooperation depend crucially on the frequency of interaction between the involved parties. This fact affects the agents’ behavior and the stability of these relationships. In the simplest setting, with perfect information in which actions and monitoring occur simultaneously at the same frequency, smaller time intervals facilitate cooperation by making agents more patient, and as a consequence the deviation incentives decrease. Conversely, large time intervals render cooperation more difficult because agents are less patient and the incentives to deviate increase. However, with imperfect monitoring, varying the time interval leads to different results, because the length of the time interval between actions affects information quality (Abreu et al. 1991; Fudenberg and Levine 2007, 2009; Osório 2012; Sannikov and Skrzypacz 2007). For instance, in small time intervals the deviations incentives are weak, but information quality might be so low that coordination and cooperation are impossible. However, we may also have situations in which information quality is high enough to make coordination and cooperation possible. In this context, information quality is critical to provide incentives and to determine the equilibrium payoffs. Information quality depends crucially on how actions feedback into signals—the information structure. Therefore, it matters whether actions affect the fundamental value of a given variable or the variance (Fudenberg and Levine 2007, 2009). For instance, at moments of market stability, when an OPEC (Organization of the Petroleum Exporting Countries) member country unilaterally decides to increase its oil supply, the oil price (i.e. the fundamental value) is likely to decrease. However, at moments of market instability and conflict, the same action may have an insignificant effect on the fundamental value, but may instead induce additional market volatility (i.e. the variance). Therefore, in the former case, the parties involved should monitor the fundamental value, while in the latter case the parties involved should monitor the variance. Is it more difficult for the agents to coordinate and cooperate when actions affect the fundamental value or when they affect the variance? How do coordination and cooperation depend on the length of the time interval between actions and monitoring activities? This paper attempts to answer these questions. We examine how different information structures depend on the time interval between actions and monitoring activities, which are assumed to have the same frequency. We examine how information quality and quantity vary with the length of the time interval between actions, and how they are used to provide incentives (Mirrlees 1974). Information quality refers to the signals’ precision, while information quantity refers to the number of signals used to enforce cooperation. In this context, the starting point is to acknowledge that the provision of incentives depends crucially on how information quality improves or decays in relation to the deviation incentives (due to discounting) and its actual level. The aim of this paper is not to present a general theory, which would be very complex, but rather to illustrate some relevant aspects related to the use of information quantity and quality in the provision of incentives. Contrary to the existing literature (Abreu et al. 1991; Fudenberg and Levine 2007; Sannikov and Skrzypacz 2007), which has focused exclusively on the limit case (i.e. when the length of the time interval between actions tends towards zero), we consider the non-limit case (i.e. any length of the time interval other than zero). While the limit case is interesting for theoretical and tractable reasons, the non-limit case is more realistic, although it requires the use of numerical and computational methods. Nonetheless, gaining an understanding of the existing information trade-offs justifies the use of these tools. 
Abreu et al. (1991) were the first to initiate the literature on frequent monitoring. In a setting with Poisson signals, they have shown that cooperation can be sustained at the limit when the observations of the process represent bad news, but not when they represent good news. Nonetheless, in both cases, the best payoffs are not obtained at the limit (see also Osório 2015). More recently, Sannikov (2007) and Faingold and Sannikov (2011) have renewed the interest in frequent monitoring by considering continuous time methods for studying repeated games. Simultaneously, by studying the limit of the discrete time games with Brownian signals, Fudenberg and Levine (2007) have considered several information structures (see also Fudenberg and Levine 2009; Osório 2012; Sannikov and Skrzypacz 2007, 2010).Footnote 1 They found that full efficient results are possible in the limit case if deviations increase the noise of the process. In this respect, the present paper can be seen as a non-limit extension to the Fudenberg and Levine’s (2007) limit analysis, and we hope it will help to provide a better understanding of the strengths and the weaknesses of each of the information structures discussed in the literature. The provision of incentives depends crucially on the rate to which information quality improves or decays relative to the deviation incentives (due to discounting) and the level of information quality. In the ideal scenario, we would like to have simultaneously the highest information quality and the lowest information quantity. However, with imperfect information this objective is not attainable because in general information quality is limited. In this context, it is natural to think that information quantity and quality are substitutes because ceteris paribus, higher information quality, at the margin, reduces the quantity of information needed, and vice versa. However, we found that substitution occurs only if the information quality gains or losses are strong enough to dispense (positive substitution) or require (negative substitution), respectively, the use of additional information quantity. The reason is that variations in the length of the time interval between actions affect not only the quality of information but also the deviation incentives. Consequently, there are no ceteris paribus situations. We also found that when the information quality gains or losses are weak, information quantity must be used to compensate for the weaknesses of information quality in providing incentives. In this sense, compensation is a weaker form of substitution in which information quantity and quality are jointly used in the same direction. Since information quality depends crucially on how actions affect the distribution of the Brownian public signals, we have considered three different information structures: In Sect. 4.1 we consider the case where actions affect the drift of the process (i.e. the fundamental value). This information structure captures the most commonly observed situations in real life. For instance, when a worker reduces its effort, the output is likely to decrease, or when a firm increases its supply, the market price is likely to decrease. With this information structure, information quality tends to improve with the time interval, except for large time intervals. The intuitive argument is that reliable inference about the drift requires a sufficiently large time interval.Footnote 2 This observation is especially true for small time intervals because increasing the length of the time interval leads to strong improvements in information quality. Consequently, information quality substitutes information quantity. However, as the time interval increases, the marginal improvement in information quality diminishes to the point where it is not enough to decrease the number of signals used to provide incentives. In this case, information quality needs to be complemented with information quantity. Lastly, for large time intervals, and before the equilibrium degenerates, signals become extremely noisy. In order to provide players with incentives, the falling information quality is substituted by information quantity. This pattern, observed for large time intervals and before the equilibrium degenerates, is common to all information structures. However, if deviations affect the noise of the process (i.e. the variance), information quality always decays with the length of the time interval between actions. Contrary to the case in which actions affect the drift, the best inference about the noise parameter is obtained in the smallest time intervals (Rao 1999). In Sect. 4.2 we consider the case where deviations increase the noise of the process. This information structure captures situations in which the observation of extreme events, such as high market volatility or high sales variation, are associated with misbehavior or lack of effort. With this information structure, for small time intervals, information quality is high but decays with the length of the time interval between actions, at a rate that compensates for the increase in deviation incentives.Footnote 3 Therefore, while the information quality is sufficiently high, it is possible to reduce the quantity of signals. For sufficiently large time intervals and before the equilibrium degenerates, the deviation incentives increase and the signals become extremely noisy (information quality is low); the falling information quality needs to be offset by increasing the quantity of signals. In Sect. 4.3 we consider the case where deviations decrease the noise of the process. This information structure captures situations in which the observation of stable events, such as unchanging profits, are associated with a lack of effort or commitment. With this information structure, we found that information quality is low and decays with the length of the time interval.Footnote 4 Efficient monitoring substitutes the falling information quality with information quantity. Section 5 concludes with additional comments, identifies possible information patterns and discusses avenues for further research.",3
52.0,2.0,Computational Economics,25 April 2017,https://link.springer.com/article/10.1007/s10614-017-9686-4,New Splitting Scheme for Pricing American Options Under the Heston Model,August 2018,Maryam Safaei,Abodolsadeh Neisy,Nader Nematollahi,Female,Unknown,Male,Mix,,
52.0,2.0,Computational Economics,27 April 2017,https://link.springer.com/article/10.1007/s10614-017-9687-3,Debt Persistence in a Deflationary Environment: A Regime-Switching Model,August 2018,Piero Ferri,Fabio Tramontana,,Male,Male,Unknown,Male,"Sovereign debt presents a rich variety of dynamic patterns. They can be stationary or they may show sudden accelerations followed by slow decelerations that remain a bounded, generating considerable persistence. However, they can also explode. Debt is one of the few economic variables that are allowed to have an explosive behavior. Different theories have tried to investigate these patterns. It is rather important to classify them according to the economic environment taken into consideration. To this purpose, two extreme cases are worth considering. The first refers to an inflationary environment, where also hyperinflation (see Sargent 1986) can be a possibility. These two phenomena can play different roles. In fact, hyperinflation may proceed in tandem with debt dynamics. In this case, default is a likely asymptotic result, while the currency can be renominated in new units. This is the main reason why an explosive pattern is allowed. From a mathematical point of view, this process is equivalent to imposing new initial conditions to a dynamic process, as Minsky (1982) has suggested sometime ago. From a historical point of view, default is more typical of emerging countries, although similar cases happened also in the advanced ones. Usually, in the latter, inflation plays a different role. By cutting the nominal value of debt, it contributes to tame its dynamics as happened in the post-war experiences. Three characteristics of the analyses referred to these environments are worth stressing. First, the events triggering the explosive behavior are attributed to powerful shocks such as wars or social unrest. Second, they stress the breakdown of the rules of the game and the role of institutional changes. Finally, they mainly refer to a monetary perspective with or without rational expectations (see Cochrane 2010). The other set of theories studies debt dynamics in a deflationary environment. In this case, prices need not necessarily to spiral downward but they can be stationary in a depressed economy. The Japanese economy during the 1990 s belongs to this variety and has stimulated many researches (see Doi et al. 2011). The same holds true for the “Great Recession” that can be attributed to the same species. In this context, not only the problem of debt sustainability has been faced (see D’Eramo and Zhang 2015), but also the conflict “austerity policies” vis-a-vis expansionary measures has been a central theme of research and of policy debate (see Cherif and Hasanov 2012). If one limits the analysis to these latter episodes and concentrate the attention on the experience of advanced countries,Footnote 1 three stylized facts emerge. Even though debt dynamics have undergone sudden accelerations, this does not mean that the explosive pattern is the most likely possibility. Rather persistence seems to be the most prominent feature. In the second place, the correlation between debt and growth is negative (see Reinhart and Rogoff 2011; Reinhart et al. 2012) Finally, the correlation is nonlinear (see Eberhardt and Presbitero 2015). The fact that the benchmark variable has become growth and not inflation witnesses that these phenomena have occurred in a context of stationary prices and slack resource. For these reasons, monetary and real aspects must be jointly considered. The aims of the present paper are threefold. It presents a model that tries to mimic these stylized facts. Furthermore, it looks for necessary and sufficient conditions that generate bounded fluctuations. Finally, it tries to identify the main driving forces underlying them. The main thesis put forward is that growth is the driver of the dynamics, while policy enhancing growth can contribute to make debt dynamics sustainable. To pursue these targets three types of assumptions have been put forward. First, debt persistence is considered within a business cycle perspective and not in a long-run vein, with one caveat. In fact, the frequencies are lower than the traditional ones, which implies a medium-run perspective. The second assumption is that fluctuations are endogenous and not driven by exogenous shocks. It follows that contrasting forces pulling in different directions are to be considered. The technique used to pursue this objective is based upon regime switching which can generate robust bounded fluctuations. The third assumption is that debt and growth are interdependent variables to be studied within a theoretical model. We have already mentioned that the “Great recession” has been characterized by unemployment and deflation. These stylized facts are in tune with a Keynesian paradigm that generates quantity adjustments driven by aggregate demand in the presence of stationary prices. The curse of dimensionality constrains the model that can only be 2-D in order to obtain analytical results. Furthermore, the Jacobian matrix has been made triangular in order to capture, geometrically, the necessary and sufficient conditions to generate bounded fluctuations. These special results are in keeping with those emerging from simulations of a generic 2-D model and this may possibly open the way to further increase in the dimension of the model. The results are very encouraging. In fact, fluctuations in an economy with debt can be very persistent, under different constellations of parameters. Furthermore, debt sustainability is obtained without reference to any Ricardian intertemporal constraint (see Ljungquist and Sargent 2004). Finally growth is the driver of dynamics while policies can be stabilizing. Even though fluctuations are generated by the struggle between quantity adjustment and the Wicksellian role of the rate of interest (see Woodford 2003), the economic mechanisms underlying these results are fundamentally Keynesian. In this context, growth seems to have a prominent role. The structure of the paper is the following. Section 2 introduces regime switching by defining the threshold and justifying the changes in the parameters of the meta model. Section 3 presents a 2-D version of the model where also simulations are carried out. Section 4 analyzes local stability referred to a single regime. Section 5 illustrates a triangular matrix and presents a one-dimensional linearized version of the model as a special case. It allows to emphasize the role of growth. Section 6 deals with the robustness of the model. Section 7 generalizes the analysis by dropping the triangularity assumption. Section 8 concludes.",1
52.0,2.0,Computational Economics,02 May 2017,https://link.springer.com/article/10.1007/s10614-017-9683-7,Multi Criteria Decision Making in Financial Risk Management with a Multi-objective Genetic Algorithm,August 2018,Sujatha Srinivasan,T. Kamalakannan,,Unknown,Unknown,Unknown,Unknown,,
52.0,2.0,Computational Economics,04 May 2017,https://link.springer.com/article/10.1007/s10614-017-9690-8,Bayesian Variance Changepoint Detection in Linear Models with Symmetric Heavy-Tailed Errors,August 2018,Shuaimin Kang,Guangying Liu,Min Wang,Unknown,Unknown,,Mix,,
52.0,2.0,Computational Economics,08 May 2017,https://link.springer.com/article/10.1007/s10614-017-9689-1,Simulation Solution to a Two-Dimensional Mortgage Refinancing Problem,August 2018,Dejun Xie,Nan Zhang,David A. Edwards,Unknown,,Male,Mix,,
52.0,2.0,Computational Economics,10 May 2017,https://link.springer.com/article/10.1007/s10614-017-9684-6,A Spatial Game Theoretic Analysis of Conflict and Identity,August 2018,Anirban Ghatak,Diganta Mukherjee,K. S. Mallikarjuna Rao,Unknown,Unknown,Unknown,Unknown,,
52.0,2.0,Computational Economics,22 May 2017,https://link.springer.com/article/10.1007/s10614-017-9695-3,Nonlinear Forecasting of Euro Area Industrial Production Using Evolutionary Approaches,August 2018,Christos Avdoulas,Stelios Bekiros,,Male,Male,Unknown,Male,"Forecasting future economic activity has always been important for policy makers and practitioners. The interest in investigating forecasting of Euro area industrial production (IP) has been growing significantly in recent works, especially between the peripheral and the core Eurozone countries. In particular, Zhou and Yang (2010) probe into the causes of the recent Eurozone crisis for PIIGS in relation to the EMU mechanism and the transmission of external shocks, and analyze the impact of the IP on their growth. Fincke and Greiner (2011) analyze how primary surpluses react to variations in debt for countries of the Euro area, including the PIIGS, wherein they allow for time-varying coefficients in different linear models. Albonico et al. (2016) find that while fiscal policies did not complement the monetary policy stimulus in response to the financial crisis, the post-2007 surge in expenditure-to-GDP and IP ratios was apparently determined by non-policy shocks that reduced output growth. Whilst most empirical works predict output growth or IP via uni- or multi-variate linear models (Stock and Watson 2003), when they are routinely subjected to nonlinearity tests they indicate inadequacy and misspecification biases. However, even when nonlinearity is found, parameter constancy is rarely tested (Eitrheim and Teräsvirta 1996). The increasing number of empirical works supporting the existence of nonlinear models has led economists to the conclusion that the linear hypothesis is not often inherent, but used just for analytic simplicity. For example, in some studies such as by Marcellino (2004), Teräsvirta (2006) and Teräsvirta et al. (2010) utilizing European monthly data series, there is evidence in favor of the usefulness of Threshold Autoregressive (TAR) models towards better predictability. Nevertheless, in those studies there seems to be no relationship between the forecast horizon and the performance of nonlinear models, although the Logistic smooth TAR model performs relatively well at shortest horizons. Marcellino (2004) finds that when linear models outperform nonlinear ones, the latter are rather poor predictors, with the difference in forecastability being significant. More recent studies in macroeconomic forecasting report the superiority of nonlinear models, however they suffer severely from efficient methods of precise coefficient estimation (Franses and van Dijk 2000). Following Stock and Watson (2003) we argue that robust forecastability is dependent upon the optimality of the estimated out-of-sample parameters. As regression or autoregressive time-series models fail to produce accurate forecasts, recent studies rely on machine learning techniques in dealing with nonlinear modeling. We propose an approach for nonlinear parameter estimation using evolutionary algorithms. Evolutionary methods have been utilized recently in the statistical literature e.g., by Qiu et al. (2014). Specifically, we compare the TAR class estimation methods i.e., Generalised Least Squares (GLS), Non Linear Least Squares (NLS) against our hybrid comprising evolutionary programming and least squares. We use various nonlinear models such as Self-Exciting TAR (SETAR), Smooth Transition AR (STAR), Logistic STAR (LSTAR) etc., and we examine their forecasting results versus linear benchmarks for three Eurozone countries, namely Germany (core), Italy and Spain. The latter (members of the so-called PIIGS) appear to be interesting to investigate as they seem to exhibit a more deviant behaviour than the northern European markets, due to the major sovereign debt crisis which hit Eurozone after 2010. Section 2 briefly discusses nonlinear modeling and provides a short overview of the new approach. Section 3 presents the empirical results and Sect. 4 concludes.",1
52.0,2.0,Computational Economics,30 May 2017,https://link.springer.com/article/10.1007/s10614-017-9702-8,Measurement Error Models for Replicated Data Under Asymmetric Heavy-Tailed Distributions,August 2018,Chunzheng Cao,Yahui Wang,Jinguan Lin,Unknown,Unknown,Unknown,Unknown,,
52.0,2.0,Computational Economics,01 June 2017,https://link.springer.com/article/10.1007/s10614-017-9704-6,A Stochastic EM Algorithm for Quantile and Censored Quantile Regression Models,August 2018,Fengkai Yang,,,Unknown,Unknown,Unknown,Unknown,,
52.0,2.0,Computational Economics,07 June 2017,https://link.springer.com/article/10.1007/s10614-017-9701-9,Labor Market Volatility in the RBC Search Model: A Look at Hagedorn and Manovskii’s Calibration,August 2018,Manoj Atolia,John Gibson,Milton Marquis,Male,Male,Male,Male,"The Shimer puzzle, or the observation that the standard Diamond, Mortensen, and Pissarides (DMP) labor market search model fails to match the high volatility of labor market variables (vacancies, unemployment, and their ratio, labor market tightness) relative to average labor productivity (ALP) that is observed in the U.S. data is well known.Footnote 1 Shimer (2005) notes that in the DMP search model, wages tend to absorb fluctuations in productivity leading to small adjustments in vacancies and unemployment over the business cycle. Real business cycle (RBC) search models (RBC models with DMP search friction) inherit this lack of labor market volatility (see Andolfatto 1996; Merz 1995; Shimer 2005; Hagedorn and Manovskii 2008). 
Shimer (2005), Hall (2005), Hall and Milgrom (2008), and Pissarides (2009) show that introducing wage stickiness in the standard DMP framework can resolve the Shimer puzzle.Footnote 2 Hagedorn and Manovskii (2008) (HM) propose an alternative solution that simply involves recalibrating the DMP model. We show that while the HM calibration resolves the Shimer puzzle in the extended RBC search model when preferences are linear, as they are in the standard DMP model, this is not the case once agents are risk averse. In addition, there is a further deterioration of the model’s predictions when the agents’ Frisch elasticity of labor supply is made finite. These findings suggest that the success of the HM calibration was driven, in part, by the assumption of linear preferences embedded in the DMP model. Given the non-linearities (in the utility function and production function) inherent in the modifications of the standard DMP model needed to place it in the RBC framework, we approximate a solution to our model nonlinearly. Specifically, we employ the Generalized Stochastic Simulation Algorithm (GSSA) described by Judd et al. (2011) and Maliar and Maliar (2014) to obtain a nonlinear approximation, and we demonstrate that this approximation yields a highly accurate solution, according to the DM-statistics test (see den Haan and Marcet 1994). Prior to HM, a standard calibration of the DMP model would set worker’s bargaining power to either yield a symmetric Nash bargain or to satisfy the Hosios criteria for the Pareto optimality of the decentralized solution, and set the disutility of work to target the replacement ratio observed in the data. With worker’s bargaining power and disutility of work pinned down, vacancy posting costs and the matching function parameter would be set to target the average values for the job finding rate and labor market tightness that are found in the data.Footnote 3 HM disagree with this procedure as it ignores the size of vacancy posting costs and the elasticity of wages with respect to average labor productivity, which are both found to be very small in the data. HM state that the primary issue with the standard calibration is its failure to match the small vacancy posting costs observed in the data. In DMP models, firms pay workers less than their marginal products in order to recoup their vacancy posting costs. As such, firms earn period-by-period accounting profits whose size is determined by the size of the vacancy posting costs present in the model. The central idea behind the HM calibration is that these profits should be small and they should fluctuate significantly in percentage terms at business cycle frequencies. Large percentage fluctuations in firm’s accounting profits give rise to strong incentives to adjust vacancy creation, leading to large movements in unemployment and labor market tightness. In order to capture the small and volatile accounting profits, HM sets vacancy posting costs to a small level that is consistent with the data, and they set worker’s bargaining power to target the elasticity of wages with respect to average labor productivity. With these two values pinned down, the disutility of work and the matching function parameter are set to target the average value of the job finding rate and labor market tightness found in the data. This calibration strategy yields a value for the replacement ratio of 0.96.Footnote 4
 The HM calibration succeeds in increasing the relative volatility of labor market variables to realistic levels in the DMP model. However, the DMP model is special—it assumes risk neutral agents, constant disutility of labor (or equivalently, an infinite Frisch elasticity), and a production process that is linear in labor. HM close their paper by stating that their calibration strategy will also resolve the Shimer puzzle in the more general RBC search framework, where agents are typically risk averse and the production process includes capital as an additional input. In this paper, we put this claim to the test by implementing the HM calibration strategy in a fully specified RBC search model using a standard Cobb–Douglas production function and a variety of commonly used utility specifications. In order to ensure an accurate approximation, we use the Generalized Stochastic Simulation Algorithm (GSSA) to solve our model nonlinearly. However, as there is currently an open debate regarding the importance of numerical accuracy in models with labor market search frictions (see Petrosky-Nadeau and Zhang 2016; Lan 2017), we also solve a linear approximation to our model. While we find that this linearized solution fails the accuracy test based on DM-statistics, the basic conclusions are consistent across solution methods. We find that while the HM calibration significantly amplifies the volatility of labor market variables relative to average labor productivity (ALP) for the various utility specifications, it is able to match the data and resolve the Shimer puzzle only when one assumes that agents are risk neutral. Increasing the degree of relative risk aversion to unity (which is at the lower end of the range used in most RBC studies), sharply reduces labor market volatility under the HM calibration. The heightened consumption-smoothing behavior reduces the intertemporal elasticity of substitution in consumption and renders the consumption-savings decision less responsive to productivity shocks thereby mitigating the demand for labor by firms. Maintaining a low wage elasticity with respect to average labor productivity coincides with a weaker employment response. Thus, our results indicate that the success of the HM calibration in resolving the Shimer puzzle is at least partially dependent on the assumption of risk neutrality embedded in the standard DMP model. A similar effect results when the Frisch elasticity is lowered to values generally in accord with macro estimates. These results suggest that if one is interested in resolving the Shimer puzzle within an RBC search model under conventional values of risk aversion and Frisch elasticity, the HM calibration may get you closer to the data, but additional amplification mechanisms, such as financial frictions, are needed (see Atolia et al. 2015). Lastly, we find that while the GSSA approximation is much more accurate than the linearized solution in terms of Euler residuals and DM-statistics, the main conclusions derived from the model remain unchanged.",5
52.0,2.0,Computational Economics,12 June 2017,https://link.springer.com/article/10.1007/s10614-017-9703-7,Multivariate Co-movement Between Islamic Stock and Bond Markets Among the GCC: A Wavelet-Based View,August 2018,Chaker Aloui,Rania Jammazi,Hela Ben Hamida,Male,Female,Female,Mix,,
52.0,2.0,Computational Economics,13 June 2017,https://link.springer.com/article/10.1007/s10614-017-9708-2,Integrated Portfolio Risk Measure: Estimation and Asymptotics of Multivariate Geometric Quantiles,August 2018,Edward W. Sun,Yu-Jen Wang,Min-Teh Yu,Male,Unknown,Unknown,Male,"Managing risk on an aggregate basis has become a requirement for an institution’s optimal decision-making processes. Implementing an integrated approach to risk management, like business portfolio management, requires sustained efforts and allows departments and agencies to develop different approaches based on their business models. Integrated business portfolio risk management is a systematic and proactive process to measure risk from an institution-wide perspective in order to make strategic decisions that contribute to the achievement of an institution’s overall objectives. Multivariate risk measures are necessary in certain applications of portfolio risk management and integrated risk management, or more commonly known as enterprise risk management (ERM). Taking a portfolio view of integrated risk management entails capturing dependence among different risk factors. Risk exposures across a large portfolio that contains heterogeneous assets, e.g., receivables and bonds, exhibit an unknown dependence structure. If a large portfolio own homogeneous assets, e.g., stocks or bonds, then risk exposures illustrate dependence due to the co-movement of the market risk of those assets (see, for example, Sun et al. 2009a, b), but the dynamics of tail dependence over a specific horizon, i.e., time-varying dependence, are still unclear. Portfolio risk management measures the distribution of losses in a portfolio over a fixed horizon, but the dependence among risk factors complicates the computation of the distribution of losses. In order to overcome the challenging complex dependence structure among risk factors, a predetermined dependence function is assumed. The known dependence structure is either from a joint multivariate distribution that assumes invariate dependence over time or a multivariate copula function that assumes time-varying dependence. Another popular application of multivariate risk measures is for the risk management of financial institutions. Basel III not only mandates banks to provide a minimum capital requirement using the VaR approach on an aggregate basis, but also requires certain sub-portfolios or business lines at banks to meet their own VaR capital requirements. Traditional univariate risk measures are insufficient in these types of portfolio risk management. Two key problems in portfolio risk management are (1) measuring the marginal loss distribution for a single asset and (2) measuring the dependence among multiple assets. To measure the marginal loss distribution of a portfolio, we need to consider loss intensity and loss size. Loss intensity is usually addressed by specifying a deterministic hazard rate or a stochastic process for intensity (see, for example, Sun et al. 2009a, b). Since the first problem could be potentially observable in market prices and reflected by the loss distribution, measuring the marginal loss distribution of a single asset is indeed needed, especially in case there is a default that could amount to losses. Therefore, an assumed loss probability should be well defined to measure the potential size of a loss. We could say that to measure portfolio risk, one must assume both the marginal loss distribution and the dependence among risk factors in the portfolio. The challenge of portfolio risk management is to determine both a “correct” marginal loss distribution and a dependence structure. If we can confidently clarify the observation of a loss and its dependence in a portfolio from a specified probability model, then parametric methods—for example, the delta-normal method—usually provide more information. However, they may also lead to a significantly biased conclusion if the “wrong” model is chosen. Non-parametric methods, on the other hand, require fewer assumptions about the data and consequently prove better when the “true” marginal distribution is unknown or cannot be optimally approximated using a known distribution. In other words, the model risk of using a non-parametric approach is lower than that of a parametric approach. Therefore, for portfolio risk management, making as few assumptions as possible is preferred, which typically means employing non-parametric methods. The model-free non-parametric estimation of VaR was proposed by Dowd (2001) based on the sample quantile, which is commonly called the historical VaR. These non-parametric estimators have the advantages of (1) being free of distributional assumptions on the returns, while being able to capture the fat-tail and asymmetric distribution of returns automatically, and (2) imposing weaker assumptions on the dynamics of the return process. A potential limitation of non-parametric methods may be the requirement of a reasonable sample size to ensure good performance. In financial risk management the value at risk (VaR) suggested by the Basel Committee on Banking Supervision is a widely used measure of the risk of loss on a specific portfolio of financial assets. Multivariate value at risk (MVaR) is defined as the quantile set of a multivariate probability distribution, and these models are usually extended from the classical univariate VaR model. For example, Préopa (2012) introduces the definitions and characteristics of MVaR and multivariate conditional value at risk measures. Cousin and Bernardino (2013) extend the classical univariate VaR model to two MVaR measures, lower-orthant MVaR and upper-orthant MVaR, where the former is constructed from level sets of multivariate distribution functions and the latter is constructed from level sets of multivariate survival functions. Chun et al. (2012) consider two ways to estimate risk measures, the value-at-risk and average value-at-risk (AVaR) measures,Footnote 1 for a single asset at given market conditions. One is based on a residual analysis of the standard least-squares (LSR) method, and the other is in the spirit of the M-estimation approach used in robust statistics.Footnote 2
 VaR is an industry standard for measuring downside risk. For a return series, VaR is defined as the high quantile (e.g. 95 or \(99\%\) quantile) of the negative value of the returns. This quantile needs to be estimated. Conventionally, the delta-normal approach with an assumption of “normal” returns can calculate a portfolio’s VaRs, but the returns are frequently not normally distributed (see Boudoukh et al. 1997; Hull and White 1998; Sun et al. 2009a, b). Other approaches such as the multivariate GARCH estimation of VaR also need a distribution assumption (see Girardi and Ergun 2013), but these distribution assumptions may increase the exposure of the model risk. This motivates the employment of a distribution free approach to estimate a portfolio return’s VaR. There are several methods for estimating the quantiles. The most comprehensive breadth of methods is available in the quantile regression and order statistic. Taylor (1999) suggests the quantile regression approach by presenting a procedure to estimate a conditional quantile model that is employed to calculate VaRs for a portfolio over a certain holding period. Chun et al. (2012) propose a method based on a quantile estimation for univariate VaR measures. Chakraborty (2003) investigates the multivariate quantile regression, and Chaudhuri (1996) examines a notion of quantiles based on the geometric configuration of multivariate data. Dowd (2001) proposes the order statistic approach, which is a consistent estimator of the sample quantile estimator from Yoshihara (1995). Falk (1984) and Sheather and Marron (1990) show that the variance of the sample quantile estimator is reduced by kernel smoothing. Chen and Tang (2005) consider the order statistic estimation of VaR and the associated standard error estimation for dependent financial returns. Chen (2008) looks at the concept of the order statistic to estimate the expected shortfall. In this study we extend the method of Chun et al. (2012) and employ two non-parametric estimation procedures, i.e., the multivariate order statistics and multivariate quantile regression approach, to estimate multivariate value-at-risk (MVaR) and multivariate average value-at-risk (MAVaR) with the risk factors. In order to illustrate the performance of the considered MVaR and MAVaR estimators, we perform Monte Carlo simulations with different multivariate distributions. In the empirical study we examine a multi-asset portfolio with different data frequencies to explore the performance of the multivariate risk measures and convergence speed of risk measure estimations. Moreover, to compare the performances of non-parametric and parametric approaches for multivariate VaR models when implemented to forecast the VaR of large portfolios, we conduct a comparison among the alternative models considered not only by using backtesting, but also with the comparative predictive ability (CPA) test. We employ the backtesting tests based on the coverage/independence criteria proposed by Kupiec (1995) and Christoffersen (1998). These tests, though appropriate evaluating the accuracy of a single model, provide an ambiguous decision about which candidate model is better. Therefore, Giacomini and White (2006) propose the comparative predictive ability (CPA) test that uses statistical tests designed to evaluate the comparative predictive performance among candidate models. The remainder of this study is organized as follows. Section 2 presents the linear model and multivariate risk measures. Section 3 shows the multivariate order statistics and multivariate quantile regression method for estimation and the statistic inferences of estimators for MVaR. Section 4 reports the backtesting procedure. Section 5 provides the results of simulation study. Section 6 offers the empirical results of real market data. We conclude in Sect. 7.",6
52.0,2.0,Computational Economics,15 June 2017,https://link.springer.com/article/10.1007/s10614-017-9711-7,Risk Assessment with Wavelet Feature Engineering for High-Frequency Portfolio Trading,August 2018,Yi-Ting Chen,Edward W. Sun,Min-Teh Yu,,Male,Unknown,Mix,,
52.0,2.0,Computational Economics,21 June 2017,https://link.springer.com/article/10.1007/s10614-017-9715-3,Pricing European Options under Fractional Black–Scholes Model with a Weak Payoff Function,August 2018,Farshid Mehrdoust,Ali Reza Najafi,,Male,Male,Unknown,Male,"Fractional Brownian motion (FBM) introduced for the first time by Kolmogorov (1940). The FBM process can be written under the following integral representation proposed by Mandebrot and Van Ness (1968) where \(H \in (0,1)\) is the Hurst parameter, \(\left\{ B_s | s \in \mathbb {R}\right\} \) is a two-sided Brownian motion and is a normalizing constant. For \(H = \frac{1}{2}\), \(B^H\), becomes the classical Brownian motion, while for \(H \ne \frac{1}{2}\), the process exhibits some nice properties. When \(H \in (\frac{1}{2}, 1)\) the increments of FBM have a positive correlation. Therefore, the process has cumulative behavior and describes a system with memory and persistence (Rostek 2009). Rostek and Schobel (2013) studied on the FBM in order to clarify when and why this process is suited for economic and financial modeling . Wang et al. (2001) applied this process in financial models and calculated the value of a European option under fractional version of the Black–Scholes model with Hurst parameter \(H \in (\frac{1}{3}, \frac{1}{2})\). They showed that this model does not have arbitrage and Delta neutral can not hedge the risk of the portfolio. Therefore, they used Delta and Gamma neutral for eliminating risk from a self-financing portfolio which contains European option and stock (Wang et al. 2001). But empirical studies on market data show that logarithmic returns of financial assets have long-range dependence (Chronopoulou and Frederi 2012; Cont 2005). For example, Chronopoulou and Frederi (2012) indicated that there is long memory property in the Bear stearns companies and S&P 500 market data. In order to take into account the anomalous diffusions of the the return of the underling asset when pricing an option, a recent contribution proposes to use a time-changed Brownian motion in the financial models, see e.g., Ballestra et al. (2016), while many researchers adopt the fractional geometric Brownian motion (FGBM), see e.g., Wilmott (1998), Dung (2013), Rostek and Schobel (2013) and Wang et al. (2012), which allows to have the long memory property of the return. Since FBM process with \(H\ne \frac{1}{2}\) is not semimartingale, thus performing this process in financial models is associated with some drawbacks. Some arbitrage opportunities have been already found under many researcher’s study such as Rogers (1997), Salopek (1998), Shiryaev (1998), and Willinger et al. (1999). “Cheridito (2003) analysed the FBM models and indicated how arbitrage can be eliminated from FBM models by restricting the class of trading strategies”. In the reality of the securities market, investors deal with considerable and non ignorable transaction costs. Leland’s hedging strategy suggests that no-arbitrage assumption can be replaced by Delta Hedging strategy under the condition of discrete time occasions and transaction costs. He assumes that the level k of transaction costs are constant and also claim that the terminal value of the portfolio approximates the payoff as the length of a revision interval approaches zero (Leland 1985). Yuri M. Kabanov and Mher M. Safarian showed in general, this strategy does not hold. They have also shown if the level of transaction costs is \(k=k_0 n^{\alpha - 1/2}\) where \(k_0 >0\) and \(\alpha \in {[0,1/2]} \) are constant and n is the number of revisions, then the limiting error of the Leland’s hedging strategy will be removed (Kabanov and Safarian 1997). In this paper, we derive fractional Black–Scholes partial differential equation by constructing a self-financing portfolio and using the Leland’s hedging strategy. Then by using the Mellin transform of a class of weak functions (Rahman 2011; Jones 1980), we obtain a solution of the fractional Black–Scholes equation with a weak payoff function and Hurst parameter, \(H \in [1/2,1)\). We apply this idea, which is described in Company et al. (2006) for solving the fractional Black–Scholes equation. The rest of this paper is organized as follows: In sect. 2, we derive fractional Black–Scholes partial differential equation. In sect. 3, we study and investigate the generalized functions and the Mellin transformation method. Section 4 proposes an explicit solution for fractional Black–Scholes partial differential equation. Finally, by considering this solution, we provide the value of a European call option under different parameters.",12
52.0,3.0,Computational Economics,04 July 2018,https://link.springer.com/article/10.1007/s10614-018-9831-8,Introduction: Special Issue on Evolutionary Dynamics and Agent-Based Modeling in Economics,October 2018,Herbert Dawid,Andreas Pyka,,Male,Male,Unknown,Male,,1
52.0,3.0,Computational Economics,28 November 2016,https://link.springer.com/article/10.1007/s10614-016-9638-4,Estimation of Sentiment Effects in Financial Markets: A Simulated Method of Moments Approach,October 2018,Zhenxi Chen,Thomas Lux,,Unknown,Male,Unknown,Male,"By taking into account features such as the existence of heterogeneous agents with different trading strategies, bounded rationality or interactions among agents, behaviorally motivated models of financial markets have undergone a burgeoning development over the past two decades. Quite a number of these models are able to replicate and, therefore, explain the documented stylized facts of financial markets, including fat tails and temporal dependence of volatility. Recent surveys of this literature can be found in Hommes (2006), LeBaron (2006) and Lux (2009b) among others. This literature got started by Day and Huang (1990) who model a market populated by fundamentalists and chartists to study randomly alternating bullish and bearish market episodes. Kirman (1993), De Grauwe et al. (1995) and Lux (1995) add further aspects of traders’ interactions such as herd behavior and switching of strategies and already make first attempts to explain selected stylized facts. Brock and Hommes (1998) initiated a related strand of literature based upon a discrete choice framework for agents’ choice of strategies. Chiarella and He (2002) additionally allow agents to have different risk attitudes within such a setting. Most of this literature uses simulations to explain some of the stylized facts based on complex nonlinear models. Alfarano and Lux (2007) and Alfarano et al. (2008) are among the few exceptions. They derive analytical solutions for the time-variation of higher moments and related measures of the stylized facts, enabling them to determine the conditions under which these particular features arise. Alfarano and Lux (2007) derive closed form solutions of variance and kurtosis of the return distribution of their model based upon the unconditional distribution of an index of the average expectations of their boundedly rational agents. In their setting, expectation formation is formalized via Kirman’s (1993) seminal “ant” model for the herding interactions among agents. Alfarano et al. (2008) further incorporate autonomous changes of sentiment into the dynamics of agents’ interactions, in addition to the herding mechanism. They also derive approximate closed form solutions of autocorrelation functions. In a companion paper to ours, Ghonghadze and Lux (2016) derive higher-order approximations for the same moments and explore their applicability in a generalized method of moments (GMM) setting. This paper uses the model of Alfarano et al. (2008) to explore the issue of efficiency of estimation of such a model via the simulated method of moments (SMM) approach. The robustness of the theoretical models in generating empirical “stylized facts” inspires the empirical application and validation of agent based models. Since much of this literature is based on simulation, the use of simulated moments seems tailor-made for bringing these models to the data. Simulated method of moments has been proposed initially by McFadden (1989) , Pakes and Pollard (1989), Lee and Ingram (1991) and Duffie and Singleton (1993). SMM or closely related approaches have been applied in a variety of settings. Molina et al. (2005) use SMM to estimate behavioral parameters in a model of a transportation network. Rahmandad and Sabounchi (2012) apply SMM to investigate population obesity dynamics. Ruge-Murcia (2007) estimates a dynamic stochastic general equilibrium model (DSGE) using SMM. Grammig and Schaub (2014) combine GMM and SMM in estimating an asset pricing model, combined with a DSGE model of the underlying fundamental dynamics. In a context closely related to ours, Gilli and Winker (2003) already developed a nonlinear optimization technique combining the Nelder–Mead and a threshold acceptance algorithm to estimate Kirman’s “ant” model. Winker et al. (2007) propose a criterion function based on moments related to the stylized facts to assess the empirical application of agent based models. Franke (2009) applies such an objective function within an SMM framework to estimate the model of Manzan and Westerhoff (2005). Franke and Westerhoff (2011) also adopt SMM to estimate a structural stochastic volatility model, but instead of the standard SMM setting of Duffie and Singleton (1993) use a bootstrap method for generating the weighting matrix of their moments. Franke and Westerhoff (2012, 2016) continue this line of research by estimating different “reduced” types of simple agent-based models and comparing their capability to explain the stylized facts. Grazzini and Richiardi (2015) illustrate the application of SMM to an agent-based framework via Monte Carlo simulations for a simple model of price formation in a double auction. Jang (2015) uses SMM to estimate the parameters of the model of Alfarano and Lux (2007). He reports a variety of hurdles in the SMM approach such as a rugged and possibly very flat surface of the objective function and difficulty to obtain unique parameter estimates from different initial conditions. Similar problems are also highlighted in a different context by Grammig and Schaub (2014) who point to principal limitations of an SMM approach in the presence of small datasets used to estimate complex theoretical models. Alternative minimum distance estimation different from GMM/SMM have been proposed by Lamperti (2015) and Barde (2016). Both approaches are based on information-theoretic measures. Barde (2016) is particularly close to the present context in that he compares three agent-based models of herding in financial markets that are close in spirit to our prototype model. While he does not estimate the parameters of these models directly, he compares the performance of a large set of parametric specifications of these models and determines the model confidence set (models that cannot be significantly outperformed by others) among the agent-based and a variety of GARCH time series models. As it turns out, among the three agent-based models, the one by Franke and Westerhoff (2016) most often appears in model confidence sets for different data while the competitor proposed by Alfarano et al. (2005)—a close cousin of the model used here—is a close second. Most of the other applications, however, confine themselves to estimate the parameters of agent based models, without systematic exploration of the performance of their estimation algorithms. While one finds a variety of simulation studies on all types of models and estimation methods in the econometrics literature, not too much evidence on the performance of SMM estimation is available. One obvious reason is certainly the computational effort involved in executing such a study that amounts to assessing the performance of simulated moments via simulations. We here attempt to close this particular gap in the literature by more systematically exploring the performance of SMM estimation via Monte Carlo simulations using the prototype simple agent-based model of Alfarano et al. (2008) for an exemplary case study. The model allows for two different scenarios: a bimodal and unimodal distribution of the sentiment index. It is shown that strong interpersonal communication corresponding to the bimodal distribution typically matches best the empirical data. As this model matches the most prevalent stylized facts, it should be observationally equivalent to a number of alternative specifications. As an added advantage and in contrast to many related models, analytical moment conditions (at least approximate ones) exist for this model so that we can compare the performance of SMM to a generalized method of moment (GMM) estimator developed in a companion paper (Ghonghadze and Lux 2016). The availability of GMM results provides the opportunity to compare the performance of SMM and GMM for the very same setting, and, therefore, allows an assessment of the relevance of the additional noise induced by simulations. As we will see, this comparison turns indeed out to reveal a relatively slow convergence of the SMM estimates to their GMM counterparts even for relatively sizable simulated samples. Another contribution of this paper is the development of a systematic approach to handle estimation problems with multiple minima and discontinuous gradients of the objective function. It is found that performing a systematic “mapping” of the objective function via an extensive grid search is a very useful first step for determination of sensible initial conditions to start an optimization algorithm for local fine-tuning of parameter estimates. As concerns the details of the SMM estimator, we find that many other variations of our setting (using different moments, adopting different weighting matrices) have very little influence on the quality of our estimates. We also find that the variability of our estimates decreases more slowly with empirical sample size than expected, and that the estimation errors are not very sensitive to the simulated sample size over a broad range of feasible choices. The rest of the paper is organized as follows. Section 2 introduces the theoretical model that basically boils down to a diffusion process of an asset price that combines fundamental factors and sentiment dynamics. Section 3 develops the methodology. After illustrating the surface properties of the objective function, we conduct Monte Carlo simulations to compare the performance of different sets of moments and other variations of the SMM designs. Section 4 applies the developed methodology to estimate the parameters of this behavioral model using a broad selection of empirical high-frequency data. Lastly, Sect. 5 concludes the paper.",37
52.0,3.0,Computational Economics,27 November 2017,https://link.springer.com/article/10.1007/s10614-017-9775-4,Modeling Firm and Market Dynamics: A Flexible Model Reproducing Existing Stylized Facts on Firm Growth,October 2018,Thomas Brenner,Matthias Duschl,,Male,Male,Unknown,Male,"The study of firm growth has a long tradition (e.g. Gibrat 1931). As a consequence, in the last couple of decades a number of robust facts about the statistical properties of firm size and growth have been established (see Coad 2009 for an overview on empirical studies). The literature has also seen many attempts to model firm growth, ranging from Gibrat’s law (Gibrat 1931) to complex Agent-Based models (e.g., Delli Gatti et al. 2005; Metzig and Gordon 2014). The more recent models are usually validated by a comparison of the implications with well-known stylized facts. However, a rather common limitation is that the approaches focus on the attempt to develop a model that is able to reproduce one or a few stylized facts. In addition, the existing approaches intend to identify one model that fits the economy. We extend this literature by two crucial aspects. First, a model is developed that shows firm dynamics in line with all well-known stylized facts in this context. In total eight stylized facts are used, namely the negative dependence of average growth on size and age, the positive size-dependence of the auto-correlation coefficient, the smaller auto-correlation coefficient at the tails of the growth rate distribution, the variance-scaling relationship, the fat-tailed growth rate distribution and the right-skewedness of size and age distributions (details are given in Sect. 2.1). Second, we develop a model with many realistic model specifications that allow to pick the adequate specification for different industries. To this end, the model builds on existing models and theoretical arguments and combines them in a new way. After the original assumption of random firm (cf. Gibrat’s law; Gibrat 1931), Penrose (1959) proposed learning and the development of competences in firms as main mechanisms. Nevertheless, only recently explicitly modeling firm dynamics became a frequent topic in the scientific literature. The approaches used differ strongly. There are many approaches that assume random processes within a hierarchical firm structure (Amaral et al. 1997; Fu et al. 2005), in order to explain the random but not normally distributed growth processes. Schwarzkopf et al. (2010) extended this approach by assuming a replication distribution for the replacement of subunits. In order to avoid any assumptions on the internal structure of firms, Bottazzi and Secchi (2006a) model the growth rate distribution by drawing on Penrose’s (1959) idea of competencies and learning and by explicitly considering the market dimension. Recently, more complex Agent-Based models have been developed, which try to include knowledge on economic processes and mechanisms in their assumptions. While Delli Gatti et al. (2005) model explicitly the demand and supply on the credit market and the financial fragility of firms, Metzig and Gordon (2014) match employees to firms, which are required to produce goods and to generate profit. Here, firms compete both for aggregate demand and workforce. While these models are able to reproduce a larger number of stylized facts, they are quite specific, focusing on certain aspects. Furthermore, there are also more complex models of firm-internal processes that, however, do not focus on the explanation of the statistical properties of firm growth but on the explanation of other characteristics, such as firm size distributions (e.g. Luttmer 2011; Caliendo and Rossi-Hansberg 2012). These models provide a good starting point for the development of our model. Conceptually, we build on the model of monopolistic competition by Dixit and Stiglitz (1977). As a consequence, we argue that firm dynamics can only be adequately modeled in connection with modeling market dynamics. This leads to a relationship with models that go beyond firm dynamics (e.g. Luttmer 2011; Caliendo and Rossi-Hansberg 2012). As a consequence, the proposed model represents market dynamics, including the emergence and disappearance of sub-markets, as well as firm dynamics, including innovation processes and firm foundation. To validate the model, we apply the approach proposed by Brenner and Werker (2007), which argues that theoretical and empirical knowledge should be used for the validation of models on the level of model building as well as on the level of model predictions and that theoretical and empirical knowledge only allow to identify a bunch of possible parameter sets (details are given in Sect. 2.2). We show that the model is able to reproduce all well-known stylized facts on firm growth and their relation to firm size and age. In line with Brenner and Werker (2007) we argue that the validation of the model does not allow to identify one real model but only allows to eliminate all unrealistic models. Hence, we build a meta-model and identify a bunch of parameter sets that lead to realistic behavior of this meta-model. The different parameter sets, which meanings in this context are discussed, are able to represent different market situations, such as different industries. The paper proceeds as follows. In the next section the empirical knowledge about firm growth and the calibration approach are described. Section 3 provides an overview on the existing firm models as well as on the simulation model developed in this paper. Section 4 contains the calibration of the model. The identified realistic parameter sets are presented, analyzed and discussed in Sect. 5. Section 6 concludes.",2
52.0,3.0,Computational Economics,13 December 2017,https://link.springer.com/article/10.1007/s10614-017-9776-3,The Role of Network Topology and the Spatial Distribution and Structure of Knowledge in Regional Innovation Policy: A Calibrated Agent-Based Model Study,October 2018,Ben Vermeulen,Andreas Pyka,,Male,Male,Unknown,Male,"In the knowledge-based perspective on innovation economics literature, there is an ongoing debate on whether technological specialization or rather diversification of research and development activities is conducive to regional innovativeness (Glaeser et al. 1992; Panne 2004; Paci and Usai 2000; Feldman and Audretsch 1999), or, as an additional alternative, whether regions should contain a mix of technologically related research activities (cf. Balland et al. 2015; Bristow 2010; Frenken et al. 2007). Taking technological discoveries as due to finding a suitable combination of existing knowledge (cf. Arthur 2009), firms in diversified regions face many unrelated knowledge flows and ‘infeasible’ combinations, thus lowering dynamic efficiency, while, in contrast, firms in specialized regions may be dynamically efficient but may not have access to technological path-breaking combinations and thus face a lock-in (Menzel and Fornahl 2010; Saxenian 1996; Hassink 2005; Martin and Sunley 2011). Arguably, this body of literature may overemphasize the regional character of knowledge flows. While actual co-location (and tighter governance) adds to the efficiency of product definition, knowledge recombination, and diffusion (Maskell and Malmberg 1999; Pinch et al. 2003), organizational proximity of innovation activities compensates for geographical distance (Capaldo and Petruzzelli 2014). As such, arguably, to enhance regional innovativeness, a social plannerFootnote 1 should not only look at the composition of the regional knowledge, but also at the (possibly interregional) topology of innovation networks and notably whether there is collaboration with agents active in different technological fields (cf. Grant and Baden-Fuller 1995, 2004; Pyka and Küppers 2002; Pyka 2002). In this paper, the contention is that the regional specialization versus diversification versus related variety debate (and the related purely regional innovation policy paradigms) cannot be settled without taking the (inter)regional innovation network into the consideration. Regional innovation systems feature a mix of public and private, intra- and inter-industry, and local and global knowledge flows (Autant-Bernard et al. 2013; Camagni and Capello 2013). Following these insights, social planners were confronted with new regional innovation policy paradigms. In this paper, we focus on three prominent ones. Firstly, the ‘smart innovation’ policy paradigm (Foray 2014; Foray et al. 2009, 2011) argues that a social planner, such as the European Commission, best prioritizes technological development in regions in particular technological fields, building upon existing strengths, and assigns other regions ‘co-innovator’ roles with specializations in other fields. In following the smart specialization paradigm, the social planner thus deliberately creates a patchwork of complementary, specialized regions connected through an interregional network. This network is crucial as, under fully fledged regional specialization, realizing path-breaking inventions requires access to alien technological knowledge that may be located in other regions (Bathelt et al. 2004; Rosenkopf and Almeida 2003). Secondly, the ‘branching’ policy paradigm (Asheim et al. 2011; Boschma 2011) [and related ‘regional resilience’ paradigm (cf. Balland et al. 2015; Bristow 2010)] argues that regions may pick particular mixes of technological fields that can continue to branch in a self-sustained manner, independent of knowledge from outside the region. Thirdly, in the ‘regional gatekeeper’ literature, a more specific network form is prescribed. Hereby innovation networks are regionally dense, and there are regionally central gatekeepers which monitor and filter technological knowledge in other regions to then diffuse that knowledge within their own region (Graf 2011; Graf and Krüger 2011; Spencer 2003; Breschi and Lenzi 2015; Bathelt et al. 2004).Footnote 2
 The research question now is: given a particular regional distribution and interrelationship of technological knowledge, notably of specialized fields, which network topology should a social planner pick to enhance technological progress in these regions, on average? In fact, we are able to prescribe the combination of knowledge distribution and network topology which a social planner should pick. To transcend the ongoing qualitative arguments and conflicting empirical findings, we define and use an agent-based model to experimentally investigate the relationship of specialization/related variety/diversification and intra-/inter-regional innovation network topologies (and notably the smart and gatekeeper network) in conjunction. Given that such an agent-based model provides full control over the topology of the network, we study combinations of spatial knowledge distribution and (inter)regional innovation networks which are not discussed in literature. Moreover, the agent-based model also allows us to study the number of relationships of each of the agents on technological progress. Ultimately, the agent-based model enables us to get insights in mechanisms at work on the complex interplay of spatial distribution, technological interrelationships, and network topology, and insights in the performance of policy paradigms followed by social planners. In addition to addressing the topical debate on which regional network innovation policy is to be preferred, we extend the debate. Our claim is that the significance of collaboration with firms in other fields and in other regions depends on structural features of the underlying technological knowledge and the distribution of knowledge over regions. In preceding simulation studies, we revealed how collaboration of agents in different specialized regions is predominantly significant if technological knowledge accumulation is ‘progressive’ (i.e. not building upon own ancestors, which would be ‘conservative’), which is the case for some technological fields more than for others (see Vermeulen and Pyka 2014b, a). In these studies, however, the technological structure is highly simplified such that conclusions have limited external validity. In the study at hand, the structural features of the technological knowledge searched by the agents in the model is empirically calibrated to citation and classification statistics of patents in the OECD patent database. In the spatial agent-based model presented here, the world consists of sea and land regions where a (controlled and fixed) number of firm agents resides in each land region. Each agent is specialized in one technological field, is engaged in knowledge discovery, and has a number of relationships with agents with whom it can (but need not) collaborate in knowledge discovery. The specializations of the agents in each region as well as the topology of the innovation network are now independent variables and the absolute ‘amount’ and ‘advancedness’ of knowledge discovered are studied. We see this paper and its model as the first step in a research line in which various regional innovation network policies are studied for realistic knowledge structures while controlling innovation system properties. The structure of this paper is as follows. In the following section, we provide an overview of the literature on (inter)regional innovation policy paradigms and notably their preoccupation with knowledge specialization/diversity and with the role of (inter)regional innovation network relationships. As our principal claim in previous work already was that the structure of the knowledge determines which (inter)regional innovation network structure is most efficient in technology development, we subsequently discuss existing ‘technology discovery models’.Footnote 3 We then specify our own ‘technological knowledge graph’ model, how it is generated and calibrated to the OECD patent database, followed by an operational definition of the spatial agent-based simulation model and the search heuristics followed by agents in searching and unlocking new parts of that knowledge graph. This is followed by a section with an overview and discussion of simulation results. The last section provides conclusions.",18
52.0,3.0,Computational Economics,30 June 2017,https://link.springer.com/article/10.1007/s10614-017-9706-4,Network Externalities and Compatibility Among Standards: A Replicator Dynamics and Simulation Analysis,October 2018,Torsten Heinrich,,,Male,Unknown,Unknown,Male,"When users-both corporate and private-consider employing a new technology, say Voice-over-IP telephony (VoIP), their choice between different available standards or products implementing this technology may be severely limited. While many standardsFootnote 1 may be available in theory, practical usability depends on which standard predominates the users’ direct environment, which ones are used by their business partners etc. This effect, network externalities, has extensively been investigated (David 1985; Katz and Shapiro 1985; Arthur et al. 1987). However, there is a second constraint, introduced by compatibility to other standards already employed by the respective users; they may for instance need to consider, if the desired VoIP software works well with the used operating system, office software, computer and network hardware, etc. Network externalities will then develop not only within but also across segments. Vendors of standards have been known to use this to their advantage: Microsoft’s breakthrough famously came with agreements to couple their software with IBM hardware. Another well-known case linked to the company is the bundling of its operating system Windows with its web browser Internet Explorer. Today, many large companies in the ICT (information and communication technology) sector maintain extensive portfolios of partly bundled or integrated products. While the phenomenon is by no means limited to the ICT sector, the strength of network externalities in ICT makes examples in this sector both more numerous and more obvious.Footnote 2
 Obvious strategies to gain an advantage for a competing standard in such a setting include increasing the compatibility with major competitors in other segments,Footnote 3 introducing spin-off standards in other segments, and reducing compatibility with weaker competitor’s products in order to drive them out of business. Can strategic exploitation of network effects of this type be demonstrated in a simple evolutionary model?Footnote 4 Can it be demonstrated for (1) the initial usage share in either segment (putting incumbants at an advantage) (2) the compatibility between standards across segments (3) the positioning of initial adopters? Is there a point beyond which reduction of compatibility is desirable for a competitor? Is it wise to expand into another segment to control the standard in that segment directly—given the capacity to do so? The present contribution offers a replicator dynamic model of standard competition with cross-segment ties. A standard replicator equation is used in which the compatibility with and the usage shares of standards in one or several other segments take the role of the evolutionary fitness. The replicator model yields a first-order dynamic system that allows to investigate the impact of initial conditions and of compatibility terms. These would be what governs the strategic actions of major competitors in sectors with densely interconnected standards such as the information and communication technology. Direct interaction between agents on the micro-level sometimes leads to the emergence of non-trivial macro-level dynamics. Therefore, it is necessary to show that benchmark models operating at an aggregated level will still work if a micro-layer of massive numbers of interacting agents is included. For this, an agent-based version of the model is added. The deterministic dynamics resulting from the (aggregated level) replicator model are replaced by transition probabilities between user groups of standards.Footnote 5 For the trivial network structure of a complete graph, this results in a stochastic dynamic system which is equivalent to the macro-level model in its behavior while for other network structures, the probabilities change locally, i.e. between agents, depending on their neighborhood. Section 2 gives a brief literature overview before the model is discussed in Sect. 3. Section 4 discusses simulations and results. These analyses are contrasted with some evidence of strategic use of standard tying in the ICT (information and communication technology) sector in Sects. 5, 6 concludes.",3
52.0,3.0,Computational Economics,30 June 2017,https://link.springer.com/article/10.1007/s10614-017-9712-6,The Impact of Credit Rating on Innovation in a Two-Sector Evolutionary Model,October 2018,Pascal Aßmuth,,,Male,Unknown,Unknown,Male,"Firms suffer from discrimination in access to credit (Canepa and Stoneman 2008; Hao and Jaffe 1993; Giudici and Pateari 2000). And the pattern of constraints can reinforce disturbances in the selection process. With an efficient selection mechanism in place the less profitable and less innovative firms are expected to grow at a comparatively low rate or to even exit the market. Empirical results show however, that an efficient selection may not take place. For instance, an Italian sample shows that even less profitable firms can sustain when there are financial constraints (Bottazzi et al. 2006, 2014, 2010). Those constraints hinder an efficient selection mechanism. One possibility of facing them is to offer public subsidies. Those can not only help the most those firms in dire need of external funding (Hyytinen and Toivanen 2003) but also increase the trust in creditworthiness by others and thus may enable external financing in the future (Takalo and Tanayama 2010). The aim of the paper is to explore more the connection between credit constraints and economic dynamics taking into account firms that pursue innovation. In particular, we examine the role of lender behaviour in the evolution of a diversified economy. Therefore, we focus on a rating process in an evolutionary setting which determines the credit supply for each individual firm of two different sectors. More precisely, we examine the role of weights put on various pieces of information from the balance sheet that are used for rating the firms, for instance cash flow or market share. Furthermore, we examine the impact of different ways of expectation formation by the bank about the prospects of each of the sectors. Under the rationale that banks use routines for their credit decisions based on balance sheet information, we ask what is the impact of the bank routines for credit supply on innovation and technology diffusion? The focus is on gaining first some insight about the dynamic of the bank decision itself in response to risk measured by balance sheet figures. Furthermore, credit rationing then may impact the selection dynamics. In an economy, firms do not only compete for credit with their peers within their own sector, but also with firms from different industries. Thus, industry specific information is also crucial, first of all due to the competitive pressure leading to firm exits which which come along with credit defaults. Hence, we consider the following more detailed questions: to what extent does the bank policy determine whether one of the two sectors benefits from higher credit supply? And which effect does the funding of the more innovative sector have on the low-tech sector? We use a two-sector approach in order to distinguish for non firm-based features and therefore bundle firms in an innovative and in an less innovative sector. We call the more innovative sector ’high-tech’ and the less innovative one ’low-tech’. Banks that use internal rating/ scoring systems evaluate the creditworthiness of a firm in comparison to its rivals. Therefore, even if a firm does better than all firms in another sector, it might still be seen as an under-performing entity. The particular importance that the bank puts on sector-specific and economy-wide indicators in measuring creditworthiness may lead to different economic dynamics. The model is set in a dynamic agent based framework building on the work of Nelson and Winter (1982) and Winter (1984) which is designed in an evolutionary environment under Schumpeterian competition. The methodological choice is based on the ability of agent-based models to employ complex interactions for heterogeneous agents which lead to results that match empirically shown stylized features (Dawid 2006). A particular strength of the approach is that it can produce ’replicator dynamics’ that mimic the selection process that can produce economy-wide observed stylized facts, for instance the firm’s current financial performance determines investment behaviour and market exit (Coad 2010). In each period the bank uses information about the firms in order to determine how much credit it would be willing to lend to each of them. It has a particular routine of doing so by relying on the assessment of creditworthiness based on multiple features like collateral, recent profitability and the sector a firm operates in. There are two sectors and each firm produces and conducts R&D to some extent. The outcome is some quantity of a homogenous good in each sector which jointly with all other firms in the sector determines the market price. Also the profit of each firm is determined automatically on the one hand. On the other hand the R&D effort might lead to the finding of a better technology which the successful firm can use in the production process of the next period. This leads the successful firm to wanting to invest more for the next round of production. However, this possibility depends on the current profit and available credit. If the firm can exploit that technological improvement it will have better access to credit in the next period. There are two feedback effects employed in the model: a better technology improves access to credit and access to credit improves the probability of finding a better technology. Both effects are however, subject to individual behaviour of both, the bank and the firms. Thus, the effectiveness of the feedback varies in the behavioural routine of the agents. The bank uses information that applies in comparison to all other firms and it uses pieces of information that are sector-specific like market share. Furthermore, the bank needs to form expectations if it wants to assess the prospects of the respective industry. Results indicate that the more the bank supplies credit based on information distinguished by sector the less innovation and output growth will take place. The reason is that innovative firms cannot benefit to the full extent from their superior productivity. That is, their success is recognized only with respect of their sector and not economy-wide and therefore they cannot gain as much from their advantage. Furthermore, the way that the bank forms expectations about productivity only matters when all other pieces of information for the rating decision are taken into account in a rather marginal way. If they are of some importance it almost does not matter how the bank asserts the future productivity because the magnitude is too low. Predominantly, expectations are overshadowed by information about current cash flow and survival ratios in an industry. Nevertheless, the expectations have an impact on the shifting of credit between the sectors. The positive impact of higher average productivity growth however, does lead to a shift of funds to the other sector. The reason is that more trend-following expectations also exhibit more short-term variation as they follow the current ,and ever-changing trend. This trend is more short-lived and works in both directions as occasionally high growth will hardly be sustained in the next period. If the expectations focus more on the long-term average occasionally high growth has a more persistent impact. Therefore, under conservative expectations, the bank shifts more funds to the more successful sector in the long run. This paper contributes to explaining empirical findings about financial constraints and innovation by assessing the role of bank routines on financing constraints and heterogeneity found in constraints. It adds to several strands of literature by using Schumpeterian Competition in an evolutionary agent-based setup and highlighting the role of credit rationing for economic growth, especially via financial fragility as selection mechanism. By modeling bank routines in an evolutionary framework this paper provides one possible explanation of observed phenomena, for example that highly innovative firms being even more restricted which is not expected if the selection process was efficient. The model also builds upon literature about risk management and R&D behaviour and on the theoretical approach of innovation embedded in an evolutionary context. The remainder of the paper is organized as follows: Sect. 2 provides literature and background, Sect. 3 introduces the model and Sect. 4 establishes a baseline case. In Sect. 5 the impact of the rating procedure on the sectoral evolution is examined. In Sect. 6 we discuss the role of expectations on the bank’s decision and the possible impact in this framework. Section 7 concludes.",1
52.0,3.0,Computational Economics,29 June 2017,https://link.springer.com/article/10.1007/s10614-017-9714-4,The Limits to Credit Growth: Mitigation Policies and Macroprudential Regulations to Foster Macrofinancial Stability and Sustainable Debt,October 2018,Sander van der Hoog,,,Male,Unknown,Unknown,Male,"In many advanced economies small- and medium-sized enterprises (SMEs) account for most of the employment in the economy.Footnote 1 Banks and other financial institutions therefore have an important social role to play, to create credit for productive investments, especially in order to support economic development and productive activities by such small businesses. In the credit view of economic development (Schumpeter 1934, Ch. 3), Schumpeter makes an important distinction between normal credit and abnormal credit. Normal credit is the primary wave of credit given to the entrepreneur to make investments and to finance innovation. Abnormal credit consists of a secondary wave of credit that finances consumption and speculation into financial assets for which no profits have yet been realized (cf. Bezemer 2014). The notion of productive credit is also a core argument in favour of Community Development Banks in Minsky (1992). The authors argue along similar lines as Schumpeter, namely that credit should support the local enterprises and communities. These are often small-scale businesses that do not require large loans in the order of millions, but rather small loans in the order of thousands of dollars or euros. The distinction between productive and unproductive credit played an important role in the Japanese Banking Crisis (Werner 2003, 2005), for which we provide a short overview in Sect. 2.2 below. Investments in unproductive activities could also serve as explanation for the economic stagnation that is hampering economic growth in both the EU and China. The main theme of this paper can be summarized as “Schumpeter meeting Minsky”, namely to study the key role played by productive versus unproductive finance for investments, and how this affects the financial stability of the macroeconomic system as a whole (see also Bank of England 2016). We consider various proposals to improve the financial stability of the banking system, with a particular focus on mitigation policies and macroprudential regulations that aim to reduce the large debt overhang and the overall economic costs that result from severe downturns. We test the efficacy of such proposals by computational experiments, using a stock-flow consistent agent-based model (SFC-ABM). The model used here was already applied successfully to similar issues related to microprudential regulation in a previous analysis (van der Hoog and Dawid 2017). This follow-up paper could therefore be seen as an extension of those earlier results, but with a stronger focus on macroprudential regulations. We consider a financially fragile economy with a high degree of financialization in which (non-financial) firms need loans from commercial banks to service their various financial commitments: to produce a final consumption good, to service their outstanding debt, and to make long-term investments. Along the business cycle the economy follows the dynamics of the Minsky base cycle in which firms traverse the various stages of financial fragility, i.e. hedge, speculative and Ponzi finance (cf., Minsky 1978, 1986, 1992).Footnote 2 In the hedge finance stage firms can repay their credit for working capital (’Betriebskredit’), including interest payments, from the current cash flow. In the speculative finance stage the cash flow is sufficient to pay for the debt principle but not for the interest payments and banks are willing to provide roll-over credits in order to prevent their outstanding loans from turning into non-performing loans. In the Ponzi finance stage neither principle nor interest payments can be repaid but the banks are still willing to keep firms alive through “extend and pretend” loans, also known as zombie-lending (Caballero et al. 2008). This lending behavior may cause credit bubbles with increasing leverage ratios. Empirical evidence suggests that recessions following such leveraging booms are typically more severe and can be associated to higher economic costs than other recessions (Jordà et al. 2013; Schularick and Taylor 2012). In the companion paper we primarily focussed on microprudential regulations, by investigating the micro-level dynamics. We showed that a strengthening of the capital adequacy ratio does not necessarily improve the financial stability of the banking system. In fact, more regulatory capital may lead to an increase in the amplitude of the most severe downturns, while a strengthening of the central bank reserve requirement (more liquidity) leads to a decrease in the amplitude of recessions. This suggests there exists a U-shaped relationship between the strength of the capital adequacy ratio and the amplitude of recessions, implying that an optimal value at which the amplitude can be minimized is either a very low or very high capital ratio, but not somewhere in the middle. This goes against the common wisdom in the debate about how to reform credit market regulations, in which the main proposals (i.e., Basel III) are for the most part geared towards strengthening the capital requirements. The reason for this somewhat counter-intuitive result is the absence of strict regulations on the volume of credit, which leads to repeated credit bubbles with each new bubble growing larger than the preceding one. Therefore, a generic result of our analysis seems to be that a more restrictive regulation on the supply of liquidity to firms that are already highly leveraged is a necessary requirement for preventing credit bubbles from occurring again and again. As a counterpoint to those microprudential results, the current paper offers a macroprudential perspective. We study policy measures that might mitigate the severity and intensity of the economic losses that ensue from such severe downturns. We investigate macroprudential regulations that are aimed at: (i) the prevention and mitigation of credit bubbles, (ii) ensuring macro-financial stability, and (iii) limiting the ability of banks to create unsustainable debt. We investigate several scenarios varying from banning banks to pay dividends (strengthening their core equity), using non-risk-weighted capital ratios, or cutting off funds to the speculative and Ponzi financed firms. Our results show that the first measure has little effect, while the third has strong positive effects. Cutting off funding to the financially unsound firms results in a strong reduction in the amplitude of recessions and reduces the cumulative economic losses significantly. However, combinations of policy measures do not necessarily increase the effect, pointing to non-linear interactions between the policies. The contributions of this paper are the following. In Sect. 2 we review the relevant literature and list the most important empirical stylized facts about the relationship between financial leverage and the business cycle. In Sect. 3 we set up our methodology such that we can analyse the synthetic data generated by the simulation model using similar methods as those used for empirical data. In Sect. 4 we describe the agent-based computational model. In Sect. 5 we consider macroprudential policies such as credit growth limits and loan eligibility criteria, full reserve banking and full equity funding. Finally, Sect. 6 concludes.",9
52.0,3.0,Computational Economics,12 September 2017,https://link.springer.com/article/10.1007/s10614-017-9740-2,Evolutionary Climate-Change Modelling: A Multi-Agent Climate-Economic Model,October 2018,Sylvie Geisendorf,,,Female,Unknown,Unknown,Female,"The paper proposes an updated version of the thus far only global but concise climate-economic model with an agent-based approach, namely the “battle of perspectives” by Janssen (1996, described in a shorter form in Janssen and de Vries 1998). In climate-economic modelling, agent-based models are still an exception, and although numerous authors have discussed the usefulness of the approach (Patt and Siebenhüner 2005; Van den Bergh 2007; Balbi and Giupponi 2010; An 2012; Gsottbauer and van den Bergh 2013; Miller and Morisette 2014), only a few models actually exist. Furthermore, most of them deal with rather specific questions, like energy consumption in houses in the UK (Natarajan et al. 2011), and often with issues related to land use and farming (Ziervogel et al. 2005; Entwisle et al. 2008; Aurbacher et al. 2013). In a literature review on agent-based modelling of socio-climate systems (Balbi and Giupponi 2010) only found one model combining an evolutionary agent’s perspective with a global climate-economic model. This “battle of perspectives” model, by Janssen and de Vries (1998), addresses the influence of agents’ beliefs regarding climate protection, and thus on climate change and global economic development. In other words, if many agents do not believe in the importance of climate protection or have other more pressing goals, such as economic growth, they will not invest in protection measures. An interesting feature of the Janssen model is that it allows for analysing the consequences of right or wrong perceptions. Climate sceptics are successful when placed in a world which is as robust as the one in which they believe. However, it also becomes visible how badly they do when placed in a vulnerable environment. Additionally, the model is evolutionary because it allows for analysing how the world population learns from each other when they realise their world view does not correspond to observed data. Thus, the model also allows for studying the effect of belated adaptation. The approach of the paper is twofold. First, the reimplementation of the model, based on the published equations by Janssen (1996) and Janssen and de Vries (1998), follows the “model to model” methodology. The procedure has been established in a series of workshops and papers, starting in 2003 (Edmonds and Hales 2003; Rouchier et al. 2008). Its advocates argue that replicating a model is a useful way to check its accuracy and robustness. Second, updating a model with new scientific evidence from climate research as well as current data on economic development is a robustness check in itself. The long-term validity and usefulness of a model depends on the variability of the data on which it is based as well as on the model’s sensitivity to data changes. If scientific evidence changes a lot over time in the domain the model covers, and if the model’s behaviour changes significantly due to new theories or evidence, results will have to be treated with a lot of caution. The same holds for a significant deviation in the model’s behaviour due to a data update. The model’s usefulness would be questionable if it depended that much on its exact calibration. If, on the other hand, new data or the model’s sensitivity do not lead to substantial changes in the general behaviour of the model, it demonstrates dynamic robustness. In particular, for climate-economic models running over decades and up to 100 years or more, such an intertemporal robustness check could be an important criterion for their long-term legitimacy. By replicating and updating the “battle of perspectives” this paper thus performs this intertemporal robustness check, and at the same time it provides an up to date version of one of the few evolutionary agent-based models in climate-economics with an overall approach. The paper is organised as follows. Section 2 discusses the problems with traditional climate economic models, and argues for the necessity to develop agent-based models. It presents the discussion around agent-based climate modelling as well as the few currently existing models. Section 3 provides an outline of the climate-economic multi-agent model by Janssen (1996), while Sect. 4 updates the input data on economic development, population and ranges of expectations for climate sensitivity and damage costs. Section 5 presents results from the original model and compares them with the update. As the main intention of the paper is remodelling and comparison, which is considered a robustness check for the model, the Janssen model is kept in its original state, except for new data. The paper concludes with Sect. 6, which comments on the results of the model’s reimplementation, discusses the strengths and limitations of the model’s assumptions and outlines possibilities for future research.",7
52.0,3.0,Computational Economics,25 November 2017,https://link.springer.com/article/10.1007/s10614-017-9773-6,Agent-Based Analysis of Industrial Dynamics and Paths of Environmental Policy: The Case of Non-renewable Energy Production in Germany,October 2018,Frank Beckenbach,Maria Daskalakis,David Hofmann,Male,Female,Male,Mix,,
52.0,3.0,Computational Economics,07 June 2017,https://link.springer.com/article/10.1007/s10614-017-9707-3,"Endogenous Economic Growth, Climate Change and Societal Values: A Conceptual Model",October 2018,Michael W. M. Roos,,,Male,Unknown,Unknown,Male,"Although climate change and environmental destruction are serious challenges to global civilization, they are only fringe topics in current mainstream macroeconomics. Fischer and Heutel (2013) argue that that there should be more work on environmental policy in macroeconomics and review some Real Business Cycle models and economic growth models that address environmental issues. Rezai et al. (2013) also call for more macroeconomic research on climate change and identify a particular need for more theoretical analysis and modeling. There is some literature that analyzes the relation between economic growth and the environment in neoclassical growth models, as reviewed in Xepapadeas (2005) and Brock and Taylor (2005). And of course, the integrated assessment models such as DICE (Nordhaus and Sztorc 2013), FUND (Anthoff and Tol 2013) and PAGE (Hope 2011) can be seen as macroeconomic models. In this paper, I propose a model that is an alternative to conventional models of growth and the environment and takes into account the manifold interaction of economic growth, temperature change, population change, and the evolution of societal values. My model is an example of a novel conceptual framework that can be extended in many dimensions. In the model, global economic growth, the evolution of the human population, \(\hbox {CO}_{2}\) emissions, and the state of the environment are endogenous. The main driver of all economic variables are societal values which determine the different types of investment, the level of aggregate consumption and employment. Societal values vary on a materialist/post-materialist spectrum and evolve over time in response to changes in the natural environment and in income per capita. When the state of the environment deteriorates or per-capita income rises, society becomes more post-materialistic and invests more into carbon efficiency improvements and the restoration of the environment. However, since the global average temperature depends on the already emitted stock of carbon dioxide, society may respond too slowly to the environmental damage caused by the industrial production of output. Conventional integrated assessment models like FUND or PAGE are very detailed with respect to the ecological part of the model, but economic growth is simply an exogenous process which is hardly convincing from the perspective of economic theory (see Bonen et al. 2014). In line with the literature on economic growth and the environment, the economic part of the DICE model is a conventional neoclassical Ramsey growth model. Yet there are well-justified methodological critiques against the Ramsey growth model and the related microfoundations paradigm in macroeconomics with representative agents, rational expectations and perfectly rational maximizing agents (Kirman 1992; Hartley 2002; Setterfield and Suresh 2012). In the presence of interaction between heterogeneous agents and market failure due to externalities the representative agent assumption is highly problematic. And as discussed in Roos (2015), climate change and the resulting transformation of the socio-economic system involve radical uncertainty so that maximization and rational expectations are inadequate assumptions. There are two ways to deal with the critique against the rational representative agent. First, one can build agent-based models in which the interaction of heterogeneous agents with bounded rationality is explicitly modeled and the aggregate behavior of the system may be very different from individual behaviors because of the interaction (see Roos 2015). Agent-based models are very flexible and can be applied to a host of different applications. Recently, a number of macroeconomic agent-based models have been published (Assenza and Delli Gatti 2013; Deissenberg et al. 2008; Delli Gatti et al. 2011; Dosi et al. 2010, 2013; Lengnick 2013; Mandel et al. 2009; Salle et al. 2013). Agent-based models are particularly useful if one is interested in the emergence of macroeconomic outcomes from behaviors at the micro- and mesoeconomic level and the bottom-up explanation of phenomena observed at the aggregate level. They are also powerful tools for the analysis of specific policy instruments. But the purpose of this paper is neither to analyze any specific public policy nor to provide a microfounded explanation of some macroeconomic phenomenon or pattern. Instead, I am interested in the interaction between variables at the macroscopic level of the global economy and the Earth’s climate and environmental system. I therefore follow the second way to avoid the aggregation problems of representative-agent models by restricting the analysis to the macroeconomic level only.Footnote 1 The model presented here is a system dynamics model without microfoundations in the tradition of the limit-to-growth literature (see Meadows et al. 1972, 2004).Footnote 2 The purely aggregate structural model approach pursued here has the advantage that it makes the interactions and feedback effects between different parts of the economic system and the environmental system more transparent. It is considerably simpler than an alternative agent-based model that would also combine global production, consumption, investment, labor supply, population dynamics and their effect on climate and the environment. An agent-based model of the same scope would have a huge number of parameters that are hard to calibrate and require many behavioral assumptions that are difficult to justify given the current state of research. Deriving the mentioned relationships and in particular analyzing the effects of various instruments of climate policy in an agent-based model is surely desirable, but requires more basic research on how to work with large macroeconomic agent-based models. The main methodological novelty of the paper is the introduction of societal values as a driving force of all macroeconomic variables. This modelling approach allows me to endogenize the economic processes without having to resort to dubious microfoundations. Societal values serve as a summary variable for potentially quite complicated social dynamics of individual agents’ motives and individual and collective decision processes. They are measurable and have been studied extensively by Ronald Inglehart (1977, 1989, 1995). Introducing social values into a macroeconomic model brings macroeconomics closer to sociology, social psychology, and political science. While in the recent years microeconomics has been strongly influenced by behavioral economics that studies the behavior of real humans, macroeconomics might benefit greatly from socio-economics (Etzioni 2003). Socio-economics analyzes the relationships between society and the economy and uses insights from history, political science, anthropology, sociology and other fields dealing with social processes. Conventional analysis in economic growth theory relies heavily on statements about the steady state of the models or the so-called balanced growth path. The focus on steady states is convenient, because it allows the researcher to study the model in analytical terms and to derive closed-form solutions which are clearly very useful. However, steady state analysis that makes statements about the long run is of little help for societal responses to climate change in the next years or decades. Knowing that a balanced growth path with sustainable levels of pollution or carbon dioxide emissions exists in the long run is not helpful if we do not know when and how it will be reached or whether it can be attained at all. Rather than analyzing the properties of some long-run steady state or balanced growth path, I simulate the potential evolution of the global socio-ecological system in the twenty-first century. The model generates scenarios how the system might evolve under plausible initial conditions and parameter values and shows how sensitive these scenarios are to the assumptions. The model is roughly calibrated to empirical data to generate realistic magnitudes of output growth and temperature change over the next century. It is not meant to be predictive in the sense that it makes forecasts about the most likely future paths of the endogenous variables, but rather anticipatory in the sense of showing which outcomes are plausible. Another important difference of this paper to other approaches in the literature is that I reject the idea of a social planner. It is common practice in models of growth and climate change or integrated assessment models (see Nordhaus 2013) to determine an optimal policy with the tool of the fictitious social or Ramsey planner whose optimization problem incorporates all market externalities. Comparing the optimality conditions of the representative private agents and those of the social planner, one can derive optimal policy instruments such as taxes or subsidies that would induce the private agents to internalize the external effects of their actions on the environment. Even if such optimal policies could be found—which is hardly possible if the uncertainty about the complex dynamics of the economy and the natural environment is properly acknowledged—,Footnote 3 this kind of research does not say anything about the crucial question of how optimal policies could be implemented in the political process. The use of a Ramsey planner always assumes that all the complications of implementing optimal policies have been solved. In contrast to social planning approaches, my model implicitly treats policy as endogenous by assuming that societal values are constraints to and drivers of both private behavior and public policy.Footnote 4 It is important to emphasize that the analysis takes place at the aggregate level which implies that all dynamics of the endogenous variables should be interpreted as the outcomes of underlying social processes, but not as the conscious choices of some fictitious social planner that has the power to determine the aggregate variables. The amount of aggregate investment into physical capital, the level of aggregate saving or the growth rate of population are not directly chosen by anybody. The aggregate variables result from many individual decisions which are not modeled explicitly, but captured by the evolution of societal values, which is also an aggregate concept. The approach pursued in this paper has implications for what can be said about public policy. Instead of suggesting optimal policies or trajectories, that might be highly misleading given the involved uncertainties, the purpose of the model is to raise awareness of how the future of the global economy and the natural environment might look like in the coming decades and how possible trajectories depend on the model assumptions. Integrated assessment models such as Nordhaus and Sztorc (2013) are used to predict how the socio-ecological system might evolve in the future under different assumptions about climate policy. Typically, there is a business-as-usual scenario that extrapolates the current situation into the future and some scenarios in which different policy measures to fight climate change are implemented. The comparison of the policy scenarios with the business-as-usual scenario allows the researchers to evaluate the effectiveness and efficiency of the various policies. This approach is reasonable if the aim is to come to a hypothetical assessment of different policies. However, it treats policy and the fundamentals of agents’ behavior as if they were exogenous to the socio-ecological system, which is implausible in the long run. Exogenously imposing some policy action on the system does not take into account how likely it is that such a policy actually will be implemented. My central argument is that effective climate policies—however they might look like in detail—are only implemented if they are in line with society’s values. If humanity cares more about consumption than about the protection of the climate system and nature, there will be no political majorities for measures such as a carbon tax, an effective emissions trading system or large scale investment into renewable energy. For this reason, I do not want to derive concrete policy recommendations but rather analyze the evolution of the underlying causes of both private behavior and public policy, such that this approach is positive rather than normative. The paper is structured as follows. In Sect. 2, I discuss the concept of social values and their relationship with the economy more in depth. Section 3 presents the model description and Sect. 4 contains the documentation of the model calibration. In Sect. 5, I analyze some properties of the model. Section 6 discusses the model and its results and presents some thought on potential extensions and applications and Sect. 7 concludes.",6
52.0,3.0,Computational Economics,18 November 2017,https://link.springer.com/article/10.1007/s10614-017-9774-5,Assortative Matching with Inequality in Voluntary Contribution Games,October 2018,Stefano Duca,Dirk Helbing,Heinrich H. Nax,Male,Male,Male,Male,"Suppose a population of agents faces the collective action (Olson 1965) challenge to provide public goods by means of simultaneous, separate voluntary contributions games (Isaac et al. 1985). In each one, the collective would benefit from high contributions but individuals may have strategic incentives (Nash 1950) to contribute less. Such situations, also known as ‘social dilemmas’, are related to collective management of ‘common-pool resources’ (Ostrom 1990; Schlager and Ostrom 1992) and often result in underprovisioning of the public good (i.e. tragedy of the commons as in Hardin 1968) because of the misalignment of collective interests and strategic incentives. Generally, grave underprovision of the public good is the unique Nash equilibrium when individual contribution decisions are independent of the matching process. Andreoni (1988)’s model of a linear public goods game with random re-matching of groups is the best-known experimental instantiation of this, and numerous studies have reported corresponding decays in contributions when such games are played in the laboratory (Ledyard 1995; Chaudhuri 2011). Predictions may change dramatically, however, when agents are matched ‘assortatively’ instead, that is, based on their pre-committed choice on how much to contribute so that high (low) contributors are matched with other high (low) contributors. Such mechanisms have been coined ‘meritocratic group-based matching’ (Gunnthorsdottir et al. 2010a), short ‘meritocratic matching’ (Nax et al. 2014).Footnote 1 Under meritocratic matching, new equilibria emerge through assortative matching that are as good as near-efficient (Gunnthorsdottir et al. 2010a; Nax et al. 2014). Indeed, when better (i.e. more efficient) equilibria exist, humans have been shown to consistently play them in controlled laboratory environments (Gunnthorsdottir et al. 2010a, b; Nax et al. 2017; Rabanal and Rabanal 2014). In this paper, we address the important question of how robust the positive predictions stemming from assortative matching are. To assess this, we generalize the baseline model on two dimensions. On the one hand, we consider a range of public-goods provision efficacies that nests the standard marginal-per-capita-rate-of-return (‘mpcr’) model as a special, linear case. On the other hand, we allow heterogeneity in players’ budgets, expressing the ex ante inequality amongst individuals. In other contexts, heterogeneity has been shown to ‘help’ cooperation (Perc 2011). Our work, in particular, builds on one prior attempt at generalizing the standard model in terms of heterogeneity by Gunnthorsdottir et al. (2010b), who consider two levels of budgets in the standard case of mpcr-linear payoffs. Methodologically, we blend analytical and computational approaches. Our results summarize as follows. We show analytically that the consequences of permitting heterogeneity in terms of provision of the public good depend crucially on the exact nature of the underlying public-good provision efficacy, but generally are devastating. Indeed, all near-efficient Nash equilibria that exist under homogeneity fall apart when heterogeneity is allowed. Instead, we are either back at the negative all-contribute-nothing equilibrium or new, previously impossible, complex mixed-strategy Nash equilibria emerge. In the latter case, the expected level of resulting public-good provision depends crucially on (i) the public-good provision efficacy and (ii) the population inequality. These mixed equilibria are virtually impossible to characterize and to evaluate analytically for general cases. We therefore use computational methods and quantify the loss resulting from heterogeneity vis-a-vis the homogeneous case as a function of parameters regarding (i) and (ii). Thus, our analysis provides novel insights regarding the possible consequences in terms of making wrong predictions when assuming a homogeneous population, which in many real-world cases may be unrealistic. The rest of this paper is structured as follows. Next, we set up the model including details about our computational algorithm. Section 3 contains the paper’s results. Section 4 concludes. An “Appendix” contains details of the analytical results.",1
52.0,4.0,Computational Economics,06 July 2018,https://link.springer.com/article/10.1007/s10614-018-9832-7,How to Apply Advanced Statistical Analysis to Computational Economics: Methods and Insights,December 2018,Malin Song,Ron Fisher,,Female,Male,Unknown,Mix,,
52.0,4.0,Computational Economics,27 February 2018,https://link.springer.com/article/10.1007/s10614-017-9791-4,Canonical Correlation Analysis Between Residents’ Living Standards and Community Management Service Levels in Rural Areas: An Empirical Analysis Based on Municipal Data in Anhui Province,December 2018,Deyou Chen,Lei Wang,Youtao Zhang,Unknown,,Unknown,Mix,,
52.0,4.0,Computational Economics,10 January 2018,https://link.springer.com/article/10.1007/s10614-017-9790-5,Environmental Protection in Scenic Areas: Traffic Scheme for Clean Energy Vehicles Based on Multi-agent,December 2018,Lei Li,Wenting Liu,Shi Wang,,Unknown,,Mix,,
52.0,4.0,Computational Economics,26 December 2017,https://link.springer.com/article/10.1007/s10614-017-9787-0,How Should the Chinese Government Invest R&D Funds: Enterprises or Institutions?,December 2018,Yuhan Zhao,Xuguang Song,,Unknown,Unknown,Unknown,Unknown,,
52.0,4.0,Computational Economics,18 December 2017,https://link.springer.com/article/10.1007/s10614-017-9788-z,Coordinated Development of Metropolitan Logistics and Economy Toward Sustainability,December 2018,Shulin Lan,Ming-Lang Tseng,,Unknown,Unknown,Unknown,Unknown,,
52.0,4.0,Computational Economics,19 October 2017,https://link.springer.com/article/10.1007/s10614-017-9745-x,Exploring Dynamic Impact of Foreign Direct Investment on China’s CO\(_{2}\) Emissions Using Markov-Switching Vector Error Correction Model,December 2018,Xiongfeng Pan,Jing Zhang,Bin Li,Unknown,,,Mix,,
52.0,4.0,Computational Economics,19 October 2017,https://link.springer.com/article/10.1007/s10614-017-9759-4,The Income Gap Between Urban and Rural Residents in China: Since 1978,December 2018,Xiao Ma,Feiran Wang,Yang Zhang,,Unknown,,Mix,,
52.0,4.0,Computational Economics,07 August 2017,https://link.springer.com/article/10.1007/s10614-017-9724-2,The Potential Gains from Carbon Emissions Trading in China’s Industrial Sectors,December 2018,Yanni Yu,Weijie Zhang,Ning Zhang,Unknown,Unknown,,Mix,,
52.0,4.0,Computational Economics,13 June 2017,https://link.springer.com/article/10.1007/s10614-017-9709-1,The Electricity Consumption and Economic Growth Nexus in China: A Bootstrap Seemingly Unrelated Regression Estimator Approach,December 2018,Jianlin Wang,Jiajia Zhao,Hongzhou Li,Unknown,Unknown,Unknown,Unknown,,
52.0,4.0,Computational Economics,18 September 2017,https://link.springer.com/article/10.1007/s10614-017-9727-z,Insights into the Effects of Cognitive Factors and Risk Attitudes on Fire Risk Mitigation Behavior,December 2018,Tianzhuo Liu,Huifang Jiao,,Unknown,Unknown,Unknown,Unknown,,
52.0,4.0,Computational Economics,04 August 2017,https://link.springer.com/article/10.1007/s10614-017-9726-0,Credit Rationing and the Simulation of Multi-bank Credit Market Model: A Computational Economics Approach,December 2018,Yu Zhang,Xiong Xiong,Xuefeng Liu,,,Unknown,Mix,,
52.0,4.0,Computational Economics,26 April 2017,https://link.springer.com/article/10.1007/s10614-017-9688-2,Explaining Environmental Sustainability in Supply Chains Using Graph Theory,December 2018,Zongwei Luo,Rameshwar Dubey,David Roubaud,Unknown,Unknown,Male,Male,"In recent years, anthropogenic climate change has become a major concern (Cook et al. 2016; Wang et al. 2015; de Sousa Jabbour et al. 2016; Belkhir et al. 2017). Carbon emissions resulting from various activities across the supply chain account for nearly 50% of the total carbon emitted by large corporations (Norton et al. 2015). However, this may vary between 20 and 80% depending upon the nature of the goods produced by the firm. In response to the growing concerns resulting from carbon emissions, several corporations have embraced various strategies (Colwell and Joshi 2013; Svensson and Wagner 2015); however, many of these strategies lack cogent conceptualization. In addition, the academic literature on environmental sustainability in supply chains and low carbon supply chains has failed to offer a holistic view of the issues involved (Christ and Burritt 2013; Matthews et al. 2016) utilizing mostly either quantitative or qualitative approaches, or have proposed case based methods as alternatives for generating comprehensive theory (see Eisenhardt 1989; Meredith 1998; Pagell and Wu 2009; Wilhelm et al. 2016). However, case research method, despite of having several strengths, has some limitations: first, such studies required extensive time and costs. Second, it is difficult to generalize prescriptive findings due to a small sample (Boyer and Swink 2008). 
Weick (1995) makes clear distinctions between theory, as an end, and theorizing, as a means. Conversely, Davis et al. (2007), argue in favour of simulation techniques for generating theory. To address the limitations of the aforementioned approaches, Boyer and Swink (2008) argue in favour of multi-methods to provide a holistic view to any research problem. In a similar vein, Chandrasekaran et al. (2016) use multi-methods to address the limitations of the case-based method. Chandrasekaran and colleagues have used agent based simulation (ABS) to further validate their theory which they have developed using case method. Besides the use of simulation techniques, graph theory can also be useful. Tichy et al. (1979) proposed the use of network analysis for the organizational research. Out of various network analysis techniques, the graph theory approach can be a powerful tool for theory generation. In the last two decades, interpretive structural modelling (ISM), a systematic application of graph theory, has attracted increasing attentions from the O&SCM community following a seminal publication by Mandal and Deshmukh (1994). However, ISM for theory development is still underutilised. This may be attributed to the limited understanding of the application of ISM method despite the attempts of scholars (e.g. Sushil 2012) to use graph theory to build theory. Given the limitations of ISM, we argue, following Boyer and Swink (2008), towards the use of multi-methods for theory building. However, the use of multi-methods in O&SCM field is scant with due exceptions (see Aksin et al. 2007; Chandrasekaran et al. 2016). Sushil (2012) evokes, Whetten (1989) to suggest that TISM can also be used for building theory like other alternative research methods which addresses the limitations of ISM method. Dubey et al. (2015a, 2016) further this argument using seminal works on theory development (see Whetten 1989; Sutton and Staw 1995; Wacker 1998), that TISM method can be used as an alternative method for theory building. Hence, we argue that TISM may help to answer three important research questions: what, why and how. The objective of our current study is to understand how we can use TISM as a multi-methods approach that can help advance low carbon supply chain literature. Following Sushil (2012) we look for alternative method to generate a theoretical model. Hence, we present an alternative approach not as a replacement but as an addition to the existing portfolio of theory building methods. Hence, our research question and its sub-questions are as follows: 
RQ1: How can TISM be used to propose an alternative theory for low carbon supply chains?
 To address our first research question, we further split it into two sub-questions in accordance with the suggestions of Whetten (1989): 
RQ1a: What are the building blocks of such a theoretical framework?
 
RQ1b: How are these building blocks linked to one another?
 By addressing these questions, our study offers two main contribution to the literature. First, building on the arguments of Sushil (2012) and Poole and Van de Ven (1989), we aim to generate a theoretical framework that leverages tensions and opposing arguments. This engenders a useful contribution to O&SCM methodological diversity. Second, we extend prior research by utilizing organizational theories to develop our theoretical model. In this way, a hierarchical relationship between drivers of supply chain sustainability can be developed. We can also address the growing concern among O&SCM academia regarding hypothesis testing using cross-sectional data. In a recent editorial note, Guide and Ketokivi (2015) have expressed their concerns related to empirical articles utilizing cross-sectional data. It has been observed that cross-sectional data pose multiple issues such as endogeneity and common method bias. However, obtaining longitudinal data poses its own challenges and methodological shortcomings. Furthermore, information asymmetry is a concern related to secondary data. We argue that TISM can help to resolve some of the existing concerns related to problems such as endogeneity and common method bias. The remainder of this paper is structured as follows. In Sect. 2, we review the literature related to low carbon supply chains and the use of graph theory in building our theoretical model. We then present our methodology in Sect. 3, introducing the selection of the drivers of low carbon supply chains from an extensive literature review, sampling design and data collection. Next, in Sect. 4 we describe our data analysis and the findings of the study. Then, the implications of the findings are discussed further in Sect. 5. Finally, we conclude our study and underline limitations and further research opportunities in Sect. 6.",36
52.0,4.0,Computational Economics,19 September 2017,https://link.springer.com/article/10.1007/s10614-017-9744-y,Spatial Pattern of Regional Urbanization Efficiency: An Empirical Study of Shanghai,December 2018,Jinyan Zhan,Fan Zhang,Yifan Li,Unknown,,Unknown,Mix,,
52.0,4.0,Computational Economics,21 September 2017,https://link.springer.com/article/10.1007/s10614-017-9747-8,Does Expressway Consume More Land of the Agricultural Production Base of Shandong Province?,December 2018,Xiangzheng Deng,John Gibson,Siqi Jia,Unknown,Male,Unknown,Male,"Expressways are a fairly recent addition to the transportation infrastructure in China. Previously, the national road network consisted of a system of at-grade China National Highways. It is an integrated system of national and provincial-level expressways in China. Since the middle of 1980s, in order to meet the increasing need of economic growth, expressway has been developed rapidly in China. At the beginning of twentyfirst century, the total mileage of expressway in China has reached 19,000 km, which ranked second in the world after the United States. With the opening of National western development strategy, strengthening the highway construction, especially the highway of high grade is an important base for developing the western region. The country’s economic growth has been accompanied by the sparkling growth of the nation’s transportation infrastructure. According to the database given by National Bureau of Statistics of China (2014), the total highway mileage of China reached an amazing 4.46 million kilometers by 2014. According to the database given by the World Bank, the cultivated land in China has decreased significantly since 1990, for the reason of ecological restoration, rapid urbanization as well as real estate development. However, as the speed of ecological restoration has slowed down since 2007, the rate of cutting down the quantity of cultivated land has lowered, or even showed an increasing trend in the area of the cultivated land (Liu et al. 2010). With rising concern over food security related to cultivated land loss, increasing efforts are being made by economists, ecologists, geographers and other scientists to understand the direction, rate and intensity of cultivated land change (Chen et al. 2009; Jiang et al. 2013) (Fig. 1). 
Data source: World Development Indicators from World Bank (from 1990 to 2010) The area of cultivated land in China from 1990 to 2010.  Nowadays, both rural development and urban development in China are experiencing a transition period that is the reconstruction of a traditional agricultural society into a modern industrial and urban society. With the accelerated rural industrialization and urbanization process, rapid population growth and development of the market economy, the industrial structure, employment structure and land-use pattern in the coastal region of China have been transformed enormously. Long et al. (2009) pinpointed that rapid industrialization along with urbanization had greatly changed the rural areas in the facet of cultivated land loss for factory workshop, and rural labors transformation for workers. Since the year of 1978, agriculture and the rural area have made a big contribution to the development of industries and the cities in China. As a result, a series of problems along with the social and economic development of China appeared, such as decreasing cultivated land, degrading environment, widening the income gap between urban and rural area, and so on (Gibson and Rozelle 2003; Xie and Zhou 2014; Wang et al. 2016). Currently, the determinants of cultivated land loss have attracted interests among a wide variety of researchers, ranging from those who are modeling the spatial and temporal patterns of land conversion, to those who try to understand the causes and consequences of land-use changes (Irwin and Geoghegan 2001; Liu et al. 2008; Gennaio et al. 2009; Deng et al. 2015). To some extent, cultivated land loss is still a complex issue regarding its process, dynamic and driving forces (Jiang et al. 2012; Kuang et al. 2016; Tegegne et al. 2016). The literature shows that various geophysical factors, such as slope and elevation, demographic factors, economic variables and policies of governments are all important correlates of cultivated land and its changes. Therefore, a single research approach does not suffice for a complete analysis on impacts of road buildings on cultivated land. Instead, a combination of multiple approaches is necessary (Long et al. 2007; Song et al. 2012; Laurance et al. 2014). There are an increasing number of researches which have focused on the relationship between roads and cultivated land. In many instances, roads are found to lead to cultivated land loss. The logic is that when a road enters an area (or when it is widened or improved), pressure will rise and then cultivated land will fall. An important implication of what we characterize as this “pressure cooker” hypothesis is that “road networks may significantly shape the spatial pattern of remaining cultivated land” (Deng et al. 2011). Hence, road investments in cultivated land are thought to lead to cultivated land loss. According to this “pressure cooker” hypothesis, when new or better roads reach into a region, access to transportation and new and more convenient linkages to the outside world encourages economic growth, produces jobs and increases agricultural productivity (by making inputs cheaper, agricultural technology more accessible and farm-gate prices of agricultural commodities higher). If these dynamics are able to refocus the livelihood strategies of households that previously were encroaching on cultivated land onto intensive (river-bottom; irrigated) agriculture and off-farm employment, including migration, the pressure on the cultivated land might be reduced. In fact, there has been a fairly large literature that discusses the mechanism that may be underlying the pressure-valve hypothesis. Such a phenomenon could arise in part as a result of increased opportunities to purchase inputs that increase or maintain yields (Gibson 2002; Gibson and Olivia 2010; Song et al. 2013; Turkseven and Ueda 2016). Since existing evidence on the effects of roads on cultivated land is unclear, and has not always benefited from latest refinements in data and methods, new evidence is required. Specifically, we use the remotely sensed digital images by the Landsat TM/ETM satellite with a spatial resolution of \(30 \times 30\,\hbox {m}^{2}\) covering Shandong province, China, to test whether the existence of roads in 2005 affected the level of cultivated land in 2010 and the rate of change from 2005 to 2010. To account for road access for each of our 1 \(\hbox {km}^2\) (‘pixel’) units of cultivated land we measure whether or not roads in the “city corridor” exists. City corridor is one of spatial forms of city system, and it has a long history in regional and urban development and planning. To account for confounding from the exclusion of other relevant variables and potentially biased estimates of treatment effects due to the endogenous placement of roads, we use covariate matching techniques, using 24 additional covariates. Our overall goal is to discover if roads are delegating more like “pressure cookers”—and are associated with lower levels of cultivated land and greater rates of cultivated land loss in Shandong province—or more like “pressure valves”—and are associated with higher levels of cultivated land and lesser rates of cultivated land loss (or are neutral). To meet these objectives, the rest of this paper is organized as follows. The next section is an overview of the study area, the Sect. 3 is about the definitions (explained and explanatory variables) and data used in this study. The dependent variable, the level of cultivated land (in some regressions—and the change in cultivated land in others), is defined and the approach that we use to measure access to expressway is described. The Sect. 4 lays out the econometric approach that we use to explore in greater depth the relationship between expressway and cultivated land loss. Our main strategy in this analysis is to look at the simple relationship between expressway and cultivated land loss, by including covariates to measure the net effect of expressway on cultivated land changes, and use matching methods to at least in part control for observed and unobserved differences between pixels that have different degrees of access to expressway in order to obtain unbiased treatment effects estimates, of what happens to the cultivated land. The final section reports the estimation results, discusses the key findings and concludes as well.",5
52.0,4.0,Computational Economics,19 June 2017,https://link.springer.com/article/10.1007/s10614-017-9713-5,Nonparametric Regression Using Clusters,December 2018,Hrishikesh D. Vinod,Fred Viole,,Unknown,Male,Unknown,Male,"Smoothing of histograms to yield Nadaraya–Watson kernel densities was developed in the 1960s. Since regressions are expectations of conditional densities, nonparametric regressions have been applied in Economics and surveyed in Vinod and Ullah (1993) and Li and Racine (2007). The ‘np’ package of R, Hayfield and Racine (2008), implements kernel regressions including estimation of partial derivative functions defined at each data point. Vinod (2008), Section 8.4 uses an average of partial derivative estimates as regression coefficients (approximations for marginal propensities or elasticities) desired in some econometric applications. Unfortunately, kernel regression estimates are sensitive to a bandwidth parameter and their performance leaves much to be desired, as we shall show with examples and simulations. This paper evaluates a newer and fundamentally distinct alternative to kernel regressions. Nonlinear Nonparametric Statistics (NNS) package (maintained by one of us), Viole (2016b), is explained in Viole and Nawrocki (2012b). In behavioral finance, the concept of loss aversion is modeled by studying lower partial moments of partitioned densities since Bawa (1975) and Vinod and Reagle (2005). Viole and Nawrocki (2012a) prove that aggregating all partial moment matrices equals the covariance matrix, providing much more disaggregated and nuanced information than possible with traditional summary statistics. Inspired by financial applications, NNS performs hierarchal and partitional clustering using partial moment quadrant means and joins them to yield a nonparametric fitted regression function, f(x). In curve fitting, an extreme overfitting option gives f(x) by simply joining all points. Similarly, in a k-means clustering limit, Bock (2008), each data point makes its own cluster. In its default setting, NNS starts with this perfect fit and reduces the number of clusters based on the signal to noise ratio in [0,1] provided by a new NNS dependence measure. Total dependence between variables will call for a perfect NNS fit. NNS offers a parameter whereby a specific number of iterative partial moment quadrant partitions can be called. The following discussion walks through an iterative example of NNS progressing in orders of partitions. Figure 1 having two vertical panels provides a visualization of how NNS fits linear segments to a sine wave.Footnote 1 The left hand panel has three figures partitioned with respect to (wrt) both axes. The right panel also has three figures, depicting the linear connection of an adaptive sequence of means illustrated in the panel to the immediate left. Note that the algorithm is simply connecting the two endpoints by straight lines, not fitting any linear regression over that subset of observations. The adaptive sequence involves computing the mean of each subset (only if the subset is not empty), further partitioning the data, then computing the means of subsequent partitions. The first partition occurs at data means, \({\bar{X},\bar{Y}}\). The point having these coordinates connected to the endpoints comprise the line segments of our ‘Order 1’ segmented partitioning approximation to the underlying nonlinear function (sine wave) depicted in the top right figure. We display higher order approximations along the second and third row of the figure. Since the mean of \(y=\sin (x)\) is zero for the given range of x the top figure on the left has a straight grey line representing the initial y-axis partition. The first partition results in four quadrants wrt the means of the two variables. These quadrants are seen along the top row of Fig. 1, where the initial partition is called ‘Order = 1’ partition. Our second order partition further partitions the joint distribution, f(X, Y). Each quadrant from the top row is then further divided into 4 quadrants. The subset means of each of the following 4 quadrant subsets are the 2nd order regression points depicted as black points in the left side middle panel. The ‘Order 2’ curve fitting approximation is seen on the right side middle panel. The bottom right panel of Fig. 1 depicts ‘Order 3’ partition approximation, which is fairly close to the sine curve. The underlying NNS clustering algorithm computes local averages, not any explicit local least squares fitting using polynomials or splines. Of course, local average can be interpreted as a regression coefficient when a column vector of data are regressed on a column vector of ones. Visualization of joint distribution f(x, sin(x)) using \(4^{(O-1)}\) quadrants, where \(order=O=1,2,3\), depicting quadrant means (left panels) and corresponding NNS regression line segments (right panels) NNS offers two types of partitioning: the first is performed by both X and Y quadrant means. The joint mean coordinate of that subset is the regression point. This is the default setting. The second type of partition, activated by a (type = “XONLY”) parameter, performs the partitioning along the X-axis only. \(\bar{Y}\) of that entire vertical “bandwidth” is the fitted regression point, loosely similar in theory to kernel based nonparametric regressions using a uniform kernel over a given bandwidth. The endpoints are treated differently in NNS than in other nonparametric regressions. NNS uses a nonlinear dependence measure, Viole (2016a), (NNS.dep function in NNS package) between X and Y to determine the endpoints. If the dependence is ‘large’ (in some sense, as defined by the algorithm), then the algorithm fixes the endpoints to the boundary observations. At the other extreme, if no dependence exists, the medians of Y values in each of the boundary partitions are used as robust alternatives to using possibly dubious endpoints.Footnote 2
 Let us provide some further discussion of the theory behind NNS’ ability to fit any curve. The beauty of the NNS method is that it yields an increasingly improved curve fitting approximation using linear segments (which serve as local linear fits) by increasing the ‘Order’ of segmentation. Stone (1961) argues that local least squares estimates approximation is best in the overall least squares sense. Bellman (1961) provides dynamic programming solutions to Stone’s examples. The linear segments (defined over \(4^{(O-1)}\) quadrants, where O denotes the order) together create a sequence of line segments comprising an approximation to the nonlinear curve. An important benefit of such line segments is that they provide readily estimated segment slopes, providing simple and direct estimates of partial derivatives \((\partial y/ \partial x)\), without involving any infinitesimal changes. Another important NNS advantage appears to be that the axis segmentation is conditional, whereas the bandwidth used in ‘np’ is fixed. This conditional partitioning offers parsimony by avoiding unnecessary quadrants when no local observations exist. For example, the bottom left panel of Fig. 1 has points only along the sine curve. The quadrants having no points are omitted without hurting the fit. Hence, NNS partitioning is un-phased even when operating on severely clustered data. Weierstrass’ (1885) famous Approximation Theorem states that any continuous function f(x) can be approximated by arbitrarily closely by a polynomial \(p_n(x)\), of a sufficiently high degree (n). That is, given a compact set K so that \(x\in K\), and \(\epsilon >0\), there exists an n offering a “close” approximation defined by: High degree polynomial approximations are rarely (if ever) used in nonlinear regressions because of estimation problems due to collinearity among regressors of various orders and because fitted high order polynomials are ill-behaved, having unpredictable behavior between observed points. NNS retains a similarly desirable limiting behavior with a perfect fit for a sufficiently large value of O, but uses well-behaved linear segments between observed points. 
Remark 1: NNS offers a perfect, not approximate, limiting fit to f(x) denoted by \(f_O(x)\), for a finitely large order parameter O. That is, given a compact set K so that \(x\in K\), there exists a finitely large O satisfying The proof is straightforward. We start with observed values of x and f(x). As O increases, the number of quadrants and line segments joining quadrant means increase exponentially according to \(4^{(O-1)}\), until each observation becomes its own quadrant mean in the limit. Then, \(f(x)=f_O(x)\) must hold, implying a perfect fit in the limit. 
Remark 2: The usual caveats of overfitting still apply, and we do not suggest specifying a high order without sufficient reasons. Accordingly, a large O is sometimes (not always) beneficial for partial derivative estimation and/or for increased accuracy of out-of-sample predictions. In all sets of experiments, the default setting of NNS order is high only when the ‘dependence’ is 1. We later note the relationship between dependence and partial derivative average percentage errors. A summary of important reasons behind the success of NNS is listed next. 
Sufficiency: NNS uses partitions of data based on a carefully designed scheme of partitions. The computed means and variances of each partition rely on the terminology of lower and upper partial moments. The algorithm assigns a partition to each data point while insisting that summing the weighted partitions always preserves the overall mean and variance.Footnote 3 Thus NNS relies on “sufficient” statistics such as the mean and variance of the data in its partitions.Footnote 4
 
Gap-free Segments: Stone’s method is based on approximating curves via “a series broken line segments”. NNS essentially overlaps the segments by sequentially connecting the mean points of the partitions, leaving no gaps, as illustrated in Fig. 1. If we try to use a least squares fit of the form \(y=a+bx\), say, over the same data within a partition, there will be separate nonzero intercepts for each partition, often leading to broken line segments. 
Perfect Limiting Fit: NNS has a limit condition described in Remark 1 whereby every observation will be its own fitted regression point implying a perfect fit in the limit as we increase the order O. k-means clustering shares this limit condition whereby at the extreme case we will have all observations as their own cluster.Footnote 5
",6
52.0,4.0,Computational Economics,01 April 2017,https://link.springer.com/article/10.1007/s10614-017-9674-8,Evaluating Design of Increasing Block Tariffs for Residential Natural Gas in China: A Case Study of Henan Province,December 2018,Chang Liu,Boqiang Lin,,,Unknown,Unknown,Mix,,
52.0,4.0,Computational Economics,13 July 2017,https://link.springer.com/article/10.1007/s10614-017-9675-7,Design and Analysis of Supply Chain Networks with Low Carbon Emissions,December 2018,Tsai-Chi Kuo,Ming-Lang Tseng,Po-Chen Chang,Unknown,Unknown,Unknown,Unknown,,
53.0,1.0,Computational Economics,23 June 2017,https://link.springer.com/article/10.1007/s10614-017-9716-2,Extracting Appropriate Nodal Marginal Prices for All Types of Committed Reserve,January 2019,Paria Akbary,Mohammad Ghiasi,Noradin Ghadimi,Unknown,Male,Unknown,Male,"Traditionally, the ownership of all the assets in power systems was in hand of governments. With different motivations (such as efficiency improvement and relieving the governments of a financial obligation), the electric power systems have been restructured in most of countries. More sophisticated control mechanisms are required to manage such systems in which all the private owners are trying to satisfy their own objectives. Electric energy is being traded in the electricity markets across the world. An electricity market is a system for purchasing and selling the electric energy. The consumers submit their bids to buy their required energy and the independent power producers submit their offers for selling their products. The prices of different services are set according to the bids and offers submitted. There are different market models based on the type of transactions, the market lead time, the market products, the entities participating and the pricing mechanism. This paper focuses on a wholesale day-ahead market which selects the energy and reserve wining offers simultaneously. Only the service providers contribute in the auction (single-side auction). Independent System Operators (ISOs) are responsible for the safe operation and development of the wholesale electricity markets. The ISOs are also responsible for introducing new legislation to combat the upcoming issues and to manage and control the electricity markets. As a commodity, electricity has some distinct features. Firstly, it is not possible to store this type of energy on a scale comparable with total system consumption. Secondly, electric power flows in the transmission lines based on the electric circuit lows. Traditionally, Unit Commitment (UC) programs were used to select the units and their production level to balance the generation and demand in every hour at minimum cost (based on the units’ cost functions). Due to non-storable feature of electric energy, each variation in the consumption should be coordinated by a corresponding variation in the units’ production. Optimal Power Flow (OPF) is used to find the optimal production level of each unit in each hour satisfying the network constraints. To formulate these constraints the circuit theory is used in terms of power flow equations. A full AC power flow considers all the relationships between the circuit variables. However, a full AC power flow is not usually used in market OPF due to computational burden. A DC power flow (which just considers the relationship between active power injected to system buses and voltage angles) is always a sufficient approximation. In an electricity market, the units’ cost functions are replaced with their offers submitted. The objective is also different from the classic UC. In a single side auction, where only the service providers submit their offers to the market, the objective is to minimize the total offer cost. Power systems should withstand sudden disturbances, such as components’ force outages. Therefore, the security constraints should be also included in UC problem. The resulting problem is referred to as Security Constrained UC (SCUC). To ensure the system security, different types of reserve should be committed. The required up-going reserve capacities are provided by the units that can increase their generation or the consumers that can decrease their consumption. Sometimes, it is necessary to decrease the power injection at some buses to relief the lines’ congestion and to secure the system during some contingencies. The units which are able to decrease their generation or the consumers which are able to increase their power demand can provide the down-going reserve capacities. Spinning reserve is generation capacity that is on-line and can respond within 10 min to compensate for system contingencies. Non-spinning reserve is offline generating capacity which can be brought online within 10 min. After market clearing step, a proper pricing scheme is necessary to find the energy and reserve prices. This paper discusses how to find the proper reserve prices. Various and fast changes in the socio-economic structure of power systems have led to the fundamental changes in the technical aspects of power systems control and management. One of these changes is the change in the pricing scheme of energy, reserve, transmission services, and other ancillary services in the new structures of power systems. The locational marginal pricing method is the dominant approach in power markets and has been widely applied in different fields from calculation of electricity prices to congestion management of transmission systems. Different pricing schemes based on locational marginal pricing have been implemented or are under consideration at a number of ISOs such as PJM ISO (that coordinates the movement of wholesale electricity market in all or parts of Pennsylvania, New Jersey, Delaware, Illinois, Maryland and some other states in the United States) and New York ISO (which operates competitive wholesale market to manage the flow of electricity across New York) in order to calculate fair energy prices at different locations, which also lead to a more optimum operation of the system in the long term (Nouri et al. 2013). Marginal pricing schemes have been used for energy, reserve, reactive power and transmission services because these schemes have all the characteristics that are necessary for pricing in the restructured power systems (Nouri et al. 2013). This paper focuses on the reserve marginal pricing and will show that the proposed pricing scheme provides proper signals for the market players to participate in the reserve market according to the system operator requirements. Maintaining a high level of system security is one of the more important aspects of power systems that should be noted as well as the economic operation of these systems. Different ISOs use different mathematical methods to obtain the security constraint unit commitment (SCUC) schedule (Shahidehpour et al. 2002). Different types of constraints such as network constraints, generating units conditions and constraints, and bilateral contracts should be considered (Fu et al. 2005). Also, the required level of the reserves can be set based on deterministic, probabilistic, or hybrid viewpoints and their corresponding criteria (Bouffard et al. 2005; Ghadimi 2015). Reference Li and Shahidehpour (2007) compared the Lagrangian relaxation and the mixed integer programming (MIP)-based methods and suitably formulated the objective function and constraints of the commitment problem, especially to solve this problem by engaging a standard MIP package. Though the formulation was proposed for the price-based unit commitment problem, the constraints such as unit ramp up/down, unit minimum up-/down-times, fuel constraints, emission constraints, and other unit constraints were skillfully modeled and can be used in SCUC problem as well. The other constraints, such as network constrains, have been modeled in other works. Reference Fu et al. (2005), for example, modeled the network constraints appropriately and proposed a framework to clear energy and ancillary services markets simultaneously. The full AC OPF and SCUC have not been widely applied in real-time operations especially in large-scale power systems. Instead, simplified OPF and SCUC tools based on linear programming (LP) and mixed integer linear programming (MILP) and decoupled system models have been often used http://www.pjm.com/contributions/pjm-manuals/manuals.html. It is hard and even impossible to assume that power system security can be maintained without considering a secure level of down-going reserve at appropriate buses all over the network. In a real power system, reserve pricing is a complicated problem due to the use of different types of reserve and this paper deals with this subject. In this paper, up- and down-going reserves at both generation and demand sides are considered. Besides, the reserve suppliers receive no extra profit for their contribution in the corrective actions in the event of a contingency, because these suppliers already profit from their willingness to provide different types of reserve. Thus, if called upon to provide reserves, they have to modify their generation or demand without any further remuneration. So, a unit is paid the capacity price for providing reserve capacity and the spot market energy price when called to generate energy in real time (Li and Shahidehpour 2007). It has been assumed that the providers of reserve services from each type should be able to ramp up or down their production or demand by the value of accepted reserve in 10 min. Reference Jalili and Ghadimi (2015) introduced the concept of marginal cost of security, using the Lagrange method to solve the SCUC problem in a one-hour period. A same marginal price was considered for all types of reserve at each bus in this reference. This concept was used in Ghadimi (2015) to calculate the revenue from the sale of reserve for each reserve supplier, in various case studies. In this paper it is shown that the notion of marginal price of different types of reserve (up- and down-going, in both generation and demand sides) does exist at all of the network buses. The results of the pricing program can be sent to the participants in the market as incentive signals to modify their offers based on the operator’s requirements. Though the proposed method uses SCUC outputs as the system operation point, it is independent from the SCUC algorithm and solution method. A framework for an hour-ahead security market was presented in Dideban et al. (2013). In this market, the operator assumes that few credible contingencies have happened and now it is necessary to commit new reserves to provide the required level of security again. Reference Nouri et al. (2013) addressed the same hour-ahead market and specially considered interruptible load contracts to provide required level of security. Though the market clearing is usually performed based on the offers of different participations, the offers may significantly differ from actual costs in practice. Using a marginal pricing scheme, Nouri and Hosseini (2015) presented a framework for alternative market-clearing procedures dependent on market-clearing prices rather than on offers. As Nouri and Hosseini (2015) discussed, this problem has recently received considerable attention, but different types of reserves were not discussed. This is the subject of our next publications. Reference Momoh et al. (2008) proposed a framework to find locational marginal prices (LMPs) for real and reactive power with a full AC OPF. The main contributions were the inclusion of production cost of reactive power from different sources including generators and reactive compensators, and decomposition of LMP into three components of energy reference, network loss, and congestion. A locational pricing method was introduce in Chen et al. (2003) for an hour-ahead joint energy-reserve market. Only the generation-side up-going reserve was considered and the main contribution of the paper was to propose a method to determine the locational price of the applied reserve (not the committed reserve). In the next subsection some weaknesses of the previous works that motivated the authors to do this work will be summarized. Reference Ghadimi (2014) proposed a pricing method for a similar commodity (generation side up- going reserve) but in a partially deregulated market. A framework for pricing energy and reserve by stochastic optimization was proposed in Wong and Fuller (2007). Linear programming was used for a one-hour time period and the effect of binary variables and coupling constraints between different intervals were not considered and again, only generation-side up-going reserve was considered. Previous works in the area of reserve marginal pricing, including the ones reported in the literature review subsection, did not cover all aspects of the problem. This motivates the authors to work toward achieving an effective pricing algorithm. The major motivations are listed below. Considering the binary variables of the SCUC problem and coupling constraints between different time intervals, a closer inspection is needed to solve the pricing problem in different intervals. In the proposed pricing algorithm the integer variables are assumed to be fixed and the pricing problem is decomposed to hourly pricing algorithms considering the coupling constraints between different hours. This will be discussed further in Sect. 3. Previous works considered only generation-side up-going reserve. Considering all types of reserves and also the problem of losing the opportunity to sell energy because of reserve provision and vice versa, it is not possible to calculate the reserve marginal prices simply using Lagrange multipliers (dual variables). The pricing scheme developed in this paper accounts for the lost opportunity costs for selling the convertible products. It should be noted that energy and reserve are convertible products. To explain further a reserve provider which dedicates a portion of its capacity to provide reserve, lost its opportunity to sell further energy. This lost opportunity cost is considered in the reserve pricing scheme. Marginal prices of energy and reserves can be different for generation and demand sides due to the effects of contingencies. So, we cannot assign the same marginal prices to both sides. A generating unit which fails to produce energy in some contingencies has lower energy marginal price comparing to the one which is available in all system states. Actually, demand side energy marginal price at a bus is equal to or larger than energy marginal price of all generating units connected to this bus. Different system constraints affect the marginal prices of different services as well as the solution point of the SCUC problem, so it is necessary to perform the market clearing and pricing processes simultaneously. Solving the clearing and pricing problems in one unified problem restricts the number of methods which can be applied to solve this problem. Various optimization methods have been adopted and applied to solve the SCUC problem by now. Most of these methods (e.g., heuristic methods) are unable to calculate the marginal prices. In this paper, the market clearing and pricing problems are performed separately. All the SCUC constraints are examined in details and those which are not necessary to be considered in the pricing process are eliminated. The others are introduced to the proposed pricing algorithm. The proposed method calculates the marginal price of energy and different types of reserves at different buses without engaging in a new optimization process and this is the main advantage of the method. When one of the market participants is called to provide one type of reserve, he may lose his opportunity to contribute in the energy market and vice versa. In order to calculate the fair and defensible prices, this lost opportunity is considered in the proposed algorithm. The proposed reserve pricing scheme is fair, since it assigns the same prices to the reserve providers which provide the same services at the same buses (see Sect. 4.3). The method which is applied to calculate the marginal prices decomposes the problem into several sub-problems for different time intervals. Energy and reserve pricing can also be performed separately. It should be noted that in each decomposition process, all the coupling constraints between the decomposed sub-problems should be considered. The rest of the paper is organized as follows. A conventional MILP-based SCUC algorithm is discussed in Sect. 2. Section 3 gives an overview of the proposed algorithm. The algorithm is applied to the modified IEEE reliability test system and the results are presented and discussed in Sect. 4. Finally, the conclusions are drawn in Sect. 5.",254
53.0,1.0,Computational Economics,24 June 2017,https://link.springer.com/article/10.1007/s10614-017-9717-1,Monetary Transmission Channels in DSGE Models: Decomposition of Impulse Response Functions Approach,January 2019,Miroljub Labus,Milica Labus,,Male,Female,Unknown,Mix,,
53.0,1.0,Computational Economics,05 August 2017,https://link.springer.com/article/10.1007/s10614-017-9718-0,Quantile-Based Inference for Tempered Stable Distributions,January 2019,Hasan A. Fallahgoul,David Veredas,Frank J. Fabozzi,Male,Male,Male,Male,"From a statistical point of view, measurement of financial risk (e.g. Value at Risk) involves two components: volatility and tail risk. The former can be understood as the expected risk, or the risk under normal market conditions, while the latter is the unexpected risk, or the risk under financial turmoil. In this paper we focus on the second component of risk. We approach it by making inference on a particular class of heavy tailed distributions, namely the class of tempered stable probability distributions. Though there are numerous ways to define heavy tails, in this paper we refer to them as tails that decrease more slowly than an exponential function. Mathematically, this is stated as follows: let X be a random variable with cumulative distribution function F(x) and associated tail function \( \bar{F}(x)=1-F(x)\). Then \( \bar{F}(x)\longrightarrow cx^ne^{-\lambda x} \) for some power of x and as \(x \rightarrow \infty \). Among all the heavy tailed distributions, the class of tempered stable distributions has attracted recent attention because of their useful theoretical properties (see for example Fallahgoul et al. 2016; Rachev et al. 2011; Kim et al. 2009, 2011, among others). Sound financial applications however are thwarted by the lack of a fast, simple, and accurate method for the estimation of the parameters. CF and approximated pdf of the conditional normal tempered stable distribution. a The real (dotted line) and imaginary (solid) parts of the CF of the normal tempered stable distribution. b The approximated pdf of the normal tempered stable distribution Indeed, a handful of methods have been proposed for the estimation of tempered stable distributions. Meerschaert et al. (2009) uses largest order statistics. Kim et al. (2010) estimates heavy tails by Maximum Likelihood (ML), computing the density with a combination of the fast Fourier transform and the characteristic function (CF). Gajda and Wyłomańska (2013) also proposes ML methods, along with two matching methods, one based on moments (for which closed-form expressions are obtained), and another based on the sample and the theoretical CFs. There are three issues related to the use of maximum likelihood estimation (MLE) for estimating the parameters of tempered stable distributions. The first issue is the high computational cost, and the second is the choice of the grid of frequencies at which to evaluate the CF. While theoretically a very thin grid of frequencies optimizes the use of the information, in finite samples a well chosen thin grid can be better. And even asymptotically, matching sample and the theoretical CFs evaluated on a continuum of frequencies introduces a fundamental singularity problem. The third issue is that the likelihood function is constructed by inverting the Fourier transform; more precisely, the probability density function is obtained by inverting the Fourier transform. By using the fast Fourier transform (FFT), the speed of computation for the inverting procedure increases substantially. However, for estimating a conditional distribution for small time intervals or an unconditional distribution with a specific set of parameters, the accuracy of the FFT for approximating the probability density function is extremely low. This weakness becomes even more serious when the distribution has a small variance (i.e. all mass is around the mean). Since all classes of tempered stable distributions belong to the class of Lévy processes, they have small variance. Indeed, the MLE cannot be used for estimating the conditional tempered stable distribution in small intervals. Figure 1a shows the CF of the normal tempered stable distribution.Footnote 1 The real and imaginary parts of the CF decay very slowly to zero (in absolute value). The slow decay of the real part of the CF to zero leads to a less accurate approximation for the probability density function based on the FFT. Figure 1b shows the probability density function of the CF in Fig. 1a. There are oscillations in the approximation of the probability density function, a phenomenon known as the Gibbs phenomenon. The oscillation of the probability density function is due to the slow decay of the real part of the CF to zero. In fact, even by choosing a very large number of terms for the series approximation, the inverting procedure is not accurate. In this article we use the method of simulated quantiles (MSQ henceforth) introduced by Dominicy and Veredas (2013) as an alternative to ML. Since it is based on quantiles, it does not make use of the frequency domain, and, since it is based on simulations, we do not need closed form expressions of any function that represents the probability law of the process. In a nutshell, MSQ is based on a vector of functions of quantiles that are informative about the parameters of interest. These functions can be either computed from data (the sample functions) or from the distribution (the theoretical functions). The estimated parameters are those that minimize a quadratic distance between both functions. Since the theoretical functions of quantiles may not have a closed-form expression, we rely on simulations. We focus on three tempered stable distributions: the classical, the normal, and the generalized classical. Henceforth we refer to them as CTS, NTS, and GTS. For each distribution we propose functions of quantiles that are informative about the unknown parameters. While the general asymptotic theory of MSQ applies to all the distributions, we study the finite sample performance of the estimators with a Monte Carlo study. The estimates are essentially unbiased and the relative efficiencies (relative to MLE) are close to one. We also show that the computational gain with respect to the MLE: estimation with quantiles takes roughly 50% of the time that estimation with the density. We illustrate the estimation method with 13 years of daily stock log returns for 21 major world-wide equity market indexes, covering America, Europe, and Asia and Oceania. As in the Monte Carlo study, MSQ estimators are nearly as efficient as MLE. Additionally, our VaR analysis and backtest shows that the estimated VaRs are reasonable. In fact, they are as good as with MLE. Note that we do not attempt to demonstrate in this paper that tempered stable distributions are suitable models for modelling financial time series. Several studies have documented their suitability (see for example, Rachev et al. 2011; Kim et al. 2011, 2012). Instead, we look at the tempered stable distribution to show the superior performance of the MSQ estimates relative to MLE, which we do: estimation by MSQ is twice faster than with MLE, MSQ is almost as efficient as MLE, and from a risk management point of view they provide similar results. The rest of the paper is laid out as follows. Section 2 explains the methodology, with a short review of tempered stable distributions, followed by the use of the MSQ for the CTS, GTS and NTS distributions. The Monte Carlo study is provided in Sect. 3, and the empirical illustration is shown in Sect. 4. Concluding remarks are given in Sect. 5. An appendix with tables closes the paper.",4
53.0,1.0,Computational Economics,18 August 2017,https://link.springer.com/article/10.1007/s10614-017-9725-1,Wavelet Multiresolution Analysis of the Liquidity Effect and Monetary Neutrality,January 2019,Olivier Habimana,,,Male,Unknown,Unknown,Male,"Unlike the traditional view of macroeconomic fluctuations, in real business cycle (RBC) models, as well as in other models that assume completely flexible prices, monetary changes have no real effects, even in the short run. As Romer (2012) points out, examining the real effects of monetary disturbances is a critical test of pure RBC models. The debate on the role of money in the economy is as old as economics. The theory of liquidity preference, as pioneered by Keynes in the General Theory of Employment, Interest and Money, postulates that interest rate is the price that equilibrates the desire to hold wealth in the form of cash with the available quantity of cash. Thus, in the short run, an increase in money supply induces a decrease in nominal interest rate. This short-run negative relationship is called the liquidity effect. Another central tenet of macroeconomics is the long-run neutrality of money. The proposition that the change in money stock changes nominal prices (and wages) and does not affect real variables such as output, employment, real interest rates and real wages. In his Nobel Prize lecture, Lucas (1996) discusses money neutrality in particular and the quantity theory of money (QTM) in general. The central prediction of the QTM is that in the long run money is neutral. However, Lucas highlights the ambiguous nature of the terminology “long run”. Since economic agents make decisions at different time horizons, this terminology is relative. In the mainstream economics, the time horizon has been simplified to short and long run. This restriction to only two time horizons has been a matter of convenience and pedagogical advantage, for, until recently, there were no tools to decompose economic time series into detailed timescales (Ramsey and Lampart 1998). There is mixed empirical evidence on these two rather important propositions in monetary economics. This paper seeks to shed more light on the relationship between money and interest rate, and money and output using wavelets to decompose series into detailed timescales. Wavelets can be defined as lens that allow zooming the series in and out to explore relationships that would otherwise be unobservable (Ramsey 2002). They are powerful tools to analyze non-stationary (non-periodic) time series, a rather typical feature of macroeconomic time series. The rest of the paper is organized as follows. Section 2 discusses previous studies. Section 3 describes the econometric methods. Section 4 describes the data and presents and discusses empirical results. Section 5 concludes.",10
53.0,1.0,Computational Economics,30 August 2017,https://link.springer.com/article/10.1007/s10614-017-9730-4,A Hybrid Monte Carlo and Finite Difference Method for Option Pricing,January 2019,Darae Jeong,Minhyun Yoo,Junseok Kim,Unknown,Unknown,Unknown,Unknown,,
53.0,1.0,Computational Economics,01 September 2017,https://link.springer.com/article/10.1007/s10614-017-9729-x,"A Stochastic Model with Inflation, Growth and Technology for the Political Business Cycle",January 2019,Gopal K. Basak,Mrinal K. Ghosh,Diganta Mukherjee,Male,Female,Unknown,Mix,,
53.0,1.0,Computational Economics,04 September 2017,https://link.springer.com/article/10.1007/s10614-017-9728-y,Hodges–Lehmann Estimation of Static Panel Models with Spatially Correlated Disturbances,January 2019,Christoph Strumann,,,Male,Unknown,Unknown,Male,"Spatial regression models are typically estimated by means of Maximum Likelihood (ML) techniques. However, several studies point out a substantial downward bias of ML estimates of the spatial correlation parameter, which increases with the degree of spatial connectivity (e.g. Yang et al. 2016; Liu and Yang 2015a, b; Yang 2015; Fujimoto et al. 2011; Neuman and Mizruchi 2010; Mizruchi and Neuman 2008; Farber et al. 2009; Smith 2009). Lee (2004) highlights a potential irregularity of the information matrix under strongly connected spatial structures, which affects the convergence rate of the ML estimator. Bao and Ullah (2007) and Bao (2013) demonstrate that the bias of the ML estimator is sensitive to the structure of the spatial weights matrix. They suggest for cross sectional spatial lag models bias corrected estimators, which are effective under weakly connected spatial structures. As opposed to the spatial lag model, a biased spatial parameter estimate has no impact on the bias of regression coefficient estimates in the spatial error model. However, if the spatial parameter is of particular interest, the respective bias matters. Recent applications of the spatial error model include the analysis of economic convergence (Fingleton and Lopez-Bazo 2006; Lundberg 2006; Lopez-Bazo et al. 2004; Villaverde 2005; Rey and Montouri 1999), house prices (Baumont 2009), technology adoption (Billon et al. 2009), local governments expenditure (Bivand and Szymanski 2000), and tax mimicking (Bordignon et al. 2003; Revelli 2002). In these models, a downward biased estimator leads to a systematic understatement of spatial responses to local shocks and, thus, weakens the empirical underpinning of economic policy advice. This paper proposes Hodges–Lehmann (HL) type interval and point estimators (Hodges and Lehmann 1983) for the spatial autocorrelation or moving average parameter in static panel models with spatially autoregressive (SAR) or moving average (SMA) disturbances. HL estimators are implemented by means of ‘inverting’ common diagnostics for spatial correlation. Confidence sets comprise all admissible values of the spatial parameter that do not involve a rejection of the null hypothesis (Dufour 1990). The particular value of the spatial parameter that maximizes the test p-value is interpreted as a point estimate (Coudin and Dufour 2011). The inversion of spatial correlation tests may follow three alternative strategies, first order asymptotic approximations, bootstrap approaches and Monte Carlo (MC) techniques (Dufour 2006). In contrast to asymptotically valid bootstrap techniques (Ren et al. 2014; Lin et al. 2010) MC testing can offer exact size control in finite samples under any spatial structure. Therefore, the corresponding interval sets have a coverage probability of the exact nominal level and provide a valid alternative to biased ML estimates. As derived from the inversion of spatial diagnostics the performance of HL estimators is most likely to reflect efficiency characteristics of the underlying spatial correlation tests. To construct HL confidence intervals (CIs) and point estimates, three alternative test statistics are considered in this study. In the first place, Moran’s I statistic (MI) (Cliff and Ord 1981) is extended to the panel regression model. Several simulation experiments recommend MI as a powerful test against spatial error correlation (e.g. Florax and Rey 1995). However, as simulation results in Anselin and Rey (1991) suggest, the power of the test declines with increasing spatial connectivity. Smith (2009) shows that for strongly connected spatial weights matrices realizations of MI are concentrated close to the mean and, thus, the test could suffer from power weakness. Generally, the application of MI and various other diagnostics requires an a-priori guess about the spatial pattern that underlies the data. Any choice of the so-called spatial weights matrix is subjected to the risk of misspecification. To raise the robustness of the HL estimation against the specification of the spatial layout, the CD test of Pesaran (2004), which does not rely on a spatial weights matrix, is also considered in the second place. For power improvement Pesaran (2004) suggests a modification of the CD statistic capturing spatial features of the regression error terms. In the third place, this statistic is also included in the list of potential candidates for HL estimation. To identify the most appropriate test statistics for the HL procedure size and power features of the three diagnostics for spatial correlation are investigated by means of a simulation study in a first step. Secondly, the empirical performance of HL interval and point estimators are compared with their ML counterparts. Noting that the actual coverage of ML interval estimates might violate the nominal target due to the estimation bias, the potential merits of HL interval estimates are highlighted for small sample scenarios with regular and irregular spatial weights matrices. Moreover, a potential finite sample bias reduction of the HL estimator is addressed. To preview some results, generally, the bias of the HL estimator based on MI is considerably smaller than its ML counterpart. With regard to finite sample coverage biases (asymptotic or MC based) HL interval estimators markedly improve the coverage of common interval estimates. In Sect. 2 the spatial panel model under the SAR and SMA error distribution is introduced along with the considered tests for spatial correlation. Section 3 outlines point and interval estimation of the spatial parameter. Furthermore, the MC test procedure is described. Section 4 documents design and outcomes of the simulation study. Section 5 concludes.",1
53.0,1.0,Computational Economics,04 September 2017,https://link.springer.com/article/10.1007/s10614-017-9735-z,The Limit of Global Carbon Tax and its Climatic and Economic Effects,January 2019,Gaoxiang Gu,Zheng Wang,,Unknown,,Unknown,Mix,,
53.0,1.0,Computational Economics,05 September 2017,https://link.springer.com/article/10.1007/s10614-017-9734-0,A Stable and Convergent Finite Difference Method for Fractional Black–Scholes Model of American Put Option Pricing,January 2019,R. Kalantari,S. Shahmorad,,Unknown,Unknown,Unknown,Unknown,,
53.0,1.0,Computational Economics,05 September 2017,https://link.springer.com/article/10.1007/s10614-017-9738-9,Observing Cascade Behavior Depending on the Network Topology and Transaction Costs,January 2019,Joohyun Kim,Ohsung Kwon,Duk Hee Lee,Unknown,Unknown,,Mix,,
53.0,1.0,Computational Economics,06 September 2017,https://link.springer.com/article/10.1007/s10614-017-9737-x,Predicting Corporate Financial Failure Using Macroeconomic Variables and Accounting Data,January 2019,Eduardo Acosta-González,Fernando Fernández-Rodríguez,Hicham Ganga,Male,Male,Male,Male,"Bankruptcy forecasting is a recurring theme in financial literature. Traditionally, bankruptcy prediction studies have sought to characterise corporate health according to financial differences among firms, measured using accounting data. Thus, various papers in the field of financial analysis have attempted to show the utility of prediction models based on firms’ annual published accounts (see Dimitras et al. 1996 for a general perspective). Although the business cycle obviously affects the state of firms, microeconomic factors were initially considered to be most responsible for financial failure, because it was held that external macroeconomic factors affected all firms equally. Many studies have sought to establish, from the statistical point of view, an empirical relation between bankruptcy and corporate accounting data, with special reference to financial ratios; this has been so since the 1960s, when discriminant analysis came into widespread use in this type of research, with Altman (1968) being a pioneer in this field. In the 1970s, linear econometric probability models and the LOGIT and PROBIT models were introduced. The LOGIT model, first used by Ohlson (1980), has come to be one of the most widely employed statistical tools for predicting the financial failure of firms, perhaps only surpassed by the use of multiple discriminant analysis (Altman 1968). Nevertheless, the LOGIT model presents certain advantages that make it clearly superior to discriminant analysis (see Balcaen and Ooghe 2004 for a general view on the advantages and inconvenience of these kinds of models). With the increasingly powerful calculating ability of computers, by the mid-1980s mathematical programming techniques were being applied to predict the failure of firms, a field in which Frydman et al. (1985) did pioneering work. Subsequently, in the 1990s, Johnsen and Melicher (1994) introduced multinomial LOGITs, and a few years later Wilson (1997) employed advanced econometric techniques based on time series and integrated LOGIT models. In addition Expert Systems (Messier and Hansen 1988), nonparametric models such as Multivariate Adaptive Regression Splines (Friedman 1991), Artificial Neural Networks (Tam and Kiang 1992), Hybrid Classifiers for combining previous procedures (Olmeda and Fernández 1997) and mixed LOGIT models (Jones and Hensher 2004), have also been proposed. Nevertheless, the wide-ranging empirical experience that has been historically accumulated shows that some of the causes of bankruptcy are macroeconomic or external. Accordingly, researchers have sought to identify other variables that may be relevant to financial failure, apart from the financial ratios. This search has given rise to a diverse body of literature, arising from Altman’s seminal 1968 paper, in which detailed economic reasoning is offered of the explanatory variables involved; another groundbreaking paper on corporate default was the theoretical study conducted by Merton (1974). Thus, a specific sub-category of research has emerged, combining internal financial information with macroeconomic information, taking into account the empirical fact that not all economic sectors and firms are equally capable of resisting a given impact during a macroeconomic crisis. Inspired by previous research into the business cycle several papers have examined cases in which variables such as profits, investment, money, credit, interest rates and assets are analysed to account for the financial failure of firms. In his seminal paper on the use of discriminant analysis in the prediction of business failure, Altman (1968) implemented a multiple regression model in which the explanatory variables included the change in GDP lagged by a quarter, the S&P 500 Index and money supply M1. In the same area, Rose et al. (1982) selected a wide variety of lagged macroeconomic variables, including the S&P 500 Index, the base interest rate, the 3-month US Treasury Bonds rate, the ratio of gross domestic private investment to GDP, and the ratio of retail sales to GDP, all of which were found to be significant. Altman (1983) employed a regression model with distributed lags, based on numerous macroeconomic variables, notably the changes in the S&P 500 Index, the formation of new entities and money supply M2 with several lags, but found the change in GDP to be nonsignificant. Fama (1986) observed signs of the influence of the business cycle on bankruptcy probability, after examining variations in quarterly premiums with the business cycle. Levy and Bar-Niv (1987) conjectured that income and the level of price fluctuation adversely affect corporate performance, and measured a positive correlation between the number of annual bankruptcies, the variance in GDP, with respect to 24 terms, and the GDP deflator; they also recorded a negative correlation with the covariance between the GDP and its deflator. Melicher and Heart (1988) used credit conditions to explain aggregate business failure, concluding that aggregate failure activity lags behind the volatility of interest rates, the cost of short-term credit (3-month T-bill) and the availability of short-term credit (free bank reserves). Lane and Schary (1989) explained the percentage of financial failure by reference to 21 macroeconomic variables, plus the age of the firm and the year in which it was founded. In this respect, too, Hol (2001) developed a model of non-performing loans for Norwegian banks, in which the following parameters were found to be relevant: the lagged GDP, the interest rate on loans, the industrial production index and money supply M1. Cressy (1992) focused on the macroeconomic effects on small-firm bankruptcy, including annual dummy variables as proxies in the macroeconomic environment. In their model, Burn and Redwood (2003) included explanatory variables such as profitability, interest coverage, indebtedness, liquidity, size of the company and the industry, growth of GDP and subsidiary dummies. Benito et al. (2004) showed that the growth of GDP in real terms and financing costs are both significant predictor variables of bankruptcy probability for Spanish firms. Hol (2007) combined financial statement analysis with an analysis of the macroeconomic environment, and concluded that GDP gap, production index and money supply M1, in combination with certain financial health indicators for individual firms, were significant predictors of default for Norwegian firms during both recovery and expansion in the 1990s. Another important model was proposed by Duffie et al. (2007), in which distance-to-default (a volatility adjusted measure of leverage) was shown to be almost a sufficient statistic for accounting variables. Besides, there are more recent studies using more powerful prediction approaches; in this sense it is worth mentioning, among others, the following papers and the references therein: Hernandez-Tinoco and Wilson (2013), which show the utility of combining accounting, market and macro-economic data to explain corporate credit risk; Lua et al. (2015), on the effects and applicability of financial media reports on corporate default ratings; Fejér-Király (2015), which surveys the historic evolution of Bankruptcy Prediction; Mousavia et al. (2015), that study the performance evaluation of competing bankruptcy prediction models in a multi-criteria assessment framework; Kumar and Rao (2015), proposing to improve the prediction accuracy of Altman’s Z-score by developing a multivariate nonlinear model for computing it; Tkáč and Verner (2016), that surveyed the recent research of Artificial Neural Networks in business, especially the one aimed at financial distress; Bauweraerts (2016), which investigates the relative importance of various bankruptcy predictors commonly used; Altman et al. (2016), that assesses the classification performance of the Z-Score model in predicting bankruptcy with the goal of examining the model’s usefulness for all parties, especially banks that operate internationally and need to assess the failure risk of firms. All of the above models, both those that employ information on the accounting status of the company and also mixed models that combine accounting and macroeconomic factors, suffer from multicollinearity problems in their estimations, due to the redundancy of the information provided by the variables. In this paper, we implement an econometric model, which reacts well to the presence of multicollinearity, for predicting bankruptcy in the Spanish construction sector, with particular reference to the effects of the most recent episodes of the economic crisis. Our methodology is guided solely by data and incorporates two kinds of the above-mentioned explanatory variables: financial ratios on the accounting status of the companies concerned (as analysed previously in other studies) and macroeconomic variables. Among the macroeconomic variables considered are the interest rate term structure, the volatility of stock markets, the country risk premium, the inflation level, the unemployment rate, credit fluctuations and the default rate. The remainder of this paper is organized as follows. Section 2 introduces the methodology used for selecting the optimal set of bankruptcy explanatory variables, describing a procedure for selecting variables in a LOGIT model via a genetic algorithm; this section also considers the multiple imputation methodology developed by King et al. (2001) as a solution to the missing values problem in financial failure data bases; in addition, the data base used is described. Section 3 then shows the empirical results for the Spanish construction industry data base. Finally, Sect. 4 presents the main conclusions drawn.",12
53.0,1.0,Computational Economics,07 September 2017,https://link.springer.com/article/10.1007/s10614-017-9739-8,Radial Basis Functions with Partition of Unity Method for American Options with Stochastic Volatility,January 2019,Reza Mollapourasl,Ali Fereshtian,Michèle Vanmaele,,Male,Female,Mix,,
53.0,1.0,Computational Economics,07 September 2017,https://link.springer.com/article/10.1007/s10614-017-9731-3,Interactional Effects Between Individual Heterogeneity and Collective Behavior in Complex Organizational Systems,January 2019,Xingguang Chen,Zhentao Zhu,,Unknown,Unknown,Unknown,Unknown,,
53.0,1.0,Computational Economics,09 September 2017,https://link.springer.com/article/10.1007/s10614-017-9733-1,Solving Deterministic and Stochastic Equilibrium Problems via Augmented Walrasian,January 2019,Julio Deride,Alejandro Jofré,Roger J-B Wets,Male,Male,Male,Male,"The economic equilibrium model proposed by Arrow and Debreu (1954) for a competitive economy implicitly assumes that the entire economic activity will take place in a single time span, implicitly instantly. As soon as one includes the agent’s concerns about the future, one has to consider an inter-temporal component, intrinsically dynamic, and take into account the uncertainty about this future. Here, our primary goal is to design numerical procedures to solve (two-stage) stochastic Walras equilibrium models where goods get transferred from time 0 to time 1 via “home production;” clearly, this will usually include the possibility of retention. The overall approach to this stochastic model is based on a fundamental decomposition-result that allows us obtain the overall equilibrium by spliting the calculation into one that deals with each state separately. Consequently, our first concern is with an algorithmic procedure that will deliver, rather efficiently, an equilibrium solution for a classical exchange model, i.e., in a static, deterministic environment. As a stepping stone to our stochastic model, we consider then, a two-stage (dynamic,deterministic) Walras model that allows for the transfer of goods between time 0 and time 1 via home production and we exploit its particular structure to streamline the computational procedure. We rely systematically on an augmentation method applied to what’s called the Walrasian: essentially, the function ‘supposedly solved’ by the Walrasian auctioneer and a maxmin characterization of an equilibrium point. The fact that the theory allows us to proceed, in the iterative process, with approximate equilibria turns out to be critical in the development of the overall numerical scheme. Our approach deviates, even in the deterministic case, from the path-breaking methods suggested by Scarf and Hansen (1973), Eaves (2011), Brown et al. (1996), Saigal (1983), and other approximation strategies described in the books by Judd (1998), and Brown and Kubler (2008). These earlier methods are not efficient when the economies have a significant number of goods or agents, even for reaching approximate equilibria, as their solution strategies rely on an enumerative argument, and smoothness of the excess demand function. Moreover, in stochastic environments, these results are prohibitively time-expensive. In this paper, we develop an approach based on an augmented Walrasian technique and a lopsided convergence approximation procedure, which allows us to cope with large equilibrium problems including uncertainty and heterogeneity on the agents. By using this approach we have designed a two-phase algorithm without computing derivatives of the demand function. We report several numerical experiments for equilibrium problems involving up to 5 agents, 7 goods and 10 stochastic scenarios, which can be easily expanded in the number of agents and goods. Finally, the procedure proposed in this paper might be parallelized in terms of agents and the multi-start strategy. The paper is organized as follows. In Sect. 2 we review the general equilibrium problem for a pure-exchange economy, setting notation, and definitions. In Sect. 3, we provide the foundation of our new approach, stating the equilibrium problem as an optimization problem of the maxinf family, and developing the augmentation technique to construct a solution algorithm. This section ends with some numerical examples of pure-exchange economies. Section 4 is a stepping stone in the economies considered in this manuscript, where the agent problem is given by a two-stage deterministic optimization problem, with home production. In Sect. 5 we proposed our final economic model, a two-stage stochastic exchange economy, where agents face uncertainty about the second stage of the economy. We provide the maxinf characterization, as well as a solution algorithm based in the augmented Walrasian previously defined. Finally, Sect. 6 concludes and provides some remarks about the numerical performance of our algorithm, as well as some prospective future lines of research.",6
53.0,1.0,Computational Economics,13 September 2017,https://link.springer.com/article/10.1007/s10614-017-9743-z,How Strong is the Relationship Among Gold and USD Exchange Rates? Analytics Based on Structural Change Models,January 2019,Manh Cuong Dong,Cathy W. S. Chen,Songsak Sriboonchitta,,Female,Unknown,Mix,,
53.0,1.0,Computational Economics,19 September 2017,https://link.springer.com/article/10.1007/s10614-017-9750-0,A New Characterization of Equilibrium in a Multi-period Finance Economy: A Computational Viewpoint,January 2019,Dong Chul Won,,,,Unknown,Unknown,Mix,,
53.0,1.0,Computational Economics,20 September 2017,https://link.springer.com/article/10.1007/s10614-017-9749-6,On Optimal Pricing Model for Multiple Dealers in a Competitive Market,January 2019,Qing-Qing Yang,Jia-Wen Gu,Tak-Kuen Siu,,,Unknown,Mix,,
53.0,1.0,Computational Economics,02 June 2018,https://link.springer.com/article/10.1007/s10614-018-9825-6,Stress Testing for Retail Mortgages Based on Probability Analysis,January 2019,Chang Liu,Raja Nassar,,,,Unknown,Mix,,
53.0,1.0,Computational Economics,06 July 2018,https://link.springer.com/article/10.1007/s10614-018-9834-5,The Complexion of Multi-period Stackelberg Triopoly Game with Bounded Rationality,January 2019,Yu Yu,Weisheng Yu,,,Unknown,Unknown,Mix,,
53.0,2.0,Computational Economics,20 September 2017,https://link.springer.com/article/10.1007/s10614-017-9751-z,Opinion Formation with Imperfect Agents as an Evolutionary Process,February 2019,Matjaž Steinbacher,Mitja Steinbacher,,Male,Male,Unknown,Male,"In the present paper, we examine the question of opinion formation in an integrated society of heterogeneous and interacting individuals. The importance of the topic stems from the fact that what people think determines the choices that they make in their everyday life. This may refer either to economics, finance, game theory, buying decisions, marketing and product recommendations, politics and voting, urban growth, norms. In a decentralized society which does not have a social planner to direct others’ behaviors, opinions are generated spontaneously by repeated local interactions between the large number of participants, we call them agents, and the feedback influences from the macrostructure onto the agents’ behaviors. It has been shown that the interacting society can produce very diverse outcomes. Even extreme and marginal views can prevail in certain conditions, including the outcomes that are considered undesired and condemned (Glaeser et al. 1996; Schelling 1971). Economists have been for a long interested in how the interaction influences the individuals’ opinions and how this transforms into the opinion dynamics within the society. It has been found that the interacting society leads to the adoption of widespread opinions, directing the society towards consensus. In fact, the ability of autonomous and interacting individuals with different priors to reach consensus is remarkable and has been thoroughly studied (Bala and Goyal 1998; Banerjee and Fudenberg 2004; Baronchelli et al. 2008; DeGroot 1974; Gale and Kariv 2003; Golub and Jackson 2010; Hegselmann and Krause 2005; Motsch and Tadmor 2014; Olfati-Saber and Murray 2004; Vicsek et al. 1995). Over the years, the question of consensus has become a central concern within the research community. However, the complex and dynamic large-scale societies exhibit much richer behavioral structures. In fact, the notion of consensus contrasts with empirical facts and can be considered an idealization of the reality and a theoretical construct, while disagreement is a norm in practice.Footnote 1 For instance, individuals regularly disagree on political agendas, movies, the cast, prefer some restaurants and places to others, have opposing views to the superstars, etc. One might mistakenly believe that disagreement is caused by the asymmetric and incomplete information alone, because it is present when individuals are exposed to the same data or news (Hirshleifer 2001). Some disagreement may be induced by the people’s attitude towards the opposing views, because some may have more firm beliefs than others. Anyway, disagreement in a society can be expressed either in the form of multiple opinion clusters or as a perpetual opinion dynamics. Recent research has demonstrated that stable diversity of opinions can be attained when bounded confidence or stubborn agents with fixed opinions are introduced (Deffuant et al. 2000, 2002; Weisbuch et al. 2002). Though very intuitive, these attempts cannot properly describe behavioral patterns on both, the individual and the aggregate level, that are induced by the joint influences of agents’ specifics and the interrelationships between the microscopic processes which both are complex. One aspect that has not received any attention yet is the agents’ sincerity in information-sharing, which is always relevant when agents refer to human beings. Relatedly, when agents update their opinions upon the opinions of others, they may contact an insincere counterpart and then make a decision upon the insincere opinion. In the large-scale societies with random meetings, insincere agents would be very hardly identified and then avoided. The interacting agents could thus not know whether the opinion they received was agents’ genuine or not. If it is not, the insincere opinion may be incorporated into their updated opinions and through interaction subsequently transferred further onto their counterparts, who then pass it over to their counterparts and so on across the society. Similar is the case with the stubborn agents who do not change their priors. Furthermore, it is well known from the theory of complex systems that even a slight modification of a single parameter may shape the process, inducing the macroscopic shifts. Multi-agent simulation based methods are used to examine the models of opinion dynamics principally because they allow for the mutual and causal influences of the micro-elements on the development of the complex system, which develops in a nonlinear way (Tesfatsion and Judd 2006). To explain the very diverse opinion structures within the interacting and interconnected society, we extend the bounded confidence model of continuous opinion formation. In particular, our goal is to improve our understanding of how the process of opinion formation goes through the phase transitions as the circumstances and the network structures are modified. More specifically, we would like to identify the conditions that lead a complex society into either a consensus, multiple opinion clusters or a perpetual opinion dynamics, and examine the evolutionary dynamics. The model is based on the principal assumption that there is no true opinion towards which agents would like to converge.Footnote 2 The society is modeled as a social network that consists of a large number of single agents who are represented as nodes and connected with one another with a set of links through which the opinions are regularly exchanged. Since reciprocal person-to-person communication is presumed, we use undirected links. A direct implication is that all of the interacting individuals can potentially affect one another. More specifically, the updating rule is conducted upon the difference in the two opinions and the adoption rate. The principle of bounded confidence implies that an agent will change his opinion if the opinion of a selected counterpart differs from his by less than his threshold level. The threshold level can be interpreted as the individuals’ tolerance to the opposing opinion and along with the adoption rate represents the crucial parameter in an agent’s choice function. Analogously, tolerance is interpreted as the agent’s willingness to consider the counterpart’s opinion. The adoption rate determines how much of the opponent’s opinion an agent will integrate into his priors. By using the continuous opinions, an agent can take only a slight portion of the counterpart’s opinion and the communication can end in an opinion which has not been present before. This would not be possible if we used discrete opinions which presume a fixed number of predetermined states. We compare the results on several different network topologies: the complete network, which considers the society an integrated entity in which everyone can meet and share his opinion with anyone, small world network (Watts and Strogatz 1998) which presume local clustering, and the scale free network in the sense of Barabási and Albert (1999). We begin with a very general setting with only regular agents and show that a widespread consensus can be reached in even a moderately tolerant society, while there cannot be a persistent opinion dynamics if regular agents have homogeneous threshold levels. A less tolerant society will end up in several opinion classes. This also holds true in the small world network and the scale-free network. Here we show that the network topology plays an important role for the process of opinion formation. In particular, the small world networks have proved to be the most effective in bringing the society to very high degree of consensus, while higher threshold levels than in the complete network are required to reach consensus. Subsequently, we extend the model by introducing a subset of stubborn agentsFootnote 3 with extreme opinions and show the susceptibility of even a moderately tolerant society to extremism. Here, we have identified such a tolerance span where the higher tolerance is offset with less extremism in society. When another group of stubborn agents is added on the opposing extreme, the group of regular agents can end either in consensus, multiple opinion clusters or even in the perpetual opinion dynamics. In the latter case, no regular agent is the extremist. In the last part of the paper, we examine the cases with a subset of inconsistent agents, who report a random opinion to the selected counterparts and show that their influence is persistent.Footnote 4 Important for the behavior of the model is the assumption that stubborn and insincere agents cannot be identified and avoided and that the regular agents’ threshold levels and opinions are also not observable to the counterparts. Our most important contribution is to show the phase transitions that are network-based and related to different tolerance levels and the social structure. We also show that the model is non-equilibrium and that consensus, if attainable, can be reached in a finite time. The remainder of this paper is organized as follows. Section 2 discusses related literature. The model is described in Sect. 3. In Sect. 4, we examine the model with regular agents and conduct several simulations on different network topologies. In Sect. 5, the model includes a portion of stubborn counterparts with the extreme opinion. In Sect. 6, we add additional group of the stubborn agents located on the other extreme. Section 7 examines the model with inconsistent agents. The last section concludes.",3
53.0,2.0,Computational Economics,21 September 2017,https://link.springer.com/article/10.1007/s10614-017-9753-x,Pricing Swaps on Discrete Realized Higher Moments Under the Lévy Process,February 2019,Wenli Zhu,Xinfeng Ruan,,Unknown,Unknown,Unknown,Unknown,,
53.0,2.0,Computational Economics,22 September 2017,https://link.springer.com/article/10.1007/s10614-017-9752-y,What Types are There?,February 2019,Sam Cosaert,,,,Unknown,Unknown,Mix,,
53.0,2.0,Computational Economics,22 September 2017,https://link.springer.com/article/10.1007/s10614-017-9754-9,Option Pricing Under a Stochastic Interest Rate and Volatility Model with Hidden Markovian Regime-Switching,February 2019,Dong-Mei Zhu,Jiejun Lu,Tak-Kuen Siu,,Unknown,Unknown,Mix,,
53.0,2.0,Computational Economics,25 September 2017,https://link.springer.com/article/10.1007/s10614-017-9748-7,Multi-scale Economic Dynamics: The Micro–Macro Wealth Dynamics and the Two-Level Imbalances of the Euro Crisis,February 2019,Hanchao Yang,Chenjie Shao,Khaldoun Khashanah,Unknown,Unknown,Male,Male,"The main contribution in this paper is to find out the relationship between foreign direct investment (FDI), corporate competitiveness, and economic growth and be able to assess emerging imbalances in the FDI compositions over time and how it may impact GDP growth. We take the Eurozone as an exemplar by analyzing the market share data of the retail industry of 13 largest Eurozone countries as a system. For some OECD (Organisation for Economic Co-operation and Development) countries, FDI might increase competition in the host country by reducing the market power of local firms, especially when the local financial markets are so weak that local firms are unable to take advantage of the new skills that they gain from the presence and competition with foreign firms. Our results are in agreement with Artus (2011) namely, that there exists a two-tier Eurozone. Moreover, we add a level of distinction between the two levels of imbalances: one is at the country’s level (or component level) and the other is at the systemic level. As a result, our model provides clarity to those hidden imbalances and feedback effects between micro and macro economic classical studies on international competitiveness and economic growth and introduces a new way to describe multi-scale economic dynamics. The foreign direct investment (FDI) refers to a direct controlling ownership in the production or acquisition of business enterprise by an investing entity in another country. It is viewed as the antithesis of indirect investment accomplished via accumulating positions in stocks and bonds owned or issued by the host country. FDI has been increasingly regarded as a vital way for resources to flow across national borders to improve economic performance and international competitiveness. The enormous increase in FDI flows across the organization for economic cooperation and development (OECD) countries is one of the clearest signs of the monetary union of the Eurozone economy over the past 15 years. Total FDI flows in Eurozone countries increased from about US $44 billion 1985 to US $1706 billion before falling back to around US $656 billion in 2013 (World Bank 2014). Current research on industrial competitiveness and economic growth can be classified into two separate areas: one is the measurement of the foreign direct investment (FDI) effects on a country’s GDP from a macroeconomic perspective and the other is the competitiveness between the performance of domestic and foreign companies in a targeted industry from a microeconomic perspective. The question of the existence of a causality relationship between FDI and GDP growth becomes intensively relevant. Ericsson and Irandoust (2001) found that a causal relationship between FDI and GDP growth existed in Sweden. Chakraborty and Basu (2002) did a causality test on India and concluded that causality ran from real GDP to FDI. Recently, Roy and Mandal (2012) showed that the causality ran unidirectionally from economic growth to FDI in Asian countries such as China, India, and Pakistan. For ten transitional Euro Union countries, Acaravci and Ozturk (2012) concluded that a causal relationship among FDI export and economic growth was recognized in four out of the ten countries. The dynamic interaction between FDI and GDP provides another perspective for our study. De Grauwe (2012) suggested the existence of inherent fragility of a monetary union in which national governments issue debt in a currency over which they exert no control. Under such circumstance, if the fiscal position of a country deteriorates, fearful investors will have a collective movement of distrust resulting in a liquidity squeeze and hence high interest rates, deepening the divergence of competitive positions. De Grauwe (2012) found that the divergence of the competitive positions led to major imbalances in the some Eurozone nations such as Portugal, Italy, Ireland, Greece and Spain, resulting in the accumulation of large current account deficits and external indebtedness. However, countries like Germany had current account surpluses of the countries that have improved their competitive positions. We find a few studies that address the micro–macro economic interaction that includes the coupling effects. Therefore the innovation of this paper is to provide a model for investigating the relationship between GDP growth and FDI, by continuously coupling between the micro and macro levels of competitiveness and growth. This model describes the relationship between domestic and foreign companies at the microeconomic level and explains how they contribute to GDP at the macroeconomic level, using the market share data of the retail industry of 13 largest Eurozone countries. This approach is unique in its attempt to dynamically relate microeconomic corporate level interactions with macroeconomic indicators in one system of differential equations. The paper is organized as follows: literature review of FDI and firm-level competitiveness and economic growth comes in the Sect. 2. In Sect. 3 the theoretical model is introduced along with essential stability analysis. In Sect. 4 the data set and methodology are described and the empirical results are presented. Section 5 presents our conclusions.",1
53.0,2.0,Computational Economics,26 September 2017,https://link.springer.com/article/10.1007/s10614-017-9736-y,Groupon and Groupon Now: Participating Firm’s Profitability Analysis,February 2019,Jenn-Bing Ong,Wee-Keong Ng,Thanh-Nghia Ho,Unknown,Unknown,Unknown,Unknown,,
53.0,2.0,Computational Economics,04 October 2017,https://link.springer.com/article/10.1007/s10614-017-9756-7,Estimation of Overall Returns to Scale (RTS) of a Frontier Unit Using the Left and Right RTS,February 2019,Mostafa Omidi,Mohsen Rostamy-Malkhalifeh,Farhad Hosseinzadeh Lotfi,Male,Male,Male,Male,"Data envelopment analysis (DEA) is a nonparametric method based on mathematical programming which is commonly used for evaluating the performance of Decision Making Units (DMUs). Charnes et al. (1978) used the term DEA in their first article to describe a new method for calculating the relative efficiency of DMUs. Returns to scale (RTS) is an important concept in economics that its different aspects has been studied by DEA. In fact, determining a DMU’s RTS type and estimating its value provide useful information about the optimal size of the evaluating DMU for managers. Given the importance of RTS in managerialdecision-makings, a variety of different methods have been presented by DEA researchers to estimate the RTS value. The CCR model (Charnes et al. 1978) and the BCC model (Banker et al. 1984)—with constant RTS and variable RTS, respectively- are the best known and most frequently used radial models in DEA literature. Using the concepts of “Most Productive Scale Size (MPSS)” and “sum of the CCR optimal lambda values”, Banker (1984) introduced the concept of returns to scale. They provided a method called CCR-RTS for RTS classification. Subsequently, Banker et al. (1984) introduced a RTS evaluation method called BCC-RTS, where the sign of “free BCC dual variable” acts as the basis of RTS classification. Later, Fare et al. (1985) used the efficiency index method for determination of RTS. In their method, evaluation of efficiency is based on three intertwined frontiers called constant, non-increasing, and variant RTS (CRS, NIRS and VRS). Another related method is the technique proposed by Tone (1996), where RTS of units is determined by classification of reference set. Three methods CCR-RTS, BCC-RTS and “efficiency index” which are known as base RTS methods are equivalent, based on studies of Banker et al. (1996), Fare and Grosskopf (1994) and Seiford and Zhu (1999). After the application of these three methods in real-world problems (see Byrnes et al. 1984; Charnes et al. 1989; Zhu 1996), it was found that both CCR-RTS and BCC-RTS suffer from the same problem: inconsistency in judgment. This problem may occur when the corresponding DEA models have multiple optimum solutions. To resolve this problem, these two methods were subject to some modifications. For example, Banker and Thrall (1992) used “minimum and maximum values of free BCC dual variable” to modify BCC-RTS. Later, Zhu and Shen (1995) proposed a modification for CCR-RTS, and then Banker et al. (1996) determined “the minimum and maximum of sum of the CCR optimal lambda values” to present a modification for CCR-RTS. Finally, Seiford and Zhu (1999) investigated advantages and drawbacks of three base RTS methods. They proposed some of modifications, and showed that the RTS evaluation method fitting for different applications can be selected among these three methods. In continuance of these studies, Soleimani-Damaneh et al. (2010) investigated the RTS units in presence of weight restrictions. Researches of Golany and Yu (1997) in determining the type of RTS has led to a more general Classification of RTS types, called “left returns to scale (L-RTS)” and “right returns to scale (R-RTS)”. They defined the “constant, increasing and decreasing” categorization for L-RTS (as well as for R-RTS). In fact, the L-RTS and R-RTS contain more accurate and complete information than the RTS, especially for units whose RTS type is constant. Jahanshahloo and Soleimani-Damaneh (2004) proposed a method for improving the one proposed by Golany and Yu (1997). In this context, Hadjicostas and Soteriou (2006) provided theoretical foundations for defining left and right returns to scales. Later, Eslami and Khoveyni (2013), and Allahyar and Rostamy-Malkhalifeh (2014) evaluated the left and right returns to scales of DMUs by using DEA multiplicative and envelopment models. The method of Golany and Yu (1997) has sought to determine the type of left and right RTS by using specific models of DEA envelopment forms, but in some cases their models are infeasible. Models provided by Eslami and Khoveyni (2013) and Allahyar and Rostamy-Malkhalifeh (2014) always are feasible and do not have the infeasibility issue as the model of Golany and Yu (1997). In other hands, models proposed in the mentioned articles cannot differentiate frontier units from other units; so, these methods have used other models to do this task. In this situation, we need to solve 4–6 models for determining L-RTS and R-RTS. Now in the article ahead, a method is presented for evaluation L-RTS and R-RTS that the models of this method do not have the weakness of Golany and Yu (1997) models infeasibility. Furthermore, contrary to models of Eslami and Khoveyni (2013) and Allahyar and Rostamy-Malkhalifeh (2014), the models of the proposed method are also able to identify the frontier units. One of the common features of Banker and Thrall (1992), Golany and Yu (1997), Eslami and Khoveyni (2013) and Allahyar and Rostamy-Malkhalifeh (2014) researches is that the RTS has been evaluated with the help of strong efficient frontier of production possibility set (PPS). In contrast, the present paper tries to assess this, in a more general context called “the whole PPS frontier”, including weak and strong efficient frontier. Therefore, one of the concerns of this article is evaluation RTS frontier units, without needing to project them on a strong efficient frontier. In the proposed method, only by solving two DEA models, frontier units are identified, and their L-RTS and R-RTS types are also determined. Based on this evaluation, we present a method to determine returns to scale of frontier units which in this paper is called overall returns to scale (O-RTS). We have shown that the results of this assessment are consistent with the results of the Banker and Thrall (1992). The rest of this paper is organized as follows: Sect. 2 briefly describes some RTS estimation methods along with some related concepts. In Sect. 3, a theoretical framework is provided for identification of left and right RTS of frontier units. This framework is then used to propose an alternative method for RTS evaluation. Section 4 presents two numerical examples to assess the performance of the proposed method. And finally, Sect. 5 presents the conclusions and suggests possible avenues of future research.",2
53.0,2.0,Computational Economics,05 October 2017,https://link.springer.com/article/10.1007/s10614-017-9758-5,Identification in Models with Discrete Variables,February 2019,Lukáš Lafférs,,,Male,Unknown,Unknown,Male,"Identification plays a central role in economic research. In most economic models, we introduce latent variables, such as unobserved heterogeneity, ability, or preference shocks, to explain relations of interest, such that the model best mimics reality. Given data that reveal the distribution of observable variables, we would prefer to learn as much as possible about the relations or features of the economic model, information often embedded in an unknown parameter. Unfortunately, as latent variables are not observable, we need to make certain assumptions about them in order to use data to say something about an unknown parameter (vector) or some feature of interest. Depending on the strength of these assumptions, knowledge of the true data-generating process for the observed variables can then be any of the following: (1) no identifying power, (2) a contraction of the set of potential parameter candidates, such that the model is partially identified, (3) the assumptions are sufficient to identify one potentially true parameter, such that the model is point identified, or (4) the assumptions are too strict and the model can be refuted. In practice, we often require strong assumptions to guarantee point identification. However, such assumptions could include knowledge of the family of probability distributions of unobserved variables, information we can rarely justify on economic grounds. The only reason is to make inference tractable. It is then interesting to question what would happen had these restrictions not been imposed, and then attempt to develop an inferential procedure that is robust with respect to assumptions that are sometimes controversial or made purely for technical convenience. The first necessary step is to know what set of models (or parameters) are compatible with both the set of assumptions made and the data in situations where we have perfect information on the probability distribution of observable variables, that is, where our data sample is of infinite length. This is the question of identification. Once this is resolved, we can proceed to inference and identify how to use imperfect data to construct confidence regions or hypothesis tests. The contribution of this paper is threefold. First, we present a new simple identification method. Second, we show how this method nests several existing results from the literature. Third, we show how this method approaches identification in cases when the strict exogeneity of instruments is relaxed. The main advantage over the existing literature is that the economic model is not restricted to the linear form, while at the same time controlling for the degree of violation of the exogeneity assumption. This paper presents a new method as an extension of an existing framework by Galichon and Henry (2009, 2011) and Ekeland et al. (2010) (henceforth, the GH framework) that traces the identified set in a richer set of economic problems when the observed variables are discrete. As a motivating example, we consider the impact of a violation of the strict exogeneity assumption in a single-equation endogenous binary response model. By complementing existing results on imperfect instruments in Nevo and Rosen (2012) and Conley et al. (2012), this method can control for departures from the strict exogeneity of the instrument, and permits us to study nonlinear models. The proposed method is also able to reproduce some other results in the partial identification literature obtained using different approaches. These include the single-equation endogenous binary response model in Chesher (2009, 2010), the triangular system of equations with binary dependent variables in Shaikh and Vytlacil (2011), treatment effects in studies with imperfect compliance as in Balke and Pearl (1997), and binary choice models with zero-median restrictions as in Komarova (2013). In the first and fourth examples, the original GH frameworkFootnote 1 also applies, but our extension helps us to formulate the problem in such a way that it is possible to relax the strict exogeneity of instruments more simply, as in Sect. 4. In the remaining examples, the extension is essential, as we cannot formulate some of the assumptions made within the original GH framework. The present extension therefore enriches the set of problems we can address. The major advantage of this new method is its algorithmic structure: that is, the identifying restrictions enter the setup in a straightforward manner and it employs effective algorithms to determine the identified set. Instead of using distinct strategies for different applications, this method thus provides a unifying framework that is conceptually simple. As the framework presented is not application specific, it thus applies to a wide range of problems including discrete variables when identification is only partial. Of course, we also recognize several limitations of the proposed method. First, the method describes how we find the identified set given perfect information on the data-generating process of the observed variables, yet we do not consider statistical inference here. Second, we restrict the observable variables in the model to be discrete. While we can discretize models with continuous observable variables, this will always bring about some degree of arbitrariness in the problem, and we do not consider the impact of this here. If the parameter of interest is scalar, one can follow the approach of Lee (2009) in the context of constructing bounds on average treatment effects (ATE) under sample selection; in order to make use of exogenous covariates to get sharper bounds on ATE, one can split the sample to a few groups based on these covariates and then take average of ATE across them. However, for a certain class of problems, we do not restrict the unobservable variables to be discrete, we can then proceed to transform a continuous unobservable variable into a discrete form, and we show that this will not affect the identified set. While the class of problems, where the discretization proposed in this paper can be employed, is rich enough to account for many relevant models, there are models that need to be approached in a different way. As an example, consider the linear model \(Y = X\theta + U\) with zero conditional mean assumption \(E(U|X) = 0\). We note that there might exist another way of discretizing U in this model, but it is not considered in this paper. Third, the practical limitation of the proposed method stems from the fact that computational complexity of the linear program that helps us to trace the identified set grows exponentially with the size of the support of the observable variables. It is therefore computationally challenging to consider models with a large number of variables or with variables with a rich support. There are several points that are worth stressing when it comes to models with continuous observed variables. Firstly, the fact that we focus on models with discrete variables is to some extent restrictive, but it also may be considered as an advantage, given that most economic variables are, in fact, discrete (e.g. yearly wages rounded to nearest thousands). With finer discretization we achieve results with an arbitrary precision. Secondly, If we use the presented method to study the impact of a relaxation of certain identifying assumptions in a model with a known analytical solution, we suggest to investigate the impact of the discretization on this solution. We may expect the discretization error to be of similar order in the model with relaxed assumptions. Consider an example where the parameter of interest is point identified under certain parametric assumptions. We wish compare results based on different discretizations of a continuous variable with the result where the variable is not discretized. It is prudent to base our decision about how fine the discretization should be on a series of simulations with different data-generating processes. Thirdly, it is also possible to assume that the probability distribution comes from some (sufficiently rich) parametric family of distributions. But such assumption brings extra information that may have an effect on the identified set. This makes this approach less appealing as the attractive feature of partially identified models is the fact the results do not hinge on artificial parametric assumptions. Forthly, another alternative to work with continuous variables is to use multidimensional spline interpolation, so that the unknown continuous probability distribution function would be represented by a finite number of points on a grid. This approach would raise questions on how to choose the grid, the type of the spline or the amount of smoothing. The researcher’s would be, therefore, left with another (undesired) degrees of freedom. This paper also contributes to the literature on partial identification that make use of a linear programming (LP) tool. Balke and Pearl (1997) used LP to derive an analytic formula for sharp bounds on average treatment effects with binary treatment and outcome under imperfect compliance. In Honoré and Tamer (2006), identified set is obtained using LP in the context of panel dynamic discrete choice models with discrete covariates. There are other papers that make use of LP as a convenient tool to address identification in problems with counterfactual choice probabilities (Manski 2007), semi-and nonparametric regression models (Chiburis 2010; Freyberger and Horowitz 2015), or average treatment effects (Laffers 2013, 2015; Huber et al. 2017). The present paper differs from all these papers, because none of them requires discretization of the unobserved component. 
Manski (1990) initiated the study of partial identification. However, these ideas were not fully appreciated at first. Early studies include Manski (1995, 2003), with useful surveys of this literature by Manski (2008) and Tamer (2010). Among the many interesting applications, the most notable include work on the returns to schooling (Manski and Pepper 2000), the demand for fish (Chernozhukov et al. 2009), and discrete choice with social interactions (Brock and Durlauf 2001). Determination of the identified set is examined in Galichon and Henry (2009, 2011) by means of an optimal transportation formulation, in Beresteanu and Molinari (2008), Beresteanu et al. (2011, 2012), and Chesher et al. (2013) using random set theory, and in Chesher (2010) using structural quantile functions. Readers interested in statistical inference in the partially identified setting are directed to Galichon and Henry (2009, 2011), Chernozhukov et al. (2007, 2013), Imbens and Manski (2004), Beresteanu and Molinari (2008), Beresteanu et al. (2012, 2011), Andrews and Shi (2013), Romano and Shaikh (2010), Bugni (2010), and Rosen (2008). The subsampling method presented in Romano and Shaikh (2010) is computationally intensive, yet directly applicable for the linear program formulation presented in this paper. Note that, however, there is a practical challenge of choosing a suitable subsample size. The remainder of the paper is structured as follows. Section 2 describes the identification strategy in GH using the proposed extension. In Sect. 3, we provide examples of how the extended framework can nest the different identification approaches. Section 4 explains how we can modify one of the examples to consider the impact of imperfect instruments and presents it on an empirical illustration. Section 5 concludes the paper and the appendices provide the proofs (“Appendix A”), technical details on the examples presented (“Appendix B”) and implementation issues (“Appendix C”).",4
53.0,2.0,Computational Economics,10 October 2017,https://link.springer.com/article/10.1007/s10614-017-9757-6,A Numerical Algorithm for the Coupled PDEs Control Problem,February 2019,Gonglin Yuan,Xiangrong Li,,Unknown,Unknown,Unknown,Unknown,,
53.0,2.0,Computational Economics,23 October 2017,https://link.springer.com/article/10.1007/s10614-017-9761-x,Trade Costs and Endogenous Nontradability in a Model with Sectoral and Firm-Level Heterogeneity,February 2019,Manoj Atolia,,,Male,Unknown,Unknown,Male,"This paper seeks to add a new insight to the simple and clever ways that have been suggested and currently being used in the literature to incorporate sectoral and firm-level heterogeneity to analyze issues in international trade and macroeconomics. Eaton and Kortum (2002) and Melitz (2003) incorporate firm-level heterogeneity in productivity to analyze trade flows. Similar efforts have been made to find answers to some of the basic questions facing open economy macroeconomics. Bergin and Glick (2007a, b, 2009) introduce sectoral heterogeneity in trade costs and productivity to analyze macroeconomic implications of endogenous nontradedness. In addition, Ghironi and Melitz (2005) have used the setup in Melitz (2003) to address the same issue. Following the seminal contribution of Dornbusch et al. (1977), all of these papers have bought analytical tractability by utilizing the ‘power of the continuum’ to simplify analysis. In particular, these papers incorporate heterogeneity in one dimension. However, for many issues of interest it would be of importance to incorporate heterogeneity both across sectors and across firms within sectors. For example, consider the issue of endogenous nontradability. The focus in Bergin and Glick is on the cross-sectoral variations in tradability, whereas in Ghironi and Melitz (2005) it is on the within-sector determination of “tradedness” based on firm-level variations in productivity. Both strands of literature recognize the complementary nature of their approach but no attempt has so far been made to incorporate both elements of nontradedness in a single tractable model. The paper takes a first step in the direction of incorporating heterogeneity in both dimensions in a tractable manner that is fairly general and amenable to use in many other situations. The tractability of the approach relies on the key insight of this paper which avoids the curse of dimensionality and the increase in complexity of numerical computation of the equilibrium, compared to existing models with heterogeneity in one dimension. This insight, which allows such gain in tractability, again exploits the power of the continuum. Dornbusch et al. (1977) sidestepped the knotty problem of the marginal good separating the nontradables and tradables (or importables and exportables) by going from a large number of goods to a continuum of goods, where the marginal good can be nontraded or traded without affecting the equilibrium. Further, by assuming relative productivity to be a continuous function of the relevant goods index, it simultaneously provided an exact link (via an equality) between prices of the traded and nontraded goods. To see how this idea extends naturally to two dimensions, consider a small economy model in which productivity varies across firms (varieties) and trade costs vary across sectors (goods) in a continuous manner with the respective indices. This is a generalization of the set up in Bergin and Glick (2009), who only consider sectoral differences in trade costs (based on Hummels 1999, 2001), to include firm-level productivity differences analyzed by Melitz (2003). This is also an empirically relevant case to consider following the empirical evidence on substantial productivity differences within narrowly defined industries. In this set up, given equilibrium prices, for each sector there is a marginal nontraded variety and all varieties with productivity higher than this marginal variety are exported. Further, as trade costs vary continuously with sectoral indices, so does this marginal traded variety. More importantly, it is possible to derive an analytical expression for this relationship between a sector’s index and the index of its marginally nontraded variety which we call the marginal variety frontier. This analytical characterization of marginal variety frontier [see Eq. (27)] considerably simplifies the computation of the equilibrium. For example, to solve for the steady state, all one needs to do is to solve one nonlinear equation in one unknown [see Eq. (41)], just as in the model without firm-level heterogeneity in Bergin and Glick (2009).Footnote 1 In Bergin and Glick, to solve for the steady state, the computation algorithm would make a guess for the index of the marginally nontraded good. In our model with a continuum of sectors/goods and a continuum of varieties in each sector, at first it appears that computational algorithm would need to make a guess for the indices of marginally nontraded variety of each good. However, in light of the analytical characterization of the marginal variety frontier, this turns out to be unnecessary: the computational algorithm needs to guess index for the marginal nontraded variety for just one sector. Thereafter, the marginal variety frontier allows solving for marginal nontraded varieties for all other sectors analytically, resulting in the avoidance of the curse of dimensionality. While the paper shows that it is possible to easily incorporate heterogeneity in two dimensions, is it really necessary? It turns out that in the most natural and empirically relevant generalization of Bergin and Glick set up described above, the failure to simultaneously incorporate heterogeneity in two dimensions results in faulty analysis. The object of analysis is the response of (endogenous) nontradedness of various goods and varieties of goods to a change in trade costs. If one considers heterogeneity in one dimension at a time and pieces together the results, one would erroneously conclude that a reduction in trade costs makes more goods and more varieties of every tradable good traded. In contrast, in the correctly specified model with heterogeneity in both dimensions, one finds that while more goods indeed become tradable, not every traded sector experiences an increase in the number of varieties that are traded. In fact, in almost 50% of the previously traded sectors the number of traded varieties falls. The result is shown to be quite robust to a range of plausible parameter values and introduction of production in the economy. An example with similar flavor is provided by Chaney (2008) which examines the sensitivity of the impact of trade barriers on trade flows to the elasticity of substitution among goods (see also Baldwin and Forslid 2010). He finds that the elasticity of substitution has opposing effects along the intensive and extensive margins. Like this paper, Chaney (2008) has a continuum of firms in each sector with Pareto distribution of productivity, but unlike this paper has a finite number of sectors. The model of this paper also, in a natural way, reconciles the contrasting predictions of the models with heterogeneity only in trade costs or productivity about the differences in the deviation of domestic price from the world price for the traded and nontraded goods. At the economy level, these deviations are shown to be dictated by heterogeneity in trade costs consistent with evidence in Crucini et al. (2005): the deviations from the world price are larger for nontraded goods. The heterogeneity in productivity à la Dornbusch et al. (1977) is relevant as well, but it is shown to operate at sector level and implies that, for a given good, deviation of domestic price from the world price is smaller for nontraded varieties than for the traded varieties. While the paper deals with specific types of sectoral and firm-level differences, namely trade costs and productivity, there are other relevant combinations of heterogeneity across sectors and firms that can be analyzed using the same idea. For example, following Bernard et al. (2007), one can have sectoral heterogeneity in factor intensity coupled with firm-level differences in productivity. Further, the method extends in a straightforward manner to a two country setting. There is a burgeoning literature studying impact of trade on business cycle comovement, both in two/multi-country and small-open economy settings. For example, Johnson (2014), in a model with finite number of sectors, studies the role of trade in intermediate inputs to replicate quantitative magnitude of empirical correlation between bilateral trade and GDP comovement, the trade-comovement puzzle. The reduction of computational complexity makes the model of this paper a viable choice to study such issues along with its sectoral and firm-level implications. In the remaining portion of the paper, Sect. 2 lays out and solves the small open economy model with endowment. The analytical details pertaining to solving for the equilibrium and how power of continuum may be used to simplify computations are outlined in Sect. 3. Section 4 numerically solves for the equilibrium of the small-open economy model with endowment. In Sect. 5, results are presented for sectoral variations in the nontradability in response to a reduction in trade costs. Section 6 briefly considers the two country model. Section 7 concludes. The “Appendix A” provides the details for solving a small open economy model with production whereas the details for the two country model are contained in “Appendix B”.",1
53.0,2.0,Computational Economics,30 October 2017,https://link.springer.com/article/10.1007/s10614-017-9764-7,Forecasting Crude Oil Prices: A Comparison Between Artificial Neural Networks and Vector Autoregressive Models,February 2019,Sepehr Ramyar,Farhad Kianfar,,Male,Male,Unknown,Male,"Crude oil constitutes the largest proportion of world’s energy sources and is a widely traded commodity (Yu et al. 2008). Price of crude oil fundamentally contributes to world economy growth and has direct impact on economies of oil exporting and oil importing countries (Godarzi et al. 2014). Therefore, many businesses and governments are affected by crude oil price fluctuations and seek to analyze and understand the behavior of oil prices in order to make better decisions and to select better policies. There are numerous elements that influence crude oil prices. First, economic factors are the major elements in the rising and falling of crude oil prices, i.e. crude oil supply and demand like any other commodity (Godarzi et al. 2014). Second, crude oil prices are affected by technical aspects of oil industry such as refining capacity and above the ground oil inventories as well as chemical properties of oil such as density or viscosity. In addition, production technologies such as shale oil extraction can considerably affect crude oil supply and subsequently oil prices. Third, geopolitical tensions and crises also play an important role in determination of crude oil prices. Since world’s largest oil reserves are located in the Middle East, a politically unstable region, geopolitical factors can sharply disrupt oil prices (Movagharnejad et al. 2011). Furthermore, economic sanctions imposed on major oil producing countries can impact oil supply and upset global energy markets. Driven by forces of different nature, crude oil price is highly volatile and hard to predict. Numerous efforts have been made to provide insight into behavior of oil prices. Some researchers have used econometric and statistical methods for forecasting the price of oil. Park and Ratti (2008) used a vector autoregressive (VAR) model to study oil price shocks in relation to stock markets. Similarly, Aloui and Jammazi (2009) used a wavelet analysis and Markov switching vector autoregressive model to analyze the behavior of crude oil prices with stock market returns in France, Japan and UK. Cheong (2009) developed a flexible autoregressive conditional heteroskedasticity (ARCH) model to forecast WTI and Brent crude oil markets. Wei et al. (2010) used generalized autoregressive conditional heteroskedasticity (GARCH) models to capture volatility of West Texas Intermediate (WTI) and Brent crude oil prices. Moreover, complexity and nonlinear behavior of oil prices have convinced researchers to use artificial intelligence methodologies such as artificial neural networks and machine learning in their forecasting models in order to deal with chaotic movements of oil prices. Mingming and Jinliang (2012) proposed a multiple adaptive wavelet recurrent neural network model using gold prices. Pan et al. (2009) provided short-term forecasts for crude oil price with an artificial neural network using futures and gold prices along with Dollar index as input factors. Furthermore, there is extensive literature on the relationship between crude oil prices and monetary policy. Amano and van Norden (1998) discuss how oil prices capture exogenous terms-of-trade shocks and emphasize on the structural relationship between exchange rates and oil price in the long run. Leduc and Sill (2004) explore the dynamics of oil-price shocks and monetary policy highlighting the role of monetary policy as the major response of the aggregate output of the economy to crude oil prices. In a later study, Kilian (2006) categorizes oil-price shocks by origin and quantifies this as compared to macroeconomic policy. Krichene (2006) further explains the relationship between monetary policy and oil prices and how exchange and interest rates disturb oil market equilibrium. The empirical analysis based on a world demand and supply model indicates the dependency of oil price stability on monetary policies. Yousefi and Wirjanto (2004) formulate the reaction of major oil producing countries to changes in the exchange rate of US dollar and illustrates a relationship between crude oil prices and exchange rate suggesting the inability of OPEC as a uniform determinant of oil prices. Crude oil has also long been studied as an exhaustible natural resource. In an earlier study, Stiglitz (1976) compares the rate of exploitation of an exhaustible natural resource with the profit maximization objective of a monopolist in the framework of a competitive market. He indicates that monopoly prices and market equilibrium prices are almost the same under mild conditions. Neumayer (2000) further explores natural resource availability and alternative options for economic growth in light of resource constraints and concludes that elimination of resource scarcity is theoretically impossible. Greiner et al. (2012) develop an empirical analysis of oil exploitation rate and uncover a U-shaped relationship between oil price and extraction rate. Maslyuk and Smyth (2008) indicate an upward trend in oil price pattern corresponding to supply constraints suggesting a consistency with theory of exhaustible natural resources by Hoteling (1931). Although many econometric and artificial intelligence models have been used to forecast crude oil prices, little attention has been focused on including parameters that take into account the exhaustible nature of crude oil and effects of monetary policy that are recently having a major impact on crude oil prices given the shift in the dynamics of oil markets. As crude oil is being increasingly traded in spot and future markets such as NYMEX, a more detailed analysis of monetary policies is required to be incorporated into forecasting models to capture this recent and essential element of the oil market (Askari and Krichene 2008). In addition, recent developments in the upstream exploration and extraction suggest a new era for crude oil with tighter scarcity of supply which dictates more careful assumptions regarding modeling supply and exhaustibility of crude oil in the forecasting model. Furthermore, some models introduced in the literature have not had a comprehensive understanding of oil market dynamics and drivers of crude oil price and have sometimes incorporated somewhat irrelevant variables such as gold prices or oil futures prices in their forecasting models (Hamilton 2008). While crude oil, like any other fossil fuel, is exhaustible, most models have not taken into consideration the exhaustible nature of crude oil and depletion of oil reserves. Moreover, monetary policy can play a major role in oil price movements and this crucial element has often been missing from forecasting models. This paper aims to provide a thorough modeling of crude oil market elements and key drivers of oil prices that include major aspects of oil price dynamics such as monetary policy and exhaustibility of crude oil. Meanwhile, the effectiveness of artificial neural networks as powerful computational tools for modeling nonlinear and volatile behavior of crude oil prices is tested by comparing its accuracy with results from an estimated VAR model. In particular, the present paper connects to and builds on research carried out in three related areas: (1) Artificial Neural Network structure determination, (2) feature engineering, and (3) crude oil prices economic modeling. Artificial Neural Network structure determination In this paper, rather than solely depending on the minimization of loss function, a data-driven procedure is implemented to provide more robust weight approximation for the neural net. This is particularly effective in the context of crude oil price forecasting given the intrinsic volatility of the prices. Thus, the robust, data-driven procedure allows for more reliable estimation and weight configuration within the ANN setting. Furthermore, the neural network implemented in this paper allows for an internal mechanism to bound the number of neurons in the hidden layer to avoid overfitting. This feature is specifically important given the sparse data set available which is prone to overfitting which is avoided by this approach in the paper. Ultimately, a final consideration while designing the ANN structure has been to allow for a correspondence between the interpretability of the results and the research question. The relevance between the machine learning structure and the context of crude oil price forecasting from an economic modeling perspective is somewhat absent in the literature and this paper has attempted to bridge this gap by incorporating and discussing both economic and analytics aspects of the problem. Feature engineering Another contribution of this paper is discussion of the potential mistakes and dangers of using machine learning approaches in prediction of commodities, and particularly, crude oil prices. As discussed in the paper, careful feature engineering can guarantee validity of the results whereas many related works in the literature involve machine learning approaches that involve irrelevant features such as futures or gold prices which may seemingly produce accurate results while in fact having little value in terms of economic interpretability. This paper has addressed this issue which can largely impact future research involving machine learning and economic forecasting. Crude oil price economic modeling Following the recent transformations in the global economy and particularly crude oil markets, this paper has a novel modeling approach to crude oil price forecasting where important variables such as monetary policy and exhaustible nature of crude oil prices are incorporated in the prediction model. This approach is a continuation of recent work such as (Askari and Krichene 2010a) and also reflective of the dominant realities in the global crude oil market. The remainder of this paper is organized as follows. Section 2 briefly reviews the underlying mechanism of artificial neural networks and the process of identifying optimal structure of the proposed MLP neural network as well as the theoretical background for VAR models. Section 3 discusses the selection of key variables that determine the price of crude oil, data collection and results of the proposed ANN model. Moreover, the vector autoregressive estimation for the forecasting model is presented and the results are validated by appropriate statistical tests and then compared with those of the ANN model. Section 4 discusses the significance of the findings and relevant policy issues. Finally, concluding remarks and policy implications are drawn in Sect. 5.",36
53.0,2.0,Computational Economics,01 November 2017,https://link.springer.com/article/10.1007/s10614-017-9763-8,Getting the Best of Both Worlds? Developing Complementary Equation-Based and Agent-Based Models,February 2019,Claudius Gräbner,Catherine S. E. Bale,Francesca Lipari,Male,Female,Female,Mix,,
53.0,2.0,Computational Economics,01 November 2017,https://link.springer.com/article/10.1007/s10614-017-9766-5,Tail-Related Risk Measurement and Forecasting in Equity Markets,February 2019,Stelios Bekiros,Nikolaos Loukeris,Christos Avdoulas,Male,Male,Male,Male,"The increased volatility of financial markets coupled with their exposure to excessive risks, which arise from variations in prices of equities, commodities, exchange rates, and interest rates during the last decade has encouraged many financial institutions to design and develop more sophisticated risk management tools. One such measure of financial risk is the Value at Risk (VaR) first developed by JP Morgan through its RiskMetrics model, and became the standard measure financial analysts use to quantify market risk. The Basel Committee on Banking Supervision (1996) at the Bank for International Settlements imposes on financial institutions to meet capital requirements based on Value-at-Risk estimates. There is no limitation regarding the VaR models utilized, only that estimates must be generated via a combination of subjective/judgmental considerations and empirical evidence. In fact, the introduction of VaR was initially implied by Markowitz (1952), in such a way that during the early 1980s in the US, the Securities and Exchange Commission (SEC) adopted a similar primary risk measure to assess capital adequacy of broker-dealers’ trading on non-exempt securities, whilst Bankers Trust used a VaR-type model for its RAROC capital allocation system. In the early 1990s, three events popularized VaR in risk estimation: (i) in 1993, the Group of 30 published a significant report on derivatives practices, (ii) in 1994, JP Morgan launched its free Risk Metrics service as CreditMetrics and (iii) during 1995, the Basel Committee on Banking Supervision implemented market risk capital requirements for banks. The different forms of risk include among other Credit risk which reflects the potential loss due to the inability of a counterpart to meet its obligations, and it depends on credit exposure, probability of default and loss in the event of default and Operational risk which represents the errors of the instructing payments or settling transactions, and includes the risk of fraud and regulatory risks Moreover, Liquidity risk is affected by extended and unexpected negative cash flow over a short period, in case of instant need for cash a firm with highly illiquid assets, may be compelled to sell assets at a discount, while Market risk estimates the uncertainty of future earnings, due to the changes in market conditions. As VaR is defined as the maximum potential change in the value of a portfolio with a given probability over a certain horizon, it also clarifies the maximum potential loss in the value of a portfolio due to adverse market movements, for the same level of probability. Hence it provides a probabilistic metric of market risk for a portfolio incorporating also regulatory requirements. Thus, Value at Risk is the maximum loss that will occur within a given period of time T, and within a fixed probability \(\alpha \): where \(S_{t}\) the logarithm of the price in time \(t, S_{t+T}\) the log-price in time \(t+T, \upalpha \) is the value of probability and VaR the maximum level of loss. However, past values cannot determine the present market risk undertaken by a portfolio. In case of miss-hedging in portfolio by traders, the level of risk has to be known before a loss is incurred, and Value-at-Risk offers this ability to institutions. As opposed to utilizing only historical volatility, VaR can measure the extreme losses at the tails of return distributions as well as phenomena of asymmetry. After the BIS 1998 report was released, allowing internal models for market risk, VaR was followed by the major G-10 banks, in intraday transactions to report regulatory capital, creating a VaR framework for credit risk too. The current BIS requirements offer many alternatives for default risk, downgrade risk, spread risk, and concentration risk. Crouhy et al. (2000), analyzed Credit VaR methodologies, currently used in the markets, and concluded that the credit migration approach of JP Morgan uses the probability of moving from one credit quality to another, with default, in a specific time horizon. In option pricing of the KMV-form based on the asset value model by Merton (1974), the default process when stock value falls below critical level is endogenous and related to the capital structure of the firm, whilst the actuarial approach of Credit Suisse Financial Products emphasizes on bonds or loans’ default according to an exogenous Poisson process. McKinsey and Company (1998) proposed the CreditPortfolioView as a discrete time multi-period model with conditional default probabilities on the macroeconomics variables. Also Crouhy et al. (2000) indicated the capital charge for specific risk as the product of a multiplier, whose minimum volume has been set to 4 times the sum of VaR at the 99% confidence level for spread risk, downgrade and default risk over a 10-day horizon. Overall, estimations of VaR can be implemented by: (I) Historical Simulation either as basic, pro-historical, contra-historical simulation or age weighted, (II) Parametric approaches and (III) Simulation approaches. Nevertheless, the VaR risk measure is not sub-additive, nor convex. Artzner et al. (1999) when proposing the main properties a risk measures must fulfill, established the notion of a coherent risk measurement. Those distinctive properties can be established by convex risk measures as in Föllmer and Shied (2004), spectral approaches i.e., by Acerbi (2002) or deviation measures (e.g., Rockafellar et al. 2006). Coherent risk measures can be implemented for optimal portfolio and capital allocation (Rockafellar and Uryasev 2002), as well as towards efficient pricing of derivatives products (e.g., options) in incomplete markets (Cherny 2006). Spectral risk measures were applied to futures clearinghouse margin requirements (Cotter and Dowd 2006). For example, Acerbi and Simonetti (2002) extend the results of Pflug–Rockafellar–Uryasev methodology to spectral risk measures. The most important aspect in quantifying VaR is volatility. Andersen et al. (2002) systematically categorized volatility under a unified continuous-time, arbitrage-free and frictionless framework. They introduced notional volatility corresponding to the ex-post sample-path return variability over a fixed time interval, an ex-ante expected volatility over a fixed time interval, and the instantaneous volatility that corresponds to the robustness of volatility process in a specific time. Parametric processes use explicit functional form assumptions regarding the expected and instantaneous volatility. In the discrete-time ARCH models, expectations are directly observable variables, while stochastic volatility models of discrete and continuous time involve latent state variables. Non-parametric procedures are generally free from functional form assumptions, able to afford estimates of notional volatility with the values of flexibility and consistency, including ARCH filters and smoothers that measure volatility in short horizons, and the realized volatility measures for time intervals of fixed length. Audrino et al. (2003) introduced a functional gradient descent—FGD algorithm, as a hybrid non-parametric statistical function and estimated volatility and conditional covariances within a high-dimensional—up to 100—financial series of asset price returns, and yielded significantly better predictions than a constant conditional correlation GARCH model. Pan and Poteshman (2006) partitioning option signals into components publicly and non-publicly observable, found that the economic source of predictability is non-public information possessed by option traders rather than market inefficiency, whilst greater predictability was observed in stocks with higher concentrations of informed traders and from option contracts with greater leverage. We examine 5 different VaR/ETL approaches: (i) the Basic Historical, (ii) the Age Weighted, (iii) the GARCH (1,1), (iv) the Normal, (v) the Student-t (5 df), (vi) the Student-t (6 df), and (vii) the Student-t (7 df), in 12 different backtesting specifications: (a) the Expected Avg. VaR Violations, (b) Avg. VaR Violations, (c) Kupiec pass rate %, (d) Independence pass rate %, (e) the Christoffersen pass rate %, (f) the Lopez 1, g) the Blanco Ihle Average, (h) the Adjusted Angelidis–Degiannakis (A–D) ETL on MSE Average, (i) the Adjusted Angelidis–Degiannakis (A–D) MSE Average, (j) the Adjusted Angelidis–Degiannakis (A–D) ETL on MAE Average, (k) the Adjusted Angelidis–Degiannakis (A–D) MAE Average, and (l) the Gross ETL Average, for the cleared 92 stocks of FTSE 100 in 4 different levels of confidence: 90, 95, 97.5, 99%. Moreover, the models of VaR violations were examined and backtested under 8 tests: (a) the Expected Avg. VaR Violations, (b) Avg. VaR Violations, (c) Standard VaR Violations, (d) Minimum VaR Violations, (e) Maximum VaR Violations, (f) Kupiec pass rate %, (g) Independence pass rate %, (h) the Christoffersen pass rate %. We showed that all the methods incorporating the Normal or Student-t allocations are inadequate, though the approaches did reveal idiosyncrasies regarding the independence of violations. The GARCH and age-weighted historical models proved to be best at forecasting ETL, as the parametric models fitted better to the data, whilst the tail losses were increasingly difficult to predict at higher confidence levels. The historical methods demonstrated increased out-of-sample accuracy on calm periods, while the non-parametric methods outperformed in turbulent/crisis times. We showed that an optimal forecast should include time, asset structure and overall market conditions. Overall, in longer periods and large capitalization stocks, the parametric models perform better, whereas in short-term trading the non-parametric methods are more suitable for measuring the tail risk of small-cap stocks. The paper is organized as follows: Sect. 2 provides a description of parametric Value-at-Risk, Conditional VaR and Expected Shortfall (ETL) approaches. Section 3 presents Simulation-based non-parametric as well as Hybrid VaR and ETL models. Section 4 analyzes how backtesting evaluation of the investigated models is performed and introduces the suitable statistical tests. Finally, Sect. 5 presents the empirical results and Sect. 6 concludes.",8
53.0,2.0,Computational Economics,07 November 2017,https://link.springer.com/article/10.1007/s10614-017-9765-6,An Artificial Neural Network-Based Approach to the Monetary Model of Exchange Rate,February 2019,Huseyin Ince,Ali Fehim Cebeci,Salih Zeki Imamoglu,Unknown,Male,Male,Male,"After the collapse of the Bretton-Woods system in 1973, the fixed exchange rate regime obliged by the system disappeared and countries became vulnerable to sudden and drastic movements in exchange rate levels. In today’s overwhelmingly globalized, economically integrated world, the exchange rate between currencies has become increasingly crucial and had profound impacts on all levels of the economy. Consequently, accurate and reliable forecasting of exchange rates emerged as an important necessity. During the last decades, rendering exchange rate movements, developing a model for forecasting exchange rates has been a dynamic and active research area. Many models have been proposed in the economics literature, attempting to unveil exchange rate dynamics such as the purchasing power parity model, monetary model, balance of payments flow model, and portfolio balance model. These models are termed economic fundamentals-based models because they associate exchange rate behavior with macroeconomic variables. A huge volume of literature exists regarding empirical tests of the various versions of the monetary model and its predictive ability. Studies following early successful implementation of the monetary model by Frenkel (1976) and Bilson (1978) produced unsatisfactory results. Meese and Rogoff (1983) argued that the monetary model is unable to outperform a random walk (RW) model, which implies that exchange rates behave in a purely random and unpredictable manner. In the literature, this phenomenon is called the Meese–Rogoff puzzle. Subsequent sophisticated empirical studies were also unable to strongly justify a solution to the Meese–Rogoff puzzle and prove that the monetary model can beat the naive RW model, especially in the short run (Ni and Yin 2009). Recent studies have claimed that adjustment of exchange rates to level implied by the monetary models is governed by a nonlinear adjustment process (Sarno et al. 2004; Taylor and Peel 2000; Junttila and Korhonen 2011; Beckmann 2013). Disappointing performance of linear models was mainly attributed to unrealistic assumptions accompanying these models about the nature of data which do not comply with real-world situations (Khashei et al. 2009; Qi and Wu 2000). Theoretical models are fundamentally sound but the linear methods used for estimating these models were the major cause of dismal empirical performance (Kilian and Taylor 2003). Wu and Chen (2001) presented a nonlinear error correction model for the monetary model of exchange rate, but the model failed to perform better than RW model for a period (one quarter) ahead forecasts in terms of root mean square error (RMSE) and mean absolute error (MAE). Furthermore, the forecasting performance of the model improved significantly and performed better than the RW model on forecasts of two to four periods ahead. Kim et al. (2010) demonstrated that the predictive ability of nonlinear monetary model increases as the forecasting period extends from one period (quarter) ahead to 16 periods ahead forecasts. However, none of the three nonlinear models in consideration performed better than the RW model for the one-period-ahead forecasts. Huber (2016) employed the monetary model to forecast the USD/EUR exchange rate by nonlinear multivariate models using RMSE as a performance criterion. These models performed slightly better than the random walk model in forecasts of one period (month) ahead, and their performance improved on longer forecasting periods. Overall, these studies suggested that the monetary model of exchange rate exhibit predictive power when the nonlinear adjustment process is taken into account and that its predictive power improves with longer forecasting periods. During the last decades, with the rapid advancements in computer and information technologies, using artificial intelligence-based models in time series forecasting has become a common practice among researchers. Artificial neural network (ANN)-based models also came into use in forecasting exchange rates as a promising forecasting tool. Zhang et al. (1998) summarized the distinctive features of ANNs as follows: (i) ANNs do not rely on assumptions about the nature of the data, which is the case for classical models; instead, they are data-driven techniques. They are nonparametric techniques in that they can learn from experience incorporated within the data and extract functional relationships with no need for a theoretical framework; (ii) ANNs can reliably reveal characteristics of the sample population by learning from the sample data even if the sample data consist of noisy information; (iii) ANNs are not subject to limitations from functional forms. They are universal functional approximators that can identify and approximate any continuous function; (iv) ANNs don’t involve any assumptions about the data-generating process. They are capable of dealing with nonlinear relationships. They extract the relationship between variables by learning from data and base their forecasts on these nonlinear approximations. ANNs have long been employed for exchange rate forecasting purposes. Studies have compared forecasting performance of ANNs with univariate time series models, with macroeconomic fundamentals-based models estimated by ordinary least squares (OLS), as well as with multivariate time series models. Wu (1995), Zhang (2003) and Khashei et al. (2013) compared the forecasting performance of autoregressive integrated moving average (ARIMA) models and ANNs in terms of RMSE and MAE. The ANNs performed substantially better than the ARIMA models. Lisi and Schiavo (1999) and Leung et al. (2000) employed ANNs for forecasting various exchange rates with respect to RW model by employing normalized mean square error (NMSE) and RMSE as performance criteria. These studies evaluated the forecasting performance only for one-period-ahead (1 month ahead) forecasts, and ANNs performed better than RW models for each exchange rate. Verkooijen (1996) showed that the monetary model estimated by ANNs provided a slight improvement in terms of RMSE but a significant improvement in terms of the correctly predicted signs over the monetary model estimated by OLS in forecasting US Dollar/Deutsche Mark exchange rate. Qi and Wu (2000) also estimated the monetary model using ANNs and compared forecasting performance using the RW model for four exchange rates over three forecasting periods. However, the ANNs did not outperform the RW model in most instances in terms of RMSE. Bissoondeeal et al. (2011) estimated the monetary model using Divisia measures of money. The monetary model with Divisa monetary aggregates estimated by ANNs performed better than the Vector Autoregressive (VAR), Vector Error Correction (VEC), and RW models in one-step-ahead forecasts of the UK/US exchange rate. The results of these studies showed that the ANNs, while being unable to solve the Meese–Rogoff puzzle, provided promising outcomes in forecasting exchange rates. 
Meese and Rogoff (1983) evaluated the forecasting performance of exchange rate models using statistical metrics based on the magnitude of the forecasting error, such as root RMSE and MAE. These metrics are subject to criticism concerning their relevance to the economic value of the forecasts. Leitch and Tanner (1991) showed that no systematic relation exists among RMSE, MAE metrics of a model’s forecasts, and the economic value of these forecasts (i.e., profits earned by these forecasts). Only the directional accuracy metric (DA) is related to economic value. Abhyankar et al. (2005) evaluated the forecasting performance of the monetary model using a measure based on economic value of the forecasts and proposed that the predictive power of the monetary model is higher than the naïve random walk model when evaluated in this manner. Moosa and Burns (2014) repeated the analysis of Meese and Rogoff (1983) by estimating the monetary models in a time-varying parameters (TVP) framework. In addition to RMSE, they employed forecast performance measures based on the average profits earned when the forecasts are simulated on a carry-trade basis, that is, the mean return (\(\bar{\pi }\)) and the Sharpe ratio (SR). The results showed that when forecasting performance is evaluated by RMSE even the TVP models cannot enhance the forecasting performance of the monetary models relative to the RW model, but when the models are evaluated by profit-based measures, the TVP models and even some OLS models outperform the RW model. Following these debates in the literature, this paper aims to investigate the predictive accuracy of the flexible price monetary model of exchange rate, estimated by an approach based on combining the VAR model and ANNs, more precisely multilayer feedforward neural networks (MLFFNNs). The forecasting performance of this nonlinear and nonparametric model is analyzed against the monetary model estimated by OLS in a static framework, estimated by OLS in a linear dynamic VAR framework, in a parametric, nonlinear, dynamic threshold vector autoregressive (TVAR) framework and naïve RW model in forecasting six different exchange rates (CAD/USD, GBP/CAD, GBP/USD, JPY/CAD, JPY/GBP, JPY/USD). Out-of-sample forecasting accuracy of each model was analyzed comparatively by utilizing RMSE, \(\bar{\pi }\), and Sharpe ratio measures. The analysis aimed to contribute to the literature by investigating the enhancement achieved through the introduction of a novel artificial intelligence-based, nonlinear, nonparametric model for forecasting the performance of the monetary model with respect to linear models, a nonlinear parametric model and the RW model in terms of both the magnitude of forecast errors and the economic value of the forecasts. The rest of this study is organized as follows; Sect. 2 provides the theoretical foundations of the monetary model of exchange rate determination; Sect. 3 describes the MLFFNNs that will be utilized in this paper. Then, in Sect. 4, each model is employed to forecast six different exchange rates to compare the forecasting performance and to assess the model’s predictive ability. Concluding remarks derived through the analyses are provided in the final section.",13
53.0,2.0,Computational Economics,07 November 2017,https://link.springer.com/article/10.1007/s10614-017-9767-4,Evolutionary Computation for Macroeconomic Forecasting,February 2019,Oscar Claveria,Enric Monte,Salvador Torra,Male,Male,Male,Male,"Evolutionary computation is being increasingly applied in economics. The main idea behind evolutionary computation is the implementation of algorithms that adopt principles of the theory of natural selection to problem solving (Fogel 2006). These algorithms are known as evolutionary algorithms (EAs). One of the most implemented EAs in optimization problems is the genetic algorithm (GA) developed by Holland (1975). Cramer (1985) proposed a generalization of GAs known as genetic programming (GP), in which the solutions are expressed in the form of computer programs. Whereas GAs codes potential solutions by means of fixed length binary string representations, GP uses tree-structured, variable length representations suitable for non-linear modelling. This more general representation scheme allows the model structure to vary during the evolution. As a result, GP is particularly indicated for empirical modelling. Empirical modelling consists on developing mathematical models from experimental data, which implies finding both the structure and the parameters of the model simultaneously. Koza (1992) developed a novel approach to empirical modelling based on symbolic regression (SR) via GP. SR can be regarded as a modelling technique used to construct regression models (such as linear regression, radial basis functions, support vector machines, kriging, etc.) by searching the space of mathematical expressions that best fit a given dataset. In this search there is usually a trade-off between accuracy and simplicity. Koza (1992) used GP to find the best single computer program that solves a given SR problem. This approach is especially useful to find patterns in large data sets, where little or no information is known about the system. In this study we implement SR via GP to find the relationship between a wide range of expectational variables and economic growth. We use survey data on expectations from the World Economic Survey (WES) carried out by the CESIfo Institute for Economic Research. By combining a SR approach with GP, we derive mathematical functional forms that can be regarded as the optimal combinations of survey variables that best fit the actual evolution of the economic activity. We use these computationally-generated expressions as building blocks to forecast year-on-year growth rates of GDP. Expectations about the state of the economy are key in economic modelling. One of the most common ways for agents to reveal their expectations is through tendency surveys. Business and consumer tendency surveys ask agents whether they expect a variable to rise, to remain constant, or to fall. The relationship between quantitative data and respondents’ expectations was first formalised by Anderson (1952) and Theil (1952), who regressed the actual average percentage change of an aggregate variable on the percentage of respondents expecting a variable to rise and to fall. The theoretical framework for quantifying these percentages was initially based on the existence of an interval around zero within which respondents perceive there are no significant changes in the variable. Thus, respondents would answer that they expect a certain variable to go up (or down) to the extent that the mean of their subjective probability distribution lied beyond a threshold level. These thresholds could therefore be regarded as the limits of an indifference or imperceptibility interval. Carlson and Parkin (1975) developed this approach by using a normal distribution, and by assuming unbiasedness over the sample period to estimate the difference limen. This approach was latter extended by Pesaran (1984, 1985), who allowed the model for an asymmetrical relationship between the actual average percentage change and the agents’ changes in periods of growth. By matching individual responses with realisations, several authors have further explored this relationship at the micro level (Białowolski 2016; Breitung and Schmeling 2013; Lahiri and Zhao 2015; Lui et al. 2011a, b; Mitchell et al. 2002, 2005a, b; Mokinski et al. 2015; Müller 2010; Zimmermann 1997). Müller (2010) proposed a variant of the Carlson–Parkin method with asymmetric and time invariant thresholds. Breitung and Schmeling (2013) found that the introduction of asymmetric and time-varying thresholds was key in order to improve the forecast accuracy of quantified survey expectations, while the individual heterogeneity across forecasters seemed to play only a minor role. Using household-level data from the University of Michigan, Lahiri and Zhao (2015) found strong evidence against the threshold constancy, symmetry, homogeneity, and overall unbiasedness assumptions of the Carlos-Parkin method. Experimental expectations generated by Monte Carlo simulations have also been used to delve into the relationship between individual expectations and their quantitative equivalent. Common (1985) generated simulated expectations to test the rational expectations hypothesis. Simulation experiments have also been used to assess the forecasting performance of different quantification methods of survey expectations. By means of individual computer-generated expectations, Claveria (2010) compared the forecasting performance of the main quantification methods, while Löffler (1999) and Terai (2009) estimated the measurement error introduced by the Carlson–Parkin method. The link between survey expectations and quantitative data at the aggregate level has been widely investigated (Abberger 2007; Batchelor and Dua 1998, 1992; Bergström 1995; Berk 1999; Bovi 2013; Bruestle and Crain 2015; Bruno 2014; Claveria et al. 2007; Dees and Brinca 2013; Driver and Urga 2004; Graff 2010; Hansson et al. 2005; Jean-Baptiste 2012; Kauppi et al. 1996; Leduc and Sill 2013; Lee 1994; Mittnik and Zadrozny 2005; Nardo 2003; Nolte and Pohlmeier 2007; Pesaran and Weale 2006; Qiao et al. 2009; Rahiala and Teräsvirta 1993; Robinzonov et al. 2012; Smith and McAleer 1995; Sorić et al. 2013; Wilms et al. 2016). Klein and Özmucur (2010) analysed the role of survey expectations in 26 European countries, and found that they improved the forecasting performance of autoregressive time series models. 
Schmeling and Schrimpf (2011) found that survey-based measures of expected inflation were significant predictors of future aggregate stock returns in France, Germany, Italy, the UK, the US and Japan, both in-sample and out-of-sample. Using survey expectations of 12 European countries, Ghonghadze and Lux (2012) obtained a superior out-of-sample forecasting performance with a canonical opinion dynamics model than with univariate time series models. Jonsson and Österholm (2012) analysed the inflation expectations formation process in Sweden using survey expectations, obtaining a poor forecasting performance that could be partly attributable to a mismeasurement of expectations. However, Österholm (2014) found that survey-based expectations improved the out-of-sample forecasting performance of GDP growth predictions in Sweden. 
Martinsen et al. (2014) constructed factor models based on disaggregate survey data to forecast inflation, unemployment and GDP in Norway. The authors obtained the most accurate results for GDP growth. Girardi (2014) found that survey expectations contained relevant information about business cycle developments in the Euro Area (EA), especially around periods of extreme cyclical swings. Guizzardi and Stacchini (2015) showed that the inclusion of business survey indicators in time series models increased the forecasting accuracy of the baseline models. In a recent study, Altug and Çakmakli (2016) generated inflation forecasts by combining data on survey expectations with the inflation target set by central banks, finding the former to increase the predictive power of the models. Similarly, Lehmann and Wohlrabe (2017) found that consumers’ unemployment expectations were a good indicator of employment growth in Germany. These studies use a wide range of econometric techniques, but none of them assesses the relationship between both official quantitative data and qualitative survey expectations by means of evolutionary methods. This research aims to break new ground by linking survey data and economic growth by means of evolutionary computation. We design a SR experiment in order to find the optimal combinations of survey expectations that best fit the actual evolution of year-on-year growth rates of GDP. Lahiri and Zhao (2015) found a significant improvement in agents’ expectations accuracy during periods of uncertainty. This finding has also led us to assess the impact of the 2008 financial crisis on agents’ ability to forecast the evolution of economic activity. Hence, we use the estimates of GDP in four Scandinavian economies and compare them to a baseline model by means of the mean absolute scaled error (MASE) proposed by Hyndman and Koehler (2006). The rest of the paper is organized as follows. Sect. 2 presents our proposed methodological approach and the experimental set up. In Sect. 3 we present the empirical results. Finally, Sect. 4 provides some concluding remarks.",15
53.0,2.0,Computational Economics,09 November 2017,https://link.springer.com/article/10.1007/s10614-017-9768-3,Improving Financial Distress Prediction Using Financial Network-Based Information and GA-Based Gradient Boosting Method,February 2019,Jiaming Liu,Chong Wu,Yongli Li,Unknown,,Unknown,Mix,,
53.0,2.0,Computational Economics,10 November 2017,https://link.springer.com/article/10.1007/s10614-017-9769-2,Experimental Analysis of Corporate Wage Negotiations Based on the Ultimatum Game: A New Approach Using a Combination of Laboratory and fMRI Experiments,February 2019,Hidetoshi Yamaji,Masatoshi Gotoh,Yoshinori Yamakawa,Male,Male,Male,Male,"In recent years, the issues surrounding corporate income distribution have become increasingly acute. For instance, looking at recent economic data, one cannot help but notice a decrease in labor’s share of income in a significant number of countries. At the level of individual companies, as long as the basis of labor’s share of income is the ratio at which the value added generated by the relevant company is divided into wages and dividends, it is not possible for us—as researchers of the accounting systems that provide basic data for labor-management negotiations in the labor market which determine such wage levels and govern the calculation of profit available for dividends—to neglect such a decrease. Accounting researchers have highlighted the fact that one of the factors behind this phenomenon is the deep-rooted trend within corporate society of management manipulating accounting information in order to keep wage levels low and maintain a certain level of dividends to shareholders (Yamaji 1986; DeAngelo and DeAngelo 1991). Another factor, as raised by Piketty (2013), is that as the distribution of corporate income tends in the long term to be advantageous to people already in possession of wealth (capital), society’s wealth therefore tends to be disproportionately distributed to the most affluent portion of that society, and the current unequal distribution of income is one aspect of that trend. As a result, media reports on economic issues in various countries have pointed out that workers worldwide share a gut feeling that income distribution is unfair. For reference, the Fig. 1 below provides data on the trends in labor’s share of income in several countries. International comparison of labor’s share of income (Labor’s share of income = Employees’ compensation/GDP). At the individual company level, labor’s share of income is normally the sum of the total labor costs divided by value added, but here we have used the sum of compensation of employees divided by GDP. The figure was created on the basis of National Accounts data from the Organization for Economic Co-operation and Development (OECD) However, even if we recognize that financiers, managers, and other such people who possess wealth (capital) are at an advantage when it comes to income distribution, it is not entirely clear what kind of mechanism leads to this state of affairs. For instance, looking at the aforementioned factors proposed by accounting researchers, there is no definitive evidence of managers consciously manipulating accounting information in the process of labor-management negotiations, despite significant amounts of research on the topic (Liberty and Zimmerman 1986; Mautz and Richardson 1992; Cullinan and Knoblett 1994; Amernic and Craig 2005; Hilary 2006; Farber et al. 2012; Bova 2013). Even Piketty has been unable to identify the specific mechanism that governs distribution. This paper therefore seeks to identify a more concrete mechanism behind the distortions in income distribution. First, one of the environmental factors that have an effect on the mechanism is polarization within the group of workers whose share of income has been declining. Another factor is that labor-management negotiations are increasingly taking the form of individual bargaining—namely, negotiations between individual managers and individual workers—as opposed to collective bargaining between labor unions and management. Let us first look at the data that confirm these environmental factors. The distinctive feature of the polarization among workers in Japan is the steady increase in the number of workers in non-regular employment as a proportion of the total number of workers: while in 1990 non-regular workers accounted for 8.7% of all male workers (and 37.9% of all female workers), this increased to 8.8% (39.0%) in 1995, 10.7% (46.2%) in 2000, 17.8% (51.7%) in 2005, 18.2% (53.3%) in 2010, and 19.6% (54.7%) in 2012 (StatisticsBureau (Japan) 2013). This shows a marked relative increase in the number of non-regular workers. At the same time, as Stiglitz (2012) notes, in countries such as the US there is also a growing new category of workers who are subject to layoffs and struggle to secure a livelihood, as reflected by recent movements spearheaded by low-income bracket workers, such as the Occupy Wall Street Movement, which coined the phrase “We are the 99%.” There has clearly been a steady decrease in the proportion of workers in Japan who are members of a labor union—although it is difficult to ascertain whether this is a factor behind or the result of polarization among workers. While the unionization rate in Japan was 34.4% in 1975, it had decreased to 25.2% by 1990, 23.3% by 1995, 21.5% by 2000, 18.7% by 2005, 18.5% by 2010, and 17.9% by 2012 (Ministry of Health, Labour and Welfare 2013). This decrease is occurring concurrently with the increase in the trend towards individual bargaining as opposed to collective bargaining, which has been the form that labor-management negotiations have generally taken in the past. Moreover, in parallel with these trends there has also been a decrease in the number of labor disputes, the means formerly employed to prevent a drop in wage levels. While in 1975 the number of disputes was 3385, this had fallen to 283 in 1990, 208 in 1995, 117 in 2000, 50 in 2005, and 38 in 2010 (Ministry of Health, Labour and Welfare 2011). This is thought to indicate that labor-management relations are increasingly being conducted between individuals. In the above, we examined the data that confirm the current state of income distribution and the distinctive labor market phenomena that appear to be related to it. We focused on the situation where there has been a decrease in labor’s share of income, polarization among workers, a decrease in labor union membership, the individualization of labor-management negotiations, and a decrease in the incidence of labor disputes, all of which have been happening concurrently. The first task is to propose a hypothesis regarding the mechanism that generates the relative decrease in the proportion of income distributed to workers. As recent labor-management negotiations are increasingly conducted between individual managers and individual workers, the psychological traits of both parties come to the fore. In particular, it is thought that the individual psychological traits of managers, who play key roles in the different types of negotiations within corporations, have a significant influence on the results of such negotiations. Here we focus on the concept of “empathy” as a more specific psychological trait of managers. Namely, while managers with high levels of empathy may take into consideration their workers’ living situations and therefore pay higher wages, those lacking such empathy may totally disregard this and therefore set wages at a low level. Particularly given the fact that wage negotiations have in recent years noticeably been tending away from negotiations between organizations—namely, between labor unions and the management of the relevant company—and toward negotiations between individual workers and individual managers, it is all the more the case that the individual psychological traits of managers have a marked influence on the results of negotiations. If it is possible that wages may be kept low as a result of the psychological traits of workers and managers, it is not the manipulation of information but psychological factors that are allowing managers to succeed in establishing the low wages demanded by capitalistic competitive pressure. In this paper we therefore use experiments aimed at verifying the possibility that in actual labor-management negotiations between individuals, managers’ psychological characteristics may lead them to take concrete actions that keep workers’ wage levels low, as a result.",1
53.0,3.0,Computational Economics,14 November 2017,https://link.springer.com/article/10.1007/s10614-017-9771-8,Indexing of Technical Change in Aggregated Data,March 2019,Sturla Furunes Kvamsdal,,,Male,Unknown,Unknown,Male,"Economists hold technological progress to be an important source of growth, but its latent nature makes measurement difficult. Measurement is, however, a natural first step towards an understanding of the role of progress in growth beyond the normative. Much of the empirical literature focuses on the estimation of technical change in industry panels or cross sections. In a number of situations, however, only aggregated, industry-wide data are available. Examples are historical data, data from poorly monitored or informal industries, and data from developing countries. Faced with such data, economists can try to apply methods like state space modelling or nonparametric estimation. Alternatively, economists can turn to crude but simpler measures. One such crude measure is to introduce time period dummies into a regression of output on inputs to estimate an index of technical change. Period dummies have a number of intuitive and methodological issues. Perhaps the most striking issue is that the estimated index is a step function with a relative coarse resolution compared to the observation frequency. In most cases, estimates at the observation frequency are desirable. Further, a regression with period dummies is not information efficient and serial correlation is almost certain to occur. As it turns out, most issues with period dummies can be resolved with a quite simple procedure. When introducing time period dummies into a regression of output on inputs, some choices have to be made. One is of period length and whether all periods should be of equal length (possibly except for a last, residual period). Periods of various lengths would require a fair amount of motivation and I will not consider various period lengths here. Once the period length, or analogously the number of periods, is decided, one is presumably forced to commit to a given structure of the period dummy variables. Embodied in this dummy structure are arbitrary period shifts decided ad hoc by the period length. The problem of arbitrary shifts is limited in that one should not interpret the step function literally, but remain because estimates are invariable to the order of observations within periods. That is, time period dummies do not exploit the full information set. Further, estimates of input coefficients are sensitive to the idiosyncratic choice of period length, and finally, period dummy regressions tend to struggle with serial correlation (Hannesson et al. 2010). The procedure I suggest consist of repeated estimations of the empirical equation, where the period shifts, and hence the dummy variables, are shifted one observation at the time. If, say, the period length is l such that each dummy variable covers l observations (with a potentially shorter residual period), one needs to shift the dummy variables l times before they have cycled through all possible configurations. For each observation, one then has l equally relevant estimates of the level of technology. The average over the l different estimates provides an index of technical change resolved at the observation frequency. The averaged index exploits the full information set in the sense that it is sensitive to the order of observations. In comparison, each of the l different period dummy estimates are invariable to the order of observations within periods of the given dummy structure. Further, the averaged index improves goodness of fit and reduces serial correlation. In some examples, all traces of serial correlation are removed with a careful choice of the period length. The key point is that with the average index, one is not forced to commit to any given period dummy structure. Rather, all possible, and at least a priori equally relevant, period dummy structures are invoked to avoid influence from ad hoc period shifts. 
Hannesson et al. (2010) studied technological change in the Norwegian Lofoten cod fishery with time series on inputs (effort and stock levels) and output (catch). The data contained only aggregated, industry-wide data, and rather than pursuing advanced methods and a more demanding analysis, for example in state space (Harvey et al. 1986), they introduced time period dummies and ran ordinary least squares. They had in mind the general index approach of Baltagi and Griffin (1988) and related work, but without panel or cross-sectional data. However, their period dummies essentially generated an artificial panel structure in the data. The estimated index became a step function with a coarse resolution relative to the observation frequency, while a finer resolution was desirable (Hannesson et al. 2010, p. 757) (obviously, they correctly interpreted their estimates as period averages, and insisting on the step function is admittedly pedantic, but is nevertheless what they estimated; on another note, they undoubtedly considered other options and probably chose period dummies because their relative ease of implementation compensated for the eventual loss in methodological sophistication and, one may speculate, the additional insight gained). Measurement of productivity and efficiency more generally is a long-standing topic in economics, and a plethora of methods and ideas have been explored. An early impulse to the literature was the seminal contribution by Solow (1957), who conceived of the notion of measure (shifts in the production function) pursued in much subsequent work; a notion that also lie at the heart of the approach I pursue here. While I cannot provide a full overview of the literature, I will mention a few interesting contributions. On a general level, Griliches (1995) provide an insightful discussion on inter alia separability of production functions, relevance of data and models, and the link between public policy influence on research and development and the importance of economic and empirical understanding. Dorfman and Koop (2005) and related papers—their paper introduces a special issue of Journal of Econometrics—draw up what may still be perceived as approximately the research front. Focusing on panel data, Stern (2004, 2005) discusses a number of different empirical methods. The unobservable nature of technical change invites state space approaches, and a number of studies have followed the lead of Harvey et al. (1986). An application to panel data is Slade (1989). State space models are now mostly applied in macroeconomics (see, for example, Fuentes and Morales 2011). After Hannesson (1983) laid out the bioeconomic production function in fisheries economics and subsequent work by Squires (1992, 1994), Kirkley et al. (1995), and others, technical change in fisheries and other renewable industries has attracted increasing interest (see, for example, Jin et al. 2002; Fox et al. 2003; Kirkley et al. 2004; Hannesson 2007; Kvamsdal 2016). Nevertheless, one may still argue that the topic has gained too little attention in the resource economics literature, in particular given its key role in growth (Squires 2009; Squires and Vestergaard 2013).",3
53.0,3.0,Computational Economics,18 November 2017,https://link.springer.com/article/10.1007/s10614-017-9772-7,Analyzing Contagion Effect in Markets During Financial Crisis Using Stochastic Autoregressive Canonical Vine Model,March 2019,Anubha Goel,Aparna Mehra,,Unknown,Female,Unknown,Female,"The paper addresses an important question whether the financial markets exhibit the contagion effect during the stress periods leading to a spread of financial disturbances from one region to the other regions? (Forbes and Rigobon 2002) defined contagion as “a significant increase in cross-market linkages after a shock to one country (or a group of countries)”, otherwise, a continued high correlation among markets is termed as interdependence and not contagion. Several studies in the literature analyzed the co-movement of markets during stress periods. Hernández and Valdés (2001) and Kaminsky et al. (2003) explained that the contagion can be driven by a debt channel, a trade channel, or a common macroeconomic cause. Arellano and Bai (2013) extended (Yue 2010) study to present a multi-country model with debt renegotiation where all defaulting countries renegotiate simultaneously. They calibrated their model during the Eurozone crisis, and observed that the co-moving interest rates can become a reason of default in one country to lead to a high probability of default in other countries. Park (2012) developed a sequential equilibrium model for contagion where an investor (lender) was the mechanism of contagion in sovereign debt. Typically the existing studies in the context pursued two different directions. In one approach, researchers look out for the existence of contagion effect by analyzing the post crisis interdependence structures between economies. The second approach investigates how the contagion effect escalate and spread and the remedial measures to contain it. For instance, Hausken and Plümper (2002) proposed a game-theoretic approach to study how a financial crisis can be contained either through intervention or due to lapsing of time. They assume a dynamic model for the spread and escalation of crisis contagiously. Baral (2013) proposed a network formulation game to model contagion. Welburn and Hausken (2015) proposed a game theoretic approach to show how an adverse shock in a single country can change the behavior of countries with respect to consumption, investment, borrowing, default, and costs of debt. They also explain how, in response to a shock, a country changes allocations of consumption, investment, and borrowing. Welburn and Hausken (2017) proposed a game theoretic model to study the strategic behavior in a highly interconnected complex systems. Their main aim was to answer how financial crises can be managed through strategic interaction without the usage of capital controls or financial repression. In the present article, we followed the first approach, and aim to study the existence of contagion effect post financial crisis periods. Henceforth, we shall be presenting the literature in this direction. 
King and Wadhwani (1990) and Baig and Goldfajn (1999) used the correlation coefficient to investigate the contagion effect focussing on sharp rises in co-movements of returns that occur after an extremely negative shock to the returns of one country. Karolyi and Stulz (1996) explored the fundamental factors that affect the cross-country returns correlations and found that large shocks in broad-based market indices positively impact both the magnitude and persistence of the returns correlations. Longin and Solnik (2001) derived the distribution of extreme correlation for a wide class of return distributions using extreme value theory. They found that correlation increases in bear markets but not in bull markets. Ang and Chen (2002) developed a new statistic for measuring, comparing, and testing asymmetries in conditional correlations. Chakrabarti and Roll (2002) compared European and East Asian countries during 1997 Asian crisis. They found that covariances, correlations, and volatilities increase from the pre-crisis to the crisis period in both regions, but the percentage increases were much larger in Asia. Forbes and Rigobon (2002) showed that there was virtually no increase in unconditional correlation coefficients (i. e., no contagion) during 1997 Asian crisis, 1994 Mexican devaluation, and 1987 USA stock market crash. Ang and Bekaert (2002) proposed a dynamic portfolio choice method using a regime-switching process which can be characterized by correlations and volatilities increase in stress periods. Cappiello et al. (2006) and You and Daigler (2010) also used conditional correlation approach to analyze the behavior of international equities and indices. These studies applied the correlation tests and time series models such as generalized autoregressive conditional heteroscedasticity (GARCH) and vector autoregressive (VAR) models to study the joint dynamics of international stock markets. These models, though good enough to encapsulate the sharp increase in return co-movements that generally occurs after an extremely large negative shock to the returns of one country, but were limited to the linear correlation coefficients and ignored changes in the extreme tails of returns, consequently failing to capture the dependence structure among the variables. Some articles, like the one by Forbes and Rigobon (2002), empirically showed that the conventional correlation coefficient is significantly inaccurate and biased due to the presence of heteroscedasticity in almost all financial returns series. In other words, they suggested to filter out the volatility in returns before studying the contagion effect. Furthermore, Mohammadi and Su (2010) showed that the stock returns can exhibit some special features such as heavy tails, autocorrelation, and leverage effect. Recently, some authors, Patton (2006b), Jondeau and Rockinger (2006), Rodriguez (2007), Sun et al. (2009), Ning (2010), Aloui et al. (2011) and Chollete et al. (2009), examined the financial interdependence of financial markets using copulas. Copulas can capture the tail dependency in joint distributions of returns which in turn allows to test whether the tails can characterize the times of increased dependence. But most of these studies focused on using bivariate copulas. Aas et al. (2009) introduced a class of higher dimensional copulas called vine copula. These copulas are very flexible and their parameters are sequentially estimated (Chollete et al. 2009; Dissmann et al. 2013). However, in order to study the change in the dependence structure of returns with time, time-varying copulas must be used. The credit of using the time-varying copulas goes to Patton (2006b) who applied them to model the changing exchange rates dependencies. Since then, quite a few authors, like (Dias and Embrechts 2010), Garcia and Tsafack (2011), Hafner and Manner (2012) and Almeida and Czado (2012), used time varying copulas in their studies. A survey by Manner and Reznikova (2009) is a good point to start knowing about the time varying copula models. Almeida et al. (2016) proposed a high dimensional time-varying copula models using D-Vines by combining the pair-copula construction with the stochastic autoregressive copula (SCAR) models. 
Chen et al. (2014) studied the quantitative measurement of contagion effect between the USA and China stock markets during the subprime crisis. They used an improved version of time-varying copula of Patton (2006a) and defined a quantitative measure of contagion effect. They used Clayton copula to study the lower tail dependence. Kenourgios et al. (2011) studied financial contagion in a time-varying asymmetric multivariate framework and focussed on emerging markets (BRIC) and two developed markets. They used a multivariate Gaussian copula model with regime-switching and the asymmetric generalized dynamic conditional correlation (AG-DCC) to capture non-linear dependence dynamics during the period 1995–2006. Wang et al. (2011) studied the dependence structure between world’s major markets and the Chinese market. They assumed that the dependence parameters follow an ARMA(1,10) time series and hence are driven by past information. Liu et al. (2017) proposed a new time-varying optimal copula model in order to capture the optimal dependence structure of two time series. They constructed half-rotated copulas to measure the asymmetric and non-linear dependence. In 2016, Zhou (2017) proposed a contagion reaction equation combined with GARCH model to study the dynamic and asymmetric contagion reactions of financial markets during the subprime crisis. Su (2017) used bivariate copulas to measure and test the tail dependence and contagion risk between major stock markets for a period covering the subprime crisis and Greek debt crisis. From the systematically reviewed studies thus far, we noticed that the spread of the financial crisis could not be explained by the fundamentals. The dependence structure is time-varying, asymmetric and non-linear. Our proposed work differs from the work reported in Su (2017) in two ways. Firstly, we use C-Vine (Canonical Vine) model to capture the joint dynamics of all the markets under consideration instead of a bivariate copula in two markets used in Su (2017). Secondly, our designed model finds both the best fit copula and its parameters before and after the crisis periods unlike in Su (2017) where all the pre-specified bivariate copulas are investigated with only best fit in their parameters. Since the financial crisis emanates from one market (root) and spreads violently to the other markets, the C-Vine copula model appears to be most natural and appropriate. The graphical architecture of the C-Vine allows to capture dependence of all variables with respect to a pre-specified variable (root market) through bivariate copulas. We propose to analyze the contagion effect among six international markets namely USA, Russia, India, China, Hong Kong and Japan, during the three financial crisis of Russia (1998), USA (2008) and China (2015). Although the Russian flu (1998) and the subprime crisis (2008) had found their places in the literature, the effects of China slowdown on other economies have not been studied till date. We employ the AR(1)-GJR-GARCH(1,1) model to obtain the conditional volatilities of six stock indices returns and hence get the standardized returns. The probability integral transformation is applied to these standardized returns to obtain the marginal distribution of each index return. A time-varying C-Vine copula model is applied to study the dependence between all possible pairs of markets. The characteristics of the dependence parameters in pre and post crisis are discussed. The layout of the paper is as follows. Section 2 describes the marginal distribution model, time-varying C-Vine copula, and stochastic autoregressive copula (SCAR) C-Vine model. Section 3 presents the proposed methodology. Section 4 reports the data and the descriptive statistics of the data. Section 5 records the empirical results and analysis. Section 6 concludes the paper.",4
53.0,3.0,Computational Economics,20 November 2017,https://link.springer.com/article/10.1007/s10614-017-9770-9,Evolutionary Dynamics of Price Dispersion with Market-Dependent Costs,March 2019,Francisco Álvarez,José-Manuel Rey,Raúl G. Sanchis,Male,Unknown,Male,Male,"The existence of different prices for a given homogeneous good—usually termed price dispersion—has puzzled economists over decades (see e.g. Hopkins 2008; Wildenbeest 2007). Empirical studies reporting the prevalence of price dispersion (see Kaplan and Menzio 2015 and the references therein) and theoretical models trying to rationalize the phenomenon (see Baye et al. 2006 for a summary) have been emerging in parallel over time. A fundamental assumption in all theoretical models is that buyers face search cost when looking for the lowest price. Yet, this assumption does not seem to be sufficient to generate price dispersion. If sellers are aware that a representative buyer faces search costs, each seller may rise his price over his marginal cost. But why different sellers should set different prices? Notice that all sellers setting the monopoly price (defined by the buyer’s willingness to pay) and the buyer looking for just one price constitutes an equilibrium in which there is search cost but not price dispersion. This possibility is known as the Diamond paradox after (Diamond 1971). Two families of models have been suggested to circumvent the Diamond paradox. One type of models considers sellers that are homogeneous with regard to costs and their pricing decisions are defined by mixed strategy equilibria—see Burdett and Judd (1983). The other type of models assumes that production costs across sellers are heterogeneous. In this second family of models, the core idea is simple: under search cost, sellers with higher costs will generally rise their price more than sellers with lower costs. Most studies in this area assume that sellers’ costs are fixed and then analyze market equilibria in a one-shot game—see Baye et al. (2006). This paper explores further the link between price and cost heterogeneity. In a long term equilibrium, it seems a natural idea that not only costs influence prices—which is the standard causality in the literature, but also prices influence costs. Notice that, if causality goes both ways, the initial explanation for price dispersion is not so obvious. We claim that causality from prices to costs rests on entry decisions that arise in a long run perspective. Sellers regularly make entry decisions, which naturally must be based on past information, essentially previous market prices. But while entry decisions determine which sellers operate in the market, they also determine inextricably what costs do. Another essential piece in our model, which seems realistic in markets with a large number of price-setters, is that agents in the market do not have fully detailed information on past prices, but rather they have access to a few summary statistics. We implement a bidirectional causality between prices and costs in a dynamic discrete-time evolutionary game between a large number of potential sellers and a representative buyer. In every period, some synthetic information about prices in the previous period is freely disclosed and serves as an input for all decisions made in the current period. Potential sellers first decide independently whether to enter the market or not by comparing their cost to past prices and then, those who enter set their optimal price given the information about past prices, which vary across sellers as long as costs do. The buyer decides how many sellers (or shops) to visit by balancing his search cost versus the expected minimum price which, as sellers do, is computed using the disclosed information about previous prices. More precisely, we will assume that the available information about past prices consists of an estimate of the distribution of previous prices using a Generalized Error Distribution (GED), which generalizes the Normal distribution to a continuum of leptokurtic and platykurtic distributions while preserving symmetry. This generalization has been reported to have relevance within the empirical literature on price dispersion (Kaplan and Menzio 2015). Our main findings can be summarized as follows. First, some basic features of previous static models in which causality is unidirectional—costs are exogenous—are still present in our model. There exists dispersion in the limiting price distribution which, in addition, is sensitive to initial conditions of the price distribution dynamics. In the first period, agents must base their decisions on prior beliefs, as there are no past prices available, and those beliefs determine to some extent the stationary price distribution. This dependence between initial conditions and the stationary price distribution is an indication of the existence of multiple equilibria. Second, in contrast to the standard literature, our model also provides endogenously a cost distribution. In all case studies in which the model converges, we have found that the standard deviation at equilibrium is larger for prices than for costs. The number of periods required for convergence also depends on initial beliefs. Our analysis shows that a parameter that accounts for the location of the initial beliefs influences negatively the speed of convergence, namely higher location implies slower convergence. Moreover, there is a link between third moments of both price and cost distributions, that is, leptokurtic prices are linked to leptokurtic costs. We thus show how the model provides leptokurtic stationary prices, which is relevant empirically since leptokurtic prices have been recently found to be prevalent in many markets (Kaplan and Menzio 2015). The sensitivity analyses with respect to other parameters of the model are detailed in the main text. In Sect. 2 we introduce our evolutionary model. The computational analysis of the model is presented in Sect. 3. In Sect. 4 we characterize parameter values in our setting that deliver leptokurtic stationary price distributions.",
53.0,3.0,Computational Economics,06 December 2017,https://link.springer.com/article/10.1007/s10614-017-9780-7,Exact Expectations: Efficient Calculation of DSGE Models,March 2019,Fabian Goessling,,,Male,Unknown,Unknown,Male,"Global solution methods for dynamic stochastic general equilibrium (DSGE) models are computationally expensive and hard to implement. In particular, complexity in the global solution of DSGE models arises from the general equilibrium and the occurring stochastics. Firstly, a general equilibrium requires the solution to satisfy a system of stochastic difference equations globally over the state-space. As an analytic solution is generally not available, the solution is approximated by minimizing a residual function at a set of points in the state-space. By choosing the set of points efficiently, e.g. by a Smolyak (1963) sparse grid and corresponding interpolants as in Judd et al. (2014), the computational effort can be reduced. Secondly, the stochastic components of the models of interest require the calculation of (multiple) conditional expectations at each point of the grid. Commonly these integrals are approximated using Gauss–Hermite quadrature rules or simulation based techniques. Both of these techniques require the evaluation of an integrand at several points in the state-space, constituting a bottleneck for the computation. In the present paper, I remove this limitation by replacing the approximated integrals with an analytic expression derived from the approximated integrand, which additionally can be precomputed in the spirit of Judd et al. (2011). As Chebyshev polynomials are a standard choice for global solution methods, see e.g. Caldara et al. (2012), Aldrich and Kung (2010) or Judd (1992), I base my approach on the latter polynomials, but provide a general framework for arbitrary polynomial approximations. Subsequently, I adapt the exact expectations technique to the neoclassical model with recursive utility, labor choice and student-t shocks to log-productivity. The rest of the paper is as follows: Sect. 2 establishes the technique and benchmarks the new method with regards to accuracy, efficiency and computational time. Section 3 applies the newly developed method to the benchmark model and Sect. 4 concludes.",
53.0,3.0,Computational Economics,07 December 2017,https://link.springer.com/article/10.1007/s10614-017-9777-2,Finite Gaussian Mixture Approximations to Analytically Intractable Density Kernels,March 2019,Natalia Khorunzhina,Jean-François Richard,,Female,Unknown,Unknown,Female,"Starting with early contributions more than a century ago by Newcomb (1886), Holmes (1892), Weldon (1892, 1893), and Pearson (1894) among others, finite mixtures have been continuously used in statistics (see section 2.18 in McLachlan and Peel 2000 for a short history of finite mixture models and Table 2.13 in Titterington et al. 1985 for an extensive list of direct applications of mixtures; see also the monographs of Everitt and Hand 1981; Scott 1992; Frühwirth-Schnatter 2006). More recently, mixtures of normal distributions have been increasingly applied in macro- and micro-economics (e.g., regime-switching models of economic time series in Hamilton 1989, or analysis of dynamics of educational attainment in Keane and Wolpin 1997; Cameron and Heckman 2001), marketing science (structured representation of market information in DeSarbo et al. 2001, and forecasting of new product sales in Moe and Fader 2002), and empirical finance (modeling stock returns in Kon 1984; Tucker 1992, value-at-risk in Duffie and Pan 1997; Venkataraman 1997; Hull and White 1998, stochastic volatility models in Kim et al. 1998; Omori et al. 2007). In the present paper we focus our attention on the specific problem of using finite mixture of Gaussian densities for approximating a non-standard density kernel. Such approximations are critically needed when inference requires numerical integration of an analytically intractable density kernel, such as a marginal likelihood for a non-linear and/or non-Gaussian state-space model or a Bayesian posterior density . Whether one relies upon direct numerical integration (Gaussian quadratures) or simulation methods such as importance sampling (IS) or Markov Chain Monte Carlo (MCMC), the numerical accuracy of the results critically depends on the quality of approximation. For example, an inefficient importance sampler might require prohibitive number of draws to produce accurate results, or might even fail to converge due to tail problems (see, e.g., Geweke 1996). Finite mixtures are conceptually attractive within this context since theoretically they can produce accurate approximations to most density functions, depending upon the number of components (Ferguson 1973). There exist a vast literature which proposes various procedures for constructing finite (mostly Gaussian) mixture approximations. In a nutshell, the key numerical issues are the selection of a distance measure to assess goodness of fit, the (typically sequential) determination of the number of terms in the approximating mixtures and the estimation of its component parameters and weights. Extending earlier proposals by West (1992), Oh and Berger (1993), Cappé et al. (2004), Douc et al. (2007), and Cappé et al. (2008) proposes an adaptive algorithm to optimize the IS performance of a mixture sampler with a predetermined number of components. Specifically, their Mixture Population Monte Carlo (M-PMC) algorithm aims at maximizing the entropy criterion between a target kernel and the mixture approximation. It is adaptive in that it relies upon sampling from the current mixture proposal in updating its weights and component parameters. Convergence is assessed on the basis of the Shannon entropy of the normalized IS ratios. 
Hoogerheide et al. (2007) propose an adaptive algorithm to construct mixtures of Student-t distributions to approximate an arbitrary target density with the objective of minimizing the variance of the corresponding IS ratios. Adaption means that the components of the mixture are introduced sequentially until a good enough fit is obtained. This algorithm has been implemented within the R package AdMit in Ardia et al. (2009). A subsequent adaptive algorithm is developed by Hoogerheide et al. (2012) and implemented into the R package MitISEM by Basturk et al. (2012). As we shall see, the algorithm we propose below is adaptive in the sense of Basturk et al. (2012), but differs in several important ways: it relies upon a different distance measure; the latter is evaluated by Gaussian quadrature instead of importance sampling (classical) or Metropolis-Hastings (bayesian); optimization relies upon an analytical gradient optimizer and initial values are computed differently. 
Giordani and Kohn (2010) propose an adaptive Independent Metropolis-Hastings algorithm for constructing mixture proposal densities. Fast re-estimation of the mixtures relies upon a k-means algorithm discussed in Bradley and Fayyad (1998) and subsequently in Hamerly and Elkan (2002) and Giordani and Kohn (2010). Efficient designs rely upon reducing the number of re-estimations as coverage improves. 
Kurtz and Song (2013) propose a Cross-Entropy-Based Adaptive Importance Sampling algorithm to construct an optimal Gaussian mixture IS density with a preassigned number of terms. The objective function that is sequentially minimized is the Kullback–Leibler cross-entropy between the target density and the mixture. The approach of Bornkamp (2011) relies upon iterated Laplace approximations to add components one by one as needed. However, only the weights of the mixture components are re-optimized with each iteration while their Laplace modes and inverted Hessians are left unchanged. It immediately follows that a mixture target cannot be reproduced. In sharp contrast our algorithm includes full sequential re-optimization to the effect that if the target density is a mixture it reproduces it exactly as we shall illustrate in Sect. 3.1 for example 2 in Bornkamp (2011). In this paper we propose a fully adaptive algorithm to construct Gaussian mixture approximations to a low-dimensional (\(n\le 3\)) target density kernel. Our algorithm is also applicable in higher dimensional models that can be factorized into a linear Gaussian conditional density and a marginal non-standard density to be approximated by a mixture. An example of such dimension reduction is provided in Sect. 3.2 below. Our algorithm includes full re-optimization with the introduction of each additional component. Since such mixture approximations will often be used as importance sampling or proposal densities, we use an efficient importance sampling (EIS) approximation of the sampling variance as our distance measure to be minimized, whereby optimization takes the form of an auxiliary non-linear least squares problem. Our algorithm is illustrated by several test cases. The first application approximates a mixture of three bivariate normal distributions and demonstrates the ability of the proposed algorithm to exactly reproduce the target mixture. The second application approximates a bivariate skew-distribution, a class of densities of growing importance in economics (modeling fertility patterns in Mazzuco and Scarpa 2015, stochastic frontier analysis in Domínguez-Molina et al. 2004, sample selection models in Marchenko and Genton 2012; Ogundimu and Hutton 2016) and finance (capital asset pricing models in Adcock 2004, 2010). Our third application deals with a basic stochastic volatility model, whose measurement density can be approximated by a mixture of normal distributions (see, e.g. Kim et al. 1998; Omori et al. 2007). The potential scope of applications of our procedure is not limited to approximating analytically intractable densities. Our procedure provides alternative numerical solutions to a wide range of problems in economics and finance, some of which we outline in the paper. The paper is organized as follows: the baseline algorithm is presented in Sect. 2; examples are presented in Sect. 3. In Sect. 4, we discuss future research plans together with pilot applications. Section 5 concludes. Technical derivations are regrouped in as Appendix.",
53.0,3.0,Computational Economics,08 December 2017,https://link.springer.com/article/10.1007/s10614-017-9783-4,Dissimilarity-Based Linear Models for Corporate Bankruptcy Prediction,March 2019,Vicente García,Ana I. Marqués,Humberto J. Ochoa-Domínguez,Male,Female,Male,Mix,,
53.0,3.0,Computational Economics,09 December 2017,https://link.springer.com/article/10.1007/s10614-017-9779-0,Quantile Regression for Dynamic Panel Data Using Hausman–Taylor Instrumental Variables,March 2019,Li Tao,Yuanjie Zhang,Maozai Tian,,Unknown,Unknown,Mix,,
53.0,3.0,Computational Economics,09 December 2017,https://link.springer.com/article/10.1007/s10614-017-9781-6,Simulation of Contagion in the Stock Markets Using Cross-Shareholding Networks: A Case from an Emerging Market,March 2019,Hossein Dastkhan,Naser Shams Gharneh,,Male,Male,Unknown,Male,"Since the financial crisis in 2008, the systemic risk terminologies are one of the main focuses of financial studies. Even though there is no consensus about the definition of systemic risk, it relates to the failure of the whole financial market consisting of many interconnected agents such as firms, banks, funds and other financial and non-financial institutions. In the finance literature, contagion in financial markets is considered as the main element of systemic risk, so that in some cases, these concepts are used interchangeably. These concepts are also known in some studies as the cascading failure. Despite the importance of the financial contagion and systemic risk in the financial systems, it is apparent that the related investigations are still in the first steps (Lorenz et al. 2009). The systemic risks could be divided in two categories according to the source of the initial failure. In the first category, a failure in one or a few firms, which they have a significant role in the structure of the financial systems can leads to a systemic risk in the whole parts of the system. These firms are so called as the systemically important institutions-SIFIs in the literature. In this case, the firm’s contributions in the systemic risks (so called as the systemic importance) are not the same and thus, the identification of the most important firms can be effective in the systemic risk management. The second type of systemic risks originates from the collective behavior of the firms over time. In this case, there are not firms with significant role in the crisis, but the buildup of the series of small shocks on the financial system result in the systemic risk event (Smaga 2014). In the recent years, there are numerous attempts in order to apply the network theory in the analysis of financial systemic risk and contagion. Most of the non-network approaches are limited to represent practical solutions to the regulators and policymakers in the actual financial systems with a large number of firms and links. As the financial networks are strictly interconnected and the network theory is ideally suitable for the analysis of the interconnected systems, many studies have applied the tools of network theory in the analysis of the systemic risks and contagions. Moreover, the network theory helps us to capture the indirect relationships between different firms which can lead contagion via the consecutive failures in the financial markets. However, given the extent of the financial markets and different ways of exposures, they have solved only a small part of the puzzle. In other words, review of the current studies indicates that the majority of the studies are related to the systemic risk analysis in the interbank networks. Generally, we can divide the related literature in two different groups. The first group focus is on the statistical characteristics of the financial network structures. In the second group, the simulation models of contagion are represented to investigate the stability and resilience of the financial network in accord to different kinds of failures (Pecora and Spelta 2015). One of the first papers in the first group is related to Mantegna (1998) who tried to use the stock correlations to introduce the stock relation network. He developed a network representation of the stock market based on the daily stock prices of the New York stock exchange. Moreover, he showed that how the correlation based network changes over time. After his paper, a lot of studies tried to develop his model in order to analyze the correlation based stock trees in different countries and used the related networks in the portfolio selection problem and market index calculation (Onnela et al. 2003). Likewise, there are some other papers in the first group which are related to the interbank debt networks. For example, Moussa (2011) tried to analyze the statistical characteristics of the Brazilian interbank debt network and introduced new systemic risk measures. There are many similar studies which analyze the general structure of the interbank network in different countries (for example, see Boginski et al. 2006; Liu and Tse 2012). In addition to the above-mentioned studies, some other studies in the first group consider the statistical analyses of the cross-shareholding networks. The pioneering work of Battiston (2004) can be categorized in this group. He represents a comparative description of the cross-shareholding networks of the New York, London and Italian stock exchange and introduced some centrality measures to identify the important nodes. After him, Glattfelder et al. applied a global cross-shareholding network to investigate the control structure in the global financial system. They compared the direct and integrated cross-shareholding networks and analyzed the control flow in the financial system. Moreover, there are some other studies which used the cross-shareholding networks of financial markets in the other countries (Ma et al. 2011; Li et al. 2014; Dastkhan and Gharneh 2016). On the other hand, Allen and Gale introduced one of the first attempts in the second group of financial network literature (Allen and Gale 2000). In their pioneering work named “financial contagion”, they dealt with the contagion in the interbank structure in accord to the liquidity preferences of banks. Using different typical example and away from empirical study, they showed that in the case of the lack of an aggregate uncertainty in the banking systems, the interbank debts are the best solution to the liquidity shocks. However, in the case of a small shock related to liquidity preference, the banking systems are very fragile and spread the shock to the economy. They have claimed that the chance of crisis depends strictly on the degree of completeness of the network and thus, a complete interbank debt structure is less vulnerable to the financial crisis. In recent years, various related studies are represented to develop their model in different aspects. For example, Karimi and Raddant (2016) applied real dataset of the interbank debt network of Italy and represented a contagion model to investigate the network stability. In a similar paper, Carciente (2015) applied stress tests to analyze the stability of Venezuelan banking system for the incidence of different scenarios of shocks in the values of banks’ assets. In another study, Paltalidis et al. discussed the systemic risk and contagion in European banking system using the maximum entropy method (Paltalidis 2015). They represented a simulation model to analyze the results of three kinds of different shocks including the interbank, asset values and sovereign credit risks. In addition to the above, there are numerous studies in the application of network theory to analyze the contagion in the financial markets (Batiz-Zuk 2015; Caggiano et al. 2014; Chen et al. 2015, 2017). Despite the vast range of studies in the field of systemic risk and contagion, there are a lot of aspects of exposure which are not considered as a source of systemic risk. In other words, most of the current studies concentrate on one or some specific layers of financial and economic systems. Up to our knowledge, even though the previous studies recognized the cross-shareholding and ownership network as an outstanding source of exposure, they do not utilize as a way of exposure in the financial contagion models. In this paper, we focus on the cross-shareholding relationships between the firms as a channel of exposure in the financial markets. Most similar study to the current paper is the paper of Elliott et al. (2014), where the cross-shareholding is applied to analyze the shock spread in the networks. They focused on the concepts of integration and diversification in a cross-shareholding network. They showed that a financial system with an intermediate level of integration and diversification is more vulnerable to the financial shocks. They also investigated the results of their model through a real example related to the cross-holding of debt between different European countries and showed the contribution of different countries in the financial crisis. In this paper, we represent a general model to analyze the contagion and systemic risk in a network of firms which are financially interconnected via ownership relations. Accordingly, the value of each firm is depended on the ownership of stocks, debts and the other liabilities of the related firms. When the value of a firm is sufficiently reduced so that passes a pre-determined failure threshold, it fails and spreads its losses to the related firms. Due to the interconnection of the network, the losses of the failed firm spread even to the firms which are not directly related to the failed firm. At each stage, the losses of the other firms can cross the failure threshold and discontinuously decrease their values. Accordingly, a small and firm-specific shock on one firm can amplify and propagate to the whole financial system. Using the cross-shareholding network, we calculate the value of a firm based on the intrinsic value of the firm and the total relative values of the related firms. We apply the multiplier asset valuation model to determine the values of the firms. It means that the firms’ values are calculated as a multiplier of the firm’s earnings per share (EPS). As a result, a shock in the EPS of a firm not only changes the value of the firm, but also reduces the earnings and the values of its owners. Similarly, the decrease in the EPS of the second wave firms decreases the EPS and the values of their owners and so on. Following the process until the steady state of the system achieve, a significant portion of firms will be failed. It is remarkable that the contagion in the financial systems occurs in “waves”. Some initial failures are enough to cause a second wave of firms to fail. Once these firms fail, a third wave of failures may occur and the process is continued until the system reaches its equilibrium. Generally speaking, this paper represents a simulation model to analyze the contagion and systemic risk in stock markets using cross-shareholding networks. According to a real data set from Tehran Stock Exchange, we investigate the probability and the extent of contagion in the case of idiosyncratic and aggregate shocks. Moreover, we analyze the results of shock incidence on the specific sectors of the market and recognize the most influential sectors in the financial contagion. Finally, the results of the real network are compared with the null models. The null models are the networks based on the real network but with different structural characteristics. The null model analysis also helps to determine which of the network structures are most similar to the real network when there is a lack of real data. In order to analyze the contagion in the financial markets, we propose a simulation algorithm to compute the probability and the extent of contagion. Using the proposed algorithm, it is possible to propagate the failure costs at each stage of contagion and determine which firms fail and to what extent the loss occur in the next waves. Policy makers can use the proposed model to run counterfactual scenarios and identify the systemically important firms. Moreover, the proposed model can analyze the results of aggregate small shocks on a large number of firms and investigate the probability and the extent of systemic risk in the financial systems. Regardless of the results, the contagion models can not accurately encompass all the aspects of the financial markets. For example, these models do not consider many kinds of activities among firms such as commercial trades and contracts. The paper do not tried to develop a comprehensive model of contagion in financial markets. In order to do so, we need a more complicated model with numerous layers of related components and dynamic structures. However, obtaining a comprehensive model requires analyzing the results and the behavior of different layers, separately. In this paper, we consider the cross-shareholding network of financial markets as one of the main layers of the model. The current paper contributes to the literature in 2 main directions. The first contribution is that we develop a new model based on the cross-shareholding network to simulate the contagion in the financial system. The second direction of our contribution is empirical. Similar to the paper of Karimi and Raddant (2016), we also use empirical data to calibrate our model and compare the results with different null models. Another empirical contribution is that for the first time, we extend our dataset to the firms in the stock market. Moreover, the paper investigates the results of different kinds of aggregate small shocks as well as the idiosyncratic shocks on a specific firm and compares the behavior of the system in the presence of both kinds of shocks. The results of the paper can help different players of the financial systems on different ways. The model helps the financial regulators and policymakers to know the systemically important firms and provided them to apply appropriate prudential policies on these firms to control the systemic risk events. Similarly, the portfolio managers and the investors are also interested to use the results of systemically important firms to mitigate the selection of portfolios with high levels of vulnerability to systemic risk events. The numerical aspect of the model makes possible to analyze the magnitude of systemic crisis in the case of different kinds of shocks in the financial markets. The remainder of the paper is organized as follows. In Sect. 2 we explain the dataset, the preliminaries for the contagion models and the proposed simulation model. In Sect. 3 we discuss the empirical results in two different parts. In the first part, we investigate the simulation results of the real network. In the second part, we compared the simulation results with those of different null models. Section 4 concludes the paper.",10
53.0,3.0,Computational Economics,12 December 2017,https://link.springer.com/article/10.1007/s10614-017-9786-1,RETRACTED ARTICLE: Analyses of Economic Development Based on Different Factors,March 2019,Goran Maksimović,Srđan Jović,Marina Jovović,Male,Male,Female,Mix,,
53.0,3.0,Computational Economics,13 December 2017,https://link.springer.com/article/10.1007/s10614-017-9785-2,A Nonlinear Optimal Control Approach to Stabilization of Business Cycles of Finance Agents,March 2019,G. Rigatos,P. Siano,T. Ghosh,Unknown,Unknown,Unknown,Unknown,,
53.0,3.0,Computational Economics,28 December 2017,https://link.springer.com/article/10.1007/s10614-017-9778-1,Optimal Stop-Loss Reinsurance Under the VaR and CTE Risk Measures: Variable Transformation Method,March 2019,Junhong Du,Zhiming Li,Lijun Wu,Unknown,Unknown,Unknown,Unknown,,
53.0,3.0,Computational Economics,04 January 2018,https://link.springer.com/article/10.1007/s10614-017-9784-3,Bayesian Testing for Leverage Effect in Stochastic Volatility Models,March 2019,Jin-Yu Zhang,Zhong-Tian Chen,Yong Li,,Unknown,,Mix,,
53.0,3.0,Computational Economics,11 January 2018,https://link.springer.com/article/10.1007/s10614-018-9793-x,A Practical Approach to Testing Calibration Strategies,March 2019,Yongquan Cao,Grey Gordon,,Unknown,Unknown,Unknown,Unknown,,
53.0,3.0,Computational Economics,13 January 2018,https://link.springer.com/article/10.1007/s10614-017-9789-y,Systematic Sensitivity Analysis of the Full Economic Impacts of Sea Level Rise,March 2019,T. Chatzivasileiadis,F. Estrada,R. S. J. Tol,Unknown,Unknown,Unknown,Unknown,,
53.0,3.0,Computational Economics,18 January 2018,https://link.springer.com/article/10.1007/s10614-018-9795-8,A New Prediction Model Based on Cascade NN for Wind Power Prediction,March 2019,Amirhosein Torabi,Sayyed Ali Kiaian Mousavy,Nasser Yousefi,Unknown,Unknown,Male,Male,"With the deterioration of the environment, decrease of old resources and global warming, clean energy has attracted people’s attention. Power generation accounted for about 24% of greenhouse gas emissions in year 2000 (World Wind Energy Association 2000). Also, 26% of greenhouse gas emissions in Europe are related to the production of public electricity and heat in year 2007 (Kiani and Kastens 2008). Wind power generation is an important green resource used in many countries to replace conventional generation and reduce greenhouse gas emissions (Akbary et al. 2017). Different multi megawatt turbines are commercially available for installation in utility scale wind farm configurations (World Wind Energy Association 2000). So, wind power generation has been growing rapidly in many countries. For instance, the records show that wind power generation has been expanded with an annual rate of 25% since 1990 and demonstrates a great potential in many regions of US (Ghadimi et al. 2017a). In Spain, wind power generation accounts for 4% of its electricity consumption (Ghadimi and Firouz 2015). In the Greek island of Crete, wind power generation may reach 20–40% of its power generation (Milligan et al. 2003). In Ahmadian et al. (2014), it has been discussed that wind will supply over 20% of the energy in the future UK system, which requires about 26 GW of installed wind power. With the fast growth of wind energy, the large-scale integration of this volatile energy source negatively impacts operation of electric power systems. Forecasting the wind power is one of the most important methods against the challenges that large scale wind power integration brings to power system. Thus, more efficient wind power forecast methods are still demanded to further improve not only the forecast accuracy but also the risk assessment (World Wind Energy Association 2000; Kiani and Kastens 2008). For this purpose, several models have been proposed by researchers to tackle wind power forecasting problem. So, recently, several prediction models have been proposed. Some of the proposed models can be described as; Auto-Regressive Moving Average (ARMA) models (Akbary et al. 2017; Ghadimi et al. 2017a), Fractional ARIMA (FARIMA) model (Ghadimi and Firouz 2015), Auto-Regressive Integrated Moving Average (ARIMA) model (Milligan et al. 2003), NN-based forecast engine in Ahmadian et al. (2014), combination of differential Empirical Mode Decomposition (EMD) and Relevance Vector Machine (RVM) in Pedregal and Trapero (2007) for prediction of short-term wind power output, two Neural Network (NN) based models in Bonfil et al. (2015), combination of nonparametric and time-varying regression and time-series model, i.e. Holt–Winters and ARMA in Lin et al. (2010), and Hybrid Iterative Forecast Method (HIFM) in Mohammadi et al. (2017) based on two stage feature selection model. Although the mentioned approaches are simple and strong forecasting methods and can be easily implemented, most of the predictors are linear, however, wind power signal is a no convex and a non-linear signal. In this paper, a new decomposition model is presented to get the input signal. Then, a new feature selection method for filtering the selected candidates will be introduced. Also, a hybrid forecast engine based on three stage cascade neural network is implemented over different test cases. The parameters of proposed forecast engine will be optimized by an intelligent algorithm. So, the contributions of this paper can be summarized as follows; A new decomposition model based on empirical mode will be presented to provide the needed details for the next stage. A new feature selection has been presented for selecting the features of wind power features based on high relevancy and low redundancy. A novel and efficient forecast engine for wind power generation based on three stage NN is proposed where, is consists of three cascade NN. The parameters of proposed forecast engine will be optimized by new intelligent algorithm to increase the accuracy of forecast procedure. Three real world engineering test cases have been considered to demonstrate the validity of proposed model in comparison with other techniques. The rest of the paper is organized as follows. In Sect. 2, the architect of the proposed feature selection is introduced. Section 3 presents the suggested forecast engine. Numerical results obtained from the proposed wind power forecasting strategy are presented and discussed in Sect. 4. Finally, Sect. 5 concludes the paper.",13
53.0,3.0,Computational Economics,14 February 2018,https://link.springer.com/article/10.1007/s10614-018-9802-0,Surrogate Modelling in (and of) Agent-Based Models: A Prospectus,March 2019,Sander van der Hoog,,,Male,Unknown,Unknown,Male,"Agent-Based Models (ABMs) are becoming a powerful new paradigm for describing complex socio-economic systems. A very timely issue for such models is their empirical estimation. The research programme described in this paper will use machine learning techniques to approach the problem, using multi-layer artificial neural networks (ANNs), such as Deep Belief Networks and Restricted Boltzmann Machines. The seminal contribution by Hinton et al. (2006) introduced a fast and efficient training algorithm called Deep Learning, and there have been major breakthroughs in machine learning ever since. Economics has not yet benefited from these developments, and therefore we believe that now is the right time to apply Deep Learning and multi-layer neural nets to agent-based models in economics. The agenda of this paper is to briefly sketch the current state-of-the-art in Artificial Intelligence (AI) and Machine Learning (ML), and apply them to economic decision-making problems. We outline a research programme that encompasses co-evolutionary learning, learning from experience, and artificial neural networks (ANN), and connect these to agent-based modelling in economics and policy-making. We begin by tracing back the common heritage of complexity economics and evolutionary economics. A rich body of work by evolutionary economists deals with decision-making by real economic agents, in an attempt to capture their routines by automatizing their decision-making processes in computer algorithms. This starts with Herbert Simon’s “A Behavioral Model of Rational Choice” (Simon 1955), it continues with Cyert and March’s “A Behavioral Theory of the Firm” (Cyert and March 1963), and culminates in Simon’s seminal work on “The Sciences of the Artificial” (Simon 1996 [1969]). Around the same time, a computer science conference on “The Mechanization of Thought Processes” was held at the National Physical Laboratory (NPL) in 1958 (National Physical Laboratory 1959). The conference proceedings contain many of today’s hot topics in Machine Learning: automatic pattern recognition, automatic speech recognition, and automatic language translation. At the time, the developments in AI, machine learning and theories of human decision-making were strongly intertwined. After describing this common heritage, we take stock of the current state-of-the-art in Machine Learning and extrapolate into the not too distant future. As a first topic, we propose to emulate artificial agent behaviour by so called surrogate modelling inside the model (this is the “in” part of the title). Such an emulator is an approximation or an imitation of the original system, that behaves in the same way but is computationally more tractable. In an ABM this could be thought of as a doppelgänger approach, in which one agent is observing another agent’s behavioural pattern and their performance. It then tries to mimic that agent’s behavioral pattern, and eventually replaces it in the simulation. In addition, the concept of an ANN-Policy-Agent is introduced who learns from observations of successful policy actions through a reinforcement learning mechanism. Reinforcement learning (RL) is a machine learning method in which an agent is situated in an environment and must select actions that result in outcomes, for which the agent receives a reward (or a penalty). The objective is to maximize the expected life-time rewards. Over time, the agent should learn what is the best course of action over a set of observed states of the environment, and this learned strategy is called the agent’s policy. Since there may be some delay between taking an action and observing the outcome and receiving the reward, it is common in RL to discuss the credit assignment problem, which is the problem of correctly associating the current reward to a particular action. For economic policies, such as monetary policy, this can be of considerable importance since a considerable amount of time could pass by between taking an action (e.g., increasing the policy interest rate) and the outcome, with many other economic processes happening at the same time. A second topic of the paper is that we propose to use ANNs as computational emulators of entire ABMs (this is the “...(and of)” part in the title). In this context, the idea of an emulator or surrogate model is that the ANN functions as a computational approximation of the non-linear, multivariate time series generated by the ABM. There are various advantages to having such an emulator. It allows for a computationally tractable solution to the issue of parameter sensitivity analysis, robustness analysis, and could also be used for empirical validation and estimation. This is particularly appealing for large-scale ABMs that are computationally costly to simulate.Footnote 1 The goal is to develop new computational methods to improve the applicability of macroeconomic ABMs to economic policy analysis. When successful, we would have drastically reduced the complexity and computational load of simulating ABMs, and come up with new methods to model economic agents behaviour. Linking the time series forecasting capabilities of ANNs to ABMs also allows us to envision the possibility of docking experiments between different ABMs: the time series output from one ABM can be fed into a deep-layered ANN as input. This ANN can then be used as an agent inside another, larger-scale ABM. This notion leads to a hierarchical modelling scheme, in which ABMs of ABMs would become feasible. Each agent in the larger ABM can have an internal mental model? of the world it inhabits, and those mental models may differ to any degree due to different experiences. On the longer term, this approach would allow the inclusion of computational cognitive models into economic ABMs, allowing the agents to be fully aware of their environment, and to consider the social embedding of their interactions. There are currently several research efforts under way to construct agent-based macroeconomic models (Dawid et al. 2018a; Dosi et al. 2017; Grazzini and Gatti 2013). These models aim to compete with standard Dynamic Stochastic General Equilibrium (DSGE) models that are currently in use by most Central Banks around the world. Using DSGE models for policy analysis requires that they are calibrated and estimated on empirical data. For the ABMs this is no different, but we need new methods and techniques to estimate such policy-oriented ABMs. The empirical estimation and validation of this type of model is an active field of research (see e.g., Werker and Brenner 2004; Brenner and Werker 2006; Fagiolo et al. 2007; Grazzini et al. 2012, 2013; Grazzini and Richiardi 2013; Yildizoglu and Salle 2012; Barde 2016, 2017; Lamperti 2015; Lamperti et al. 2017; Lux and Zwinkels 2018; Barde and van der Hoog 2017). However, until now no clear consensus has emerged on how to resolve the empirical validation problem. In order for agent-based models to be useful for economic policy analysis so called “what-if scenarios” are used to test counter-factuals in would-be worlds. It is therefore necessary to use models with a sufficiently high resolution in terms of behavioural patterns and institutional details. For the large-scale agent-based models that we consider in this paper, ‘large-scale’ means on the order of thousands of agents. High-resolution may refer to the resolution of time-scales, geographical scales, decision-making scales (number of options to consider), or other dimensionality of the agents’ characteristics. The advantage of such large-scale, high-resolution, high-fidelity agent-based models is that they can be used as virtual laboratories, or as laboratory “in silico” (Tesfatsion and Judd 2006). The model can be used for testing various economic policy scenarios (Dawid and Fagiolo 2008; Dawid and Neugart 2011; Fagiolo and Roventini 2012b, a), that may not be feasible to test in the real world (e.g., due to ethical objections). Examples os such scenarios include: What happens when the biggest banks go bankrupt? Or: What happens when a Euro member leaves the Euro? Obviously, these are not things we want to simply test in the real world, considering the detrimental social consequences and ethical objections. In many cases, we do not know the correct equations of the economic model, and we might only know how to model the behaviour of the artificial economic agents approximately by observing the behaviour of their empirical counterparts (e.g., through direct observation of market participants, or through laboratory experiments). Therefore, we do not have access to the mathematical description of the economic system, and have to resort to computational modelling. The disadvantage of such large-scale ABMs is that they are quite heavy computationally. It is easy to generate overwhelming amounts of data, and reach the boundaries of what is commonly accepted to be computationally tractable, in terms of simulation time, number of processors used, and data storage requirements. If we then want to apply such models to perform policy analyses, we have to test the robustness of the model, i.e., test whether the empirical stylized facts are still reproduced for many parameter settings. This involves performing a global parameter sensitivity analysis and a robustness analysis against small changes in the economic mechanisms, or with respect to changes in the individual behavioural repertoires of the agents. This usually requires a large number of simulations (on the order of thousands), in order to obtain a large enough sampling of the phase space, and to be able to ascertain whether the model is sensitive, stable, robust, or fragile. In combination with the computational burden of running these simulation models, the current generation of large-scale agent-based simulation models require large computing systems, such as multi-processor servers, HPCs, or grids of GPUs, in order to run sufficiently many simulations. This not only involves running large numbers of simulations for producing results for publications, but also to perform rigorous robustness testing, parameter sensitivity analyses, and general verification and validation (V&V) procedures to ensure the correctness and validity of the computer simulations (cf. Sargent 2011; Kleijnen 1995, 2015). The issue of computational intractability is therefore looming and ubiquitous. But this problem has been around for a long time in physics and climate science, where research using many-particle systems and large-scale climate models is constantly pushing the frontier of what is feasible from a computational point of view. For economics, the scientific relevance and innovativeness of this line of research is that it tries to solve the generic problem of computational tractability of computer simulation models (and more specifically, of agent-based economic models) from an algorithmic point of view, and not by resorting to technological solutions (e.g., parallel computing, or GPU grids). The innovation is to use machine learning algorithms in order to reduce the computer simulation to a lighter form, by emulating the models. One such method is to use artificial neural networks as the surrogate modelling approach, but other methods exist as well. Note however that in order to use the machine learning algorithms a large amount of training data has to be generated.",20
53.0,3.0,Computational Economics,19 February 2018,https://link.springer.com/article/10.1007/s10614-017-9782-5,Pricing Perpetual American Lookback Options Under Stochastic Volatility,March 2019,Min-Ku Lee,,,Unknown,Unknown,Unknown,Unknown,,
53.0,3.0,Computational Economics,28 March 2018,https://link.springer.com/article/10.1007/s10614-018-9807-8,Quanto Option Pricing with Lévy Models,March 2019,Hasan A. Fallahgoul,Young S. Kim,Jiho Park,Male,,Unknown,Mix,,
53.0,4.0,Computational Economics,07 March 2018,https://link.springer.com/article/10.1007/s10614-018-9805-x,An Integrated Approach to Forecasting Intermittent Demand for Electric Power Materials,April 2019,Aiping Jiang,Qiuguo Chi,Maoguo Wu,Unknown,Unknown,Unknown,Unknown,,
53.0,4.0,Computational Economics,17 March 2018,https://link.springer.com/article/10.1007/s10614-018-9806-9,Risk: An R Package for Financial Risk Measures,April 2019,Stephen Chan,Saralees Nadarajah,,Male,Unknown,Unknown,Male,"A risk measure say \(\rho \) (from a class of random variables to the positive real line) is said to be a coherent risk measure (Artzner et al. 1999) if its satisfies the following axioms: for all X and all real numbers \(\alpha \), we have \(\rho (X+\alpha ) = \rho (X)-\alpha \) (Translation), for all \(X_1\) and \(X_2\), \(\rho \left( X_1 + X_2 \right) \le \rho \left( X_1\right) + \rho \left( X_2 \right) \) (Sub-additivity), for all \(\lambda \ge 0\) and all X, \(\rho (\lambda X)=\lambda \rho (X)\) (Positive homogeneity), for all X, Y with \(X\le Y\), \(\rho (Y)\le \rho (X)\) (Monotonicity). A risk measure is deemed to be good if it is a coherent risk measure. Value at Risk (VaR) and Expected Shortfall (ES) are the two most popular measures of financial risk. The VaR is due to Till Guldimann in the late 1980s. He was then the head of global research at J. P. Morgan. The ES is due to Artzner et al. (1999). Both the VaR and the ES have some disadvantages. The VaR not only ignores any loss beyond the VaR level and also can not satisfy the axiom of sub-additivity. Some other disadvantages of the VaR are mentioned in Yamai and Yoshiba (2002). A disadvantage of the ES is that backtesting (a method for checking forcastability of the measure) is difficult. Another disadvantage of the ES is that its estimate may not be as accurate as that of the VaR, as shown by Yamai and Yoshiba (2002). There are however backtesting methods for the ES. Two methods based on excesses are proposed in Embrechts et al. (2005). Other backtesting methods can be found in the excellent book McNeil et al. (2015). In the last ten years or so, many other measures of financial risk have been proposed. But there have been hardly any applications of these measures. The aim of this paper is to provide a software for these relatively new measures. The software also computes some risk measures predating the VaR that are not well known. This software could encourage applications and also development of further risk measures. We are not aware of any (R Development Core Team 2017) package for risk measures other than the VaR and the ES. There are not comprehensive R packages even for the VaR and the ES. The ones we are aware of are actuar (Goulet et al. 2017), ghyp (Luethi and Breymann 2016), PerformanceAnalytics (Peterson and Others 2014), crp.CSFP (Jakob et al. 2016), fAssets (Rmetrics Core Team et al. 2014a), fPortfolio (Rmetrics Core Team et al. 2014b), CreditMetrics (Wittmann 2009), fExtremes (Wuertz 2013), rugarch (Ghalanos 2015), evir (Pfaff et al. 2012), QRM (Pfaff and Others 2016) and VaRES (Nadarajah et al. 2013). Except for the last one, most of these packages compute the VaR and the ES for restricted distributions like the normal distribution. VaRES (Nadarajah et al. 2013) computes the VaR and the ES for over 100 distributions. The newly introduced R package computes twenty six risk measures for any user specified continuous distribution. The risk measures computed are: VaR, ES, tail conditional median, expectiles, beyond value at risk, expected proportional shortfall, elementary risk measures, Omega, Sortino ratio, Kappa, Wang’s risk measure, Stone’s risk measures, Luce’s risk measures, Sarin’s risk measures and Bronshtein and Kurelenkova’s risk measures. The details of the package are given in Sect. 2. Its practical use is illustrated in Sect. 3. Some concluding remarks are given in Sect. 4.",6
53.0,4.0,Computational Economics,02 April 2018,https://link.springer.com/article/10.1007/s10614-018-9809-6,An Optimal Mortgage Refinancing Strategy with Stochastic Interest Rate,April 2019,Xiaoxia Wu,Dejun Xie,David A. Edwards,Unknown,Unknown,Male,Male,"A fixed-rate mortgage contract is a financial product that requires the contract holder (the mortgage borrower) to make a periodic repayment of the loan to the contract issuer (the mortgage lender) until the end of the contract. Matching the payment of principal and interest method is a widely used scheme, i.e., the borrower regularly makes the same amount of repayment before its maturity. The residential mortgage market has seen a slow recovery since the 2008 financial crisis and the recent low level of mortgage rates has caused mortgage borrowers’ great interest in seeking refinancing opportunities. For instance, the borrower can prepay the outstanding balance to terminate the existing mortgage contract by entering a new mortgage contract with a lower mortgage rate based on the current spot rate. Consequently, the total interest payment is reduced. However, refinancing costs (attorney, document, title fees, etc.) and transaction costs are considerably large. Frequent refinancing can quickly cancel out the potential gain. In this work, we concentrate on the optimal refinancing problem where only one refinancing opportunity is allowed under the stochastic interest rate environment. One then can relax this restriction and allow more than one refinancing through suitable generalizations of the model. In a deterministic interest rate environment, it is always optimal to refinance the mortgage whenever the value of gain from refinancing is positive (Siegel 1984). However, in the stochastic interest rate environment, a potential benefit may exist if one waits for the rate to decline further. In order to see how the stochastic factor influences the optimal refinancing time, we assume no refinancing costs, transaction costs or prepayment penalty. The one-opportunity no-cost refinancing problem can be considered as an optimal early exercise decision for American options, in which a possible solution is known as the option-based approach. That is, given a lower prevailing mortgage rate, at any time one can exercise the option to refinance the mortgage debt. Kalotay et al. (2004) worked backwards through the lattice interest rate model and compared the value with no refinancing and the value of a newly refinanced mortgage. Stanton (1995), Dunn and Spatt (1999) and Longstaff (2004) studied discrete-time models with a finite horizon that allowed computation of the endogenous values of the fixed rate mortgages. Agarwal et al. (2002) and Agarwal et al. (2013) considered minimizing the net present value of the interest payments with fixed discount rate. Chen and Ling (1989) built a backward-solving model and calculated the minimum differential between the contract rate and the current interest rates under the stochastic environment. Gan et al. (2012) focused on using Monte Carlo simulation in finding a desirable refinancing time that minimizes the total payment under the stochastic interest rate of the Vasicek model. Such a simulation approach has been recently extended by Xie et al. (2017) to solve for the two-dimensional refinancing problem. In this paper, we provide analytical modeling for refinancing decisions from a mortgage borrower’s point of view and construct the mortgage repayment and refinancing in a continuous-time setting. We use a Vasicek model for the stochastic interest rate due to its analytical tractability and statistical flexibility; however, this framework can be extended to other affine models. We then derive a closed form of the net present value (NPV) function, and define the optimal refinancing time to be the time at which its expected value is minimized. By introducing the simplifications of an infinite-time horizon of the mortgage and the Vasicek interest rate model, we obtain an expression which can be studied both numerically and analytically. We shall verify that these simplifying assumptions do not materially affect the results of our model. Our results demonstrate three separate behaviours, depending on the parameters in the model. Two indicate optimal refinancing in finite time. Our analytical results illustrate parameter regimes where decisions can be made simply, and others where numerical calculations must be made. We outline a strategy whereby a borrower can continually incorporate new data into the calculations to determine whether or not to refinance. The paper proceeds as follows. Section 2 contains a few assumptions and notations of the optimal mortgage refinancing. Section 3 gives a general solution to the refinancing problem, an outline of the refinancing strategy, and a closed form of the approximation to the refinancing function with infinite maturity under the Vasicek model. Section 4 presents the results of numerical simulations illustrating the effect of parameters in the Vasicek model on the refinancing decision. An analytical discussion of our results is given in Sect. 5, and we conclude in Sect. 6. The Appendix presents simulation results that verify that the Vasicek model is appropriate for our system.",3
53.0,4.0,Computational Economics,19 April 2018,https://link.springer.com/article/10.1007/s10614-018-9816-7,The Likelihood of the Consistency of Collective Rankings Under Preferences Aggregation with Four Alternatives Using Scoring Rules: A General Formula and the Optimal Decision Rule,April 2019,Eric Kamwa,Vincent Merlin,,Male,Male,Unknown,Male,"One of the main objectives of social choice theory is the aggregation of individual preferences into a collective ranking that is the determination of a complete order over the set of the alternatives. Each agent (also called a “voter”) is asked to provide a (complete) ranking of alternatives and a given voting or decision rule is used to determine the collective ranking (social outcome). The aggregation of preferences has wide applications in group decision making. In the framework of statistics, it is called the “consensus ranking problem” [see the seminal papers of Kendall (1938) and Mallows (1957)]. Scoring rules are among the most widely used decision rules; with a scoring rule, voters give points to candidates according to their rankings and the total number of points received by a candidate defines his scores for this rule. So, the alternative with the greatest (resp. with the lowest) score will appear at the top (resp. at the bottom) of the collective ranking and so on. Sometimes, the aggregation of individual preferences can lead to counterintuitive outcomes called “voting paradoxes”. For a non-exhaustive review of voting paradoxes, the reader can refer to Felsenthal (2012), Gehrlein (2006), Gehrlein and Lepelley (2010, 2017) and Nurmi (1999). Researchers in the field of social choice theory not only describe or analyze voting paradoxes but also focus on their likelihood. They sometimes use probabilities as a criterion for comparing the decision rules; given a paradox, one voting rule is better than another one if the likelihood of this paradox is less than that with the other rule. The Impartial Culture (IC) assumption is one amongst others, of the well-known hypotheses under which the computations are carried out.Footnote 1 First introduced in the social choice literature by Gehrlein and Fishburn (1976)), it is assumed that each agent (voter) chooses his preference (ranking) according to a uniform probability distribution. With m alternatives (\(m\ge 3\)), the probability of each of the m! possible strict rankings to being chosen independently is equal to \(\frac{1}{m!}\) and the likelihood of a given voting situationFootnote 2\({\tilde{n}}=(n_{1},n_{2},\ldots ,n_{t},\ldots ,n_{m!})\) is with n the size of the electorate and \(n_t\) the number of agents with ranking of type t among the m! possible rankings. For more details about the IC assumption, see among others Berg and Lepelley (1994), Gehrlein and Fishburn (1976) and Gehrlein and Lepelley (2010, 2017). According to Gehrlein (1979), one can derive the likelihood of most voting events under the IC assumption by using existing results on the representations of quadrivariate normal rules as suggested by Plackett (1954).Footnote 3 Gehrlein-Fishburn’s technique usually needs a good knowledge of the existing formulas in statistics for the representation of quadrivariate positive orthants (Abrahamson 1964; Gehrlein 1979). One advantage with this method is that it leads to compact formulas that help to determine which are the optimal decision rules given a voting event. In most of the social choice literature, when computing the exact probability of voting events under IC, authors tend to deal with events described by no more than four constraints [see Gehrlein and Lepelley (2010, 2017)]. With more than four constraints, most of the authors rely on Monte-Carlo simulations. It is usually more tricky to estimate the probability of events described by five constraints and to determine the optimal decision rules. Gehrlein and Fishburn (1980) have tried, but their conclusions are based on conjectures. To be more precise, Gehrlein and Fishburn (1980) have analyzed what we call the “the consistency of collective ranking”: going from a set A with four alternatives to B a proper subset of A with three alternatives, they tried to evaluate for all the scoring rules, the mean limiting probabilityFootnote 4 that a collective ranking on A will be consistent with the collective ranking on B when one element is removed from A. Gehrlein and Fishburn (1980) were not able to say which scoring rules maximize or minimize this limiting probability: they simply concluded with a conjecture. In this paper, we follow a technique suggested by Saari and Tataru (1999) and we show how computations with five constraints can be applied to the framework of Gehrlein and Fishburn (1980), and we use some algorithms to find out which scoring rules maximize/minimize the limiting probability of consistency. Independently from the works of Gehrlein (1979), Gehrlein and Fishburn (1976, 1980, 1981) and Saari and Tataru (1999) suggested a geometric technique to derive the likelihood (under IC) of voting events described by four constraints. The Saari–Tataru’s technique can be used without having recourse to the Plackett formulas and just needs good software to implement the procedure. The drawback is that formulas are not compact in general. However, as the Saari and Tataru (1999)’s technique is based upon the integration of a differential volume, a wise choice of the integration parameter can sometimes lead to compact formulas (Merlin et al. 2002, 2000; Tataru and Merlin 1997). Fortunately, though the Gehrlein-Fishburn’s technique and the Saari–Tataru’s technique may lead to different formulas, their derivation lead to the same figures. Using the Saari–Tataru’s technique, we obtain a general formula for the limiting probability of the consistency for all the scoring rules. Our approach can also be used in order to extend Gehrlein and Fishburn (1980)’s results to the following problem: given a set A with four alternatives and B a proper subset of A with three alternatives, what is the probability to getting any of the rankings on the three-alternative subset? Given the general formula we obtain, we suggest a simple algorithm to determine the scoring rules that maximize/minimize the corresponding probability. So, using our algorithm based on optimization tools, one can then remove the Gehrlein-Fishburn’s conjectures. Due to the fact that Saari–Tataru’s technique leads to formulas that are not compact, we are not going to report all the general formulas here.Footnote 5 In four-alternative situations, we also highlight some important relations that one can use in order to derive the likelihood of other voting events ranging from that of a particular event. We also provide a generic MAPLE-sheet for our approach which can be used for computations of the likelihood of any four-alternative voting event described by five constraints when scoring rules are used.Footnote 6 First of all, let us present the basic definitions of the social choice literature.",2
53.0,4.0,Computational Economics,21 April 2018,https://link.springer.com/article/10.1007/s10614-018-9817-6,"Carl Chiarella, Willi Semmler, Chih-Ying Hsiao and Lebogang Mateane: Sustainable Asset Accumulation and Dynamic Portfolio Decisions, Dynamic Modelling and Econometrics in Economics and Finance 18",April 2019,Xue-Zhong He,,,,Unknown,Unknown,Mix,,
53.0,4.0,Computational Economics,23 April 2018,https://link.springer.com/article/10.1007/s10614-018-9814-9,Enhanced Predictive Models for Construction Costs: A Case Study of Turkish Mass Housing Sector,April 2019,Latif Onur Ugur,Recep Kanit,Mursel Erdal,Male,Male,Unknown,Male,"The analysis of a construction project, regarding cost, is one of the most vital problem in planning. Detailed drawings are not available to the estimator at the pre-estimation stage, when the investment decision is made and resources required for the investment and final project cost are determined. The only information that can be utilized is the data available from past projects. When one considers the construction sector’s strong competitive environment, it is obvious that solution-oriented, fast and efficient methods are a priority for the technical personnel dealing with planning and cost control studies. An effective cost control model should have various properties. Firstly, it should be appropriate for the organizational process and procedures for which the model will be used. The data to be entered into the model should be accurate and up to date. The model should be usable by all the relevant groups (employer, construction firm, subcontractor, etc.). Taking into consideration construction methods, timing of construction activities and various properties of the building in the building production process, several different cost models have been developed for determining the impact on costs of decisions by decision-makers and for performing cost control by planners. By using the cost model the factors affecting costs, such as materials, time and production process, can be controlled. Since the 1970s, regression-based estimation models have been developed for construction cost estimation. These are based on statistical and mathematical tools, and regression techniques have been found to be successful in interpreting of the cost and the variables relationships (Verlinden et al. 2008). Bromilow (1969), Bowen and Edwards (1985) and Oberlender and Trost (2001) studied regression models for forecasting the final cost of construction projects. In the construction industry, past history can’t be definable as an effective predictor of the future, so accurate prediction requires an expert approach. The most accurate estimation is only possible with detailed design. Collection of required information for correct estimation of construction costs is difficult and also time-consuming. Moreover, forecasts must be based on expert judgment due to the complexity, non-linear relationship and uniqueness of construction projects (Al-Tabtabai 1998). Against the background of Artificial Intelligence (AI) mimicking human intelligence and expertise, Flood and Kartam (1994) mentioned the advantages and application of artificial neural networks (ANN) in civil engineering. Since then many researchers have used AI methods for construction cost estimation. Kim et al. (2004a, b) cited studies showing that AI-based estimation models are superior to traditional regression models. McKim (1993) and Li (1995) utilized the neural network (NN) method for cost prediction of buildings. Adeli and Wu (1998) applied an NN model for predicting the reinforced-concrete pavements cost in highway constructions. Bode (2000) compared linear and non-linear parametric regression with an NN algorithm for estimating construction cost and finally achieved lower deviations with the NN method. Yeh (1998) used a logarithm-neuron network (LNN) for steel and reinforced-concrete buildings quantity estimation. An et al. (2007) developed an algorithm that assesses classified error rate ranges for the quality of cost estimates. Son et al. (2012) proposed a principal component analysis and support vector regression (PCA–SVR) model which is able to estimate commercial building projects’ cost performance. Cheng and Hoang (2014) developed a model called EAC-LSPIM that uses least squares support vector machine (LS-SVM), AI based interval estimation (MLIE), and differential evolution (DE) for predicting construction project cost. There are also hybrid systems used for construction cost estimation. Yu and Skibniewski (2009) proposed a hybrid model for cost estimation data mining. Kim et al. (2005) presented a hybrid model based on NNs and genetic algorithms (GA) for building cost prediction. Cheng et al. (2013) established an intelligence system called ELSVM and used 122 historical cases for the development of the system, based on the fusion of LS-SVM and DE. Niknam and Karshenas (2015) presented semantic web technology based construction cost estimation system. The system they developed based on information accessing, combining and sharing via the network platform. Thomas and Thomas (2017) developed a statistical regression method for estimation of project cost and duration by using past project data. This study discusses models for the pre-estimation of construction costs of multi-story reinforced concrete houses built by TOKI and TURKKONUT and which will be used for further projects. As pointed out earlier, in recent years the Turkish Republic has started a serious urban regeneration movement in parallel to its economic development. As of the end of 2014, in the last few years, as a result of close cooperation among central government, local municipalities, government organizations and private sector companies, 65,000 houses have been built within urban regeneration projects in 128 regions. In 205 projects where bidding stages are completed, construction of 90,971 houses have started and in 226 different regions bidding works have been performed for projects including 103,873 houses. In addition, in 334 projects 283,020 housing units of shanties and urban regeneration works have been planned. The main objective of this study is to improve the estimation accuracy of individual machine learning (ML) techniques, namely multi-layer perceptron (MLP) and classification and regression trees (CART) and compares the performance of two ML meta-algorithms (i.e., bagging and random subspace) on a real world construction cost estimation problem. According to our literature research ensemble learning methods have not been utilized for construction cost prediction, particularly in mass housing cost prediction. Besides, to our knowledge there are many MLP applications within construction cost prediction but only limited applications of CART. The remainder of the study is organized as follows. In Sect. 2, ensemble learning methods utilized in this paper, are briefly introduced. In Sect. 3, firstly construction sites are described, then the experimental setup and the data set are presented, and finally performance statistics and empirical analyses are demonstrated. The discussion is provided in Sect. 4. The study completed with conclusion, consisting the future study directions.",5
53.0,4.0,Computational Economics,27 April 2018,https://link.springer.com/article/10.1007/s10614-018-9815-8,Unified Approach for the Affine and Non-affine Models: An Empirical Analysis on the S&P 500 Volatility Dynamics,April 2019,Shunwei Zhu,Bo Wang,,Unknown,Male,Unknown,Male,"The first model for option pricing, provided with simple closed-form formulas, was established by Black and Scholes (BS model thereafter) (1973). Its simplicity is related not only to easy implementation, but also to its drawbacks. An extensive empirical literature has documented the empirical biases of the BS model. More specifically, two major features of the equity index options market cannot be captured through BS model. First, in terms of volatility, a popular observation is volatility clustering, the constant volatility parameter is not sufficient to model this effect. Second, observed market prices for both in-the-money and out-the-money options are higher than Black–Scholes prices. This stylized fact is known as volatility “smile.” As a popular approach, stochastic volatility can model smile efficiently. It is worth mentioning the empirically observed “leverage effect”, indicates the increase in the volatility values is apt to result in a decrease in the underlying asset price, i.e. they are negatively correlated. The “leverage effect” induces negative skewness in stock returns, which in turn yields a volatility smile. Moreover, stochastic volatility models can also address term structure effects by modeling mean reversion in variance dynamic. Popular single-factor stochastic volatility models include the Heston model and the 3/2 model of Heston (1997) and Platen (1997). Heston model has been justified as a successful model in literatures, e.g., the model can reproduce smile and skew with parsimonious number of parameters, and each parameter has a clear financial meaning and is tractable. However, some shortcomings have been shown immediately in calibration of the model on real data. Feller condition is frequently violated because a high volatility-of-volatility parameter is required to fit the steep skews in equity markets. Moreover, the Heston model predicts that, in periods of market stress, when the instantaneous volatility increases, the skew will flatten. The Heston model therefore assigns significant weight to very low and vanishing volatility scenarios and is unable to produce extreme paths with very high volatility of volatility. The selection of the 3/2 model is motivated by several observations in recent literatures. Contrary to the Heston model, the 3/2 model reverts more quickly when the process is higher, which is a reasonable feature from the empirical point of view. The Heston model predicts that, in periods of market stress when the instantaneous volatility increases, the skew will flatten. Under the same scenario, the skew will steepen in the 3/2 model. This implies very different dynamics for the evolution of the implied volatility surface. The single-factor stochastic volatility models, such as the Heston model and the 3/2 model, can generate smiles and skews. However, the data suggest that there are low volatility days with a steep smile slope as well as a flat smile slope, and high volatility days with steep and flat smile slopes. A single-factor volatility model can generate steep smiles or flat smiles at the given volatility level, but cannot generate both for the given parameterization. In a purely cross-sectional analysis, this is not a problem, because we can estimate different parameter values for the one-factor model to calibrate the time-varying nature of the cross-section. However, when estimating model parameters by using multiple cross-sections of option contracts, a one-factor model has a structural problem. If its parameters are geared towards explaining a slope of the smile that is on average high over the sample, it will result in large model error on those days when the slope of the smile is relatively flat, and vice versa. In order to match precisely with the market implied volatility surface, Grasselli (2016) proposed the 4/2 model that combines the properties of both the Heston and the 3/2 models. The 4/2 model behaves as a two-factor model where the stochastic factors \( \sqrt {V_{t} } \) and \( 1/\sqrt {V_{t} } \) are closely related but still exhibit different properties in explaining implied volatility surface. In this paper, we extend and supplement Grasselli’s work. First, Grasselli provided characteristic function methods in his paper. But we do not apply the methods due to the complexity of the moment generating function. Using Lewis’s (2000) fundamental transform approach, one can obtain a partial differential equation (PDE) of the form \( u_{t} = \sigma x^{\gamma } u_{xx} + f\left( x \right)u_{x} - g\left( x \right)u \), which possesses a sufficiently large symmetry group. We use the result derived by Craddock and Lennox (2009) using Lie Symmetry analysis. Their method reduced the problem to the evaluation of a Laplace transform of joint transition densities, which is a simpler form than that of Grasselli. Second, Grasselli did not provide empirical study in pricing. To illustrate, we calibrate the 4/2 model using S&P 500 index options. Our parameters are estimated by using Nelder–Mead minimization algorithm to loss function. Finally, we investigate the 4/2 model along with the Heston model and the 3/2 model and compare their different performances in practice. Our results show that the 4/2 model outperforms other classical models. The paper is structured as follows: In Sect. 2, we recall Lewis’s fundamental transform approach for pricing options. In his approach, the Fourier transform of the option value is not required, only the Fourier transform of the option payoff. In Sect. 3, we formally introduce the 4/2 model of Grasselli (2016), and compute the fundamental transform for the 4/2 model in the light of Lie Symmetry theory. In Sect. 4, we obtain two expressions for the call price using the fundamental transform, and investigate the arbitrage-free fair value when the martingale property does not hold. In Sect. 5, we estimate the model parameters applying the S&P 500 data. Then, we investigate the 4/2 model along with the Heston model and the 3/2 model and compare their different performances in practice. Section 6 concludes the paper.",10
53.0,4.0,Computational Economics,02 May 2018,https://link.springer.com/article/10.1007/s10614-018-9813-x,Programming Language Choices for Algo Traders: The Case of Pairs Trading,April 2019,Pedro Vergel Eleuterio,Lovjit Thukral,,Male,Unknown,Unknown,Male,"In order to test each programming language, we implement our own proprietary trading algorithm. The algorithm is based on a number of econometric and statistical tests, which have been enhanced to provide the required level of risk adjusted returns in a live trading environment. Each of the econometric and statistical tests is calculated for each pair of stocks in our sample. For the purpose of this paper, we calculate a total of five factors with various levels of complexity. Each factor is aimed to measure a characteristic of the relationship. Our factors include variants of correlation, co-integration, speed of mean reversion metrics, and a measure of dislocation of both time series. The most computationally demanding factors are those that require several steps, such as co-integration tests. Co-integration tests are meant to differentiate from short and long term variations. Classic methodologies for testing co-integrated relationships are those of Engle and Granger (1987) and Johansen (1991). A prerequisite for testing co-integration is that both series are I(1) processes—they have unit roots and their first differences are stationary. If co-integration exists, both time series move together over a long period with fluctuations in the short term. Engle and Granger propose a two-step procedure. Firstly, a linear regression is estimated: In theory, due to the asymptotic properties of the Engle and Granger test, the choice of the dependent variable affects the coefficients but not the distribution of the test statistics (see Eq. 1). However, in practice, the procedure is repeated with each of the variables as the dependent variable (Geman and Vergel Eleuterio 2015). Secondly, the estimated residuals are calculated and tested for stationarity (see Eq. 2) using a Dickey Fuller test: Additionally, a co-integrated relationship can be expressed as an Error Correction Model (see Eq. 3), which provides the magnitude of the correction in the short term, i.e. the speed at which the spread between both stocks narrows, θ. Using first differences, we write the model as: If the coefficient θ is negative, it indicates a narrowing of the spread and therefore the existence of co-integration. The dataset comprises daily quotes on the constituents of the S&P 500 from 2010 to 2015. Each implementation loads the prices from a SQLite database. This step is dependent on the database library used, hence it is not included when considering the performance of each language. The implementations in each language all follow the same structure. We start by loading the data in memory and computing each of the factors separately. Additionally, we also compute all the factors together to obtain a global benchmark for the whole strategy. Our main objective is to get the best performance out of our implementation reducing the development times required from a pure C++ implementation. In order to provide a meaningful overview of the speed of the languages, we run each factor multiple times and we collect a set of summary statistics related to run time. In order to provide a good benchmark, we avoid compilers optimizations, such as dead code elimination, hence we made sure to use the results of each computation. We now describe implementation details that are specific to each language. These details should help the trader to understand the implications of writing high performance programs in each language. In order to keep the language comparison fair, we follow best practice as much as possible for each implementation. The C++ implementation is the one that requires by far the most time to implement. This is due to the fact that, although the language allows the trader great flexibility, it also requires careful control over memory allocation and access. We test a number of different compiler flags to see which one obtained the best performance. Unlike the other languages in this comparison, the resulting C++ program does not require further work to obtain acceptable performance. We concur with Ceccon et al. (2016), who show that implementing the algorithms in C++ takes three times longer than in Python. Cython development times are just slightly higher than those of Python, while Julia is by far the fastest language to implement. These relationships can also be observed during code maintenance, although maintenance times can be greatly reduced by following best practices in commenting and documenting existing code. In the Python implementation we use the NumPy library to handle vector operations. NumPy implements vectors operations in C code, hence, using NumPy as much as possible in the Python code is the best way to obtain good performance. Moreover, unlike the other languages considered in this paper, vectorizing the algorithms results in improved performance. Using NumPy functions, to the best of our knowledge and experience, over pure Python functions can increase speed by a factor of 100 to a 1000 times, depending on algorithm complexity. With regards to Cython, programming is similar to writing code in pure Python. Once the functions are implemented in Cython, they are called from the main Python program. Calling the Cython modules from Python simulates the real world usage of Cython, i.e. computationally expensive functions are called from the main Python program. In order to make our code more efficient in Python, we disable Python division, bounds check, and wrap-around indexing. Disabling these features in Python requires the trader to be more careful about accessing arrays and doing divisions, but the result involves less operations at run time. The interaction between C and Python code results in an additional cause degradation in performance but, fortunately, Cython provides a means of highlighting these interactions through annotations in the source code. Julia is a programming language has a similar syntax to programs such as Matlab and R. However, non-vectorized code results in better performance in Julia compared with Matlab and R. Another advantage for the user is that Julia allows the trader to use for loops without penalizing performance. Writing for loops is a more familiar way to express operations on vectors for most users, such as those traders that generally use VBA, Matlab or R in their day to day activities. Moreover, Julia provides excellent tools that allow for the writing of optimized versions of programs in a short time period. Among Julia’s recommendations for optimizations are to disable bounds checking on a function-by-function basis and to avoid using abstract types, especially for containers to avoid the use of array pointers—using the @code_warntype macro to highlight these abstract types makes this easy to perform.",
53.0,4.0,Computational Economics,04 May 2018,https://link.springer.com/article/10.1007/s10614-018-9818-5,Internal and External Cartel Stability: Numerical Solutions,April 2019,Christos Papahristodoulou,,,Male,Unknown,Unknown,Male,"Imperfect cartels is a market structure where the cartel participating firms compete with other fringe firms. A well-known example is oil. The OPEC is the world’s largest cartel. Moreover, it is an imperfect cartel, because its 13 members’ market share for oil in 2015 was almost 36% (OPEC 2016). There are at least 50 countries (like the US, Russia and Norway) that account for the remaining 64%. If the market shares or the proportion of OPEC/non-OPEC share remain stable, one can conclude that this imperfect cartel is stable. Would it be better if one OPEC country left the cartel or if Russia joined it? Still there are more countries with proven oil reserves that might start production. What will happen to stability if a new oil-producing country enters the world market as a fringe, or if it is welcomed by the OPEC? The stability of imperfect cartel has a long research history. Selten (1973) was the first to argue that four firms in a market are a few while six are too many. Ten years later, D’Aspremont et al. (1983), modelled the behavior of fringe firms (j-firms) as price-takers. Donsimoni et al. (1986) and Daskin (1989), based on D’Aspremont et al. (1983), showed that stable cartels exist with a rather small number of firms and when the number of firms in the market increases, the relative size of cartel firms (k-firms) decreases. Shaffer (1995), assuming constant marginal costs, was the first to introduce the Cournot behavior of the j-firms as followers and the k-firms as the price-leaders. The conditions she proposed for internally stable cartels for \( n \ge 4, \) are \( k \le \frac{n}{2} + 1, \) for even n, respectively \( k \le \frac{n + 1}{2} + 1, \) for odd n, and reverse bounds for externally stable cartels. It is noteworthy, as Shaffer argues, that the values of parameters do not influence these conditions. A similar model was used by Escrihuela-Villar (2008), arguing that, in stable cartels, the number of cartel firms with \( n \ge 4 \) is independent from the parameters and is determined, either by \( \frac{1}{4}(1 + 3n - \sqrt {\left( {n - 2} \right)n - 7} , \) or by the same condition plus one. Konishi and Lin (1999) modified Shaffer’s model by assuming quadratic cost function, without a fixed part, and setting the slope of demand curve equal to unit. Due to algebra complexity, they conducted numerical simulations. Zu et al. (2012), followed Konishi and Lin and provided an analytic approach to determine the size of stable cartels. Their estimates are similar to Konishi and Lin, or slightly larger, especially when the cost parameter increases. They also concluded that stable cartels will consist of relatively fewer firms when the cost parameter rises. On the other hand, they questioned the uniqueness of the stable cartel, argued by Konishi and Lin. In this paper I present two general models. The first model is based on D’Aspremont et al. (1983), where the cartel faces j-competitive firms. The second model is based on Shaffer (1995) and Konishi and Lin (1999), where the k-firms are the Stackelberg leader while the j-firms behave as Cournot followers. The models are more general in a sense that they include two more parameters, one for the slope of demand and another for the slope of the marginal cost function, including an intercept. Precisely as these referred models, both are single period models, where firms make entry to/exit from cartel decisions by observing their ex-ante profits as j- or k-firms, with their ex-post profits as k- or j-firms. It is assumed also that tensions, cheating and monitoring the k-firms is independent from its size. All k-firms comply with their optimal production (or price) and their commitment to co-ordination will not be jeopardized, even if they are granted amnesty (i.e. by applying the discriminatory leniency policy) to those who become “whistleblowers”, as Clemens and Rau (2014) find. In both models it is assumed that there is no a ringleader among the k-firms to coordinate the cartel and ensure its success. In fact, according to Bos and Wandschneider (2011) there might exist more than one ringleaders and prices can be higher without a cartel ringleader. Obviously, if the leniency policy is successful, as the European Commission (2017) argues, the theoretically optimal number of k-firms should be lower and in best case the cartel will be abolished. Notice though that the OPEC example above is not appropriate for anti-trust policies, because OPEC’s activities are protected by U.S. foreign trade laws. Since I am interested only in integer firms, the algebra complexity will be more tedious than, for instance, in the Konishi and Lin model. Consequently, it would be extremely difficult, if possible, to obtain general, integer stability conditions. To my help I relied on two powerful Mathematica functions “Reduce” and “NMinimize”. “Reduce” finds all possible conditions that satisfy the objective function and “NMinimize” obtains global constrained (min) numerical values. The paper is organized as follows. Section 2 derives the quantities, prices and profits in both models; Sect. 3 presents some numerical estimates of cartel stability for both models; Sect. 4 presents the formulation for cartel stability, while the estimates are shown and compared in Sect. 5. Finally Sect. 6 concludes the paper with some policy implications.",
53.0,4.0,Computational Economics,07 May 2018,https://link.springer.com/article/10.1007/s10614-018-9821-x,Monitoring the Impact of Economic Crisis on Crime in India Using Machine Learning,April 2019,Mamta Mittal,Lalit Mohan Goyal,D. Jude Hemanth,Female,Male,Unknown,Mix,,
53.0,4.0,Computational Economics,07 May 2018,https://link.springer.com/article/10.1007/s10614-018-9819-4,Efficient Semi-Discretization Techniques for Pricing European and American Basket Options,April 2019,Fazlollah Soleymani,,,Unknown,Unknown,Unknown,Unknown,,
53.0,4.0,Computational Economics,09 May 2018,https://link.springer.com/article/10.1007/s10614-018-9820-y,Stress-Testing U.S. Macroeconomic Policy: A Computational Approach Using Stochastic and Robust Designs in a Wavelet-Based Optimal Control Framework,April 2019,David Hudgins,Patrick M. Crowley,,Male,Male,Unknown,Male,"The purpose of this paper is to compare optimally integrated fiscal and monetary policies computed under stochastic and worst-case assumptions in order to “stress test” the predicted performance for key U.S. macroeconomic variables. The paper implements stochastic optimal linear–quadratic (LQ) tracking control policies alongside mixed stochastic/multiple-parameter minimax robust policies, where an accelerator model is formulated within the time–frequency domain based on a wavelet decomposition. This paper therefore expands the wavelet-based deterministic model of Crowley and Hudgins (2017b) to include stochastic and robust treatment of the disturbances, and applies the robust model of Hudgins and Na (2016) within a wavelet-based framework. Hudgins and Na (2016) analyzed a multiple-parameter design and a mixed stochastic/single parameter H∞-optimal control design, but the study did not consider a mixed stochastic/multiple-parameter design. The combination of stochastic disturbances and multiple-parameters for the worst-case disturbances is a necessary extension in a wavelet-based system, since there are a large number of disturbances arising at different frequencies in the components of Gross Domestic Product (GDP). This paper is the first, therefore, to integrate wavelet analysis with stochastic optimal control in economics, and furthermore, this is the first paper that combines wavelets with robust optimal control and dynamic non-cooperative game theory. Part of the design of economic policymaking involves the joint setting of monetary and fiscal policy so as to foster an adequate level of economic growth. Although the conditions under which policymakers are working cannot be specified a priori, there are assumptions we can use which will allow us to verify the robustness of decision-making in terms of the outcomes for growth in the main endogenously-determined variables of consumption and investment. In this sense, the simulation of outcomes for key economic variables are a form of “stress testing” of macroeconomic policy. But the impact of policies will likely be different over different time horizons as well, so not only can a time–frequency analysis of the data suggest policies that are designed to work over specific time horizons so as to stimulate growth, but also it is important to model the interaction of these policies over different frequency ranges. This is the main thrust of this research program to date, where we have taken the work of Kendrick (1981) and others in the formulation of LQG models, and have extended them both into the time–frequency domain, and also by employing different design assumptions. This paper is focused on an analysis of two different design assumptions on the results, with reference back to our earlier work in Crowley and Hudgins (2017b). Kendrick and Amman (2010), Kendrick and Shoukry (2014), and Hudgins and Na (2016) employed accelerator-based models to present strong empirical evidence in favor of implementing a quarterly fiscal policy instead of an annual policy. Using simulations, Kendrick and Shoukry (2014) compare the tracking performance and debt structure of the quarterly and annual models within a closed-economy macroeconomic model that includes monetary policy exerted through the interest rate. Hudgins and Na (2016) examine optimal robust fiscal policy in the U.S. using an accelerator model that does not contain monetary policy or interest rates. So following Crowley and Hughes Hallett (2014), which used a Maximal Overlap Discrete Wavelet Transform (MODWT) with U.S. real GDP component data, Crowley and Hudgins (2015) was the first study to incorporate the combination of time–frequency analysis with an LQG model in the form of a wavelet-based optimal control model, analyzing fiscal policy within a closed economy accelerator formulation. Crowley and Hudgins (2017a, b) then expanded the wavelet estimation and optimal control model to incorporate monetary policy through the central bank’s influence on market interest rates. In robust optimal control, the design explicitly accounts for imprecise system models or error models in order to synthesize a control policy that stabilizes a family of models (Bernhard 2002). H∞-optimal control has been a primary subject of robust control research, and has been extensively applied in engineering and economics (Basar and Bernhard 1991; Hansen and Sargent 2008; Hudgins and Na 2016). Linear–quadratic (LQ) H∞-optimal control specifications are used to formulate a minimax approach that achieves a robust design by minimizing a performance index under the worst possible disturbances, where those disturbances maximize that same performance index. Robust is defined as guaranteed performance for any disturbance sequence that satisfies the H∞-norm bound. Robust approaches seek to address the primary problem with adaptive and linear–quadratic Gaussian (LQG) probabilistic approaches, where minimizing the expected value of a performance index leads to maximum system performance in the absence of misspecification, but can lead to poor performance and instability if the model or disturbances are misspecified (Tornell 2000). 
Most of the macroeconomic models that are widely used for dealing with uncertainty, forecasting, and quantitative policy formulation utilize vector autoregressive (VAR), Bayesian vector autoregressive (BVAR), time-varying parameter VAR (TVP-VAR), dynamic stochastic general equilibrium (DSGE), and hybrid DSGE (DSGE-VAR) formulations (Bekiros and Paccagnini 2013; Ravenna 2007). The primary problem with the classical (unrestricted) VAR models is overfitting. Overfitting occurs because the number of estimated coefficients is large relative to the number of observations, creating misleading coefficient relationships that can cause large-scale VAR forecasts to be inaccurate and overly sensitive to changes in economic variables (Todd 1984). BVAR models attempt to reduce overfitting by limiting the data’s influence on the coefficients. Compared with VAR models, BVAR models more closely resemble structural models and strongly utilize the modeler’s prior beliefs in order to reduce overfitting. BVAR models, however, rely more on statistical theory and observations, whereas structural models, such as DSGE, rely more upon economic theory (Todd 1984). Nevertheless, when considering the results of a variety of current studies, the simple or hybrid DSGE is found to improve the forecasting performance compared to VAR and BVAR models (Bekiros and Paccagnini 2013). The underlying objective of these time-series approaches is to extract the most useful information from the data to obtain forecasts and predict policy effects. Wavelet methods, however, are motivated by the ability to capture information contained in the time–frequency domain that cannot be captured by the approach taken by the aforementioned techniques. Wavelet-based decomposition has proven to be useful in large-scale time-series models, since Power et al. (2017) found evidence that decomposition is often more effective for estimation and forecasting than is adding more factors to the model. Wavelet-based control models utilize wavelet-decomposed variables and then estimate coefficients in reduced-form equations, thereby maximizing the informational content from the variables for modelling and policy formulation exercises. Since classical VAR models are only appropriate for stationary time series, TVP-VAR models are often used when the time series are nonstationary. TVP-VAR models assume that all parameters follow a random-walk process, and this leads to such a large amount of parameters to estimate that it often becomes difficult to maximize the likelihood function (Bekiros and Paccagnini 2013). Alternatively, the time series can first be decomposed using wavelets, and then appropriate wavelet-based reduced form models can then be employed for non-stationary series. The coefficients in the linearized wavelet-based model can then either be assumed to be constant or time-variant (Crowley and Hudgins 2015). The motivation for our current method of combining wavelet methods with worst-case modeling is clearly demonstrated when examining two facts regarding the transition from the Great Moderation into the Great Recession. First, DSGE models have been criticized for their failure to predict the Great Recession following the Great Moderation (Blanchard 2018). Using quarterly data from the U.S. and the UK, Crowley and Hughes Hallett (2014) found that the Great Moderation appears only at certain frequencies, which explains the difficulty in reliably detecting periods of moderation in the aggregate data. Thus, detection of such periods requires separating the GDP components into their frequency components over time. Secondly, Lindé (2018) shows that the downturn in the Great Recession is far outside the feasibility set spanned by the both the DSGE model and the BVAR models, so that both approaches showed the severe downturn to be a highly unlikely event. This demonstrates the need for the consideration of worst-case approaches. The uncertainty bands in the BVAR and DSGE forecasts do not test the limits of the policy options under the worst-case disturbances, and they do not account for the asymmetry where the negative effects of recessionary gaps are far more damaging than the positive effects of unexpectedly strong growth. These two issues give rise to the usefulness of obtaining forecasts from a wavelet-decomposed model under a minimax worst-case design. Following this introduction, Sect. 2 discusses the MODWT wavelet decomposition of data in the time–frequency domain, including the robustness of the various procedures, where the quarterly lag structure of macroeconomic data obtained using wavelet analysis is first utilized to capture the interplay of the short-term lags with the long-term cyclical components (Crowley and Hudgins 2015, 2017a, b).Footnote 1 Section 3 then derives the macroeconomic time–frequency accelerator model. Section 4 augments the model employed by Crowley and Hudgins (2017b) with an expanded form of the stochastic and mixed stochastic/minimax robust methods employed by Hudgins and Na (2016) to determine optimal control feedback rules for monetary and fiscal policy. We develop a MATLAB software program that computes the optimal joint fiscal and monetary policy based on the large-scale 80-equation state-space framework under the stochastic and mixed designs. Section 5 simulates the time–frequency model using U.S. quarterly data from 1947 quarter 1 to 2015 quarter 1. The simulations assume a politically motivated targeting strategy, as in Crowley and Hudgins (2015, 2017b), whereby policymakers place larger weights on the tracking errors for government spending, consumption, investment, and the interest rate, at frequency ranges between 2 and 8 years. Our simulations compare optimal fiscal and monetary policy when all of the disturbances are stochastic random variables, with the scenarios when the disturbances at each frequency range in the consumption and investment equations take on their worst-case values that stem from a soft-constrained dynamic game. This paper lastly provides simulation results for both the stochastic LQG and minimax controller designs, which are then compared to the deterministic simulations of Crowley and Hudgins (2017b). The simulations compare three different policy scenarios: (1) dual emphasis on the active use of both fiscal and monetary policy, (2) emphasis on active use of fiscal policy with relatively uncooperative passive monetary policy that closely tracks a fixed interest rate target, and (3) active use monetary policy with relatively uncooperative passive fiscal policy that closely tracks a government spending target. Our model is not meant to be a complete forecasting tool, but the method demonstrates the merits of using the wavelet-based model to analyze performance under different disturbance structures. Computing the policy changes under the worst-case design gives the policymakers a “stress-test” that aids in formulating fiscal and monetary policies that are better-able to perform well under differing intensities of cyclical phases.",8
53.0,4.0,Computational Economics,31 May 2018,https://link.springer.com/article/10.1007/s10614-018-9822-9,A Nationwide or Localized Housing Crisis? Evidence from Structural Instability in US Housing Price and Volume Cycles,April 2019,MeiChi Huang,,,,Unknown,Unknown,Mix,,
53.0,4.0,Computational Economics,02 June 2018,https://link.springer.com/article/10.1007/s10614-018-9823-8,An Efficient Algorithm for Options Under Merton’s Jump-Diffusion Model on Nonuniform Grids,April 2019,Yingzi Chen,Wansheng Wang,Aiguo Xiao,Unknown,Unknown,Unknown,Unknown,,
53.0,4.0,Computational Economics,05 June 2018,https://link.springer.com/article/10.1007/s10614-018-9826-5,Developing a Risk-Based Approach for American Basket Option Pricing,April 2019,Ehsan Hajizadeh,Masoud Mahootchi,,Male,Male,Unknown,Male,"Options, as widely used financial contracts, can play an important role in reducing the risk of investors and practitioners in the financial markets. A new recently developed type of option entitled as high dimensional or multi-asset (basket) options, are used in some real-world applications. The option, which could be considered as an exotic option, encompasses a group of securities, commodities, or currencies whose owner/s could exercise it with a predefined price. The price of the option and the weights of tradable assets are officially determined at the time of issuing that option. Obviously, the weighted values of assets or portfolio in the basket options influences the option payoff. The main advantage of having a basket option would be that it is cheaper than a portfolio including single vanilla options. Furthermore, the risk management of a portfolio having a basket option would be more comfortable than of a portfolio including single vanilla options because of their different early exercise times. Finally, the transaction cost of trading a basket would be remarkably lowered as a buyer/seller only pays a single transaction cost for trading the respective basket option while he/she should pay multiple transaction costs for trading vanilla options. There are two main types of basket options in the real-world applications in terms of the expiry time of option: American and European basket options. Pricing and valuation of American option, even the single asset option, is a hard problem in the field of quantitative finance (Mitchell et al. 2014). There are also some research studies in which lower and upper bounds are suggested for the prices of European basket options. D’aspremont and El Ghaoui (2006) found the bound using linear programming. They used an efficient linear programming relaxation based on an integral transform interpretation of the call price function. Caldana et al. (2016) proposed a general closed-form bound which is applicable for all types of European options without having any new assumption on the characteristic function. Cho et al. (2016) also found the bounds using an efficient solution procedure based on the separation problem. To the best of our knowledge, there is no simple closed-form solution for pricing/valuation of American options. Because of practical importance of this type of options in real-world problems, there are few attempts to develop approximation and numerical methods for pricing/valuation of this type of option (Moon et al. 2008; Cortazar et al. 2008; Bandi and Bertsimas 2014; Breitner 2000; Samimi et al. 2017). Generally speaking, the developed methods for pricing/valuation of American options would be classified in three main categories: partial-differential-equations-, lattice- and simulation-based methods. In the first category, the dynamic programming is initially used for finding the optimal stopping point and the partial equations based on Black–Scholes model are derived for the pricing of American option. One of the limitations of these methods is the high computational efforts to reach the reasonable accuracy (Huang et al. 1996). There are a lot of research papers in this category in the literature; see for example: Benth et al. (2003), Wang et al. (2015) and Dehghan and Bastani (2017). Lattice-based method can be also categorized in three branches: binomial trees (Breen 1991), trinomial trees (Yuen and Yang 2010), and finite difference approaches (Cont and Voltchkova 2005). These approaches have been widely used for single-asset options. However, when the dimension of problems increases, usually up to four, the extensions of binomial and trinomial methods would be computationally expensive. In other words, even though the lattice-based methods are usually easy especially where the early exercise feature exists, there is a remarkable computational burden where the dimension of problems grows up (Cox et al. 1979; Barraquand and Martineau 1995). In contrary to most lattice-based methods, Monte Carlo simulation methods would be more appropriate to cope with pricing/valuation in high dimensional American options leading to more suitable outcome. However, some type of simulation-based methods such as forward versions have a main drawback in dealing with the issue of early exercise features for these options. To overcome this problematic issue, few research works have been accomplished (Stentoft 2011; Kohler and Krzyżak 2012; Lian et al. 2015; Antonelli et al. 2013). Dynamic programming is a popular and well-known technique which has been widely used in the literature to tackle with the early exercise features of American options (Cornuejols and Tütüncü 2007). However, DP-based methods mostly suffer from the curse of dimensionality issue in real-world applications (Powell 2011). Approximate Dynamic Programming (ADP), as an efficient simulation-based method used in many real problems, could remarkably alleviate the curse of dimensionality (Powell 2007). To even alleviate the issue of dimensionality, Barraquand (1995) used a stratified state aggregation technique to generate a single-dimensional problem. In spite of the fact that the proposed method is extremely fast in calculating an approximation for the price, its accuracy mainly depends on the way of finding the optimal exercise policy, which is usually obtained through the single payoff function. Broadie and Glasserman (2004) demonstrated that the stratification algorithm will not converge to the correct value. Longstaff and Schwartz (2001) presented a least squares regression with basis functions to approximate the value function. Ben-Ameur et al. (2007). proposed a DP-based method for pricing options embedded in bonds. A main advantage of their method is that it could be applied to the problems considering interest rates. Gagliardini and Ronchetti (2013) developed a semi-parametric estimator of American option prices in discrete time. They suggested a semiparametric estimator for Stochastic Discount Factor (SDF) parameter and then used dynamic programming for pricing American option. Generally, using DP in pricing/valuation is mostly limited to small- or mid-size problems. Therefore, it could be a challenging issue to apply this method in large-scale problems, such as pricing American basket option included with different assets. In this situation, different versions of approximate DP, each of which might be called a simulation-based DP, could be suitable methods for these types of problems (Powell 2011; Gosavi 2014). In spite of different advantages of using options in hedging, in many cases practitioners in a financial market really believe that the mispricing of derivatives could be the main reason of the collapse in some financial institutions. Mispricing or incorrect valuation of derivatives might be because of not considering risk measurements in the process of option pricing (Elliott and Siu 2011). Considering risk in pricing American option would be very attractive topic, which has been rarely investigated in the literature. The motivation of this study is to develop new simulation-based models to consider the worst-case (pessimistic/risk-averse) and best-case (optimistic/risk-taking) scenarios for pricing the American basket options with respect to the level of risk aversion for each investor. Moreover, there are some degree of auto-correlation and, more importantly, heteroscedasticity in most financial time series. GARCH-type models have been extensively used, specifically for modeling auto-correlation and heteroscedasticity of financial data (Hajizadeh et al. 2015; Tian and Hamori 2015; Gouriéroux 2012; Davari-Ardakani et al. 2016; Maciel et al. 2016). To generate scenario for a specific basket of assets, we use a method based on GARCH and Copula models to consider both auto-correlation of each time series and the correlations between assets. The remainder of this study organized into the following sections: Sect. 2 presents the proposed models for pricing/valuation of the American basket option; third Section expresses the data description and scenario generation performed using Copula and GARCH models; the two last sections are dedicated to the computational results and conclusions.",1
53.0,4.0,Computational Economics,06 June 2018,https://link.springer.com/article/10.1007/s10614-018-9827-4,On Jackknife-After-Bootstrap Method for Dependent Data,April 2019,Ufuk Beyaztas,Beste H. Beyaztas,,Male,Female,Unknown,Mix,,
53.0,4.0,Computational Economics,07 June 2018,https://link.springer.com/article/10.1007/s10614-018-9824-7,Entropy Pooling with Discrete Weights in a Time-Dependent Setting,April 2019,Martin van der Schans,,,Male,Unknown,Unknown,Male,"Allocation decisions are often based on a combination of expert knowledge and forecasting models. The forecasting models combine historical data with forward-looking market assumptions to produce a multivariate density forecast for the relevant asset classes. Allocation decisions, however, can be sensitive to small adjustments in the density forecast, see Michaud and Michaud (2008) and the reference therein. In practice, adjustments in the density forecast can arise from a need to investigate the impact of using different modeling assumptions or from a need to incorporate expert opinion in the density forecast. In this paper, we present a framework for adjusting density forecasts. The framework can be used in a wide range of applications among which are: sensitivity analyses, incorporating expert opinion and stress testing. Although adjustments in the density forecast can sometimes be traced back to modeling assumptions, adjusting the density forecast directly instead of indirectly through its modeling assumptions has two major advantages. First, adjusting the density forecast can be done computationally efficient for both simple and complex models. For complex models, the density forecast is, typically, represented by sample paths, called scenarios, generated through a computationally costly Monte Carlo simulation, see Fig. 1. The computational cost makes simulating with different modeling assumptions at least impractical and at worst practically infeasible. In contrast, the framework in this paper takes a representation of the density forecast by scenarios as the starting point and, as we will show, can be implemented at low computational cost. Second, for practitioners, adjusting the density forecast through interpretable characteristics such as expectation and correlation is more easily grasped than adjusting the density forecast through abstract model parameters. Example of two marginal distributions of a multivariate density forecast represented by 2000 scenarios on a monthly basis over a horizon of 32 years. The black lines are the deciles and the gray lines are the first 100 scenarios. a US equity in log index shape, b US economic growth in log index shape The framework presented here can adjust several characteristics of the density forecast: the forecasted expected value, the forecasted variance and, in particular, the forecasted correlation structure. In addition, it can be applied in both a one-period setting, i.e., when the density forecast can be represented by sample points, and in a time-dependent setting, i.e., when the density forecast can be represented by scenarios as in Fig. 1. The one-period setting is a direct application of Meucci’s entropy pooling method. Meucci (2008) covers the method’s theoretical foundation and shows how to incorporate several adjusted statistics, e.g., correlations, and adjusts the density forecast as little as possible. Under simplifying assumptions, such as normality, Meucci’s entropy pooling method can determine the adjusted forecast analytically. In general, i.e., when a density forecast is represented by sample points, we have to resort to its computational approach which incorporates the adjustments by assigning weights to the sample points. The weights are non-negative and sum to one. Meucci and Nicolosi (2016) presents an extension of the theoretical foundation to a time-dependent setting. This paper contributes to the literature in two ways. First, to apply Meucci’s entropy pooling method to scenarios instead of sample points, we place the computational approach in a time-dependent setting. As noted, in the one-period setting, the computational approach incorporates the adjustments by assigning weights to the sample points. In the time-dependent setting, the computational approach incorporates the adjustments by assigning weights to the scenarios, i.e., the weights of the scenarios differ, but the weight of scenario j is the same for each variable. Second, to ease the method’s usability in practice, we prefer it to be usable in applications assuming equally weighted scenarios. Therefore, we present a heuristic that converts the resulting weights to discrete weights. So, instead of continuous non-negative weights summing to one, the weights are non-negative integers and sum to a required number of scenarios. The adjusted density forecast can now, instead of by scenarios with weights, be represented by a new set of scenarios containing each scenario multiple times.",
53.0,4.0,Computational Economics,10 July 2018,https://link.springer.com/article/10.1007/s10614-018-9829-2,Solving Rational Expectations Models with Informational Subperiods: A Comment,April 2019,Frank Hespeler,Marco M. Sorge,,Male,Male,Unknown,Male,,2
53.0,4.0,Computational Economics,04 July 2018,https://link.springer.com/article/10.1007/s10614-018-9830-9,A Reply to Reaction on Kormilitsina (2013): “Solving Rational Expectations Models with Informational Subperiods: A Perturbation Approach”,April 2019,Anna Kormilitsina,,,Female,Unknown,Unknown,Female,,1
53.0,4.0,Computational Economics,11 July 2018,https://link.springer.com/article/10.1007/s10614-018-9833-6,Possibilistic Moment Models for Multi-period Portfolio Selection with Fuzzy Returns,April 2019,Yong-Jun Liu,Wei-Guo Zhang,,,,Unknown,Mix,,
53.0,4.0,Computational Economics,13 July 2018,https://link.springer.com/article/10.1007/s10614-018-9835-4,Modeling Persistence and Parameter Instability in Historical Crude Oil Price Data Using a Gibbs Sampling Approach,April 2019,Nima Nonejad,,,,Unknown,Unknown,Mix,,
54.0,1.0,Computational Economics,27 February 2018,https://link.springer.com/article/10.1007/s10614-018-9804-y,Introduction to Advanced Statistical Analyses for Computational Economics and Finance,June 2019,Fredj Jawadi,,,Unknown,Unknown,Unknown,Unknown,,
54.0,1.0,Computational Economics,25 April 2019,https://link.springer.com/article/10.1007/s10614-019-09893-z,Correction to: Introduction to Advanced Statistical Analyses for Computational Economics and Finance,June 2019,Fredj Jawadi,,,Unknown,Unknown,Unknown,Unknown,,
54.0,1.0,Computational Economics,11 April 2017,https://link.springer.com/article/10.1007/s10614-017-9681-9,Forecasting Corporate Bankruptcy Using Accrual-Based Models,June 2019,Philippe du Jardin,David Veganzones,Eric Séverin,Male,Male,Male,Male,"Bankruptcy prediction models that have been designed over the last 50 years, either by academicians or financial institutions, mostly rely on accounting data. Indeed, since the papers by Beaver (1966) and Altman (1968), they are generally estimated using financial ratios derived from firm income statements and balance sheets given that accounting data are easily available and rather standardized. However, even if such data present some major advantages for forecasting bankruptcy, they suffer from several drawbacks, including the fact that they can be “manipulated” (Ooghe and Joos 1990; DeFond and Jiambalvo 1994; Sweeney 1994). As a consequence, if the quality of accounting information can be called into question, one may wonder to what extent such alterations can influence the accuracy of bankruptcy prediction models. This is the reason why we have studied a way to overcome the loss of model performance due to firms “manipulating” the figures of their annual accounts. Our work relies on the hypothesis that a firm may “manipulate” its accounts depending on its degree of financial health and, more precisely, it may alter its level of earnings (otherwise known as earnings management); we may speculate that distressed firms tend to increase their perceived value by engaging in upward earnings behavior so as to hide their weaknesses, whereas healthy firms tend to do the opposite and engage in behavior that downplays their earnings, so as to minimize their level of taxation. Thus, taking into consideration the fact that both healthy and post-bankrupt firms engage in accounting manipulation through different forms of earnings management (d’Argenti 1976; Rosner 2003; Charitou et al. 2007), we propose in this paper to study the influence of a measure that is intended to embody potential accounting manipulations, and not effective manipulations, on the performance of bankruptcy prediction models. Firstly, we examine the influence of an earnings management variable on model accuracy. Secondly, we try to identify the extent to which models are sensitive to earnings management. As far as we know, this is the first paper that studies the influence of earnings management and bankruptcy models. We used two specific models that are commonly used in the financial literature to estimate an earnings management variable using an accounting concept called “accruals”: the model presented by Dechow et al. (1995) and that of Kothari et al. (2005). Finally, we compared the results achieved using solely financial ratios with those that were estimated using both financial ratios and an earnings management variable. Models were designed with methods traditionally used to build bankruptcy models (linear discriminant analysis, logistic regression, a neural network, an extreme learning machine and a support vector machine) and two ensemble methods (bagging and boosting). The results were assessed using several performance criteria and estimated using different samples, so as to make sure that our measures are reliable, and different time-horizons so as to analyze the way earnings management may affect model accuracy depending and the forecasting horizon (1, 2 and 3 years). The remainder of this paper is organized as follows. In Sect. 2, we present a literature review that explains our research question. In Sect. 3, we describe the data, samples and methods used in our study. In Sect. 4, we present and discuss the results and, in Sect. 5, we conclude.",14
54.0,1.0,Computational Economics,11 April 2017,https://link.springer.com/article/10.1007/s10614-017-9680-x,Testing for Periodic Integration with a Changing Mean,June 2019,Tomás del Barrio,Mariam Camarero,Cecilio Tamarit,Male,Female,Male,Mix,,
54.0,1.0,Computational Economics,25 April 2017,https://link.springer.com/article/10.1007/s10614-017-9682-8,Performances of Model Selection Criteria When Variables are Ill Conditioned,June 2019,Peter S. Karlsson,Lars Behrenz,Ghazi Shukur,Male,Male,Male,Male,"In many empirical contexts in which one is using multiple regression models to explain a phenomenon, the researcher very often faces a number of competing specifications of the regression model regarding how many and which regressors should be included in the final model. In such situations, it is very common for researchers to base their decisions on model selection criteria. Model selection criteria are used to evaluate a number of competing model specifications; based on the values of these criteria, the researcher makes a final choice by selecting the model for which the criteria yield the highest or smallest value. A number of model selection criteria have been developed over the years; in this paper, we are mainly interested in four information criteria that are commonly used in multiple regression contexts: the adjusted coefficient of determination (\(\hbox {R}^{2}\)-adj), Akaike’s information criterion (AIC), the Hannan–Quinn information criterion (HQC) and the Bayesian information criterion (BIC). Applying any of these criteria would be straightforward in an ideal context (i.e., when all underlying assumptions in the regression model are met, the \(\hbox {R}^{2}\) is close to one and the sample size is very large in relation to model complexity). Applying information criteria is however not straightforward when any of these assumptions are not met, for example if \(\hbox {R}^{2}\) is low, or the sample size is very small. Then the properties of these information criteria can change, and some might not work as well as others in some situations. Our focus in this paper is to determine how these model selection criteria are affected by ill conditioned regressors (Frisch 1934) when one is trying to decide what model to choose and this problem arises in situations in which the explanatory variables are highly inter-correlated. It then becomes difficult to disentangle the separate effects of each explanatory variable on the explained variable. This is of great importance because regressors are non-orthogonal in most empirical applications in social science, especially in economics in which regressors is more likely to be correlated due to links between almost all types of economic activity. Should we therefore base our model selection decisions on model selection criteria when multicollinearity is present? To the authors’ knowledge, the performance of model selection criteria has not yet been investigated when the regressors are ill conditioned, i.e. when multicollinearity is present. There is therefore a need to evaluate and compare the properties of the four selected model selection criteria when the situation is not ideal, especially when various degrees of multicollinearity are present, which will be investigated in this paper. The paper is organized as follows. In the next section, the four different model selection criteria are presented. In Sect. 3, a number of factors that can affect these properties are introduced. In Sect. 4, the design of our Monte Carlo experiment is presented. In Sect. 5, we describe the results for the rate of successfully identifying the true model for the four different criteria. The conclusions of the paper are presented in the final section.",5
54.0,1.0,Computational Economics,12 May 2017,https://link.springer.com/article/10.1007/s10614-017-9691-7,Fast and Adaptive Cointegration Based Model for Forecasting High Frequency Financial Time Series,June 2019,Paola Arce,Jonathan Antognini,Luis Salinas,Female,Male,Male,Mix,,
54.0,1.0,Computational Economics,17 May 2017,https://link.springer.com/article/10.1007/s10614-017-9693-5,Testing for Constant Parameters in Nonlinear Models: A Quick Procedure with an Empirical Illustration,June 2019,J. del Hoyo,G. Llorente,C. Rivero,Unknown,Unknown,Unknown,Unknown,,
54.0,1.0,Computational Economics,29 May 2017,https://link.springer.com/article/10.1007/s10614-017-9696-2,"Asset Returns Under Model Uncertainty: Evidence from the Euro Area, the US and the UK",June 2019,João M. Sousa,Ricardo M. Sousa,,,Male,Unknown,Mix,,
54.0,1.0,Computational Economics,29 May 2017,https://link.springer.com/article/10.1007/s10614-017-9697-1,Nowcasting GDP Growth for Small Open Economies with a Mixed-Frequency Structural Model,June 2019,Ruey Yau,C. James Hueng,,Unknown,Unknown,Unknown,Unknown,,
54.0,1.0,Computational Economics,31 May 2017,https://link.springer.com/article/10.1007/s10614-017-9698-0,Predicting US Banks Bankruptcy: Logit Versus Canonical Discriminant Analysis,June 2019,Zeineb Affes,Rania Hentati-Kaffel,,Unknown,Female,Unknown,Female,"The financial crisis starting in 2007 is considered as the first real crisis of excess financial complexity. It has illustrated the degree of the existing inter-connectivity between banks and financial institutions and highlighted the phenomenon of contagion that might exist in the interbank market. Since then, a swarming literature has been developed on the subject of quantification, prediction and control of systemic risk. One of the methods proposed to prevent contagion of bank failures is to assess the bank failure rate. This approach helps to establish an early warning model of bank difficulties. Thus, interactions between solvency and refinancing risk can identify banks which have the most difficulties to refinance and therefore it would be perceived as risky by the other institutions. This flagging process would limit counterparty risk and warn the financial authorities of a pending liquidity risk in case of default of these banks. The financial literature is rich of methods and models which aim to identify institutions whose financial situations appear alarming and call supervisors to act. In this study, we use random subspace method to compare the classification and prediction of both Canonical Discriminant Analysis (CDA) and Logistic Regression (LR) model with and without misclassification costs, applied to a large panel of US banks over the period 2008–2013. Main questions raised in our paper are how to aggregate classifications results, what EWS should be proposed to regulators and also how Area Under Curve and H-measure (Hand 2009) perform classification problem? The specificity of our study lies in the extensive list of used financial ratios (solvency ratios, quality of assets, cash or liquidity...) and the depth of sample (US banks:1224 per years) including large ans small bank. The choice of the period is justified by the high number of failed banks which have taken place there. Also, we show in this paper how the clustering quality is improved by using a more appropriate cut-off. By contrast, studies in the literature wrongly use the probability “0.5” as a cut-off value in the logit and “0” in the CDA model. Moreover, the majority of papers discussing the same topics did not consider the cost of misclassifying an active or a defaulted bank. Our main contribution here is the use of the misclassification cost to enhance the conformity between obtained results and reality. The empirical literature distinguishes two methods: parametric and non-parametric validation. Beaver (1966) was the pioneer in using a statistical model for predicting bankruptcy. The approach is to select from thirty financial ratios those which are the most effective indicators of financial failures. The study concludes that the (Cash flow/total debt) ratio is the best forecasting indicator. 
Altman (1968) tested Multiple Discriminant Analysis (MDA) to analyze 70 companies, first by identifying the best five significant explanatory variables from a list of 22 ratios and then by applying the (MDA) to calculate a Z-Altman score for each company. This score was almost accurate in predicting bankruptcy one year ahead. This model was then subsequently improved in Altman and Narayanan (1997) by proposing the zeta model that includes seven variables and classified correctly 96% of companies one year before bankruptcy and 70% five years before bankruptcy. Since then, the use of discriminant analysis has grown through the different published studies (Bilderbeek 1979; Ohlson 1980; Altman 1984; Zopounidis and Dimitras 1993...). The vast majority of studies achieved after 1980 used the logit models to overcome the drawbacks of the DA method (Zavgren 1985; Lau 1987; Tennyson et al. 1990...). The logit analysis fits linear logistic regression model by the method of maximum likelihood. The dependent variable(the probability of default) gets the value “1” for bankrupted banks and “0” for healthy banks. Numerous comparative studies were carried out (Keasey and Watson 1991; Dimitras et al. 1996; Altman and Narayanan 1997; Wong et al. 1997; Olmeda and Fernández 1997; Adya and Collopy 1998; O’leary 1998; Zhang et al. 1998; Vellido et al. 1999; Coakley and Brown 2000; Aziz and Dar 2004; Balcaen and Ooghe 2006; Balcaen et al. 2004; Kumar and Ravi 2007). However, the supremacy of one method over another remains subject to various controversies because of the heterogeneity of used data in the validation (database, number of points in the data, sample selection, validation methods for forecasting, the number and the nature of explanatory variables tested in the models (financial, qualitative...). The aim of this paper is twofold: descriptive and predictive. Descriptive is to be understood as a detailed analysis of used models inputs from financial and statistics point of view. Thus, we proceed by describing and analyzing key financial ratios of the active and non-active banks for the entire period from 2008 to 2013. We combined two parametric models (Canonical Discriminant Analysis and Logit) with the descriptive Principal Component Analysis model (PCA) to construct an Early Warning System (EWS). First, (PCA) reduced the dimension size of data and insure an uncorrelated blend of variables framework. Then, factor scores were estimated for each bank. These scores were used to estimate (CDA) and Logit models. One among the important results of this paper is to have compared several methods to calculate the theoretical value of the probability of default that will serve as threshold to split the bank universe into two set : failed and healthy. The paper consists of five sections. After the introduction, an overview of the existing literature dealing with the bank failure prediction is given. Section 2 describes used data and the methodology. In Sect. 3 we implement Principal component Analysis on our data. Section 4 provides the empirical results with and without misclassification costs. Final section contains concluding remarks.",17
54.0,1.0,Computational Economics,09 June 2017,https://link.springer.com/article/10.1007/s10614-017-9705-5,Asset Market Volatility and New Keynesian Macroeconomics: A Game-Theoretic Approach,June 2019,Namun Cho,Tae-Seok Jang,,Unknown,Unknown,Unknown,Unknown,,
54.0,1.0,Computational Economics,08 July 2017,https://link.springer.com/article/10.1007/s10614-017-9720-6,Low Complexity Algorithmic Trading by Feedforward Neural Networks,June 2019,J. Levendovszky,I. Reguly,A. Ceffer,Unknown,Unknown,Unknown,Unknown,,
54.0,1.0,Computational Economics,07 July 2017,https://link.springer.com/article/10.1007/s10614-017-9719-z,Applying Independent Component Analysis and Predictive Systems for Algorithmic Trading,June 2019,Attila Ceffer,Janos Levendovszky,Norbert Fogarasi,Male,Unknown,Male,Male,"In the recent decades, algorithmic trading combined with portfolio optimization has been one of the most intensively researched areas of financial mathematics. Ever since the seminal paper of Markowitz (1952), selecting portfolios which are optimal in terms of risk-adjusted returns has been in the centre of interest of both academics and financial practitioners. There are many approaches to algorithmic trading on profitable portfolios ranging from prediction based trading (Gestel et al. 2001; Kim 2003; Kumar et al. 2011) to mean reverting trading (Chan 2013; d’Aspremont 2007; Fogarasi and Levendovszky 2013). Many researchers use autoregressive (AR) models to describe financial time series. For model identification and prediction, the results of machine learning have been deployed using different learning architectures (e.g. feed forward neural networks, support vector machines etc.) (Kaastra and Boyd 1996; Lahmiri 2011; Saad et al. 1998). However, in most of the cases these methods did not provide high performance on real data for two reasons: (i) overfitting (a large number of free parameters are built in the model which is then poorly trained on a limited training set), and (ii) the large amount of noise superposed on the data, which makes it difficult to identify the tendencies. Furthermore, profit produced by algo-trading tended to be very slim in the presence of bid–ask spread and transaction costs. To improve trading efficiency, one of the most important issues is to avoid the effects of overfitting. A sophisticated learning architecture to capture the characteristics of the observed data series may have a large number of free parameters which, in turn, yields overfitting and poor training performance. To avoid this, one way to reduce the number of free parameters of the model is to minimize the dimension of input data. One possible approach is to use principal component analysis (PCA), however, by aggressive dimension reduction, one may also leave out important information from the input (Back and Weigend 1997, 1998). A better approach to reduce the chance of overfitting is to deploy noise filtering by using independent component analysis (ICA). In this case, we decompose the process into independent components and filter out the most “noise-like” component. 
Back and Weigend (1997) first showed in 1997 that ICA is a powerful technique for analyzing multivariate financial time series by applying it to three years of daily returns of the 28 largest Japanese stocks and compared the results with those using PCA. They concluded that stock prices can be reconstructed remarkably well using a small number of thresholded weighted ICs and ICA performed much better than PCA in their tests. Cheung and Xu (2001) confirms this finding and proposes a method for ordering the ICs according to their joint contributions to data reconstruction which we adopt in this paper. 
Kaastra and Boyd (1996) provides a very practical guide and an eight-step procedure to designing neural networks for forecasting economic time series data including a discussion of tradeoffs in parameter selection, some common pitfalls and points of disagreement among practitioners. 
Saad et al. (1998) compares different neural network architectures to identify short term stock trends and trading opportunities of blue chip US stocks based on historical daily closing prices. They conclude that all three neural network architectures examined (time delay, recurrent and probabilistic) are capable of producing profits on the examined data set. 
Kim (2003) applies the support vector machine method developed by Vapnik (1995) to forecast the direction of change in the daily Korea composite stock prices index (KOSPI) and concludes that SVM outperforms neural networks and is a promising alternative for stock market prediction. Lahmiri (2011) applies both SVMs and neural networks to the task of forecasting S&P500 daily market trends and concludes that neural networks work better with technical information (historical returns) whilst SVMs work best with macroeconomic inputs, yielding better results than neural networks. 
Wen et al. (2010) propose a new intelligent trading system based on oscillation box prediction by combining stock box theory and SVM. The experiments on 442 S&P500 components show a promising performance is achieved and the system dramatically outperforms buy-and-hold strategy. 
Lu et al. (2009) were first to use ICA as a pre-processing method and combine it with the regression model of SVMs, called support vector regression (SVR) and found that on Nikkei 225 and TAIEX data, the combined method outperforms SVR on non-filtered forecasting variables in terms of the mean squared error criterion for prediction. However, they did not consider the effect of ICA on the trading performance even though low mean squared error may not necessarily yield profitable trading (Kaastra and Boyd 1996). As such, the objective of this paper is to develop profitable trading algorithms by using NARX networks (Nonlinear AutoRegressive network with eXogenous inputs) and SVMs (support vector machine). Both architectures use an ICA based pre-processing, which is optimized for trading performance. The novelty of this paper lies in the following contributions: the NARX network in the present form, where the inputs are filtered and the output is the trading action to take (buy, sell or hold) has not yet been applied to algorithmic trading; a comparative analysis of NARX and SVM in terms of algorithmic trading performance; evaluating the relative trading performance of ICA and PCA pre-filtering methods. The new method was tested both on single asset price series and on mean reverting portfolios. The numerical results have proven that in both cases profit can be achieved. The tests were first performed on artificially generated data and then on real data selected from exchange traded fund (ETF) time series including bid–ask spread. In both cases the proposed method could achieve positive returns. The rest of this paper is organized as follows: in Sect. 2, the notations and mean reverting portfolio theory are introduced; in Sect. 3, the applied learning architectures (NARX network and SVM) are introduced; in Sect. 4, the application of ICA to filter out the input noise is shown; in Sect. 5, the computational model is detailed; in Sect. 6, a detailed performance analysis is given based on historical and generated data; in Sect. 7, some conclusions are drawn.",6
54.0,1.0,Computational Economics,04 August 2017,https://link.springer.com/article/10.1007/s10614-017-9721-5,Agent-Based Modeling of a Non-tâtonnement Process for the Scarf Economy: The Role of Learning,June 2019,Shu-Heng Chen,Bin-Tzong Chie,Ragupathy Venkatachalam,Unknown,Unknown,Unknown,Unknown,,
54.0,1.0,Computational Economics,04 September 2017,https://link.springer.com/article/10.1007/s10614-017-9732-2,Enhancing Quasi-Monte Carlo Simulation by Minimizing Effective Dimension for Derivative Pricing,June 2019,Ye Xiao,Xiaoqun Wang,,,Unknown,Unknown,Mix,,
54.0,1.0,Computational Economics,14 September 2017,https://link.springer.com/article/10.1007/s10614-017-9742-0,A Diffusion Model for Long-Term Optimization in the Presence of Stochastic Interest and Inflation Rates,June 2019,Farid Mkaouar,Jean-Luc Prigent,Ilyes Abid,Male,Male,Unknown,Male,"The continuous-time portfolio theory has been introduced by the seminal paper of Merton (1971). This has allowed to better understand and analyze the optimal dynamic portfolios in particular how it depends on both the financial assets parameters and investor preferences. Recall that when investors display constant relative risk aversion (hereafter CRRA) and when asset prices follow a geometric Brownian motion with a constant interest rate, the optimal portfolio weights are constant over time. However, other specificities must be taken into account, for example when dealing with long term investment. Long term investment under inflation risk is a rather involved problem. Indeed, the prediction of real asset returns is difficult for long time horizons: are equity returns riskier than bond returns? what is the exact impact of inflation on portfolio performance? How can we take account of the rolling positions, since bonds with very long maturities are not available on the financial markets? Does there exists financial instruments on which we can invest to hedge against risk inflation?Footnote 1 How can we model inflation rates and calibrate nominal interest rates? For example, Fisher relation (1930) allows to measure the impact of inflation on interest rates. According to this result, the nominal interest rate should be the sum of the real interest rate and the expected inflation rate. Cox et al. (1981, 1985), Benninga and Protopapadakis (1983) and LeRoy (1984) has proved that the Fisher relation does no longer hold, under uncertainty. Usually, younger investors, thus with a long investment horizon, invest a higher fraction of their portfolio value in stocks than older investors. But, as proved by Samuelson (1969), this is not consistent with portfolio allocations determined from basic models of portfolio optimization. Canner et al. (1997) note also that popular investment advice does not conform these allocations. Additionally, many empirical studies show that allocations between stocks, bonds and cash depend a lot on risk aversion. In particular, bond/stock ratios differ for conservative, moderate or aggressive investors. Bajeux-Besnainou et al. (2001), later on referred to as Bajeux-Besnainou et al. (2001), address this inconsistency issue between mutual fund property and popular advice. They consider that the investor’s horizon exceeds the maturity of the cash asset and they introduce a continuous-time portfolio rebalancing. In that case, cash may be a money market security with a short maturity (one to six months) and may be no longer the common riskless asset in the standard theory. A well-known property of optimal portfolio is the mutual fund separation theorem, which proves that a rational investor divides her investment between two assets: a riskless one and a risky mutual fund, the composition of which is the same, whatever the investor’s risk aversion. Cass and Stiglitz (1970) established conditions for two-fund separation, which holds for a large class of utility functions, such as the HARA functions. This result is also true when examining dynamic strategies (without problem of portfolio rebalancing, liquidity or transaction costs). We consider that inflation-indexed bonds are available on the financial market.Footnote 2 Lamm (1998) shows how such inflation protection securities can enhance the portfolio performance. He concludes that the ideal portfolio would correspond to \(100\%\) of investment on equities and inflation-protected bonds while zero-investment on the nominal bonds. Campbell and Viceira (2002) prove also that incorporating long-term inflation-indexed bonds in the portfolio increases significantly the utility of rather risk-averse investors. Brennan and Xia (2002) prove that a conservative investor would use a mix of nominal bonds to replicate the return of an inflation-indexed bond with maturity equal to the remaining investment horizon. Dealing with the asset allocation problem with stocks, conventional treasuries, inflation-indexed bonds, and a riskless asset, Kothari and Shanken (2004) and Roll (2004) conclude that a portfolio diversified between equities and nominal bonds would be improved by the addition of treasury inflation-indexed securities (TIPS) (see also Illeditsch 2016; Lamm 1998; Carteaa et al. 2012; Brière and Signori 2012). In this paper, first we introduce a general multifactor term structure precluding any arbitrage opportunity. We adopt the model introduced by Chiarella et al. (2007). In this framework, both inflation-indexed bonds and nominal bonds can be priced under a risk-neutral probability. Second, we solve the portfolio optimization problem, in the presence of both stochastic interest rate and inflation rate that exhibit mean-reverting returns.Footnote 3 We assume that the investor searches to maximize the expected utility of her real wealth. Our goal is to analyze how the introduction of inflation-indexed bonds modifies the investor’s portfolio allocation, usually based on standard money market account, bonds and stocks. Note that a representative bundle of commodity goods is taken into account in the utility of the final wealth. It corresponds to the inflation-protected cash.Footnote 4 As in Chiarella et al. (2007), we assume that inflation-indexed bonds are available on the financial market, which can potentially reduce the inflation risk. But contrary to Chiarella et al. (2007), we consider constant maturation bonds as proposed by Bajeux-Besnainou et al. (2001). Indeed, such bonds allow to obtain a Bond/Stock ratio which increases with time, when there exists no inflation. This nice property is in accordance with popular advice. The investor will usually continuously reinforce (in relative terms) her cash position over time. This is not in contradiction with the mutual fund separation theorem. Indeed, the T-maturity bond can be duplicated by investing both in the cash and in the constant maturation bond. The portfolio is optimally rebalanced as a function of the remaining time, as well as a function of current asset values.Footnote 5 In this framework, we compute the corresponding portfolio weights as in Chiarella et al. (2007) but for constant maturity bonds. Additionally, we extend both the results of Chiarella et al. (2007) by determining also the optimal portfolio payoffs and the results of Bajeux-Besnainou et al. (2001) by taking account of the inflation risk. For this purpose, to obtain closed form solutions for the optimal portfolios, we use the transformation of the dynamic problem into a static problem as developed in Cox and Huang (1989). Using this method, we first obtain the optimal portfolio value and then the optimal strategy (weights). Indeed, using the martingale approach developed by Cox and Huang (1989),Footnote 6 we provide explicit solutions for the optimal portfolio payoffs and the corresponding portfolio weights.Footnote 7 To analyze the final portfolio wealth, we also provide and analyze the cumulative distribution functions of the optimal portfolio values for the most standard utility functions, such as CRRA and CARA utilities. As a by-product, we examine if the result of Wachter (2003) is also true for constant duration bonds, namely that, when there is no inflation, the most risk averse investors invest all their wealth in the nominal bond maturing at the end of the investment horizon. Our results show that, even we can be partially hedged against inflation by investing on an inflation-protected cash, the inflation-index bonds are profitable to investors having at most a moderate risk aversion, as soon as the investment horizon rises. Note that the fact that inflation-indexed bonds can be beneficial for investors does not necessarily mean that they would take a large share in their portfolio relative to other outstanding debt. The paper is organized as follows. Section 2 presents the financial market. The multi factor model that describes both the nominal bond and the inflation-indexed bonds is in particular detailed. We compute also explicitly the risk-neutral density and determine all the risk premia. Section 3 provides the solution of the optimization problem, for quite general utility functions. We detail and illustrate the solution for the logarithmic, CRRA and CARA cases. We analyze and compare the cumulative distribution function of the portfolio return for various both financial and risk-aversion parameter values and also with respect to portfolio horizon. Several technical proofs are relegated in Appendix.",2
54.0,1.0,Computational Economics,15 September 2017,https://link.springer.com/article/10.1007/s10614-017-9746-9,Good Policies or Good Luck? New Insights on Globalization and the International Monetary Policy Transmission Mechanism,June 2019,Enrique Martínez-García,,,Male,Unknown,Unknown,Male,"An ongoing topic of discussion among policymakers is how best to think about the role of openness for the conduct of monetary policy (Fisher 2005, 2006; Bernanke 2007; Trichet 2008, and more recently Draghi 2015; Kaplan 2017). Policymakers increasingly recognize that international linkages cannot be ignored in guiding policy, yet in many cases the closed-economy model—often a variant of the workhorse New Keynesian model (Woodford 2003)—largely remains the starting point for policy analysis. Much research is devoted nowadays to explore questions relating to how openness influences policy analysis and to what extent the closed-economy setting offers a useful approximation for policy-making, whenever economies have in fact become more interconnected. How do natural rates and potential output depend on foreign developments? Is the Phillips curve relationship between domestic inflation and domestic slack flatter or steeper for open economies and what does that entail? Can greater openness contribute to lower volatility and to alter the persistence and cross-country comovement of macro aggregates characteristic of the Great Moderation period? And, perhaps most notably, how does openness influence the policy trade-offs confronted by policymakers under a Taylor (1993)-type monetary policy regime? The main purpose of this paper is to investigate monetary policymaking under alternative monetary policy regimes within the workhorse two-country New Keynesian model, explicitly incorporating a role for openness. I build on the model of Martínez-García and Wynne (2010) and Martínez-García (2015b) characterized by flexible nominal exchange rates, trade openness, and asymmetric shocks across countries—which provides a straightforward extension of the standard three-equation (closed-economy) New Keynesian model to an open-economy setting. The approach I pursue here is to inspect the mechanism of the open-economy New Keynesian model more closely by focusing on its three main building blocks in log-linear form: the open-economy Phillips curve, the open-economy dynamic IS curve, and the monetary policy rule [specified in the form of conventional Taylor (1993)-type rules] for each country. The orthogonalization method I use to solve the model helps characterize analytically the main macro variables of each country in terms of aggregates and the corresponding differences between the two countries.Footnote 1 This paper makes an important methodological contribution as well illustrating how to decompose the solution whenever monetary policy rules differ across countries. My main postulate is that trade and economic integration have altered the environment in which monetary policy must be conducted. In fact, this model helps show that even modest trade linkages expose the domestic economy to significant impacts from foreign shocks as well as to foreign policies. Here, my analysis contributes to the ongoing debate on globalization in two ways: First, it fleshes out an analytical framework to understand how the monetary policy transmission mechanism under different policy regimes is altered by the degree of openness and, second, it provides closed-form solutions that are tractable and facilitate a positive analysis of international monetary policy transmission and coordination. An important conclusion of this paper is that Taylor (1993)-type policy rules involving some form of international monetary policy coordination contribute to decoupling the dynamics of the aggregates from those that characterize the dispersion across countries. This has an impact on the propagation of shocks across countries and on macro volatility as well which varies with the openness of the economy. I find that generally the impact of globalization is underpredicted by standard measures of trade openness, as they do not fully capture the strength of trade effects. In fact, I show that the effects of the trade channel do not depend solely on the extent of trade openness—but critically depend on the trade elasticity of substitution between locally-produced and imported goods too. The trade channel gives greater significance to foreign developments on domestic macro aggregates than what standard trade openness measures would suggest given how demand shifts across countries—which are sensitive to the trade elasticity of substitution—propagate the effects of foreign shocks indirectly also through movements in international relative prices (real exchange rate, terms of trade). Furthermore, I illustrate some of the pros and cons of explicit agreements among central banks for the coordination of monetary policy and for the formation of a currency union. I show that a common monetary policy is an important benchmark for policymaking, but it is also key to determine how policy asymmetries across countries propagate and modify the equilibrium dynamics under the common monetary policy regime. I also show that deeper monetary policy integration in the form of a currency union has no bearing on the aggregate dynamics of the countries that adopt the common currency and common monetary policy, but may result in an indeterminate solution at the country-level unlike under international monetary policy coordination. With this framework at hand, I make the theoretical case for why trade openness (globalization) matters more than what we generally think: The model shows that the trade channel provides a plausible avenue to explain a number of still-debated stylized facts in the international macro literature [such as the findings of Roberts (2006) and IMF WEO (2013) on the flattening of the Phillips curve or Ciccarelli and Mojon (2010), Duncan and Martínez-García (2015), Bianchi and Civelli (2015) and Kabukcuoglu and Martínez-García (2016b) on the dynamics of inflation]. The literature has intensely debated whether the Great Moderation was the result of good luck, good monetary policy internationally, or changes in the structure of the economy (Benati and Surico 2009; Woodford 2010).Footnote 2 I provide an analytical assessment of the trade channel and its significance showing that changes from greater trade integration can reduce volatility—broadly in line with the Great Moderation experience. Trade also influences the trade-offs faced by policymarkers, alters the propagation of shocks, and even the contribution of different shock types (productivity shocks, cost-push shocks, monetary policy shocks) to the business cycle. The paper expands on the existing literature on the monetary policy transmission mechanism in an open economy setting (Benigno 2004; Woodford 2010). I conclude that the persistence of the macro variables is largely unaffected by either the strength of the trade channel or the features of the monetary policy rule. Most notably, I show that a coordinated common monetary policy isolates the effects of the trade channel to operate solely through the cross-country dispersion but not on the macro aggregates. I also note that forming a currency union as a means of deepening monetary policy integration may in turn lead to indeterminacy. This is a novel insight that, to my knowledge, has not been discussed elsewhere in the optimal currency area literature. To conclude, I argue that structural change—in particular, greater trade integration (globalization)—as well as good monetary policy based on a strengthened anti-inflation bias have effectively altered the vulnerabilities to shocks of the economy over the past several decades. Hence, I claim that trade openness and monetary policy do influence the effects of shocks transmitted on the economy and that those theoretical implications derived from the model appear largely consistent with the stylized facts of the Great Moderation. The rest of the paper is organized as follows. In Sect. 2, the log-linear approximation to the equilibrium conditions is discussed. Section 3 analyzes the monetary policy framework investigated in the paper and defines alternative monetary policy regimes resulting in the adoption of a common monetary policy—international monetary policy coordination—and in the formation of a currency union. Section 4 characterizes the analytical solution of the linear rational expectations model using Taylor (1993)’s monetary policy set-up as a benchmark. It then offers a detailed assessment of the policy trade-offs between slack and inflation and the implications for volatility amongst open economies—under independent (asymmetric) monetary policies, under common (coordinated) monetary policies, and within a currency area. Finally, Sect. 5 outlines some concluding remarks.Footnote 3",8
54.0,1.0,Computational Economics,16 January 2018,https://link.springer.com/article/10.1007/s10614-018-9794-9,Forecasting Inflation Uncertainty in the United States and Euro Area,June 2019,Zied Ftiti,Fredj Jawadi,,Male,Unknown,Unknown,Male,"Uncertainty is a situation of doubt, unpredictabilityand riskiness, which varies with regard to the market state and exogenous shocks. For example, geopolitical uncertainty becomes more volatile with the level of geopolitical tensions (e.g. in Eastern Europe and after the Arab Spring), terrorist threats, and war. In economics, uncertainty is a particularly important issue as it significantly affects different macroeconomic variables (Bloom et al. 2007; Bachmann et al. 2013; Jurado et al. 2015; Rossi and Sekhposyan 2015). Indeed, economic uncertainty constitutes a type of economic post-traumatic stress disorder for householders, investors, and the entire market. Economic uncertainty is also a source of financial instability and economic inequality. It affects consumers, investors, and householders, pushing them to question the underlying mechanisms of economic growth, unemployment, economic policy, and inflation.Footnote 1 Interestingly, economic uncertainty has recently increased because of the fragility of the financial system, increase in public and private debt, decrease in economic growth, and the renewal of secular stagnation combined with a global liquidity trap since the aftermath of the global financial crisis (2008–2009). In particular, economic policy uncertainty has been remarkable given the absence of a clear policy and institutional structure. In such a context, further doubt about the effectiveness of central banks’ tools, clarity of their policy, and transparency of their action and communication might yield inflation uncertainty (IU; Holland 1993). IU is related to uncertainty about future inflation and therefore to the action of the central bank. According to Holland (1993), IU might also arise when the policy effect takes time to work. It can reduce economic well-being via decisions on business investment and consumer saving (Holland 1993) and might imply uncertainty about other variables such as the interest rate and therefore about the real value of future payments (Golub 1994). Different approaches have been used to estimate IU. On the one hand, IU can be estimated through surveys distributed to consumers and economists asking them to provide an acceptable range of inflation. Such surveys (e.g. Lahiri and Lui 2006), compared with the true level of inflation, can facilitate the measurement of IU. On the other hand, forecasting strategies are employed to predict inflation with large forecast errors suggesting further evidence of IU. Moreover, in the related literature, several approaches have been used to model and forecast IU. Early studies considered the standard deviation of inflation to be a proxy of IU. However, conditional variance quickly became the most popular proxy to measure IU based on the ARCH (autoregressive conditional heteroscedasticity) and GARCH (generalized ARCH) models of Engle (1982) and Bollerslev (1986), respectively to take into account further persistence in the data (Holland 1993; Ben Nasr et al. 2015). Over the past decade, the stochastic volatility model has also been applied to model IU. For example, Berument et al. (2009) employed the stochastic volatility in mean model in line with Koopman and Hol Uspensky (2002) and Chan (2017) extended the model of Koopman and Hol Uspensky (2002) to allow for time-varying parameters of IU. Bårdsen et al. (2002) theoretically and empirically evaluated the advantages of three theoretical models and highlighted the use of standard Phillips curves for modelling IU compared with the new Keynesian Phillips curve and incomplete competition models. More recently, Bauer and Neuenkirch (2017) employed a standard new Keynesian model to forecast inflation and economic growth uncertainty. In this study, we follow this second approach to measure IU through symmetric and asymmetric conditional variance models as well as stochastic volatility models (Poon and Granger 2003). The use of these three models offers a suitably flexible framework to account for further asymmetry as well as outliers in IU dynamics. In addition, we focus on two major regions, namely the United States and the Euro Area, in which inflation has experienced different episodes over recent decades, suggesting further evidence of IU. For example, since the subprime mortgage crisis, the US inflation rate has been volatile, falling from about 4.08% in 2007 to 0.09% in 2008, pushing policymakers to adopt unconventional monetary policy rules. As a result, US inflation increased in 2009 to 2.96%, and it has showwn several increases and decreases.Footnote 2 In the Euro Area, the inflation rate has shown similar behaviour. In 2007, inflation reached 3.11%, while it has remained below its target rate (2%) since 2013.Footnote 3 Our findings show the superiority of the stochastic volatility model for forecasting the dynamics of IU over the short (1 year) and medium (4 years) terms. This finding is particularly interesting to better estimate the main inflation cost, namely IU, and its effect on the real economy. To our knowledge, it is the first essay to propose forecast of inflation uncertainty with the stochastic volatility model. The remainder of this paper is structured into three sections. Section 2 presents the econometric methodology. Section 3 describes the main empirical results and the last section concludes.",7
54.0,2.0,Computational Economics,18 July 2018,https://link.springer.com/article/10.1007/s10614-018-9836-3,Solving Transfer Pricing Involving Collaborative and Non-cooperative Equilibria in Nash and Stackelberg Games: Centralized–Decentralized Decision Making,August 2019,Julio B. Clempner,Alexander S. Poznyak,,Male,Male,Unknown,Male,"Prior to an evaluation of the different related works on transfer pricing methods and the properties commonly associated with them, we briefly address why transfer prices are one of the fundamental problems in accounting and economics. We present an understanding and appreciation of the underlying causes of why transfer pricing is required before presenting any proposal for solving the problem. Next, we introduce the main results of the paper. Transfer pricing is the setting of the price for goods and services sold between related legal intermediary or sub-holdings of a multinational enterprise (MNE) (Hirshleifer 1956, 1957, 1964). From a financial point of view, transfer pricing is perhaps the most important challenge tax issue in the last decades. It is a fundamental tool for corporate tax avoidance (base erosion and profit shifting). Transfer prices involve two fundamental activities: coordination and profit allocation. Coordination is related to the centralized and decentralized transfer pricing decision-making process. The profit allocation is related to maximizing the well-being of the MNE. There exist many reasons that justify why transfer pricing has become an interesting topic for MNEs: (a) the rise of new economies in developing countries, (b) the rapid advancement of technology, (c) the new way the world operates, (d) the communication facilities that rise a new commerce and business, (e) low production costs, and many others. MNEs usually have divisions or sub-holdings in different countries. For instance, production of the assembly of final products may take place in many different places around the world. The decision-making process vary from centralized structures to extremely decentralized structures having profit responsibility assigned to certain strategic executives. The structure of the MNE plays an important role in the computation of the transfer pricing model. The classical transfer pricing model considers a cooperative approach taking into account that each division purchases goods from an upstream division in the supply chain. However, divisions of a MNE not necessarily act cooperatively. Divisions are considered associated enterprises when they belong to a decentralized structures: structure forced by the local law, represent different stakeholder groups, etc. The profit or expenses allocation to one or more divisions of the MNE is not sufficient for computing the optimal level of transfer pricing. Particular attention must be concentrated on the meaning and scope of associated enterprises, which is a subject of importance, but it is not discussed properly in the literature. An interesting case of transfer pricing arises in response to the existence of decentralized divisions in which responsibility centers trade among themselves. Decentralization is a necessary condition for developing a model for transfer pricing. This directly leads us to a key issue of transfer pricing: sub-optimizations by individual divisions of a MNE do not result in an optimum (Enzer 1975). The organizational structure literature (Eccles 1983; Swieringa and Waterhouse 1982) assumes the existence of benefits that divisional decentralization is thought to provide. Thomas (1980) presented a review of the psychological theories related to divisional decentralization. Nevertheless, this is where the conflict theory arises: division executives might engage in actions that would benefit their divisions to the detriment of the entire MNE. This behavior is against to global MNE objectives of maximizing the well-being of the entire firm, and they would want the decentralized divisions to behave as if the firm was centralized. The transfer pricing problem consists in fixing the transfer pricing in such a way that divisional sub-optimizations imply an equilibrium involving cooperative and non-cooperative behavior. It is related to the pricing manipulation when the expected tax penalty is a function of the tax enforcement and the market price parameter. For small companies several simple methods can be considered for computing the transfer pricing: average cost, market price, market price minus a margin, market-based negotiated price, etc. However, for MNE the problem must consider several international restrictions. The arm’s length principle allows to an adequate allocation of profit taxation rights among countries that conclude double tax conventions, implying the existence of a range of acceptable prices determined by market. The arm’s length standard is instrumental to determine how much of the profits should be attributed to one entity: a MNE can manipulate transfer prices more easily if market price range is open, or if its restrictions are complicated. Home taxation of foreign profits can reduce income shifting incentive, depending on the portion of repatriation for tax purposes. A number of approaches to resolving the transfer pricing conflicts are documented (for a survey see, for instance, Abdel-Khalik and Lusk 1974; Alm 2012; Beer and Loeprick 2015; Devereux and Maffini 2007; Grabski 1985a, b; Göx and Schiller 2007; McAulay et al. 2001). Most of the literature dealing with transfer pricing from an economic viewpoint is based on the analysis of Hirshleifer (1956, 1957, 1964) who assumed a firm with two profit centers suggesting an optimal transfer pricing method and output level aimed at overall profit maximization using the marginal price determination theory considering a net marginal revenue. Kanodia (1979) improved the work of Hirshleifer (1956) incorporating uncertainty and allowing risk-sharing between divisional managers and central management, but does not formulate a mechanisms for achieving uncertainty, risk-sharing and Pareto optimality. Blois (1978) enhanced the work of Hirshleifer (1956) showing that although the firms are independent, a large customer might be able to impose the transfer price rule of marginal cost upon its suppliers. Enzer (1975) extended Hirshleifer’s work (Hirshleifer 1956) by relaxing of the constraint of a linear homogeneous production function employing a linear programming approach obtaining as a result that transfer prices at some form of average cost. As well as, Jennergren (1977) changed the proposal of Enzer (1975) considering that the production division must operate at production level, eliminating the concept of a decentralized firm and centralizes decision making for the production division. This coincides with the proposal of Thomas (1980), Dearden (1973) and Henderson and Dearden (1966) who noted that serious transfer pricing problems arise from an inconsistent organization structure. Horst (1971) explored the profit-maximizing strategy for a monopolistic firm selling to two national markets simultaneously. Kassicieh (1981) discussed the behavioral consequences of the setting of transfer prices by top management and presented a model of transfer pricing that is developed further to include all of the issues that affect the total profits of the multinational corporation through transfer prices. Schjelderup and Sørgard (1997) developed transfer pricing model for affiliates encountering Cournot as well as Bertrand competition. Vaysman (1998) suggested a model of negotiated transfer pricing incorporating private divisional information involving a compensation system utilizing divisional performance evaluation employing a dynamic bargaining model to capture managerial negotiations. Wettstein (1994) constructed a mechanism whose Nash equilibria generate a profit-maximizing allocation of inputs as well as a system of (endogenously determined) transfer prices that uses smaller strategy spaces and intuitively evaluation measures. Zhao (2000) presented a simple oligopoly model of a partially decentralized MNE in competition with a rival firm showing that transfer pricing can be used as a rent-shifting device by the MNE to compete with the rival. Chang and Hajime (2004) established a vertically-related but non-integrated model with one upstream and two downstream firm presenting the upstream firm’s discriminating and uniform pricing policies towards the downstreams and also a model of bargaining over the input price. Lakhal et al. (2005) developed a transfer pricing method using a mathematical programming model. Lakhal (2006) improved the previous method proposing a framework and methodology for profit sharing and transfer-pricing between network companies that enabled maximization of operating profits, suggesting a departure from the model that maximizes profits for the individual company within the sphere of its own supply chain. Arya and Mittendorf (2008) showed that imperfect competition may justify market-based transfer prices, considering that transfer price will deviate from marginal cost and thereby distort subsidiary choices can lead a parent to undertake actions to influence the market price of the upstream good. Villegas and Ouenniche (2008) presented a model that integrates many factors such as transport costs and duty drawbacks, which are critical for supply chains that operate under international trade regulations. They studied the optimality conditions of the corporate decision variables to derive managerial guidelines and to determine how decisions regarding trade quantities, transfer prices, and transport cost allocations affect the amount of taxes to be paid to host governments as well as the total after-tax repatriated earnings of the corporation. Rosenthal (2008) suggested a cooperative game theory solution approach for transfer pricing in a vertically integrated supply chain. Dürr and Göx (2011) studied the equilibrium accounting and transfer pricing policies in a multinational duopoly with price competition in the final product market finding that the firms in a duopoly can benefit from strategically using the same transfer price for tax and managerial purposes instead of using separate transfer prices for both objectives. Matsui (2011) investigated the welfare consequences of the arm’s-length principle on transfer prices in multinational firms when they face Cournot competition not only in the upstream market of the exporting country but also in the downstream market of the importing country. Leng and Parlarb (2012) suggested a transfer pricing model for a multidivisional firm in a Stackelberg game with an upstream division and multiple downstream divisions. The downstream divisions can independently determine their retail prices, and decide on whether or not they will purchase from the upstream division at negotiated transfer prices. Huh and Park (2013) considered two commonly used transfer pricing methods for tax purposes: the cost-plus method and the resale-price method and compared the supply chain profits under these two methods. Winston and Han (2013) found key factors that affect a firm’s optimal transfer pricing policy examining two minimalist vertical models considering four modes of competition Cournot, Bertrand, Stackelberg quantity, and price. Hammami and Frein (2014) developed a large scale optimization model using two methods that are specific to the problem of supply chains redesign while addressing decisions, costs, and complexity factors. Fernandes and Gouveia (2015) presented a transfer pricing framework for distribution network strategy and studied how various transfer-pricing schemes cope with stochastic demand under different countries tax policies. Gao and Zhao (2015) provided a model for optimal transfer price in a multinational corporation to maximize the entire profit incorporating elements such as international transportation costs, holding costs, taxes, tariffs and exchange rates. Clempner and Poznyak (2017b) proposed a solution to the transfer pricing problem from the point of view of the Nash bargaining game theory approach (Trejo and Clempner 2018). The agreement reached by the divisions in the game is the most preferred alternative within the set of feasible outcomes, which produces a profit-maximizing allocation of the transfer price between divisions. Although many works have been devoted to resolve the problem, no completely satisfactory solution is known in the absence of a model able of combining transfer prices cooperative and non-cooperative assuring economically solutions maintaining divisional autonomy. The transfer price is the internal price of products created within the company. One of the main functions of transfer prices is the coordination of the management of both the selling and the buying divisions. Transfer prices are of fundamental importance in MNEs having a centralized structure that pursue for decentralization. The main results of this paper are as follows: We present a multiplayer model for transfer pricing that is able to maintain divisional decentralization by computing the Nash equilibrium and at the same time encourage divisions to achieve central management optimal results by computing the strong Nash equilibrium. The model is oriented for MNEs which, having a centralized structure, pursues for decentralization but minimizing the consequences of the market forces. The model is able to provide a measure of the divisional profit maximization of the MNE. We also consider the supply chain as a bi-level structure and solve the problem employing a Stackelberg game approach. We represent the transfer pricing game problem in terms of a coupled nonlinear programming equations implementing the Penalty approach. For ensuring the convergence of the utility-functions to a Strong Nash/Nash equilibrium point we introduce a regularization method for the Penalty approach based on the Tikhonov regularization theory. For computing the equilibrium point we employ the minimization of the Euclidean distance method. We transform the regularized Penalty problem into a new system of equations in the Euclidean distance format. For computing the Euclidian distance we employ a gradient method approach. A numerical example validates the effectiveness and usefulness of the proposed model for transfer pricing. The remainder of this paper is presented in five sections. The next Section deals with the transfer pricing problem presenting the formulation of the problem. Section 3 presents the game theory model presenting the strong Nash/Nash game, the Stackelberg game and the penalty optimization method for ensuring the existence and uniqueness of equilibrium points. Section 4 discusses our ad-hoc solution approach presenting the game theory solver including the Euclidean distance approach and the gradient method. A numerical example that validates the effectiveness and usefulness of the proposed model for Nash and Stackelberg is suggested in Sect. 5. Section 6 closes the paper with some concluding remarks and directions for future research.",
54.0,2.0,Computational Economics,26 July 2018,https://link.springer.com/article/10.1007/s10614-018-9838-1,Do Energy and Banking CDS Sector Spreads Reflect Financial Risks and Economic Policy Uncertainty? A Time-Scale Decomposition Approach,August 2019,Nader Naifar,Shawkat Hammoudeh,Aviral Kumar Tiwari,Male,Male,Unknown,Male,"The 2008–2009 global financial crisis, which started in the U.S. subprime mortgage market and resulted in banking defaults, has caused a severe damage to the energy and financial sectors. It has caused a negative impact on the energy sector since it triggered a sharp decline in prices of oil & gas which fuel the world’s economies. This energy sector (mainly oil & gas) is among the most important commodity-producing sectors that contributes to growth of the global economy. The financial sectors are crucial to the functioning of the economy at both the micro and macro levels. Given the high volatility of energy prices and the sharp widening of financial credit spreads affecting credit flows, the financial institutions, non-financial companies and other asset managers have opted to protect themselves from credit risks by buying credit default swap (CDS) contacts. The CDS is the most common type of credit derivatives that offers protection against default and other credit events. The market price of a CDS (called a CDS premium or a spread) reflects the risk of the underlying credit asset. The financial sector CDS spreads reflect the credit risk of the financial sectors, while the sector CDS of oil and oil-related sectors gauge expected credit risks, fear, greed and the future economic health of the petroleum industry (Hammoudeh et al. 2013a). A widening of a sector CDS spread in response to a credit event or a certain oil price shock indicates an increase in the level of credit risk in the relevant sectors, while a narrowing in the spread reveals a decrease in the credit risk in such sectors. On the other hand, economic policy uncertainty and financial risk should in theory affect the economy by increasing the risk that investors perceive they will face when making economic decisions. Economic policy uncertainty is closely related to unexpected changes that may affect economic policy. Credit risk indices such as VIX (the Chicago Board Options Exchange Volatility Index for the U.S. Stock market) and MOVE (Merrill Lynch Option Volatility Estimate) gauge market risks in the form of expected volatility in the equity and bond markets, respectively. These risk factors affect sector CDS markets and then impact CDS spread dynamics. In contrast, oil is the most volatile globally traded commodities. Its price dropped from $147/barrel in July 2008 to $32/barrel in March 2009 (Hammoudeh et al. 2013b). From 2010 until mid-2014, oil prices had been fairly stable and hovered around $100 a barrel. In January 2016, the U.S. oil prices plunged below $27 dollars a barrel for the first time since 2003, with traders also worried that the crude supply glut could last longer.Footnote 1 The financial CDS premiums spiked in the wake of the 2008 global financial crisis that wreaked havoc on banks, investment services companies and insurers. The damage had spread to other sectors of the economy including newsprint making, transportation, etc. As CDS contracts are generally used to hedge adverse credit events, it will also be interesting to examine how energy CDS indices are interconnected with expected risks in financial markets and economic policy uncertainty under different market conditions (normal, bullish and bearish markets) in the short-, intermediate- and long-run investment horizons for both the U.S. and Europe. The different investment horizons reflect the behavior of long and short speculators on a company’s credit quality (e.g., hedge funds, market-makers etc.) in the short run, intermediate investors (e.g., arbitrageurs that exploit the spread between a company’s CDS and its equity in certain situations) and long term investors (e.g. pension funds, banks, insurers, etc.). The alarming widening of the CDS spreads in the credit markets and the collapse in oil prices during the global financial crisis, the European sovereign debt crisis and the elevation in economic uncertainty have motivated us to investigate the dynamic co-movement between banking and energy CDS sector indices and economic and financial uncertainty and risk factors for the U.S. and Europe. The main purpose of this paper is to investigate how major risk indices in financial (equity and bond) markets and economic sectors shape the U.S. and European financial and energy-related CDS spreads’ distributions. The financial risk and uncertainty indices include the stock market uncertainty as captured by the CBOE volatility index (VIX), the global bond market risk as arrested by the Merrill Lynch option volatility estimate (Move) index and the economic policy uncertainty as defined by the US economic policy uncertainty (EPU) index. We have chosen the United States and Europe because they are the incubators of the most recent global crises, and the financial and energy sectors since the crises started in the financial sector and then hit hard both the financial and energy sectors. More precisely, we address the following research questions: Does dependence exist between the energy and financial related CDS spreads and the global financial and economic uncertainty factors in the United States and Europe? Does the dependence structure change across time-scales and region in the United States and Europe? Is dependence symmetric or asymmetric and does it evolve across quantiles and frequency levels? Are the financial CDS spreads more affected by the uncertainty factors than by the energy CDS spreads or vice versa and whether the effect of the uncertainty factors and the energy CDS spreads different on financial CDS spreads of US and Europe? Do uncertainty factors shape differently the US and Europe CDS spreads? Can the energy and financial CDS spreads reflect financial and economic uncertainties factors and provide international investors with more alternatives for hedging purposes? The study is conducted firstly by the standard quantile regression analysis (QRA) that allows one to investigate the dependence dynamics of the sector CDS spreads under different market circumstances in the United States and Europe, including states of downturns or bearish markets (lower quantiles), upturns or bullish markets (upper quantiles) and normalcy (intermediate quantiles). Secondly, we combine the quantile regression with the wavelet decompositions (QRWD) as this methodology allows one to deepen the analysis of the strength of the dependency dynamics among the U.S. and European energy and financial CDS spreads and the uncertainty and risk factors in terms of different time-scales or investment horizons. The advantage of combining both quantile regression and wavelet decomposition approaches in one methodology is to investigate the sensitivity of the asymmetric tail dependence under both the extreme market conditions and the time-scale domain. The wavelet decomposition allows one to revisit the level and the structure of dependence among the decomposed series on a scale by scale basis. Furthermore, this approach allows analyzing non-stationary data and localization in time. In reality, the combined methodology allows us to examine the behavior of speculators, hedgers, arbitrageurs and long-term investors in different market conditions of the sector CDS spreads. This, as indicated earlier, has significantly enriched and deepened the analysis and the empirical results by combining two relatively new approaches in one methodology. Using the daily data from December 14, 2007 to July 31, 2015, our empirical results show the following. (i) There is dependence and co-movement between the U.S. and European financial and energy-related sector CDS spreads and the VIX index at all time-scales. (ii) There is also dependence and co-movement between the U.S. and European financial and energy CDS spreads and the economic policy uncertainty in the intermediate and lower frequency domain or long investment horizons (i.e., higher time scales) but independence in the higher frequency domain or short term horizons (i.e., lower time scale). (iii) There is also dependence and co-movement between the MOVE index and the European financial CDS spreads at all time-scales but with the U.S. financial CDS spreads at the intermediate and lower frequency domains. This study differs from and adds to the related literature on CDS markets in three important ways. First, we investigate the U.S. and European financial and energy sector CDS dynamics with major global financial and economic policy uncertainty and risk measures across different distribution quantiles. Second, we compare the sensitivity of the U.S. and the European CDS sector spreads to global economic and financial uncertainty and risk factors. Third, we deepen and enrich the investigation of the strength of the co-movements between those U.S. and European financial and energy CDS spreads and uncertainty and risk factors in terms of specific time scales or investment horizons. Finally, we compare the results between the U.S. and European CDS markets under different market conditions and diverse investment horizons. The remainder of the study is organized as follows: Sect. 2 reviews the related literature. Section 3 provides the QRA and wavelet coherence analyses. Section 4 presents the data and preliminary statistics. Section 5 discusses the empirical results. Section 6 concludes and presents policy implications.",2
54.0,2.0,Computational Economics,14 August 2018,https://link.springer.com/article/10.1007/s10614-018-9841-6,Computational Approach for the Firm’s Cost Minimization Problem Using the Selective Infimal Convolution Operator,August 2019,L. Bayón,P. Fortuny Ayuso,M. M. Ruiz,Unknown,Unknown,Unknown,Unknown,,
54.0,2.0,Computational Economics,17 August 2018,https://link.springer.com/article/10.1007/s10614-018-9840-7,How Unemployment Affects Bond Prices: A Mixed Frequency Google Nowcasting Approach,August 2019,Thomas Dimpfl,Tobias Langen,,Male,Male,Unknown,Male,"Government bonds are by no means risk-free investments. In the aftermath of the financial crisis investors demanded considerable compensation for taking the risk of owning government bonds from nearly insolvent countries like Greece, Spain or Portugal. Economic theory suggests that a combination of a high level of government debt and low tax revenue due to high unemployment rates may increase the default risk of a country. Still, government bonds are affected by monetary policy which could halt or even reverse the effect. Earlier studies on the relationship between government debt and unemployment rates did not find a clear-cut pattern of interdependence. Therefore, the question whether credit markets have a disciplining effect on government spending in a setting of imperfect information (as put forward by Jaffee and Russell 1976; Stiglitz and Weiss 1981) is not satisfactorily answered. Empirical studies are hampered by the fact that unemployment rates and government bond yields are observed on different frequencies: Bonds are continuously traded while unemployment rates are only announced once a month. The general solution in the empirical literature is to align the data on a monthly frequency. Decreasing the frequency of the bond data, however, entails a substantial loss of information. In this study, we propose to lift unemployment to a higher frequency instead of aggregating bond yields to a lower one. More precisely, we examine the relationship between unemployment and government bond yields on a weekly basis, i.e. on a frequency that is higher than the publishing frequency of unemployment figures. To this end, we describe a generally applicable method to increase the resolution of a lower frequency time series when additional, closely related high-frequency data are available. This method implies a nowcasting of the low-frequency time series on a higher frequency. We refer to our model as M-HAR model as it combines elements from the literature on mixed-frequency nowcasting (M; see, inter alia, Marcellino and Schumacher 2010) and heterogeneous autoregressions (HAR; in particular following Corsi 2009). In our application, we use Google search query data to nowcast unemployment data on a weekly basis. Google search queries have been used successfully in the economics literature to capture sentiment and the behavior of market participants (see, inter alia, Bank et al. 2011; Da et al. 2011, 2015; Dimpfl and Jank 2016) and there are also some recent applications that draw on google data for nowcasting purposes (e.g. Barreira et al. 2013; Sekhavat 2016; Coble and Pincheira 2017). Becoming unemployed has severe implications for the individual and the obvious way to deal with such a situation is to collect information. An individual’s behavior that is indicative of becoming unemployed in the future can be observed far ahead of the date where she actually becomes officially unemployed. We exploit such information and show that including Google search query data greatly increases the nowcasting accuracy compared to a pure autoregressive model. The resulting weekly unemployment time series is subsequently used in a heterogeneous vector-autoregression (HVAR) of unemployment changes and bond yields. We consider eight European countries, the United States of America, and Australia. For seven out of eight European countries in our sample we consistently find that bond yields react positively to a rise in unemployment while for the United States and Australia this effect is negative. In contrast, there is virtually no impact of shocks in bond prices on unemployment. To illustrate the informational gains of our methodology we also estimate a VAR model on a monthly basis. We find that results based on our methodology provide a better fit with timing considerations of unemployment shocks. In general, the higher frequency (and thus more data points) allows for a more precise estimation and inference. Currently, the literature that directly analyzes the relationship between unemployment and bond prices is scarce. A notable exception is Bayoumi et al. (1995) who find that debt financing costs rise by nine basis points if unemployment rises by one percentage point. Still, there is quite a number of event studies that investigate the immediate impact of the announcement of unemployment figures on government bond rates. Balduzzi et al. (2001) find that short-term (three-month T-bill, two-year note) and long-term bonds (10-year note) in the US react positively to a surprise rise in jobless claims. Similarly, Fleming and Remolona (1999) report that employment announcements are the macroeconomic announcements that have the greatest effect on bond prices. In line with these findings, Afonso et al. (2011) document that the structural level of unemployment has a negative long-run impact on the credit rating of a country which may ultimately lead to rising refinancing costs, i.e. higher bond prices. A possible explanation for why this kind of analysis is rare is the aforementioned data issue. In this paper, we exploit both the advances in the literature on modeling long-memory components of a time series (cp. Corsi 2009; Corsi and Renò 2012) and the literature on mixed-frequency nowcasting (cp. Ghysels et al. 2004; Marcellino and Schumacher 2010). By combining these methodologies we can conduct an analysis of the relationship between unemployment and government bond yields at a higher frequency (i.e. weekly) than previous studies. Changes in unemployment are preceded by changes in relevant search activity several months earlier. Using the information available in search activity data significantly increases the nowcasting accuracy of unemployment for each of the ten considered countries. The reaction of government bond yields to a rise in unemployment is country-specific. In Europe, government bond yields increase in reaction to a shock in unemployment. This result is in line with economic theory which suggests that countries with higher unemployment face increased insolvency risk. Government bond yields of Australia and the United States decrease in reaction to a shock in unemployment. This might be explained by a policy of more aggressive monetary interventions, as pursued in particular by the US federal reserve system. The remainder of this paper is structured as follows. Section 2 introduces the M-HAR method to nowcast unemployment and presents the heterogeneous VAR that is used to investigate the relationship of bonds and unemployment. Section 3 describes the data and extensively discusses the intricacies of working with Google’s search volume data. Section 4 presents the empirical results and Sect. 5 concludes.",2
54.0,2.0,Computational Economics,18 August 2018,https://link.springer.com/article/10.1007/s10614-018-9844-3,How Many Agents are Rational in China’s Economy? Evidence from a Heterogeneous Agent-Based New Keynesian Model,August 2019,Wei Zhao,Yi Lu,Genfu Feng,,,Unknown,Mix,,
54.0,2.0,Computational Economics,20 August 2018,https://link.springer.com/article/10.1007/s10614-018-9845-2,Computing the Substantial-Gain–Loss-Ratio,August 2019,Jan Voelzke,Sebastian Mentemeier,,Male,Male,Unknown,Male,"The Substantial-Gain–Loss-Ratio (SGLR) was derived to overcome certain theoretical and practical problems of the Gain–Loss-Ratio (GLR) as in Bernardo and Ledoit (2000). The GLR is an attractiveness measure that can evaluate arbitrary uncertain payoffs, e.g. created by a risky asset. The evaluation is based on a stochastic discount factor (SDF) that models a market representing or a particular investor. It can either be used as a performance measure or for the computation of so-called Good Deal bounds. As a performance measure the GLR fulfils all properties of an acceptability index and has an intuitive interpretation as it is the ratio of (discounted) gains and losses. The second application—Good Deal bounds—denotes price intervals which consist of all prices implying an attractiveness below a certain bound, based on a given attractiveness measure. Using the GLR results in an interval which contains the unique price implied by the SDF and is included in the interval of no arbitrage prices. By changing the attractiveness limit from one to infinity, one can continuously fade from one to the other, making the concept an unification of model-based pricing and no arbitrage pricing. Given a theoretical model and a suitable attractiveness limit, the best GLR on a market can be calculated. However, this best GLR is infinity in many standard models, as is proven and discussed in Biagini and Pinar (2013). The SGLR has all the properties of an acceptability index as well.Footnote 1 However, it does not show the abovementioned problems of the GLR. Additionally, calculations of the SGLR for different values of its parameter \(\beta \), which can be plotted as the so-called \(\beta \)-diagram, allow to investigate the distribution of performance. In particular, it provides an insight into the amount of (over)performance which can be attributed solely to extreme events. Concerning the calculation of the SGLR, Voelzke (2015) only considers the trivial case of a risk-neutral benchmark investor, i.e. an investor whose preferences can be described by a constant stochastic discount factor (SDF). Additional empirical considerations are left out entirely. Unfortunately, calculating the SGLR of an asset for arbitrary benchmark stochastic discount factors is not straightforward and generally means solving a nonlinear optimization problem with several non-linear constraints. However, in this paper we discuss an algorithm that allows to calculate the SGLR, using historic data. Therefore we focus on assets and SDFs the probability law of which is given by empirical distribution functions. Nevertheless, we further derive upper bounds for the SGLR of assets and SDFs with certain continuous distributions. The paper proceeds as follows. Section 2 provides the necessary definitions and theoretical statements. Section 3 introduces the algorithm and explains its usage either for empirical applications or for the determination of an upper bound in specific theoretical models. Section 4 discusses two empirical applications and Sect. 5 concludes. Longer proofs are deferred to the “Appendix”.",1
54.0,2.0,Computational Economics,21 August 2018,https://link.springer.com/article/10.1007/s10614-018-9843-4,Bayesian Estimation of Beta-type Distribution Parameters Based on Grouped Data,August 2019,Kazuhiko Kakamu,Haruhisa Nishino,,Male,Unknown,Unknown,Male,"In seminal works estimating hypothetical income distributions, a number of alternative probability density functions (PDFs) have been proposed as models of the income distribution, as shown by Salem and Mount (1974). These PDFs provide a reasonably close approximation to the true distribution and their parameters should be sufficiently simple to interpret in an economically meaningful way. For example, the Pareto and lognormal distributions have parameters that can be directly related to inequality measures, but neither fits the full range of income data very well. Therefore, alternative flexible PDFs such as the double Pareto-lognormal distribution proposed by Reed and Jorgensen (2004) and \(\kappa \)-generalized distribution of Clementi et al. (2007, 2016) have been considered. A family of beta-type size distributions can also be successfully fitted to income data. For example, the generalized beta (GB) distributions of the first and second kinds (GB1 and GB2 distributions, respectively) proposed by McDonald (1984), Dagum (DA) distribution proposed by Dagum (1977), Singh–Maddala distribution proposed by Singh and Maddala (1976), gamma distribution, chi-square distribution, and exponential distribution (see Fig. 2 in McDonald and Xu 1995, for details on the relationship of the distributions) have been considered to be hypothetical distributions in the family of beta-type size distributions. In particular, the successful empirical estimation of the GB2 distribution was complemented by Parker’s (1999) theoretical models of income generation and Bordley et al. (1997) found that the GB2 distribution generally provides a significantly better fit than its nested distributions when fit to income data from the United States. However, although such flexible distributions including the GB2 distribution provide us with an estimation approach because of computational progress over time, we must also consider the smallest possible number of parameters to ensure an adequate and meaningful representation (see Dagum 1977). Therefore, several hypothetical distributions have been compared in studies such as Atoda et al. (1988), Bordley et al. (1997), Kloek and van Dijk (1978), McDonald and Mantrala (1995), McDonald and Ransom (1979b), Majumder and Chakravarty (1990), Salem and Mount (1974), Slottje (1984), Tachibanaki et al. (1997) and recent studies support the fit of the GB2 distribution. As a more flexible distribution, McDonald and Xu (1995) proposed the GB distribution, which should perform at least as well as other beta-type distributions because it includes the GB2 distribution as a special case. However, although McDonald and Xu (1995), Bordley et al. (1997), McDonald and Ransom (2008) empirically examined the GB distribution, the success of its estimation has not thus far been reported to our knowledge. Grouped data has been widely used to estimate income distributions because they are available in many countries (see Kleiber and Kotz 2003, “Appendix B”).Footnote 1 In grouped data, suppose that the income units are grouped into \((k+1)\) income classes, viz., (\(x_{0}\) to \(x_{1}\)), (\(x_{1}\) to \(x_{2}\)), ..., (\(x_{k}\) to \(x_{k+1}\)) with \(x_{0} = 0\) and \(x_{k+1} \le \infty \): let n be the total number of units and \(n_{i}^{*} = n_{i} - n_{i - 1}\) with \(n_{0} = 0\) and \(n_{k+1} = n\) be the amount of earning income in the intervals \(x_{i-1}\) and \(x_{i}\), which varies from 1 to \(k+1\). In this situation, the estimation of the hypothetical distribution parameters explores the distribution (i.e., the dotted line), which is fitted to the data, from the histogram shown in Fig. 1. Quintile data Although maximum likelihood estimation (MLE) is one of the most common methods of estimating the parameters of hypothetical distributions from grouped data (see McDonald and Ransom 2008), several other estimation methods have been used, including the minimum chi-square, scoring, least squares, method of moments, least lines, generalized method of moments, generalized least squares, and Markov chain Monte Carlo (MCMC) methods (see e.g. Chotikapanich and Griffiths 2000; Chotikapanich et al. 2007; McDonald and Ransom 1979a, b; Nishino and Kakamu 2011; van Dijk and Kloek 1980). Moreover, the GB distribution has already been estimated using MLE by McDonald and Xu (1995), and Bordley et al. (1997) applied the method to the U.S. income data. However, the MCMC method is a powerful tool not only for the estimation but also for the inference as an alternative to MLE. As shown by Gelfand et al. (1990), the MCMC method can yield valid inferences on the nonlinear functions of the parameters, such as Gini coefficients. For example, the credible intervals are obtained not only for the parameters but also for the Gini coefficients. Moreover, as stated by McDonald and Ransom (1979b) and Kakamu (2016), it is sometimes difficult to find the mode of the parameters as their number increases by using MLE. Therefore, it would be worthwhile taking a Bayesian approach to reexamine the parameter estimation of the GB distribution. This study considers an efficient Bayesian estimation procedure of the GB distribution by using the tailored randomized block Metropolis–Hastings (TaRBMH) algorithm,Footnote 2 which was proposed by Chib and Ramamurthy (2010) over the existing procedure, namely the random walk Metropolis–Hastings (RWMH) algorithm, to focus on the efficiency of the algorithm and accuracy of the Bayesian estimates. Moreover, by using the proposed algorithm, the goodness-of-fit of the GB distribution is explored, using a simulated dataset and two real datasets from the United States and Japan. First, from the simulated dataset, we confirm that this procedure can estimate the parameters more efficiently than the existing method in terms of mixing and that the parameters are estimated precisely from the grouped data. When we compare the fit of the distributions, the GB distribution is preferred to the DA distribution. However, the fit of the GB2 distribution is almost equivalent to that of the GB distribution. In other words, the use of the GB2 distribution is preferred to the GB distribution in terms of the smallest possible number of parameters, even if the parameters of the GB distribution are estimated accurately. Second, by using the two real datasets, the empirical results of the U.S. data show that the GB2 distributions are preferred to the GB distribution, although the maximum likelihood estimates are slightly different from the Bayesian estimates. We should notice that the Baysian estimates of c of the GB distribution in Table 3 are sometimes substantially different from 1, and it suggests that the GB distribution could be more suitable for the data than the GB2 because in the case of \(c=1\) the GB distribution is reduced to the GB2 distribution. The empirical results of the Japanese data also confirm that the GB2 distributions are preferred to the GB distribution, although the GB distributions are preferred to the DA distribution. Moreover, the accuracy of the Gini coefficients also suggests the use of the GB2 distribution. The rest of this paper is organized as follows. In the next section, we introduce the features of the GB distribution, including the PDF, cumulative distribution function (CDF), and likelihood function, and explain the MCMC estimation procedure for this distribution. In Sect. 3, we examine the numerical example of the simulated dataset focusing on the efficiency of the MCMC draws and accuracy of the Bayesian estimates. Moreover, the goodness-of-fit of the GB distribution is examined. In Sect. 4, three real datasets that include income data from the United States and Japan are examined to confirm the results in Sect. 3 from an empirical point of view. In the Japanese dataset, the performance of the Gini coefficients is also discussed. In Sect. 5, we conclude the discussion and state the remaining issues.",5
54.0,2.0,Computational Economics,25 August 2018,https://link.springer.com/article/10.1007/s10614-018-9842-5,Customer Satisfaction Prediction in the Shipping Industry with Hybrid Meta-heuristic Approaches,August 2019,Stelios Bekiros,Nikolaos Loukeris,Frank Bezzina,Male,Male,Male,Male,"The robust and reliable measurement of customer satisfaction is of vital importance for firm policy and decision making. In transportation science and especially in the shipping industry where competition is extremely intense, the utilized evaluation tools so far present vast difficulties and uncertainties. The prediction of passengers’ total satisfaction realized a priori could lead to resource saving and eventually to maximization of firms earnings. The domain includes models such that of the Analytical-Synthetical Multicriteria Satisfaction Analysis, Grigoroudis and Siskos (2002), or via a hybrid methodology combining Multicriteria Analysis and Data Mining by Matsatsinis et al. (1999). Other methods also exist mainly developed as in-house applications in large maritime companies, yet with limited success. The radical evolution of information technology and artificial intelligence provides with a wide variety of methods to solve these particular operational industry problems. While decision makers are really interested in the results of their approaches, however they do not efficiently deal with the theoretical aspects and assumptions of their practices. In this direction we move forth toward a comparative evaluation of four different methods widely used in many industries, but only in a limited degree in shipping. Those approaches range from Data Mining and Rough Sets, to Artificial Intelligence and Multicriteria Analysis. In accordance with Matsatsinis (2002) and Matsatsinis et al. (1999), Loukeris et al. (2009, 2017, 2018), we implement an original methodological approach of consumer evaluation, combining multicriteria preference, disaggregating analysis and rule induction data mining. Specifically, Matsatsinis et al. (1999) examined total customer satisfaction and its prediction via the replacement of missing data by Multicriteria estimations and Data Mining as well as through the application of Multicriteria Satisfaction Analysis (MUSA) wherein aggregate customers satisfaction is derived by decomposing individual criteria satisfaction. The original methodology presented in this paper combines preference-disaggregation with rule-induction fuzzy-based data mining techniques. The aggregation of individual preferences into a collective value function is the main objective of our multicriteria approach. More specifically, it is assumed that consumers’ selection of a product is rationalized by a set of criteria or variables representing their characteristic dimensions. The preference disaggregation methodology is an ordinal regression-based approach introduced by Jacquet-Lagreze and Siskos (1982) and Siskos and Yannacopoulos (1985) as a potential solution to the problem of “no response” by the data set due to insufficiently completed questionnaires. Moreover, this methodology develops rules that relate global utilities to functions of marginal utilities which compose it, and then makes a prediction upon the value of global utilities based on the revealed rules. Grigoroudis et al. (2001) received measurements of customers’ satisfaction via questionnaires in coastal shipping and utilized a mathematical approach of analytical-synthetical preference modeling where the composition of multiple criteria leads to a decision; the aforementioned analysis also detects the specific criteria which determine the implemented decision. Also, Sarawagi and Nagaralu (2000) considered that data mining may be applied to internet services, extracting composed preferences off of the total number of clients having access to those services. In our work, we evaluate comparatively hybrid methods of Artificial Intelligence and Multicriteria Decision Analysis in order to predict efficiently and robustly passenger satisfaction in maritime industry, based on the methodologies evolved by Grigoroudis and Siskos (2002), Slowinski (1992), Slowinski and Stefanowski (1993) and Slowinski and Zopounidis (1995). We find that hybrid methods prove to be quite promising in delivering satisfactory results. We used a vast structure of questionnaires regarding passenger satisfaction evaluated by the customers of the largest Greek shipping enterprise. We comparatively evaluated enhanced artificial intelligence methods such as Neural Networks and their Neuro-Genetic Hybrids: the Multilayer Perceptrons (Loukeris and Eleftheriadis 2015a), the Support Vector Machines (Loukeris and Eleftheriadis 2015a, 2016), the well-known “Wiz Why” as a data-mining approach, the ROSETTA method based on the theory and implementation of Rough sets, various topologies of Neural Networks and the Multi-Group Hierarchical Discrimination (M.H.DIS) technique. We enforced the AI aspect adding a significant variety of influential ANN models: (a) 51 models of MultiLayer Perceptrons, out of which 21 were of NN form and the other 30 of Hybrid MLP-GA form of various topologies from 1 to 10 layers (b) the Voted Perceptron, (c) 18 Support Vector Machines out of which 3 in plain and 15 in hybrid SVM-GA form in 2 different learning processes: the batch learning and the on-line. This meta-analysis will define the optimal prediction models in shipping. Section 2 provides a brief overview of each methodology, while the next section presents a preliminary analysis of the datasets, the architectural characteristics for the specific application as well as the calibration of each operational system. Section 4 includes a comparative evaluation of model forecastability vis-à-vis passenger satisfaction. Finally, Sect. 5 concludes.",4
54.0,2.0,Computational Economics,10 September 2018,https://link.springer.com/article/10.1007/s10614-018-9851-4,Technical Trading Behaviour: Evidence from Chinese Rebar Futures Market,August 2019,Guanqing Liu,,,Unknown,Unknown,Unknown,Unknown,,
54.0,2.0,Computational Economics,10 September 2018,https://link.springer.com/article/10.1007/s10614-018-9846-1,Option Implied Risk-Neutral Density Estimation: A Robust and Flexible Method,August 2019,Arindam Kundu,Sumit Kumar,Nutan Kumar Tomar,Unknown,Male,Female,Mix,,
54.0,2.0,Computational Economics,11 September 2018,https://link.springer.com/article/10.1007/s10614-018-9849-y,A Continuous Differentiable Wavelet Shrinkage Function for Economic Data Denoising,August 2019,Fan He,Xuansen He,,,Unknown,Unknown,Mix,,
54.0,2.0,Computational Economics,12 September 2018,https://link.springer.com/article/10.1007/s10614-018-9852-3,Approximating the Solution of Stochastic Optimal Control Problems and the Merton’s Portfolio Selection Model,August 2019,Behzad Kafash,,,Male,Unknown,Unknown,Male,"Optimal control is a main part of control theory, which has been widely studied over the past several decades. Recently, Kushner presented a survey of the early development of selected areas in non-linear continuous-time stochastic optimal control (Kushner 2014). As is well known, Bellman’s dynamic programming and Pontryagin’s maximum principle are the two most commonly used approaches and principal in solving deterministic and stochastic optimal control problems. In the statement of a Pontryagin maximum principle there is an adjoint equation, which is a stochastic differential equation in the stochastic case and an ordinary differential equation in the deterministic case. The system consisting of the adjoint equation, the original state equation, and the maximum condition is referred to as a Hamiltonian system. On the other hand, in Bellman’s dynamic programming, there is a partial differential equation, of first order in the deterministic case and of second order in the stochastic case, namely the Hamilton–Jacobi–Bellman (HJB) equation. The solution of the HJB equation is the value function which leads the maximum cost for a given dynamical system with an associated cost function. The HJB equation is a necessary and sufficient condition for an optimum and also permits the solution of the closed loop problem (Bellman 1957; Fleming and Rishel 1975; Fleming and Soner 2006; Yeung and Petrosyan 2005). In general, the HJB equation has no analytical solution, thus finding a numerical solution is at least the most logical way to treat them. The study of numerical methods has provided an attractive field for researchers of mathematical sciences, which has given rise to the appearance of different numerical computational methods and efficient algorithms in solving optimal control problems (Lin et al. 2014; Jaddu 2002; El-Kady and Elbarbary 2002; Kafash et al. 2012a, b, 2014; Jajarmi et al. 2012). Kafash et al. presented numerical methods to solve deterministic optimal control problems (Kafash et al. 2013) and later, extended their ideas to solve stochastic optimal control problems (Kafash et al. 2016), in which authors used the variational iteration method (VIM) for optimal control problems. In Kafash and Nadizadeh (2017), the authors solved stochastic optimal control problems through the method of separation of variables by guessing a solution via terminal condition of HJB equation. In Saberi et al. (2012), the authors used homotopy perturbation method to solve the HJB equations. Huanga et al. (2006) proposed a numerical algorithm based on a variational iterative approximation for the HJB equation, and a domain decomposition technique based on this algorithm is also studied. In Matsuno (2015), a stochastic optimal control method is developed for determining three-dimensional conflict-free aircraft trajectories under wind uncertainty. Wu and Wang (2015) studded the optimal control problem of backward stochastic differential delay equation under partial information. In Marti and Stein (2015), the authors presented two approaches to find approximating solutions of stochastic optimal control problem with two independent approximation stages. The authors in Ni et al. (2015), first presented results on the equivalence of several notions of \(L^2\)-stability for linear mean-field stochastic difference equations with random initial value. Then, shown that the optimal control of a mean-field linear-quadratic optimal control with an infinite time horizon uniquely exists, and the optimal control can be expressed as a linear state feedback involving the state and its mean, via the minimal non-negative definite solution of two coupled algebraic Riccati equations. In Wang et al. (2015), an adaptive dynamic programming algorithm based on value iteration proposed to solve the infinite-time stochastic linear quadratic optimal control problem for the linear discrete-time systems with completely unknown system dynamics. In Kortas et al. (2015), the authors investigated potential energy savings of a Double Star Induction Motor. Ieda et al. proposed a new numerical method for solving the HJB quasi-variational inequality associated with the combined impulse and stochastic optimal control problem over a finite time horizon (Ieda 2015). In Zhu (2014), the authors studied the optimal stochastic control problem for stochastic differential equations on Riemannian manifolds. Luo et al. (2015) purposed a data-based off-policy reinforcement learning method is proposed, which learns the solution of the HJB equation and the optimal control policy from real system data. Two classical approaches in solving stochastic optimal control problems numerically are known. One is using finite difference method to solve the corresponding HJB equation. The other is to use finite state Markov chain approximation to reduce the problem to a finite-dimensional problem, which can be solved using vector-matrix operations (Kushner and Dupuis 1992). The Markov chain approximation approach in Kushner (1990) adapted to the standard set up of continuous-time finance. In Yin et al. (2009), the authors developed an approximation procedure for portfolio selection with bounded constraints. Also, Krawczyk (2001b) examined an alternative method of deriving numerical solutions to a class of stochastic optimal control problems and later, proposed a numerical method based on a Markov chain approximation and applied it for solving Merton’s model of optimal portfolio selection (Krawczyk 2001a). Note that, Markov chain methods are essentially explicit finite difference schemes, and hence suffer from the usual time-step limitations due to stability considerations (Huang et al. 2012). Here, the numerical method based on Markov chain approximation approach in Kushner and Dupuis (1992) is applied for solving stochastic optimal control problems. Note that, the numerical partial differential equation method is a useful approach for stochastic control of fully observable systems via HJB. However, all real problems are, in fact, of the stochastic control of partially observable systems. So in the stochastic control of partially observable systems, the Markov chain approximation for solving stochastic differential equations instead of the numerical partial differential equation method for solving HJB, is preferred. The basic idea of the Markov chain approximation method is to approximate the original controlled process by an appropriate controlled Markov chain on a finite state space. Also, the cost functional is appropriate for the approximating chain. The finite difference approximations are used in the construction of locally consistent approximating Markov chain, the coefficients of the resulting discrete equation can serve as the desired transition probabilities and interpolation interval. The convergence is analogous to the convergence of a sequence of finite difference or finite element approximations to an original problem. Finally, we propose a backward iteration technique for solving stochastic optimal control and efficiency of the proposed algorithm is illustrated by Merton’s portfolio selection model. This paper is organized into following sections of which this introduction is the first. In Sect. 2, we introduce stochastic optimal control and present the HJB equation. Section 3 derives the method. A special case to illustrate the efficiency and reliability of the presented method are described in Sect. 4. Finally, the paper is concluded with the conclusion.",3
54.0,2.0,Computational Economics,15 September 2018,https://link.springer.com/article/10.1007/s10614-018-9850-5,Exploring House Price Dynamics: An Agent-Based Simulation with Behavioral Heterogeneity,August 2019,Tolga A. Ozbakan,Serdar Kale,Irem Dikmen,Male,Male,Unknown,Male,"This study rests on a debate that has been developing over the last four decades about the nature of housing prices. On the two sides of this debate are neoclassical economists and behavioral economists who have different takes on the determinants of house prices as well as on the interpretation of price changes. The former perspective maintains that housing markets are efficient mechanisms in which participants make rational pricing decisions by utilizing all available information about market fundamentals. Accordingly, the perspective argues that prices reflect the fundamental value of the housing units, i.e., there is no long-term mispricing. On the other hand, the behavioral perspective raises questions about the efficiency of housing markets as well as the rationality of their participants and argues that house prices may deviate from their fundamental values. In agreement with a growing body of literature, our study echoes the view that incorporating insights from behavioral economics can enhance the extant neoclassical housing market models. To support this statement, we use a well-known neoclassical real estate market model (Wheaton 1999) as a benchmark and extend it to experiment with behavioral heterogeneity among market participants. Specifically, we aim to answer the following two interrelated questions: To what extent does the inclusion of (1) behavioral heterogeneity and (2) dynamic market behavior enhance the benchmark model? To meet this aim, we utilize an analytically tractable class of agent-based models known as Heterogenous Agent Models (HAMs). While HAMs were originally conceptualized for financial markets (See Hommes 2006 for a very in-depth review), recently they have been applied to real estate markets as well. For example, Burnside et al. (2011) proposed a model in which housing market participants have heterogeneous beliefs about fundamentals, i.e., some agents believe that these fundamentals will improve while others do not. Furthermore, although agents can update their prior beliefs in a Bayesian fashion, they have limited access to data. Another key feature of the model is an element that the authors refer to as social dynamics, i.e., agents meet randomly with each other and some of them change their prior beliefs about long-run fundamentals after these meetings. These features of the model generate a market in which home sales and prices display a sharp positive correlation. Dieci and Westerhoff (2012) published a HAM which attempted to account for the “forces of human psychology that drive international financial markets” in the context of housing markets. Notably, some agents in the model believe that housing prices will revert to a long-run fundamental steady state, while others maintain the view that the recent trends will continue. Furthermore, both groups of agents dynamically switch their beliefs based on market sentiment. Specifically, these behavioral changes in agent strategies occur according to how far housing prices deviate from a long-run fundamental steady state. Regarding the generated outcome, the authors report cyclical or even chaotic price dynamics under various market conditions. A study by Bijman (2012) included a model with similar characteristics. The model modifies Wheaton’s (1999) study to include heterogeneous expectations and a natural vacancy rate. Like other HAMs, the agents in the model switch behaviors dynamically, but this time between rational and naive expectations. The generated price dynamics include quasi-periodic and chaotic behavior under specific parameter settings. Bolt et al. (2013) published a housing market model with heterogeneous expectations that links rental levels to prices via imputed rents. The results display nonlinear aggregate price fluctuations around the fundamental value with two non-fundamental equilibrium prices on either side of the fundamental price. Using quarterly data on rents and house prices, the authors estimate the model parameters for five different countries, namely the US, the UK, the Netherlands, Japan, and Switzerland. The study concludes that the data supports heterogeneity in expectations, with temporary switching between trend extrapolation and mean-reversion beliefs. Eichholtz et al. (2015) published an article with interacting heterogeneous agents with a focus on whether a fundamental factor or a trend explains house prices in the Netherlands. The authors find that agents in the housing market switch their expectations about future changes in house prices between fundamental and momentum strategies. Furthermore, the results of the study imply that neither fundamental variables nor recent trends explain housing price dynamics alone. Instead, both factors contribute to the variation in house prices and the relative importance of these factors vary over time. More recently, Campisi et al. (2018) introduced a housing market model following Dieci and Westerhoff (2012). Their model utilizes a time lag variable on the supply side. Furthermore, it extends the heterogeneity of the original model by considering naive expectations as well as a mix between naive and rational expectations. Like its predecessors, the study suggests that competition between distinct market strategies may lead to complex price dynamics. In the remaining parts of this paper, we present a theoretical framework to explain the properties of the benchmark model (Wheaton 1999) and to propose our behavioral extensions. Next, we expound the computational representation of the extended model, followed by verification and experimentation. After validating the model in the empirical context of Istanbul’s housing market, we conclude our paper.",1
54.0,2.0,Computational Economics,19 February 2019,https://link.springer.com/article/10.1007/s10614-019-09881-3,Jump Detection and Noise Separation by a Singular Wavelet Method for Predictive Analytics of High-Frequency Data,August 2019,Yi-Ting Chen,Wan-Ni Lai,Edward W. Sun,,Unknown,Male,Mix,,
54.0,3.0,Computational Economics,17 September 2018,https://link.springer.com/article/10.1007/s10614-018-9853-2,Introduction to Network Modeling Using Exponential Random Graph Models (ERGM): Theory and an Application Using R-Project,October 2019,Johannes van der Pol,,,Male,Unknown,Unknown,Male,"Networks are representations of relational data. Nodes represent entities while the links connecting them represent any form of interaction or connection between the entities. A large diversity of networks exists ranging from networks of social contacts between individuals to inventor networks, collaboration networks, financial networks and so on. These networks attract the interest of researchers who want to explain the structure of these networks. In other words, the would like to know why certain agents are central in a network and why other are at the periphery, why some networks are densely interconnected and why others are sparse. In essence, we want to know why firms a is connected to firm b, or what it the probability that firm a will connect to firm c and how does this impact the overall structure of the network. This allows us to identify if technological proximity between firms has a significant impact on the choice of collaborator between firms and if this strategy has a significant impact on the structure of the network. The motivations for link creation cannot be observed directly from a visual representation of interactions, nor are they clear from glimpsing at a database containing relational data. In order to identify the motivations for entities to create links and identify the global network structure, a more in-depth analysis is required. Methods such as, Interpretive Structural Modeling, Total Interpretive Structural Modeling, and Graph Theory Matrix allow for an analysis of the interactions between variables but do not allow to for conclusions on the structure of the networks we aim at analysing. In other words, the methods allow for identifying a correlation between technological proximity and social proximity but do not explain the structure of the network that allows this conclusion. Econometric analysis could shed more light on the motivations behind an observed link through logistic regressions. The probability of a link could be explained by a number of variables. There is one important limitation to this method. Due to the hypothesis of independence of the observations the probability of a link between two nodes can never be explained by the presence of another link inside the network. It is feasible that a link between two nodes exists only because of the presence of other links in the network. Take for instance the idea that John and Mary are connected solely because they have a common contact: Paul. Methods such as Block-models cannot account for the impact of the structure of the network on the probability of a link. ERGM models are modified logistic regressions that allow for the probability of a link to depend upon the presence of other links inside the network (amongst other variables). ERGMs are able to take into account directed interactions as well as weighted interactions between nodes. The latter is an important point, since it allows an analysis beyond a simple binary relation between nodes approaching the idea of fuzzy logic. An ERGM identifies the probability distribution of a network so that it can generate large samples of networks. The samples are then used to infer on the odds of a particular link inside a network. Applications for this method are numerous in many fields of research as shown by the increasing trend in the number of publications using ERGM models (see Fig. 1). In economics the number of published papers appears to be relatively low when compared to the other social sciences. Only 19 published papers could be found in the Scopus database (and even less in the web of science database). The topics are however quite diverse: knowledge sharing in organizations (Caimo and Lomi 2015), alliance networks (Cranmer et al. 2012; Lomi and Pallotti 2012; Lomi and Fonti 2012; Broekel and Hartog 2013; van der Pol 2018) and geographic proximity (Ter Wal 2013). (Source: Scopus) Evolution of the number of publications involving ERG models for all disciplines (statistics included) Since ERGMs allow for hypothesis testing, they can be put into use rather quickly within existing theoretical frameworks adding the possibility to analyse relational data in addition to normal data. The growing interest, and development of a theory of economic networks, provides a fertile ground for the use of ERGM models from the geography of innovation to venture capital investments. The aim of this paper is to provide an overview of the basic statistical theory behind ERGM models, which will be dealt with in the first section. Section 3 discusses the concept of dependence and the explanatory variables that can be included in the models. Section 4 discusses estimation methods while Sect. 5 provides the R scripts and the interpretation of an example using data for the French aerospace sector alliance network. An example of a sub-graph. This particular structure is called a two-star",21
54.0,3.0,Computational Economics,21 September 2018,https://link.springer.com/article/10.1007/s10614-018-9847-0,A Reformulation-Based Simplicial Homotopy Method for Approximating Perfect Equilibria,October 2019,Yin Chen,Chuangyin Dang,,,Unknown,Unknown,Mix,,
54.0,3.0,Computational Economics,22 September 2018,https://link.springer.com/article/10.1007/s10614-018-9855-0,Individual Satisfaction and Economic Growth in an Agent-Based Economy,October 2019,João Silvestre,Tanya Araújo,Miguel St. Aubyn,,Female,Male,Mix,,
54.0,3.0,Computational Economics,22 September 2018,https://link.springer.com/article/10.1007/s10614-018-9854-1,Physician Emigration: Should they Stay or Should they Go? A Policy Analysis,October 2019,Mário Amorim-Lopes,Álvaro Almeida,Bernardo Almada-Lobo,Male,Male,Male,Male,"An adequate medical workforce is a critical requirement to an healthy population, as the delivery of health care services is still highly dependent on physicians. In general, health authorities are aware of the consequences of a disproportion in the physician workforce, and put this issue on top of their political agendas (Ono et al. 2013). Notwithstanding, shortages of physicians or a geographical maldistribution of health human resources are still very common and affect all countries alike, from low- to high-income countries (Labonté et al. 2015; Gauld and Horsburgh 2015), suggesting that, despite the frequent proclamations, no public policies have been enacted to address this issue or such policies have been ineffective in achieving their goal. Several factors originating from both the demand- and the supply-side may cause an imbalance in the medical workforce (Amorim Lopes et al. 2015). Institutional and regulatory barriers, such as numerus clausus, licenses to practice or mandatory retirement age; service delivery arrangements that may condition both the skill mix and the productivity of the health care workforce; non-controllable factors, such as mortality; changes to demography, in particular to the size or age structure of the population; migration flows; epidemiological needs; and many other factors may cause a change in the medical workforce. Of all these factors, emigration is one of the most difficult to handle, as it exerts an immediate influence on the supply of physicians. Physicians are no different from other people, and emigrate with the purpose of being better off (ex ante). The international migration literature lists a significant number of “push” and “pull” factors that may act as stimuli to leave or attract one to other countries, respectively. In the particular case of human resources for health, several authors have identified critical factors (Clark et al. 2006; Ribeiro et al. 2014), for instance: inadequate compensation, when compared to other sectoral wages, but also to wages in other countries; remuneration system not related to performance; heavy workloads; poor working conditions; political instability; or unmet demand for medical education due to numerus clausus or lack of residency positions are reasons frequently provided for wishing to move abroad. Regardless of the working and living conditions in the home country, foreign countries may also offer fringe benefits that may attract physicians. For instance, training opportunities; better compensation, in terms of wage differential; active recruitment; or higher demand for health professionals. In the case of an oversupply of medical doctors, emigration may actually act as a safety valve in clearing the health labour market and minimizing the negative welfare impact arising from unemployment. However, if there is a shortage in the medical workforce, emigration aggravates the problem further. Actually, if it happens repeatedly, even small numbers of physicians moving abroad may be responsible for large workforce variations if no action is taken to mitigate it. Monte Carlo simulations run by Amorim Lopes et al. (2016) demonstrate that small variations in the emigration rate may cause large variations in the workforce size, which may put the health system under severe pressure. Although in some cases retention policies may exhibit a limited effect on dissuading physicians from moving abroad (Hussey 2007), empirical evidence seems to support the view that some may actually work. For instance, Okeke (2014) found evidence that a wage increase programme in Ghana reduced the foreign stock of Ghanaian physicians by approximately 10%. The same conclusion was reached by Labonté et al. (2015), who also found evidence from South Africa that wage incentives seem to work. In the same line, Ortega and Peri (2009) estimated that an increase in the wage differential between origin and destination countries of 1000 US $ increases the flow of migrants by 10–11%, with the corollary being that reducing the wage differential would decrease the flow of migrants. On the opposite direction, Vujicic et al. (2004) found little correlation between the supply of health care migrants and the wage differential between source and destination countries, and suggests that wage differentials may be so large that a small salary raise would produce little to not effect. Theoretically, it is hard to argue against the role played by wages in the international migration flows. Either when the physician is modelled as a rational utility-maximizer agent that moves based on the wage differential, as it is commonly found in classic theories, or when migration is taken as part of an investment in human capital, an approach typically found in modern migration theories, the expected income is hardly negligible (Bodvarsson and Van den Berg 2013). Even if other less tangible factors, such as working or living conditions, exert a bigger influence, there is theoretical ground to assume that income, and therefore wages, is a relevant factor as well. Wage incentives, framed within a suitable incentive system, may then be an adequate way of targeting current physicians. Ono et al. (2014) points to this factor as a means to respond to imbalances in the geographical distribution of physicians, but it applies also to international migration. Overall, the authors identify three broad strategies available to policymakers: (i) targeting current physicians by devising an incentive system, mostly focused on financial incentives (a short-term strategy); (ii) targeting future physicians in order to maximize the pool of physicians available to practice, which implies increasing the number of qualified individuals (a long-term strategy, as earning a license to practice is a decade-long effort); (iii) do with less, which means accepting the current levels and optimizing the delivery of health care services to compensate for the lack of physicians. Although it is quite clear by now that policy levers exist to mitigate the emigration of physicians, it is not always the case that we can grasp the full extent of their impact. In very flexible health labour markets featuring few rigidities and institutional barriers one can use analytical tools, such as dynamic stochastic general equilibrium (DSGE) models, to study the effects of such policies, contingent upon no severe nonlinearities being present. Otherwise, the model loses tractability and it is very hard, if not impossible, to account for second-order effects, and in some cases for first-order effects also. In such cases, the predictive power of the model is simply non-existent for setting policy (Bouchaud 2008). In practice, some health labour markets can be extremely rigid, with uncertainty, asymmetric information, barriers to entry and a quasi-monopsonistic employer causing it to divert strongly from textbook Walrasian equilibrium. In fact, non-market clearing and Pareto inefficiency is the rule and not the exception. For a real-case example consider the Portuguese health system. It features many rigidities: numerus clausus exist to limit access to medical schools, since education is subsidized; residency training is also limited by the Medical Association, who decides how many vacancies to open for each medical specialty; the health labour market conveys many features of a monopsony, with the government being by far the largest employer and with restrictions to supply reducing the short-run elasticity of supply; pay for performance is used seldom, with few incentives existing to adjust physicians’ wage levels according to their productivity; wages in the public sector are very sticky, being fixed by the government through collective bargaining; wages in the public sector put an“anchor” on the salaries practiced by the private sector; the wage ceiling in the public system caps the quantity of health care services supplied; very limited “freedom of choice” in the public sector, with citizens unable to choose their family doctor; low access fees that virtually have no impact on the demand for health care services; etc. On top of this, Portugal features one of most rigid labour markets in the EU according to OECD (2013). Despite not being suitable for a traditional analysis, studying emigration in an health system so far away from theoretical competitive equilibrium models, evaluating and quantifying potential retention policies and informing policymakers accordingly is still very relevant. In the end, the apparatus should be able to answer the question of how policymakers should deal with physician emigration, although a one-size-fits-all optimal policy is unlikely to exist. For this purpose we have developed a theoretical economy that reproduces all the traits of the Portuguese health system, calibrating it with real data. Using this model we simulate the evolution of the health system and conduct policy experiments to tame emigration, tracking the impact at the micro-level – at the level of detail of a single agent. We play with three policy levers: adjusting numerus clausus, the number of job offers to the public sector and physician wages, and we observe their impact on the simulated economy, as well as first- and second-order effects. Furthermore, we estimate the impact of return migration on net migration numbers. To implement the model we resorted to agent-based computational economics (ACE), as these models go beyond Walrasian equilibrium foundations, representative agents and the assumption of market clearing (Colander et al. 2008). In a non-Walrasian health labour market, as is most likely the case of Portugal for the reasons aforementioned, an agent-based simulation model (ABM) is a very adequate tool for the problem at hand. Despite not being ubiquitous, ABMs are gaining traction, and several authors are advocating for its use (Farmer and Foley 2009). Fagiolo and Roventini (2012) provides several examples of the use of ABM in modelling economics and policy, comparing it to typical DSGE models. Neugart and Richiardi (2012) elaborates on the use of ABM to analyse the labour market, while Dosi et al. (2010) documents a very interesting application of ABM to devise a policy-friendly model that reproduces endogenous growth and business cycles. The remainder of this paper is organized as follows. Section 2 presents the economic model and its components. Section 3 runs policy invariant simulations so as to obtain a baseline scenario. Section 4 experiments with some policy levers, Sect. 5 discusses the results and their implications and Sect. 6 concludes.",1
54.0,3.0,Computational Economics,27 September 2018,https://link.springer.com/article/10.1007/s10614-018-9859-9,Computing the Bargaining Approach for Equalizing the Ratios of Maximal Gains in Continuous-Time Markov Chains Games,October 2019,Kristal K. Trejo,Julio B. Clempner,Alexander S. Poznyak,Female,Male,Male,Mix,,
54.0,3.0,Computational Economics,06 October 2018,https://link.springer.com/article/10.1007/s10614-018-9848-z,Modifying Hybrid Optimisation Algorithms to Construct Spot Term Structure of Interest Rates and Proposing a Standardised Assessment,October 2019,Aryo Sasongko,Cynthia Afriani Utama,Zaäfri Ananto Husodo,Unknown,Female,Unknown,Female,"The government’s spot interest rates play an important role in domestic economies as they are the base interest rates which underlie all other local-currency interest rates. The spot term structure of interest rates is an implied zero coupon yield curve comprising of spot interest rates to price other assets. In (1942), Durand had the traditional term structures (YTM curve) which were followed by McCulloch (1971) who developed term structure estimation. The traditional term structure can not provide accurate interest rates to price bonds especially when coupon rates are various or the curve is not flat (Penza and Bansal 2001). Investment managers therefore need to use the spot term structure instead of the traditional term structure in asset pricing. Information system terminals such as Bloomberg and Datastream provide traditional term structure instead of spot term structure. Treasury Bills and the US STRIPS yields also do not supply adequate data to construct spot term structure. The US Department of Treasury does not issue any medium to long term US Treasury Bills. Moreover, Daves and Ehrhardt (1993) and Jordan et al. (2000) noted that the US STRIPS yields vary from the US spot interest rates as a result in inadequate market liquidity. Therefore, bootstrap estimation is the only methodology to construct the term structure. Sophisticated technology is available to provide spot term structures. This research aims to implement traditional bootstrap estimation methodologies instead. Hence, familiarity with those methodologies encourages market participants to use spot term structures in much the same manner as Gürkaynak et al. (2007), who primarily focused upon data sections. The estimation employs three components (Bolder and Stréliski 1999), i.e., a curve model, an objective function and an optimisation algorithm. Figure 1 shows the cycle of bootstrap estimation. Through the use of curve model parameters, the curve model provides a set of interest rates through which the discounted cash flow and bond price can be calculated. The first estimation component is the curve model. Diebold and Li (2006) and Svensson (1994) said that the Nelson Siegel model was a flexible and parsimonious model. BIS (2005) noted that the Nelson Siegel model had been so popular that some central banks have implemented it in their systems. These countries include Belgium, French, Finland, German, Italy, Spain, Sweden and Switzerland. The flowchart shows the three components of bootstrap estimation: curve model, objective function and optimisation algorithm A number of researchers that developed the original Nelson Siegel Model including Alfaro, Björk Christensen and Svensson have undertaken further research and created extensions. Except Alfaro model, researchers already have empirical results of the others. The second component is an objective function calculating sum of squared errors (SSE) which compares observed and estimated bond prices produced by their curve models. Even though Nelson Siegel and its associated models are popular, they have a common disadvantage. The curve models specify non-linear (exponential) equations, so that the optimisation algorithm empirically generates inherent local minima (Bolder and Stréliski 1999). Ioannides (2003) and Hladíková and Radová (2012) also found the local minima in their research before they got global minimum solutions. Bolder and Stréliski (1999) and Dutta et al. (2005) assert that the local minima constitute a serious flaw which makes the development of term structures be difficult and undermine the validity and accuracy of the results. The third component is an optimisation algorithm or numerical method to calculate a minimum solution. According to Manousopoulos and Michalopoulos (2009), there are three traditional optimisation algorithm families, i.e., global (such as: Monte Carlo algorithm), gradient [such as: Newton, Quasi Newton and Sequential Quadratic Programming (SQP) algorithms] and direct search (such as: Nelder Mead algorithm) families. The most popular traditional algorithm family is gradient. Its proven geometric theory drives its popularity (Manousopoulos and Michalopoulos 2009). It uses the first and the second derivatives of SSE function to minimize an objective function. Hardly do we find any literature utilizing the exact first and second derivatives of the function. Instead, some software packages have provided estimated gradient functions (such as: gradient() function in MATLAB and grad() function in R Language) to simplify the works. Furthermore, using the traditional algorithms, researchers could not immediately get the global minimum solution. Bolder and Stréliski (1999) said that the SQP had to be repeated to converge to the solution. Adams et al. (1997) needed ten manual repetitions of simulated annealing algorithm. In order to obtain a credible estimation, we can no longer rely upon basic forms of optimisation algorithms, it should be a hybrid optimisation algorithm instead. Hybrid optimisation algorithm is the arrangement of some (at least two) distinct algorithms to estimate the same numerical problem (Cung et al. 2006). In the field of bootstrap estimation, Bolder and Stréliski (1999) have successfully designed a credible and accurate hybrid optimisation algorithm which consists of SQP algorithm, as a primary algorithm, and the Nelder Mead algorithm, as an alternative algorithm. The system activated the alternative as the primary was not punctual. In another research, Manousopoulos and Michalopoulos (2009) have proposed two hybrid optimisation algorithms which consist of two sequential algorithms. The first algorithm was the sequence of Nelder Mead and Quasi Newton algorithms. The second algorithm was the sequence of simulated annealing and Quasi Newton algorithms. Bolder and Stréliski (1999) have pioneered in the proven hybrid algorithm and Manousopoulos and Michalopoulos (2009) have given their recommendation, however theirs are not yet adopted by other researchers. For example, Ioannides (2003), Yallup (2012), Hladíková and Radová (2012) and Gauthier and Simonato (2012) still operate using the traditional optimisation algorithms. The reluctance to utilize the hybrid optimisation algorithm may be attributed to undefined algorithm’s parameters or scarcity of hybrid algorithm alternatives. In order to construct a feasible hybrid optimisation algorithm, we evaluate and pick traditional algorithms to arrange the hybrid and define necessary parameters. This is the primary issue that must be overcome through the modification of the hybrid optimisation algorithm. As the above first gap is concerned with structuring hybrid optimisation algorithm, its performance requires proper criteria and adequate indicators. Renders and Flasse (1996) had stated three estimation criteria, i.e., computation time, accuracy and reliability in a geography case study. Bolder and Stréliski (1999) were the first to use the criteria to assess term structure measurement which consist of similar terms: estimation time, goodness of fit and robustness. The first criterion is estimation time. It is the time required by computational executions which reflects the complex and effective programming and data size, such as: iteration numbers, number of benchmark bonds and number of curve parameters. The goodness of fit indicates flexibility of the curve to fit the real term structure. The robustness indicates algorithm capability to find the lowest minimum solution and to reject saddle points. Some indicators of goodness of fit and robustness have been present. We find that there is no measurement standard which provides indicators of goodness of fit and robustness. However, there are various goodness of fit indicators as explored in previous studies such as the root of mean square error (RMSE) of yield (Bolder and Stréliski 1999), parameters (Renders and Flasse 1996) and bond price error (Ioannides 2003). The existing robustness indicators are log-likelihood of objective function (Bolder and Stréliski 1999) and the successful rate to find global minimum solution in some trials (Renders and Flasse 1996). In our experience, these indicators do not effectively represent robustness, since the estimation may not reach the global minimum solution and the second algorithm’s iteration number—in repeating estimation—is always changing even if the estimation has the same result. Previous researchers have payed greater attention to stable plots of time-series parameters such as: \(\lambda _0\) and \(\lambda _1\) (Manousopoulos and Michalopoulos 2009) and stable plot \(\beta _0\) + \(\beta _1\) (Gauthier and Simonato 2012). We interpret their implicit assumption that a reliable algorithm produces stable parameters. We adopt this assumptions as our concept of robust estimation indicator, the interest rate was slightly changing from day-to-day when the market was stable, and the curve parameters were also slightly fluctuating. This is the second issue of our research, is that the bootstrap estimation requires proper and standardized indicators for goodness of fit and robustness. Researchers therefore require uniform indicators in order to compare various methodologies and data. This research paper has been structured as follows: Sect. 2 provides a literature review and Sect. 3 mainly defines some algorithm parameters and modifies hybrid optimisation algorithms to improve its performance. The section also promotes performance measurements of the second and third criteria. Section 4 discusses empirical results of the modified methodologies and indicators, and Sect. 5 concludes this paper.",2
54.0,3.0,Computational Economics,12 October 2018,https://link.springer.com/article/10.1007/s10614-018-9861-2,Accounting for Heterogeneity in Environmental Performance Using Data Envelopment Analysis,October 2019,George Halkos,Mike G. Tsionas,,Male,Male,Unknown,Male,"Environmental performance of countries or firms may be calculated from productive efficiency categorizing variables into inputs and outputs and identifying an environmental production technology for representing the joint production of good and bad outputs. In this case there are two approaches for measuring environmental performance: either calculating an efficiency or productivity measure for simultaneously increasing good or reducing bad output production or creating a proper environmental performance index (EPI) using DEA and relying on Shephard or Malmquist distance functions (Färe et al. 2004, 2006). In these lines, Pittman (1983) performed an early study including pollutants into productivity measurement while Färe et al. (1989) presented a neat theoretical basis for the use of Data Envelopment Analysis (hereafter DEA) a non-parametric frontier method to assess productive efficiency with undesirable outputs. DEA as non-parametric method relies on a set of inputs and outputs to measure efficiency of various decision making units (DMU) (Zhou et al. 2017). To calculate the relative efficiency of the DMU’s an important consideration is the selection and processing of the data (selection of inputs, outputs and number of DMUs). Another important consideration, particularly in large data sets is that the assumption of a homogeneous technology is likely to be violated. This can be due to exogenous factors affecting the results (Sakris 2007; Haas and Murphy 2003). Heterogeneity among decisions making units stems from various sources like differences in market conditions, different stages of economic growth, differences in technological advances and differences in industrial structure, among other things.Footnote 1 The purpose of this paper is to account for heterogeneity of production technology in different DMUs for more accurate efficiency measurement.Footnote 2 Homogeneity may be tested by pre-processing the statistical distribution of the data set and omitting possible influential observations (outliers) or applying post-processing analysis such as analysis of returns to scale or multi-tiered DEA (Barr et al. 1994) if the number of DMUs is large, to examine the existence of homogeneity (Sakris 2007). Along the lines of the former approach one can use (1) cluster analysis to separate a data set into mutually exclusive groups (Okazaki 2006; Osei-Bryson and Inniss 2007). Cluster analysis may be used with DEA either by applying it to DEA efficiency results aiming at creating multiple reference subsets from the original DMUs (Bojnec and Latruffe 2007; Meimand et al. 2002) or (2) by bounding a comparison of each DMU to its reference subset isolating multiple homogeneous subsets (Azadeh et al. 2007). Samoilenko and Osei-Bryson (2010) suggested a way to tackle scale-heterogeneous data when performing DEA by using cluster analysis: As claim, this does not require dividing the sample into multiple peer groups, it does not require large data sets and it does not require comparisons that are external to DEA like “external comparators” mentioned by Dyson et al. (2001) and used in Sarrico and Dyson (2000). The use of Neural Networks (NN) is another data mining technique appropriate when there is an unknown mathematical relationship between inputs and outputs when one is interested in predicting rather than explaining (Samoilenko and Osei-Bryson 2010). Athanasopoulos and Curram (1996) compared DEA with NN while Azadeh et al. (2007), Santín et al. (2004) and Wang (2003) used NN as alternative, Wu et al. (2006) as complement and Çelebi and Bayraktar (2008) as a pre-processing tool of the data, prior to using DEA. Although NN can account for complicated production structures, heterogeneity cannot be properly accounted for and it provides misleading results. Haas and Murphy (2003) suggested three more strategies to account for heterogeneity. First, a multi-stage method as in Sexton et al. (1994) to perform DEA in the raw data and extracting DMUs’ efficiencies, running a stepwise multiple regression on the efficiency scores taking into consideration a set of exogenous characteristics expected to account for the differences in efficiency and then performing another DEA analysis to the adjusted data to extract a revised set of efficiency scores. The second method is based on the magnitude of forecast error (actual minus fitted values) using regression analysis to determine inputs and outputs with the factors creating heterogeneity considered as explanatory variables. Then DEA is used in the differences between actual and predicted inputs and outputs. The third method is the “forecast error” method where ratios of actual input to forecast input and actual outputs are used to predict outputs performing DEA. Simulation experiments have shown that these methods do not perform well as stated in Sakris (2007) with alternative methods to cope with heterogeneity to be limited. Actually he encourages for the search of new methods more adequate in tackling heterogeneity. The task of our research is to propose a straightforward way to tackle heterogeneity in the use of DEA. This is, as mentioned, an important issue in preparing the data before extracting the efficiency scores. Using a data set of 44 countries for the time period 1996–2014 the unknown production technology is modeled relying on a local set of 44 countries for the time period 1996–2014 the unknown production technology is modeled relying on a local likelihood estimator with multivariate mixture-of-normals-distributions (MMND). These models are quite flexible, permitting a consistent merger of pre-processing the data and estimating the efficiency scores by DEA. The sampling properties are explored using Monte Carlo simulations and we show that the new techniques perform well in simulated data. As we use panel data we also take into consideration the possibility of structural breaks or changes in the technology over time, as we should allow for the possibility of different technologies particularly when the number of observations in the time dimension is large. In this respect, we do not treat different DMUs and time periods as different units. Instead we focus on the DMUs, and we allow for changes in technology over time, using a penalized likelihood estimator with a penalty term that reflects the fact that technology changes are possible but relatively rare.",1
54.0,3.0,Computational Economics,17 October 2018,https://link.springer.com/article/10.1007/s10614-018-9860-3,An Evolutionary Game Approach in International Environmental Agreements with R&D Investments,October 2019,Giovanni Villani,Marta Biancardi,,Male,Female,Unknown,Mix,,
54.0,3.0,Computational Economics,25 October 2018,https://link.springer.com/article/10.1007/s10614-018-9864-z,Machine Learning and Sampling Scheme: An Empirical Study of Money Laundering Detection,October 2019,Yan Zhang,Peter Trubey,,Male,Male,Unknown,Male,"Money laundering is the criminal practice of processing illegal money through a series of transactions to gain access to the legal financial system. It is a type of “financial crime with potentially devastating social and financial effects” (FFIEC 2014, p. 7). The amount of money laundered globally in a year is estimated to be 2–5% of global GDP, or 0.8–2 trillion in US dollars.Footnote 1 The United States Congress passed the Bank Secrecy Act (BSA) in 1970, which mandates that financial institutions identify and prevent dirty money from entering the U.S. financial system. The Money Laundering Control Act of 1986 further establishes money laundering as a federal crime. The USA PATRIOT Act of 2001 provides a legal framework to prevent international money laundering, focusing on terrorist financing. Suspicious activity can be detected manually, whereupon observing abnormal behavior by a customer, an employee of the financial institution files a report. However, as financial activities become prolific, the tremendous amount and speed of financial transactions call for an effective monitoring system that can process transactions quickly, preferably in real time. Moreover, as technologies advance, more and more transactions are conducted in the absence of personal interaction, eliminating the opportunity for manual scrutiny. This trend of change has accelerated recently, showcased by the rapid growth of financial technology companies and financial disintermediation. Through this need, automated transaction monitoring systems have been established as an indispensable part of the anti-money laundering regime. Typical transaction monitoring systems utilize rules to detect money laundering activities. Rules are basically if and then conditions with thresholds pre-defined based on historical data or business intuition. As money launderers continuously attack monitoring systems and modify their strategies to find loopholes, the rules need to be continuously validated and calibrated, which could be labor intensive and thus subject to delay. Moreover, rule-based systems focus on individual transactions or simple transaction patterns, therefore they might not be sufficient to detect complicated transaction patterns. Conversely, machine learning develops models that are able to independently adapt when exposed to new data. They learn from previous computations to produce reliable and repeatable decisions and results. With the development of high-speed computing, machine learning can analyze large amounts of data quickly. Many of the machine learning algorithms are designed to capture complex patterns like nonlinear relationships between a dependent variable and explanatory variables, where the linear rules fall short. The abilities of machine learning to handle large data, analyze unstructured information, identify complicated patterns, and self-update with new information make it a good candidate model for detecting money laundering activity. Despite an expanding strand of literature documenting applications of machine learning to credit risk and fraud,Footnote 2 literature on using machine learning in detecting money laundering activity is still limited. Most of the relevant literature are of survey nature (Bolton and Hand 2002; King et al. 2018; Phua et al. 2010; Sudjianto et al. 2010). While a few authors study the use of machine learning in anti-money laundering by utilizing actual bank or firm data in foreign countries (Tang and Yin 2005; Wang and Yang 2007; Liu et al. 2008; Lv et al. 2008; Le-Khac et al. 2010; Villalobos and Silva 2017), documentation of machine learning applications in anti-money laundering in the U.S. is often descriptive or suggestive, with little detail on data, model, and performance (Senator et al. 1995; Kirkland et al. 1999). This is largely due to the lack of public datasets that can be used to analyze money laundering detection. While general transaction data can be utilized to perform unsupervised machine learning so as to identify outliers, it is hard to obtain money laundering detection outcomes that are needed by supervised machine learning to conduct predictive analysis and more importantly to evaluate model performance.Footnote 3 Investigation outcome measures such as Suspicious Activity Report (SAR) or case indicator are considered sensitive information to be disclosed to the public; not to mention that law enforcement agencies typically do not provide feedback on whether reported suspicious activities are actually found to be of money laundering nature or not. Another challenge in modeling money laundering activities is that they are rare events. Rarity of an event refers to the situation that the events of interest are few in number relative to the non-events. Examples of rare events include natural disaster, terminal disease, terrorist attack, war, financial fraud, or money laundering. These events are not frequent, yet their effects can be so detrimental that it is important to understand their causes in order to prevent them. In such cases, the false negative error (inaccurately classifying events as non-events) carries a much higher cost than the false positive error (inaccurately classifying nonevents as events). However, when models fail to take the cost differences associated with classification errors into account in cases of rare events, sub-optimal performance in discrimination is likely to occur. In addition, due to their rare occurrence, the information associated with the events tends to be buried by the dominating non-events, thus imposing challenges for the model to effectively sort through the various signals in the data and to accurately separate the two types of events. Because of these problems, models can perform sub-optimally in discriminating events from non-events when the events are rare. Approaches proposed to address the rare event issue can be divided into two main categories. One is to tailor the algorithm, for example, adjusting the misclassification costs for the various classes. The other is to manipulate the data prior to modeling in order to magnify the information contained in the event observations. This category is not model specific as it preprocesses the data so that models are estimated on a then well-balanced dataset. There are two approaches loosely falling into this category. The first is to modify the target variable. An alternative target variable can be used so that it is closely related to the original target variable and yet occurs more often. For example, given the low frequency of corporate default, the agency rating can be used instead to model corporate default risk. But such an alternative is not always readily available and may not necessarily reflect the true relationship to be modeled, thus arises the second and more common technique, which involves sampling to magnify the event rate. Two popular sampling approaches to deal with rare event data are to either under-sample the majority class or over-sample the minority class. This paper conducts an empirical study that provides insights into the use of machine learning and sampling schemes to better detect a particular type of rare event, money laundering activities. The analysis is based on actual transaction data from a U.S. financial institution, with investigation outcomes appended. This data enables us to evaluate predictive models. We study four representative supervised machine learning algorithms, including Decision Tree (DT), Random Forest (RF), Support Vector Machine (SVM), and Artificial Neural Network (ANN). The traditional regression model used in handling a binary target, Maximum Likelihood Logistic Regression (MLLR), is included in our study as the benchmark. To address the impact of rare event data on MLLR fitting, a machine learning algorithm, Bayes Logistic Regression (BLR), is also studied. We introduce under- and over-sampling to alleviate the data imbalance and evaluate the sensitivity of the models to subsequent changes in the event rate. To the best of our knowledge, this is so far the most comprehensive empirical study of supervised machine learning models in the area of money laundering detection involving actual transaction data of a U.S financial institution. It is also believed to be the first attempt in modeling money laundering activities that applies under- and over-sampling to improve model performance. Our analysis reveals advantages of machine learning algorithms in modeling money laundering events given the data and model specifications used. In particular, ANN consistently outperforms parametric logistic regressions with and without sampling; in high event rate regimes with sampling, SVM surpasses logistic regressions and RF delivers comparable performance. The rest of the paper is organized as follows. Section 2 describes the data used. Section 3 discusses the models, sampling schemes, model performance evaluation criteria, and model parameter optimization. Section 4 presents the results. Section 5 concludes the paper.",29
54.0,3.0,Computational Economics,26 October 2018,https://link.springer.com/article/10.1007/s10614-018-9863-0,On the Convergence of the Generalized Ibn Ezra Value,October 2019,Louis de Mesnard,,,Male,Unknown,Unknown,Male,"In an allocation problem, when the estate to be shared E is larger than the sum of claims d, each claimant is obviously served without any rationing. The allocation problem, the so-called “bankruptcy problem”, begins when the value of the estate amounts to less than the sum of the claims. When the estate to be shared E is just equal to the sum of the claims d, i.e., \(E=d\), the whole estate can be allocated without any competition between the claimants, each one trivially receiving exactly what he claims. Ibn Ezra (1146), Rabinovitch (1973), O’Neill (1982), Bergantinos and Mendez-Naya (2001), Chun and Thomson (2005) and Alcalde et al. (2008) proposed a method for solving the bankruptcy problem that he called the rights arbitration problemFootnote 1 where the estate E is equal to the greatest claim of n claimants. As expounded by O’Neill (1982), it can be solved in n steps and poses no difficulty. However, when the maximum claim is for less than the available estate, the question is: what do we do with the difference between the estate and the largest claim? Who receives it? A number of solutions are possible. The most popular is O’Neill’s (1982) Minimal Overlap Rule.Footnote 2
Alcalde et al. (2005) criticize this procedure because it mixes two principles of equity: “Up to a certain amount of estate we should follow the recommendations by Ibn Ezra and, after it, we should divide divide the extra estate trying to equalize agents’ loses” (Alcalde et al. 2005, p. 15).Footnote 3 This is why they propose another attractive solution, the Generalized Ibn Ezra Value, by imposing “that the general principle in which the recommendations by this author [Ibn Ezra] are inspired should remain fixed” (Alcalde et al. 2005, p. 15). This method is iterative, that is, we have T iterations (each one including the n steps of Ibn Ezra—O’Neill). In this paper, we examine the computational and mathematical properties of the algorithm of the Generalized Ibn Ezra Value to explain why the method poses problems.",
54.0,3.0,Computational Economics,30 October 2018,https://link.springer.com/article/10.1007/s10614-018-9868-8,A Spectral Approach to Pricing of Arbitrage-Free SABR Discrete Barrier Options,October 2019,Nawdha Thakoor,Désiré Yannick Tangman,Muddun Bhuruth,Unknown,Male,Unknown,Male,"The appearance of strike and maturity dependent volatilities, the so-called smiles and skews, in market option prices showed the limitations of the constant volatility lognormal model (Black and Scholes 1973) for hedging options on a single asset with different strikes. A local volatility model capable of explaining all market European prices was proposed by Dupire (1994), but as shown in Hagan et al. (2002), local volatility models exhibit smile dynamics which are opposite to market behavior and to address this problem, they proposed the Stochastic Alpha Beta Rho (SABR) model which extends the constant elasticity of variance (CEV) model (Cox 1996) by incorporation of a stochastic volatility process driven by a driftless process. This model has the capability of capturing the volatility smiles and skews observed in the market (Wu 2012). The popularity of SABR stems from the fact that an analytical approximation for the implied volatility exists and this leads to a simple pricing mechanism via Black’s formula (Black 1976) for short-dated options. Doust (2012) showed that for long-dated options, the approximation formula can lead to a negative probability density function. Improvements to the approximation formula for the implied volatility based on singular perturbation techniques exist (Berestycki et al. 2004; Oblój 2008). These expansions are good for small maturity but when maturity grows, second-order expansions can produce negative volatilities for low strike prices (Paulot 2015). Rebonato et al. (2009) showed that there is a possibility that the forward price may hit zero resulting in arbitrage opportunities and an absorbing boundary condition must be specified at zero to avoid arbitrage opportunities. 
Hagan et al. (2014) provided an alternative method using asymptotic techniques to reduce the SABR model from two dimensions to one dimension which leads to an effective one-dimensional forward equation for the probability density. Arbitrage-free option prices are then obtained by numerically solving the effective pde with an absorbing boundary condition. Yang et al. (2017) developed a closed-form arbitrage-free approximation to price European options under the SABR model. Defining the total volatility of volatility as the product of the volatility of volatility and the square root of the maturity time, these authors mentioned that the formula is quite good for small total volatility of volatility parameter and can result in biased results for large total volatility of volatility. It is therefore necessary to have a SABR pricing mechanism that is able to address the problem associated with the valuation of long dated options and also, a technique which can price options where the implied volatility approach cannot be used. This is the case for discretely monitored barrier options and such a pricing method is the main contribution of this paper. It is to be noted that Tian et al. (2012) have considered the pricing of discretely monitored barriers but their quasi Monte-Carlo method with Brownian bridge and conditional probability correction is computationally expensive. A much simpler partial differential equations approach is proposed here. It is a well-known fact that there are significant differences in prices of continuously monitored and discretely monitored barrier options for the Black–Scholes model, a special case of the SABR model where the elasticity parameter beta is one and where the constant volatility of the stochastic volatility process is zero. The pricing of discretely monitored barriers under SABR is of particular interest since the effect of the elasticity parameter on price of the discretely monitored option has not been previously considered in the literature. In order to illustrate the good performances of the new algorithm, pricing is carried out for special cases of the SABR model where analytical solutions are available. For European options, comparisons of numerical results with analytical prices available for the CEV process, reveal high accuracy for long maturity problems whereas the implied volatility approximations yield much less precise option prices. For discretely monitored barriers, comparisons are first drawn against benchmark prices available in the literature under the Black–Scholes model to demonstrate the algorithm’s capability for accurate pricing of discrete barriers. The pricing is then extended to the arbitrage-free SABR model and option prices for different values of the elasticity factor are given. An outline of this paper is as follows. First we present the SABR model and the implied volatility approximations in Sect. 2. The two-dimensional pde under this model for pricing European and barrier options are given in Sect. 3. Then in Sect. 4, we present the components of the new spectral scheme for pricing options with different features. Numerical results are presented in Sect. 5 to demonstrate the arbitrage-freeness of the proposed scheme and the pricing of European and continuously and discretely monitored barrier options. Our conclusions are presented in Sect. 6.",3
54.0,3.0,Computational Economics,30 October 2018,https://link.springer.com/article/10.1007/s10614-018-9866-x,Price Convergence under a Probabilistic Double Auction,October 2019,Xiaojing Xu,Jinpeng Ma,Xiaoping Xie,Unknown,Unknown,Unknown,Unknown,,
54.0,3.0,Computational Economics,31 October 2018,https://link.springer.com/article/10.1007/s10614-018-9867-9,Uniqueness and Multiple Trajectories for the Case of Lucas Model,October 2019,C. Chilarescu,I. Viasu,,Unknown,Unknown,Unknown,Unknown,,
54.0,3.0,Computational Economics,12 November 2018,https://link.springer.com/article/10.1007/s10614-018-9865-y,On the Numerical Solution of Mertonian Control Problems: A Survey of the Markov Chain Approximation Method for the Working Economist,October 2019,Simon Ellersgaard,,,Male,Unknown,Unknown,Male,"It is well known to anyone who has dabbled in continuous time continuous state stochastic control problems à la (Merton 1969) that the search for closed form solutions is an onerous and all-too-often fruitless endeavour. This is unfortunate given the far-reaching scope of the field (see e.g. Cartea et al. 2017; Munk 2017; Rogers 2013 and the references therein), but hardly surprising given the highly non-linear nature of the governing PDEs. What is at least somewhat puzzling is the scarcity of pedagogical resources that “take the bull by the horns”, as it were, and tackle these problems from a numerical angle. More concretely, while a plurality of such methods exist, there has been a widespread failure to disseminate their core principles to students of quantitative finance. Said principles include hands-on discretisations of the Hamilton–Jacobi–Bellman equation (see e.g. Forsyth and Labahn 2007; Wang and Forsyth 2008), Monte Carlo methods (see e.g. Cvitanić et al. 2003; Detemple et al. 2003), and forward-backward stochastic differential equations (see e.g. Ludwig et al. 2012). Most prominently, perhaps, stands Kushner and Dupuis’ monograph on the so-called Markov chain approximation method (Kushner and Dupuis 2013), which although predominantly concerned with control-independent diffusions, straight-forwardly can be generalised in this direction (Kushner 1999). This paper pertains to this latter method, the central premise of which is to substitute the continuous time continuous state controlled state process by a discrete time discrete state Markov chain. As shown by Kushner and Dupuis, by choosing the transition probabilities of the Markov chain based on standard finite difference discretisations of the governing Bellman equation, convergence can be established. The purpose of this paper is two-fold: first, it serves as an exegesis of the Markov chain approximation method for anyone who seeks a swift and comparatively unconvoluted theoretical overview of the area. To this end, even numbered sections aim to equip the reader with the fundamental theoretical tools needed in order to implement numerical control problems in simple environments where uncertainty is driven by a Brownian motion. The second, and arguably more intriguing part of this paper is the odd numbered sections, which provide a detailed account of just how well the Markov chain approximation method fares when we deploy it in a Merton type portfolio optimisation context. As it will soon become apparent, actual implementations require a non-negligible amount of Fingerspitzengefühl, the key lessons of which are not communicated by Kushner and Dupuis. For example, inexact boundary conditions turn out to have a corrosive effect on the accuracy of the numerical controls for a rather large region into the finite difference grid. However, we show that if an exact relationship between adjacent grid nodes can be established, this problem altogether dissipates. Other matters of interest include the effect of the granularity of the mesh, the magnitude of the discount factor and the agent’s risk aversion, and the sanctity of maintaining positive probabilities. Admittedly, this is not the first paper to deal the numerical implementation of the Merton problem. Concrete examples include Fitzpatrick and Fleming (1991) and Munk (1997, 2003). Nevertheless, these papers deal with the infinite horizon single state process case, and only focus on the implicit implementation procedure. We, on the other hand, focus on finite horizon investment problems both from an explicit and implicit perspective, with generalisations to higher spatial dimensions. For readers interested in exploring the employ of the Markov chain approximation framework in less run-of-the-mill control problems in finance, we refer to Hindy et al. (1997) in which a free boundary consumption problem is considered, Munk (1999) in which rational reservation prices for European options are established, and Munk (2000) in which optimal consumption-investment policies are determined when the agent receives stochastic undiversifiable labour income.",
54.0,3.0,Computational Economics,20 November 2018,https://link.springer.com/article/10.1007/s10614-018-9869-7,Modeling Credit Risk with Hidden Markov Default Intensity,October 2019,Feng-Hui Yu,Jiejun Lu,Wai-Ki Ching,Unknown,Unknown,Unknown,Unknown,,
54.0,3.0,Computational Economics,08 December 2018,https://link.springer.com/article/10.1007/s10614-018-9870-1,Financial Market as Driver for Disparity in Wealth Accumulation—A Receding Horizon Approach,October 2019,Raphaele Chappe,Willi Semmler,,Unknown,Male,Unknown,Male,"Disparities and inequality are rising in most developed economies—with respect to income, but even more so with respect to wealth. As many recent observers have stated income inequality is worse in the U.S. than in other western countries.Footnote 1 Top income shares for some higher income groups in the U.S. are now higher than before World War II (Piketty and Saez 2003; Piketty 2013). After the war, the share of the top decile income (excluding capital gains) fluctuated between 31 and 33% until the 1970s. By 1998 it had risen to 44% in the U.S. Today it is as much as 50% of total income, with the top 1% alone receiving 20% (Piketty and Saez 2003; Wolff 2010). Many observers have stated that wealth disparity is even greater than income inequality. Yet, there are long swings in wealth inequality, declining with the rise of income tax in the prewar until the post World War II period and then again secularly rising again in the last few decades, in particular in the U.S. since the end of the 1970s (See Wolff 1996, 2006, 2010), with the top of the wealth distribution exhibiting a power law distribution (Klass et al. 2007). In the U.S., the wealthiest 5% of American households held 54% of all wealth reported in the 1989 Survey of Consumer Finance; this share has now reached 63% as of 2013 (Yellen 2014).Footnote 2 Ownership of financial assets is even more concentrated, with the wealthiest 5% of U.S. households holding nearly two-thirds of all financial assets in 2013, and the bottom half as little as 2% (Yellen 2014). Economic theory has provided us with some explanations of the distribution of income and wealth since long. Those explanations start with classical economists on the laws governing the allocation of income between wages, rent and profit. The modern theory of income distribution beginning after World War II (the works of Kaldor, Pasinetti, Kuznets, Kalecki) mainly explored the issue of whether a country’s level of economic prosperity and stage of development has implications for income disparity. Kaldor (1956, 1961) proposed a Keynesian model of economic growth, distribution and inequality arising from differences in the saving rates between wage earners and asset owners, to which Pasinetti (1962) responded by pointing out that workers will also own assets.Footnote 3 Notably, Kuznets (1955) studied the relation between income distribution and a country’s state of development. Kuznets’ hypothesis was that the distribution of income is more unequal in early stages of development (economic inequality first increases while a country is developing), but that this trend is reversed after a certain income threshold is reached—the famous inverted U-shape function (the Kuznet curve) relating inequality to the level of GDP.Footnote 4 The success of Piketty’s book has placed distributional issues and inequality at the forefront of the public economic debate. Piketty’s recent hypothesis is that the main driver of inequality is the tendency of returns on capital r (note that Piketty uses “capital” interchangeably with wealth) to exceed the rate of economic growth g. This is Piketty’s key inequality relationship \(r>g\) which in principle is obvious, following classical growth theory (since part of those returns are consumed and cannot make the economy grow). Income can be decomposed between, roughly speaking, labor income and capital income (and possibly an in-between category of entrepreneurial income for self-employed entrepreneurs). Piketty’s original contribution directly relates to the dynamics of the share of capital income in national income (the “alpha”) relative to labor income (Piketty 2013). Labor income and capital income each have, of course, their own distributional features. We can understand total income inequality in terms of the distribution of labor income, the distribution of capital income (itself dependent on capital ownership), and the possible skewed or bimodal distribution in each of those categories. As to the different types of income we will not pursue a further differentiation. Over the reference period 1913–1998, Piketty and Saez (2003) find a steady decline in top capital incomes (dividend, interest, rents, royalties, capital gains) since the 1960s, and a significant increase of the share of wage income since 1929, for all income groups. The fall in top capital incomes was for most countries attributable to key macro-economic and fiscal shocks (World War I, the Great Depression, and World War II). Piketty and Saez (2003) explain the decline in top capital incomes in terms of the inability of large fortunes to recover from such shocks, leading to a decreased concentration of capital income (rather than a decline in the share of aggregate capital income in the economy, which is relatively steady in the long run at around 25–30%). This leads Piketty and Saez (2003) to conclude that there has been a change in how high income earners derive their incomes, and that “the working rich have now replaced the coupon-clipping rentiers.” (Piketty and Saez 2003, p. 3). This could suggest that inequality today is mainly driven by factors that affect top wage income. One hypothesis is that the wage distribution itself may have worsened through factors that shape the supply and demand of skilled and unskilled labor, forces such as skill biased technical change, institutional change, globalization and outsourcing, labor laws and job protection, union membership, etc.Footnote 5 Regarding top labor income specifically, Piketty expresses some reservations as to whether the standard theory of the marginal productivity of labor can properly account for the explosion of very high salaries for the top 1% (and especially 0.1%) of earners (Piketty 2013, p. 529). Yet the above is not to say that capital income does not play an important role in explaining rising levels of inequality. Many observations point to a measurement issue, namely the issue of whether part of labor income might actually be capital income. Moreover, capital income tends to play an increasingly important role relative to labor income as one climbs up the social ladder to top income earners (specifically within the top 1%).Footnote 6 The top 1% is characterized by a combination of both top capital income earners and top labor income earners, rather than the complete replacement of one by the other (Piketty 2013, p. 475; Wolff and Zacharias 2007). Capital income exceeds labor income as a share of total income for the top 0.1% of top income earners. Of particular relevance here is the fact that high net worth individuals are growing their wealth largely on the strength of the strong performance of global financial markets, deriving stronger benefits than other investors. Since the 1980s the wealth of high net worth individuals has grown faster than that of average investors, and faster than world GDP (see Table 12.1 in Piketty 2013). After the financial crisis, the wealth of investors having investable assets of $1 million or more actually increased from $46.22 trillion in 2012 to to $52.62 trillion in 2013 (Capgemini and RBC Wealth Management 2014)—this strong performance occurring in spite of the fact that the global economy was still rather sluggish. Much of this wealth is managed by a thriving global wealth management industry offering different products and services that appeal to different categories of investors: from mainstream portfolios and products (indexed funds, exchange traded funds, passively managed mutual funds) to more aggressive actively-managed strategies (such as the world of “alternative investments”—hedge funds and private equity funds). There has been a steep and marked increase in the fraction of households holding stock since the 1960s, with almost 50% of households owning stocks, thereby earning both labor income and capital income (Wolff 2012)—average investors, or investors from low net worth brackets, are typically exposed to financial markets directly or indirectly through individual retirement accounts managed by institutional investors. Yet their performance in terms of wealth accumulation may show different results than for high net worth individuals. A somewhat controversial claim is the idea that wealthy investors earn higher returns in the long run, contrary to the efficient market hypothesis (Piketty 2013; Saez and Zucman 2014; Wolff 2012). Piketty’s hypothesis that the main driver of wealth disparity is the tendency of returns on assets to exceed the rate of economic growth is a reasonable consideration if the returns on assets for certain groups of wealth holders are greater than the growth rate of income of the rest of the population. This of course also assumes that the consumption fraction of income of the first group does not erode the asset accumulation of that group—so we have both the saving rate as well as returns as a driver of the growth of assets. There are potentially differences between average investors and sophisticated (accredited) investors that would affect rates of return in this way, for example through informational advantages. The portfolio composition of household wealth shows differences between households in terms of the types of investments made, giving rise to portfolio composition effects. Investment portfolios of wealthy investors typically reflect a balanced allocation between equities, fixed income, real estate, alternative investments, as well as cash and cash equivalents.Footnote 7 For average investors their major asset is their home, pension and more liquid assets (as to personal investment and savings, low income investors usually invest in low return assets—in risk-free deposits in thrift organizations and savings accounts in commercial banks). Beside the return on assets and saving rates there is also the issue of the access to credit and leveraging. Low net worth individuals typically cannot easily lever up when higher returns are expected; investment allocations of high net-worth individuals tend to reflect an appetite for riskier products, and can scale up the operations through leveraging. More sophisticated investors and management styles (e.g. hedge funds) do rely on investment strategies that involve higher average returns, either through leverage and higher risk, or informational advantages that might secure “alpha” (i.e. some return over and above the risk-adjusted return).Footnote 8 Such differences in asset/portfolio composition may even expose households differently to expansionary monetary shocks.Footnote 9As Saez and Zucman (2014) note, “wealthy families might be able to earn 6% on their bond portfolio (e.g., by investing in foreign markets or in high return convertible bonds) while the rest of the population might earn 3% only”. In this paper we explore the forces causing disparities and inequalities stemming from financial markets. While most of the literature is focused on understanding disparities and inequality in terms of the rise of top income from labor, we propose to model and study wealth disparity and the process of wealth accumulation from a microeconomic perspective, focusing on the drivers arising from the financial market. Running away of some “top labor income” and “super star” incomes may end up there, but important effects for rising wealth disparities mainly seem to stem from forces in the financial markets. For the rise of wealth in the last few decades—real and financial wealth as Piketty defines it—there might have been several factors at work: (1) rising superstar income, enlarged through the financial market, (2) higher saving rates of certain groups of wealth holders (consumption may have upper limits of saturation, so that saving rates can rise), (3) higher returns of certain groups of wealth holders on the higher end of the distribution relative to average investors and low income earners, and (4) the possibility of leveraging up investments using larger assets and net worth as collaterals. These are the forces we want to explore. To analyze the wealth distribution and the recent trend of high inequality of labor income in the U.S. specifically, we turn to three frameworks in the literature: the general life-cycle models (the main framework for studying wealth inequality); models with stochastic multiplicative dynamics; and the literature of portfolio choice. Empirically, observed patterns for savings for retirement vary greatly between households. In the life-cycle hypothesis, these variations originate from any factor that might impact households’ consumption and saving paths (Bernheim et al. 2001). Robert Becker has shown that differences in consumption preferences over time could alone lead to the full concentration of wealth within a single household (Becker 1980).Footnote 10 Models study the distribution of retirement wealth as a function of households’ optimal decisions about how much to consume or save from their labor and investment income.Footnote 11 Yet though there is indeed empirical evidence of heterogeneity in time preferences,Footnote 12 it is unlikely that this alone is sufficient to account for the degree of wealth inequality observed in the data. A process for labor income with heterogeneous earning shocks can also be added to the life-cycle framework,Footnote 13 but again though a stochastic labor endowment process can conceivably generate skewness in the wealth distribution, these models also cannot generate wealth inequality levels that match empirical data (Aiyagari 1994; Huggett 1993).Footnote 14 To recreate wealth dispersion levels that are observed empirically, we have to turn to models that contain “multiplicative” wealth dynamics that are at the core of highly concentrated (power-law) wealth distributions. Power-law dynamics can emerge from simple features of capital markets, such as the idiosyncratic risk component associated with the realization of capital income, this independently of any portfolio optimization problem and optimal decisions about how much to save or consume.Footnote 15 It is argued by some that Pareto distributions are a fundamental property of capital markets and of the investment process because of stochastic multiplicative dynamics (Levy 2006). This in turn has been linked to the efficient market hypothesis, with idiosyncratic risk (e.g. the standard deviation of Brownian motions) the major factor affecting the wealth distribution.Footnote 16 This suggests that inequality can result from luck and stochastic forces alone. An added framework is that of portfolio choice (or asset allocation), i.e. how households allocate their investments between assets with different characteristics (risk and return profiles), a recurring and classic problem in financial economics; see Campbell and Viceira (2002) for a general overview.Footnote 17 The problem of portfolio choice is relevant for all categories of investors, and not just the wealthiest households. We approach the problem of wealth inequality by asking what characteristics of the wealth distribution can be obtained from the dynamic behavior of heterogeneous investors over time. This leads us to a dynamic portfolio model using a stochastic dynamic model of wealth accumulation with preferences, saving and consumption, heterogeneity between investors, and portfolio choice. We identify as main drivers of wealth inequality (1) higher saving rates stemming from differences in investor-level characteristics, including discount rates, risk aversion, and decision horizon, (2) higher asset returns through higher risk (beta) and leveraging, and (3) higher asset returns through alpha. In this paper we focus more specifically on the first two factors. Our main findings show that controlling for differences in earning power as well as for idiosyncratic market risk, differences in saving patterns are sufficient on their own to create significant wealth inequality and that, further, the length of the optimization horizon is the investor-level parameter that most significantly impacts the saving rate. When leverage is made available to some investors but all investors share the same characteristics and saving patterns, the impact on the wealth distribution is minimal. Though insignificant on a stand-alone basis, leverage increases the Gini coefficient by 5% points when combined with differences in the optimization horizon, and as much as 9% point when idiosyncratic labor income risk is introduced. This suggests that access to leveraged investments (high betas) for aggressive investors is a significant factor in shaping the wealth distribution. The rest of the paper is organized as follows. In Sect. 2, we outline the stylized facts we wish to study. Section 3 introduces a stochastic dynamic model of wealth accumulation. Section 4 presents our main results and simulations. Section 5 concludes the paper.",1
54.0,4.0,Computational Economics,02 January 2016,https://link.springer.com/article/10.1007/s10614-015-9558-8,Analysis of China’s Regional Eco-efficiency: A DEA Two-stage Network Approach with Equitable Efficiency Decomposition,December 2019,Junfei Chu,Jie Wu,Beibei Xiong,Unknown,,Unknown,Mix,,
54.0,4.0,Computational Economics,06 January 2016,https://link.springer.com/article/10.1007/s10614-015-9560-1,Environmental Performance and Benchmarking Information for Coal-Fired Power Plants in China: A DEA Approach,December 2019,Xiaohong Liu,Qingyuan Zhu,Xingchen Li,Unknown,Unknown,Unknown,Unknown,,
54.0,4.0,Computational Economics,20 February 2016,https://link.springer.com/article/10.1007/s10614-016-9564-5,The Co-movement Between Chinese Oil Market and Other Main International Oil Markets: A DCC-MGARCH Approach,December 2019,Malin Song,Kuangnan Fang,Jianbin Wu,Female,Unknown,Unknown,Female,"China has experienced vigorous economic as well as income growth in the past 30 years. In conjunction with these, consumption of primary energy as a whole, particularly oil has been increasing steadily from 2.25 million B/D in 1990 to 4.88 million B/D in 2010, at an average annual rate of 7.3 %. The International Energy Agency (IEA), Energy Information Administration (EIA) of the U.S., and other organizations expect energy consumption in China to steadily increase at an annual rate of approximately 3 % for the next ten to twenty years. The energy needs of the country are undoubtedly too large to be met easily by domestic production, leading to an inevitable increase in imports (Ang et al. 2011). Net oil imports of China increased uninterruptedly from 11.1 Mt. in 1993 to 218.4 Mt. in 2009 and its share of the world total imported (exported) oil expanded significantly from 0.6 % in 1993 to 8.3 % in 2009 (British Petroleum, 2010). According to National Bureau of Statistics of China, China’s dependence on imported oil has reached over 50 % in the year of 2010 with its import dependency (measured by the percentage of net imports over total demand) rocketing 53.5 % in 2009 (Leung 2010). While China is doing everything it can to secure the supply of energy and the shift to a low-carbon economy is also progressing steadily, the general consensus is that the country will still rely largely on fossil fuels in the coming decades. In order to solve the energy problem, China is actively engaging itself in the world oil market and has become unprecedented integrated with the international oil market. One dimension of China’s rapid emergence as a major force in world energy markets is that China is opening itself up to potential disturbances in the world market and oil supply disruptions will negatively affect China’s economic growth and hence it arouses concerns about energy security for the country. This is compound with high energy prices and the concern about global warming and sustainable development where international oil price fluctuate greater than before (Zhou et al. 2012). Therefore exploring the co-movement of Chinese and international oil market and paying more attention to reduce the risk of oil price volatility are of vital importance to China. Combining security and profit considerations, the Chinese government has developed a number of strategies to minimize the risk of its increasing reliance on imported crude oil, which requires detailed information on the relationship between Chinese oil markets and international oil markets. The interrelation between international oil markets has been widely examined in the existing literature. Sharma (1998) uses a GARCH model to study return volatility of oil price, and find that GARCH model based on GED distribution can display the features of steep-peak and heavy tails better than when it based on normal distribution. Investigating the volatility forecasting of crude oil prices, Wei et al. (2010) capture the volatility features of two crude oil markets—Brent and West Texas Intermediate (WTI) using a greater number of GARCH class models (e.g., FIGARCH, HYGARCH). Besides, Chen et al. (2009) employ data in seven crude oil markets across the world for the period between 1997 and 2007 using a DAG model to measure the volatility of crude oil prices between China and international markets and show that the crude oil price innovations in China are significantly driven by the OPEC and US markets. Reboredo (2011) examine how oil prices co-move for the WTI-Brent, WTI-Dubai, WTI-Maya and Dubai–Maya pairs using several copula models with different conditional dependence structures and time-varying dependence parameters and the findings suggest that crude oil prices are linked with the same intensity during bull and bear markets. Li and Leung (2011) investigate the integration of China into the world oil market using co-integration and granger causality test; however, they didn’t study the volatility spillover effect between Chinese oil market and world oil markets. Much has been written about the co-movements among international oil markets in a static way, but there are not many literatures that comprehensively study how Chinese oil market has been correlated with international markets in a dynamic way. Therefore, in contrast to much of the existing literature, this paper seeks to examine both the static as well as dynamic relation between Chinese oil market and other major international oil markets. Specifically, we wish to extend the existing literature by exploring the co-movement of volatility between Chinese oil market and the main international oil markets. First, based on the Chinese oil market price formation mechanism and fluctuation, this paper uses an asymmetric dynamic conditional correlation multivariate GARCH (DCC-MVGARCH) model proposed by Engle (2002) to explore price fluctuations among Daqing crude oil in China, West Texas, Brent, Dubai oil markets during January 3rd, 1997 and March 18th, 2011. DCC-GARCH is developed from the constant conditional correlation (CCC) GARCH model of Bollerslev (1990). DCC-GARCH models have been widely used to capture the volatility and co-movement features, regarding energy markets (Chang et al. 2011; Lee 2006; Marshall et al. 2009). A major advantage of using this approach is that the detection of possible changes in conditional correlations over time is data dependent when accounting for the time-varying volatility behavior of data series (Lee 2006). After obtaining the dynamic correlation coefficient, we then investigate the variation characteristics of co-movement of China and the world oil market more accurately. Second, EG two-step method is employed to test the co-integration of price of Daqing crude oil and other three international crude oil markets in a long-run perspective. Considering the great fluctuations of oil prices due to diverse oil policies at national level after the financial crisis, it is crucial to engage in profound research on whether changes of co-movement happen among different oil markets, in particular when China is taken into consideration. The remainder of the paper is structured as follows. In the next section we introduce how oil pricing strategies have evolved in China and Sect. 3 presents DCC-MVGARCH model. Section 4 describes the data. Section 5 presents empirical analysis. Discussions are given in Sect. 6. We conclude and provide policy recommendations in Sect. 7.",7
54.0,4.0,Computational Economics,06 April 2016,https://link.springer.com/article/10.1007/s10614-016-9570-7,"Hidden Carbon Emissions, Industrial Clusters, and Structure Optimization in China",December 2019,Shu-Hong Wang,Ma-Lin Song,Tao Yu,,Unknown,,Mix,,
54.0,4.0,Computational Economics,08 September 2016,https://link.springer.com/article/10.1007/s10614-016-9621-0,Measuring the Efficiency of Two-Stage Production Process in the Presence of Undesirable Outputs,December 2019,Yalei Fei,Gongbing Bi,Yan Luo,Unknown,Unknown,Male,Male,"During the period of the “11\({\mathrm{th}}\) Five Year Plan”, China achieved its economic growth at a mean annual rate of 11.2 % that was well above the average of the world, however, the high speed economic growth in China is accompanied by high energy consumption (Zhou et al. 2013). Of the energies, coal is an important one and is used widely, for example, Wang and Nakata (2009) indicated that coal remained to be predominant fuel to generate electricity in China. Not only in the electricity generating process but also in many production processes in China, coal is widely used. Evidence shows that the amount of coal consumption accounts for more than 68 % of the primary energy consumption in China during 2006–2010,Footnote 1 which presents the domination of coal in China. As an important resource, coal brings us both required power and undesirable outputs such as sulfur dioxide, carbon dioxide, and solid waste, etc. How to use coal in an efficient way becomes a hot issue concerned by many scholars and clean-coal is just one of the most efficient technologies to solve the problem of efficient coal utilization. There is already lots of literature on clean-coal research. For examples, Johnson (2004) indicated the significance of clean-coal technology for the climate-change by carbon-sequestration. He also showed the importance to treat the coal ash and use coal in a more efficient way in Johnson (2009). Sen (2010) provided an overview of clean-coal technologies and indicated the importance of clean coal technologies for the minimization of the environmental impacts. Sharma (1994), Sharma and Singh (1995) and Steel and Patrick (2001) separately proposed different approaches to producing clean-coal. The above clean-coal technologies tend to use physical and chemical approaches to improve the efficiency of coal utilization and emissions control. In this paper, we evaluate the efficiencies of different production processes to find the inefficient processes and then improve their efficiencies which may require adjusting scales of inputs or outputs of the production processes. For a typical production process in China, such as gross domestic production (GDP) generating process, it needs to consume energy inputs, coal et al. and non-energy inputs to produce desirable outputs and undesirable outputs. In the process, GDP is a desirable output and sulfur dioxide, carbon dioxide and solid waste, etc. are undesirable outputs. And if the volumes of above undesirable outputs are too large, they should be treated properly to prevent environment pollution. In this way, GDP generating process and undesirable outputs abatement process are linked into an overall process by the intermediate products. And to evaluate the efficiency makes the decision makers get a good knowledge of one production process from economic and environment view. Thus, we will model GDP generating process to measure the efficiencies of the overall process and its two sub-processes by Data envelopment analysis (DEA) approach. DEA, firstly proposed by Charnes et al. (1978), is a widely-used multi-criteria decision making method with multiple inputs and multiple outputs. An advantage of the approach is that it does not require prices of inputs and outputs of a production process when evaluate the performance of the production process. It is sometime difficult to get the prices of undesirable outputs and calculate the disposing cost undesirable outputs. From this point of view, DEA is a good method to evaluate the energy or environmental performance. In previous studies, DEA models were often used to measure the energy or environmental performance, such as CCR model (Charnes et al. 1978), BCC model (Banker et al. 1984) and so on. For examples, using BCC model, Seiford and Zhu (2002) modeled undesirable outputs; Pekka and Mikulas (2003) reviewed four methods and a generalized model to treat undesirable output and calculate the eco-efficiency. In addition to above radial DEA approaches, Zhou et al. (2007) proposed a non-radial DEA model to measure environmental performance with undesirable outputs included in the production process. Bian and Yang (2010) presented five approaches to evaluate the comprehensive performance of energy and environment and selected a proper one through Shannon’s entropy. Bi et al. (2014) used slack-based DEA approach to evaluating the performance of the Chinese thermal power plant where energy inputs, non-energy inputs and desirable outputs, undesirable outputs are all included in the production technology. Although approaches mentioned above take undesirable outputs into account and are proved efficient in modeling single-stage process, they may be not appropriate to model a two-stage process because of ignoring the connection of the two stages. And a better modeling approach is to substitute a two-stage DEA model for single-stage model and consider the connection between the two stages in the two-stage process, such as network DEA technology. Some typical researches on network DEA are listed as follows. For examples, Färe and Grosskopf (2000) proposed the first Network DEA model; Liang et al. (2006) considered an extended two-stage network case with new-added inputs included in the second stage; Kao and Hwang (2008) indicated that it ignored the sequential relationship of those linked stages and was improper to take the network process as a “black box”, then they proposed a new approach to measuring and decomposing the efficiency of the two-stage production process by considering the sequential relationship between the two stages. Different from Kao and Hwang (2008), Liang et al. (2008) used the Stackelberg game approach to decomposing the efficiency of the two-stage process. Kao (2009) proposed to transform a complicated network structure into a series of stages, with each stage containing one or multiple parallel processes, and then to calculate the efficiency of the network structure. Different from radial decomposition, Chen et al. (2009) used an additive DEA efficiency decomposition approach to derive the efficiency of each stage in the two-stage process. Li et al. (2012) proposed a type of extended two-stage network structure DEA model to measure and decompose the efficiency of the producing process. These above models can be used to measure efficiencies of the different two-stage production process, and some of them can be used to decompose that. Based on above typical researches of single stage production processes and network structure, we propose our models to measuring the efficiency of a two-stage process that contains both the process of producing undesirable outputs, desirable outputs and that of disposing of undesirable outputs. Our proposed models is different from the common two-stage model and the single-stage efficiency model since it takes the undesirable outputs, the internal structure of the DMU, the free-setting weights of the two sub-stages etc. into account. In addition, the modeling method is consisted with the decision objective of maximizing the efficiencies of GDP generating and pollutant abatement. The following sections are organized as follows. In Sect. 2, we describe the two-stage production process to be evaluated and review some previous efficiency measure models. Based on these models, we then propose our models to measure the efficiencies of two single stages and the overall stage of the two-stage production process. In Sect. 3, through our models, we calculate and analyze efficiencies of thirty administrative regions in China. Finally, we conclude and present the future research directions.",3
54.0,4.0,Computational Economics,06 January 2017,https://link.springer.com/article/10.1007/s10614-016-9644-6,An Outlook on the Biomass Energy Development Out to 2100 in China,December 2019,Zhihui Li,Xiangzheng Deng,Wei Qi,Unknown,Unknown,,Mix,,
54.0,4.0,Computational Economics,13 January 2017,https://link.springer.com/article/10.1007/s10614-016-9645-5,The Usage Analysis and Policy Choice of CNG Taxis Based on a Multi-stage Dynamic Game Model,December 2019,Xiaoyao Xie,Yuhong Wang,Xiaozhong Li,Unknown,Unknown,Unknown,Unknown,,
54.0,4.0,Computational Economics,08 February 2017,https://link.springer.com/article/10.1007/s10614-017-9663-y,Revealing Energy Over-Consumption and Pollutant Over-Emission Behind GDP: A New Multi-criteria Sustainable Measure,December 2019,Xiang Ji,Jiasen Sun,Qianqian Yuan,,Unknown,Unknown,Mix,,
54.0,4.0,Computational Economics,06 June 2017,https://link.springer.com/article/10.1007/s10614-017-9700-x,"Fiscal Decentralization, Economic Growth, and Haze Pollution Decoupling Effects: A Simple Model and Evidence from China",December 2019,Liangliang Liu,Donghong Ding,Jun He,Unknown,Unknown,,Mix,,
54.0,4.0,Computational Economics,21 June 2016,https://link.springer.com/article/10.1007/s10614-016-9600-5,Diversification Measures and the Optimal Number of Stocks in a Portfolio: An Information Theoretic Explanation,December 2019,Adeola Oyenubi,,,Unknown,Unknown,Unknown,Unknown,,
54.0,4.0,Computational Economics,01 August 2017,https://link.springer.com/article/10.1007/s10614-017-9722-4,Buying on Margin and Short Selling in an Artificial Double Auction Market,December 2019,Xuan Zhou,Honggang Li,,,Unknown,Unknown,Mix,,
54.0,4.0,Computational Economics,04 August 2017,https://link.springer.com/article/10.1007/s10614-017-9723-3,Effect of Information Exchange in a Social Network on Investment,December 2019,Ho Fai Ma,Ka Wai Cheung,Kwok Yip Szeto,,,Unknown,Mix,,
54.0,4.0,Computational Economics,05 February 2018,https://link.springer.com/article/10.1007/s10614-018-9792-y,Dynamic Interaction Between Asset Prices and Bank Behavior: A Systemic Risk Perspective,December 2019,Aki-Hiro Sato,Paolo Tasca,Takashi Isogai,Unknown,Male,Male,Male,"Understanding the interplay among banks through several channels is a crucial issue in the globalized world economy (Beale et al. 2011; Montagna and Lux 2013). In general, banks obtain profits from the difference between deposit interest rates and interest rates in interbank markets, stock markets, credit markets, and so on. Of course, money flows and interest rates are deeply interrelated to international economic conditions. It is recognized that systemic risks are created by interconnections among banks. Helbing argues that systemic failures and extreme events are consequences of the highly interconnected systems and networked risks. He proposes a general theory to analyze, understand, and manage systemic failures for various types of fields in socioeconomic–technological–environmental systems under a framework of global systems science (Helbing 2013). He further addresses a list of common drivers of systemic instabilities that may destabilize anthropomorphic systems over time. The following drivers are to be considerably significant: (1) increasing system sizes, (2) reduced redundancies due to attempts to save resources (implying a loss of safety margins), (3) denser networks (creating increasing interdependencies between critical parts of the network), and (4) a high pace of innovation (producing uncertainties or ‘unknown unknowns’). These systemic risks probably stem from the positive feedback loop among financial markets and banks’ interactions. The risks can be formed by several reasons, such as leverage trading, trend followers’ trading, loss cut trading, bank bankruptcies, bunched sales, the housing market and the real economy, employment, excess concentration of wealth, a trade imbalance, political power, extreme low interest rates, excessive or lack of regulation, and so on. Several types of nonlinear positive feedback mechanisms can concurrently trigger serious crashes that damage all the financial systems. This study collocates with that stream of works that aim to estimate systemic risks among banks under the exposures of risk assets traded in the stock market (Tasca and Battiston 2016). Some existing studies concern risk propagation through lending and borrowing networks. This approach focuses on interactions of debt and credit among banks. For example, Iori et al. analyze the systemic risk in interbank money markets by simulating the banks’ lending activities (Iori et al. 2006). Their simulation model assumes that banks carry a liquidity risk caused by the maturity gap between funding and investment activities. The model introduces a feature according to which banks pool this risk and further creates the potential for one bank’s crisis to propagate through the system. Furthermore, the heterogeneity of banks is analyzed, which can cause knock-on effects in shock propagation, while the interbank market comprised of homogeneous banks tends to stabilize absorbing shocks. In this model settings, two types of banks are assumed: those with positive cash and those with negative cash. Accordingly, they are classified as potential lenders and potential borrowers. Banks invest their money first and lend the remainder. The total demand in the market does not always match the total supply. Thus, a default risk exists for banks due to the shortage of liquidity, and shocks can be propagated through the system. As for the performance of the interbank market in its role as a safety net, the insurance role of interbank lending prevails when banks are homogeneous; higher reserve requirements can lead to a higher incidence of bank failures. When banks are heterogeneous in average liquidity or average size, contagion effects may arise. Iori et al. found that such effects can be of a direct (i.e., knock-on from a failing bank to its direct creditors) and an indirect (i.e., causing critically in the system as a whole) nature. Despite the potential to create contagion, Iori et al. insist that inter-bank lending always seems to stabilize the system: The elapsed time before the first failure is always observed to increase with connectivity. Their simulation results also indicate that heterogeneity alone can contribute to instability. Gai and Kapadia study probability and the potential impact of contagion, which are influenced by aggregate and idiosyncratic shocks, change in the network structure, and asset market liquidity (Gai and Kapadia 2010). A model of contagion in arbitrary financial networks is developed by using a directed and weighted network model to express the widespread transmission of shocks through a numerical simulation of shock transmission. Banks are linked together by their financial claims on each other, including through interbank markets and payment systems. They model contagion stemming from unexpected shocks in complex financial networks with an arbitrary structure and then use numerical simulations to illustrate and clarify the intuition underpinning our analytic results. The result of the simulation analysis suggests that financial systems exhibit a robust-yet-fragile tendency: Although the probability of contagion may be low, the effects can be extremely widespread when problems occur. Adverse aggregate shocks and liquidity risk amplify the likelihood and extent of contagion. Gai and Kapadia also clarified why the resilience of the system in withstanding fairly large shocks before 2007 should not have been taken as a reliable guide to the system’s future robustness. It means that we need more flexible assumptions when we build network-based models. The approach provides the first step toward modeling contagion risk when the true links are unknown. Gai and Kapadia suggest a further extension of the analysis by relaxing the assumption that the defaulting bank is randomly selected and, considering the implications of targeted failure that affect big or highly connected interbank borrowers. As mentioned, added realism could also be incorporated into the model by using real balance sheets for each bank or endogenizing the formation of the network. Such an extension of the model would be beneficial from a systemic risk research viewpoint. Haldane and May study possible effects of risk optimization by individual financial institutions on the stability of the system as a whole (Haldane and May 2011). They explore the interplay between complexity and stability in deliberately simplified models of financial networks to find policy lessons with the explicit aim of minimizing systemic risk. They claim that the network dynamics of what might be called ‘financial ecosystems’ has parallels with ecology, where too much complexity implies instability. The well-known arbitrage pricing theories (APTs) and other derivative pricing theories often assume perfect competition, market liquidity, no arbitrage and market completeness. Crucially, these conditions are not always satisfied; therefore, trading activities that assume these conditions can destabilize markets, having a possible effect on the dynamical behavior, while such activities seem to be successful at the initial stage. Haldane and May also delve into the shock propagation mechanism, applying network system approach originally developed in ecology. A financial system is expressed as a network in which many banks are connected with credit linkage, forming an inter-bank money market. An initial shock that arose in some node are transmitted to other connected nodes when their shock absorbing capacity of a node is insufficient to the incoming shock. In addition the liquidity factor plays a major role in the shock propagation. The losses in the value of bank external assets, caused by a generalized fall in market prices, such liquidity shocks amplify as more banks fail accordingly. Thus, relatively small initial liquidity shocks have the potential to make strong contributions to systemic risk. Iori et al. also emphasize the cascading effect of shock propagation, in which diminished availability of interbank loans caused serial failure of liquidity funding by banks. These complicated interactions between nodes in a network cannot be clarified by the traditional economic theory. The traditional rationale for setting regulatory capital/liquidity ratios is that idiosyncratic risks are reduced to the balance sheets of individual banks. Prudential regulation is following in the footsteps of ecology, which has increasingly drawn on a system-wide perspective when promoting and managing ecosystem resilience. The scientists also listed two policy implications from their topological network analysis: the diversity across the financial system and the modularity within the financial system. They warn that banks’ balance sheets and risk management systems became increasingly homogeneous; homogeneity bred fragility. As for the modularity within the financial system, it protects the systemic resilience of both natural and constructed networks by limiting the potential for cascades. They emphasized the importance of encouraging modularity and diversity in banking ecosystems as a means of buttressing systemic resilience. One of the authors (P.T.) proposed DebtRank to measure default risk of banks. The paper assumes that bankruptcy may influence the balance sheet situation of other banks and that contagion effect may happen (Battiston et al. 2012). DebtRank is one of the indicators to calculate the risk of the contagion effect for each bank. Reducing procyclicality and promoting countercyclical buffers is one of the most important issues in the Basel III Accords (Basel Committee on Banking Supervision 2011). It provides a message that “One of the most destabilizing elements of the crisis has been the procyclical amplification of financial shocks throughout the banking system, financial markets and the broader economy.” In this study, we focus on correlations among financial assets. This may play a role of common factors and create procyclicality. To do so, we study interactions between banking systems and the financial market. Financial prices fluctuate and show volatility clustering and volatility synchronization. In fact, fluctuations of financial prices seem to be random, however, volatility of financial assets are sometimes synchronous. If many banks have positions in financial assets, then their capital adequacy ratios sometimes may vary synchronously. This is a common factor effect of financial assets in balance sheet. In order to investigate this scenario, we construct an agent-based model consisting of banks and the financial market. The link via a underlying common factor is widely observed in the financial market. Shocks prevail over the whole market within a short period of time through arbitrary transactions by market participants. Individual assets return; therefore, they tend to synchronize in terms of volatility fluctuation. Large-scale volatility fluctuations are observed at specific times, resulting in a market shock event. Asset price fluctuation in the financial markets has been studied by many researchers (Sornette 2003; Bouchaud and Potters 2000). Stochastic models and agent-based models are well studied (Miccichè et al. 2002; Preis et al. 2006). Some researchers paid significant attention to the network effects in the financial markets (Bonanno et al. 2004; Huang et al. 2013). According to empirical studies on financial time series, asset price fluctuations follow fat-tailed distributions. This is often modeled as a random number drawn from a Student t distribution (Beale et al. 2011; Owen and Rabinovitch 1983; Sato 2012). However, since asset price fluctuations are generated from trading by market participants, high volatility regimes are not independent of banks’ behavior. How does banks’ behavior affect financial markets, and how do asset price fluctuations influence banks’ behavior? This forms a circular causality. This is the main question of this study. To answer this question, we consider a model of interaction among banks with an agent-based model. We assume that banks have lending and borrowing relationships with other banks and invest their money in an asset. We focus on two viewpoints: the capital adequacy ratio and the interaction between the lending and borrowing network and financial markets. One of the authors (T.I.) analyzed the Japanese stock market by applying GARCH model to individual stock returns separately (Isogai 2014). The results show the market-wide synchronization of extreme volatility (larger than the 95th percentile of the empirical distribution of individual volatility), which occurred mostly during crisis periods. The purpose of this paper is to clarify the influence of procyclicity. We assume that the financial assets on the balance sheet play a role as a common factor in the banking system. From an agent-based model consisting of a banking system and the financial market, we construct a model and measure the correlation of the capital adequacy ratio among banks. We find that the synchronous behavior of the capital adequacy ratio was influenced by the financial markets. The rest of the paper is organized as follows: Sect. 2 defines a banking system, the behavior of the market participants and a model of chain-reaction bankruptcy. Section 3 shows the empirical evidences of banks’ balance sheets. Section 4 exhibits simulation results obtained from the proposed agent-based model. Section 5 discusses limitations of the model. Section 6 includes our conclusions.",4
54.0,4.0,Computational Economics,16 November 2019,https://link.springer.com/article/10.1007/s10614-019-09946-3,Retraction Note to: Analyses of Economic Development Based on Different Factors,December 2019,Goran Maksimović,Srđan Jović,Marina Jovović,Male,Male,Female,Mix,,
55.0,1.0,Computational Economics,26 September 2018,https://link.springer.com/article/10.1007/s10614-018-9828-3,Increment Variance Reduction Techniques with an Application to Multi-name Credit Derivatives,January 2020,Pierre Rostan,Alexandra Rostan,François-Éric Racicot,Male,Female,Unknown,Mix,,
55.0,1.0,Computational Economics,12 December 2018,https://link.springer.com/article/10.1007/s10614-018-9875-9,Estimating Non-stationary Common Factors: Implications for Risk Sharing,January 2020,Francisco Corona,Pilar Poncela,Esther Ruiz,Male,Female,Female,Mix,,
55.0,1.0,Computational Economics,14 December 2018,https://link.springer.com/article/10.1007/s10614-018-9877-7,Invertibility and VAR Representations of Time-Varying Dynamic Stochastic General Equilibrium Models,January 2020,Maddalena Cavicchioli,,,Female,Unknown,Unknown,Female,"Recently, DSGE models have gained a central role in formalizing the mechanisms of propagation of economic shocks. Solutions of linearized DSGE models can be expressed in linear state space form using several solution methods. However, it is known that economic shocks of DSGE models cannot always be recovered from VARs. This difficulty is related to the problem of noninvertibility of economic models. The invertibility condition serves to infer economic shocks and impulse responses by a vector autoregression of the observable variables. Some results on the invertibility for time-invariant DSGE models have been already established in the literature. Fernández-Villaverde et al. (2007) show that if a certain matrix is stable (i.e., its eigenvalues are strictly less than one in modulus), there exist infinite order VAR representations of time-invariant DSGE models. The condition of stability of that matrix is called the poor man’s invertibility condition. Franchi and Vidotto (2013) prove that a finite order VAR representation of a time-invariant DSGE model exists if and only if the matrix defined in Fernández-Villaverde et al. (2007) is nilpotent (i.e., its eigenvalues are all equal to zero). Franchi and Paruolo (2015) present a necessary and sufficient condition for noninvertibility of time-invariant DSGE models. Some discussions on the identification of DSGE models can be found in Consolo et al. (2009) and Reicher (2015). Aruoba et al. (2006) compare solution methods for dynamic equilibrium economies, and document the performance of such methods in terms of computing time, implementation complexity and accuracy. An estimation of the approximate nonlinear solution of a small DSGE model on euro area data is given in Amisano and Tristani (2010). A nice paper of Giacomini (2013) presents a selective review of the literature on the econometric relationship between time-invariant DSGE and VAR models from the point of view of estimation and model validation. It is shown that the mapping consists of three stages. In the first stage, the equilibrium conditions from a DSGE model are mapped into a linear state-space model; in the second, the state-space model is represented as a VAR with an infinite number of lags; in the last stage, the VAR\((\infty )\) is either shown to be a VAR with a finite number of lags if the model satisfies some testable conditions, or the VAR\((\infty )\) is approximated by a finite order VAR. This survey paper also illustrates some open research questions, such as understanding the effects of log-linearization on estimation and identification, and incorporating into DSGE models information from atheoretical models and from real data. While convenient, assuming time-invariant coefficients and variances in the state space form turns out to be quite restrictive in capturing the evolution of economic time series. As remarked in Bekiros and Paccagnini (2013), the time-varying properties in VAR or DSGE models capture the inherent nonlinearities and the adaptive underlying structure of the economy in a robust manner. For this reason, in this paper I study VAR representations of multivariate time-varying DSGE models, and propose necessary and sufficient conditions for the invertibility of the process involved. This is an important contribution to draw inference when recovering shocks of a DSGE model from a VAR\((\infty )\) form with time-varying coefficients. In addition, VAR representations are a powerful tool for empirical validation of macroeconomic models. In fact, they are essentially more easy statistical models to estimate and can be used to evaluate the impact of economic shocks on key variables. Furthermore, VAR representations also allow shifts in coefficients or volatility, and are useful for forecasting. Since time-varying models appear to be all the rage, my analysis would potentially be useful to modellers because it provides testable conditions of checking whether certain classes of such models are invertible. For some applications on monetary policy of time-varying models, I refer to Canova and Gambetti (2009), Clark (2009), Cogley and Sargent (2001), Cogley et al. (2010), Primiceri (2005), Sims and Zha (2006). Exact likelihood computation for nonlinear DSGE models, where the variance of exogenous innovations is subject to stochastic regime shifts, can be found in Amisano and Tristani (2011). As a special case, I focus on DSGE models whose coefficients are driven by a Markov chain. As shown, for example, in Farmer et al. (2009) and Liu et al. (2011), it is very important to study this class of models. In fact, these authors examine the sources of macroeconomic fluctuations by estimating a variety of richly parameterized DSGE models within a unified framework that incorporates regime switching in shock variances and in the inflation target. As an empirical application, they show that the model that best fits the US inflation data is the one with synchronized shifts in shock variances across two regimes. In this setting, I propose tractable methods to check the existence of VAR representations for Markov switching (MS) DSGE models, that is, their invertibility. My results imply that every MS DSGE model can be studied by techniques and methods from the theory of MS VARMA models. This makes MS DSGE models special in terms of invertibility problem since the theory of MS VARMA has been largely developed in the literature. Results concerning the stationarity, estimation, and consistency of MS VARMA models, along with other statistical inference procedures, have been already established and are well-known to modellers. Bickel et al. (1998) and Douc et al. (2004) have described asymptotic properties of the maximum likelihood estimators of vector MS autoregressions. Ergodicity, estimation and stationarity of multivariate MS autoregressions and consistency of the maximum likelihood estimator of model parameters have been studied in Hamilton (1990), Francq and Roussignol (1998), Yang (2000), and Francq and Zakoïan (2001). See also Cavicchioli (2014c), where explicit matrix expressions of the maximum likelihood estimator of the parameters in multivariate MS VAR models and MS models with ARCH disturbances (MS VARCH) have been derived. Higher-order moments and asymptotic Fisher information matrix of MS VARMA models are provided in Cavicchioli (2017a, b) respectively. I refer to Hamilton (1994) and Krölzig (1997) as basic textbooks on MS VAR models together with statistical inference and application to business cycle analysis. For basic definitions and maximum likelihood estimation of general MS state-space models see Billio and Monfort (1998). The asymptotic behaviour of Markov systems on general state spaces has been studied by Vassiliou (2014). The paper is organized as follows. Section 2 studies VAR representations of multivariate time-varying DSGE models and proposes necessary and sufficient conditions for the invertibility of the process involved, that is, for the process to have a VAR(\(\infty \)) representation with time-varying coefficients. Furthermore, I derive a necessary and sufficient condition for a process driven by a time-varying DSGE model to have a moving average representation (generally of infinite order) with time-varying coefficients. In Sect. 3, the focus is on DSGE models whose coefficients are driven by a Markov chain. The main contribution is to provide conditions under which the shocks to the system can be recovered from the history of the observables and the history of the realized states of the Markov chain. Specifically, I derive two sufficient conditions, based on matrix algebra (hence easily testable), for the invertibility of the process driven by a MS DSGE model. Such matrix conditions improve computational performance since they are readily programmable and greatly reduce the computational cost. Section 4 is devoted to derive explicitly MS VARMA representations of MS DSGE models. Then I describe time-varying autoregressive representations (generally of infinite order) for MS DSGE models. In this setting I propose further sufficient conditions for the invertibility of the process involved in a MS DSGE model. The results are illustrated via computations and examples in Sect. 5. Section 6 concludes. Proofs are given in the “Appendix”.",
55.0,1.0,Computational Economics,17 December 2018,https://link.springer.com/article/10.1007/s10614-018-9876-8,Estimation of STAR–GARCH Models with Iteratively Weighted Least Squares,January 2020,Murat Midiliç,,,Male,Unknown,Unknown,Male,"Nonlinear models with heteroskedastic errors have been used in analyzing financial time series data for a long time. One special type of models under this family is the Smooth Transition Autoregressive (STAR) model with conditional variance. Unlike STAR models, which are estimated by nonlinear least squares (NLS), these models are estimated by the maximum likelihood estimation (MLE), or quasi-maximum likelihood estimation (QMLE). However, in real world applications the numerical features of these models and computational aspects of the method used may lead to a complicated estimation procedure. For both NLS and MLE, the experience shows that the final estimates are highly dependent on the starting values [for STAR models, see Teräsvirta (1994), for STAR models with heteroskedastic variance, see Lundbergh et al. (1999) and van Dijk et al. (2002)], the bias of the slope parameter estimator is higher than the biases of other parameter estimators, and estimation of the slope parameter gets more difficult as the parameter value increases. This study presents a potential solution to these problems by using Iteratively Weighted Least Squares (IWLS) and compares its performance with other established algorithms. As pointed out by Teräsvirta (1994), Lundbergh et al. (1999), van Dijk et al. (2002); one problem with the estimation of the STAR type models is to find sensible starting points for the algorithm. Due to the highly nonlinear nature of the STAR type models, estimation results are sensitive to the starting points (Chan and McAleer 2003). Moreover, according to Teräsvirta (1994) and Chan and McAleer (2002), the degree of difficulty in estimation changes depending on the type of the transition function used in the model. A comparison of the models with logistic transition function and exponential transition function shows that the STAR models with logistic transition function proved to be more problematic than the other. Simulation studies of Chan and Theoharakis (2011) show that the main reason for these problems seem to be the complex nature of the log-likelihoods of these functions stemmed from the slope parameter(s). For the STAR–GARCH model, Chan and Theoharakis (2011) show how the log-likelihood function behave around the optimum values of the parameters. They illustrate graphically that the log-likelihood function might be flat for exponential transition functions or lumpy for logistic transition functions around the optimum value of the slope parameter(s). These results underline the need for solution methods that are robust to local optima in estimating the STAR-type models. One common feature of the studies that use the STAR models with conditional variance is that the models are estimated by the maximum likelihood (ML).Footnote 1 Although, in theory the ML estimation is the correct way to handle the estimation problem and gives consistent estimates; from a numerical point of view, the algorithm used in solving the ML can be sensitive to starting values and might have a poor performance in dealing with local optima issue; so the algorithm might be the reason for the problems described above. Therefore, an approach targeting robustness of the ML estimation of nonlinear models would be a potential solution to the referred problems. One such an approach is the IWLS estimation of nonlinear models with conditional variance. Mak (1993) and Mak et al. (1997) show that the maximum likelihood problems can be transformed into a problem that can be solved by the IWLS, and the performance of the algorithm is compatible with or better than the traditionally used ML algorithms in the literature for the set of models we are interested in. The purpose of this study is to show the performance of the IWLS algorithm in estimating STAR models with conditional variance in a basic setup. For this purpose, the STAR–GARCH model is chosen to be the model used in the study. Monte Carlo (MC) simulations are carried out with a STAR–GARCH model to show the performance of the algorithm conditional on different initial values. Robustness of the algorithm to initial value selection is checked by carrying out simulations with randomly generated initial values and initial values provided by a heuristic algorithm, which has been used in the STAR model estimation literature. Brooks et al. (2001) show that there might be significant differences in the results from different software for the same problem. Therefore, the performance of the IWLS is also compared with other functions that are commonly used for the maximum likelihood and quasi-maximum likelihood estimations. These are fmincon function of MATLAB, and maxLik function of R. Special attention is paid to the slope parameters of the transition function since in practice, this parameter is found to be the most difficult to estimate in the literature. In order to account for the effect of the dynamics in the variance component of the model, simulations are carried out with several GARCH specifications. Practical implications of using the IWLS algorithm are studied with empirical applications. The contribution of the study to the literature is twofold. First, in a basic setup, it shows that for the STAR–GARCH model, it is possible to have slope parameter estimators with smaller bias and variance with the IWLS algorithm. Second, as an empirical contribution, daily exchange rates and stock indices are forecast by the STAR–GARCH model; thus, the study contributes to the exchange rate and stock index forecasting discussions in the literature. Simulation studies convey three key results. The first result is that when the real value of the slope parameter is low, the IWLS performs better than other methods in estimating the parameter while the IWLS does not perform worse in estimating the slope parameter when the real value of the parameter is high. The second result states that for the IWLS algorithm, bias of the slope parameter estimator from randomly generated initial values is smaller than the bias of the estimator received from the benchmark initial value selection method. According to the third result, estimation performances of the methods change as the real value of the persistency parameters in the variance equation change. In cases where performances of other methods deteriorate, the IWLS performs better in estimating these parameters. In the empirical part of the study, daily US Dollar (USD)/Australian Dollar (AUD) exchange rate and the Financial Times Stock Exchange Small Cap (FTSE SC) returns are forecast with the STAR–GARCH model. According to the out-of-sample forecast error performance and prediction accuracy tests, the IWLS algorithm performs better than the benchmark random walk (RW) model as well as the competing algorithms. For the exchange rate forecasts, the statistical significance of the performance of the IWLS algorithm is robust to different predictive accuracy tests while robustness is not observed for the stock index forecasts, though most of the tests give significant results. Empirical exercises suggest that the traditional methods used in the literature might give misleading results in the sense that even though the smooth transition model performs better than the benchmark model; the algorithms cannot demonstrate the performance. The IWLS is shown to correct this problem in practice. The study is organized as follows. The model and the IWLS algorithm are described in the next section. Section 3 presents the simulations, summarizes the results, and discusses their implications. In Sect.  4, the STAR–GARCH model is used in empirical illustrations to forecast daily exchange rate and stock index returns. The final section summarizes and concludes.",1
55.0,1.0,Computational Economics,02 February 2019,https://link.springer.com/article/10.1007/s10614-019-09880-4,A Computational Method Based on the Moving Least-Squares Approach for Pricing Double Barrier Options in a Time-Fractional Black–Scholes Model,January 2020,Ahmad Golbabai,Omid Nikan,,Male,,Unknown,Mix,,
55.0,1.0,Computational Economics,06 February 2019,https://link.springer.com/article/10.1007/s10614-019-09885-z,Entropy and Efficiency of the ETF Market,January 2020,Lucio Maria Calcagnile,Fulvio Corsi,Stefano Marmi,Male,Male,Male,Male,"The process of incorporating the information into prices does not occur instantaneously in real markets, giving rise to small inefficiencies, that are more present at the high frequency (intraday) level than at the low frequency level. Since inefficiencies are always present, rather than efficiency in absolute terms it is more interesting to study the notion of relative efficiency, i.e., the degree of efficiency of a market measured against the benchmark of an idealised perfectly efficient market. In this paper we investigate to what extent assets depart from perfect efficiency and to what degree the known sources of regularities (the intraday pattern, the volatility and the microstructure) contribute to the creation of inefficiencies. The latter analysis, to the best of our knowledge, is carried out in this article for the first time in the literature on market efficiency. As tool to measure the randomness of the time series, we employ the Shannon entropy of 2-symbol and 3-symbol discretisations of the data. In the 2-symbol discretisation (one symbol for the positive returns, the other for the negative returns) the intraday pattern and the volatility have no effect, since they are modelled as multiplicative factors. The market microstructure, instead, gives rise to linear autocorrelation and has been modelled in the literature as autoregressive moving average (ARMA) processes. By means of a theoretical study on the entropy of AR(1) and MA(1) processes, and a more empirical study on ARMA residuals, we develop two innovative methodologies to assess the inefficiency beyond the microstructure effects. In the 3-symbol discretisation, contrary to existing literature that fixes absolute thresholds, we propose a rather general and flexible approach based on the tertiles of the return distribution. Such a definition has many advantages, since, unlike a fixed-threshold symbolisation scheme, it adapts to the distribution. Our flexible tertile symbolisation can be applied not only to the raw return series, but also to series of processed returns, such as the volatility-standardised returns and the ARMA residuals. Using the tertile symbolisation, we investigate to what degree the intraday pattern, the volatility and the microstructure contribute to create regularities in the return time series. Summarising, our main contributions and the main findings of the application of the 2-symbol and 3-symbol discretisation are the following: (i) we show how microstructure effects can be effectively discounted when measuring efficiency from high frequency data, by developing two innovative methodologies, one based on the theoretical results we find for the entropy of AR(1) and MA(1) processes, the other more empirically-driven for general ARMA(p, q) processes; (ii) we introduce a flexible ternary data symbolisation procedure, based on the distribution tertiles, which overcomes the serious shortcomings of fixed-threshold discretisation schemes used in previous market efficiency literature; (iii) the ternary symbolisation allows us to quantify the regularity due to the intraday pattern, the volatility and the microstructure effects—amounting on average to 18%, 62% and 20%, respectively—providing the evidence, in particular, of the necessity to filter out the volatility when studying market efficiency. In the literature only few papers have studied the relative efficiency of financial markets and ranked assets according to their efficiency degree. In Cajueiro and Tabak (2004), Giglio et al. (2008), Shmilovici et al. (2003), Shmilovici et al. (2009), Risso (2009), Oh et al. (2007) different tools were used to measure it as distance from perfect randomness, such as the Hurst exponent, algorithmic complexity theory, a variable order Markov model, Shannon entropy and Approximate Entropy. The great majority of this literature analyses daily data (an exception is Shmilovici et al. (2009)), while our study is focused on intraday high frequency data and their peculiarities. In Giglio et al. (2008), Shmilovici et al. (2003), Shmilovici et al. (2009) return data are symbolised with a three-symbol discretisation with absolute thresholds, which in our opinion introduces some redundancy due to the long-memory properties of the volatility. A ternary discretisation with fixed thresholds incorporates some predictability in periods of high or low volatility, even under the null assumption of no correlation among the returns. When we deal with ternary discretisations of high frequency returns in Sect. 5, we will pay great attention to the removal of intraday patterns and long-term volatility. The paper is organised as follows. In Sect. 2 we introduce the Shannon entropy and present the theoretical study on the entropy of the AR(1) and MA(1) processes, in Sect. 3 we present the data, Sect. 4 presents the analyses performed with the binary symbolisation of the data, Sect. 5 reports on the analyses done with the ternary symbolisations and Sect. 6 concludes. “Appendix A” details on the entropy estimator used, “Appendix B” contains further details on the theoretical entropy of the AR(1) and MA(1) processes and the proofs of the propositions stated in Sect. 2.3 and “Appendix C” reports on the data cleaning procedures.",4
55.0,1.0,Computational Economics,12 March 2019,https://link.springer.com/article/10.1007/s10614-019-09883-1,Static Hedges of Barrier Options Under Fast Mean-Reverting Stochastic Volatility,January 2020,Jeonggyu Huh,Jaegi Jeon,Yong-Ki Ma,Unknown,Unknown,Male,Male,"Hedging exotic options in financial markets is a pivotal work for managing the financial instruments because risk may take quite a toll thereon. Hedging methods are classified into two categories: dynamic hedging methods and static hedging methods. The dynamic hedging method continuously adjusts the weights in a portfolio consisting of risky assets and risk-free assets as time passes. This dynamic adjustment may be problematic in the case of illiquid markets or in circumstances in which the option sensitivities become fickle. For example, the delta of an up-and-out call option is highly sensitive to the movement of the risk asset when it is close to the barrier and the option is on the verge of expiration. In addition, the idea of dynamic hedges of financial derivatives has some obvious shortcomings incurred by the presence of transaction costs. These problems have motivated the development of so-called static hedging strategies, which avoid continuous dynamic adjustments of the hedging portfolio. Static hedging is to form a portfolio of vanilla options with various strikes and maturities whose weights are fixed all the time. Static hedges may cost less than dynamic hedges when the transaction costs are large. For a barrier option with high gamma, the dynamic hedging portfolio has to be adjusted often, which in turn makes the transaction costs accrue as much. Especially, Engelmann et al. (2006) conducted an empirical comparison of dynamic hedges with static hedges of barrier option based on the local volatility model. They showed that some static hedges can perform better than dynamic hedges and the static hedge proposed by Nalholm and Poulsen (2006) is robust across all the market scenarios. However, they did not consider the influence of transaction costs on the hedging performance. This might have drawn conclusions different from the study of Engelmann et al. (2006). Most researches on static hedges is concerned with developing algorithms for numerical computation thereof. Derman et al. (1995)’s method (DEK method) was proposed to hedge a large class of barrier options statically when vanilla options are available for any maturity. The DEK method is to set up a portfolio of vanilla options which replicates a barrier option as close as possible. However, its result is only valid in very special cases satisfying constant volatility assumption and applying it directly to more advanced models seems rather difficult. For example, Toft and Xuan (1998) showed the DEK method performs inadequately in an environment of stochastic volatility. Fink (2003) tried to improve Toft and Xuan’s result by matching the barrier option on time–volatility grid. Nevertheless, the options with the same maturity but different strike prices in Fink’s study produce a very ill-conditioned problem causing potentially very large option positions. Many articles tried to supplement the method with, for instance, the singular value decomposition or optimization-based arguments, to approximate the static hedge with circumventing the problem (Giese and Maruhn 2007; Nalholm and Poulsen 2006). Although these approaches relieve it, they fail to provide an intuitive explanation for the solution and still run into the same numerically ill-conditioned problem. This is indeed troublesome because it may lead to escalation of transaction costs (refer to Sect. 2.2 for a detailed explanation). Extensive research has corroborated that the volatility of stock price is not constant over time. Engle (1982) and Bollerslev (1986) introduced the family of autoregressive conditional heteroskedasticity and generalized autoregressive conditional heteroskedasticity, respectively, to describe the evolution of the volatility of the stock price in discrete time setting, and showed that econometric tests of these models reject the assumption of constant volatility and find evidence of volatility clustering over time. In a continuous time setting, Nandi (1998) demonstrated a posteriori the importance of price-volatility correlation when hedging options on the \( S \& P~500\) and found that the Heston model produces better hedges than the Black–Scholes model (BS model). Also, transaction costs are as important as stochastic volatility. That is because, in reality, transaction costs might burden an exorbitant expense, a byproduct of the above trading strategies. Taking the costs into consideration, Leland (1985) obtained a reformed option pricing formula for discrete hedging executed at regular intervals in the face of market frictions. There are also articles studying discrete hedging in the presence of transaction costs when the hedge is fixed unless the current portfolio deviates from the optimal holdings to a certain extent (see Hoggard et al. 1994). However, to the best of knowledge of the authors of this paper, there has been no literature on the influence of the transaction costs in static hedges. The contribution of this paper is basically an extension of the static hedging method of Derman et al. (1995) on the BS model to stochastic volatility models without causing any ill-conditioned problem. We opt for stochastic volatility models driven by a fast mean-reverting process, which enables us to apply an asymptotic method developed by Fouque et al. (2003) to static hedges for barrier options. This method converts static hedging on time–volatility grid into the problem of designing two simpler static hedges on time grid. Our approach is an effective means to statically replicate the option under stochastic volatility and to improve the approach of Fink (2003). Numerical studies shows the superiority of the proposed method to Fink’s method in terms of performance of static hedges. Also, it clearly shows that our method ensures numerical stability, that is, the weights of the portfolio neither blow up nor fluctuate, and thus solves the problem of transaction costs. Although we consider only the up-and-out European call option, the static hedges for barrier options of other types can be obtained in the same fashion. The rest of this study is organized as follows. The following section reviews the static hedging approaches developed by Derman et al. (1995) and Fink (2003), respectively. In Sect. 3 we formulate the stochastic volatility model and show how the static replication portfolio is formulated thereunder using a perturbative analysis. In Sect. 4, we show how accurate our static hedging method is. Detailed numerical examples show that our method improves static hedging performance compared to Fink’s method in Sect. 5. The final section concludes the study.",
55.0,1.0,Computational Economics,27 March 2019,https://link.springer.com/article/10.1007/s10614-019-09889-9,Modeling Technique Based on the Ranges of Values: Implementation Using Conventional Regression Method,January 2020,Arthur Yosef,Eli Shnaider,,Male,Female,Unknown,Mix,,
55.0,1.0,Computational Economics,10 April 2019,https://link.springer.com/article/10.1007/s10614-019-09890-2,Boosting Exponential Gradient Strategy for Online Portfolio Selection: An Aggregating Experts’ Advice Method,January 2020,Xingyu Yang,Jin’an He,Yong Zhang,Unknown,Unknown,,Mix,,
55.0,1.0,Computational Economics,27 April 2019,https://link.springer.com/article/10.1007/s10614-019-09895-x,Liquidity in Financial Networks,January 2020,Hitoshi Hayakawa,,,Male,Unknown,Unknown,Male,"The financial crisis of 2008 raised the concern that default of a bank may cause a “domino” of default through interconnected financial obligations among banks. To ensure the resilience of financial system, the Basel Committee introduced a liquidity regulation under the framework of Basel lll; banks are required to hold sufficient liquidity against short-term liquidity shocks. For the purpose of liquidity regulation in general, it is crucial to assess the required liquidity appropriately. The assessment hinges on, among other things, the assumption of an underlying settlement system. In this respect, seminal theoretical studies in the literature on financial contagionFootnote 1 effectively assume “simultaneous” settlement, by which financial obligations are canceled out whenever possible, although “sequential” settlement is the standard in the modern interbank settlement systems. The assumption of simultaneous settlement leaves the relevant analyses highly tractable by allowing fixed-point arguments, although it could considerably underestimate the amount of funds to prevent a “domino” effect of default. 
Purpose and Framework
 This study offers a framework to evaluate the amount of liquidity—funds available for settlements—that is necessary to prevent default under “sequential” settlement. Our aim is to clarify how the interconnected feature of financial obligations could affect the required amount of funds. For this purpose, we assume an exogenous network of obligations and abstract their formation. We focus on the total amount of funds necessary to settle all obligations, and investigate how the amount depends on network topologies. We perform our investigation along with two polar liquidity scenarios. One is assumed as a situation in times of financial distress, by which funds circulate least efficiently. The other is assumed as a benchmark situation in times of non-distress, by which funds circulate most efficiently. In our formulation as graph problems, the pair of scenarios amounts to deriving the upper bound and lower bound of the required amount of funds against possible “orders” of settlements. 
Approach and Technique
 We characterize each lower bound and upper bound, applying a basic technique to analyze “flow” networks. Specifically, we decompose a network of obligations on the basis of cycles. This helps to separate mathematically the relevant “ordering” factor from the relevant “flow” factor. We refer to the former as the synchronization factor, and the latter as the domain factor, for reasons that become clearer soon. In terms of the interconnection of network, the synchronization factor captures the interconnected feature of network, while the domain factor—cycles and the relevant amount of obligations—serves as the reference basis for the interconnection. 
Main Results: Qualitative Aspects
 The main contribution of this study is to reveal the qualitative aspect of the synchronization factor with two of our original concepts, arc-twisted and vertex-twisted, which are formally defined on a directed graph. Specifically, with regard to the lower bound, the domain factor refers to the efficient recycling of funds within each domain—a cycle of obligations—without considering the interrelation between domains, while the synchronization factor, which is characterized by arc-twisted, refers to the additional amount of funds required that is sourced from the interrelation between domains. With regard to the upper bound, the domain factor refers to the least efficient recycling of funds within a domain without considering the interrelation between domains, while the synchronization factor, which is characterized by vertex-twisted, refers to the amount of subtraction that is sourced from the interrelation between domains. Thus, the interconnected feature of network is essentially captured by arc-twisted for the case of the lower bound, and by vertex-twisted for the case of the upper bound. A natural concern is the relation between the two concepts. In this respect, we show a simple relation, such that vertex-twisted indicates arc-twisted but not vice-versa. This result consolidates our findings on the qualitative implications of the interconnected feature. We illustrate that our characterization with the “twist” concepts provides a consistent and fundamental perspective on the underlying mechanism for a hub or other network structures to affect each lower/upper bound. 
Results: Quantitative Aspects
 We further analyze the quantitative implications of the “twist” concepts for real-world payment networks. Our approach is to focus on classes of network that capture key properties of real-world payment networks. The key properties observed in real-world payment networks have been discussed as a combination of high degree of clustering and relatively short average path lengths, as highlighted by Soramäki et al. (2007) with regard to Fedwire, by Rordam and Bech (2009) with regard to Danish interbank money flows, and by Inaoka et al. (2004) and Imakubo and Soejima (2010) with regard to BOJ-NET. This pair of properties is typically referred to as a small-world structure.Footnote 2 We show that for each lower and upper bound, an increase of obligations has a non-linear effect on the required amount of funds through the synchronization factor. This result suggests that appropriate consideration of the synchronization factor is indispensable in evaluating the funds required for real-world payment networks. 
Interbank Settlement Systems
 Here, we elaborate on the background of our assumption of “sequential” settlement. A traditional interbank settlement system used to be a simultaneous-type system called a designated-time net settlement system, whereby settlements are conducted on a designated-time basis and the obligations are offset against each other as much as possible. Against the backdrop of the expanding volume of obligations and technological advances for real-time transactions, many interbank settlement systems have changed and now adopt a sequential-type system called real-time gross settlement (RTGS), which settles obligations on a real-time basis.Footnote 3 In the RTGS system, obligations are no longer offset against each other but instead are settled on an individual gross basis. Although several interbank settlement systems further adopt systems that partially combine the offsetting service to the RTGS system, they still operate on the basis of RTGS system.Footnote 4 Our assumption of “sequential” settlement simulates the RTGS system without any offsetting service, and is intended to serve as a benchmark analysis for applications that incorporate the offsetting service. 
Literature and Contribution
 In the literature of financial network, a distinctive feature of our two concepts, arc-twisted and vertex-twisted, is their reference to dynamics. Such existing concepts as connectivity and concentration are largely static,Footnote 5 because the main focus of such research is examination of the consequence of balance-sheet linkage, effectively assuming “simultaneous” settlement. Few theoretical studies explicitly assume certain “sequential” settlement to discuss the relevance of network topologies. An exception is Rotemberg (2011), whose analysis is most relevant to the present study. Rotemberg (2011) theoretically examines the lower bound of the required amount of funds in a different setting. The author assumes “sequential” settlement, which allows an obligation to be settled in arbitrary installments.Footnote 6 However, in reality, the unit of settlements is not flexibly adjusted in many interbank settlements.Footnote 7 In our study, we do not allow arbitrary installments “ex-post,” that is, the unit of settlement is fixed when a network of obligations is given; when the given obligations are settled, each obligation needs to be settled at once in each given unit.Footnote 8 Rotemberg (2011) shows that multiplicity of cycles could be a source of inefficient circulation of funds in deriving the lower bound. From the perspective of our characterization, multiplicity of cycles corresponds to the “domain” factor. The assumption of flexible installments fails to capture the “synchronization” factor.Footnote 9 Indeed, the key contribution of our study is to reveal that the static concept of “domain” factor is insufficient; however, a dynamic concept regarding the “synchronization” factor is indispensable for examining RTGS systems. The implications of the dynamic aspect of RTGS systems have also been examined in simulation-based studies, such as Beck and Soramäki (2001) and Galbiati and Soramäki (2011).Footnote 10 Our theoretical analysis complements these works by clarifying the source of complications relevant to the dynamics. 
Applications of the Framework
 
Hayakawa (2018) applies our framework to discuss the liquidity issues of introducing central clearing counterparties (CCPs), and discusses how a CCP’s offsetting service could affect (i.e. increase or decrease) overall liquidity needs. By understanding CCPs’ offsetting service as the elimination of arcs in the network model, Hayakawa (2018) reveals potential negative effects of introducing a CCP, and discusses the effects quantitatively in relation to network topologies. The study illustrates the usefulness of the framework in discussing the liquidity issues of settlements from the perspective of network topologies. The rest of the paper is structured as follows. Section 2 introduces our model and mathematical framework. Section 3 provides motivational observations, especially regarding several example “networks” that are particularly relevant to our analysis. Section 4 overviews our main results in reference to the relevant theorems, and provides preliminary analysis. Section 5 shows our main results, formally introducing the “twist” properties—arc-twist and vertex-twist. Section 6 presents additional results regarding the quantitative implications of the “twist” properties. Section 7 provides concluding remarks. The appendix includes proofs of the relevant results.",3
55.0,1.0,Computational Economics,05 December 2018,https://link.springer.com/article/10.1007/s10614-018-9871-0,Comments on “Opinion Dynamics Driven by Various Ways of Averaging”,January 2020,Youzong Xu,Yunfei Cao,,Unknown,Unknown,Unknown,Unknown,,
55.0,1.0,Computational Economics,05 December 2018,https://link.springer.com/article/10.1007/s10614-018-9873-y,Reply on Comments on “Opinion Dynamics Driven by Various Ways of Averaging” by Youzong Xu and Yunfei Cao,January 2020,Ulrich Krause,,,Male,Unknown,Unknown,Male,"This result in Hegselmann and Krause (2005, p. 401/402) relies on Lemma 2 Hegselmann and Krause (2005, p. 400/401). The result holds true for the standard HK-model as well as for the various concrete means considered (Hegselmann and Krause 2005, equations (1) and (2), p. 382). The reason is that the standard HK-model converges in finite time (see Hegselmann and Krause 2002, Appendix D). It is easy to see that this implies the same for the various concrete means. (This does follow also from Lemma \(2^*\) in the Appendix A; see also Part B.) The authors of Xu and Cao (2018) are right, however, in pointing out that Lemma 2 in Hegselmann and Krause (2005) needs an additional assumption for PAMs more general than the various concrete means. The invariance property (*) as in Lemma \(2^*\), Appendix Part A, provides a sufficient and necessary condition for Lemma 2 to hold. This condition is satisfied in particular if f is continuous but does by no means require continuity as exemplified by the models studied in Hegselmann and Krause (2005, equations (1) and (2)). Assuming the invariance property the Theorem on Opinion Stabilization does follow as in Hegselmann and Krause (2005) (see Part A of the Appendix).",1
55.0,1.0,Computational Economics,20 June 2016,https://link.springer.com/article/10.1007/s10614-016-9590-3,Bankruptcy Prediction Using Logit and Genetic Algorithm Models: A Comparative Analysis,January 2020,Leila Bateni,Farshid Asghari,,Female,Male,Unknown,Mix,,
55.0,1.0,Computational Economics,09 December 2016,https://link.springer.com/article/10.1007/s10614-016-9641-9,A Comparative Study of Technical Trading Strategies Using a Genetic Algorithm,January 2020,Luís Lobato Macedo,Pedro Godinho,Maria João Alves,Male,Male,Female,Mix,,
55.0,2.0,Computational Economics,12 March 2013,https://link.springer.com/article/10.1007/s10614-013-9365-z,International Assets Allocation with Risk Management via Multi-Stage Stochastic Programming,February 2020,Libo Yin,Liyan Han,,Unknown,Unknown,Unknown,Unknown,,
55.0,2.0,Computational Economics,04 May 2019,https://link.springer.com/article/10.1007/s10614-019-09891-1,Impact of Electronic Liquidity Providers Within a High-Frequency Agent-Based Modeling Framework,February 2020,Alexandru Mandes,,,Male,Unknown,Unknown,Male,"The recent technological advances and the development of electronic financial markets have led to an unprecedented level of market automation, for which the consequences are still a heated subject of media debate. According to a TABB group estimate presented in November 2009, the market volume share accounted for by high-frequency trading (HFT) in the USA has reached 61%.Footnote 1 A more recent report published by the ESMA, covering a sample of 100 stocks from nine EU countries for May 2013, shows that the HFT activity accounted for between 24% or 43% of the total traded value, depending on the estimation method, i.e., based on the primary business of firms (direct approach) or on the lifetime of orders (indirect approach). For the number of trades the corresponding numbers for HFT activity were 30% and 49%, and for the number of orders 58% and 76% respectively.Footnote 2 Each pair of percentage levels can be seen as lower and upper bounds for the actual share of HFT. According to Mathew Szeto, these levels should be adjusted by removing the closing auction volume, which usually accounts for around 15% of the total daily volume, thus leading to a larger share of HFT.Footnote 3 In 2012 the UK Government Office for Science has commissioned a foresight project to address the complex issue of computer generated trading in financial markets and to provide policy strategic options (Government Office For Science 2012). The scientific evidence shows that, on one side, the general market quality regarding liquidity, transaction costs and price efficiency has been improved, but on the other side there appears to be a greater risk of periodic illiquidity. Still, many specific questions have remained unanswered due to the significant challenges in the empirical evaluation of HFT, determined either by a lack of proper data identification (Friederich and Payne 2011) or by endogeneity issues—HFT growth coincides with the 2008–2009 market turmoil (Brogaard 2010). For example, one open problem refers to the liquidity providing behavior of high-frequency traders (HFTs) during market stress versus regular market conditions: do high-frequency market makers retreat from the market or become liquidity consumers? Friederich and Payne (2011) question the robustness of a system relying on the liquidity provided by HFTs without any obligations of staying in the market, as well as the economic value of predatory strategies that profit at the expense of traditional investors. Nevertheless, even if passive HFTs would not flee during the periods of high order flow toxicity,Footnote 4 it is argued that their trading strategies could temporarily amplify some liquidity issues, even if they are not their direct cause. Regarding the causal relation between HFT and volatility, Brogaard (2010) and Linton (2011) show that it is not clear whether larger volatility is the effect of HFT or just a favorable condition that stimulates HFT activity. Given that groups of stable systems could interact in highly unstable ways (“fallacy of composition”), Danielsson and Zer (2012) suggest that the systemic risk of computer-based strategies is currently not well understood. One factor which might contribute to an increase of the systemic risk is a large homogeneity of strategies, e.g., when large clusters of market participants are following exactly the same strategies. Farmer and Skouras (2011) and Sornette and Von Der Becke (2011) underline the chaotic properties of the financial system, determined by the complex interactions between the market participants, which can sometimes exhibit very different behaviors as a result of very small initial state changes. Also, it is not clear to what extent do HFTs contribute to these non-linear dynamics and reinforcing feedback loops. Regarding the price discovery mechanism, Brogaard (2010) and Brogaard et al. (2013) conclude that by trading in the direction of the incoming information, HFTs positively contribute to price efficiency. However, since HFTs put more emphasis on short term rather than fundamental information, the way information arrives—concentrated or diffused—is also important. A final conclusion of the foresight project recommends further analysis for a better understanding of the potential impact of the various regulatory options in order to prevent any unexpected and detrimental market reactions. Given the challenges faced by the empirical approach, a viable alternative for testing the various hypothesis regarding the impact of HFT as well as the potential effect of policy regulations is to run simulations within an artificial stock market by means of agent based modeling (ABMFootnote 5). However, building an ABM gives the designer a large number of options and a set of individual design decisions need to be made— in other words, “agent-based models have a menu to offer that allows us to incorporate into our models economic, institutional and behavioral structure” (Dawid and Neugart 2011). In general, ABMs range from very simple and stylized models, which can be used to develop and test general theories or thought-ideas with no occurring probabilities attached, to higher-fidelity models, which are designed to explain reality and are usually calibrated to empirical data. Basically, the degree of realism and its associated model complexityFootnote 6 depend on the goal of research. The importance of building a realistic and robust model, which could then be successfully used as a laboratory for policy simulation, has been underlined by Alan Kirman: “if we wish to build a model which will be useful for studying the effect of policy measures we should base it on realistic assumptions derived from observed empirical behavior. However, we should also make sure that simulations of that model do not depend on a very limited set of parameter values for these assumptions. We want a model which is robust in the sense that small modifications in the assumptions do not change radically the nature of the results.”Footnote 7 Regarding what aspects to include/exclude into/from the model, Dawid and Neugart (2011) suggest that a closed ABM for economic policy design has to contain all relevant economic mechanisms, but this does not imply that all of these need to be modeled with identical granularity such as those directly responsible for the transmission of the policy measure. One of the first representative models for designing intraday financial market models have beed introduced by Chiarella and Iori (2004), Chiarella et al. (2009). The authors developed a continuous double auction model with stylized agents corresponding to human strategies, and showed that, by including chartist strategies, the simulated price series exhibits realistic stylized facts, such as the fat tails of returns, volatility clustering and the fat-tailed distribution of limit order placement relative to the midpoint. Gsell (2008) has extended the previous model and made a first step towards modeling computer-based trading by adding two different implementations of an algorithmic trader. More recently, Vuorenmaa and Wang (2013) have included market-makers with a tight inventory control and simulated the May 6, 2010 Flash Crash scenario, where a sudden market drop was triggered by a volume-inline execution algorithm, i.e., an algorithm that targets a given rate of the market actual volume. Their results show that the probability of a flash crash increases with the number of high-frequency (HF) agents, tightness of inventory sizes and smaller tick size, while quality metrics such as spread-at-touch and volatility depend diametrically on the previous parameters. However, the previous intraday market designs suffer from a series of limitations. For once, the time dimension is not properly operationalized—being reduced to an atemporal sequence of simulation steps. Therefore these models produce outcomes only at a step-by-step frequency, which, due to variable intraday trading activity, cannot be sampled/aggregated at various different time frequencies. As detailed in Sect. 2, this leads to agents not functioning as expected, when they are supposed to act at distinct time frames. Secondly, in these models the agents place their trading orders randomly and do no react to the changing microstructure conditions. In reality, especially in the intraday “world”, the limit order book is not just a trading instrument, but also a very important source of endogenous information that influences to a large extent the way traders behave. Mandes (2015) shows that a random order submission generates a convex shape for the market price impact function, which is not in line with what real markets exhibit. The concavity of the impact function is a trace of plain rational behavior—otherwise, if the unconditional impact function would be concave, there would be no incentive to split a large order into smaller ones, as the total market impact of the individual small orders would be larger than the one of the initial total order. The current contribution proposes a higher fidelity (in the sense of more realistic) intraday market model, with a proper implementation of time and with a microstructure-based order submission. This model allows for correctly simulating trading strategies that are active at different temporal frequencies, as well as for computing various statistics and implementing regulatory options that are time-related. By being able to derive statistics at lower frequencies than tick-by-tick we can both improve our understanding, due to their higher interpretability,Footnote 8 as well as calibrate the model with respect to lower-frequency statistics that are often found in the literature (e.g., Kirilenko et al. 2011). Moreover, the time dimension provides new insights into the functioning of regulatory policies, such as minimum resting time, which could not be implemented otherwise. The microstructure-based order submission allows the low-frequency traders to adapt their behavior to the changing liquidity and volatility conditions, e.g., by becoming more aggressive when the transaction costs are lower. Thanks to this feature, the model also provides better insights into the dynamics of this high- and low-frequency traders “ecosystem”. Based on the proposed model, a series of simulations is run in order to answer a set of research questions regarding the impact of HFT on market quality and systemic risk, what mechanisms and conditions are “responsible” for flash crashes, what features of HFTs contribute to such extreme volatility events, as well as what are the effects and side-effects of two potential policy measures. Section 2 introduces the underlying framework of the agent-based model, as well as the various LF and HF market participants. In Sect. 3.1 a set of simulated scenarios is described, covering different market configurations, agent parameterizations and possible market states. The main outcomes with respect to the overall market quality are interpreted in Sect. 3.2 and the flash crash event is analyzed distinctly in Sect. 3.3. The two potential regulatory policies, i.e., imposing a financial-transaction tax and minimum holding/quote resting times, are evaluated and their different transmission mechanisms and implications are compared in Sect. 4. Section 5 summarizes the results and proposes further research options.",1
55.0,2.0,Computational Economics,13 May 2019,https://link.springer.com/article/10.1007/s10614-019-09896-w,Forecasting Financial Returns Volatility: A GARCH-SVR Model,February 2020,Hao Sun,Bo Yu,,,Male,Unknown,Mix,,
55.0,2.0,Computational Economics,14 May 2019,https://link.springer.com/article/10.1007/s10614-019-09897-9,Wavelet Estimation Performance of Fractional Integrated Processes with Heavy-Tails,February 2020,Heni Boubaker,,,Unknown,Unknown,Unknown,Unknown,,
55.0,2.0,Computational Economics,21 May 2019,https://link.springer.com/article/10.1007/s10614-019-09898-8,Robust Estimation of Finite Horizon Dynamic Economic Models,February 2020,Thomas H. Jørgensen,Maxime Tô,,Male,,Unknown,Mix,,
55.0,2.0,Computational Economics,19 June 2019,https://link.springer.com/article/10.1007/s10614-019-09901-2,Measuring CoVaR: An Empirical Comparison,February 2020,Michele Leonardo Bianchi,Alberto Maria Sorrentino,,Female,Male,Unknown,Mix,,
55.0,2.0,Computational Economics,19 June 2019,https://link.springer.com/article/10.1007/s10614-019-09900-3,Are Central Bankers Inflation Nutters? An MCMC Estimator of the Long-Memory Parameter in a State Space Model,February 2020,Fredrik N. G. Andersson,Yushu Li,,Male,Unknown,Unknown,Male,"Inflation targeting has become an increasingly popular monetary policy regime since the early 1990s (Hammond 2012). Bank of Canada was the first central bank, in the modern era, to shift to inflation targeting in 1991, and the Federal Reserve was one of the last to do so in 2012. Although the Federal Reserve was late in adopting an official inflation target, it had targeted the rate of inflation since at least the late 1970s, but without having announced an official target rate. Its inflation target was in other words implicit rather than explicit until 2012. All inflation targeting central banks are faced with a dilemma. A strict focus on inflation targeting may increase volatility elsewhere in the economy (Svensson 1997). On the other had a too flexible approach to inflation targeting may undermine credibility in the target. Most central banks have opted for policy strategy somewhere in the middle between strict and flexible inflation targeting. In the long-run, the bank focuses narrowly on inflation, while it takes other non-inflation considerations into account over the short- to medium-rum. Or in the words of the former Governor of the Bank of England, Mervyn King, central bankers are not ‘inflation nutters’. How the rest of the economy develops also has an impact on the decisions of the central bank. In fact, one reasons the Federal Reserve had an implicit rather than explicit inflation target was to ensure that the bank had the flexibility to respond to non-inflationary events in the economy (Lindsey et al. 2005). Although there are benefits from having a flexible inflation target (for a discussion see e.g. Woodford 2003; Kuttner and Posen 2012; Andersson and Jonung 2017) it also raises a few questions. First, it becomes more difficult to hold the central bank accountable when the target is flexible. Because inflation is allowed to deviate from the target under a flexible regime, comparing the inflation outcome with the inflation target is not necessarily a good indicator of the success of the central bank. Second, for consumers, firms, and investors, it becomes more difficult to forecast the future behavior of the central bank. A simple measure of how flexible the inflation target policy is would help to solve some of the issues around a flexible policy. So far, no such simple measure has been found. In this paper, we propose that the fractional integration order from an autoregressive fractionally integrated moving average (ARFIMA) model can serve as an estimate of the degree of flexibility. Several studies have found that inflation is a highly persistent series. In fact, several studies have failed to reject that inflation contains a unit root, even in those cases in which the central bank has an inflation target and inflation clearly fluctuates around a stationary long-run mean (Hassler and Wolters 1995; Caggiano and Castelnuovo 2011). The results from those studies suggest that inflation is mean-reverting, but a covariance non-stationary series, i.e., fractional integration with an integration order between 0.5 and 1. Under the assumption that in the long-run inflation is entirely caused by monetary policy, as is commonly assumed (see e.g. ECB 2004), the fractional integration order represents the central bankers preferences. The higher (lower) the fractional integration order is, the longer (shorter) are the deviations from the mean (i.e., the target), and the more flexible (strict) is the inflation target policy. Hence, by estimating the fractional integration order, we obtain a simple estimate of the degree of flexibility. Several estimators of ARFIMA models have been proposed in the econometric literature. These include the parametric method, which is based on the maximum likelihood function (Fox and Taqqu 1986; Sowell 1992; Giraitis and Taqqu 1999) and the regression-based approach in spectral domain (Geweke and Porter-Hudak 1983). Additional estimators include the semi-parametric (Robinson 1995a, b; Shimotsu and Phillips 2005) and the wavelet-based semi-parametric (McCoy and Walden 1996; Jensen 2004) methods. Chan and Palma (1998) established a theoretical foundation to estimate the ARFIMA model with an approximate maximum likelihood estimation (MLE)-based state space model. The authors truncated the infinite autoregressive (AR) or moving average (MA) representations of the ARFIMA model into finite lags and calculated the approximate maximum likelihood using the Kalman filter. Chan and Palma (1998) show that the approximate MLE-based state space model has desirable asymptotic properties and a rapid convergence rate. Recently, Grassi and Magistris (2014) conducted a simulation study to compare the state space model-based long-memory estimation with several widely applied parametric and semi-parametric methods. Grassi and Magistris (2014) show that compared with the other estimations, the state space model method is robust to the t distribution and missing value, measurement error and level shift. Chan and Palma (1998) and Grassi and Magistris (2014) consider the stationary case with \( 0 < d < 0.4 \). While inflation likely has an integration order above 0.5, we first propose a Metropolis–Hastings algorithm in Markov chain Monte Carlo (MCMC) to extend the possible range of integration orders to also include the non-stationary case with \( d > 0.5 \). Simulation studies provided in the paper shows that the approach works well even when the integration order is close to 1. We then applied the algorithm to estimate the fractional integration order for seven economies (Canada, Euro area, Germany, Norway, Sweden, the United Kingdom, and the United States) between 1993 and 2017 using monthly data. All countries have central banks with inflation targets, although both Germany (1993–1997) and the United States (1993–2012) had implicit rather than explicit inflation targets during parts of the sample period. Our estimation results show that all central banks have flexible targets and are no “inflation nutters”. The fractional integration order is high and close to 0.9. The integration order tends to be higher for larger economies and lower for smaller economies. There is no evidence of having an implicit rather than explicit inflation target makes the target more flexible. The fractional integration order appears to be unrelated to whether the target is explicit or implicit. Out results also indicate that the inflation targets have become more flexible since the financial crisis of 2008/09. Financial stability and the economic crisis that followed have likely changed the focus of central banks away from inflation targeting to other important variables. The remainder of the paper is organized as follows: Sect. 2 discusses inflation and inflation targets. Section 3 introduces the state space model-based MLE for long-memory series, Sect. 4 combines the state space model with the MCMC algorithm to estimate the fractional difference parameters, Sect. 5 applies empirical examples, and the conclusion can be found in Sect. 6.",
55.0,2.0,Computational Economics,25 June 2019,https://link.springer.com/article/10.1007/s10614-019-09903-0,Observation Driven Long Run Equilibria,February 2020,Katarzyna Łasak,Johannes Lont,,Female,Male,Unknown,Mix,,
55.0,2.0,Computational Economics,27 June 2019,https://link.springer.com/article/10.1007/s10614-019-09904-z,Approximating Walrasian Equilibria,February 2020,Aad Ruiter,,,Male,Unknown,Unknown,Male,"
Scarf (1960) presents three examples of small pure exchange economies, in which prices either converge to or orbit around the competitive equilibrium (clockwise or counter clockwise), depending on the initial allocation. These examples have inspired different lines of research. First and foremost, they have raised the question which price adjustment processes do converge to Walrasian equilibria [e.g. Uzawa (1962), Negishi (1962), Hahn and Negishi (1962), Scarf (1967), Scarf and Hansen (1973), Smale (1976), Saari and Simon (1978), Van der Laan and Talman (1987), Herings (1997), Herings (2002)]. But, the examples have also sparked an interest in the stabilization of these so-called Scarf economies themselves. 
Oehmke and Oehmke (1991) argues that aggregate excess demand of any commodity must become positive if its relative price goes to zero. This requirement can be used to constrain the set of admissible initial allocations, ruling out the examples of Scarf (1960). Kumar and Shubik (2004) considers the stabilization of the Scarf economies as an exercise in designing an appropriate feedback controller. It shows that adding higher order derivatives to the feedback mechanism can result in convergence. Anderson et al. (2004) is of particular interest. It submits the Scarf examples to experimental trading in a continuous double auction. Despite introducing a few complications, aimed at making trading more challenging for human agents, its results resemble those of Scarf (1960). Only in the stable example, prices closely fluctuate around the values of Walrasian equilibrium prices. Prices in the clockwise and counter clockwise examples have similar, unstable dynamics, in opposite directions.Footnote 1 Gintis (2007) claims that the lack of stability in Anderson et al. (2004) is due to the continuous double auction. It presents an agent-based model with bilateral trading and learning through imitation. In the context of the unstable Scarf examples this leads to convergence. However, if expectations are updated in a coordinated manner, then instability emerges.Footnote 2 Goeree and Lindsay (2016) succeeds in stabilizing an experimental Scarf economy by introducing a schedules-market, in which traders reveal part of their demand schedules to an auctioneer, who then uses the algorithm of Smale (1976) to compute new prices. Following the lead of Goeree and Lindsay (2016), this paper proposes a more parsimonious approach to obtaining demand schedules. We demonstrate that the auctioneer can reliably approximate demand schedules by assuming that all agents have Cobb–Douglas utility functions. Here, “reliable” means implying global convergence (i.e. convergence from any starting point) to the Walrasian equilibrium. Demand at previously quoted prices suffices to identify the hypothetical Cobb–Douglas preferences. This way, a Cobb–Douglas exchange economy can be associated with any exchange economy under consideration. This Cobb–Douglas economy has unique equilibrium prices, which feed into the next iteration. We show convergence in exchange economies, in which traders have preferences that can be represented by CES utility functions, ranging from Leontief to Cobb–Douglas functions. This class covers the examples of Scarf (1960). In the clockwise and counter clockwise economies, we find prices spiraling towards the Walrasian equilibrium in the same direction as found by Herbert Scarf. Convergence is due to the process generating prices that are not a linear combination of previous prices. If a price adjustment process converges globally and universally (i.e. for every economy) then it is called an effective price mechanism. Saari and Simon (1978) stipulates that effective price mechanisms require knowledge of most elements of the Jacobian of the aggregate excess demand function. This result is predicated on the trajectories of prices following a differential equation. The amount of information required by our process, \(\mathcal {P}\), consists of the initial allocation of commodities and individual demand at a finite number of (arbitrary) prices. That may be more than is needed for an effective price mechanism. However, absent any prior knowledge that a given economy is characterized by a particular aggregate excess demand function, the latter (and possibly its Jacobian) will have to be derived from individual demand before they can be used in any algorithm. In that case, \(\mathcal {P}\) allows a reduction of information. The complexity of the proposed process depends on how the speed of convergence relates to scale: in each iteration, solving the new prices in an associated Cobb–Douglas economy requires at most \(m^{3}/3+m^{2}n\) multiplications and additions, with m the number of commodities and n the number of agents [c.f. Eaves (1985)]. We explore the complexity of the process by simulating the maximum number of iterations required for convergence in scaled-up versions of the Scarf economies. The speed of convergence appears to be a polynomial function of the size of the economy.Footnote 3 If \(m=n\) is even, then large scale Scarf economies have an infinite number of equilibria with prices \(\mathbf {p}^{*}=\left( 1,\alpha ,\ldots ,1,\alpha \right) \), \(\alpha >0\). Here, on average, the proposed algorithm converges faster than if \(m=n\) is odd and \(\alpha =1\). However, in the even-sized unstable examples, there is also a small probability that an excessive number of steps is required for reaching convergence (depending on how the equilibrium prices in the associated Cobb–Douglas economy are computed). This paper is organized as follows. After deriving some auxiliary results (Sect. 2), we will prove convergence (Sect. 3). In Sect. 4, we apply our approach to the examples proposed by Scarf (1960). Here, we also simulate the relation between speed of convergence and scale. Finally, Sect. 5 offers some concluding thoughts.",1
55.0,2.0,Computational Economics,09 July 2019,https://link.springer.com/article/10.1007/s10614-019-09906-x,A Fitted Multi-point Flux Approximation Method for Pricing Two Options,February 2020,Rock Stephane Koffi,Antoine Tambue,,Male,Male,Unknown,Male,"Pricing multi-assets options is of great interest in the financial industry (see Persson and Sydow 2007). Multi-asset options are options based on more than one underlying. There are several kinds of multi-assets options, few of them are exchange options, rainbow options, baskets options, best or worst options, quotient options, foreign exchange options, quanto options, spread options, dual-strike options and out-performance options. Pricing these options lead to the resolution of the following second order degenerated Black–Scholes partial differential equations (PDE) (see Persson and Sydow 2007) where r is the risk free interest, U is the option value at time \(\tau \), \(\tau =T-t\) with t and T respectively the instantaneous and maturity time, \(\,S_i~\) represents the asset i price, \(\sigma _i\) represents the volatility of asset i, \(\rho _{ij}\) represents the correlation between the assets i and j, where \(i, j=1,\ldots ,n\). The main difference between multi-assets options is their payoff functions which represent the initial condition of the corresponding backward PDE. The spatial domain of the PDE is infinite, but for its numerical resolution, a truncation is required (see Duffy 2013, Chapter 3). It has been observed that when the stock price S approaches the region near to zero, the Black Scholes PDE is degenerated (see Duffy 2013, chapter 30.3). Moreover, the initial condition of the PDE has a discontinuity in its first derivative when the stock price is equal to the strike K. This discontinuity has an adverse impact on the accuracy when the finite difference method is used (see Wilmott 2005, chapter 26). Therefore, for the spatial discretization of the PDE, it is suitable to use non-uniform grids with more points in the region around \(S=0\) and \(S=K\) in order to handle the degeneracy and the discontinuity. To overcome the above challenges, many methods have been proposed in the literature. Thereby, Wang (2004) proposed a fitted finite volume method for one dimensional Black Scholes PDE and the rigorous convergence proof is provided by Angermann and Wang (2007). Besides, Huang et al. (2006) adapted the fitted finite volume discretization method for the two-dimensional Black–Scholes PDE and its rigorous convergence proof is analysed by Huang et al. (2009). Although these two fitted finite volume methods are stable, they are only order 1 with respect to asset price variables. In this paper, we present two novel discretization methods for the two-dimensional Black Scholes PDE based on a special kind of finite volume method, the so-called multi-point flux approximation (MPFA) method. This method was introduced by Aavatsmark (2002) and has been used in fluid dynamics for flow and transport equations (see Sandve et al. 2012 and references therein). Actually, the MPFA was designed to give a correct discretization of the flow equation for general grids including fractures (see Aavatsmark 2002; Sandve et al. 2012). The MPFA method is essentially based on the approximation of a linear function gradient over a triangle, the calculation and the continuity of flux through edges of this triangle. The convergence of MPFA method is usually second order in space domain on rough grids (see Aavatsmark 2007; Stephansen 2012). Our first numerical method here is the standard MPFA , which is fully used to approximate the second order operator. To the best of our knowledge, this method was not yet used to solve degenerated Black Scholes PDE in finance. To build our new fitted MPFA method, we couple the standard MPFA with the upwind methods (first and second order) to approximate two dimensional options pricing. Besides, the fitted finite volume proposed by Wang (2004) is used to handle the degeneracy of the PDE in the region where the stock price approaches zero (degeneracy region). In the region, where the PDE in not degenerated, we apply the MPFA method. The novel numerical technique from this combination is called fitted MPFA method and will obviously improve the accuracy of the current fitted finite volume in the literature, since more approximations involving are second order in space. Naturally, these two methods are applicable to other types of multi-asset options and also to financial models such as Heston (1993) model and Bates (1996) model on non-uniform grids. Another advantage of our novel fitted MPFA is that it can easily be adapted to more structured commercial or open-source softwares as the standard MPFA (see Lie et al. 2012). The rest of the paper is organized as follows. In Sect. 2, we start by introducing the Black Scholes model for option with 2 stocks and the corresponding partial differential equation. Afterwards, we set the frame of the numerical domain of study suitable for the finite volume method application. Section 3 is devoted to the spatial discretization of the PDE. We describe the multi-point flux approximation method for the discretization of the diffusion term of the PDE. The upwind methods (first and second order) are used for the the convection term discretization. We end the Sect. 3 with the fitted MPFA which is a combination of a fitted finite volume method and the MPFA method. The time discretization is performed using the \(\theta \)-Euler methods in the Sect. 4. In Sect. 5, we perform numerical experiments. Those numerical simulations show that the two proposed schemes (the standard MPFA method and fitted MPFA method ) can be more accurate than the current fitted finite volume method proposed in the literature. General conclusion is given in Sect. 6.",6
55.0,2.0,Computational Economics,11 July 2019,https://link.springer.com/article/10.1007/s10614-019-09905-y,Bayesian Inference of Local Projections with Roughness Penalty Priors,February 2020,Masahiro Tanaka,,,Male,Unknown,Unknown,Male,"Local projections introduced by Jordà (2005) provide a statistical framework that accounts for the relationship between an exogenous variable and an endogenous variable, measured at different time points. Typical applications of local projections include impulse response analyses and direct (non-iterative) forecasting (Stock and Watson 2007). A local projection has several advantages over standard methods, such as vector autoregression (VAR). First, it does not impose a strong assumption on the data-generating process, making it robust to misspecification. Second, it can easily deal with asymmetric and/or state-dependent impulse responses (e.g., Riera-Crichton et al. 2015; Auerbach and Gorodnichenko 2013; Ramey and Zubuairy 2018). On the other hand, local projections have several disadvantages. First, when using a local projection, the exogenous variable must be identified beforehand. Second, a local projection is statistically less efficient than other methods, and typically obtains a wiggly impulse response function, (e.g., Ramey 2016). In an impulse response analysis, the shape of an estimated impulse response function is of concern. Therefore, if an estimated impulse response function is wiggly and has wide confidence/credible intervals, it is difficult to interpret the result, and one might wrongly reject or accept a hypothesis. In this study, we address the second disadvantage of local projections. In order to improve the statistical efficiency, we develop a fully Bayesian approach that can be used to estimate local projections using roughness penalty priors as well as B-spline basis expansions.Footnote 1 The proposed priors, which are adapted from Bayesian splines (Lang and Brezger 2004), are generated from an intrinsic Gaussian Markov random field; that is, they induce random-walk behavior on a sequence of parameters. By incorporating such prior-induced smoothness, we can use information contained in successive observations to enhance the statistical efficiency of an inference. We compare the proposed approach with the existing approaches through a series of Monte Carlo experiments. The proposed approach is applied to an analysis of monetary policy shocks in the United States to show how the roughness penalty priors successively smooth impulse responses and improve statistical efficiency in terms of predictive accuracy. Furthermore, we show that such improvements are almost entirely attributable to the roughness penalty priors and not to the B-spline expansions. There are three strands of studies related to this work. For the first, Barnichon and Matthes (2019) approximate a moving average representation of a time series using values from Gaussian basis functions. Their approximation is simpler, but much coarser than ours. As a result, their estimated impulse responses may be excessively smoothed and vulnerable to model misspecification. For the second, to smooth an impulse response estimate, Miranda-Agrippino and Ricco (2017) penalize the estimate based on deviations from an impulse response derived from an estimated VAR. However, their approach seems not to work well in cases with asymmetric and/or state-dependent impulse responses (e.g., Riera-Crichton et al. 2015). Furthermore, their approach uses the same dataset twice. This shortcoming can be resolved if a time series is long enough to be split into training and estimation samples, but this is not the general situation in macroeconomic studies. In contrast, our approach does not require a reference model, thus it is free from these problems.
 For the third, the most relevant studies are those of Barnichon and Brownlees (2019) and El-Shagi (2019), who develop frequentist methods using roughness penalties. Although our approach can be regarded as a Bayesian counterpart to theirs, it confers four additional benefits. First, our approach is more flexible than Barnichon and Brownlees’s (2019) approach: they allow a single parameter to control the smoothness of all parameter sequences, whereas we can assign different smoothing parameters to individual sequences. Second, our Bayesian approach can evaluate credible intervals in a consistent and straightforward manner, while the frequentist approaches cannot provide a theoretically grounded confidence interval. Third, in our approach, smoothing parameters are inferred from priors, implying that we can systematically consider uncertainty in the smoothness of an impulse response. In contrast, the frequentist approach prefixes smoothing parameters; Barnichon and Brownlees (2019) choose a smoothing parameter via cross-validation, while El-Shagi (2019) determines smoothing parameters on the basis of some information criteria. Fourth, our approach has better finite-sample performance than El-Shagi’s (2019) approach, as shown in Sect. 5.
 The rest of the paper is organized as follows. Section 2 introduces the model, the priors and the posterior simulation. Section 3 conducts a set of Monte Carlo experiments and reports the result. Section 4 demonstrates our approach in an analysis of the macroeconomic effects of monetary policy shocks in the United States. Section 5 compares the proposed approach with the existing frequentist approaches. Section 6 concludes this paper.",
55.0,2.0,Computational Economics,13 July 2019,https://link.springer.com/article/10.1007/s10614-019-09907-w,Quantifying the Advantages of Forward Orthogonal Deviations for Long Time Series,February 2020,Robert F. Phillips,,,Male,Unknown,Unknown,Male,"Generalized method of moments (GMM) estimation of a panel data regression model typically relies on transforming the data to remove unobserved time-invariant effects. But, if certain conditions are met, how the data are transformed does not matter in terms of sampling behavior. This is because, if suitable restrictions are satisfied, GMM estimation is invariant to how the data are transformed (see Schmidt et al. 1992; Arellano and Bover 1995; Phillips 2019). For example, Phillips (2019) provided the necessary and sufficient restriction on instruments that ensures that, in some cases, a GMM estimator can alternatively be computed as a filtered two-stage least squares (2SLS) estimator. But if a GMM estimator based on, say, the first-difference (FD) transformation can alternatively be computed using filtered 2SLS, in what sense does it matter which transformation is used? It does matter: one advantage of using filtered 2SLS is computational. Arellano and Bover (1995) suggested as much for a special case—specifically, GMM based on the forward orthogonal deviations (FOD) transformation. Yet to date, there appears to be no published evidence illustrating how much of a computational advantage is conferred by using filtered 2SLS. This paper investigates the computational advantage of filtered 2SLS. It provides an example illustrating dramatic reductions in computing time. The example examined is the forward orthogonal deviations GMM (henceforth FOD-GMM) estimator. When the instruments condition described in Phillips (2019) is satisfied, the FOD-GMM estimator is numerically identical to the one-step first-difference GMM (FD-GMM) estimator proposed in Arellano and Bond (1991). But, when the length of the time series (T) is not short, the FOD version of the estimator can be calculated orders of magnitude faster than when the FD formula is used. On the other hand, if the instruments condition is not met, the FD and FOD transformations cannot lead to the same estimator. This begs the question: When the estimators differ, which estimator has the better sampling properties? The answer is not obvious a priori, because when the estimators differ, they are based on different, non-overlapping moment restrictions. The question of which estimator has the better sampling propoerties, therefore, is addressed in this paper with Monte Carlo experiments. I find that, regardless of whether the one-step FD-GMM estimator or Arellano and Bond’s (1991) two-step FD-GMM estimator is used, the FOD-GMM estimator generally has smaller absolute bias. Moreover, it is usually more efficient (i.e., has smaller standard deviation) than the one-step FD GMM estimator, regardless of the size of T, and it is also typically more efficient than the two-step FD-GMM estimator when T is not small. In the next section I review the conditions under which a GMM estimator can alternatively be calculated with 2SLS after filtering the data. Section 3 provides a specific example illustrating how much faster computations can be performed with filtered 2SLS. And Sect. 4 provides Monte Carlo evidence illustrating the finite sample properties of the FOD-GMM estimator compared to the one-step and two-step FD-GMM estimators.",2
55.0,2.0,Computational Economics,16 July 2019,https://link.springer.com/article/10.1007/s10614-019-09908-9,Prediction of Unemployment Rates with Time Series and Machine Learning Techniques,February 2020,Christos Katris,,,Male,Unknown,Unknown,Male,"Accurate forecasting of unemployment is central to economic decision-making and to the design of policy-making in order to recognize early and reduce the problem. Many time series models have been employed extensively for the forecasting of macroeconomic variables, including unemployment. ARIMA models have been used in empirical studies such as for Czech Republic (Stoklasová 2012), for Romania (Dobre and Alexandru 2008) and for Nigeria (Etuk et al. 2012). In Mladenovic et al. (2017) seasonal ARIMA model is used to forecast unemployment at the EU28 level. In Floros (2005) many variations of ARMA and GARCH models for forecasting UK unemployment were compared. GARCH model assumes heteroskedasticity which depends on past values and offer additional insight in the case of heteroskedastic time series. ARIMA models cannot allow for persistent effects of shocks in unemployment—evidence for hysteresis (Blanchard and Summers 1986)—and fractional ARIMA (FARIMA) models which take into account the long-memory effect seem more suitable for unemployment prediction. These models have been used in papers as in Gil-Alana (2001) for the forecasting of the UK unemployment rate, in Kurita (2010) for forecasting of the Japan’s unemployment rate and in Katris (2015) for the forecasting of Greece’s unemployment rate. Another issue is non-linearity and when is present other approaches are more suitable. The forecasting performance of a variety of linear and nonlinear time series models using the U.S. unemployment rate was compared in Montgomery et al. (1998). In Rothman (1998) six nonlinear models were compared according to their out-of-sample forecasting accuracy, while in the paper of Proietti (2003) is examined the forecasting accuracy of several linear and nonlinear forecasting models for the US monthly unemployment rate and in Johnes (1999) are reported the results of a forecasting competition between linear autoregressive, GARCH, threshold autoregressive and neural network models of the UK monthly unemployment rate series. Artificial Neural Networks (ANN) appear promising in modeling more accurately data which display non-linearity, thus to give better forecasts. The paper of Aiken (1996) shows how a neural network may be used to forecast unemployment rates in the United States. More recently, in Olmedo (2014) neural network techniques are used for the forecasting of unemployment in Spain. In Katris (2019), FARIMA and FARIMA/GARCH time series models and Multilayer feed-forward neural network models have been used for the 1-step ahead forecasting of unemployment in nine Mediterannean countries. Support Vector Machines (specifically, Support Vector Regression—SVR) is another popular machine learning method for time series prediction (Sapankevych and Sankar 2009). A tutorial on SVR can be found on Smola and Schölkopf (2004), while in Basak et al. (2007) are discussed SVR applications and in Trafalis and Ince (2000) SVR is used for financial forecasting. Moreover, is employed the modern data mining method of multivariate adaptive regression splines (MARS) (Friedman 1991) which is a form of non-parametric regression analysis and can be seen as an extension of linear models which take into account and “automatically” models nonlinearities and interactions between variables. When the method is used in a time series context and lagged variables are used as the predictors, such as in this work, then the term TSMARS (Time Series multivariate adaptive regression splines) is used. This extension of MARS to a time series context is discussed in the paper of Lewis and Stevens (1991). The method has been used widely in many fields including economy and finance, for example in De Gooijer et al. (1998) where MARS model is used to estimate and forecast non-linear structure in weekly exchange rates for four major currencies during the 1980s and in Yüksel and Adali (2017) where MARS model is used to determine influencing factors of unemployment in Turkey. There are also extensions of the MARS model, such as CMARS (Weber et al. 2012) where the backward pass of MARS is replaced by Tikhonov Regularization and Conic Quadratic Programming. The CMARS method extended in order to cope with data uncertainty, by using robust optimization under polyhedral uncertainty and ellipsoidal uncertainty and RCMARS introduced (Özmen et al. 2010, 2011). A modification of RCMARS in order to reduce computational complexity is the RMARS model which introduced in 2014 (Özmen and Weber 2014) and the MARS and RMARS methods applied to a financial dataset and although RMARS displayed smaller variance under different uncertainty scenarios, the MARS method produced slightly more accurate forecasts.
 This paper is a comparison of time series, neural network, support vector machine and multivariate adaptive regression splines models to forecast unemployment of twenty-two countries using monthly seasonally adjusted data for unemployment (Eurostat database). For each one of the considered forecasting approaches, we consider multiple models and we adopt a searching strategy to decide the best. For the FARIMA and FARIMA/GARCH models we have set the rules for the selection of the order and we are searching between models with different error distributions. For the feed-forward fully connected feed forward neural networks (ANN approach) we consider models with 1–4 lagged variables as inputs and 1–10 hidden nodes, for SVR and MARS models we consider 1–4 lagged variables as inputs while ARIMA and Holt-Winters are considered as benchmark models. The comparison of models is performed with the RMSE and MAE criteria for each country. To compare the performance of models overall, we consider some metrics which are based in the RMSE and MAE criteria. Furthermore, to decide the suitability of models for each case we perform the Model confidence set procedure (Hansen et al. 2011).
 Finally, is performed a comparison of models in different time horizons and geographic regions to decide if a unique approach is better in all cases and to detect if either forecasting horizon and/or geographic location of a country affects the performance of a model. These comparisons are performed via Friedman tests and posthoc comparisons. In Sect. 2 are described the characteristics of the unemployment data and potential problems of heteroskedasticity, non-linearity, non-normality and long-memory, while in Sect. 3 are presented FARIMA, FARIMA/GARCH, ANN, SVR and MARS modeling approaches and their application in this paper. Section 4 presents the criteria for the comparison of models for multiple series and Sect. 5 the methodology for testing the effects of geographic location and of forecasting horizon to the performance of models. Section 6 contains the data analysis and Sect. 7 the summary and conclusions of the paper.",25
55.0,2.0,Computational Economics,22 July 2019,https://link.springer.com/article/10.1007/s10614-019-09910-1,SABCEMM: A Simulator for Agent-Based Computational Economic Market Models,February 2020,Torsten Trimborn,Philipp Otte,Martin Frank,Male,Male,Male,Male,"Over the last two decades, the new research field of Econophysics benefited from a rapidly increasing community and gained lots of momentum (Bouchaud 2002). Due to several financial crises, the interest in new financial market models has risen, not only in the scientific society (Farmer and Foley 2009; Bouchaud 2008) but especially in the work of practitioners like Trichet (2010) and Bernanke (2010). One subject of Econophysics are so called agent-based computational economic market (ABCEM) models. These models resemble an artificial market of interacting agents usually analyzed with the help of Monte Carlo simulations. Many classical financial market models are based on the Efficient Market Hypothesis(EMH) originally introduced by Malkiel and Fama (1970), Granger and Morgenstern (1970). The EMH faces extensive criticism and controversial discussions are still carried out (Malkiel 2003). One reason for this is the existence of market anomalies, usually named stylized facts, which cannot be explained by the EMH. Stylized facts are statistical observations in financial data which can be documented on different time scales and for various stock markets all over the world. The stylized fact probably best known is the inequality of income and wealth which was first discovered by Pareto (1897). Additional examples are heavy tails in stock return distributions and volatility clustering, originally identified by Mandelbrot in 1963 (Mandelbrot 1997). For further discussion of stylized facts, we refer to (Cont 2001; Lux 2008). Stylized facts seem to play a major role in the emergence of financial crises (Cowan and Jonard 2002) and the need to investigate the origins of stylized facts has been emphasized by several authors (Farmer and Foley 2009). The common goal of ABCEM models is to replicate financial data containing stylized facts and thus to discover reasons for their appearance. Hence ABCEM models can help to better understand the emergence of financial crisis. ABCEM models indicate that stylized facts are introduced, for example, by behavioral aspects and psychological misperceptions (Cross et al. 2005; Lux 2008) of agents or learning mechanisms (Adam et al. 2016; Ehrentreich 2007; Timmermann 1993). Many ABCEM models are heavily influenced by behavioral finance (Kahneman 2003) and do not share many similarities with classical financial market models. Thus, the investors within ABCEM models, usually called agents, do not follow the homo oeconomicus (Mill 1994) paradigm of rational utility maiximizers. They are rather modeled as bounded rational agents in the sense of Simon (1955, 1957). Furthermore, is the demand of each agent aggregated to the so called excess demand, which is defined as the average of the difference between demand and supply of all agents. In classical economic theory the supply matches demand. This theory is known as general equilibrium theory and dates back to John Locke, James Denham-Steuart and Adam Smith. In the 19th century, the general equilibrium theory has been further developed by Antoine Cournot, Carl Menger and León Walras. Probably the most influential model in the general equilibrium theory has been introduced by Walras (1896). The model considers an auctioneer, who determines the price in a so called tâtonnement process. Here, one assumes a rational market in the sense that we have perfect information and no transaction costs. There are further developments of the general equilibrium theory due to McKenzie, Arrow and Debreu in the 1950s. We refer to the book (Walker 2006) for a general discussion. The equilibrium can heuristically be expressed as: where \(N\in {\mathbb {N}}\) denotes the number of market participants and \(v^{(\cdot )}(S,t)\) the volume of stocks or assets which each agent demands or respectively offers at a certain price S. The equilibrium price \(S_{eq}\) is then given as the price, where the supply matches the demand. From a mathematical perspective, this results in a fixed point problem, for which the existence of a solution often is not a priori guaranteed and which is usually difficult to solve. The general equilibrium theory is criticized (e.g. Heertje 2002; Ackerman 2002) due to the restricted nature of the assumption of a rational market which seems to be often violated in real world economics. In addition, there is an ongoing discussion whether market prices represent an equilibrium. For example, Beja and Goldman (1980) point out that observed prices are usually not identically to the equilibrium prices. This happens if the tâtonnement process is taking too long, such that the computed solution is again in disequilibrium. This has lead to the theory of market prices in disequilibrium (Beja and Goldman 1980; He 2011; Day and Huang 1990) in which the price adjustment speed is finite and the actual market price represents a price in disequilibrium. In fact, most ABCEM models employ the concept of bounded rational agents and consider an irrational market mechanism. In the last decade there have been many contributions of new ABCEM models. We only present a short overview over the most influential ABCEM models: the Levy–Levy–Solomon model (Levy et al. 1994), the Lux–Marchesi model (Lux and Marchesi 1999, 2000), the Brock–Hommes model (Brock and Hommes 1997, 1998), and the Cont–Bouchaud model (Cont and Bouchaud 2000). ABCEM models describe a diverse field of applications. Several models focus on the creation of crises (cf. Kim and Markowitz 1989; Kaizoji 2000; Harras and Sornette 2011) while others try to explore the influence of new regulations of policy makers on the market behavior (cf. Teglio et al. 2012; Da Silva et al. 2015). We refer interested readers to reviews (LeBaron 2000; Bouchaud 2002; Chakraborti et al. 2011; Sornette 2014; Iori et al. 2012; Hommes 2006; Samanidou et al. 2007; Ehrentreich 2007; Chen et al. 2012; Tesfatsion 2002, 2006) for a general introduction to ABCEM models. Generally, ABCEM models suffer from the drawback of painting a limited picture of reality. In fact, an explanation of stylized facts in one model does not necessary hold true in another model. In addition, critics might argue that all the results are based on computer simulations and cannot be trusted blindly. This is a severe issue and earlier studies (Egenter et al. 1999; Zschischang and Lux 2001; Challet and Marslii 2002; Kohl 1997; Hellthaler 1996) have shown that the obtained stylized facts in many models are only numerical artifacts. More precisely, these studies revealed that for example the very influential Lux–Marchesi model and the Levy-Levy-Solomon model exhibit finite size effects (Egenter et al. 1999; Zschischang and Lux 2001). Finite size effects generally describe that different numbers of agents may lead to qualitative different model outputs. For that reason it is of paramount importance to simulate ABCEM models with a large number of agents. Further, Monte Carlo simulations have in general a poor convergence rate and thus a large amount of samples is required to obtain reliable results. Nevertheless, many ABCEM models are far to complex to study them by analytical methods and therefore computer simulations present the only feasible way. While a huge amount of ABCEM models is presented in literature, to our knowledge a unified model and perspective on ABCEM models is missing. Furthermore, there is no objective comparison possible between different models, since the models are implemented in different languages and simulated on different machines. In addition, we experienced difficulties while reproducing the results published in literature. This may have several reasons. First of all ABCEM models are usually non-linear dynamical systems, very sensitive to their parameters. Secondly, ABCEM models heavily depend on pseudo random numbers and thus it is impossible to reproduce published results exactly. Finally, many publications provide incomplete information regarding the implementation details, e.g. initial values of model quantities. These obstacles motivate us to establish a large scale simulation tool for multi-agent ABCEM models. Our software tool allows the implementation of many different ABCEM models with a reduced amount of coding. This is achieved by providing an object-oriented simulator implemented in C++. The main building blocks are agents and market mechanisms. The object-oriented software design enables the user to easily add and test new models by recombining existing agent type or market mechanism implementations using the XML-based configuration mechanism in SABCEMM. We refer to Sect. 3.2 for further discussion of the possibilities of creating new models with the SABCEMM (Simulator Agent-Based Computational Economic Market Models) tool. Another advantage of well-implemented C++ code is the computational speed which enables us to run models with several million agents on a Laptop. This lends SABCEMM particularly well for analysis of statistics of and sensitivity analyses for ABCEM models free of finite size effects. We note that SABCEMM is designed to run in serial, i.e. to not facilitate multiple cores on a multi-core system or multiple nodes on a compute cluster. This choice has been made acknowledging that for statistics and sensitivity analysis many simulations are required. Hence, parallelism can be achieved naturally by executing multiple instances of SABCEMM at the same time. SABCEMM is built on the novel unified model of ABCEM models derived in Sect. 2. We point out that the SABCEMM tool supports numerous pseudo random number generators and enables the user to carry out fair comparisons between different models. Our goal with the SABCEMM tool is to introduce a unified simulator which helps to compare and test ABCEM models. Especially the computational speed enables the user to make a large amount of numerical simulations. Thus, is is possible to obtain reliable (converged) simulation results. This converged state of the simulation output are necessary in order to perform a sensitivity analysis. To aid reproducibility, we publish the code and all examples discussed in this publication under an open source license. We implemented three ABCEM models in our simulator, namely the Levy-Levy-Solomon (LLS) model (Levy et al. 1994), the Cross model (Cross et al. 2005) and the Harras model (Harras and Sornette 2011). We carry out several experiments to analyze the computational efficiency of the SABCEMM simulator. Furthermore, we study the impact of different pseudo random number generators on the computational efficiency. We conducted unit tests and qualitative tests on the model output to verify our implementation. The outline of the paper is as follows: In Sect. 2 we properly define a unified model from an economic perspective. Then we present the SABCEMM software, which is the core part of this paper. More precisely, we introduce the software architecture in Sect. 3 and analyze the SABCEMM software with respect to computational efficiency in Sect. 3.4. This includes scaling behavior and the impact of different pseudo random number generators on the run time of our simulations. We finish this paper with a short conclusions of this work.",7
55.0,3.0,Computational Economics,30 July 2019,https://link.springer.com/article/10.1007/s10614-019-09911-0,Short Term Firm-Specific Stock Forecasting with BDI Framework,March 2020,Mansoor Ahmed,Anirudh Sriram,Sanjay Singh,Male,Unknown,Male,Male,"A stock market is defined as one where investors trade and issue shares of publicly held companies over Stock Exchanges. Since the advent of online trading more and more people are involved in stock trading. Now-a-days, trading is done in an electronic medium through Stock Exchanges. Mainly there are three types of investors in stock market: intraday traders, short-time investor and long-term investor. Stock market investments are subjected to great financial risk. Stock market investment decision has made many people rags to riches and vice-versa also. In view of high financial risk involved in stock market, an investor hesitates to invest in stock market. When a person starts investing in stock market he needs to make informed decision about stock related trading. In order to make a decision for stock market investment, an investor needs to read and analyze news about the firms or companies he is interested to invest in from various sources. However, in this Internet world, when everyday lots and lots of data is being generated, it becomes increasingly difficult to go through all the information related to stock market. For such a scenario there needed a comprehensive system which takes data from various business news sources, analyze and extract meaningful insight from those sources. The knowledge obtained from such a system can be used along with the previous stock market data for a particular company to make an informed decision about the stock market investment. Investors refer to individuals or institutions that provide capital resources to company, with the expectation of a financial return. Companies that trade in a stock market are broken up into smaller pieces of ownership, called shares. Each company contains shares of varying quantity. The investors obtain a portion of the company’s ownership in exchange to the money invested in stocks, giving them a stronghold in the company’s financial achievements. Stock Investors get returns through the dividends that companies provide and on selling the stocks they possess at an appreciated price. Investors may also end up in losses if they sell the Stock below the purchased value. Equity markets are considered to be a major source for financial gain and carry along with it, several other benefits. For example, $1 invested in the stock market in 1802 would be worth over $700,000 in today’s dollars. Over the years, stock markets have outperformed bonds, bills, gold and real estates. This factor has been highly motivational in our efforts to forecast the movement of stock prices. The performance of the stock market can be analyzed by looking at the Index. Indexes are a measure of value of a certain sector of the market. DOW JONES and NASDAQ in the United States, SENSEX and Nifty in India, S&P500, Morgan Stanley Europe, are some of the popular Indexes. The share price movement is governed by supply and demand in the marketplace. An increase in demand for a share leads to an increase in its valuation and vice-versa. However, there is no theory which can model the behavior of stock prices with absolute accuracy. Three factors and several sub factors that play a major role in determining a company’s stock prices are as follows (Money GSA 2014) Fundamental Factors: Expected growth in Earnings Per Share (EPS) Valuation Multiple (P/E Ratio), which is valuation ratio of a company’s current share price Announcement of Dividends Introduction of new Products/Services Management efficiency Management policies, Takeovers, Mergers Company’s performance relative to Industry standards Economic Factors: Interest Rates provided by commercial banks Economic outlook of the market Political dynamics of the country Economic policy changes GDP of the country, inflation/deflation factors Sentimental Factors: The sentiment of the investor plays a crucial role in the movement of stock prices. A bull market refers to a strong market which is closely associated with economic improvement and investor optimism. A reduced investor confidence points to a bear market whereas a weak market is often the result of factors like recession that affect the economy. This category is interwoven by a multitude of parameters, making it difficult to analyze stock behavior. The efficient-market hypothesis (EMH) states that financial markets are “informationally efficient” (Wikipedia 2014a). The hypothesis states that as new information enters the system the unbalanced stock is immediately discovered and quickly eliminated by the correct change in the price. This makes the future stock prices unpredictable based on historical data (Fama 1964). Stock market forecasting aims to determine the future value of a company stock to yield significant profit. In the coming subsections, we provide a review of the categories of common forecasting methods. Technical analysis is the forecasting of future financial price movements based on an examination of past price movements. Like other data exhibiting seasonality and trends like predicted rainfall, technical analysis does not result in absolute predictions about the future; rather, technical analysis gives investors a “window” of possibilities i.e., a tentative forecast of what is “likely” to happen. Technical analysis uses a wide variety of charts that show price over time. Thus, technical analysis is based on the premise that prices move in trends and that history repeats itself (StockCharts.com 2014b). Fundamental analysis is the examination of the underlying forces that affect the well-being of the economy, industry groups, and companies. It involves a thorough study of financial data, management, business concept as well as the relevant competition. It combines economic, industry, and company analysis to derive a stock’s current fair value and forecast future value (StockCharts.com 2014a). Fundamental analysis is beneficial in forecasting long-term trends and identifying companies with good valuation. In this paper we have largely used machine learning techniques which is a branch of Artificial Intelligence that is concerned with the construction and study of systems that can learn autonomously from data. Supervised learning is the subset of machine learning that is concerned with learning from labeled data. Traditionally, supervised learning algorithms have fallen into one of the following paradigms: Ensemble Methods Decision Trees Linear Regression Neural Networks Logistic Regression Support Vector Machines Perceptron Naive Bayes In this paper, we have endeavored to include at least one representative example of each of the categories for a thorough comparison across all supervised learning paradigms. The primary goal of this paper is to challenge the claims of the efficient market hypothesis and if successful, to provide a model for the short term prediction of stock value for a particular company. Simultaneously, we also set out to evaluate different classification paradigms and assess the efficiency and accuracy of each when applied to the stock market. In this paper we come up with a rebuttal for the Efficient Market Hypothesis by providing a model that enables the generation of above-parity predictions for next day forecasting for specific term vis-a-vis random guessing as the EMH implies. We first detail the procurement of the numerical as well as textual data that we require for the generation of the forecasts. We first utilize Technical Analysis tools on the numerical data and construct a sentiment analysis module to get useful features from the textual data. We also construct a neural network model to combine a separate set of factors closely correlated with the stock market with adjustable weights. Finally, we conduct a comprehensive comparison of various machine learning paradigms to choose the ones with the most consistent and accurate results. To model this entire process and to prioritize the sub-processes, we utilize a BDI agent framework and present the detail for the same. The impact of sentiments on stock prices has already been demonstrated (Mittal and Goel 2014; Bollen and Mao 2011) and we used this knowledge as one of the assumptions in our work. Sentiment analysis helps us to provide a pseudo-fundamental overview of a company on a real time basis thus, bridging the shortfalls encountered in a purely technical analysis. We have also performed a kappa analysis, which has been used for accuracy analysis in stock prediction (Gupta et al. 2013), of all the different parameters in order to assess the inter-annotator agreement. Lastly, we perform unsupervised clustering and compare it to different machine learning and statistical algorithms in order to find underlying trends in the parameters themselves. Be it the stock price forecasting or gauging the directional index of shares, analysis has primarily been fixated to markets on a whole and not on individual companies. This is essentially attributed to the presence of enormous sums of repositories on economic, technical and fundamental factors that are directly associated with the markets. Also, as all the global markets have amalgamated to form a closely correlated network, the market performance of a country can be estimated to a reasonable degree of certainty by evaluating the stock behavior of its counterpart across a different time-zone. This is in stark contrast to the case of predicting the behavior of a single company’s stock prices; the reason being a relatively lesser knowledge base to work on and feed to the system, to ascertain noticeable patterns for prediction. On a long term, the Stock Market Forecasting models previously researched upon reflect the performance of a unit business entity to a moderate level of confidence. But they do not accurately mimic the daily or short-term behavior of individual stocks. In order to reap financial rewards, it is vital for the institutional investors and day traders to know how individual stocks are expected to behave every single day. Knowing the outcome of tomorrow facilitates investors to trade the right stocks at the right time, thus maximizing profits. Our paper serves as a framework towards achieving the same by overcoming the challenges in firm-specific forecasting, and adopting a radical approach that projects the directional price movement of a single company for the forthcoming day. Various approaches for stock market forecasting are presented in Sect. 2, we do not claim it to be an exhaustive review of literature in stock forecasting. In order to prioritize and model the process of forecasting we have used BDI framework which is briefly discussed in Sect. 3. Sections 4–6 explains about the proposed system for stock forecasting. Performance analysis of the results obtained is discussed in Sect. 7. Finally, Sect. 8 concludes this paper with some future direction for this work.",5
55.0,3.0,Computational Economics,19 August 2019,https://link.springer.com/article/10.1007/s10614-019-09916-9,Financial Contagion in Core–Periphery Networks and Real Economy,March 2020,Asako Chiba,,,Female,Unknown,Unknown,Female,"The recent global financial crisis has revealed that borrowing and lending among banks and other financial intermediaries enables a shock on a single bank to spread through the entire system. As pointed out in Acharya et al. (2009), the financial crisis started with the bankruptcy of a subprime mortgage lender in the United States. Although subprime mortgages were just one of the many financial products, this event led many other financial intermediaries around the world to default and eventually caused the global financial crisis. The reason why a single shock led to such heavy systemic damage is that due to developments in the design of financial products, claims are more likely to be used as collateral for other claims. Many investment vehicles engaged in the issuance of new liabilities backed by assets obtained through the purchase of other claims. This activity led to multiple layers of complex borrowing and lending relationships, hiding how much and what kinds of risks each bank was exposed to. In other words, financial intermediaries have come to indirectly hold many assets through borrowing and lending. Given that many of these intermediaries ended up in default highlights that the interdependencies among financial institutions have the potential to generate chain reaction bankruptcies. Therefore, it is essential to understand financial markets as networks when designing regulations in order to prevent a system-wide breakdown. Against this background, to provide a stylized representation of the interbank lending market, the present paper constructs a model in which banks’ liabilities form a core–periphery network. The model is then employed to simulate contagion in the network in the wake of the hypothesized insolvency of one of the banks. A key feature of the model is that asset prices decline as contagion spreads. The main contributions of the analysis are as follows. First, the simulation shows that when asset prices decline as contagion spreads, the degree of contagion in response to an insolvency is significantly greater than when prices are assumed to be exogenous. Second, the core–periphery network analysis shows that insolvency increases non-monotonically in the strength of the links. Given the linkages of cross-holdings between core banks and peripheral banks, the likelihood of contagious insolvency is high if the strength of the linkages between core banks is in the intermediate range. On the other hand, if the linkages between core banks are extremely strong or extremely weak, the likelihood of contagious insolvency is low. The reason is that, if the core banks cross-hold only a small fraction of liabilities with one another, contagion between core banks is not likely. If their cross-holdings are strong, each bank’s vulnerability to asset price fluctuations is small, so that a devaluation of assets does not have sufficient impact to damage banks’ market capitalization. The view that financial interlinkages tend to expand the magnitude of shocks has been gaining traction among economists. A large body of work has focused on the role of interconnections among banks as amplifiers of shocks. Most studies take the structure of interbank relationships as given and analyze the likelihood of contagion using comparative statistics. This strand of literature dates back to the work of Allen and Gale (2013), Freixas et al. (2000), and Eisenberg and Noe (2001). Constructing a hypothetical network with four banks, Allen and Gale (2013) illustrate that the network structure among banks determines whether contagion occurs or not. When all banks are completely connected to each other, the impact of a shock is so small that there is no contagion, since the shock is shared among banks. However, if each bank has links to only a fraction of the other banks, those that do have links with the bank initially hit will incur substantial losses when trying to liquidate long-term assets, which in turn will spread the contagion to other banks. Freixas et al. (2000) show that networks may also be fragile if there are money-center banks and the banks are only linked to these banks at the center. Eisenberg and Noe (2001) show that a set of payments of banks that satisfies their obligations uniquely exists when the possibility of other banks’ default in the future is taken into account. Awiszus and Weber (2015) incorporate direct liabilities, cross-holdings and fire sales, and characterize the equilibrium as the set of clearing payments and the price of a liquid asset, under the assumption that banks hold a common liquid asset. While these prominent studies on financial networks mainly deal with the liquidity of financial intermediaries, some more recent work focuses on asset prices and the corporate value of banks. Eisenberg and Noe (2001), for example, provide a comparative analysis of the extent of financial contagion as a function of the structure of the linkages among financial institutions. The main results of their analysis, which focuses primarily on random networks, show how the probability and the degree of cascades are affected by two features of the networks: integration and diversification. Integration refers to how much of a bank’s value is held by other banks. Diversification refers to how many other banks a bank’s value is held by. Each of these two features has a non-monotonic effect on the extent to which the insolvency of one bank is amplified. The model in this paper borrows heavily from their work to examine the spread of default from one bank to another bank connected through a given network of liabilities. There are two differences between their study and this paper. First, the model in this study includes a feedback effect between the spread of bank failures and the reduction in asset prices accompanying such bank failures. Specifically, the model describes how the price of each non-financial claim decreases as the number of failed banks increases. This extension reflects the frequently observed fact that asset prices fall together rapidly when the economy is experiencing contagious insolvency. The main contribution of this paper is to provide an analytical explanation of this fact. The second difference is that the model employed in this study mainly focuses on core–periphery networks. As several empirical studies show, interbank networks consist of two types of banks: local banks and global banks. Using the data in the banking sector in the United States, Battiston et al. (2012) show that a group of banks which are strongly connected with one another becomes systemically important in the crisis time. Also, they show that the systemic default can be caused by a small shock. Fricke and Lux (2015) detect that the interbank lending in Italy takes the form of core–periphery structure, and remarkably, they show that the banks’ identified position of whether they are cores or peripherals is persistent over time. By the fact that this feature is observed in many developed and developing countries, such as India, Mexico, the Netherlands, and the United Kingdom, they emphasize that the core–periphery structure in banking sectors is a new stylized fact. To reflect this reality, the model in this paper distinguishes global banks and local banks to examine insolvency in overall banking sector followed by a shock to a small local bank. Local banks borrow from and lend to households or corporates in limited regions, whereas global banks’ financing and investment relies on many other global banks and local banks. In the model, global banks are treated as hub nodes that are linked to each other, while local banks are treated as small nodes that are linked to a small number of hub nodes. The contribution regarding this setting is to provide an analytical explanation to contagious phenomena in terms of the strength of the links. Despite the persistence of core–periphery structure in banking sectors, few features have been revealed about core–periphery graph in the existing literature. This would be partly because the graph includes heterogeneity in nodes which require to be treated differently, where random graphs and ring graphs consist of homogeneous nodes, and are intensely focused on by a growing body of literature. To the best of the author’s knowledge, this paper is the first to analytically show that insolvency increases non-monotonically in the strength of the links between core banks. The model in this paper does not include the endogenous formation of linkages although there are many recent studies dealing with this issue. Zawadowski (2013), Babus (2016), Farboodi (2014), and Acemgolu et al. (2015) explicitly model interbank liabilities to investigate the relationship between counterparty risk and the equilibrium interest rate. Farboodi (2014) develops a model of endogenous intermediation among debt-financed banks and shows that if some banks have access to investment, equilibrium networks have a core–periphery structure and may not be efficient. Acemgolu et al. (2015) also consider the endogenous formation of financial linkages, but in their setting, it is the interest rate that is endogenously determined, whereas Farboodi’s model takes the face value of debt and the allocation of intermediation rents as exogenously given. This strand of literature points out, first, that banks enter contracts voluntarily, and second, that models should treat network structures as something that is endogenously determined. In other words, regulations based on network analyses that do not regard the network structure as the equilibrium are subject to the critique suggested by Lucas (1976). Although this critique is reasonable, it is still meaningful to examine the impact that a shock to one of the nodes in an exogenously given network has from a short-term perspective. That is, when modeling the contagion triggered by a crisis event, the network structure can be regarded as fixed since in the short-run agents’ ability to optimize linkages is extremely limited. Against this background, the aim of the current study is to examine the short-term behavior of banks in a network, focusing in particular on the case when asset prices are assumed to decline as contagion spreads.",1
55.0,3.0,Computational Economics,19 August 2019,https://link.springer.com/article/10.1007/s10614-019-09913-y,Risk-Constrained Kelly Portfolios Under Alpha-Stable Laws,March 2020,Niels Wesselhöfft,Wolfgang K. Härdle,,Male,Male,Unknown,Male,"Given a set of investment opportunities, how should the investment weights be chosen in order to have more wealth than any other investor at the end of the investment period? The Kelly growth-optimum strategy is a betting scheme for an investor, who seeks to asymptotically maximize his growth rate of capital. This strategy outperforms any other significantly different strategy, given knowledge of the true underlying process (Breiman 1961). But, the sole use of the Kelly criterion implies larger bets than a representative, risk-averse investor would accept in terms of risk (Clark and Ziemba 1987; Hausch and Ziemba 1985). Thus, the Kelly optimization needs to be restricted by a risk measure. We use \(\alpha \)-stable laws and its scaling behavior in order to model the underlying financial market returns. Upon the Generalized Central Limit Theorem (GCLT), the horizon distribution is modelled in an discrete i.i.d. framework. The aim is to maximize the geometric portfolio return, i.e. Kelly Criterion and restrict the objective to a subjective risk constraint, formulated as spectral risk measure, including quantile (VaR) or expected shortfall as special cases. The formulated trade-off introduces a mapping over growth and risk in order to evaluate the investment decision. The contribution of this paper is three-fold: The first contribution represents the application of multidimensional \(\alpha \)-stable laws, in the form of elliptically \(\alpha \)-stable distributions, to the constrained Kelly portfolio. Second, instead of simulating from the class of elliptically \(\alpha \)-stable distributions, a semiparametric scaling approximation, based on the data set itself, is proposed. Third, assets with non-linear payoff structure, long put-options, are incorporated into the nonlinear optimization to allow for asymmetric payoffs, which lead to a higher growth criterion, given a fixed risk constraint. The Kelly criterion originates from Kelly (1956), dealing with, from the point of information theory, an optimal investment strategy in a binary channel. Breiman (1961) formally proves the asymptotic outperformance of the Kelly strategy for arbitrary distributions in an i.i.d. world. For arbitrarily distributed, possibly non-stationary processes, those results have been extended by Algeot and Cover (1988). Incorporating risk measures into the Kelly optimization, MacLean et al. (1992) discuss the growth-risk trade-off in terms of efficiency. Roll (1973) compares the Markowitz arithmetic mean maximization with the Kelly geometric mean maximization. In contrast to Constant Proportion Portfolio Insurance (CPPI), the investment strategy remains fixed fraction, given stationarity. More recently, Busseti et al. (2016) introduce an alternative risk constraint, limiting the probability of a drawdown of wealth to a given undesirable level. The distribution of financial market returns for a chosen horizon is modelled as the sum of hourly random variables. As the distribution in some horizon is presumed to be non-Gaussian, the classical Central Limit Theorem (CLT) does not apply as second and higher moments may not exist. Thus, the generalized central limit theorem (GCLT) of Gnedenko and Kolmogorov (1954) is applied for the sum of random variables, whose second and higher moments may not be bounded. For the financial application this implies the use of \(\alpha \)-stable laws (Fama 1965; Lévy 1925; Mandelbrot 1963). As multidimensional \(\alpha \)-stable random variables are difficult to evaluate for larger dimensions, elliptical \(\alpha \)-stable distributions are employed, allowing for efficient portfolio estimation for dimensions \(k\le 40\) (Nolan 2013) in the presence of linear dependence. Price data, both for assets with linear and non-linear payoff structure, were gathered from Lobster and Bloomberg. For computation, Matlab 2016a was utilized. In order to solve the formulated nonlinear optimization problem the sequential quadratic algorithm in fmincon was employed. The paper is organized as follows: In chapter one the portfolio allocation problem is stated. The financial model is formulated by using generalized measures for growth and risk. Chapter two, the estimation, starts with a case for non-Gaussianity of financial log-returns of different sampling frequencies, reasoning the utilization of \(\alpha \)-stable laws. For the multidimensional case, elliptically \(\alpha \)-stable distributions are introduced in order to have an analytically tractable class of distributions. As the semiparametric scaling approximation is introduced, the estimation of location and scale is illustrated. An application is given in chapter three, the implementation. For a representative investor with a planning horizon of one year, the optimally VaR/ES-constrained Kelly portfolios are found, benefitting from the protective put strategy.",3
55.0,3.0,Computational Economics,19 August 2019,https://link.springer.com/article/10.1007/s10614-019-09915-w,An Optimal Stopping Problem of Detecting Entry Points for Trading Modeled by Geometric Brownian Motion,March 2020,Yue Liu,Aijun Yang,Jingjing Yao,,Unknown,Unknown,Mix,,
55.0,3.0,Computational Economics,20 August 2019,https://link.springer.com/article/10.1007/s10614-019-09914-x,Applying the Explicit Aggregation Algorithm to Heterogeneous Macro Models,March 2020,Takeki Sunakawa,,,Unknown,Unknown,Unknown,Unknown,,
55.0,3.0,Computational Economics,05 September 2019,https://link.springer.com/article/10.1007/s10614-019-09912-z,Estimating a Dynamic Factor Model in EViews Using the Kalman Filter and Smoother,March 2020,Martin Solberger,Erik Spånberg,,Male,Male,Unknown,Male,"Dynamic factor models are used in data-rich environments. The basic idea is to separate a possibly large number of observable time series into two independent and unobservable, yet estimable, components: a ‘common component’ that captures the main bulk of co-movement between the observable series, and an ‘idiosyncratic component’ that captures any remaining individual movement. The common component is assumed to be driven by a few common factors, thereby reducing the dimension of the system. In Economics, dynamic factor models are motivated by theory, which predicts that macroeconomic shocks should be pervasive and affect most variables within an economic system. They have therefore become popular among macroeconometricians; see, e.g., Breitung and Eickmeier (2006), for an overview. Areas of economic analysis using dynamic factor models include, for example, yield curve modeling (e.g., Diebold and Li 2006; Diebold et al. 2006), financial risk-return analysis (Ludvigson and Ng 2007), monetary policy analysis (e.g., Bernanke et al. 2005; Boivin et al. 2009), business cycle analysis (e.g., Forni and Reichlin 1998; Eickmeier 2007; Ritschl et al. 2016), forecasting (e.g., Stock and Watson 2002a, b) and nowcasting the state of the economy, that is, forecasting of the very recent past, the present, or the very near future of indicators for economic activity, such as the gross domestic product (GDP) (see, e.g., Banbura et al. 2012, and references therein). Information of the economic activity is of great importance for decision makers in, for instance, governments, central banks and financial markets. However, first official estimates of GDP are published with a significant delay, usually about 6–8 weeks after the reference quarter, which makes nowcasting very useful. Despite the attractiveness of dynamic factor models for macroeconomists, statistical or econometric software do not in general provide these models within standard packages. In this paper, we illustrate how to, by means of programming, set up the popular two-step estimator of Doz et al. (2011) in EViews (IHS Global Inc. 2015a, b, c, d), a software specialized in time series analysis that is broadly used by economists, econometricians, and statisticians. The parameters of dynamic factor models can be estimated by the method of principal components. This method is easy to compute, and is consistent under quite general assumptions as long as both the cross-section and time dimension grow large. It suffers, however, from a large drawback: the data set must be balanced, where the start and end points of the sample are the same across all observable time series. In practice, data are often released at different dates. A popular approach is therefore to cast the dynamic factor model in a state space representation and then estimate it using the Kalman filter, which allows unbalanced data sets and offers the possibility to smooth missing values. The state space representation contains a signal equation, which links observed series to latent states, and a state equation, which describes how the states evolve over time. Under the assumption of Gaussian noise, the Kalman filter and smoother provide mean-square optimal projections for both the signal and state equations. The method we set up in this paper is a two-step procedure, in which parameters are first estimated by principal components, and then, given these estimates, the factors are re-estimated as latent states by the Kalman smoother. The rest of the paper is organized as follows. Section 2 outlines the notion and conventional estimation of dynamic factor models. Section 3 derives a state space solution. Section 4 describes the estimator considered in this paper. The estimator is evaluated in a simulation study in Sect. 5, and applied in an empirical example in Sect. 6. Section 7 concludes. A subroutine containing the estimator and programs that replicate our results are provided as supplementary material.",7
55.0,3.0,Computational Economics,01 October 2019,https://link.springer.com/article/10.1007/s10614-019-09922-x,Classifier Based Stock Trading Recommender Systems for Indian stocks: An Empirical Evaluation,March 2020,V. Vismayaa,K. R. Pooja,P. N. Kumar,Unknown,Unknown,Unknown,Unknown,,
55.0,3.0,Computational Economics,04 October 2019,https://link.springer.com/article/10.1007/s10614-019-09921-y,Solving Stochastic Dynamic Programming Problems: A Mixed Complementarity Approach,March 2020,Wonjun Chang,Michael C. Ferris,Thomas F. Rutherford,Unknown,Male,Male,Male,"Dynamic programming (DP) is a standard optimization principle used to solve dynamic optimization problems. Due to the simple yet flexible feature of the Bellman equation, DP serves as a tractable solution method for dynamic optimization problems, especially when uncertainty is involved. Over the past two decades, advances in numerical integration and approximation methods have made the application of DP to numerical economic modeling more accessible. Models are now less limited by algebraic tractability and can make more room for essential details of the economic system (Judd 1998; Rust 1996; Manuelli and Sargent 2009; Wright and Nocedal 1999; Miranda and Fackler 2004). Aided by the increasing use of DP in large scale economic models, numerical solution methods that can deal with a great number of continuous and discrete state variables have also been researched (Judd et al. 2014; Maliar and Maliar 2014, 2015; Powell 2011). As a result, there exists a multitude of DP based algorithms, each designed to achieve solution accuracy and computation efficiency for various types of numerical problems. One field that benefits from these methodological advancements in particular, is climate economics. Stochastic recursive formulations of climate integrated assessment models (IAMs) are important tools in assessing policy implications of the uncertainty that surround climate change. However, given the complexity and size of even the simplest climate IAMs, the application of DP would be severely restricted without the use of advanced numerical methods. A popular and effective solution method used for stochastic IAMs is value function iteration (VFI), in which projection methods are employed to approximate the value function (Cai et al. 2012; Lemoine and Traeger 2014, 2016; Rudik 2016; Traeger 2014a, b; Cai and Judd 2015). Prevalence of VFI as a solution method in climate, environmental and resource economics has called for surveys on frontier numerical methods that further strengthen VFI as an effective and viable solution method in these fields (Lemoine and Rudik 2017; Cai 2018; Cai and Judd 2014). In general, projection based VFI methods are widely considered workhorse solution methods for discrete-time DP problems. This paper presents a mixed complementarity problem (MCP)Footnote 1 formulation of projection based VFI for discrete time, continuous state DP problems (DP-MCP). To illustrate our approach, we use the collocation method illustrated in Judd (1998) and Miranda and Fackler (2004). VFI collocation (henceforth simply referred to as VFI) is a standard fixed-point solution method in DP, used widely for its monotonic convergence properties and straightforward implementation. Despite its stability however, conventional VFI has several drawbacks that limit application to complex models. The first is that convergence is slow, which is particularly the case in economic growth models with a discount factor close to unity. A second drawback is the curse of dimensionality. In VFI, the cost of computation increases exponentially in the number of state variables, rendering the solution method intractable for large scale applications.Footnote 2 We also point to the iterative component itself as a drawback. Iteration between function approximation and value maximization can make VFI implementation time consuming, which is made worse by aforementioned weaknesses. Reformulating conventional VFI as a complementarity problem removes the iterative aspect of the algorithm. The MCP of VFI is written as a square system of equilibrium constraints that encompass: Bellman’s optimality conditions with respect to a vector of control variables, a, given a vector of coefficients in value function approximant, \(\alpha \); optimality conditions for function approximation with respect to the vector of coefficients in value function approximant , \(\alpha \), given a vector of control variables,a; for which the solution is a pair \((a,\alpha )\) that characterizes an equilibrium between the two alternating objectives of VFI. And although the MCP formulation per se does not attend to the curse of dimensionality, the use of computational methods such as Smolyak sparse grids (Smolyak 1963; Judd et al. 2014; Maliar and Maliar 2014) and Nonlinear Certainty Equivalent Approximation (Cai et al. 2017) extends the application of our method to large scale DP problems. The practicality of our solution method however comes at a cost. Formulating the complementary conditions of VFI involves the tedious task of hand-coding both primal and dual equations, which can double the scope for coding error. We hence introduce a framework for implementation–the use of the Extended Mathematical Programming (EMP) framework in GAMS (Kim and Ferris 2019; Ferris et al. 2009) mitigates such errors by automating the formulation of the MCP. The EMP framework also enables the modeler to flexibly adapt the solve procedure to the properties of the DP problem, further enhancing computational performance. Our interest in the use of complementarity methods for solving dynamic programming problems was inspired by the work of Dubé et al. (2012) and Su and Judd (2012). Their work addresses structural estimation of discrete choice problems in a math programming with equilibrium constraints (MPEC) framework. Equilibrium conditions are used to obtain solutions to the Bellman equation in a one-shot fashion. Our objective is to demonstrate how their use of equilibrium conditions can be extended to discrete-time, continuous-choice problems, particularly to ones that feature occasionally binding constraints in the form of complementary conditions. The paper lays out three sample applications: two standard nonlinear problems of stochastic economic growth; and a complementarity problem, with which we demonstrate the extension of DP-MCP to DP problems that are more naturally formulated as an MCP. To keep things simple, we deal with infinite-horizon problems and use Chebyshev collocation as the default projection method. For further simplicity, we minimize the sum of square residuals to compute the coefficient vector of the function approximant. The paper is organized as follows. Section 2 provides an introduction to complementarity problems and presents the DP-MCP formulation of VFI. Section 3 provides an overview of the EMP framework and provides the EMP representation of DP-MCP. In Sects. 4 and 5, we provide simple numerical examples of stochastic optimal growth models, including a 4-sector model based on Global Trade Analysis Project (GTAP) data. In Sect. 6, we extend our method to a complementarity problem–a DP problem already formulated as an MCP. Section 7 concludes.",2
55.0,3.0,Computational Economics,05 October 2019,https://link.springer.com/article/10.1007/s10614-019-09923-w,A Numerical Solution of Optimal Portfolio Selection Problem with General Utility Functions,March 2020,Guiyuan Ma,Song-Ping Zhu,Boda Kang,Unknown,,Unknown,Mix,,
55.0,3.0,Computational Economics,08 October 2019,https://link.springer.com/article/10.1007/s10614-019-09925-8,Forecasting Financial Networks,March 2020,Petre Caraiani,,,Male,Unknown,Unknown,Male,"There is a rapidly growing interest in applying networks to various topics in finance and economics. Most of the research has focused on either the static properties of the financial (and economics) networks, see Tumminello et al. (2005) or applying static network models to the financial system, see Acemoglu et al. (2015), or the macroeconomy, see Acemoglu et al. (2012), to cite only a few of the most representative papers. A more in-depth review of the current state of literature, though with a focus on the macroeconomy, can be found in Carvalho (2014). Lately, there has also been some interest regarding the dynamics of the economics and financial networks, although the research is more limited here. A significant part of this research has rather focused on deriving dynamic measures using financial (or economic) networks. For example, Song et al. (2011) used daily data from multiple stock markets around the world to reveal two types of dynamics. On one hand, there are the so-called slow dynamics related to globalization, while, on the other hand, there are fast dynamics which have a pronounced link with the crisis episodes on the stock markets. A similar study, Kenett et al. (2012) applied a so-called index cohesive force of financial networks as well as meta-correlations, using data from world financial markets. They distinguished between two types of behavior: one characteristic to the Western stock markets from developed economies, and a second one characteristic to Asian financial markets from emerging economies. However, only recently there has been a growing interest in forecasting financial networks. For example, Hautsch et al. (2014) focused on forecasting the systemic risk associated to financial networks, while Huang et al. (2015) used time series to predict the link structure of dynamic networks. However the first paper does not focus on forecasting financial networks per se, while the second rather focuses on the link structure and does not actually compare the methods in a rigorous econometric framework. Up to now, there is no research focusing on how to forecast with networks and evaluate the forecasts of financial networks using traditional econometric approaches. Forecasting using networks is a significant topic for several reasons. First of all, there is a rapidly growing literature on using networks in economics, however their ability to be used in forecasting has not been questioned. Second, networks include significant information by taking into account the relationship between the nodes (which can be economic agents, financial stocks, banks, etc) and implying a certain network structure. This paper tries to fill a significant gap in the literature by asking two pertinent questions and proposing answers to them: First, can we forecast networks using traditional econometric methods? Second, does the use of the network structure improve the forecasting accuracy of the networks? The research is related to several strands of literature. It is firstly related to the use of network in economics and finance and it enriches this strand of literature by considering a new application: forecasting networks. It is also related to the field of forecasting by proposing an econometric methodology to forecast financial networks. Not at last, it is also related (though in a looser manner) to the newer literature that seeks to study the structure of networks and infer it from data, see van Borkulo et al. (2014), for binary data, or McCarty et al. (2001) for personal data, although the present paper rather asks whether we are able to perform forecasts of financial networks using time series methods and how accurate are these forecasts. This study contributes to the literature in the following ways: I propose an indirect way to forecast financial networks by doing forecasts of the underlying time series. I forecast the underlying time series using traditional time series approaches. I further evaluate the accuracy of forecasting using econometric approaches by focusing on the structural properties of the forecasted networks.",2
55.0,3.0,Computational Economics,09 October 2019,https://link.springer.com/article/10.1007/s10614-019-09924-9,A Radial Basis Function-Generated Finite Difference Method to Evaluate Real Estate Index Options,March 2020,Xubiao He,Pu Gong,,Unknown,,Unknown,Mix,,
55.0,3.0,Computational Economics,11 October 2019,https://link.springer.com/article/10.1007/s10614-019-09919-6,A Perturbation Method to Optimize the Parameters of Autoregressive Conditional Heteroscedasticity Model,March 2020,Xuejie Feng,Chiping Zhang,,Unknown,Unknown,Unknown,Unknown,,
55.0,4.0,Computational Economics,08 May 2020,https://link.springer.com/article/10.1007/s10614-020-09992-2,Macroeconomic Dynamics and Modelling on Chinese Economy,April 2020,Ling-Yun He,Hua-Qing Wu,,,,Unknown,Mix,,
55.0,4.0,Computational Economics,08 March 2019,https://link.springer.com/article/10.1007/s10614-019-09888-w,Modified Distance Friction Minimization Model with Undesirable Output: An Application to the Environmental Efficiency of China’s Regional Industry,April 2020,Qingxian An,Xiangyang Tao,Jinlin Li,Unknown,Unknown,Unknown,Unknown,,
55.0,4.0,Computational Economics,29 November 2018,https://link.springer.com/article/10.1007/s10614-018-9872-z,Reducing Overcapacity in China’s Coal Industry: A Real Option Approach,April 2020,Wei Wu,Boqiang Lin,,,Unknown,Unknown,Mix,,
55.0,4.0,Computational Economics,12 August 2018,https://link.springer.com/article/10.1007/s10614-018-9839-0,Measuring the Energy Saving and CO2 Emissions Reduction Potential Under China’s Belt and Road Initiative,April 2020,Yue-Jun Zhang,Yan-Lin Jin,Bo Shen,,,Male,Mix,,
55.0,4.0,Computational Economics,20 September 2018,https://link.springer.com/article/10.1007/s10614-018-9857-y,Dynamic Correlation and Risk Contagion Between “Black” Futures in China: A Multi-scale Variational Mode Decomposition Approach,April 2020,Qunwei Wang,Xingyu Dai,Dequn Zhou,Unknown,Unknown,Unknown,Unknown,,
55.0,4.0,Computational Economics,21 September 2018,https://link.springer.com/article/10.1007/s10614-018-9858-x,Forecasting Short-Term Oil Price with a Generalised Pattern Matching Model Based on Empirical Genetic Algorithm,April 2020,Lu-Tao Zhao,Guan-Rong Zeng,Ya Meng,Unknown,,,Mix,,
55.0,4.0,Computational Economics,18 July 2018,https://link.springer.com/article/10.1007/s10614-018-9837-2,Abandonment Decision-Making of Overseas Oilfield Project Coping with Low Oil Price,April 2020,Hui-Ling Zhou,Bao-Jun Tang,Hong Cao,,,,Mix,,
55.0,4.0,Computational Economics,20 September 2018,https://link.springer.com/article/10.1007/s10614-018-9856-z,Game-Theoretic Analysis of Price and Quantity Decisions for Electric Vehicle Supply Chain Under Subsidy Reduction,April 2020,Jinshi Cheng,Jiali Wang,Bengang Gong,Unknown,Unknown,Unknown,Unknown,,
55.0,4.0,Computational Economics,12 February 2019,https://link.springer.com/article/10.1007/s10614-019-09884-0,Analysis of China’s Regional Economic Environmental Performance: A Non-radial Multi-objective DEA Approach,April 2020,Tao Ding,Zhixiang Zhou,Liang Liang,,Unknown,,Mix,,
55.0,4.0,Computational Economics,21 March 2019,https://link.springer.com/article/10.1007/s10614-019-09886-y,Forecasting Trade Potential Between China and the Five Central Asian Countries: Under the Background of Belt and Road Initiative,April 2020,Rongji Huang,Tengfei Nie,Shaofu Du,Unknown,Unknown,Unknown,Unknown,,
55.0,4.0,Computational Economics,15 October 2018,https://link.springer.com/article/10.1007/s10614-018-9862-1,A Novel Decomposition-Ensemble Based Carbon Price Forecasting Model Integrated with Local Polynomial Prediction,April 2020,Quande Qin,Huangda He,Ling-Yun He,Unknown,Unknown,,Mix,,
55.0,4.0,Computational Economics,27 February 2019,https://link.springer.com/article/10.1007/s10614-019-09882-2,Markov Regime-Switching in-Mean Model with Tempered Stable Distribution,April 2020,Yanlin Shi,Lingbing Feng,Tong Fu,Unknown,Unknown,,Mix,,
56.0,1.0,Computational Economics,01 July 2020,https://link.springer.com/article/10.1007/s10614-020-10011-7,Introduction to Topics in Modelling Financial and Macroeconomic Time Series,June 2020,Fredj Jawadi,,,Unknown,Unknown,Unknown,Unknown,,
56.0,1.0,Computational Economics,28 May 2020,https://link.springer.com/article/10.1007/s10614-020-09991-3,Optimal Portfolio Choice Under Shadow Costs with Fixed Assets when Time-Horizon Is Uncertain,June 2020,Mondher Bellalah,Detao Zhang,Panpan Zhang,Unknown,Unknown,Unknown,Unknown,,
56.0,1.0,Computational Economics,11 May 2019,https://link.springer.com/article/10.1007/s10614-019-09894-y,Optimal Portfolio Positioning on Multiple Assets Under Ambiguity,June 2020,Hachmi Ben Ameur,Mouna Boujelbène,Emna Triki,Unknown,Female,Unknown,Female,"
Markowitz (1952) introduced for the first time the key notion of risk when dealing with portfolio optimization (see the so-called mean-variance theory). Since the publication of this seminal paper, many extensionsFootnote 1 have been proposed to determine the optimal financial portfolio when facing risk and randomness. For instance, Merton (1971) generalized Markowitz’s results in a continuous-time setting, using the expected utility theory (EUT) introduced by von Neumann and Morgenstern (1947). Further extensions have been proposed to take account of market incompleteness, of specific constraints on portfolio weights, of labor income and random horizon (see e.g. Cox and Huang 1989; Cvitanic and Karatzas 1996; El Karoui et al. 2005; Prigent 2007). However, the expected utility theory is not often compatible with observed attitudes towards risk of many investors, leading to various paradoxes such as the Allais paradox (1953). Indeed, one of the key axiom of EUT, namely the independence axiom, is often violated. To take account of such feature, alternative decision criteria have been introduced yielding among others to behavioral finance. The EUT supposed also that individuals share the same complete information about probability distributions of random events. However, as emphasized by Knight (1921), a first fundamental distinction between risk (the probability of occurrence of an event is known) and uncertainty (the probability of occurrence of an event is unknown) must be made. Additionally, as illustrated by Ellsberg (1961), individuals react differently to subjective and objective probabilities: they are ambiguity averse. For instance, facing two options, an individual is more likely to bet on the scenario where the probability of occurrence of the different alternatives is known. Considering that individuals have not only one belief (unique subjective distribution) but a set of believes (second order subjective distribution), Gilboa and Schmeidler (1989) have introduced the multiple-priors approach. In this ambiguity framework, the decision maker does not determine only one probability for one state of nature but faces instead a set of probability distributions. This leads to the maxmin expected utility (MEU) criterion, which allows to differentiate risk and ambiguity and to introduce ambiguity aversion . For this purpose, Maccheroni et al. (2006) model the preferences under ambiguity by considering both an utility function U on outcomes and an ambiguity index C on the set of probabilities defined on the random events. Maccheroni et al. (2006) propose the following representation of preferences: For all random variables X and Y representing potential results or consequences and with values in \([-M,M]\), we have: The function U corresponds to the risk aversion while index C models individual attitude towards ambiguity. For a given set \(\Delta \), the lower C, the higher the ambiguity aversion. Therefore, the model introduced by Maccheroni et al. (2006) includes the case of multiple-prior preferences introduced by Gilboa and Schmeidler (1989) and the mean-variance preferences of Markowitz (1952) and Tobin (1958). The MEU criterion of Gilboa and Schmeidler (1989) corresponds to the case where \(C=0\) and the convex set \( \Delta \) is the individual’s set of multipriors which models ambiguity. The mean-variance preference of Markowitz (1952) and Tobin (1958) corresponds to \(U(X)=X\) and \(C(X)=-\frac{\varphi }{2}Var(X)\). This theoretical result allows among others for a rational procedure for evaluating risky portfolios in an environment characterized by uncertainty. Indeed, in the portfolio optimization framework, the prediction of financial parameters is rather hard. To remedy this issue, Hansen and Sargent (2000, 2001) have introduced a robust control approach, allowing to take account of misspecification of the probability due to deficient information about randomness. Compared to the decision criterion of Maccheroni et al. (2006), the approach developed by Hansen and Sargent (2000, 2011) corresponds to a particular choice of the index C, namely the relative entropy. Thus, the ranking of decisions proposed by Hansen and Sargent (2000, 2011) is given by: where \(R(\mathbb {P},\mathbb {P}_{0})\) denotes the relative entropy with respect to the given probability distribution \(\mathbb {P}_{0}\).Footnote 2 This kind of preferences is known as “multiplier preferences”. Parameter \(\xi \) corresponds to the weight given by the investor to the possibility that \( \mathbb {P}_{0}\) is not the appropriate probability distribution, due to the lack of information. The choice of optimal portfolios under ambiguity has been studied in various settings. Recall that Markowitz (1952) considers a static portfolio allocation where portfolio returns are linear combinations of basic asset returns. In this framework, Pflug and Wozabal (2007) consider a maximin criterion which is based on a confidence set for probability distribution. They analyze the trade-off between return, risk and robustness with respect to ambiguity (see also Wozabal 2012) for the case of non-parametric ambiguity sets). Assuming that a given nominal discrete return distribution, Calafiore (2007) provides the optimal robust portfolios when the true distribution is unknown except that it lies within a given distance from the nominal one computed according to the Kullback-Leibler divergence criterion. Pflug et al. (2012) prove that, for a large class of risk measures, the equally weighting investment strategy is rational for investors facing a significantly high degree of ambiguity about loss distributions. Koziol et al. (2011) take account of the ambiguity of institutional investors towards specific assets. They show that institutional investors have a strong ambiguity aversion and that equities and bonds have much lower ambiguity than other investments such as real estate, private equities, and hedge funds. In the continuous-time setting, Fei (2007) deals with the portfolio optimization with respect to the recursive multiple-priors utility, providing explicit solutions for power and logarithmic felicity functions. Liu (2011) studies the same problem but when expected risky returns are driven by a hidden Markov chain. He emphasizes the importance of hedging demands in optimal portfolio strategies under ambiguity aversion. Note that robust optimization approach permits to investors to determine solutions for optimization problems when financial parameters are ambiguous. It is a matter of determining the optimal allocation of assets in presence of static errors at the level of the estimation of financial parameters. In this framework, many works were proposed to take account of the ambiguity effect. In this context, Tütüncü and Koenig (2004) have developed for example a robust optimization approach. These authors assume that uncertainty is about the expected return vector and covariance matrix. They treat the robust optimization problem as a saddle-point problem with some semi-defined constraints. Also, Maenhout (2004) has extended the analysis of return-to-average risk premium and has obtained a solution for the optimal portfolio in presence of ambiguity. Later, Fei (2007) has established the optimal consumption and the optimal portfolio on taking into consideration the ambiguity and the investor anticipation. He has found that martingale methods and Malliavin calculus can be used in order to solve the optimization problem of the investor and that ambiguity indeed affects the investor choice. Pflug and Wozabal (2007) have determined the optimal portfolio when the underlying distribution of the probability is not perfectly known. They have applied the maxmin approach which allows to take explicitly account of the uncertainty on the “right” probability distributions. Their approach uses a confidence interval to determine the distribution of probability. Liu (2011) has dealt with the portfolio optimization problem in uncertain framework in which risky assets follow an hidden Markov chain. He has found that ambiguity aversion has an important effect on the request for coverage in the portfolio optimization strategy. Fabozzi et al. (2010) have studied the robust portfolio optimization strategy not only in terms of mean-variance but also in terms of two risk measures, namely VaR (Value-at-Risk) and CVaR (conditional Value-at-Risk). In addition, El Ghaoui et al. (2003) have used the VaR as a criterion to select the robust optimal portfolio. 
Ismail and Pham (2016) examine the problem of selection of the robust optimal portfolio through the mean-variance approach in dynamic framework and in presence of the uncertainty in the level of variance and covariance matrix of risky assets. Their model is placed in the probabilistic framework of Epstein and Ji (2013) linked to G-expectation of Peng (2006). It consists in using the method of mean-variance formulated through the minmax problem. Ismail and Pham (2016) use a simulation method to evaluate and compare the performance of Sharpe ratio for robust investor and standard investor, this latter one using a misspecified mean-variance approach. Their results show that the Sharpe ratio of the robust optimal portfolio is significantly better than that of the mean-variance approach. Fouque et al. (2016) have studied the asset allocation problem in continuous time economy applying the Merton model where the correlation between the returns of the two available assets is ambiguous. Fouque et al. (2016) deal with the ambiguous correlation as an ambiguous parameter and the volatilities as stochastic process. The optimal portfolio is obtained from the resolution of the robust optimization maxmin problem using the Black and Scholes theory and assuming that the risky asset dynamics follows a geometric Brownian motion. These authors provide analytical and explicit solutions for the optimal financial strategies. These solutions allow the managers of portfolio to combine prospective information from surfaces of volatility of options in order to achieve an optimal investment from one risk-free asset and several risky assets when correlations are ambiguous. Fouque et al. (2016) show that information, contained in options through the surfaces of volatility, can be used by the portfolio managers facing uncertainty in the level of correlation. Biagini and Pinar (2017) establish a closed form solution for the portfolio optimization for uncertain investor with CRRA function utility in the level of the estimation of the expected return and the volatility using the maxmin approach. They establish a simple and mathematical solution for the robust problem of Merton. The contribution of Biagini and Pinar (2017) lies in the level of asset ambiguous correlation and market incompleteness. In fact, Biagini and Pinar (2017) established an ellipsoidal uncertainty sets for drift and volatility. Their results are based on the differential partial equation of maxmin of Hamilton–Jacobi–Bellman–Issacs. According to Chen and Epstein (2002), Biagini and Pinar (2017) show that the robustness in the level of decision reduces the optimal request for stocks for risk and ambiguity averse investors as this investor acts like a risk averse investor whose risk aversion coefficient is rising. In practice, ambiguity is generally due to the misspecification of financial parameters. Thus, portfolio optimization taking account of ambiguity is crucial in structured portfolio area. In this paper, we provide the optimal structured portfolio payoff under ambiguity, in the optimal portfolio positioning framework introduced by Leland (1980), and Brennan and Solanki (1981). Compared to Ben Ameur and Prigent (2013), we deal with the multidimensional case, which allows in particular to take account of the correlations between several risky assets. Note that portfolio positioning refers to static strategies. But actual portfolio hedging strategies in fact correspond to discrete-time trading. Additionally, structured portfolio management is based particularly on initial positioning on financial derivatives. Contrary to the standard portfolio theory, the optimal portfolio positioning does not assume a priori that portfolio are simple linear combination of basic assets. On the contrary, it is shown that options are to be included on the optimal portfolio. The portfolio value is a function of a given benchmark. The portfolio payoff maximizes the investor’s expected utility. A particular case of optimal positioning is Option Based Portfolio Insurance (OBPI), introduced by Leland and Rubinstein (1976). This consists of a portfolio invested in a risky asset S (usually a financial index such as the S&P 500) covered by a listed put written on it. Whatever the value of S at given horizon T, the portfolio value is always above the strike K of the put. Portfolio insurance theory usually considers portfolio payoffs which are functions of a benchmark (a specified portfolio of common assets). At maturity, downside risk is limited (investors can receive a given percentage of their initial capital, even in bearish markets), while investors may also participate in upside markets. However, more specific insurance constraints can be introduced, for example for institutional investors (see e.g. Bertrand et al. (2001) for quite general insurance constraints). Our purpose is to analyze how the optimal portfolio profile is modified when taking account of ambiguity. Indeed, when taking account of the ambiguity index, the investor’s risk aversion and ambiguity index characterize jointly the optimal portfolio profile, which involves option-based strategies. In practice, ambiguity is typically due to uncertainty about the true values of financial parameters. Determining the optimal portfolio in such a framework therefore has important applications for the structured portfolio management industry. The paper is organized as follows. Section 2 provides first a basic example of the determination of a structured financial portfolio under ambiguity when investing basically on two risky assets with their corresponding put options. It corresponds to an extension of the OBPI considered by Leland and Rubinstein (1976) to the multidimensional case. Second, a general result of portfolio positioning under ambiguity aversion is provided in the multidimensional setting. We show in particular how the various ambiguity aversions and the correlations of the risky assets impact the optimal portfolio profile. Within this framework, Sect. 3 illustrates the general result by examining a fundamental example that emphasizes the role of aversions to both risk and ambiguity. Section 4 is devoted to empirical illustrations based on both data on the S&P 500 and the Euro Stoxx 50 for two time periods, namely the financial crisis (i.e. 2008 considered in our study) and a “standard” period, namely 2016.",1
56.0,1.0,Computational Economics,28 April 2020,https://link.springer.com/article/10.1007/s10614-020-09986-0,Intertemporal Similarity of Economic Time Series: An Application of Dynamic Time Warping,June 2020,Philip Hans Franses,Thomas Wiemann,,Male,Male,Unknown,Male,"In this paper we illustrate the non-parametric technique Dynamic Time Warping (DTW) to examine similarities of business cycles. DTW is a widely applied algorithm in non-economic fields such as speech, pattern and movement recognition [see, for example, Cedras and Shah (1995), Geiger et al. (1995), and Sakoe and Chiba (1978)] and data mining [see, for example, Keogh and Pazzani (2000), among others]. While its non-parametric similarity measure has been shown to be superior to other measures such as the Pearson correlation coefficient (Petitjean et al. 2011), economic research has not yet used Dynamic Time Warping to its potential. To the best of our knowledge, Wang et al. (2012) and Raihan (2017) are the exceptions, where the DTW technique is used in its basic standard format. In our study, we design the DTW technique in particular for our purposes to identify dynamic leading and lagging relationships between time series. We illustrate the application of our modified DTW technique by investigating business cycle similarities across US states. There is substantial literature on similarities across time series variables, predominantly using parametric techniques. Usually, it is assumed that when one variable is leading the other, it does so throughout the entire sample. Bai and Ng (2004) consider a dynamic factor model with a common factor structure. Engle and Kozicki (1993) examine common cyclical and other features. Cubadda et al. (2009) study common autoregressive parts across individual series as an indication of common features in multivariate models. Fröhwirth-Schnatter and Kaufmann (2008) propose a clustering method based on mixture models, where each mixture component follows the same underlying model specification. Due to the parametric nature of these approaches to time series’ similarity, however, neither can adequately describe a time-varying dynamic specification. In particular, and to put it simply, these methods restrict that variables \(y_{t}\) and \(x_{t}\) are linked via, for example, the (vastly simplified) specification \(y_{t} = \alpha + \beta x_{t - j} + \varepsilon_{t}\) with j fixed over the sample. In this paper we propose to study co-movements using the DTW technique where we allow for the possibility that j is time-varying and may switch from positive to negative or vice versa. A prominent strand of economic literature for which this seems particularly relevant is the research on the similarity of business cycles. Here, most studies analyse correlation structures with, for example, common factor models or their adaptations (Francis et al. 2017). Recently, also non-parametric clustering techniques have received increased attention. For example, Crone (2005) and Papageorgiou et al. (2017) consider K-means clustering to group business cycles of US states and European countries, respectively. A common caveat among these previously considered approaches, however, is the strict assumption of identically timed business cycles. As suggested by the findings of Hamilton and Owyang (2012), this is potentially problematic as a substantial amount of idiosyncratic business cycles seems to stem from differential timing around national recessions. If one is interested not only in the co-movement at a particular point in time, but also the similarity in the extent or gravity of the economic movements, existing techniques only allow for limited insights if their results are largely driven by even slight temporal shifts of time series. DTW can alleviate such concerns by directly incorporating potentially dynamic timing differences in the analysis. For a first and intuitive impression on how DTW can contribute to existing literature, consider Fig. 1. The graph shows the DTW-produced alignment of quarterly, seasonally adjusted real GDP of Florida and Idaho for the period 2006Q1 to 2017Q4. The lines that link two points of the respective series show their closest correspondence. While in some cases the connected points concern the same quarter, the figure indicates that in most cases, earlier or later quarters are apparently a better match. This feature is the key reason that we rely on this non-parametric technique.Footnote 1 From Fig. 1 we see that the time-alignment of the two series, denoted by j earlier, seems to vary over the sample, ranging over values such as 0, 1, − 1 and even − 4 and 4. One could now decide to make j a time-varying parameter, for example \(j_{t}\), however, given the size of typical samples on real GDP data, a non-parametric technique without the need for high degrees of freedom is more appropriate. Dynamic Time Warping, at least in the variant proposed below, seems to be a useful technique: it not only illustrates the link of two time series visually but it also provides an estimate of their temporal alignment, as we will show below. Temporal point mapping. Quarterly real GDP of Florida (x, blue) and Idaho (y, green) for the period 2006Q1 to 2017Q4, standardized at 1.00 for 2006Q1. (Color figure online) The introduction of DTW—as a computer science method originally developed for pattern recognition—complements the recent developments in machine learning applications in economics, although with a clearly distinct contribution. In reviews of the relevant literature, Athey (2017) and Mullainathan and Spiess (2017) illustrate the potential of machine learning for advanced methods on causal inference and for predictive applications in economics. For DTW, we suggest that it contributes most to descriptive data analysis by revealing temporal dynamics between time series and allowing for assessment of their similarity. In a similar manner as machine learning methods can complement conventional econometric models, we believe DTW can complement—and in some contexts replace—the conventional Pearson correlation coefficient and other similarity measures. At the same time, the potential of this method is beyond its descriptive abilities. For example, Raihan (2017) makes promising suggestions for the use of DTW techniques for predictive purposes. In the rest of this paper, we proceed as follows. In Sect. 2, we outline standard DTW and introduce our modifications for our analysis of economic time series. Our adjusted DTW is illustrated by an application to real GDP series of three US states. In Sect. 3, we discuss an application of DTW-based time series clustering to assess similarities of all mainland US state real GDP series. Section 4 concludes with some ideas for further research.",17
56.0,1.0,Computational Economics,11 December 2018,https://link.springer.com/article/10.1007/s10614-018-9874-x,Conditional Correlation Demand Systems,June 2020,Apostolos Serletis,Libo Xu,,Male,Unknown,Unknown,Male,"The consumer demand systems literature that developed after the publication of Theil’s (1965) important paper, along with Diewert (1974), Christensen et al. (1975), Theil (1975, 1976), have advanced dramatically over the years. For authoritative surveys of that literature, see Brown and Deaton (1972), Blundell (1988), Lewbel (1997), and Barnett and Serletis (2008). More recently, Serletis and Isakin (2017) and Serletis and Xu (2019) merge the flexible demand systems literature with recent advances in the financial econometrics literature, by relaxing the homoscedasticity assumption in empirical demand analysis, assuming that the covariance matrix of the errors of the demand system is time varying. Unlike static demand systems, demand systems with heteroscedastic disturbances provide a way of dealing with empirical regularity in demand system analysis and also allow applied economists to extract information about the variances and covariances of demand responses. Models of this kind are particularly promising for empirical use, because any elasticities calculated from the demand system are affected by the variances and covariances of the demand responses, and hence estimating a demand system with heteroscedastic disturbances will give different estimates than a static demand system. 
Serletis and Isakin (2017) and Serletis and Xu (2019) consider the VECH and BEKK parameterizations for the covariance matrix, and analytically prove the invariance of the maximum likelihood estimator with respect to the choice of the good deleted from a singular demand system. They also prove a number of important practical results regarding how to recover the mean and variance equation parameters (and their standard errors) of the full demand system from those of any subsystem obtained by deleting an arbitrary good. In this paper, we extend the work in Serletis and Isakin (2017) and Serletis and Xu (2019) and consider the constant conditional correlation (CCC) and dynamic conditional correlation (DCC) parameterizations of the variance model. We derive a number of important practical results and provide an empirical application in order to support our methodology. We show how the results in Serletis and Isakin (2017) and Serletis and Xu (2019) can be applied to the CCC and DCC parameterizations of the variance model.",
56.0,1.0,Computational Economics,07 February 2019,https://link.springer.com/article/10.1007/s10614-019-09879-x,Modelling Time-Varying Parameters in Panel Data State-Space Frameworks: An Application to the Feldstein–Horioka Puzzle,June 2020,Mariam Camarero,Juan Sapena,Cecilio Tamarit,Female,Male,Male,Mix,,
56.0,1.0,Computational Economics,19 December 2018,https://link.springer.com/article/10.1007/s10614-018-9878-6,A Monte Carlo Study of Time Varying Coefficient (TVC) Estimation,June 2020,Stephen G. Hall,Heather D. Gibson,Mike G. Tsionas,Male,Female,Male,Mix,,
56.0,1.0,Computational Economics,19 May 2020,https://link.springer.com/article/10.1007/s10614-020-09988-y,Computing the Time-Varying Effects of Investor Attention in Islamic Stock Returns,June 2020,Nabila Jawadi,Fredj Jawadi,Abdoulkarim Idi Cheffou,Female,Unknown,Unknown,Female,"From behavioral finance theory, we learn that in addition to their own fundamentals, financial markets may be driven by news, rumors, uncertainty, irrational exuberance, animal spirits, investors’ feelings, etc. (Shiller 2015; Akerlof and Shiller 2009). For example, high uncertainty can lead to more anxiety and panic for investors, subsequently creating more volatility on the financial markets (Jawadi et al. 2017). The recent and ongoing risk of the Coronavirus epidemic, for instance, has made investors highly uncertain and anxious, and as a result, the French, German and US stock markets lost over 10% of their value in the last week of February 2020 and more recently, which are the highest losses since 2008. This recent occurrence illustrates the significant relationship between the stock market and investors’ behavior, attention and attitude. To analyze the linkage between investor sentiment and the stock market, this paper investigates the impact of investor attention on stock return dynamics. In particular, we focus on the Islamic stock market in the US and its interaction with investor attention. Islamic finance emerged as an alternative form of finance in the aftermath of the global finance crisis in 2008. The huge losses experienced by investors resulted in severe criticism of the conventional financial system. This led to an increase in investors’ interest in the Islamic stock market since the latter appeared to provide an investment framework that is more social, societal, ethical and less risky than the conventional financial system. The origins of Islamic finance can be traced back to the 1970s, when it was developed as an investment framework compliant with Shariah law to provide a financial framework that met the investors’ faith requirements. The Shariah law regulations are based on specific rules, such as prohibition of interest rates, speculation and uncertainty; the sharing of both profit and loss; and greater transparency (Jawadi et al. 2014). However, investor interest in Islamic finance has mainly been observed in the last decade, especially during the severe downturns in the conventional financial investment markets, in the aftermath of the recent global financial crisis. The relationship between investor attention and the Islamic stock market is particularly relevant since Islamic finance -at least from a theoretical point of view- has built its ethical investment rules in keeping with the principles of Sharia-compliant investors, while conventional investors or speculators might behave differently, sometimes embracing mispricing activities in order to maximize their profits. Accordingly, and given the presence of heterogeneous investors (Jawadi et al. 2017), investor sentiment could impact the Islamic stock market differently depending on the actors who dominate the market. Indeed, the dynamics of the Islamic stock market may be galvanized in two ways: by pure drivers of conventional stock markets (unethical investors according to Shariah law) or investor attention, corresponding to the stabilizing actions of ethically principled investors. This paper proposes a new specification per quantile to model the dynamics of Islamic stock returns, taking into account the effects of investor attention. In particular, it enables us to further capture the effects of investor attention and interest in Islamic Finance for lower as well as higher quantiles. This specification has the advantage of capturing further asymmetry, nonlinearity and time-variation in the relationship between investor attention and Islamic stock returns. Further, unlike previous related studies, we proxy investor sentiment with a Google Search index using specific research and interest in the Islamic stock market, and we test its capacity to forecast future Islamic stock returns. Our findings show that investor attention has a significant impact on Islamic returns, and that investor attention helps to drive Islamic stock returns higher. The remainder of this paper is structured as follows. Section 2 presents a brief literature survey. We rapidly present the econometric methodology in Sect. 3. The empirical results are discussed in Sect. 4, while Sect. 5 concludes.",5
56.0,1.0,Computational Economics,09 December 2019,https://link.springer.com/article/10.1007/s10614-019-09949-0,OPTCON3: An Active Learning Control Algorithm for Nonlinear Quadratic Stochastic Problems,June 2020,V. Blueschke-Nikolaeva,D. Blueschke,R. Neck,Unknown,Unknown,Unknown,Unknown,,
56.0,1.0,Computational Economics,17 May 2019,https://link.springer.com/article/10.1007/s10614-019-09892-0,A Testing Procedure for Constant Parameters in Stochastic Volatility Models,June 2020,Juan del Hoyo,Guillermo Llorente,Carlos Rivero,Male,Male,Male,Male,"The literature on volatility modeling is vast. It has been one of the most important areas of research in empirical finance and time series econometrics over the past two decades. Volatility estimates are key inputs in empirical applications (portfolio formation, asset allocation, risk management, VaR, pricing derivatives, etc.). The main parametric references used to model conditional volatility in discrete financial time series are (Generalized)-Autoregressive Conditional Heteroskedasticity (G-ARCH) models and stochastic volatility (SV) models. Taylor (1986, 1994) formulates a discrete SV model as an alternative to (G)ARCH representations. Volatility is assumed to follow an unobservable (latent) stationary autoregressive stochastic process, usually autoregressive of order one.Footnote 1 Thus, the likelihood function for the SV models is difficult to evaluate, and consequently there are several ways to estimate these models.Footnote 2 The usual discrete SV models produces rich patterns in conditional volatility (clustering, thick tails, changing conditional volatility, etc., see Carnero et al. 2004). Its stationary and weakly dependent properties produces theoretical autocorrelations which decrease exponentially. Nevertheless, some empirical findings indicate autocorrelations that decay very slowly, as in long memory models. Above that, it is difficult for some SV specifications to accommodate sudden changes in volatility and the associated premia documented in empirical studies.Footnote 3 SV models with structural breaks and/or changing parameters in the volatility equation have been proposed as a possible explanation for the long-memory behavior, the sudden changes in volatility, and the empirical premia.Footnote 4 Despite the extensive literature on alternative SV specifications and estimation methods, model evaluation has received less attention. Most papers analyze the model according to their ability to fit the data. Some authors use a particular kind of simulated method: simulated generalized method of moments (minimizing the distance between the sample moments of actual and simulated data as in Duffie and Singleton 1993) or efficient method of moments (minimizing the expectation under the simulated model of the score of some auxiliary model, Gallant et al. 1997; Andersen et al. 1999). Another approach test the time series properties of the residuals (Shephard 1996; Kim et al. 1998; Durham 2006, 2007). Both approaches either test for individual coefficients, t-type tests (i.e. Andersen et al. 1999) or model specification (goodness of fit, diagnostic checking and model comparison), based on large sample approximations. Dufour and Valery (2009) focus on hypothesis testing in parametric SV models and develop both exact and asymptotic tests. Tests based on a two-step moment estimator. The testing procedures mentioned above are based only on data and simulated data on observable variables, in some way avoiding the issue of non observability of the volatility processes. Direct comparison of actual (observed) and simulated moments or empirical distribution of actual and simulated volatility equation is not possible given that the volatility process is not observable. Thus an alternative approach is to test only for the volatility process, given that, in several applications (hedging, pricing derivatives) the main interest lies in the specification of the variance. In particular, formal evaluation of the volatility equation in SV models, via specification testing using changing parameter tests, has not been common in empirical studies. Therefore, a formal and easy to implement statistical methodology is important to test for changing parameters (breaks) in SV specifications. As far as we are aware, there is no method currently available to test for constant parameters in a given discrete SV representation. The method presented here fills this gap.Footnote 5 This paper develops a general parametric method to test for constant parameters in the volatility equation for discrete SV models. This method can be considered as a misspecification test. It is a conditional two-step misspecification/diagnostic test. Given a full sample estimation of any SV model, the main concern is the null hypothesis of constant parameters in the latent volatility equation. If it were possible to observe the volatility process, the solution would be straightforward (Andrews 1993; Stock 1994). This paper shows that, given a consistent full sample estimation of the SV model and using the Kalman filter (to estimate either the filtered or the predicted component), it is possible to test for constant parameters in the volatility equation. If the null hypothesis is rejected, this means the original model is misspecified. Thus, it can be used as an evaluation tool for any SV model that admits a state space representation. Changing parameters may have multiple origins: omitted variables, breaks, incorrect functional forms, etc. Consequently, although the test is shaped by the need to check the adequacy of constant parameters on SV models, the rejection of the null hypothesis does not reveal the nature of the problem, but only points to a misspecification in the model. Therefore, the proposed method can be considered as an omnibus misspecification test. The diagnostic tool developed has several interesting features. Firstly, it only requires one estimation of the SV model—the full sample one—(using QML, MCML, MCMC or any other consistent estimation method under the null). Secondly, the proposed test has a well-known asymptotic distribution (free of nuisance parameters) under the null. This distribution is a function of a Brownian bridge, which is easy to simulate and characterize. Thirdly, the testing procedure is very simple to implement in most software programs. Nowadays, the Kalman filter and recursive least squares are standard tools either available or simple to program in most software. Fourthly, its computational cost is very low; it takes seconds to calculate the value of the statistics. Consequently, the paper presents a practical and easy to implement method to study the stability of the volatility equation in SV models. It does not deal with techniques for SV model estimation or whether a priori the model should consider breaks or alternative specifications. Instead, it focuses on an a posteriori evaluation of a previously estimated SV model under the null of constant parameters. As far as the present authors are aware, no other study approaches the problem of SV model stability (evaluation, diagnostics) in this way. Even though the general approach presented here can be applied to several SV specifications, most of the paper is devoted to the simplest so-called log-normal stochastic volatility model with autoregressive volatility of order one, SV-AR(1). This model has been widely studied in the SV literature and has served as the usual testing ground for alternative specifications. The model is commonly referred as the stochastic volatility model, although it is a special case. It is nonetheless attractive because of its parsimony, and because it provides a reasonable first approximation to the properties of most financial return series. It maintains the fundamental problems associated with the presence of an unobserved latent volatility factor. Latter in the paper we consider a number of extensions of the basic SV-AR(1) model that can be treated using our methodology. The paper is organized as follows. Section 2 briefly reviews the recursive sup-Wald test used to test for breaks in linear models. Section 3 describes the SV model, states the problem of testing for constant parameters and presents the misspecification test proposed. In Sect. 4 several Monte Carlo simulations examine the statistical properties of the proposed test. Section 5, the empirical application, tests for constant parameters in an SV model specification taken as a reference in several papers. This section uses the same real data as in Gallant et al. (1992) . Section 6 provides extensions to the basic setup while Sect. 7 concludes.",
56.0,1.0,Computational Economics,09 April 2020,https://link.springer.com/article/10.1007/s10614-020-09975-3,Predicting Extreme Financial Risks on Imbalanced Dataset: A Combined Kernel FCM and Kernel SMOTE Based SVM Classifier,June 2020,Xun Huang,Cheng-Zhao Zhang,Jia Yuan,,Unknown,,Mix,,
56.0,1.0,Computational Economics,18 May 2020,https://link.springer.com/article/10.1007/s10614-020-09993-1,Technological Change and Catching-Up in the Indian Banking Sector: A Time-Dependent Nonparametric Frontier Approach,June 2020,Sushanta Mallick,Aarti Rughoo,Wei Xu,Unknown,Female,,Mix,,
56.0,1.0,Computational Economics,01 June 2019,https://link.springer.com/article/10.1007/s10614-019-09899-7,About Long-Term Cross-Currency Bermuda Swaption Pricing,June 2020,Bünyamin Erkan,Jean-Luc Prigent,,Male,Male,Unknown,Male,"Before focusing on Bermuda swaptions, let us first define the different types of swaptions. The European swaptions correspond to the second category of basic options on interest rates. These options give the right to enter into a payer or receiver interest rates swap at a future date (the maturity date of the swaption). The length of this underlying swap is called the tenor of the swaption. Compared to a standard cap option, the swaption cannot be broken down into different one-flow options. The decision to exercise the option is made on the expiration date and then we receive the underlying in the portfolio. As a result, the option depends on a multitude of forward rates. The swaption is called Bermuda when it has several exercise dates instead of one as in the European case. For American swaptions, the option holder can exercise the option at any time after a predetermined date. Recall also that Bermuda and American options are among the most common exotic derivatives. Even though Bermuda swaptions are common, their prices are not easily determined. We do not have closed-form formulas for this class of products. Therefore we need to perform a lengthy pricing process. The difficulty of the Bermuda option pricing comes from the determination of the corresponding optimal control problem’s solution, which is a crucial step to get the price of this product. We choose to work in the seminal Libor Market Model (LMM) framework to model the pricing problem. The Hull and White (HW) model would be also an appropriate model but the LMM is more suitable due to its compatibility with the Black model which is the common market model for vanilla products. Properties of the LMM are detailed by Brigo and Mercurio (2006) who examine in particular the pricing of Bermuda options. Guyon and Labordère (2013) devote also an entire chapter to the pricing of Bermuda options, considering the dual problem associated to the optimal control. The duality of the problem is indeed a main element of the pricing when focusing on the price accuracy. The dual gap can also be considered as a metric for the comparison of different methods/models and benchmarking their uncertainty. Andersen and Broadie (2004) propose a method to solve the dual problem and provide a detailed algorithm for practical implementation. More generally, the Bermuda option pricing has been previously studied in the financial literature both from the theoretical and numerical points of views. For mathematical aspects, we refer to Schweizer (2002), Andersen and Broadie (2004) and Rogers (2010). Recently Borovykha et al. (2017) explore an extension when dealing with Lévy processes. For numerical aspects, we refer for example to Belomestny (2011), Henrard (2003), Henrard (2005) and Zhang and Oosterlee (2012). Note in particular that Jensen and Svenstrup (2002) improve Monte Carlo techniques by testing a range of control variates to value Bermudan swaptions in a Libor market model. A review about multi-factor cross-currency LIBOR market models can be found for instance in Ahsan (2003), focusing on the calibration of cross-currency market models to FX markets. Dempster and Hutton (1997) provide numerical valuation of cross-currency swaps and swaptions. Note also that the Lévy Libor or market model has been extended to a multi-currency setting by for example Eberlein and Koval (2006) and Wang and Huang (2018). But issues relating to missing data when dealing with long-term derivatives have not been fully considered, especially for the cross-currency Bermuda swaption. Therefore, the first contribution of this paper is to build an interpolation algorithm that allows the pricing of long-term funding notes of banks which have early redemption options. We face an interpolation problem on the tenor dimension of the quoted swaption matrix due to the long-term properties of this product. Indeed, swaptions on long maturities have no quote or less liquid quotations. Given the fact that these long-term notes have quite often early redemption options, we have to consider the issues that we must face when pricing long-term Bermuda swaptions. This makes us revisit the Rectangular Cascade Calibration with the Endogenous Interpolation Algorithm (RCCEIA) of Brigo and Mercurio (2006). We consider different hypotheses that can be used to make an interpolation. After a back-testing phase, we keep the hypothesis that provides the best results. The second main contribution of the paper is to investigate the special case of Bermuda callable cross-currency swaptions. When a bank issues a note in a foreign currency, it also hedges the position on foreign currency by a cross-currency callable swap. We analyze the cash-flows of the swap and then illustrate how the price depends on the different risk factors, namely the domestic interest rates market, the foreign interest rates market and finally the foreign exchange market (FX). Our aim is to test the accuracy of the proposed model by considering each previous risk factor one by one. The results of our tests allow us to understand the relationship between this cross-currency derivative and each of these three markets. Finally, we note the crucial role of the product’s margin in the sensitivity analysis. Our work is organized as follows. Section 2 describes the general framework of Bermuda swaption pricing. Section 3 deals with calibration problems due to missing data that are specific to long-term property of the product. Indeed, it is the long-term property that makes us revisit the calibration procedure, taking account of the problem of missing data in swaption volatility for long tenors. Section 4 presents the product that we investigate, namely the long-term cross-currency Bermuda swaption. We test also the relevance of the three involved markets when pricing the cross-currency Bermuda swaption. Finally, Sect. 5 concludes the paper by highlighting the advantage of the proposed calibration method and by summarizing the sensitivities of a cross-currency Bermuda swaption to the different risk factors.",
56.0,2.0,Computational Economics,12 October 2019,https://link.springer.com/article/10.1007/s10614-019-09926-7,"Crises Beyond Belief: Findings on Contagion, the Role of Beliefs, and the Eurozone Debt Crisis from a Borrower–Lender Game",August 2020,Jonathan W. Welburn,,,Male,Unknown,Unknown,Male,"The term “contagion” carries considerable ambiguity. While the name implies an infectious spread of economic malaise from a given carrier outward to new hosts, reality in economics is less straightforward. Indeed, contagion can follow from the direct transmission of adverse economic events through debt and trade channels. However, the effect of a common cause may result in the appearance of contagion. Adding further ambiguity to the meaning of contagion is its relationship with time; some crises develop over years, spreading through traditional economic linkages, while others spread over days if not hours. As a result, it is necessary to clarify our working definition of “contagion.” Following the work of Reinhart and Rogoff (2008), who define “fast and furious” contagion, we differentiate between spillover effects evolving over extended periods, and true contagion, which we define as having significant near-term effects. While we highlight debt and trade channels as transmission mechanisms for contagion, we also discuss the importance of common-cause factors, which can lead to the same result This paper presents models of both debt and trade channels allowing for both true contagion and common-cause factors resulting in the appearance of contagion. The debt model explains how a crisis in one country can spread to another through a common lender. The trade model explains how a crisis in one country can lead to near-term adjustments in trade, thus spreading the crisis to a trading partner. Furthermore, we highlight the possibility of a common cause that results in two countries experiencing near-simultaneous crises without contagion. Through the debt model, we explain how the potential for common-cause crises can drive a common lender to adjust its beliefs about one country after observing a crisis in another, thus resulting in the appearance of a contagious effect even in the absence any true contagion. This paper seeks to answer key questions regarding each channel of contagion. For the debt channel, we address the following three questions: (1) when will an adverse economic shock result in sovereign default; (2) at what point will a default by one country cause contagion in another; and (3) how does this depend on the debt levels of the two countries? For the trade channel, we address the following two additional questions: (1) Can a shock in one country significantly affect the credit standing of a trading partner? If so, how large of a shock is needed? (2) How does this depend on the level of trade between the two countries? This paper presents a new contribution to models of economic contagion by explaining how beliefs about a potential common cause can drive the spread of crises between countries, in addition to the more traditional assumptions of debt and trade linkages. We use this contribution to provide insight into potential policy solutions for the effective management of crises with contagious potential. In this paper, a brief review of relevant literature is presented in Sect. 2. Two models of contagion: a debt model and a trade model, are presented in Sect. 3. Computational solution approaches for each model are discussed in Sect. 4. Sensitivity analysis on player behavior and equilibrium outcomes is presented in Sect. 5. Specific insight is provided into the Eurozone crisis in Sect. 6. Finally, conclusions and policy insights are presented in Sect. 7.",
56.0,2.0,Computational Economics,14 October 2019,https://link.springer.com/article/10.1007/s10614-019-09920-z,Multifractal Analysis of Realized Volatilities in Chinese Stock Market,August 2020,Yufang Liu,Weiguo Zhang,Xiang Wu,Unknown,Unknown,,Mix,,
56.0,2.0,Computational Economics,14 October 2019,https://link.springer.com/article/10.1007/s10614-019-09928-5,Using Genetic Algorithm and NARX Neural Network to Forecast Daily Bitcoin Price,August 2020,Jin-Bom Han,Sun-Hak Kim,Kum-Sun Ri,Unknown,Unknown,Unknown,Unknown,,
56.0,2.0,Computational Economics,15 October 2019,https://link.springer.com/article/10.1007/s10614-019-09931-w,Equilibrium Working Curves with Heterogeneous Agents,August 2020,Atle Oglend,Vesa-Heikki Soini,,Male,Unknown,Unknown,Male,"In 1933, Holbrook Working documented a statistical regularity between the US wheat futures basis and marketable wheat storage (Working 1933). Almost 65 years later, his statistical analysis were confirmed by Carter and Giha (2007). This regularity is now known as the Working curve. Working later developed his empirical findings into a theory of the supply of storage (Working 1949, 1948; Telser 1958), where the curve defines an economic “necessary return to storage”. Since then, much work has been dedicated to explaining the Working curve, which has led to fruitful insights into the behavior of commodity markets. The main economic problem has been to reconcile the empirical fact a of negative return to storage at low, but non-zero, storage levels. The negative return has been ascribed to a “convenience yield” on carrying stocks in states of aggregate commodity scarcity. The first mention of the convenience yield is due to Nicholas Kaldor, which states that “in normal circumstances, stocks of all goods posses a yield, measured in terms of themselves, and this yield which is a compensation to the holder of stocks, must be deducted for carrying costs proper in calculating net carrying costs. The latter can therefore, be negative or positive” (Kaldor 1939). A modern definition is due to Brennan (1991); “the (net) convenience yield is the (net) flow of services (per unit time and per monetary unit of the commodity) that accrues to a holder of the physical commodity, but not to a holder of a contract for future delivery of the same commodity”. In the asset pricing literature commodity pricing, the convenience yield is the defining feature that separates commodities from other assets, such as fixed income assets or common stocks. Standard practice when pricing commodity claims is to either directly (Gibson and Schwartz 1990; Schwartz 1997; Doran and Ronn 2008; Hilliard and Reis 1999; Cortazar and Naranjo 2006; Liu and Tang 2011) or indirectly (Miltersen and Schwartz 1998; Crosby 2008; Trolle and Schwartz 2009) define the convenience yield as a reduced form exogenous stochastic process, or to price it as a real option on storage (Heaney 2002, 2006; Hochradl and Rammerstorfer 2012; Dockner et al. 2015) This literature rarely goes beyond defining the yield as an exogenous “pricing factor” in the market. In economics, the competitive storage model (Wright and Williams 1991; Deaton and Laroque 1992; Cafiero et al. 2006; Pirrong 2012) can only partially account for the Working curve. The model explains the positive return to storage as a competitive marginal cost of storage. However, negative returns cannot be accounted for without introducing an explicit parametric convenience function, similar to the financial literature. This approach has been criticized as an ad-hoc fix to the model without clear economic justification (Deaton and Laroque 1996; Brennan et al. 1997). The purpose of this paper is to demonstrate that by extending the storage model to allow for risk aversion and heterogeneity in stock ownership, the model can produce Working curves that match empirically observed curves. In the model, the apparent problem of positive storage with negative economic return vanishes when the negative economic returns are balanced by positive marginal utility returns from storage. Such a pricing restriction is already well known in the consumption based asset pricing literature. For this insurance mechanism of storage to generate an aggregate convenience yield, the economic benefits of storage must increase in states of aggregate scarcity. This occurs with inelastic consumption demand, which is commonly assumed to apply for commodity markets at consumption frequencies relevant to storage. As such, the desire to smooth utility flows in markets where storage realizes super profits in aggregate scarcity states will support equilibrium convenience yields. In this interpretation, the convenience yield is an equilibrium insurance premium on storage. In the canonical risk neutral representative agent storage model, the aggregate storage policy is kinked at zero storage for stocks below a given threshold level. As such, the resulting equilibrium Working curve will only support negative storage returns at zero aggregate storage, making the curve itself kinked. With heterogeneity in stock ownership, the aggregate storage policy will be smoothed over individual policies. Importantly, the aggregate policy includes the extensive margin of storage, i.e. as agents move in and out of the storage market. The equilibrium Working curve in the market is only determined by those agents that remain in the storage market to price the storage return. As aggregate scarcity increases, an increasing fraction of agents drop out of the storage market, leaving the remaining agents to price the curve. This extensive margin effect together with the insurance effect discussed in the previous section generates the smooth and concave Working curve often observed in empirical studies. The solution and analysis of the heterogeneous agent model in this paper has been made possible by recent advances in heterogeneous agent modeling in macroeconomics. This has introduced new tools to extend the traditional representative agent modeling of recursive rational expectations equilibrium (Krusell and Smith Jr 1998; Krusell et al. 2009; Maliar et al. 2010). A computationally feasible solution to the heterogeneous agent model is achieved by reducing the dimensionality of the state space to fixed moments of the distribution of heterogeneous variables. The fit of the approximate recursive equilibrium allows us to determine which elements of the distribution of stocks across risk averse agents matters to modeling the commodity price. Krusell and Smith Jr (1998) showed in their seminal paper that the behavior of macroeconomic aggregates such as aggregate capital can be almost perfectly described by the mean of the wealth distribution when individuals face uninsurable income risk. Their results suggest that if commodity agents are well insured by storage, the mean stock level in the market should also be a sufficient statistic to describe the commodity price. Our results confirm this. When storage occurs in the market, the propensity to store is approximately constant, and so any dispersion around the mean stock level does not have much influence on the aggregate price. However, when aggregate stocks become sufficiently low, a significant share of agents drop out of the storage market, reducing the aggregate propensity to store. It turns out that the occurrence of this state is also sufficiently informed by the mean of the stock distribution. Since the mean is a sufficient statistics, representative agent modeling remains valid. However, in order to mimic the extensive margin effect of a gradual decline in the propensity to store at lower mean stock levels, the representative agent model should include a parametric convenience yield function that smooths the kink in the resulting storage policy function. As such, our finding provides a microeconomic foundation for the inclusion of parametric convenience yield functions in commodity market models. The rest of the paper is structured as follows. In the next section, we present the Working curve and show some empirical examples of the curve. Following this, we introduce our heterogeneous agent model. We start by building some intuition on the effects of introducing risk aversion and heterogeneity in stock ownership into the model. We then present model details and solution method. Having solved the model, we present and discuss the main findings of the numerical solution. Finally, we add some concluding remarks.",
56.0,2.0,Computational Economics,19 October 2019,https://link.springer.com/article/10.1007/s10614-019-09938-3,Nonlinear Scaling Behavior of Visible Volatility Duration for Financial Statistical Physics Dynamics,August 2020,B. Zhang,J. Wang,G. C. Wang,Unknown,Unknown,Unknown,Unknown,,
56.0,2.0,Computational Economics,21 October 2019,https://link.springer.com/article/10.1007/s10614-019-09929-4,Pricing Vulnerable Options with Stochastic Volatility and Stochastic Interest Rate,August 2020,Chaoqun Ma,Shengjie Yue,Yong Ma,Unknown,Unknown,,Mix,,
56.0,2.0,Computational Economics,21 October 2019,https://link.springer.com/article/10.1007/s10614-019-09930-x,Distributional Assumptions and the Estimation of Contingent Valuation Models,August 2020,James B. McDonald,Daniel B. Walton,Bryan Chia,Male,Male,Male,Male,"The contingent valuation (CV) method is a well-established technique to measure the value of goods and services that are not transacted in markets and has been used for decades to measure willingness to pay (WTP).Footnote 1 Because the data produced by many CV studies are reported in intervals, where the upper and lower thresholds of the latent variable (WTP) are observed instead of the actual data point, producing efficient and consistent estimates can be challenging.Footnote 2 In a regular regression framework, estimator unbiasedness and consistency don’t depend on the presence of heteroskedasticity or on the underlying error distribution. However, non-normality or heteroskedasticity in censored regression models can result in ordinary least squares (OLS) and Tobit estimators being biased and inconsistent. This has led to the use of various semiparametric and alternative methods. For example, Day (2007) addresses some difficulties associated with a semi-parametric specification. Other researchers have adopted specific distributions such as the Weibull or log-logistic. Partially adaptive estimation, using flexible probability density functions to model diverse distributional characteristics of WTP, provides a middle ground between these two approaches. Using this approach for interval regression models, Cook and McDonald (2013) find that the use of flexible distributions can reduce the impact of distributional misspecification and produce more robust and efficient parameter estimates corresponding to homoskedastic interval-censored data.Footnote 3 In summary, CV methods are widely used in diverse applications and partially adaptive methods of estimation have the potential to provide improvements over some common methods of estimating. This paper illustrates how partially adaptive estimation methods can be used to estimate contingent valuation models. The remainder of this paper is organized as follows: Sect. 2 reviews the CV regression methodology and outlines a partially adaptive estimation framework. Section 3 applies these procedures to estimate the WTP to protect Australia’s Kakadu Conservation Zone from mining described by Carson et al. (1994). Section 4 presents some simulation results and Sect. 5 summarizes and concludes.",2
56.0,2.0,Computational Economics,22 October 2019,https://link.springer.com/article/10.1007/s10614-019-09934-7,A Non-parametric Test and Predictive Model for Signed Path Dependence,August 2020,Fabio S. Dias,Gareth W. Peters,,Male,Male,Unknown,Male,"The arrival of new information into asset price formation has been subject of extensive discussion. If such information is always fully reflected and incorporated into market prices, asset returns should be largely unpredictable and in the long run driven by compensation for taking market risk. Such a perspective on the driver for returns on investment is most directly justified in settings in which there exists a competitive double auction market with informed rational market participants that will not make irrational errors on a systematic basis, and instead will exploit any known informational inefficiency until its exhaustion. This is typically formulated in financial mathematics pricing theory through concepts such as the Efficient Market Hypothesis (EMH), von Neumann Morgenstern rationality and other related financial assumptions on markets and agent behaviours as captured in, for instance, the technical discussions on such assumptions in Chapters 9 and 14 of Platen and Heath (2006). Initial work on the EMH focussed heavily on statistical properties of stock prices (Fama 1963, 1965a, b), arguing that in strongly efficient markets asset prices fluctuate in unpredictable and completely random ways. The seminal work in Fama (1970) has subsequently provided the definition of three forms of financial market efficiency: weak, where information contained in past prices is fully reflected in current prices; semi-strong, where all public information available (including past prices) is fully reflected in current prices; and strong, where all public and private information available is fully reflected in current prices. Such school of thought has evolved considerably and several different tests have been developed; however, it is still common practice to focus on the serial dependence of asset price changes to test for market efficiency, with a notable recent example seen in Urquhart and McGroarty (2016). These tests look for violations of the weak form of financial market efficiency that, if clearly present, could be used as evidence that the EMH does not hold in any of its forms. Results from tests on serial depedency have been mixed. Nevertheless, the fact that a specific test does not detect serial dependence in a given price series does not guarantee in itself that serial dependence is not present: it is only evidence that a particular form of dependence is not present, but there might still be another form of serial dependence that was undetected due to test misspecification. This motivates a continuous research on finding potential new models that can cope with such alleged dependence and, ideally, use the detected dependence to make informed predictions of future market behaviour. It is a stylized fact that asset returns do not exhibit linear serial correlation (Cont 2001); however, a number of different tests have been proposed in the literature which have detected statistically significant evidence that asset returns exhibit some form of serial correlation which is not necessarily linear. Section 3 of Lim and Brooks (2011) provides an extensive review of the most well-known methods. One of the most popular tests is the variance ratio test (VR), which postulates that, under the absence of serial correlation, the variance of the k-period return should be equal to k times the variance of the one-period return; therefore, the ratio of such variances should not be statistically different from one under the null hypothesis of absence of serial correlations. Nevertheless, this has been found not to hold across several global equity markets (Griffin et al. 2010). Other well-known tests aiming to reject the hypothesis of the absence of serial correlations in financial markets include the automatic Box–Pierce test in Lim et al. (2013), long-memory tests in Chung (1996), Hurst Exponent tests in Qian and Rasheed (2004) and tests on the frequency domain that can be found in Lee and Hong (2001). However, despite several tests demonstrating the presence of serial correlation in financial markets and the potential for predictability, there aren’t many studies in the literature developing actual predictive models and validating them in empirical financial data. Notably, Moskowitz et al. (2012) defines a linear regression of the return of an asset scaled by its ex-ante volatility against the sign of its return lagged by some amount of time and shows empirically there is strong evidence of predictability when a lagged 1-year return is used to predict the return 1 month ahead. Similar studies of this nature can also be found in Baltas and Kosowski (2013) where the authors establish relationships between univariate trend-following strategies in assets in futures markets and commodity trading advisors in order to examine questions of capacity constraints in trend following investing. Additionally, Corte et al. (2016) found evidence of negative dependence in daily returns of international stock, equity index, interest rate, commodity and currency markets, with such negative dependence becoming even stronger when the returns are decomposed into overnight (markets closed for trading) and intraday (markets open for trading). These studies though lack a stronger theoretical basis, being instead more focussed on the empirical aspects. This paper provides theoretical contributions to the literature by proposing in Sect. 2 a non-parametric definition of signed path dependence and demonstrating the sufficient and necessary conditions for it to be present in covariance stationary time series. Further, Sect. 3 proposes a formal inference procedure to detect serial correlation of unknown form based on a hypothesis testing formulation of signed path dependence, which is validated on experiments on synthetic data in Sect. 4. This paper provides empirical contributions to the literature in Sect. 5 by using the test previously defined to detect evidence of serial correlation in a number of equity index and foreign exchange markets. Additionally, based on the test statistic proposed, a predictive model is defined and used to feed trading strategies whose out-of-sample performance is analysed, gross and net of transaction costs. Section 6 concludes the work. The proofs for all theorems and lemmas given in this paper can be found in the Technical Appendix (other than for commonly known theorems, which are just enunciated for the sake of completeness and clarity). It is noted that this paper uses cross-validation and bootstrap techniques to calibrate and test the predictive model proposed. A comprehensive review of these techniques can be seen in Kohavi (1995). Numerous studies have used similar techniques to extract serial dependence and predict future behaviour in market prices: Qian and Rasheed (2007), Choudhry and Garg (2008) and Huang et al. (2005) to name a few. This paper contributes to such area of research by developing its own classifiers which are bespoke to markets that have signed path dependence and hence will have better classification performance than models such as Support Vector Machines, Genetic Algorithms for trading rules and others, given that these aim to be more generic and hence less sensitive to peculiar market features. After all, it is expected that a test whose null hypothesis is more specific to a particular problem in financial time series is likely to have more power and more accurate estimation properties than a test based on wider machine learning tools that will consequently imply a wider scope for its equivalent null hypothesis. This paper focuses solely on both how to detect serial correlation in a formal statistical manner and how to model it, paving the way to build predictive models that can be used for the purposes of general market forecasting/risk management or even for the development of trading strategies. As such, technical discussions around assumptions on the price discovery process which might or might not generate price predictability are left outside of the scope of this paper. We also remark that some studies claim that apparent trend-following profits might actually not be arising out of positive serial correlation in returns but possibly out of intermediate horizon price performance (Novy-Marx 2012) or exogenous factors such as the presence of informed trading (Chena and Huainan 2012) or imbalances in liquidity and transaction costs (Lesmonda et al. 2004). In such cases, the use of time series models will not yield significant benefits to an investor, as claimed in Banerjee and Hung (2013). Further, as with any time series, its behaviour can abruptly change due to exogenous reasons; it is a common problem that can possibly reduce the usability of time series models, even though the tuning scheme based on a rolling observation window used in the calibration of the predictive model hereby proposed can alleviate this problem by readapting the model to most recent data and leaving obsolete data out of the fitted model.",4
56.0,2.0,Computational Economics,22 October 2019,https://link.springer.com/article/10.1007/s10614-019-09939-2,An Analytic Approximation for Valuation of the American Option Under the Heston Model in Two Regimes,August 2020,Junkee Jeon,Jeonggyu Huh,Kyunghyun Park,Unknown,Unknown,Unknown,Unknown,,
56.0,2.0,Computational Economics,22 October 2019,https://link.springer.com/article/10.1007/s10614-019-09935-6,"Bitcoin as Hedge or Safe Haven: Evidence from Stock, Currency, Bond and Derivatives Markets",August 2020,Sang Hoon Kang,Seong-Min Yoon,Gazi S. Uddin,,Male,Male,Mix,,
56.0,2.0,Computational Economics,23 October 2019,https://link.springer.com/article/10.1007/s10614-019-09933-8,Optimal Filter Approximations for Latent Long Memory Stochastic Volatility,August 2020,Grace Lee Ching Yap,,,Female,Unknown,Unknown,Female,"Volatility is an unobserved quantity, and various proxies are used as approximations. Andersen and Bollerslev (1998) showed that realized volatility is an adequate approximate of volatility, whilst some other authors showed the advantages of other proxies such as daily squared returns and daily log absolute returns (Lu and Perron 2010). Apart from these, there are some others suggested improved versions to address to various issues such as microstructure bias and abrupt jumps in financial markets. Amongst the popular estimators are the bipower variation (Barndorff-Nielsen and Shephard 2004) and the minimum and median realized volatility (Andersen et al. 2012). As none of the proxies is perfect, we may want to estimate volatility using filtering methods instead of traditional time series techniques that rely heavily on the proxies. In general, a generic filter problem consists of two stochastic differential equations, one as a transition model for the hidden state variable and another one an observation model that relates the noisy observations to the state variable. Classically, the transition model is a Markov process where the stochastic equation depends only on the previous term whilst the observation model depends on the state variable and some random noise. Nonetheless, long memory property is one of the stylized fact for volatility of returns (Andersen et al. 2001; Andersen and Bollerslev 1997; Ding et al. 1993) of which autoregressive fractionally integrated moving average (ARFIMA) is a popular approach to model the long memory time series. To incorporate the long range property, a state space approach with truncated AR and MA polynomials was proposed to compute the maximum likelihood estimates of the ARFIMA parameters (Chan and Palma 1998). The estimation is done by means of Kalman filter (KF) that recursively computes the optimal predictor of the states and the variance of the predictor errors. An extension to use the state space approach to account for measurement errors and level shifts was carried out in Grassi and de Magistris (2014). Compared to the traditional parametric and semi-parametric estimates, they reported that the maximum likelihood estimate of the long memory parameter via KF in the state space approach have good finite sample performance even when the long memory innovations are not Gaussian. Also, by adjusting to the filter of Lu and Perron (2010), they showed that the state space approach produces unbiased estimates of the long memory parameter even in the presence of level shifts. Alternative to the general state space model with KF, there is an increasing interest in the regime-switching models in finance. A comprehensive review can be found in Mamon and Elliott (2014). To allow an optimal filter to sequentially search for the latent variable from the observations, some switching models such as conditionally Markov switching hidden linear model (CMSHLM) (Pieczynski 2011) and conditionally Gaussian observed Markov switching model (CGOMSM) (Abbassi et al. 2015; Gorynin et al. 2017a) were proposed. CMSHLM consists of a Markov triplet \( \left( {\varvec{X}_{1}^{N} , \varvec{R}_{1}^{N} ,\varvec{Y}_{1}^{N} } \right) \) where \( \varvec{R}_{1}^{N} \) is a chain of switches, \( \left( {\varvec{R}_{1}^{N} ,\varvec{Y}_{1}^{N} } \right) \) is Markovian and \( \left( {\varvec{X}_{1}^{N} , \varvec{R}_{1}^{N} } \right) \) is not necessarily Markovian. This allows an exact fast filtering algorithm that is as fast as Kalman filter. CGOMSM is a sub-model of CMSHLM such that \( \left( {\varvec{X}_{1}^{N} ,\varvec{Y}_{1}^{N} } \right) \) is Gaussian conditional on \( \varvec{R}_{1}^{N} . \) It approximates any nonlinear non-Gaussian Markov model in which expectation–maximization (EM) algorithm can be applied to adjust the optimal filter to the said model. The exact fast filter is claimed to be optimal in the sense of mean square error (Abbassi et al. 2015). With the experiments on stochastic volatility models, Gorynin et al. (2017a) showed that the fast filter in CGOMSM approximation is significantly faster than particle filter, and it is more efficient than other filters such as Gaussian sum filter, unscented Kalman filter and unscented Gaussian sum filter. As the observation series in CGOMSM is Markovian, this paper aims to examine if it can be used to approximate the long memory stochastic volatility (LMSV) model to predict the latent long memory stochastic volatility from the observation of returns. The performance of the Markov switching model is compared to the performance of the state space model with KF recursions. The robustness of the models is checked by examining their performances in LMSV series with short memory contamination and level shifts, as well as non-Gaussian innovations. As the state space model involves maximum likelihood estimation whilst the fast filtering is coupled with EM algorithm, it is believed that CGOMSM with fast filtering may be more efficient in terms of computation time. The paper is organised as follows. Section 2 reviews long memory process and the state space model, Sect. 3 reviews long memory process and Markov switching model whilst Sect. 4 reviews long memory process and CGOMSM. The performance of these models on LMSV with various contaminations are examined with Monte Carlo simulation in Sect. 5 and Sect. 6 concludes.",
56.0,2.0,Computational Economics,18 December 2019,https://link.springer.com/article/10.1007/s10614-019-09960-5,Machine learning with parallel neural networks for analyzing and forecasting electricity demand,August 2020,Yi-Ting Chen,Edward W. Sun,Yi-Bing Lin,,Male,,Mix,,
56.0,3.0,Computational Economics,07 August 2020,https://link.springer.com/article/10.1007/s10614-020-10036-y,Guest Editorial: Special Issue on Experimentation in Economics,October 2020,Hans M. Amman,Marco P. Tucci,,Male,Male,Unknown,Male,,
56.0,3.0,Computational Economics,17 December 2019,https://link.springer.com/article/10.1007/s10614-019-09959-y,Heterogeneous Expectations and Uncertain Inflation Target,October 2020,Stefano Marzioni,Guido Traficante,,Male,Male,Unknown,Male,"Recent economic literature emphasized the relevance of central bank’s communication to manage expectations and pursue macroeconomic stability (Woodford 2011). Aligning expectations to the inflation target is among the most relevant objectives for a transparent central bank. In a benchmark model where information is symmetrical and expectations are rational, as in Blinder et al. (2008) and Svensson (2009), having complete transparency does not improve the overall effectiveness of monetary policy. On the other hand, whenever the private sector and the central bank do not share the same information set, a role for different communication strategies may arise. For instance, as pointed out by Branch and Evans (2017), a central bank failing to correctly communicate a target may induce the private sector to consider temporary shocks as permanent, and viceversa. Such a misperception triggers higher actual inflation rates through higher and more persistent inflation expectations. Moreover, in the aftermath of the Great Recession, given historically low interest rates and deflationary pressures, the academic and policy debate has discussed about the desirability of raising the inflation target. Blanchard et al. (2010) and Ball (2014) suggested to raise the inflation target from two to four percent to ease the constraints on monetary policy arising from the zero lower bound on interest rates. This proposal, however, did not find unanimous consensus because it could unanchor inflation expectations as discussed by Bernanke et al. (2010) and Ascari et al. (2017). In this paper we analyze the effect of a change in the inflation target on the macroeconomic stability of a new Keynesian DSGE model. We allow for heterogeneous information within the private sector. In particular, in a setting where agents learn adaptively the misperceived data generating process of the economy, we assume that the real-time knowledge of the current policy target is costly and that only a fraction of agents is willing to bear the cost for a fully updated information set about the state of the inflation target. This approach to model heterogeneous expectation strategies has been extensively used in the literature about heterogeneous beliefs, as in Anufriev et al. (2013) and Agliari et al. (2017), in order to endogenously determine the impact on monetary policy effectiveness when the information is not uniformly taken into account. The fraction of the private sector that does not have full information about the inflation target observes a signal, without distinguishing between permanent and temporary changes. This signal is then used to compute the best estimate for current components in the policy target by means of a Kalman filter procedure. Differently from the contributions mentioned above, which account for information heterogeneity in a framework where agents know the model of the economy, here we assess how information heterogeneity affects macroeconomic stability after a change in the inflation target.
 In this way we characterize a setup where the central bank announces the target to anchor private sector expectations, but a biased perception of the target may arise due to information imperfections or rational inattention as in Mankiw et al. (2003).Footnote 1 Therefore, idiosyncrasies in the process of understanding and processing information may prompt heterogeneity of beliefs about the true nature of inflation target, affecting, in turn, agents’ adaptive learning process. In such a framework we investigate how information heterogeneity affects the dynamics of our economy and whether there is a strategy that performs systematically better under adaptive learning. We assess this issue numerically, by simulating the economy over time and evaluating how this heterogeneous and time-varying learning process affects the volatility of inflation and output gap. We find that the lower the proportion of agents using costly information the larger the expected volatility in the economy. Moreover, costly full-information strategy underperforms with respect to a Kalman signal extraction procedure in the aftermath of the shock because the latter allows to follow more closely the actual dynamics of the economy. This paper contributes to the literature about imperfect information and expectations management. Erceg and Levin (2003) document the discrepancy between actual and expected inflation (measured with the survey of professional forecasters) during the Volcker disinflation period. Such distance was considered as a signal of Federal Reserve’s lack of credibility. A similar argument could be applied in the exercise considered in the present paper. Figure 1 shows actual inflation and inflation expectations measured by University of Michigan over the period January 1978–June 2019. The figure provides us with two important messages: (1) especially during recessions (indicated by shaded bars), the distance between actual and expected inflation widens, (2) the variability (and, in turn, the disagreement) in forming expectations increases in those periods. Therefore, the proposal to raise the inflation target (such as those of Blanchard et al. (2010) and Ball (2014) discussed above) should be carefully evaluated in order to assess if it hampers the management of private sector’s expectations, which is one of the key objectives pursued by central banks. If we consider the last part of the graph, we observe that actual inflation is regularly below its expected value. We believe that one of the determinants of such a spread was the debate about the need to increase the inflation target to stimulate the economy after the Great Recession. People expected higher inflation which on the other hand was smaller. Consistently to what documented by Erceg and Levin (2003), the graph suggests that in a time period close to the debate about a change in the target (credibility issue) expectations diverge largely from the true values. In our model adaptive expectations introduce inertia in an otherwise benchmark fully forward–looking DSGE model. Informed agents do not fully take into account the correct sluggish dynamics of the economy because they have no information on how fast the economy converges to the true restricted perception equilibrium. Source: FRED Saint Louis Actual and expected inflation in the US over the period Jnuary 1978–Jube 2019  The rest of the paper is organized in the following way. Section 2 presents the model and the learning problem, while Sect. 3 reports loss evaluations based on numerical simulations and assesses the impact of a permanent target shock. Section 4 concludes.",
56.0,3.0,Computational Economics,07 December 2019,https://link.springer.com/article/10.1007/s10614-019-09951-6,Heuristic Switching Model and Exploration-Exploitation Algorithm to Describe Long-Run Expectations in LtFEs: a Comparison,October 2020,Annarita Colasante,Simone Alfarano,Eva Camacho-Cuena,Female,Female,Female,Female,"The origin of heterogeneity across individual expectations and the role that it plays in shaping aggregate outcomes is an important topic in theoretical as well as empirical research in macroeconomics. Unlike the stock prices, volumes of sold books, interest rates of bonds, downloads or number of likes, which can be precisely measured and recorded in the new world where almost “everything” is now in an electronic format, expectations are not directly observable. This means that there is a significant limitation when it comes to fully understanding the precise role played by expectations in driving macroeconomic aggregates. One way to circumvent this problem and rigorously model the expectations of individuals is to assume consistent expectations, i.e. rational expectations, following the seminal idea of Muth (1961), which has been further developed by Lucas Jr and Prescott (1971). From a formal point of view, the main advantage of rational expectations is that agents can be rational in one way only. The argument put forward by Friedman and Friedman (1953) on the irrelevance of “irrational” individuals in the long run gives further intuitive appeal to the rationality assumption. As an alternative to the rational expectations paradigm, it certainly plays a central role in the bounded rationality assumption of economic agents introduced by Simon (1966). In the world of bounded rational agents, we typically lose the uniqueness of the behavior, as agents can be “non-rational” in many different ways. Laboratory experiments have been largely demonstrated to be an essential methodology for shedding light on the degree of bounded rationality of individuals. Countless experiments have shown that in complex environments, subjects follow simple adaptive rules, called heuristics, in order to form expectations, changing their mind as a function of the evolution of the environment and adapting to the new circumstances. The principle of anchor and adjustment, introduced by Kahneman and Tversky (1973), is a sufficiently general and flexible framework that can be certainly cast into the bounded rationality paradigm and is able to realistically describe the way individuals form and adapt their expectations. Laboratory experiments is one of the methodologies that allows us to directly elicit individual expectations using performance-based incentives. In particular, within the experimental literature, Learning-to-Forecast Experiments, in the following LtFEs, (see Marimon et al. 1993), are designed to study the formation of individual expectations within different expectations feedback systems in a market where the price depends on subjects’ (short-run) predictions. Such an experimental framework makes it possible to study the conditions under which individual predictions converge to the rational expectations equilibrium. Moreover, it enables efficient testing of alternative formulations of expectation formation models. A large number of LtFEs have been conducted to analyse the way individuals form and adapt their short-run expectations in different economic environments: in financial markets (Hommes et al. 2005), real estate markets (Bao and Ding 2016), commodity markets (Bao et al. 2013) and in simple macroeconomic frameworks (Assenza et al. 2011; Anufriev et al. 2013; Cornand and M’baye 2016). The vast majority of those LtFEs focus on eliciting individual short-run expectations, i.e. providing incentives to forecast the next period market price. The novel contribution to this experimental literature of Colasante et al. (2018a, 2019) is to focus on the entire spectrum of expectations, as they elicit contemporaneously short- and long-run expectations about the evolution of the market price under positive and negative feedback systems. In particular, they explicitly give incentives to the subjects to submit their predictions regarding the market price at the beginning of every period, giving the possibility to revise the predictions as new information becomes available. This experimental design makes it possible to study how expectations form and co-evolve with the price at different forecast horizons. Their results concerning short-run predictions are in line with the literature (see Heemeijer et al. 2009): fast convergence and slow coordination in the markets with negative feedback, and slow convergence and fast coordination in the markets with positive feedback. Regarding the long-run predictions, Colasante et al. (2018a, 2019) observe that in markets with positive feedback treatments the market price plays a pivotal role in the expectations formation process, whereas in markets with negative feedback it turns out that subjects use the fundamental value as their main reference point. We can find few computational learning algorithms in the literature to describe individual expectations in LtFEs (see Heemeijer et al. 2009; Assenza et al. 2011; Bao et al. 2013; Hommes and Lux 2013). The most commonly used is the so-called Heuristic Switching Model, HSM hereafter (see Brock and Hommes 1998). Using experimental data on long-run expectations, Colasante et al. (2018b) introduced an alternative adaptive learning model of bounded rationality, the Exploration-Exploitation Algorithm (EEA) that, contrary to the HSM, accounts contemporaneously for subjects’ short- and long-run expectations. The aim of this paper is to evaluate the performance of the EEA and the HSM to reproduce the long-run expectations in markets with positive and negative feedback. Since the original version of the HSM can account for short-run expectations only, we introduce a modified version of the HSM to capture the short- as well as long-run expectations. In the present work, we propose a very basic generalization of this algorithm based on the idea that subjects linearly extrapolate their short-run predictions. The aim of this simple modification is to have a benchmark useful for understanding on what extent subjects implement such a simple extrapolation rule or if they take into account different information to make predictions in different forecast horizons. One can evaluate the performance of this benchmark to precisely measure deviations from the linear extrapolation hypothesis. We take the position that comparing the capability of different algorithms in describing the dynamical properties of the formation mechanism of individual expectations is a valuable contribution. Comparing the basic constituents of the algorithms helps to identify which are the determinants in designing reliable models of expectations formation. Furthermore, it allows to devise new models by possibly combining the crucial ingredients of the considered algorithms in a more efficient architecture to describe the data and to model expectations. The paper is organized as follows: in the Sect. 2, we illustrate the experimental setting of the LtFE and the experimental results. In Sect. 3, we describe the details of the two learning algorithms, namely, the HSM and its modified version and the EEA. In Sects. 4 and 5, we describe the simulation results and propose a modified version of the EEA to include a different anchor, respectively. Finally, in Sect. 6 we present the main conclusions.",
56.0,3.0,Computational Economics,02 January 2020,https://link.springer.com/article/10.1007/s10614-019-09961-4,An Evolutionary Approach to Passive Learning in Optimal Control Problems,October 2020,D. Blueschke,I. Savin,V. Blueschke-Nikolaeva,Unknown,Unknown,Unknown,Unknown,,
56.0,3.0,Computational Economics,20 January 2020,https://link.springer.com/article/10.1007/s10614-020-09968-2,How Active is Active Learning: Value Function Method Versus an Approximation Method,October 2020,Hans M. Amman,Marco P. Tucci,,Male,Male,Unknown,Male,"In recent years there has been a resurgent interest in economics on the subject of optimal or strategic experimentation also referred to as active learning, see e.g. Amman et al. (2018), Buera et al. (2011) and Savin and Blueschke (2016).Footnote 1 There are two prevailing methods for solving this class of models. The first method is based on the value function approach and the second on an approximation method. The former uses dynamic programming for the full problem as used in studies by Prescott (1972), Taylor (1974), Easley and Kiefer (1988), Kiefer (1989), Kiefer and Nyarko (1989), Aghion et al. (1991) and more recently used in the work of Beck and Wieland (2002), Coenen et al. (2005), Levin et al. (2003) and Wieland (2000a, b). A nice set of applications on optimal experimentation, using the value function approach, can be found in Willems (2012). In principle, the value function approach should be the preferred method as it derives the optimal values for the policy variables through Bellman’s (1957) dynamic programming. Unfortunately, it suffers from the curse of dimensionality, as is shown in Bertsekas (1976). Hence, the value function approach is only applicable to small problems with one or two policy variables. This is caused by the fact that solution space needs to be discretized in such a fashion that it cannot be solved in feasible time. The approximation methods as described in Cosimano (2008) and Cosimano and Gapen (2005a, b), Kendrick (1981) and Hansen and Sargent (2007) use approaches, that are applied in the neighborhood of the linear regulator problems.Footnote 2 Because of this local nature with respect to the statistics of the model, the method is numerically far more tractable and allows for models of larger dimension. However, the verdict is still out as to how well it performs in terms of approximating the optimal solution derived through the value function. By the way, the approximation method described here, should not be mistaken for a cautious or passive learning method. Here we concentrate only on optimal experimentation—active learning—approaches. Both solution methods consider dynamic stochastic models in which the control variables can be used not only to guide the system in desired directions but also to improve the accuracy of estimates of parameters in the models. There is a trade off in which experimentation of the policy variables early in time deviates from reaching current goals, but leads to learning or improved parameter estimates and improved performance of the system later in time. Ergo, the dual nature of the control. For this reason, we concentrate in the sections below on the policy function in the initial period. Usually most of the experimentation—active learning—is done in the beginning of the time interval, and therefore, the largest difference between results obtained with the two methods may be expected in this period. Until very recently there was an invisible line dividing researchers using one approach from those using the other. It is only in Amman et al. (2018) that the value function approach and the approximation method are used to solve the same problem and their solutions are compared. In that paper the focus is on comparing the policy function results reported in Beck and Wieland (2002), through the value function, to those obtained through approximation methods. Therefore those conclusions apply to a situation where the controller is dealing with a nonstationary process and there is no penalty on the control. The goal of this paper is to see if they hold for the more frequently studied case of a stationary process and a positive penalty on the control. To do so a new value function algorithm has been written, to handle several sets of parameters, and more general formulae for the cost-to-go function of the approximation method are used. The remainder of the paper is organized as follows. The problem is stated in Sect. 2. Then the value function approach and the approximation approach are described (Sects. 3 and 4, respectively). Section 5 contains the experiment results. Finally the main conclusions are summarized (Sect. 6).",
56.0,4.0,Computational Economics,02 November 2019,https://link.springer.com/article/10.1007/s10614-019-09927-6,The Use of Partial Fractional Form of A-Stable Padé Schemes for the Solution of Fractional Diffusion Equation with Application in Option Pricing,December 2020,H. Ghafouri,M. Ranjbar,A. Khani,Unknown,Unknown,Unknown,Unknown,,
56.0,4.0,Computational Economics,29 October 2019,https://link.springer.com/article/10.1007/s10614-019-09936-5,ORPIT: A Matlab Toolbox for Option Replication and Portfolio Insurance in Incomplete Markets,December 2020,Vasilios N. Katsikis,Spyridon D. Mourtas,,Male,Male,Unknown,Male,"In finance, the minimization of costs in portfolios is always of high importance. One way to minimize the cost of a portfolio is by minimizing its insurance cost (see Zymler et al. 2011; Annaert et al. 2019; Lee et al. 2010; Bouyé 2009; Matsumoto 2008; Liu 2009). For example, in Zymler et al. (2011) the authors propose a novel robust optimization model for designing portfolios that include European-style options. They extend robust portfolio optimization to accommodate options and they show how the options can be used to provide strong insurance guarantees. Matsumoto (2008), by considering portfolio insurance in a low liquid market, the author solve the optimal expected growth rate problem with a floor in a random trade time model and provides an optimal constant proportion portfolio insurance strategy in a simple form. Another way to minimize the cost of a portfolio is by replicating its options (see Hübner 2016; Lim 2018; Matsumoto 2013; Sorokin and Ku 2016; Tich 2006; Pelsser 2003; Meucci 2010). For instance, the author in Matsumoto (2013), by considering the replication problem with illiquidity in the discrete time model, shows that the replication strategy depends on the settlement method and because of that the initial replication cost can be determined independently of the physical probability measure. Tich (2006), the author shows how replication methods of digital options work if the underlying process is not known or cannot be utilized when the replication portfolio is constructed both in the case of dynamic replication and static replication within four distinct models, three of which are incomplete. In this paper we describe the ORPIT Matlab toolbox (ORPIT standing for ‘Option Replication and Portfolio Insurance Toolbox’) comprising ten Matlab functions that determine the lattice-subspaces and sublattices of \(\mathbb {R}^{\ell }\) and make calculations in them. They apply the previous in order to do options replication in the case of \(\mathbb {R}^{\ell }\) and to solve the cost minimization problem of the minimum-cost insured portfolio both in the case of \(\mathbb {R}^{\ell }\) and the space of continuous functions defined on an interval [x, y], i.e., C[x, y] . ORPIT, also, comprises a GUI (Graphical User Interface) and the Matlab toolbox inference_with_classifiers (see Olivetti 2014) which includes five Matlab functions. The software can be downloaded from the website: https://sites.google.com/view/orpit/, where one can find supportive material, including the algorithmic procedures, and several worked examples. Main results of the present paper are summarized as follows. Section 2 gives preliminaries and notation. Section 3 classifies ORPIT’s functions and explains their input and output arguments. Section 4 gives the basic features of GUI and offers examples that demonstrate the strengths and capabilities of the different ORPIT’s functions. Finally, in Sect. 5, we give concluding remarks.",8
56.0,4.0,Computational Economics,04 November 2019,https://link.springer.com/article/10.1007/s10614-019-09940-9,Multiple Shooting Method for Solving Black–Scholes Equation,December 2020,Somayeh Abdi-Mazraeh,Ali Khani,Safar Irandoust-Pakchin,Unknown,Male,Unknown,Male,"The options are the common financial derivatives in the markets. An option is a contract that gives the right to its holder to buy or sell an underlying asset at a determined date (maturity or expiration date) with a determined price (strike or exercise price). With regard to the expiration date, options would be divided into two types; European options and American options (Jiang 2004; Wilmott et al. 1993). European options can be exercised only at the expiration date. On the other hand, American options can be exercised at any time up to the expiration date. The options are priced using mathematical models. One of the popular models for the pricing of an option is the B–S model which leads to a PDE. The obtained PDE is called the B–S equation. In the B–S model, the underlying asset price S follows the geometrical Brownian motion where \(\mu \) and \(\sigma \) are (constant) expected return rate and volatility, respectively. Also, W is a Wiener standard process. Let V(S, t) denotes the option price. By applying It\(\hat{o}\) lemma and making an investment portfolio which is risk-free, the (B–S) equation can be obtained as follows (Black and Scholes 1973; Chang et al. 2007; Wilmott et al. 1993); wherein r is the risk-free interest rate. The B–S equation is derived under certain assumptions such as the risk-free interest rate is a constant or there are no transaction costs. Clearly, in real markets, such assumptions cannot be established. Therefore, the researchers are always trying to improve this model and make it more realistic. Note that it is impossible to include all the factors affecting the market in a mathematical model. Suppose T and E denote the expiration date and the strike price, respectively. The European option pricing would be modeled as Eq. (1.2) with initial condition and the boundary conditions which has an exact solution (Hull 2000; Wilmott et al. 1993). Moreover, Eq. (1.2) with initial condition (4.2) and following boundary conditions is named the problem of up-and-out barrier call option pricing with barrier price B which has an exact solution (Farnoosh et al. 2015, 2017; Haug 2007; Jiang 2004). Different approaches are presented to solve the (B–S) equation, such as finite difference method (in’t Hout and Volders 2009; Wade et al. 2007), finite element method (Golbabai et al. 2013; Markolefas 2008), radial base functions (Bastani et al. 2013), collocation method (Mohammadi 2015) and so on. The C–N method is one of the most famous methods for the discretization of PDEs. Therefore, this method is sometimes applied for pricing of financial derivatives (Giles and Carter 2006; Wade et al. 2007). In addition, the equipped methods with variable step size are utilized for solving the options pricing problems (Khaliq et al. 2008; Persson and Sydow 2010; Vaquero et al. 2014). The variable step size method provides the possibility to select the most appropriate step size in each time step to a solver. Also, the method with variable step size gives to solver the ability to achieve the desired accuracy by controlling the computational error and stability. One of the approaches for solving boundary value problems (BVPs) is to apply a shooting method which converts the BVP to a sequence of initial value problems (IVPs) (Chang et al. 2007; Stoer and Bulirsch 2002). There are different types for the shooting methods and the multiple shooting method can be the most common one (Carbonell et al. 2016; Dueias et al. 1999; Zhang 2012). A multiple shooting method partitions the given domain to several subinterval. Then on each subinterval, it replaces BVP to IVPs. After obtaining an approximate solution for each subinterval by using of an IVP solver, it patches them together to get a solution on the whole given domain. Thereupon, the non-smooth behavior of the option’s payoff in B–S model can be modified by selecting an appropriate partition in the multiple shooting method. The multiple shooting method has been first introduced by Riley et al. (1962), later generalized by Keller (1976). In this study, A numerical technique based on the C–N method and equipped with variable step size is utilized to discretisize the time domain. Consequently, a system of BVPs is obtained. Moreover, the multiple shooting method combined with Lagrange polynomials is used for the spatial direction of the B–S equation. Note that extreme points of Chebyshev polynomials which are called Chebyshev-Gauss-Lobato (CGL) points are exploited as interpolation points of Lagrange polynomials. This paper is organized as follows: In Sect. 2, the Lagrange polynomials based on CGL points are presented. In Sect. 3, the multiple shooting method is described. The implementation of the C–N scheme in the time domain and the multiple shooting method in the spatial direction are explained in order to solve the B–S equation in Sect. 4. The stability of the proposed method is studied in Sect. 5. In Sect. 6, the method of preselecting an upper bound of the spatial direction and implementing variable step size are explained. Also, the numerical examples are included for indicating the order of convergence and the accuracy of the proposed scheme. Finally, the conclusion of the proposed method is given in the last section.",5
56.0,4.0,Computational Economics,04 November 2019,https://link.springer.com/article/10.1007/s10614-019-09941-8,Forecasting with Second-Order Approximations and Markov-Switching DSGE Models,December 2020,Sergey Ivashchenko,Semih Emre Çekin,Rangan Gupta,Male,Male,Unknown,Male,"Dynamic Stochastic General Equilibrium (DSGE) models are frequently used by academics and researchers at public institutions for policy-analysis and forecasting purposes.Footnote 1 Most of these models make use of a framework that is based on linear first-order perturbations, where it is assumed that the sample of data that is used in the estimation is not subject to any regime-switching behaviour. In a recent review of research that has been conducted using DSGE models, Christiano et al. (2018) note that the extensive application of first-order approximations for the model solution may be motivated by the fact that these models appear to provide an accurate characterisation of the effects of small shocks that arose during the post-war period in the United States. In addition, these techniques also allow researchers to make use of linear filters and estimation methodologies that are not subject to the computational complexity of non-linear counterparts. However, despite the attractive features of linear models that employ first-order approximations for the model solution, there are those who suggest that these models may fail to capture many of the non-linear features that are present in macroeconomic data. For example, Stiglitz (2018) notes that the use of linear approximations in such a macroeconomic model would be inappropriate, as it would not provide an accurate description of the effects of large shocks. In addition, Fernández-Villaverde et al. (2016) and Schmitt-Grohé and Uribe (2004) have suggested that the use of higher-order perturbation techniques could provide improvements in terms of the accuracy of the model solution.Footnote 2 In this paper we seek to evaluate the use of high-order perturbation techniques for the model solution, where such models may also incorporate different regime-switching features. To assess the degree to which the different perturbation techniques and regime-switching features are able to explain the underlying data, we compare the out-of-sample forecasting performance of the respective models.Footnote 3 From an intuitive perspective, the use of higher-order solutions may be more accurate when the model has no analytical solution and it incorporates a number of non-linear features. In such a case, a smaller approximation error for the model solution would potentially allow the model to provide an improved out-of-sample fit of the data. However, we should also acknowledge that all macroeconomic models include mis-specification errors and higher-order approximations may be more sensitive to such errors, if the true underlying relationship between the variables may be described with sufficient accuracy by a first-order solution. As noted by Granger and Teräsvirta (1993), parameter estimation in models that incorporate various forms of non-linear relationships is inherently more difficult, as there are more possibilities and many more parameters to estimate, which would imply that there are also potentially more mistakes (or mis-specification errors) that could be made. Hence, the model that employs higher-order techniques to approximate the solution may provide inferior out-of-sample forecasts. Such a deterioration would potentially be more prominent when the likelihood function in models that utilise higher-order solution techniques are not restricted to the same extent as the first-order counterpart.Footnote 4 A similar situation arises when we consider the use of regime-switching features in the model. After incorporating these stochastic non-linear parameters, we may differentiate between periods where there are differences in the underlying behaviour of economic agents. This may give rise to an improved forecasting performance if such regime-switching behaviour is present in the underlying data and it has been accurately described by these parameters. However, if such features do not exist, are very small, or may not be described by these particular regime-switching features, then the regime-switching model would incorporate a larger mis-specification error and when working with data samples that have a finite length, the parameter estimates may be imprecisely estimated. In such cases, the quality of the forecasts that are produced by a mis-specified regime-switching model may decrease. When approximating the solutions for regime-switching models with perturbation techniques of different orders one would potentially introduce slightly different mis-specification and approximation errors and as a result we evaluate the out-of-sample forecasting performance of each of these models individually. Therefore, this paper seeks to contribute towards the literature that considers the forecasting performance of models that make use of first- and second-order approximations, to extend the work of Pichler (2008) and Balcilar et al. (2015), were we also consider the use of higher-order perturbation approximations for models that incorporate regime-switching features. As noted in Liu et al. (2009, 2011), Liu and Mumtaz (2011) and Foerster et al. (2016), the use of Markov-switching DSGE (MS-DSGE) models allow for the analysis of more complex dynamic features that may be present in the data. In most cases, these models require the use of extensions to solution algorithms that are typically applied to single-regime models, such as those described in Maih (2015), and non-linear filters for the evaluation of the likelihood function. To complete the analysis in this paper we make use of the methodology that is described in Ivashchenko (2016) for the estimation and filtering of MS-DSGE models that make use of second-order approximations for the model solution. The underlying model incorporates several New Keynesian features, with the addition of partial indexation on previous inflation. Nominal rigidities are introduced into the price setting mechanism of the firm and investment adjustment costs, where we make use of Rotemberg (1982) pricing. Two variants of the MS-DSGE model are considered, where the first considers the possibility of regime-switching in the policy rule and the second considers switching in the volatility of the shocks. By allowing for regime-switching in the volatility of the shocks, the model could potentially distinguish between the effects of relatively large and small shocks, where one would expect that higher-order solution techniques could potentially provide more accurate results, when large shocks would take us further away from the steady-state. All the models are then estimated over recursive data samples for the United States economy and the forecasts are evaluated on the basis of the root-mean squared-error and log-predictive score (where we use both Gaussian and mixed-Gaussian distributions). The results suggest that while the use of higher-order approximations may provide a superior out-of-sample fit of the data in a model that is restricted to a single regime, this is not the case for those models that allow for regime-switching. This would imply that when we allow for the possibility of Markov-switching, a single-order approximation of the model solution may be sufficiently accurate. Hence, when estimating a relatively parsimonious single-regime model, a second-order approximation, which incorporates a number of additional components in the evaluation of the likelihood function, provides a superior out-of-sample result. However, in the case of the MS-DSGE model, which is less parsimonious, the incorporation of these additional components does not result in an improved out-of-sample fit. These results differ somewhat from those of Pichler (2008), who found that when using simulated data, the forecasting performance of a model that utilises a second-order approximation and a particle filter for the evaluation of the likelihood function may outperform a model that makes use of a first-order approximation and a linear Kalman filter. However, he also showed that when applied to economic data for the United States, the first-order approximation of the model solution provided a superior out-of-sample result. The structure of this paper is as follows: Sect. 2 presents the model structure, which includes details of the regime-switching behaviour. Section 3 provides details of the data and the methodology for the evaluation of the forecasts, while Sect. 4 contains details relating to the derivation of the first- and second order approximations for the model. Section 5 discusses the results of the different specifications and Sect. 6 concludes.",
56.0,4.0,Computational Economics,18 November 2019,https://link.springer.com/article/10.1007/s10614-019-09943-6,An Iterative Approach to Ill-Conditioned Optimal Portfolio Selection,December 2020,Mårten Gulliksson,Stepan Mazur,,Male,Male,Unknown,Male,"Modern portfolio theory has drawn much attention in the academic literature starting from 1952 when Harry Max Markowitz published his seminal paper about portfolio selection (see Markowitz 1952). He proposed efficient way of portfolio allocation that guarantees the lowest risk for a given level of the expected return. A number of papers are devoted to questions like, e.g., how can an optimal portfolio be constructed, monitored, and/or estimated by using historical data (see, e.g., Alexander and Baptista 2004; Golosnoy and Okhrin 2009; Bodnar 2009; Bodnar et al. 2017a; Bauder et al. 2018), what is the influence of parameter uncertainty on the portfolio performance (cf., Okhrin and Schmid 2006; Bodnar and Schmid 2008; Javed et al. 2017), how do the asset returns influence the portfolio choice (see, e.g., Jondeau and Rockinger 2006; Mencia and Sentana 2009; Adcock 2010; Harvey et al. 2010; Amenguala and Sentana 2010), how is it possible to estimate the characteristics of the distribution of the asset returns (see, e.g., Jorion 1986; Wang 2005; Frahm and Memmel 2010), how can the structure of optimal portfolio be statistically justified (Gibbons et al. 1989; Britten-Jones 1999; Bodnar and Schmid 2009). Björk et al. (2014) studied the mean–variance portfolio optimization in continuous time, whereas Liesiö and Salo (2012) developed a portfolio selection framework which uses the set inclusion to capture incomplete information about scenario probabilities and utility functions. Chiarawongse et al. (2012) formulated a mean–variance portfolio selection problem that accommodates qualitative input about expected returns and provided an algorithm that solves the problem, while Levy and Levy (2014) analyzed the parameter estimation error in portfolio optimization. There is another strand of research which focuses on factor models with applications to finance. In particular, factor models are commonly used in asset pricing theory and classical examples are the Capital Asset Pricing Model (CAPM) and the Arbitrage Pricing Theory (APT) (see Ross 1976; Sun et al. 2019 and references therein). Practically speaking, factor models can be used in the portfolio construction, the evaluation of performance of portfolio managers, the predicting asset returns, etc. (Meucci 2005; Chincarini and Kim 2006). Different estimation techniques for the covariance and precision matrices which are based on factor models in small and large dimensions with applications in portfolio theory are proposed by Ledoit and Wolf (2003), Fan et al. (2008), Fan et al. (2012), Fan et al. (2013), Bodnar and Reiss (2016), De Nard et al. (2019) and among others. There is also the well known Barra Risk Factor Analysis which is pioneered by Bar Rosenberg, founder of Barra Inc. (see Grinold and Kahn 2000; Christopherson et al. 2009; Connor and Korajczyk 2010). In its core the model involves a number of factors that can be utilized to predict and control risk.Footnote 1 All above discussed papers are focused on the case when the covariance matrix of the asset returns is positive definite. In practice, covariance matrix is unknown, therefore, it needs to be estimated using historical data. Most common estimators are sample and maximum likelihood estimators. If the number of observations is greater than the number of assets in the portfolio then both estimators of the covariance matrix are positive definite. However, if the number of observations is less than the number of assets in the portfolio then both estimators of the covariance matrix are singular.Footnote 2 In this paper, we focus on the second case that leads us to the singular estimators of the covariance matrix. In practice, one could face this situation because of different reasons which are discussed by Bodnar et al. (2016, 2017b) and Bodnar et al. (2019c). For example, it is very common to consider the sample size that is shorter over time period to avoid varying dependence between assets. Additionally, one can have a large number of assets in the portfolio: the S&P 500 index consists of 500 companies that are traded on the NYSE and NASDAQ. It is also possible to face the multicollinearity because of high correlation of assets within a specific industry branch. Since optimal portfolio weights depend on the inverse of the covariance matrix, the original optimization problem formulated by Markowitz (1952) will have an infinite number of solutions when the covariance matrix is singular. In Pappas et al. (2010), the solution to the optimization problem with singular covariance matrix is obtained by replacing the inverse with the Moore–Penrose inverse. It leads us to the unique solution with the minimal Euclidean norm. Statistical properties of the optimal portfolio weights for small sample and singular covariance matrix are well studied by Bodnar et al. (2016, 2017b) and Bodnar et al. (2019c). There are also various regularization methods that can be used when the covariance matrix is singular. The most common techniques are the ridge-type approach, the Landweber–Fridman algorithm, the spectral cut-off method, and the Lasso-type approach. The ridge-type approach is constructed by adding a diagonal matrix to the covariance matrix (Tikhonov and Arsenin 1977), while the Landweber–Fridman algorithm delivers a convergent iterative scheme (Kress 1999). The spectral cut-off method is based on the removing the eigenvectors associated with the smallest eigenvalues (Chernousova and Golubev 2014), and the Lasso-type approach penalizes the \(l_1\) norm of the optimal portfolio weights (Brodie et al. 2009). The main aim of the present paper is to deliver an alternative approach. In particular, we employ an iterative method that solves the linear ill-posed problem approximately. Iterative methods for linear ill-posed problems are certainly not new and include classical methods like Landweber iteration with Nesterov acceleration and Conjugate gradient methods, see Neubauer (2000, 2017). Very recently a new approach has been developed based on second order damped dynamical systems, see Gulliksson et al. (2019) for an introduction and overview and specifically Zhang and Hofmann (2018) for the ill-posed linear case. In Zhang and Hofmann (2018) it is shown that the method is a regularization method, i.e., loosely speaking for an ill-posed linear problem there exists a unique solution when the number of iterations tend to infinity and the error tends to zero. We have applied and extended this method to the rank-deficient and linearly constrained portfolio selection problem considered here. Specifically, we show that the method is convergent and how to choose optimal parameters (time step and damping). As seen in Sects. 3.1 and 3.2 the iterative method generally performs better in the sense of giving a smaller variance of the portfolio. The rest of the paper is structured as follows. In Sect. 2, an iterative approach to ill-conditioned optimal portfolio selection is discussed. The results of numerical and empirical studies are discussed in details in Sect. 3, while Sect. 4 summarizes the paper.",10
56.0,4.0,Computational Economics,12 November 2019,https://link.springer.com/article/10.1007/s10614-019-09944-5,Posterior Inference on Parameters in a Nonlinear DSGE Model via Gaussian-Based Filters,December 2020,Sanha Noh,,,Unknown,Unknown,Unknown,Unknown,,
56.0,4.0,Computational Economics,14 December 2019,https://link.springer.com/article/10.1007/s10614-019-09945-4,Measuring Spatio-temporal Efficiency: An R Implementation for Time-Evolving Units,December 2020,Georgios Digkas,Konstantinos Petridis,Ali Emrouznejad,Male,Male,Male,Male,"Evolution occurs in all business, economic, and technological systems. They evolve as their constituent parts, such as means of production, market mechanisms, processes etc., change over time. Computer software can also be regarded as an evolving system since it gradually undergoes maintenance in order to correct errors and accommodate change and new requirements. This evolution is clearly evident in software systems from the multitude of releases that they offer over time. In order to study the evolution of software (or any other evolving system) which comprises a series of successive versions, the current version of the system should be compared with temporally previous versions. What would be useful in this case is to identify a previous, but near in time, version with similar functionality which could be regarded as more efficient than the current version, in terms of its ability to maximize output (e.g. certain software metrics) for a given input (e.g. software functionality). In such a case and in the context of software, the previous version could serve as a benchmark for the current version, meaning that its characteristics could be used as a guide to examine more carefully the characteristics of the current software version. A similar reasoning applies to the temporal analysis of system evolution in other domains, such as economies or businesses. One of the most widely known methodologies in order to measure efficiency is Data Envelopment Analysis (DEA) (Banker et al. 1984; Charnes et al. 1985). In DEA, the entity under study is called a decision making unit (DMU). For each DMU, a virtual input and a virtual output are formed (input and output items respectively, multiplied by weights) and then the weights are determined, using linear programming, so as to maximize the ratio of the virtual output over the virtual input. What is actually measured in DEA is the relative efficiency of a DMU against the other DMUs. The efficient units, i.e. the units with the maximum efficiency, form a surface named as “efficient frontier” which envelops all the inefficient units. The efficient DMUs can be used as benchmarks for the inefficient ones in order to improve their efficiency. Dynamic forms of Data Envelopment analysis have been applied from Supply Chain optimization models (Grigoroudis et al. 2014; Petridis et al. 2017) as well as to the selection of forecasting techniques (Emrouznejad et al. 2016). As already mentioned, the right choice, when time series data are examined, is to compare the entity under study with preceding entities. In that way, the efficiency that should be measured is not only determined according to the spatial distance of DMUs from the efficient frontier (as usually occurs), but also according to their time distance (how close the DMUs are in terms of time). Thus, efficiency is considered with regard to both the spatial and temporal dimensions. In the context of this study, this is called spatio-temporal efficiency. The new concept of spatio-temporal efficiency is graphically depicted in Fig. 1. Each DMU is arranged temporally and spatially, as well. In this hypothetical case, three DMUs are considered: DMU(6) which is the DMU under investigation and its peers, DMU(2) and DMU(3). The horizontal axis represents the closeness of a unit to other units (their spatial distance), while the vertical axis represents the temporal difference of a unit from the other units. In this example, \( \lambda_{2} \) equals to 0.7 while \( \lambda_{3} \) equals to 0.3. However, DMU(2) is temporally more distant than DMU(3) (the temporal distance is 4); DMU(3) is temporally closer to DMU(6) (with temporal distance equal to 3). Since the aim of the proposed approach is to find the DMU which is closer to the DMU under investigation, values in the horizontal axis are presented in a decreasing order; the largest the lambda value of a peer, the more resemblance it bears with the inputs/outputs of the DMU under study. Based on this example, there is not a rule of thumb to assist in making the decision on which unit, 2 or 3, should be selected as a single peer because the DMU under investigation should be compared with a temporally and spatially closer DMU. The previous illustrative example indicates the need for a unique peer selection in terms of both the spatial and temporal dimensions. The resulting efficiency is the spatio-temporal efficiency, which is analyzed more extensively in the next sections of the paper. This type of efficiency cannot be addressed by conventional DEA methods or other DEA techniques that deal with time series data. Units arranged over space and time The rest of the paper is organized as follows: in Sect. 2, a literature review with relevant works on DEA is presented, also identifying the gap in the literature. In Sect. 3, the R implementation is analytically described, demonstrating all the stages of the proposed methodology. The interface and the functional characteristics of the proposed R package are demonstrated in Sect. 4, with an application in a real-life example from the discipline of software engineering. Finally, conclusions are drawn in Sect. 5.",
56.0,4.0,Computational Economics,11 November 2019,https://link.springer.com/article/10.1007/s10614-019-09947-2,Fast Monte Carlo Simulation for Pricing Equity-Linked Securities,December 2020,Hanbyeol Jang,Sangkwon Kim,Junseok Kim,Unknown,Unknown,Unknown,Unknown,,
56.0,4.0,Computational Economics,11 December 2019,https://link.springer.com/article/10.1007/s10614-019-09953-4,Optimal Grid Selection for the Numerical Solution of Dynamic Stochastic Optimization Problems,December 2020,Karsten O. Chipeniuk,,,Male,Unknown,Unknown,Male,"Robust and accurate global solution methods are indispensable in the solving of dynamic stochastic optimization problems characteristic of heterogeneous agent macroeconomics models. This class of models features substantial nonlinearity and high dimensionality inherent in the optimization problems facing the various economic agents, which are largely absent in the representative agent framework. Consequently, the numerical methods applied must be finely tuned to each individual model and its parameters. This has led to a large body of literature in recent decades aiming to address the accuracy, speed, and versatility of the numerical solution of baseline heterogeneous agent models (Krusell and Smith 1998; Algan et al. 2008; Reiter 2009; Den Haan 2010a; Kim et al. 2010; Maliar et al. 2010; Reiter 2010; Den Haan and Rendahl 2010; Young 2010; Achdou et al. 2017; Ahn et al. 2018). A recurring challenge to applying these solution methods is that their implementation can be ad-hoc in places where the researcher is lacking theoretical guidance on how to proceed optimally, with potential consequences for the accuracy and efficiency of the method being used. An example of the sort of discretionary choice one faces when globally solving a given dynamic stochastic optimization problem is that of constructing a discrete approximation to an economic agent’s continuous state space. Some rules of thumb do apply to this selection. For example, Krusell and Smith (1998) and Reiter (2009) construct grids that include several points near the household borrowing constraint, citing the high degree of curvature in this region. However, they do not examine the sensitivity of the solution accuracy to this choice. On the other hand, Maliar et al. (2010) solves the model of Krusell and Smith (1998) while sampling several different choices of grids which are distributed linearly, logarithmically, or polynomially. These authors find that the most accurate solution of their particular calibration is obtained when the grid points are distributed according to a polynomial of degree 7. However, this procedure is ad-hoc, lacking theoretical motivation, and moreover cannot produce broadly applicable results. One algorithm which partially automates the selection of a grid for interpolation is the endogenous grid of Carroll (2006). This procedure is primarily motivated by efficiency rather than accuracy, as the algorithm provided replaces a costly root finding procedure required to solve the household’s consumption problem with algebraic manipulation. Moreover, it also requires specifying an initial discrete state space representation from which the endogenous grid points are derived, so that there remains the question of how best to choose this. The endogenous grid method has subsequently been extended to settings with occasionally binding constraints (Hintermaier and Koeniger 2010), discrete and continuous choices (Fella 2014; Iskhakov 2015), and high dimensional problems (White 2015; Druedahl and Jorgensen 2017). However, these studies focus primarily on the speed improvements, leaving a gap in the literature regarding the efficient choice of a discrete state space representation of a dynamic stochastic optimization problem with respect to the accuracy of the solution. This article aims to fill this gap by drawing on numerical analysis which addresses accurate linear interpolation of continuous functions. In particular, we consider the method of Gavrilović (1975) for the approximation of a known function by way of a piecewise linear interpolant, which is optimal in the sense of minimizing the worst absolute error. Linear interpolation is a method which is commonly applied to approximate a nonlinear function when no convenient way to compute arbitrary function values is known. While it often fails to capture the smoothness and curvature properties of the target function, its simplicity and asymptotic convergence to the target mean it continues to be widely used in the solution of dynamic stochastic optimization problems. We investigate Gavrilović (1975)’s method in the context of the savings decision of a finitely lived household facing incomplete markets and income fluctuations of the type considered in Huggett (1993), Aiyagari (1994), and Achdou et al. (2017). These models underlie an extensive body of literature which has studied the role of incomplete markets and heterogeneity for the cost of business cycles (Krusell and Smith 1999), monetary and fiscal policy (Kaplan et al. 2018; McKay and Reis 2016), and life cycle dynamics (Storesletten et al. 2004), among other topics. A key feature of this framework is that the equilibrium includes a continuous distribution of agents across asset states, necessitating a global solution of the individual consumption-savings dynamic stochastic optimization problem. By focusing on a stylized version of such a model, we identify qualitative and quantitative features of accurately approximated solutions of dynamic stochastic optimization problems which potentially extend to a wide class of the applications noted above. Conventional wisdom has long held that wealthy households in models of this type behave linearly, while all nonlinearity in savings behavior occurs at low wealth levels. This observation suggests that many grid points should be placed near the constraint on household borrowing, and relatively fewer far away from it. By combining the conditions of optimality of Gavrilović (1975) with the theory of Chipeniuk et al. (2016), we establish a clear asymptotic story for the accurate linear approximation of the savings behavior of wealthy households in our model. True to the conventional wisdom, we find that in the appropriate asymptotic sense the density of the Gavrilović grid is decreasing in household wealth when the households of interest are sufficiently wealthy, regardless of the model parameters. In particular, the asymptotic features of the grid are independent of the qualities of the risky income process. We then implement a modified version of the numerical approach of Gavrilović (1975) to investigate the properties of an approximate Gavrilović grid which includes poor and middle class households in the model. For these agents we find that the conventional approach to grid selection may fail to match Gavrilović’s optimum in a broad qualitative sense: the latter’s grid density need not be decreasing in household wealth. While the algorithm does indeed place many grid points in sections of the choice set where savings displays high curvature, the curvature itself is nondecreasing in wealth for poor agents. A theoretical investigation of the analogous static model reveals this non-monotonicity to be tied to the quality of risky household income as measured by its Sharpe ratio. Quantitatively, we find that approximations to the Gavrilović optimum improve accuracy of the interpolated savings functions noticeably relative to many standard constructions along several different metrics. While a naive implementation of the method increases computation time significantly to achieve these gains, several alternatives which lessen or eliminate this burden are investigated. In particular, a careful adaptation of the procedure of Gavrilović (1975) to the current setting reduces run time by two orders of magnitude. Moreover, in a model with a binding borrowing constraint, our theoretical results motivate an specification which provides accuracy comparable to Gavrilović grids with a negligible loss in computational efficiency relative to exogenous specifications. Lastly, a modification of Gavrilović (1975)’s algorithm allows us to solve each stage of the consumption savings problem to within a desired tolerance within a few seconds. The ability to pre-set the desired accuracy is not a feature of any of the other specifications considered. The paper proceeds as follows. Section 2 summarizes the content of Gavrilović (1975), focusing on the motivation and intuition behind the method. Section 3 introduces a dynamic stochastic optimization problem in which a finite-lived household faces independent shocks to its income each period. Section 4 shows that an accurate approximate solution to this problem for wealthy households has decreasing grid point density as wealth increases, and provides intuition for why this need not be the case for poor households. Section 5 confirms this intuition by numerically constructing a variety of approximate Gavrilović interpolants, and moreover investigates their accuracy and computational efficiency. Section 6 concludes.",2
56.0,4.0,Computational Economics,12 December 2019,https://link.springer.com/article/10.1007/s10614-019-09954-3,Portfolio Optimization in Incomplete Markets and Price Constraints Determined by Maximum Entropy in the Mean,December 2020,Argimiro Arratia,Henryk Gzyl,,Male,Male,Unknown,Male,"There are many ways of optimizing portfolios. One can safely say that all of them, except for the naive portfolio diversification, consist of some form of trade off between return and risk subject to an available capital constraint. One of the standard versions of the portfolio optimization problem, originally due to Markowitz (1952), has many variants, all of which use the volatility (or the standard deviation) of the portfolio as risk measure, and the trade off between risk and return is set up in a direct form, like for example: find the portfolio of minimum variance that yields a given expected return. This last constraint is clearly equivalent to a cost constraint upon the future values of the assets when a risk neutral probability is available. A few standard references for this matter are Pliska (1997), Karatzas (1998) and Föllmer and Schied (2016). Motivated by the impossibility of using non-arbitrage arguments when the market is incomplete, or when the class of risk neutral measures is not known, or more importantly, when the initial price for each asset can not be specified, an alternative theoretical methodology, which has been termed “conic portfolio theory”, was developed recently by Madan (2016) [see also Madan and Schoutens (2016)], and others. In this proposal, ideas coming from risk analysis in the insurance industry are used to provide the trade off between return and risk, by letting the role of the risk constraint to be played by either the bid or ask prices of the assets, which are estimated using a distortion function. Here we shall use distortion functions in a different, almost opposite way. For this we think of the price of each asset as the price of a collection of risks specified up to a bid-ask range, and then we determine a risk pricing measure that yields prices within the bid-ask ranges. The prices of assets so determined will be the current prices which are to be used to compute the returns of the portfolio. The numerical procedure we propose to obtain the distortion function is based on the method of maximum entropy in the mean. This is a convenient method to solve inverse problems subject to convex constraints upon the solution, which allows for the data to be specified in intervals. This procedure has the important advantage of being model free, which in particular does not call for the calibration of parameters. The historical antecedents of the maximum entropy in the mean method can be traced back to Jaynes (1957), who proposed and used a standard maximum entropy methodology in statistical physics. This form of the maximum entropy method is later used as a valuable approximation scheme to solutions of some cases of the problem of moments in Mead and Papanicolaou (1984), and the mathematical foundation of the maximum entropy in the mean method is developed in Dacunha-Castelle and Gamboa (1990). Here the maximum entropy in the mean method is used to assign a price to a market asset, which in a certain sense corresponds to the discounted expected value of the asset under some (unknown) risk-neutral measure, but what can only be said is that this price is within some market bid-ask range. We termed these prices, determined by the market, as conservative market prices, in concordance with similar terminology use in the literature [Madan (2016), Madan and Schoutens (2016)]. Once these conservative market prices for the assets comprising our portfolio are at hand, we can do two things. First, we can use the prices to compute returns, and second, to assign value to the price constraint for the portfolio optimization problem. Interrelationships between derivative valuation in incomplete markets and risk pricing in insurance have been brought up many times. When the market model is incomplete, it was noted that instead of a given price, one could assign a price range to any given asset. One could also use methods from utility (or risk) theory to determine a market price of the asset. In the insurance industry a similar problem exists: How to assign a price to a potential loss, or a price to risk. For this purpose, a risk pricing axiomatics and a risk pricing methodology were developed, which in many ways parallels and overlaps that developed to value the risk of financial assets. For a few references about this see Young (2006) and Laeven and Goovaerts (2008). We begin with briefly describing the market models in finance and in insurance, at the same time that we introduce the necessary notation in Sect. 2. There are two issues related to bid-ask ranges. On one hand, bid-ask ranges for assigning prices to derivatives are related to market incompleteness. When the market is incomplete but the class of risk neutral measures is known, a bid-ask range can be applied [but see Cerny and Hodges (2000) for a characterization in term of no good deal prices]. On the other hand, the existence of bid-ask ranges for current prices does not allow the construction of the class of risk neutral probabilities, which impedes risk neutral pricing. But equally important, since to solve the problem of portfolio optimization we need to be able to calculate asset returns, we have to device a way of going around the absence of current asset price. We briefly describe the way this is taken care of in Madan and Schoutens (2016) and Madan (2016). Since their proposal involves the use of distorted probabilities and their connection to coherent risk measures, we devote some space to coherent risk measures, risk pricing in insurance, and eventually establish a connection between distorted risk measures and risk neutral probabilities, that can be summarised as follows (see Sect. 2.6). We take the bid-ask range as the only available market data, and determine a distortion function that yields asset prices within the bid-ask range. Since the distorted probability law so obtained is equivalent to a risk neutral probability yielding the computed price as actual price, we shall consider that price as the base price against which we compute the returns of the basic assets for the purpose of portfolio optimization. In Sect. 3 we describe the numerical procedure to obtain the distortion function, show how the discretization is carried out, and write down the solution of the problem by the method of maximum entropy in the mean. It will turn out that there will be two possible solutions, depending on whether we do, or do not, impose bounds upon the derivatives of the distortion function. We present both results there, but we work out the details of this procedure in Appendix 6.1. We consider in Sect. 4 a portfolio optimization problem. For that we shall consider an exponential utility and make use of the connection between that problem and entropy maximization, in a way similar to that developed by Föllmer and Schied (2016). In Sect. 5 we sum it all up, and in the Appendices 6.1 and 6.2 we collect all material set aside for not to interrupt the main flow of ideas.",
56.0,4.0,Computational Economics,17 December 2019,https://link.springer.com/article/10.1007/s10614-019-09955-2,Degrees of Rationality in Agent-Based Retail Markets,December 2020,Georgios Methenitis,Michael Kaisers,Han La Poutré,Male,Male,,Mix,,
56.0,4.0,Computational Economics,17 December 2019,https://link.springer.com/article/10.1007/s10614-019-09956-1,Optimization of Backtesting Techniques in Automated High Frequency Trading Systems Using the d-Backtest PS Method,December 2020,D. Th. Vezeris,C. J. Schinas,I. P. Karkanis,Unknown,Unknown,Unknown,Unknown,,
56.0,4.0,Computational Economics,12 December 2019,https://link.springer.com/article/10.1007/s10614-019-09957-0,Liquidity Constraints for Portfolio Selection Based on Financial Volume,December 2020,Eduardo Bered Fernandes Vieira,Tiago Pascoal Filomena,,Male,Male,Unknown,Male,"Liquidity control is an essential issue for portfolio management and it increases its relevance in smaller markets which present lower trading volumes. In some of the world’s largest financial markets such as the USA, UK and Japan, the liquidity conditions are much more favourable to portfolio managers. The New York Stock Exchange, the Japan Exchange Group and the London Stock Exchange Group are among some of the most significant world’s exchanges. In April 2018, these exchanges presented a market capitalization of USD 23,139 billion, USD 6,288 billion and USD 4,596 billion and monthly trading volumes of USD 1,452 billion, USD 481 billion and USD 219 billion, respectively.Footnote 1 The difficulty to generate and liquidate the portfolio is considerably reduced by the high levels of volume traded. However, in markets with lower trading volumes, such as the B3 in Brazil, the Australian Securities Exchange in Australia and the JSE Limited in South Africa, the portfolio liquidity control becomes more important. In April 2018, these markets exhibited a market capitalization of USD 1,073 billion, USD 1,442 billion and USD 1,165 billion and monthly trading volumes of USD 62 billion, 56 USD billion and USD 29 billion, respectively. Compared to the most significant financial markets in the world, it is evident the greater portfolio liquidation difficulty in these markets. Therefore, the study of liquidity control can be justified, especially, for those who are in markets where the trading volume is smaller. Liquidity represents the ability of an economic agent to quickly transact an asset with low cost and limited effects on market prices. It is a critical factor for market efficiency and financial stability. Liquidity risk can be divided into two types: market liquidity risk, i.e., the possibility of the investment fund not liquidating its positions in a given market breakdown; and cash flow liquidity risk, i.e., a fund’s investment not having the capacity to honour its obligations or outflows (Acharya and Pedersen 2005). These risks are intensified when portfolio managers apply a high proportion of the portfolio to fewer liquid assets while providing immediate or daily liquidity to the investor. Traditional portfolio selection models such as the mean-variance (Markowitz 1952) or the minimum variance models (Best and Grauer 1992; DeMiguel and Nogales 2009), just to name a few, usually only consider the risk and return relationships in investment analysis. Moreover, assuming that the assets can be traded continuously in any quantity. Generally, these models do not consider the assets’ liquidity, which can cause adverse effects for the investor. During the decision-making process, not only risk and return but also the period that the investor is willing to remain in the position and the possibility of quickly selling the asset are considered relevant. The position of an investor in a particular asset is related to the number of assets owned and their price. However, depending on the quantity of assets owned, closing the position becomes extremely difficult at a given price. A small investor can probably sell all of his/her assets at the desired price, but a large investor will hardly have the same opportunity. A price reduction would be necessary to make it possible. Therefore, a large supply of assets leads to a drop in the asset price, reducing the return from its sale (Amihud et al. 2012). One of the main characteristics of liquidity is its multidimensionality, which makes it extremely hard to be measured directly. Thus, several metrics and approaches are proposed to capture more precisely the behaviour of the various determinants of asset liquidity. Generally, liquidity approximation metrics can be divided into two groups: price-based indexes such as the difference between the bid and ask prices, and volume-based indexes, such as turnover rate. Amihud’s illiquidity metric (Amihud and Mendelson 1986) is considered one of the main liquidity measures. The financial volume and the turnover rate are also worth mentioning among the main liquidity metrics (Gabrielsen et al. 2011). Several articles have studied the definition of liquidity and have proposed ways to capture its behaviour, for instance, Brennan et al. (1998), Chordia et al. (2001) and Gabrielsen et al. (2011). However, there are few examples of papers addressing the more practical problem of integrating liquidity directly on the portfolio selection process as Lo et al. (2006). Our paper’s primary aim is to insert liquidity directly in the portfolio selection process. The model’s construction follows some directions proposed by Lo et al. (2006), regarding the insertion of liquidity in the portfolio selection problem and choice of liquidity approximation metrics. However, our study differs from Lo et al. (2006) regarding the use financial volume as the liquidity measure which is (i) readily observable and obtained and (ii) more intuitive, especially, for portfolio managers or practitioners. Another difference in the present study is that it considers the portfolio monetary value directly in the imposed constraint. In Lo et al. (2006), portfolio liquidity is not a function of its monetary value. The present study considers the monetary value as an input parameter of the model, generating effects on the portfolio liquidity. The higher the value attributed to the portfolio, the harder the task to obtain the required liquidity. The objective of this work is to design liquidity constraints for portfolio selection models. The implemented constraints consider some parameters of liquidity control. Furthermore, it proposes liquidity constraints which can be applied in any standard portfolio investment models; it analyzes the liquidity of the portfolio as a whole, and not the individual liquidity of the participating assets; it considers the portfolio monetary value as a liquidity variable. In this article, three liquidity control parameters are proposed: the limit percentage of the total volume traded (\(\rho \)), the liquidation period (\(\gamma \)), and the acceptable liquidation level (\(\phi \)). The percentage limit of the total volume traded is the maximum percentage to be liquidated without an important impact on the asset price. The liquidation period is the maximum period for the value allocated on the asset to be liquidated. The level of acceptable liquidation represents a relaxation of the requirement of total liquidation of the position in such a way that complete liquidation might not be required within the considered term. Mathematical optimization models can be of various types. The simplest and easiest to implement are the linear ones. There are also models with non-linear, integer, stochastic, among other characteristics. The constraints proposed in this work have the advantage of being linear, which facilitates the implementation, based on easily collected and manipulated parameters and data. The insertion of the constraints proposed in the present study shows excellent results for the portfolios liquidation percentages. The results are consistent when compared to different portfolio selection models. The three tested emerging markets showed the model robustness. Comparing with portfolios formed in the absence of the constraint, a considerable increase in the liquidation is observed. When considering the out-of-sample performance, the percentage liquidated is very close to the designed in-sample acceptable level. As expected, the risk of liquidity-constrained portfolios is positively correlated with a higher demand for liquidity. In the next section, a review of the literature on liquidity is presented; in Sect. 3, the proposed constraints are discussed; Sect. 4 presents several applications of the proposed constraints.",2
57.0,1.0,Computational Economics,04 February 2021,https://link.springer.com/article/10.1007/s10614-021-10094-w,Machine Learning in Economics and Finance,January 2021,Periklis Gogas,Theophilos Papadimitriou,,Male,Unknown,Unknown,Male,,30
57.0,1.0,Computational Economics,28 July 2020,https://link.springer.com/article/10.1007/s10614-020-10019-z,Gold Against the Machine,January 2021,Vasilios Plakandaras,Periklis Gogas,Theophilos Papadimitriou,Male,Male,Unknown,Male,"The use of gold as an investment instrument dates back to 4000 B.C. when the Egyptians started trading gold (Green 2007). Gold as a mean of great intrinsic value plays a non-trivial role in the global economy. The prevalence of the stock market as the main financial vehicle of investment in the post Bretton Woods era did not reduce the relevant interest in gold. There is a consistently negative correlation of gold prices to the stock markets, so that especially after the 2008 global financial crisis, gold is being used as a safe-haven for risk averse investors or a heading financial instrument. The existing literature regarding gold price forecasting does not present a generally acceptable theoretical model or a robust empirical application. Moreover, there is no consensus regarding the variables that influence gold prices. The most popular candidate variables are commodity prices, exchange rates, stock returns, interest rates and macroeconomic data. The predominance of certain variables over others in forecasting gold returns is dependent on the period under examination (O’Connor et al. 2015). The only common ground among researchers is that gold price volatility is rather low. Starting from the relationship between gold prices and the stock market, Emmrich and McGroarty (2013) find that the poor gold returns in the 1980s and 1990s contrast with the high Sharpe ratios of the stock market over the same period. Bredin et al. (2015) find no causal linkage between the stock and the gold market for the U.S., the U.K and Germany. Baur and McDermott (2010) extend their analysis to an international sample and conclude that gold is ineffective in protecting from stock market crashes in Australia, Canada, Japan and the BRIC’s. Regarding exchange rates, Joy (2011) detects a positive correlation between gold prices and the U.S. dollar, while later Reboredo (2013) argues against the relationship between the U.S. dollar and gold prices, even in the tails of the distribution. Given the high correlation of gold with most U.S. dollar exchange rate pairs, gold seems to comove with the exchange rate markets. Ciner et al. (2013) detect positive conditional correlation of gold prices with the U.S. dollar and the U.K. pound. Moore (1990) attempts to forecast gold prices based on the U.S. inflation in order to create portfolios based on gold, stocks and bonds. Portfolios that include gold that is forecasted using U.S. inflation achieve lower returns than portfolios where gold is forecasted by including stocks and bonds. Reaching to different results, Levin et al. (2006) conclude that while on the short run macroeconomic variables have low forecasting ability of gold prices, on the long run inflation is the sole determinant of gold price volatility. Fortune (1987) builds a model where gold and nominal interest rates are related through the substitution channel. As he points out, increases in the expected interest rate should encourage gold owners to sell gold and buy interest bearing assets. This shift in the investing preferences would push gold prices down, suggesting a negative relationship between gold prices and interest rates. Silva (2014) finds no co-movement between gold prices and interest rates using 10 years of annual observations, while Baur (2011) supports the findings of Fortune (1987). This lack of consensus among researchers on the ability of individual variables to forecast gold prices has sparked a vast literature on the subject. Nevertheless, most studies are limited to in-sample forecasting (see among others Shafiee and Topal 2010; Baur et al. 2016; Malliaris and Malliaris 2015; Nguyen et al. 2019) despite the fact that the true forecasting ability of any model is only evaluated using out-of-sample or in other words “unknown” during the training period, data, to simulate actual trading conditions. Only few studies evaluate the statistical and economic perspective of gold forecasting in a true out-of-sample exercise. Aye et al. (2015) and Baur et al. (2016) evaluate the forecasting performance of various competing methodologies in out-of-sample forecasting based solely on statistical metrics, using the Random Walk (RW) model as a benchmark. Pierdzioch et al. (2014, 2016) focus on the true value of any model in forecasting gold returns for a practitioner; its ability to produce profitable trading strategies. The empirical findings suggest that the forecasting models used in the study produce small returns in excess of a risk-free asset. Risse (2019) is one of the very few studies that attempts to forecast gold returns based on a pool of variables and provide both statistical and economic comparisons, with the naïve historical mean used as a benchmark. In doing so, he builds various forecasting models based on several econometric and machine learning methodologies to forecast next month’s excess returns (monthly gold returns in excess of the 3-month treasury bill) over the period May 1973–December 2016. The training scheme starts with 220 observations and expands with the addition of one observation each time. The forecasting accuracy of all models is evaluated at the period January 1992–December 2016. In doing so, he extracts the available information encompassed in the regressors using a wavelet decomposition scheme. The components of the decomposition are fed to the forecasting models, with the best model being the wavelet-Support Vector Regression (SVR) model. His empirical findings suggest that the wavelet-SVR model outperforms all competing methodologies both statistically and economically. In this paper, we attempt to forecast gold prices based on a model that blends the Ensemble Empirical Mode Decomposition (EEMD) filtering technique and the SVR forecasting algorithm (Plakandaras et al. 2015a, b). The use of the EEMD technique as part of a hybrid forecasting methodology is common to the engineering literature but not to economics and finance. In our approach, we apply the EEMD decomposition to filter out the noise from our regressors, so that the independent variables adhere closer to the true deterministic underlying data generating process. These filtered variables, clean from any random noise, are then used to train our SVR models in-sample. Finally, we test the true forecasting accuracy in out-of-sample observations. The innovations with respect to previous studies are: a) we decompose the original time series of gold prices into a long run trend and a short-term noise component and filter out market noise that is irrelevant to gold price evolution. Our approach avoids the decomposition of the series into several components that often lack a theoretic and practical justification (see among others Risse 2019) to the very best of our knowledge, most previous studies are based on an “expanding” estimation scheme, where the model starts with an initial training sample and expands with the addition of new observations (see among others Rapach et al. 2016; Risse 2019). Instead, we choose to use a rolling window approach, that is able to better focus on the current dynamics and characteristics of the data. Our empirical findings suggest that our decomposition scheme improves the forecasting performance of all methods both in terms of statistical and economic performance. Section 2 describes the data and the methodology, Sect. 3 presents the empirical results, while Sect. 4 concludes.",7
57.0,1.0,Computational Economics,10 August 2020,https://link.springer.com/article/10.1007/s10614-020-10022-4,Forecasting Realized Volatility of Bitcoin: The Role of the Trade War,January 2021,Elie Bouri,Konstantinos Gkillas,Christian Pierdzioch,Male,Male,Male,Male,"In the wake of the recent United States–China trade war (hereafter, US–China trade war), and soaring Bitcoin prices, there have been claims made by both financial practitioners, covered in the financial press, and academics that these two facts are not necessarily independent, but rather can be considered as an indication of Bitcoin’s hedging ability. To give an example, in an recent interview in Fortune’s entitled “Balancing the Ledger”, the founder of the Digital Currency Group, Barry Silbert, claimed that Bitcoin behaves as an asset which seems to be independent from various uncertainties that exist in the traditional financial system. In other words, according to Barry Silbert, the digital currency can be considered as a “flight to safety”. Such behavior, however, is not new. Bitcoin acted as a “flight to safety” quite earlier, for example, during the begging of “Brexit” negotiations and at the peak of “Grexit” debates in Europe. Not surprisingly, using Google to search the terms “Bitcoin” and “Trade war” gave 14,000,000 results (Google.com accessed on December 20, 2019). In this vein, as the trade war intensified, several market watchers thought that indeed Bitcoin benefited from jitters in international financial markets, which, in turn, gave rise to a downward pressure on stocks and China’s currency. The claims of the hedging and “flight to safety” property of Bitcoin mentioned in the preceding paragraph are based on anecdotal evidence, while academic research on this matter is relatively scarce. Formal empirical evidence of Bitcoin acting as a hedge against trade-related uncertainties can be found in the recent works by Gozgor et al. (2019) and Bouri et al. (2020). The former researchers, based on a wavelet analysis, claim that, with some exceptions, there is a positive correlation between Bitcoin returns and a newspaper-based measure of trade policy uncertainty. Bouri et al. (2020), in turn, report that the (realized) correlation between US equities and Bitcoin returns is negatively affected by the same measure of trade uncertainty. Taking into account the claims discussed above along with the general importance of volatility for risk management and portfolio choice, a relevant question to ask is: “How has the US–China trade war impacted the volatility of Bitcoin returns?”. And, in particular: “Does uncertainty caused by the US–China trade add incremental forecasting value useful for forecasting the volatility of Bitcoin returns?”. Volatility is a widely used measure of risk and, therefore, its accurate modeling and forecasting play a key role in investment decisions and portfolio choice (Poon and Granger 2003). In light of this, as pointed out by Baur and Dimfl (2018) for the cryptocurrency market, positive returns are closely associated with higher levels of volatility. One explanation of this returns-volatility nexus points to the influence of uninformed investors’ herding, because such investors buy due to their fear of missing out on rising cryptocurrency valuations and pump and dump (known also as P&D) scams. In other words, one can argue that increases in Bitcoin returns associated with the US–China trade war, and uncertainty in general (see e.g., Bouri et al. 2017, 2018; Fang et al. 2019; Aysan et al. 2019; Bouri and Gupta 2019; Wu et al. 2019), are likely to result in heightened volatility. However, Baur and Dimfl (2018) find this evidence to be (statistically) weak for Bitcoin and Ethereum among 20 cryptocurrencies considered, which can be interpreted to indicate that these two cryptocurrencies are possibly dominated by informed investors. Against this backdrop, we study the role of the US–China trade war in forecasting out-of-sample daily (realized) volatility of Bitcoin returns. To this end, we use intraday Bitcoin returns as measured in 60 min-intervals (hourly basis) covering the period from 1st July 2017 to 30th June 2019. We focus on realized volatility estimated by non-parametric techniques, which provides an accurate estimator of volatility on the basis of the actual variance if intraday returns. In particular, any non-parametric estimator is based on quadratic variation, which, in turn, is considered to be the best estimator of (latent) volatility.Footnote 1 Moreover, usage of the realized version of daily volatility, which is basically defined as the sum of non-overlapping squared intraday returns for a given interval (e.g., within a day), transforms volatility into an observable process. According to McAleer and Medeiros (2008), intraday data contain rich information about market conditions (such as the microstructure of the market), producing more accurate estimates of daily realized volatility. As for the econometric framework concerned in this research for forecasting realized volatility, we make use of the heterogeneous autoregressive realized-volatility model (HAR-RV). This model, as proposed by Corsi (2009), allows stylized facts (such as multi-scaling behavior and long-memory, as detected for Bitcoin by Bouri et al. 2019a and Takaishi 2018) of the volatility to be captured in a straightforward and simple way. We also control for the discontinuities, known as jumps, given that jumps are well-known to improve the overall fit of realized volatility models (see, e.g., Andersen et al. 2007, Corsi et al. 2010, Neuberger and Payne 2018, Gkillas et al. 2020), and observed for cryptocurrencies (Bouri et al. 2019b). In order to capture left-tail events far away from the mean, following Amaya et al. (2015), we use realized skewness and realized kurtosis (see also Mei et al. 2017, and Gkillas et al. 2019 in particular in relation to the Bitcoin market). Importantly, we include in the HAR-RV model our primary forecasting variable of interest, namely a metric of the US–China trade tension, for which we rely on Google Trends (as has been done for returns by studies such as Kristoufek 2013; Panagiotidis et al. 2018, 2019; Nasir et al. 2019, Subramaniam and Chakraborty 2019) for search-terms like: “US–China Trade War” (primarily), “US–China Tariffs”, “Tariffs War”, “Tariffs War US–China”, “Trump Trade War”, and “New Trade War Tariffs”, and then consider a composite of these terms based on principal component analysis. As an alternative measure, we also consider the news-based measure of US trade policy uncertainty as developed by Caldara et al. (2019a). In terms of estimation strategy, we rely on a machine-learning technique that is known in the statistical-learning literature as random forests. Recent applications of this technique in the empirical finance literature include Gupta et al. (2019), Demirer et al. (2019), and Pierdzioch and Risse (forthcoming), among others. As compared to the ordinary-least squares technique (OLS) commonly applied in earlier literature to estimate the HAR-RV model and its various extensions, random forests have several advantages (for a discussion of random forests and a comparison with other machine-learning techniques, see Hastie et al. 2009). One advantage is that random forests can be interpreted as a data-driven modeling environment that renders it possible to study, in a unified framework, the forecasting ability of a large number of forecasting variables for realized volatility. Such a modeling environment is ideally suited for our investigation (which concerns the incremental forecasting value of trade-related uncertainty for realized volatility) due to the fact that we control for the impact of various measures of jumps (i.e., upside/downside, asymmetric, truncated large/small jumps), higher-order moments (i.e., realized skewness and realized kurtosis), returns, and a leverage effect in addition to the usual components of the benchmark HAR-RV model. Another advantage of random forests is that this modeling environment makes it not only possible to capture in a data-driven way the interdependencies between the forecasting variables but also, at the same time, renders it possible to account for potentially nonlinear links between realized volatility and its forecasting variables. This advantage is particularly important in our study given the extremely volatile behavior of cryptocurrency returns and their nonlinear dynamics (Gkillas and Longin 2018). Finally, random forests, by their very construction, guarantee that forecasts of realized volatility, even when we study relatively complex models that include simultaneously several forecasting variables, are always nonnegative, a feature not shared by the ordinary-least squares technique. Finally, it must be noted that a large literature has emerged that has aimed to forecast (in- and out-of sample) daily price movements in the volatility of cryptocurrencies and in particular Bitcoin, based on same-frequency or mixed-frequency variants of the popular Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model, using various types of financial and macroeconomic variables as forecasting variables (see, e.g., Chu et al. 2017; Conrad et al. 2018; Ardia et al. 2019, and Walther et al. 2019, for detailed reviews of this literature). However, our study is the first study that uses random forests to analyze the role of US–China trade war in forecasting Bitcoin’s daily realized volatility by accounting for jumps, realized skewness and realized kurtosis, all constructed from intraday data. In sum, we use random forests to estimate HAR-RV models featuring several control variables and measures of trade-related uncertainty. We call the resulting variant of the standard HAR-RV model the HAR-RV-RF model. We then use several specifications of the HAR-RV-RF model to study the out-of-sample forecasting power of trade-related uncertainty at different forecast horizons. Our main empirical result is that trade uncertainty, when being measured using data from Google Trends, has forecasting power out-of-sample for realized volatility of the returns of Bitcoin and other cryptocurrencies. In order to derive our main empirical result, we organize the remainder of this paper as follows. Section 2 presents the empirical methods used. Section 3 describes the data. Section 4 summarizes the empirical results. Section 5 concludes.",23
57.0,1.0,Computational Economics,28 July 2020,https://link.springer.com/article/10.1007/s10614-020-10032-2,Support Vector Machine Algorithms: An Application to Ship Price Forecasting,January 2021,Theodore Syriopoulos,Michael Tsatsaronis,Ioannis Karamanos,Male,Male,Male,Male,"This study proposes a modern, challenging, and innovative theoretical and empirical framework to financial forecasting, taking advantage of novel developments in the dynamic field of artificial intelligence and machine learning algorithms. The support vector machine (SVM) methodology, a competitive supervised learning mathematical algorithm, is incorporated to specify, test, and validate the generation of robust and reliable global ship price predictions for different (bulker–tanker) vessel types and shipping markets. Broadly, machine learning models incorporate mathematical algorithms to learn from the properties of underlying sample datasets, supporting data analysis and identifying specific prevailing patterns with minimal human intervention. To this end, SVMs can be incorporated initially for classification purposes, typically referred to as Support Vector Classification (SVC) models. SVMs can be further incorporated for regression specification and estimation purposes, including time-series analysis and critical factor impact analysis, referred to as Support Vector Regression (SVR) models. SVRs are linear learning machine models, based on a linear function to solve regression problems, and offer the flexibility of determining an error range the model is to accept and estimate a fitting line (or hyperplane in higher dimensions) to match the underlying dataset. This comes in contrast to typical linear regression models, targeting minimization of the sum of squared errors. A critical SVR advantage is that this setting can solve a convex minimization problem, estimating a unique global minimum, thus avoiding local minima that are estimated by other machine learning techniques and standard forecasting approaches. For the purposes of this study, this practically supports the notion that the estimation of the ‘global optimum’ can be incorporated as a benchmark reference for long-term financial decisions. Early research on SVMs originates from the mathematical computing field (pioneered by Vapnik 1992; Vapnik 1995, 1999; Cortes and Vapnik 1995; inter alia). Recent growing literature pays attention to reliable predictions of economic and financial time-series generated by modern machine learning approaches (Hardle et al. 2009; Khandani et al. 2010; Ogut et al. 2012; Papadimitriou et al. 2014; Plakandaras et al. 2014; Rubio et al. 2011; inter alia). The novel forecasting approach proposed herewith bears challenging and useful merits, in support of shipping investment, financing and risk managerial decisions, as it attempts to produce robust and reliable ship vessel price predictions, a critical input to shipping business. The ship vessels are core operational ‘real’ assets of substantial commercial value, vital for the efficient world trade flow transportation, as they regularly connect international production and consumption centers, via short- and long-haul sea-routes. The boosting impact of global shipping to world economic growth is well documented (Stopford 2009; Grammenos 2010; UNCTAD 2019). In a highly volatile international environment of critical risk complexities, induced by economic shocks, financial crises, trade wars, rigid market disequilibria, cyclicality and seasonality, shipping managers require reliable planning and forecasting tools to deploy competitive ship investment projects of high capital-intensity, design alternative funding sources under efficient capital cost structures and contain diverse risk exposures of potentially detrimental business impact. The market value of a shipping company is formed by the interrelated value of its assets and their potential to generate dynamic earnings growth. Vessel prices, however, can fluctuate dramatically, affected by shipping demand–supply imbalances, freight revenue volatility, and global economic conditions, inter alia. Reliable ship price forecasts should be a valuable input to efficient investment, financing, operating, and risk management decisions by shipowners, charterers, financiers, investors, brokers, and shipyards. The study attempts to contribute a set of challenging and innovative findings to global ship price forecasting by incorporating modern machine learning approaches, for the first time to the authors’ best knowledge. Past research indicates that vessel prices exhibit a highly volatile behavior and, in spite of potential profitable asset-play opportunities, this can induce material business risks as well (indicatively, Kavussanos 1996a, b, 1997: Syriopoulos and Roumpis 2006; Alizadeh and Nomikos 2007; Syriopoulos 2007, 2010; Syriopoulos and Bakos 2019; inter alia). Broadly, vessel market value shifts are driven by shipping demand and supply fundamentals. In practice though, vessel market prices are based on the latest trade price evaluation of similar vessels (Alizadeh and Nomikos 2010). Shipowners can decide on newbuilding or secondhand vessel acquisitions, targeting fleet capacity expansion during booming shipping markets or disinvest (liquidate/sell/demolish vessels) during downward and recessionary markets. Unstable newbuilding and secondhand vessel prices have been argued to reveal potentially conflicting market signals and to affect shipping management decisions dramatically (Stopford 2009). Modeling newbuilding and secondhand ship price dynamics and generating reliable ship price forecasts can contribute to more rational market-timely shipping management decisions. Motivated by past research on ship price dynamics (Kavussanos 1996a, b; Syriopoulos and Roumpis 2006; Adland and Strandenes 2006; Adland et al. 2006; Merikas et al. 2008; Alizadeh and Nomikos 2010; Syriopoulos 2007, 2010; Kagkarakis et al. 2016; inter alia), this empirical study attempts to shed light on ship price behavioral patterns and trends from a different perspective. Attention is paid to model, assess, test, and evaluate alternative ship price forecasting approaches over a diverse set of vessel types, market segments and time-horizons, based on updated machine learning models. Recent developments in this field, indicate that SVM approaches can offer a promising setting to financial time-series predictions. Thus far, SVM models have been employed predominantly in stock market analysis, outperforming conventional econometric forecasting models, and contributing promising empirical results. Standard forecasting approaches, such as the Autoregressive Integrated Moving Average (ARIMA) models, are seen to be impaired by certain limitations. This predominantly relates to the critical concern that the predicted values are based on past values, on the assumption of a linear relationship. Though this may be practical and helpful in certain aspects, it still may not generate robust, accurate and reliable predictions of the underlying variables, particularly for financial datasets, widely argued to exhibit nonlinearities. To this end, the SVR framework offers a step forward departing from standard norms, as it acknowledges the presence of nonlinearities in the datasets at hand. Furthermore, since SVR uses the same classification algorithm as SVM to generate prediction values (instead of a class), SVR can provide a reliable and competent prediction model alternative option. To sum up, this study proposes and investigates an innovative approach to predict newbuilding ship prices, taking advantage of recent developments in the SVM field. It specifies and estimates an SVR model to test its efficiency, robustness, and predictive power on ship price forecasts. This novel approach is compared against a set of standard time-series forecast models of the ARIMA family, as developed along the established Box and Jenkins (1970) framework. Model performance is evaluated for both ‘training’ and ‘testing’ sample subsets, based on the Root-Mean-Square-Error (RMSE) and Mean-Absolute-Percentage-Error (MAPE) criteria, widely employed statistical metrics to time-series forecast quality evaluation. The empirical analysis is calibrated with rich sample datasets on different vessel types and markets, that is, handysize, handymax, panamax and capesize bulkers (dry bulk segment), as well as handysize, panamax, aframax, suezmax and VLCC (Very Large Crude Carrier) tankers (tanker segment). The study time-horizon runs from 1976 M01 (January) to 2018 M06 (June) on monthly frequencies. The experimental findings indicate that the SVR model outperforms alternative conventional forecast models, such as the ARIMA model, and render support to the SVM approach as a promising setting for ship price forecasts, useful to shipping market practitioners. As mentioned, the application of SVR models in ship price predictions is undertaken for the first time herewith. The study contributes a range of innovative and challenging ideas and empirical findings that can lead to efficient management in global shipping business. The structure of the paper develops as follows. Section 2 reviews briefly past empirical research on ship price formation, dynamics, and predictions. Section 3 develops and proposes an alternative empirical forecast methodology, implementing the SVR model as an innovative, and effective approach to ship price predictions. Prior to that, the sample datasets and their statistical properties are discussed for different vessel types and shipping market segments. Section 4 critically analyses and assesses the empirical findings generated by the SVR model and statistically evaluates the forecasts generated. These are compared against alternative forecasts produced by ARIMA model specifications, based on statistical metrics convenient for the quality evaluation of time-series forecast. Finally, Sect. 5 concludes.",22
57.0,1.0,Computational Economics,02 November 2020,https://link.springer.com/article/10.1007/s10614-020-10067-5,Monitoring Liquidity Management of Banks With Recurrent Neural Networks,January 2021,Ron Triepels,Hennie Daniels,Ron Berndsen,Male,Female,Male,Mix,,
57.0,1.0,Computational Economics,17 August 2020,https://link.springer.com/article/10.1007/s10614-020-10038-w,Modelling Stock Markets by Multi-agent Reinforcement Learning,January 2021,Johann Lussange,Ivan Lazarevich,Boris Gutkin,Male,Male,Male,Male,"General problem Due to the steadily increasing role played by financial stock markets in global economics, and the rise of their societal impact at many levels, a main topic of economic research has historically been that of stock market stability. At the heart of such market stability studies lie the processes pertaining to price formation (Dodonova and Khoroshilov 2018; Naik et al. 2018). However, research efforts in this direction have historically been hindered by the structural difficulties inherent to the bottom-up approach to system complexity inference. Yet bottom-up models could open a window on the heart of the price formation, help quantify the learning processes of financial market operators, and hence yield a wealth of information on the underlying nature of stock markets. Past research From a broader perspective, economic research has explored various types of quantitative models for its statistical inference of stock market data, among which, two general classes of models stand out as particularly prevalent. The first and most encountered ones are autoregressive time-series models aimed at predicting future values from past history (Greene 2017). The second class, MAS (Wellman and Way 2017) (agent-based models and related methods such as order book models (Huang et al. 2015; Biondo 2018a, b; Sirignano and Cont 2019), and dynamic stochastic general equilibrium models (Sbordone et al. 2010)), rather address the causal sources of financial markets activity, and thus take a bottom-up approach to market complexity. The MAS can be applied to both high and low-frequency trading (Way and Wellman 2013; Aloud 2014) as well as to the study of supply and demand (Benzaquen and Bouchaud 2018) in the form of game theory (Erev and Roth 2014) and the so-called minority game (Martino and Marsili 2006). From a regulatory point of view, the MAS approach has an ever-increasing role to play (Boero et al. 2015), as it may be applied to model specific macroeconomics phenomena (Gualdi et al. 2015). New trends Recent technological advances have potentially given MAS approaches in finance a new level of realism, which to our knowledge has received only a limited attention. These trends emerge from the association of two present-day major scientific breakthroughs: the steady advances of cognitive neuroscience and neuroeconomics (Eickhoff et al. 2018; Konovalov and Krajbich 2016), and the progress in reinforcement learning theory due to a resurgence of machine learning methods and especially multi-agent learning (Silver et al. 2018a, b). This has been accompanied on both ends with the emergence of reinforcement learning algorithms incorporating decision-theoretic features from neuroeconomics (Lefebvre et al. 2017; Palminteri et al. 2015), and neuroscience models approached from the angle of reinforcement learning (Duncan et al. 2018; Momennejad et al. 2017). These developments offer a way to go beyond the former generation of MAS with zero-intelligence agents (Gode and Sunder 1993), and their potential financial applications (Ganesh et al. 2019; Hu and Lin 2019; Neuneier 1997; Deng et al. 2017) have started to be extended to the class of order book models, coupled with reinforcement learning (Spooner et al. 2018). Our contribution In order to explore and exploit the technological impact of these breakthroughs, we have developed a next generation stock market simulator based on a MAS architecture, where each agent represents an economic investor trading via a centralised order book. In this model, the simulated agents have three novel features: (1) each agent learns to both forecast and trade by independent reinforcement learning algorithms in a fully autonomous way; (2) each agent learns a pricing strategy for this forecasting and trading that is more or less chartist (i.e relying on market price) or fundamental (i.e relying on intrinsic economical value); (3) each agent can be endowed with certain traits of behaviour, cognition, and learning gleaned from behavioural economics. The three key aspects can be readily implemented thanks to the reinforcement learning framework and its direct correspondence with decision theory. These features provide a whole new level of realism in simulated data and its emulation of real stock markets data. A basic aspect of the validation of such a model is to emulate market stylised facts. Structure In this Paper, we first make a brief literature review (Sect. 2), where we define what are these stylised facts (Sect. 2.1), and recall some basic concepts of reinforcement learning (Sect. 2.2), as the core method we use to model financial stock markets. We proceed to describe in Sect. 3 the architecture of our MAS model in depth (Sect. 3.1) and its calibration process to real stock market data (Sect. 3.2). Finally we display some of its performance results in stock market emulation (Sect. 4), and end with a sum up of its key aspects (Sect. 5).",19
57.0,1.0,Computational Economics,28 August 2020,https://link.springer.com/article/10.1007/s10614-020-10039-9,Time-Varying Dictionary and the Predictive Power of FED Minutes,January 2021,Luiz Renato Lima,Lucas Lúcio Godeiro,Mohammed Mohsin,Unknown,Male,Male,Male,"According to Gentzkow et al. (2017), the information encoded in text is a rich complement to the more structured kinds of data traditionally used in empirical research. Indeed, in recent years, we have seen an intense use of textual data in different areas of research. The idea consist of transforming strings into numeric variables, and then use it as predictors in different models. Several studies have already explored this additional source of information. In the finance literature, Garcia (2013) studies the effect of sentiment on asset prices during the twentieth century (1905 to 2005). The author uses texts from the New York Times to construct sentiment variables using textual analysis. He concludes that the predictability of stock returns using news content is particularly strong during recessions. Engelberg and Parsons (2011) compare the behavior of investors with access to different media coverage of the same information event. They focus on the relation between local media coverage and local stock portfolio trading volume and find that local media coverage is a strong predictor of local trading returns. In the economics literature, Dossani (2018) analyzes how the tone of the Central Bank press conferences impacts risk premia in the currency market. He measures the tone as the difference between the number of hawkish and dovish phrases made during a press conference. He used four currency future contracts traded on the Chicago Mercantile Exchange (CME) and found that implied risk aversion increases when Central Banks are hawkish and decreases when Central Banks are dovish. Other examples where textual data is used for macroeconomic analysis include Armesto et al. (2009), Boukus and Rosenberg (2006), Cecchetti et al. (2003), among others.Footnote 1 A common assumption among these studies is that the content of the dictionary (set of words) used to construct sentiment/information indexes is constant over time and subjectively chosen by the user. In practice, this implies that, given a fixed dictionary, we compute the frequency that each word (or combination of words) appears on texts and use this information to construct predictors (sentiment/information indexes) that can be used for forecasting or impulse-response analysis. The assumption of time-invariant dictionary is particularly difficult to hold in documents that introduce new words over time or if the vocabulary used in periods of recession differs from the one used in periods of economic expansions. Even if the vocabulary were the same across time, the predictive power of some words can vary, but the existing literature does not account for such an effect, and therefore the resulting predictors do not reflect the most predictive textual information encountered in documents at a given time. In this paper, we allow the content of dictionaries to vary over time, making it entirely determined by the predictive power of its words. The goal is, therefore, to maximize the predictive power of the dictionary, making it totally adapted to the problem of forecasting. Our methodology can be summarized by three steps: in the first step, we transform words into numerical values (time series) without using a pre-specified (fixed) dictionary. This numerical representation is high dimensional and sparse, so dimensionality reduction must be employed in the next step; in the second step, we use supervised machine learning (SML) to select the most predictive time series (words) and use them to construct new predictor(s). For instance, the new predictors could be common factors of the selected (most predictive) words or can be an index of sentiment based on the selected (most predictive) words. Finally, in the third step, we use the new predictor(s) to make out-of-sample forecasts of our target variable. This 3-step procedure is repeated recursively towards the end of the sample (recursive out-of-sample forecasting), implying that the content of the most predictive dictionary changes over time. Our paper is related, in motivation, to recent works by Thorsrud (2018) and Hansen et al. (2017) who used Latent Dirichlet Allocation (LDA) to address limitations imposed by pre-specified dictionaries. The main difference between our approach and the one based on LDA is that the latter is an unsupervised machine learning (UML) technique whereas our approach relies on supervised machine learning. As explained in Chakraborty and Joseph (2017), the difference between the two approaches is the existence of a target variable y. Supervised learning is the classical case of modelling y using inputs (predictors) x in that we choose a learning algorithm that will fit the target using the given input features. In the unsupervised learning, there is no target y and the algorithms merely aim to find structure in the data, for example by grouping observations, or grouping words in topics as done by the LDA technique. Recursive estimation is a key component to generate time-varying dictionaries. Indeed, our methodology consists of choosing recursively a set of words that minimizes a strictly convex loss function, implying that only the most predictive words will be included in the dictionary at each time. As shown by Thorsrud (2018), the LDA approach is very compute-intensive and updating recursively the training sample to re-estimate the topic model is unfeasible.Footnote 2 Our methodology, on the other hand, allows for low-cost recursive updating of the training sample, making re-estimation of the forecasting model computationally feasible.Footnote 3 More recently, Kalamara et al. (2020) proposed using supervised machine learning to select words from a large dictionary. Specifically, instead of considering a fixed dictionary with few words, they combined many fixed dictionaries to create a large dictionary with more than nine thousand words,Footnote 4 and then recursively used supervised machine learning to select the most predictive words from their super large - but still fixed - dictionary. Our paper, however, does not assume any fixed dictionary (large or small) in the first step. The idea is to let the data speak for themselves by indicating which words are more important to predict output growth over time. We use FED minute based predictors to forecast real time output growth.Footnote 5 Central banks rely on output forecasts to make decisions about changes in the monetary policies whereas the private sector uses output growth forecasts to make decision about investment, marketing and risk management. A large pool of output growth forecasting models has been suggested recently by the literature, which includes simple linear autoregressive (AR) models as well as judgmental forecasts and more sophisticated methods based on the dynamic stochastic general equilibrium (DSGE) model. To be clear, the main methodological contribution of this paper is not proposing an ultimate forecasting model for output growth but rather combining new developments from text analysis and supervised machine learning in a manner that is novel for identifying the most predictive information from FED minutes. Our results indicate that the textual data found on FED minutes contains non-trivial predictive power but fixed dictionaries are not the best approach to identify such an information. If we take the minutes by section, then it is shown that the information contained in the policy action section is more predictive than the one contained in the policy discussion one. However, taking the minute as a whole leads to better forecasts of output growth. This result suggests that words from both sections are important to predict output growth. We also show that forecasts obtained by using the most predictive information from FED minutes can “pos-cast” the Greenbook forecast, which is normally released with a 5-year delay. This is specially true for short horizons whereas the FED minute based forecasts tend to outperform the greenbook forecast at longer horizons, suggesting that the information generated during FOMC meetings does contain non-trivial predictive power. The choice of the SML (shrinkage) method also matters. We show that time-varying dictionaries that rely on elastic net to select the most predictive words have more predictive power than the ones that rely on LASSO or ridge regression. An explanation to this result is that ridge regression does not perform model selection (it does not shrink coefficients to zero) and, unlike LASSO, the number of predictors (words) selected by elastic net is not bounded by the sample size, T, implying that all relevant conditioning information will be considered for forecasting of output growth. Moreover, elastic net is also robust to group effects, which is particularly important to our analysis because many words appearing on the FED minutes, such as inflation and unemployment, are highly correlated.Footnote 6 Thus, elastic net not only selects the most predictive words, but also guarantees that equally predictive words will not be randomly thrown out just because they are highly correlated with other predictive words. In the forecasting literature, a short list of papers that successfully employed elastic net includes Bai and Ng (2008), Li et al. (2015), Li (2015) and Lima et al. (2018). Our results suggest that macroeconomic prediction could be improved through the combination of textual data and supervised machine learning in a way that only the most predictive words would be considered for forecasting. In an attempt to disseminate the methodology and render the results reproducible, we have released the R package TextForecast (Lima and Godeiro 2019),which implements all of the steps described in this paper in the R statistical language. We hope that this paper and the R package will encourage practitioners to use and test our time-varying dictionary approach for optimizing the use of textual data for forecasting their variable(s) of interest. The remainder of this paper is organized as follows. Section 2 presents the proposed methodology to identify the most predictive information in the FED minutes. Section 3 introduces our empirical analysis, including a full description of the dataset, the forecasting models and the methods used to evaluate the out-of-sample forecasts. Section 4 presents the main results followed by concluding remarks in Sect. 5.",2
57.0,1.0,Computational Economics,19 August 2020,https://link.springer.com/article/10.1007/s10614-020-10040-2,Unemployment Rate Forecasting: A Hybrid Approach,January 2021,Tanujit Chakraborty,Ashis Kumar Chakraborty,Shramana Bhattacharya,Unknown,Unknown,Unknown,Unknown,,
57.0,1.0,Computational Economics,25 September 2020,https://link.springer.com/article/10.1007/s10614-020-10042-0,Explainable Machine Learning in Credit Risk Management,January 2021,Niklas Bussmann,Paolo Giudici,Jochen Papenbrock,Male,Male,Male,Male,"Black box Artificial Intelligence (AI) is not suitable in regulated financial services. To overcome this problem, Explainable AI models, which provide details or reasons to make the functioning of AI clear or easy to understand, are necessary. To develop such models, we first need to understand what “Explainable” means. Recently, some important insitutional definitions have been provided. For example, Bracke et al. (2019) states that “Explanations can answer different kinds of questions about a model's operation depending on the stakeholder they are addressed to and Croxson et al. (2019)” ‘interpretability’ will be the focus will be the focus—generally taken to mean that an interested stakeholder can comprehend the main drivers of a model-driven decision"". Explainability means that an interested stakeholder can comprehend the main drivers of a model-driven decision; FSB (2017) suggests that “lack of interpretability and auditability of AI and Machine Learning (ML) methods could become a macro-level risk”; Croxson et al. (2019) establishes that “in some cases, the law itself may dictate a degree of explainability.” The European GDPR EU (2016) regulation states that “the existence of automated decision-making should carry meaningful information about the logic involved, as well as the significance and the envisaged consequences of such processing for the data subject.” Under the GDPR regulation, the data subject is therefore, under certain circumstances, entitled to receive meaningful information about the logic of automated decision-making. Finally, the European Commission High-Level Expert Group on AI presented the Ethics Guidelines for Trustworthy Artificial Intelligence in April 2019. Such guidelines put forward a set of seven key requirements that AI systems should meet in order to be deemed trustworthy. Among them three relate to the concept of “eXplainable Artificial Intelligence (XAI)” , and are the following. Human agency and oversight: decisions must be informed, and there must be a human-in-the-loop oversight. Transparency: AI systems and their decisions should be explained in a manner adapted to the concerned stakeholder. Humans need to be aware that they are interacting with an AI system. Accountability: AI systems should develop mechanisms for responsibility and accountability, auditability, assessment of algorithms, data and design processes. Following the need to explain AI models, stated by legislators and regulators of different countries, many established and startup companies have started to embrace Explainable AI models. In addition, more and more people are searching information about what “Explainable Artificial Intelligence” means. In this respect, Fig. 1 represents the evolution of Google searches for explainable AI related terms. From a mathematical viewpoint, it is well known that “simple” statistical learning models, such as linear and logistic regression models, provide a high interpretability but, possibly, a limited predictive accuracy. On the other hand, “complex” machine learning models, such as neural networks and tree models, provide a high predictive accuracy at the expense of a limited interpretability. To solve this trade-off, we propose to boost machine learning models, that are highly accurate, with a novel methodology, that can explain their predictive output. Our proposed methodology acts in the post processing phase of the analysis, rather than in the preprocessing part. It is agnostic (technologically neutral) as it is applied to the predictive output, regardless of which model generated it: a linear regression, a classification tree or a neural network model. The machine learning procedure proposed in the paper processes the outcomes of any other arbitrary machine learning model. It provides more insight, control and transparency to a trained, potentially black box machine learning model. It utilises a model-agnostic method aiming at identifying the decision-making criteria of an AI system in the form of variable importance (individual input variable contributions). A key concept of our model is the Shapley value decomposition of a model, a pay-off concept from cooperative game theory. To the best of our knowledge this is the only explainable AI approach rooted in an economic foundation. It offers a breakdown of variable contributions so that every data point (e.g. a credit or loan customer in a portfolio) is not only represented by input features (the input of the machine learning model) but also by variable contributions to the prediction of the trained machine learning model. More precisely, our proposed methodology is based on the combination of network analysis with Shapley values [see Lundberg and Lee (2017), Joseph (2019), and references therein]. Shapley values were originally introduced by Shapley (1953) as a solution concept in cooperative game theory. They correspond to the average of the marginal contributions of the players associated with all their possible orders. The advantage of Shapley values, over alternative XAI models, is that they can be exploited to measure the contribution of each explanatory variable for each point prediction of a machine learning model, regardless of the underlying model itself [see, e.g. Lundberg and Lee (2017)]. In other words, Shapley based XAI models combine generality of application (they are model agnostic) with the personalisation of their results (they can explain any single point prediction). Our original contribution is to improve Shapley values, improving the interpretation of the predictive output of a machine learning model by means of correlation network models. To exemplify our proposal, we consider one area of the financial industry in which Artificial Intelligence methods are increasingly being applied: credit risk management [see for instance the review by Giudici (2018)]. Correlation networks, also known as similarity networks, have been introduced by Mantegna and Stanley (1999) to show how time series of asset prices can be clustered in groups on the basis of their correlation matrix. Correlation patterns between companies can similarly be extracted from cross-sectional features, based on balance sheet data, and they can be used in credit risk modelling. To account for such similarities we can rely on centrality measures, following Giudici et al. (2019) , who have shown that the inclusion of centrality measures in credit scoring models does improve their predictive utility. Here we propose a different use of similarity networks. Instead of applying network models in a pre-processing phase, as in Giudici et al. (2019) , who extract from them additional features to be included in a statistical learning model, we use them in a post-processing phase, to interpret the predictive output from a highly performing machine learning model. In this way we achieve both predictive accuracy and explainability. We apply our proposed method to predict the credit risk of a large sample of small and medium enterprises. The obtained empirical evidence shows that, while improving the predictive accuracy with respect to a standard logistic regression model, we improve, the interpretability (explainability) of the results. The rest of the paper is organized as follows: Sect. 2 introduces the proposed methodology. Section 3 shows the results of the analysis in the credit risk context. Section 4 concludes and presents possible future research developments.",81
57.0,1.0,Computational Economics,24 September 2020,https://link.springer.com/article/10.1007/s10614-020-10047-9,"Should Deep Learning Models be in High Demand, or Should They Simply be a Very Hot Topic? A Comprehensive Study for Exchange Rate Forecasting",January 2021,Firat Melih Yilmaz,Ozer Arabaci,,Unknown,Unknown,Unknown,Unknown,,
57.0,1.0,Computational Economics,08 October 2020,https://link.springer.com/article/10.1007/s10614-020-10054-w,Forecasting of Real GDP Growth Using Machine Learning Models: Gradient Boosting and Random Forest Approach,January 2021,Jaehyun Yoon,,,Unknown,Unknown,Unknown,Unknown,,
57.0,1.0,Computational Economics,09 November 2020,https://link.springer.com/article/10.1007/s10614-020-10057-7,The Determinants of Bitcoin’s Price: Utilization of GARCH and Machine Learning Approaches,January 2021,Ting-Hsuan Chen,Mu-Yen Chen,Guan-Ting Du,Unknown,Unknown,Unknown,Unknown,,
57.0,1.0,Computational Economics,02 November 2020,https://link.springer.com/article/10.1007/s10614-020-10059-5,A Synthetic Penalized Logitboost to Model Mortgage Lending with Imbalanced Data,January 2021,Jessica Pesantez-Narvaez,Montserrat Guillen,Manuela Alcañiz,Female,Female,Female,Female,"Predicting binary decision problems is important in empirical economics. For instance, identifying whether an applicant will default in future or be turned down under the Home Mortgage Disclosure ActFootnote 1 (HMDA) contributes to the study of financial inclusion policy. In fact, the notion of having events versus non-events (a binary response) can be the result of a latent and unobserved random variable that triggers an event when it is high enough, so that extreme values then turn into event responses. Class-imbalanced data are relevant primarily in the context of supervised machine learning involving two (dichotomous) or more classes. Imbalanced means that the number of observations is not the same for each class of a categorical variable, in other words, one class is represented by a large number of observations while the other is represented by only a few (Japkowicz and Stephen 2002). In the context of mortgage lending, for example, Munnell et al. (1996) have dealt with an imbalanced class problem. They found that black and Hispanic applicants were more likely than whites to be denied mortgage loans. Thus, the class corresponding to the applicants who were denied was much smaller than the applicants who were approved. The minority class (denied mortgage lending) could be coded as one, while the majority class (approved for mortgage lending) could be coded as zero. There is evidence that the prediction accuracy of this type of events seems to remain problematic. King and Zeng (2001) note that classical econometric methods can underestimate the probability of occurrence in the minority class, while Krawczyk (2016) finds that machine-learning methods tend to exhibit a bias towards the majority class. There is a vast literature devoted to proposing techniques to handle the class imbalance problem. Barandela et al. (2003), Kotsiantis et al. (2006), Longadge et al. (2013), and Lin et al. (2017) summarize four types of techniques: Data preprocessing (balancing the data by oversampling, which increases the number of observations in the minority class, or by undersampling, which reduces observations in the majority class) using an algorithm approach (creating or modifying algorithms with the threshold and one-class learning methods), Cost-sensitive solutions (minimizing the costs of misclassification), Feature selection (finding the optimal combination of covariates that gives the best classification), and Resampling techniques incorporated in classifier ensembles such as boosting or bagging, which have given risen to proposals such as Synthetic Minority Oversampling (SMOTE) (Chawla et al. 2002), RUSBoost (Seiffert et al. 2009), UnderBagging (UB) (Barandela et al. 2003), and OverBagging (Wang and Yao 2009). 
In our view, however, the modelling and interpretability of imbalanced class phenomena in a joint process without overfitting data remains a subject beyond the scope of machine learning. We propose a Synthetic Penalized Logitboost that aims to decrease the mean square error in the highest and lowest prediction scores of the probability of minority class occurrence, by introducing a weighting mechanism that recalibrates a Logitboost to reduce the risk of overfitting. The Synthetic Penalized Logitboost improves the detection of extremes in the data if the purpose is to look for unusual patterns rather than for average cases. For this purpose, we borrow the specification of the model put forward by Munnell et al. (1996) to predict mortgage loan denial with a logistic regression. The paper is divided into five sections after the introduction. Section 2 describes the theoretical framework that motivates the paper. Section 3 describes the methodology in detail, specifically logistic regression (econometric model for binary prediction), Logitboost, Gradient Tree Boost (boosting-based machine learning for binary prediction) and the proposed algorithm. Section 4 describes the data set used in an illustrative example. Section 5 sets out the results and predictive performance measured by the root-mean-square error and includes the model’s interpretation. Finally, Sect. 6 contains the conclusions.",4
57.0,1.0,Computational Economics,17 November 2020,https://link.springer.com/article/10.1007/s10614-020-10060-y,Predicting Stock Price Falls Using News Data: Evidence from the Brazilian Market,January 2021,Juvenal José Duarte,Sahudy Montenegro González,José César Cruz Jr,Male,Unknown,Male,Male,"After the 2008 global financial crisis, international interest rates reached historical low levels, especially in developed countries such as the United States, the U.K., and Japan (Del Negro et al. 2019). Even though interest rates in developing countries are still relatively high, several countries tried to use monetary policy to reduce their interest rate levels during the same period. For instance, according to Fund (2019), the Brazilian monetary policy-related interest rate was cut by roughly 84% since 2008, from 13.75 to 2.25% APR, in 2020. Interest rate cuts directly impact the levels of national debts and indirectly impact firms’ and individuals’ decisions to invest in real and financial assets. The study developed by Bernanke and Reinhart (2004) highlights that economic decisions are affected by prices and yields of financial assets, which are influenced by changes in interest rates promoted by monetary policy. As an example, we can assume that, when low-risk asset yields decrease, investors become more likely to face more risk to increase their portfolio returns. Therefore, we can expect that investors rebalance their portfolios, including a more substantial portion of assets with higher expected returns, which in turn brings more risk to their investment. Stocks are good examples of alternative investment for those who are willing to diversify their portfolio composition, and are aiming for (possible) high returns. Due to their high volatility, forecasting stock returns can be considered a hard task for all types of investors. Consequently, several academic and industry studies have already questioned whether or not one can forecast future price movements. Even though this is a widely studied question, it remains with no conclusive answer. The most accepted hypothesis is that the possibility of predicting price movements varies according to the observed market, which may be more or less efficient (Fama 1970). Therefore, the majority of forecast techniques presented in the related literature rely on historical price analysis as the basis of their models. Even though quantitative approaches are often used in the development of prices/returns forecast models, Economics is classified as a human science, once prices are directly linked to the feelings and reactions of the people involved in the trades (Shiller 2000). The area of Economics that is interested in understanding how human factors relate to trading decisions, especially emotions, is known as Behavioral Economics (Khadjeh Nassirtoussi et al. 2014; Shiller 2000). Therefore, we can assume that rational investors make their decisions based on the available information set and use their best knowledge and skills to decide whether to buy, hold, or sell a certain asset. Risk-averse individuals also try to avoid losses in the market, especially when they believe that losses can occur with a higher (subjective) probability than gains. Such behavior can be verified when an individual’s utility function is represented with a steeper curve for losses than for gains. This theoretical framework is presented in various studies, such as in Tversky and Kahneman (1991). Given the current low-interest rates that induce investment in assets of higher risk in Brazil, we can rely on rationality and on the market efficiency hypotheses (Fama 1970) to present the following research question: can investors avoid losses in the Brazilian stock market using information from the news? Although conclusively answering this question may be difficult, we can observe the relationship between historical prices and the flow of news to determine return patterns. Identifying such patterns in the presence of market anomalies (de Camargos and Barbosa 2003) and using high volumes of daily news can limit manual approaches. This limitation can be overcome using text mining procedures, which can reduce processing time and prevent financial losses (Aase and Öztürk 2011). Historical events and empirical studies support the hypothesis that, if not decisive, news reports have a major influence on investor’s decisions, especially when they are negative (Shiller 2000). Therefore, several studies have already tried to formulate models based on textual information to predict price trends. Particularly those applying classical classification algorithms were relatively successful (Chowdhury et al. 2014; Aase and Öztürk 2011; Kumar and Ravi 2016; Khadjeh Nassirtoussi et al. 2014). Our main objective with this study is to integrate textual information to historical stock prices in order to predict financial losses on the Brazilian stock market. We evaluate the performance of several machine learning algorithms, using a method that fits one classifier for each stock, based on news obtained from several public sources. We also aim to answer the following research question: can textual information help predicting financial losses on the Brazilian stock market?  Since studies that aim to predict asset return using textual information are rare in the Brazilian stock market (Lopes et al. 2008; Rehbein 2012; Alves 2015), and are usually limited to analyze only few assets, from limited business segments (de Oliveira et al. 2013; de Carvalho 2018; Yim and Mitchell 2005; Lopes et al. 2008; de Araújo and Marinho 2018; Alves 2015), we have various contributions to the current literature in the area. First, we use an extensive variate of stocks (64) and classifiers in our analysis (11). Second, we compared all classifiers to the traditional trading strategies Buy & Hold (B&H) and the exponential moving average (EMA). Third, differently from most studies in the field, we focus on financial loss, which is assumed to be a more clear parameter to indicate the viability of our algorithms as trading strategies. And fourth, we implement an automatic search based only on news published in Portuguese, and using Brazilian websites. We expect that our results can be widely used by regulators, market participants, and news media. For instance, regulators may use our approach as an additional source of information to anticipate extreme events that could cause circuit breaks in the market. Market participants, such as individual investors and hedging funds, could prevent possible losses by reviewing their trading strategies beforehand. The news industry could follow our results to evaluate the impact of information on investors’ decisions.",7
57.0,1.0,Computational Economics,29 October 2020,https://link.springer.com/article/10.1007/s10614-020-10065-7,A New Scalable Bayesian Network Learning Algorithm with Applications to Economics,January 2021,Michail Tsagris,,,Male,Unknown,Unknown,Male,"Learning the causal relationships among variables using observed data is of high importance in many scientific fields, such as economics (agricultural economics, production economics) and econometrics.Footnote 1 In the field of marketing for instance, A–B testing plays an increasingly pivotal role in e-commerce who perform it as a means of finding the best factor that affects a specific service on their website (Bhat et al. 2020). When the aim is to recreate the causal mechanism that generated the data, common approaches include the use of graphical models, such as causal networks and Bayesian networksFootnote 2 (BNs), formulated on the basis of representing conditional independencies and causal relationships among the variables. BNs enjoy applications to numerous fields, but the focus of the current paper is on economics related fields applications, some of which are briefly discussed below. In production economics, Chen and Chihying (2007) defined a class of time series causal models based on stationary BNs which were later used to study the wage-price dynamics of the Australian economy and to estimate the causal relations among the time series variables (Chen and Hsiao 2010). More recently, Hosseini and Barker (2016) proposed the use of BNs, in the context of supplier evaluation and selection, to effectively model the causal relationships among variables so as to quantify the appropriateness of suppliers across primary, green, and resilience criteria. In macroeconomics, Spiegler (2016) supported the use of BNs as a means of getting deeper insights into a) the causal effects of endogenous and exogenous variables and b) the implications of a decision maker who misunderstands the role of an endogenous variable in the causal chain from his action to some target variable. For example, when a government evaluates tax or tariff reforms, it may take certain consumption or investment quantities as given, whereas in fact they are endogenous variables that respond to changes in policy. At the same year, Fennell et al. (2016) demonstrated the equivalence of any stock flow consistent macroeconomic model to a DAG using condensation graphs. In environmental resource economics,Footnote 3 Barton et al. (2008) used a BN as a meta-modelling tool to conduct decision analysis of nutrient abatement measures in a Norwegian territory. To effectively model environmental flows in oasis areas associated with agricultural economic loss, Xue et al. (2016) proposed a decision-making framework using BNs. Finally, Xue et al. (2017) used a BN to implement a trade-off framework to coordinate the water-use conflict between agriculture and the eco-environment, based on economic compensation for irrigation stakeholders. In banking and finance,Footnote 4
Häger and Andersen (2010) suggested BNs as a powerful framework for quantitative analysis of the causal mechanisms determining loss determinants in financial institutions. Leaning on available data and expert knowledge, their approach provided improved credibility of the loss predictions without being dependent on extensive amounts of data. Sanford and Moosa (2015) were more narrow focused as they targeted specifically on a structured finance operations unit located within one of Australia’s major banks. They developed a BN based tool to provide a-posteriori predictions of operational risk events, operational aggregate loss distributions, and operational Value-at-Risk. Specifically for systemic risk, Cerchiello and Giudici (2016) proposed a novel model based upon BNs to estimate the interrelationships between financial institutions. Their goal was to establish which of them are more central and, therefore, more susceptible to risk contagion. Tavana et al. (2018) dealt with the problem of liquidity risk. They proposed BNs as an intelligent system of defining the determining factors of liquidity risk in order to approximate it and predict its value. Finally, Chong and Kluppelberg (2018) demonstrated how the BN theory can be applied to detect contagion channels within the financial network, to measure the systemic importance of selected entities on others. Another application of BNs was in credit scoring, where Leong (2016) proposed a BN to address censoring, class imbalance and real-time implementation issues. In insurance, Cowell et al. (2007) considered the application of a BN to model a specific operation risk event common to insurance companies: fraudulent claims. BNs were alo used to estimate aggregate claims losses from a range of risk factors which are based on Pay-as-You-Drive and Pay-How-You-Drive insurance schemes (Sheehan et al. 2017). At a more business oriented field, customer service, Blodgett and Anderson (2000) used a BN to model the consumer complaint process. By using the conditional probabilities computed from the BN they provided insight into the determinants and subsequent behavioral outcomes of the complaint process. Cugnata et al. (2014) used BNs to examine the reasons that affect the overall customer satisfaction. Gupta and Kim (2008) combined Structural Equation Models with BNs to identify the factors that have causal effects on customer retention and how to support decision making regarding customer retention in a virtual community. One could also add complex networks to the above list. Caraiani (2013) used network analysis to examine the business cycle convergence using correlations between GDP growth rates for a time period. Papadimitriou et al. (2016) on the other hand evaluated the dynamic evolution of the network that corresponds to the GDP growth rate cross-correlations of 22 European economies. Sarantitis et al. (2018) employed network analysis to model the United Kingdom’s consumer price index and studied the network’s evolution through time.Footnote 5 Graphical models have been studied from a theoretical view in econometrics.Footnote 6 Sriboonchitta et al. (2014) showed that BNs can be seen as a special case of vine copulas and Boucher and Mourifié (2017) explored the asymptotic properties of strategic models of network formation in very large populations.Footnote 7 In the same spirit, Mele (2017) studied some asymptotic results for the class of directed exponentially random graph models. Another application of BNs is in agricultural economics for the task of synthetic population (or data) generation, a key component of the agent-based models. Berger and Troost (2014) examined how these models provide room for developing robust policies and effective climate-related land-use strategies in agriculture in response to climate changes. The rationale is to use the available data and generate synthetic populations so as to measure the economic impact of the change of some particular variables. Sun and Erath (2015) promoted the use of BNs for the task of synthetic generation showing their advantages. Finally, BNs have particular applications in bioinformatics and drug discovery where although they seem not related to economics in fact they are very closely related. Learning the causal pathways among genes can help effectively halt a disease (Langarizadeh and Moghbeli 2016) and reduce companies economic costs at many levels, for instance the creation of a cost-effective diagnostic or prognostic test (Sherif et al. 2015). The aforementioned publications manifest the breadth of applications of networks, causal networks and particularly of BNs. Despite the plethora of publications and the requirement for BNs, the economics literature is lacking in development of BN learning algorithms that are more commonly encountered in the field of machine learning. Max–Min Hill Climbing (MMHC) (Tsamardinos et al. 2006) is a widely used hybrid algorithm that has been implemented, and is publicly available, in the R package bnlearn (Scutari 2010). The relevant paper has received 1408 citations by the 30th of August 2020, according to google scholar, making it one of the classic papers in the Artificial Intelligence field. MMHC was initially devised for inference on small sample size datasets frequently seen in bioinformatics. For large sample sizes though, MMHC becomes computationally expensive and in some cases prohibitively expensive. The computational cost is increasingly evident as the sample size grows to millions, even for a few tens of variables. This computational cost is more prevalent and problematic today due to the explosion of (big) data, either in terms of large sample sizes, high dimensionalities, or both. The evolution of data acquisition requires not necessarily novel but mainly computationally efficient algorithms. The direction of small sample sizes and thousands or millions of variables has been addressed successfully in the twenty-first century via highly efficient algorithms, such as LASSO (Wu and Lange 2008), which has been extensively examined, applied, developed and implemented in almost all mathematical and statistical software. However, limited attention has been given to the presence of few tens of variables but millions of observations. The computational cost is always inextricably linked to economic and environmental costs. Algorithms that require heavy and very lengthy computations, consume tremendous amounts of electricity. Reducing the computational time reduces electricity expenses Kwok (2010) and results in faster decision making. The consequence of high electricity consumption is high carbon emission and thus high pollution. Hence, seen from the production economics perspective, reduction of the computational cost reduces the amount of bad output exposure and hence increases the production efficiency of the associated decision making units (Sickles and Zelenyuk 2019). To circumvent the computational disadvantage of MMHC, this paper proposes a computationally highly efficient algorithm termed PCFootnote 8 Hill-Climbing (PCHC). PCHC employs the PC algorithm, coupled with a scoring phase.Footnote 9 PCHC is significantly faster than MMHC, it can handle millions of observations in just a few minutes and retains similar or better accuracy levels than MMHC. Further, PCHC enjoys a very nice scalability property, its computational cost is proportional to the sample size of the data. Increasing the sample size by a factor, increases the execution time by the same factor. The rest of the paper is structured as follows. Section 2 briefly presents some preliminaries regarding BNs, some of their properties, the skeleton identification phase of MMHC and conditional independence tests for continuous and categorical data. Section 3 introduces the PCHC algorithm and presents a short comparison with MMHC, both theoretical and computational. Extensive Monte Carlo simulation studies presented in Sect. 4, using synthetic and real BNs illustrate the computational savings of PCHC compared to MMHC. Section 5 compares the MMHC and the PCHC BN learning algorithms on two real datasets. The first consists of continuous data on the credit history for a sample of applicants for a type of credit card (Greene 2003). Key questions include, but are not limited to, the key causal factors for example that affect the bank’s decision on issuing a credit card for a client and the detection of the factors that are causally associated with one’s major derogatory reports. The second dataset contains income related and demographic categorical variables (Friedman et al. 2001) where key questions include the determination of the causal relationships among the income related variables. Ultimately, conclusions are presented in Sect. 6.",4
57.0,1.0,Computational Economics,27 November 2020,https://link.springer.com/article/10.1007/s10614-020-10077-3,Textual Machine Learning: An Application to Computational Economics Research,January 2021,Christos Alexakis,Michael Dowling,Michael Polemis,Male,Male,Male,Male,"We use a corpus of research articles to demonstrate the power of machine learning applied to textual learning. Our applied technique is a Latent Dirichlet Allocation (LDA) topic modeling approach, popular in the computer sciences but largely not previously applied in economics. Our purpose, as part of the journal Special Issue on Machine Learning in Economics and Finance, is to demonstrate how machine learning has opened up bodies of text knowledge for analysis and data input in a way that traditional techniques in economics have not easily allowed. We take as a dataset all articles published in the Computational Economics journal and show how we can group these articles based on shared topics obtained through a textual machine learning technique. The main reason for choosing Computational Economics journal articles as our dataset of text documents is reader familiarity, but a second important reason is that it is an interesting text dataset to explore because of the development of the area. The significant advances in computing power in the last decades led to the creation of the new discipline of computational economics. Computational economics is a field of economics at the interface of computer science, economics, and management science. It focuses on the integration of information technology into economics and the automation of manual processes. Specifically, computational economics utilize advanced computing to solve empirical and theoretical problems in economics. The main areas of study within the field of computational economics include: computational methods in econometrics, agent-based computational modeling, computational aspects of dynamic systems and the development of programming tools for computational economics. Publications in this field, in just the Computational Economics journal alone, have echoed the development of the area and shown rapid growth, with about 1450 research articles since 1988 and a strong upward trend. When an area is developing this fast and in such volume of published research, it is interesting to see how research coheres around topics of common interest. Our application of the LDA technique shows how the sub-topics of this field can be identified through probabilistic learning with limited prior knowledge of the field. In this study we demonstrate the application of the technique, including the choices faced, show the topics extracted and their coherency, and discuss potential other applications of topic modeling approaches. LDA combines natural language processing of article content with probabilistic machine learning of the latent (hidden) topics that link groups of documents. The LDA technique was developed in Blei et al. (2003) and has proven a foundational technique in textual machine learning. As an approach it sits at the intersection between natural language processing and machine learning, with the machine learning aspect of the technique primarily present because of the use of an expectation-maximisation computer algorithm to cluster word co-occurence and to determine topics. The technique has been widely applied in computer science and the sciences more generally (Boyd-Graber et al. 2017), with the original study receiving over 30,000 citations, but has only recently come to the attention of business and economics scholars (Hannigan et al. 2019). Notable examples within the context of economics and business include the studies by Piepenbrink and Nurmammadov (2015) and Huang et al. (2018). The former examines the structure of research on emerging markets, while the latter the extent to which topics raised in corporate calls appear in stock analysts’ reports. Our study is, however, the first application of topic modeling in computational economics and, to the best of our knowledge, the first such study applied to understanding topics in the economics research literature more generally. This is the primary contribution of our study—the clear and structured introduction of the method to, and the demonstration of its benefits for, computational economics research. Through LDA topic modeling, we identify 18 topics which we argue are a reasonable characterization of the structure of research in computational economics. While these identified topics are not claimed to be the only way of structuring the topics of computational economics, we nevertheless consider them to be a robust representation. This robustness is demonstrated in the rigour of the LDA method and a number of statistical checks provided in the study. We consider this an important secondary contribution of our research. While there are many review articles in computational economics (such as Kwon and Bessler (2011) and Chen (2012)), these tend to be limited in scope to individual topics within computational economics. Our contribution is to provide an overall structure based on a comprehensive and wide-ranging database of computational economics research. This approach allows both an overview of the area, as well as tracking how topics have developed relative to each other, something which we discuss in the results. The remainder of the paper is structured as follows. In Sect. 2, we present the technique of topic modeling and describe the specific methodological decisions of our analysis. The identified topics of computational economics based on our technique and their evolution over time are presented and analysed in Sects. 3 and 4. Finally, Sect. 5 concludes the paper by discussing the broader uses of the technique in computational economics.",4
57.0,1.0,Computational Economics,07 January 2021,https://link.springer.com/article/10.1007/s10614-020-10083-5,Nowcasting US GDP Using Tree-Based Ensemble Models and Dynamic Factors,January 2021,Barış Soybilgen,Ege Yazgan,,Male,Female,Unknown,Mix,,
57.0,1.0,Computational Economics,07 January 2021,https://link.springer.com/article/10.1007/s10614-020-10085-3,A New Hybrid Instance-Based Learning Model for Decision-Making in the P2P Lending Market,January 2021,Golnoosh Babaei,Shahrooz Bamdad,,Unknown,Unknown,Unknown,Unknown,,
57.0,2.0,Computational Economics,02 November 2019,https://link.springer.com/article/10.1007/s10614-019-09932-9,Leveraging Social Media to Predict Continuation and Reversal in Asset Prices,February 2021,Patrick Houlihan,Germán G. Creamer,,Male,Male,Unknown,Male,"Today’s computing power makes it possible to peek into the minds of millions to gather intelligence on what brands are most popular, opinions of millions and what the overall mood of society is at any given point. The internet has provided an avenue for millions to express opinions about products and services (Pang and Lee 2008). Before the internet, the spread of opinions was in large part due to word-of-mouth (Nazzaro and Blackshaw 2006; Hong et al. 2005; Chen and Xie 2008). In modern times, through leveraging technology, one can catch a glimpse into the minds of millions and extract their sentiment. Many recent papers have explored how extract relevant financial information automatically from news and social media. In this respect, Kearney and Liu (2014) conduct a literature review about the use of information from textual based context and its influence on investor behavior and firms’ strategy. Chen et al. (2014) show that views of articles and comments from readers can predict stock prices and earnings surprises. Garcia (2013) restricts the capacity of news to predict stock returns only during recessions. Andrei and Hasler (2014) show that stock return variance and risk premia increase with both investors’ attention to news and learning uncertainty. Ben-Rephael et al. (2017) observe that institutional attention is more responsive to major news events and facilitates permanent price adjustment. Dugast (2018) find that there is a negative relationship between news frequency and market depth and the variability-volume covariance. Zhang et al. (2014) and Shen et al. (2016) find that the volume of Baidu news is a predictor of volatility of the stock market in China. Engelberg et al. (2018) report that abnormal return is influenced by biased expectations and can be partially corrected by the arrival of news. Edmans et al. (2018) find that CEOs release more positive news with a short-term positive impact on prices during the months when they are expected to sell equity. Luo et al. (2013) show that indicators from social media have a faster predictive value than information from conventional media to predict equity values. Bukovina (2016) explores a transmission mechanism between social media and capital markets from the behavioral finance perspective. Some studies indicate that emotions from social media such as Twitter play an important role in predicting real world outcomes such as movies’ box-office revenues (Asur and Huberman 2010), stock prices in the long term (Sul et al. 2017), and stock market volatility of S&P 500 firms (Sul et al. 2014). Even more, Antweiler and Frank (2004) classify the stock messages into three categories (bullish, bearish, neutral) and find that the correlation between message volume (number of daily messages) and volatility was 0.132, message volume and trading volume was 0.332, and trading volume and volatility was 0.063. Stock chatter messages have a higher correlation with volatility than trading volume, a market data derived variable. Das and Chen (2007) reinforces these findings combining five different algorithms. There is an extensive literature about how to extract sentiment from tweets (Pak and Paroubek 2010) or measure the intensity of moods in blogs (Mishne and De Rijke 2006 and Bollen et al. 2011a). However the development of financial blogs such as StockTwits (www.stocktwits.com) whose users microblog about topics related to the stock market and Seeking Alpha (seekingalpha.com), a famous site for individual investors to collect advice from peers and channels (Chen et al. 2014) have generated very rich specialized datasets. Researchers have extracted sentiment from these financial blogs and harnessed to predict future directional moves of the market (Bollen et al. 2011b; Kim and Kim 2014; Siganos et al. 2014; Martínez-Cámara et al. 2014; Aisopos et al. 2016; Saif et al. 2016; Brown and Cliff 2005). Messages from the Yahoo Finance board have been used to extract both topical and sentiment-based features associated with the direction of asset prices’ movements (Nguyen et al. 2015). This research reinforces the idea that human emotions, measured through social media, help dictate buying and selling behaviors, from the purchase of goods and services to investing in a company. The current development of social media has impacted, for the most part, on a reduction of the time-consuming process of manually polling individuals and assimilating the results (O’Connor et al. 2010). Leveraging technology to analyze online text enables the automatic and real-time extraction of sentiment directly from social media and news feeds, which is both less costly and less time consuming than traditional polling techniques.
 Zhang et al. (2011) measure various emotions in tweets: hope, fear, and worry. This study shows that as the volume of emotionally charged tweets increase, the markets decrease in value and the volatility index, VIX, increase. Inversely, the research indicates that as the emotional tweets decrease, the Dow Jones average increase and VIX decline. When a majority calm mood is detected from tweets, the accuracy to forecast the daily direction of the DOW Jones using a Self-Organizing Fuzzy Neural Network model was 86.7% (Bollen et al. 2011b). Schumaker and Chen (2009) yield an average 3.6% return, 20-minute look ahead, by extracting particular noun phrases, named entities, in conjunction with support vector machines. A 0.4% loss in the S&P index was illustrated by observing increases in anxiety from the weblog LiveJournal; worry and fear became more prevalent (Gilbert and Karahalios 2010). Oh and Sheng (2011) show that sentiment extracted from a stock microblog can predict the direction and underreaction of future stock prices. In addition to social media, Tetlock et al. (2008) report that a high proportion of negative words in firm-specific news predicted low firm earnings. Also, Google query search volume was shown to be a strong predictor for future economic activity in various industries (Choi and Varian 2012). Engelberg and Gao (2011) propose that the search frequency in Google volume index as a measure of investors’ attention predicts higher stock prices in the next 2 weeks and price reversals in a year. Da et al. (2014) find that Internet search volume from millions of households can predict short-term return reversals, augment in volatility and the flow from equity to bonds. Bollen et al. (2011b) use Google’s Profile of Mood states (GPOM) which categorizes human moods into six dimensions (calm, alert, sure, vital, kind and happy) to predict future directional moves in stocks with a high degree of accuracy. All the studies about sentiment analysis using news or social media extract sentiment indicators directly from the text. However, the dynamic of the sentiment indicators by themselves also provide information about the market dynamics and this is an area of research underexplored. For this reason, in this research, we detect large changes in message sentiment and message volume from StockTwits, and incorporate both into a framework for asset pricing and predictive analytics. The main question that we explore is if aggregated message volume and sentiment extracted from social media contain information that can be ranked and used as a proxy for reversal and continuation effects of stock price movements in a 5-day horizon.",5
57.0,2.0,Computational Economics,11 December 2019,https://link.springer.com/article/10.1007/s10614-019-09958-z,Bayesian Estimation for High-Frequency Volatility Models in a Time Deformed Framework,February 2021,Antonio A. F. Santos,,,Male,Unknown,Unknown,Male,"High-frequency volatility modeling is essential due to how financial markets operate nowadays. There are thousands of decisions and trades taken each second. High-frequency trading firms are very active in the markets. Algorithm trading is a reality with computing and information technology available. Through computers, orders of buy or sell can arrive at markets in milliseconds. Due to the referred facts, prices can move very fast, possibly with big drops within minimum periods. Volatility changes during a negotiation day, and sudden moves happen frequently. One example of such sudden changes is related to “Flash Crashes”. In electronic security markets, large withdrawals of stock orders can produce significant price declines. Even an operator’s error, pressing the wrong key on the computer keyboard, can induce large movements of prices. The biggest one occurred on May 6, 2010, when the Dow Jones Industrial Average within a period less than an hour dropped more than one thousand points. If these risks exist, there is also new data (intraday data), computational power, and statistical models available. They can help understand, and inform against sudden changes in volatility within a trading day. There is a wide variety of models aiming to characterize the components associated with volatility evolution. The models try to account for many features of financial returns, namely their nonlinear evolution, discontinuities (jumps), and latent states that influence both returns and volatility. Models can become quite complicated, and one of our primary motivations is to understand the results found in the literature associated with parameter estimates that represent an interpretation challenge. We try to understand the reasons for such findings and develop a framework that may help to address the challenging interpretations. Studies dealing with the analysis of high-frequency volatility are not many. Stroud and Johannes (2014) present a seminal study in this area, where there is an analysis based on using Stochastic Volatility (SV) models and estimating the models’ parameters through Bayesian methods. For estimation of the model’s parameters, the authors use Markov chain Monte Carlo (MCMC) simulation methods to approximate the posterior parameters’ distributions. The simplest version considered is a two factors Stochastic Volatility (SV\(_2\)) model for 5-min logarithmic price returns. The novelty is the data frequency, as two-factors SV models are considered in Liesenfeld and Richard (2003), Durham (2006) and Dobrev and Szerszen (2010). In all four studies, they consider the S&P500 financial returns series, although Stroud and Johannes (2014) use intraday, while the others use daily returns. For model versions that are comparable, while Liesenfeld and Richard (2003) and Durham (2006) found negative values for the parameter characterizing fast-speed volatility, Stroud and Johannes (2014) estimates are well above 0.9, even being a model for intraday returns. Including a leverage effect, no significant change is reported in Stroud and Johannes (2014). However, in Durham (2006), there is a change in the sign of the coefficient. Dobrev and Szerszen (2010) considered a different model version with the introduction of a new observation equation, and found an estimate for the fast-move volatility parameter of around 0.33. The first two studies concluded that the fast-moving volatility persistence parameter was not statistically significant. In contrast, Stroud and Johannes (2014) concluded the opposite. These are contradictory results, revealing that further research is needed on how to fit these complex models to financial data available. The volume of trade is related to volatility, and the first author to account for this fact was Clark (1973). Other authors have also addressed this issue, as Tauchen and Pitts (1983), Andersen (1996), Tauchen et al. (1996), just to cite a few. However, the joint analysis of prices, returns, and volume has been scarce, at least when compared with studies addressing exclusively the evolution of prices. Citing a sentence in Granger (2002) can elucidate the claim; “...newspapers have many prices, they have no indication of volatility except possibly daily price range. On the other hand, they all have a column marked ’vol’ but this is for daily (or weekly) volume rather for volatility. Given its ready availability, until recently volume has been the missing variable in finance ..”. Volume conveys information on volatility. However, the attempts of introducing this variable in models characterizing the evolution of volatility did not present convincing results. The main reason is that they underestimate the persistence of volatility, basically a characteristic that these models intend to explain. Two examples are associated with the models in Lamoureux and Lastrapes (1990) and Watanabe (2000). We extended the SV models, and instead of considering the volume directly, a new variable is introduced, the duration, and indirectly the volume through the utilization of a time-deformed representation, defined using the volume itself. The theoretical background is associated with the change of time technique for a given stochastic process. Details can be found in Barndorff-Nielsen and Shiryaev (2015) and Swishchuk (2016). Other authors have used the volume and time-deformation representations within a financial econometric analysis (Gourieroux and Le Fol 1997; Maillet and Michel 1997; Le Fol and Ludovic 1998; Darolles et al. 2000, 2015, 2017). We consider time-deformed returns as a component of SV models frequently used to characterize volatility evolution. The aim is twofold, provide a new interpretation of volatility evolution, and through the harnessing of new information sources, improve estimation processes. With the models recently considered, the high level of parameter estimation uncertainty is a dominant feature. We redefine models with the introduction of a new variable, the duration, which results from the new type of return used. Two types of models are combined, SV-return, and SV-duration models. We only address the estimation problem issues. Since SV models were first proposed by Taylor (1986), the amount of research it has induced is overwhelming. Since Jacquier et al. (1994), until recently Djegnéné and McCausland (2015), the number of papers addressing ways of estimating the model’s parameters are in the order of hundreds. The primary reason for addressing again the issue of parameters estimation associated with SV models is that, with the increased models’ complexity, parameters’ estimation turned to be again a challenging task, and estimation parameter uncertainty a problem that needs a solution. In this context, a reduction in estimation parameter uncertainty can represent further gains for forecasting and even for the economic significance of the models. Bayesian methods and MCMC techniques are used to estimate the parameters of SV models. The aim is to analyze the influence of the new time representation adopted in the reduction of estimation uncertainty. Building on the time-deformation and volume-adapted returns adopted, we define a new SV-type model version, a bivariate SV model which includes returns and duration as observable variables. It can assume the characteristics of a one- or two-factors model, and jumps in volatility are also considered. Bayesian methods and MCMC techniques are adopted to estimate the parameters.",4
57.0,2.0,Computational Economics,01 January 2020,https://link.springer.com/article/10.1007/s10614-019-09964-1,Robust Solutions to the Life-Cycle Consumption Problem,February 2021,Lorenzo Reus,Frank J. Fabozzi,,Male,Male,Unknown,Male,"A well-known problem in economics is how to explain the best consumption pattern of individuals over time and therefore the best investment allocation of individuals over their lifetime. This problem is referred to in the economics literature as the life-cycle consumption problem, the optimal consumption and portfolio-choice problem, or simply the life-cycle problem (LCP). In this paper, we propose solving the LCP with the robust optimization approach introduced by Ben-Tal and Nemirovski (1998) who derived the so-called Robust Counterpart (RC) technique. The drawback of this approach, however, is that solving the RC can be hard for certain problems like the LCP, unless the formulation of the problem is written with a certain structure. Thus, we introduce a methodology that changes the formulation of the LCP before applying the RC technique. Our findings show that this approach can solve the LCP for long horizons and with different assumptions about the degree of uncertainty of future returns. Previous research has focused on finding the consumption that maximizes the expected value of the life-cycle utility function. Our approach focuses on finding a consumption that reduces the left-tail of this function, that is, to achieve higher levels of utility for worst case returns. Our simulations show that the consumption derived by the RC approach produces a higher level of utility in worst-case scenarios, as compared to a benchmark that maximizes the expected utility. We use the conditional value-at-risk (CVaR) measureFootnote 1 to compare the left-tail of both utility distributions. In this way, we provide an alternative consumption pattern for agents that prefer to reduce the potential downside of utility levels through their life. Another contribution of this work goes beyond solving the LCP with the RC. As explained latter, the methodology to leave the LCP with the proper structure involves applying variable changes (transformations) to the original problem. This methodology is inspired by the work of Ben-Tal et al. (2000) and Bertsimas and Pachamanova (2008), which use a change of variable to solve a multistage portfolio problem. We formalize this methodology, in order to employ it in other applications.
 The organization of this paper is as follows. We next provide a brief literature review of the LCP and the RC approach. In Sect. 2, we explain a general methodology to derive the RC for problems, like the LCP, with a certain structure. We explain how to derive the RC for the LCP in Sect. 3. Results and performance of the proposed method are provided in Sect. 4. Our conclusions are summarized in Sect. 5. The LCP has been widely discussed following the seminal works of Ramsey (1928), Modigliani and Brumberg (1954), and Friedman (1957). Researchers have sought to solve the LCP under a more realistic setting by adding complexity with respect to each of the following: Income For example, Viceira (2001) takes into account nontradable income and Bodie et al. (1992) deal with labor flexibility. Solutions considering uncertain income is dealt with in Polkovnichenko (2006) and Guiso et al. (1996). Utility functions There are extensions beyond time-separable expected utility preferences. See, for example, Hindy and Huang (1993), Duffie and Epstein (1992), and Detemple and Giannikos (1996). Consumption and portfolio constraints Besides the common no borrowing and no leverage constraint, there are other constraints involving liquidity as in Bodie et al. (2004), transaction costs as in Dumas and Luciano (1991), and asset selection from a pool of candidate assets. Retirement issues For example, determining retirement endogenously as in Chai et al. (2011), adding health risk post retirement as in De Nardi et al. (2010), Yogo (2016), and Koijen et al. (2016). Description of the investment opportunity set (returns): This includes time-varying expected returns, stochastic volatility, jumps in returns, jumps in volatility, and the like. See, for example, Barberis (2000), Liu and Pan (2003), Garlappi et al. (2006), Kan and Zhou (2007), Liu (2010) and references therein. Methods for finding optimal optimal consumption and investment decisions have been suggested by several researchers.Footnote 2 Most of them are based on solving discrete dynamic programming models or stochastic differential equations by numerical methods, such as finite differences. Due the complexity of the LCP, however, the application of these approaches can result in what is referred to as the curse of dimensionality. That is, the number of possible states (wealth) during an individual’s lifetime becomes very large, even with few outcomes generated on the investment opportunity set. There are, however, techniques, like the Stochastic Dual Dynamic Programming (SDDP) and the Approximate Dynamic Programming (APD)Footnote 3 that can handle the curse of dimensionality. They do this by approximating the value function of the Bellman equation instead of computing and saving the exact value for every possible state.Footnote 4. The martingale approach, first proposed by Karatzas et al. (1991), also avoids the curse of dimensionality when solving the intertemporal portfolio choice. However it becomes untractable when including simple portfolio rules.
 The other problem with previous methods is that a prior distribution must be given to uncertain variables. For example, the SDDP only works for specific dynamics of the uncertain parameters. Besides, finding the best model for the returns behaviour can be as difficult as solving the LCP itself. Errors in choosing the right model might lead to suboptimal investment and welfare losses.Footnote 5 The methodology we propose does not need a prior distribution or model for the returns. Thus, it avoids the curse of dimensionality. What’s more important, it offers a different life-consumption pattern. Previous methodologies generally maximize the expected utility function.Footnote 6 Instead, we provide a consumption that achieves better levels of utility for the worst cases of the future returns. Agents that want to reduce the potential downside of utility levels might prefer this consumption pattern over the one that maximizes the expected utility. The technique we propose for solving the LCP is a specific type of robust optimization. This approach seeks near-optimal solutions under changing parameter values. One well-known approach to this objective was introduced by Ben-Tal and Nemirovski (1998) who derived the so-called Robust Counterpart technique. Let \(\mathcal {P}\) be the following problem with parameter \(a \in R^m\) uncertain. Let \(\mathcal {S}\) be the set defining the possible values of a, which is called the uncertainty set. Constraint \(\phi (x,a) \in \mathcal {K}\) must satisfy \(\forall a \in \mathcal {S}\). Assume \(\mathcal {K} \subset \mathcal {R}^m\) and \(\mathcal {S}\) are bounded and closed sets. The RC of \(\mathcal {P^*}\) is defined as The minimization problem is called the inner problem and determines the objective value of each feasible solution x under the worst-case scenario of parameter values a in the uncertainty set. The robustness of the solution depends on the characteristics of \(\mathcal {S}\). Ben-Tal and Nemirovski (1999), Bertsimas et al. (2004), and Ben-Tal et al. (2006) analyzed different types of \(\mathcal {S}\) such as ellipsoids, boxes, conics or sets induced by norms, which determine the type of optimization to be solved. One of the benefits of this approach is that the size of these problems does not increase exponentially as the number of time periods increases. Another advantage is that there is no need to assume a distribution for the parameters as other approaches do. Even if uncertainty sets determine the domain for the parameters, this specification is minimal compared to having to specify a distribution. The RC formulation has been widely used in portfolio optimization.Footnote 7 The idea behind robustness is to find solutions that remain optimal or near optimal if parameters change their values. A criticism of the RC formulation is that it can generate results that are too conservative [(see Bertsimas and Sim (2004) and Gregory et al. (2011)]. This means that the optimal solution is optimal in worst case scenarios, but performs poorly in the rest of the scenarios. To overcome that problem, Ben-Tal et al. (2004) introduced the Adjustable Robust Counterpart (ARC) for linear problems. ARC consists of classifying variables in non-adjustable or first-stage variables and “wait and see” variables, similar to a two-stage stochastic model. Although the decomposition is less conservative, it is not tractable in most cases. Fischetti and Monaci (2009) define Light Robustness as a methodology mixing robust optimization with two-stage stochastic programming to reduce conservatism. Ceria and Stubbs (2006) introduced the concept of Zero Net Alpha Adjustment for asset allocation problems, a method that involves adding a constraint to the inner problem when the parameters are symmetrical by nature. This rules out solutions in which total deviations from the mean estimate are negatively or positively biased, hence increasing the value of the inner problem.",1
57.0,2.0,Computational Economics,04 March 2020,https://link.springer.com/article/10.1007/s10614-020-09971-7,Correction to: Robust Solutions to the Life-Cycle Consumption Problem,February 2021,Lorenzo Reus,Frank J. Fabozzi,,Male,Male,Unknown,Male,"In the original publication of the article, there were several errors in the text and equations. The corrections are given below: In Abstract, the sentence beginning “We generalize the methodology…” should read as “We generalize the methodology and illustrate how it can be used to solve other types of problems.” In section “1.2 Literature Review”, the sentence beginning “Due the complexity of…” should read as “Due to the complexity of the LCP, however, the application of these approaches can result in what is referred to as the curse of dimensionality.” In section “1.2 Literature Review”, the sentence beginning “They do this by approximating the value…” should read as “They do this by approximating the value function of the Bellman equation instead of computing and saving the exact value for every possible state.4” In section “2 Change of Variables for the RC”, the sentence beginning “We now explain how to change…” should read as “We now explain how to change the original problem \(\mathcal{P}\) before applying the RC for certain types of problems, which includes the LCP.” In section “2.2 Closed Solutions for Inner Problem”, below Eq. (4), the sentence beginning “These type of uncertainty…” should read as “These types of uncertainty sets includes the box (p=1), the ellipsoid (p=2) and other polyhedrons.” In section “3 RC for the LCP”, sentence beginning “An individual must decide…” should read as “An individual must decide his or her consumption pattern for the following N periods.” In section “3 RC for the LCP”, sentence beginning “The uncertain parameter are the returns…” should read as “The uncertain parameter is the returns rjt of each asset j at each time t and represents parameter a in model (1).” Below Eq. (9), the sentence should read as “Following Eq. (8), \(w_1=M_1(w_0+i_0-c_0)\) and Eq. (9)”. Below Eq. (10), the sentence should read as “where L is a triangular matrix with \(LL^T=C\) and z a standard normal independent and identical distributed (i.i.d.) vector. We consider log returns to follow an AR1 process of coefficient \(\rho \), hence \(C_{t,t+k}=\sigma ^2\rho ^k.\)9 Under this framework, the parameter defining the uncertainty set is10”. Below Eq. (11), the sentence should read as “where \(A(t,s):=\sum _{k=1}^t\sum _{l=1}^s C_{k,l}\)”. Below Eq. (13), the sentences should read as “Figure 2 shows the mean and the CVaR (at a 5% level) of UN.12 The CVaR results show that the consumption pattern provided by the LCP presents a utility distribution with a smaller left-tail than its benchmark, except for close-to-neutral agents with high confidence about future returns. We can expect results to have worst outcomes in these situations, because we have implicitly assumed uncertainty in the returns by using the model given by Eq. (10).” In “Appendix: Proof of Eq. (11)”, the sentence beginning “Denoting A(t,s)…” should read as ""Denoting \(A(t,s):=\sum _{k=1}^t\sum _{l=1}^s C_{t,s},\) we have shown that \(E(e^{\gamma \sum _{k=1}^t L_{k^{z}}})=e^{\frac{\gamma ^2}{2}A(t,t)}.\) z is a standard normal i.i.d. vector and \(LL^T=C,\) thus"".",
57.0,2.0,Computational Economics,04 January 2020,https://link.springer.com/article/10.1007/s10614-019-09965-0,"Co-movement and Dynamic Correlation of Financial and Energy Markets: An Integrated Framework of Nonlinear Dynamics, Wavelet Analysis and DCC-GARCH",February 2021,Indranil Ghosh,Manas K. Sanyal,R. K. Jana,Unknown,Unknown,Unknown,Unknown,,
57.0,2.0,Computational Economics,08 January 2020,https://link.springer.com/article/10.1007/s10614-019-09967-y,A Guide to Using the R Package “multiColl” for Detecting Multicollinearity,February 2021,Román Salmerón-Gómez,Catalina García-García,José García-Pérez,Male,Female,Male,Mix,,
57.0,2.0,Computational Economics,09 January 2020,https://link.springer.com/article/10.1007/s10614-019-09966-z,Forecasting Inflection Points: Hybrid Methods with Multiscale Machine Learning Algorithms,February 2021,Julien Chevallier,Bangzhu Zhu,Lyuyuan Zhang,Male,Unknown,Unknown,Male,"This paper proposes a new hybrid method (Ensemble Empirical Model Decomposition—Particle Swarm Optimization—Least Square Support Vector Machine) in order to forecast inflection points. In what follows, we characterize the forecasting of inflection points as a classification problem. The methodology unfolds according to several steps that are: the pre-processing of the data (using EEMD), then a classifier (LLSVM) to identify the inflection points, and finally, the estimation of the LLSVM model parameters using the PSO method. An inflection point is a state of a peak or a valley that causes a cycle of time series. An inflection point creates uncertainty and chaos while causing a redistribution of capital. The decisions of investors, especially medium- and long-term investors, depend not only on the level of the financial price with a particular marketing timing but also on the upward or downward trend of the price over a period (Zhang and Wang 2010). Investors who can foresee the inflection point can use it in their decision making to gain more profits. On the other hand, those who cannot foresee the inflection point will suffer from a loss of opportunities. Therefore, the problem of how to predict an inflection point correctly has attracted more and more attention in many types of research in the field of economics. However, due to the complexity of a vast universe of relevant time series, behavioral elements, and exogenous factors, such as political decisions (Sermpinis et al. 2017), that can cause an inflection point, predicting an inflection point has been a particularly challenging problem. More interestingly, these factors have continuously changed over time. Considering all these issues, it would be a wise approach to forecast the future inflection point via the past inflection points and past data. Various methods have been used to predict an inflection point, the foremost of which is the leading index method. Alvarez-Ramirez et al. (2005) used the leading index method to examine a cycle of the crude oil price. Croce and Haurin (2009) employed the leading index method to predict turning points in the housing market. Zhang and Wang (2010) adopted the same method to forecast the inflection points of the crude oil price. Eric (2012) used the leading index method to predict US recessions. Nyberg (2013) employed it to predict bear and bull stock markets, and Levanon et al. (2015) used it to predict turning points in the business cycle. Although applications of the leading index method have become widely popular, this method cannot deliver stable leading relationships, which can lead to a failure to predict the inflection points. Thus, it is tough for the leading index method to meet investors’ practical expectations about future price movements. In recent years, a large number of classification techniques have been used for inflection point forecasting. These techniques can be categorized roughly into the following two groups (Billio et al. 2012; Chang et al. 2011; Chauvet and Senyuz 2016; Luo and Chen 2013; Martínez-García et al. 2015; Schreiber and Soldatenkova 2016): (1) statistical models, including discriminant analysis, logistic regression, probit regression, the k-nearest-neighbour classifier, and the decision tree; (2) artificial intelligence models, such as the fuzzy system, artificial neural networks (ANNs), support vector machines (SVMs), and least squares support vector machines (LSSVMs). However, financial trading series is dominated by high non-linearity, noise, and a non-stationary nature, while most statistical models are established on the assumption that the data are stationary and linear. They seem to have low power and volatile behavior over time. As a result, they fail to deal effectively with the non-linear patterns, which can lead to unsatisfactory prediction results. Therefore, statistical models seem to be unable to help investors in this task. On the other hand, artificial intelligence models and heuristics have provided promising empirical evidence in financial trading applications (Sermpinis et al. 2017), which have shown considerable superiority in predicting inflection points. In the field of signal processing, the multiscale machine learning paradigm has attracted considerable attention (for a review, see Cang and Yu 2014). As a novel SVM classifier, the LSSVM is capable of modeling high-dimensional, noisy, and complex feature problems. On the one hand, a single LSSVM cannot identify an inflection point. On the other hand, its atheoretical nature and the lack of sufficient robustness checks on its performance have generated skepticism among finance professionals and researchers (Sermpinis et al. 2017). Meanwhile, previous studies have mainly predicted the inflection points in the fields of business and macroeconomics. Since the existing forecasting methods appear to be insufficient due to non-linearity and the non-stationarity of prices, new hybrid machine learning methods exhibit academic appeal for forecasting inflection points in time series. In the context of pattern recognition problems, hybrid models typically combine multiple models in parallel in order to improve single models’ fit. In practice, hybrid models can be based on serially combining multiple models or integrating two or more algorithms into one model. At its core, this paper aims at developing a new methodology for inflection points forecasting. Against the background of signal processing, machine learning, and intelligent optimization methods, it introduces several forecasting algorithms that combine ensemble empirical mode decomposition and least squares support vector machines. These new hybridization methods can be seen as an extension of the existing least squares support vector machine methods to allow simultaneous optimization of parameters. Such hybrid models constitute a recent trend in operations research, as evidenced by the M4-CompetitionFootnote 1 at the University of Nicosia (Greece) discussing the accuracy of statistical versus machine learning methods. This study is aimed at developing an inflection point hybrid forecasting system with ensemble empirical mode decomposition (EEMD), particle swarm optimization (PSO), and the LSSVM. To address the primary motivation behind a particle swarm optimization-based model of predicting market price inflection points, let us recall first the interest of PSO in computer science. PSO is a computational method for the optimization of parametric and multiparametric functions. The PSO algorithm is a meta-heuristic method, which has been providing suitable solutions for problems of global optimization functions with box-constrained. As in most heuristic methods that are inspired by biological phenomena, the PSO method is inspired by the behavior of flying birds. The philosophical idea of the PSO algorithm is based on the collective behavior of birds (particle) in search of food (point of global optimal). The PSO optimizes a problem by having a population of candidate solutions and moving these particles around in the search-space according to simple mathematical formulae over the particle’s position and velocity. The movement of the particles in the search space is randomized. Each iteration of the PSO algorithm, there is a leader particle, which is the particle that minimizes the objective function in the corresponding iteration. The remaining particles arranged in the search region will follow the leader particle randomly and sweep the area around this leading particle. In this local search process, another particle may become the new leader particle, and the other particles will follow the new leader randomly. Each particle arranged in the search region has a velocity vector and position vector, and its movement in the search region is given by changes in these vectors. At a conditional stopping criterion considered, the algorithm will stop providing the global minimum value. The PSO function is an efficient function for global minimization, wherein it is not necessary to provide initial values. Theoretically, we document that hybrid machine learning methods provide superior prediction accuracy of inflection points when compared with the benchmark EEMD transformed data for a simple sine wave function. Therefore, this paper can be seen at its core as a methodological contribution. Synthetic and real-world data practical applications are designed to harness its effectiveness, drawn from arbitrary time series in financial markets (S&P 500), commodities (WTI oil price), or cryptocurrencies (Bitcoin). The motivation for this study is to introduce into operations research a hybrid LSSVM architecture of PSO and EEMD for inflection point forecasting, which tries to overcome some of the above limitations. The main argument for selecting EEMD rather than wavelet analysis unfolds as follows. As a new self-adaption data analysis approach for non-linear and non-stationary data, EEMD can solve the dilemma between the difficulties in modeling and the lack of economic meaning. It does so by decomposing the data into a set of intrinsic mode functions (IMFs) and one residue based on local characteristic scales to identify their concrete implications according to their timescales. For example, an IMF with a scale of 12 months can be recognized as a seasonal component. However, we cannot obtain these by wavelet analysis. Therefore, we select EEMD for forecasting inflection points. The main innovation of this study lies in three aspects: firstly, to hybridize our model, we borrow from the time-series pre-processing literature the EEMD decomposition of price series into several IMFs and one residue, which are identified as the trend (T), cycle (C), seasonal (S), and irregular (I) components, by using the average period and series reconstruction algorithms. On this basis, the inflection points of the cycle are identified as the ones of the price by the Bry–Boschan method. Secondly, PSO is applied to determine adaptively the parameters of the LSSVM classifier, which is used to predict the inflection points of the price with high prediction accuracy and efficiency. Thirdly, arbitrary time series are taken here as research samples from financial and commodity markets. The empirical results obtained show that, compared with the popular benchmarking forecasting models, our proposed PSO-based LSSVM (PSO-LSSVM) model is superior and capable of accurately predicting inflection points on arbitrary theoretical as well as time series with high statistical performance. In addition to particle filtering, we consider in this paper two new state-of-the-art machine learning algorithms that, when considered in hybridization, can be expected to improve regarding the directional predictive accuracy of inflection points for a given series. On the one hand, genetic algorithms (GA) are adaptive methods, which may be used to solve search and optimization problems (Beasley et al. 1993). They are based on the genetic process of biological organisms. On the other hand, grid search (GS) algorithms can be seen as advanced methods of cross-validation among various pairs of values that are tried, and the one with the best cross-validation accuracy is picked (Hesterman et al. 2010). Regarding performance comparisons, it seems that the proposed PSO-LSSVM model has the highest performance for all the data sets. One important reason is that PSO can find optimal model parameters for LSSVMs. At the same time, PSO has an efficient global searching ability that can effectively enhance computational efficiency. Compared with the prevailing search algorithms, such as the genetic algorithm (GA) and grid search (GS), PSO is found to enhance the forecasting accuracy (as measured by several metrics, including the Diebold and Mariano (1995) tests). To verify the computational efficiency, we also use the GA and GS for the parameter selection of the LSSVM (Zhou et al. 2009), called GA-LSSVM and GS-LSSVM, respectively. Finally, we conduct a ‘horse race’ of hybrid models versus standard models. We gauge the superiority of hybridization over soft-computing (BPNN, SOM, Elman, FNN, FLM, and SVM) and hard-computing (LDA, QDA, LogA, C4.5, and KNN10) models as well in a trading performance exercise. The rest of this paper proceeds as follows. Section 2 details the methodology and describes the data. Section 3 contains the practical application. Section 4 concludes.",9
57.0,2.0,Computational Economics,31 January 2020,https://link.springer.com/article/10.1007/s10614-020-09969-1,The Success of the Deferred Acceptance Algorithm Under Heterogenous Preferences with Endogenous Aspirations,February 2021,Ismail Saglam,,,Male,Unknown,Unknown,Male,"In a seminal paper, Gale and Shapley (1962) showed that for every two-sided marriage population where individuals have complete and transitive preferences over potential mates there always exists (between men and women) a one-to-one matching which is stable in the sense that no individual prefers being single to his/her mate and no pair of individuals prefer each other to their mates. They proved this result by proposing an algorithm, namely the deferred acceptance (DA) algorithm, which always produces a stable matching. In this algorithm, which has two symmetric versions with respect to the roles of men and women, individuals in one side of the population, say men, make proposals and individuals in the other side, women, give deferred acceptances or rejections. The algorithm with men proposing stops after any step where no man is rejected or every single man has already proposed to every acceptable women in his preference list. The DA algorithm, with various extensions for many-to-one matching, has been used in many markets, even before it was proposed by Gale and Shapley (1962).Footnote 1 Many of these works have put meaningful restrictions on preferences of individuals who aim to match with each other. For example, in the student placement problem (Balinski and Sönmez 1999) where a centralized clearinghouse implements a stable matching between colleges and students using some version of the DA algorithm, preferences of colleges over students are usually assumed to commonly derive from some general exam scores of students. On the other hand, in the same problem students are typically allowed to have imperfectly correlated preferences over colleges, reflecting—in addition to the publicly known performance rankings of colleges—students’ subjective assessments about the various attributes of colleges, such as academic rigor, research opportunities, job market placements, campus size and location, student body, cost of education, scholarships, accommodations, etc. However, despite the fact that many works in the matching theory assume, like in the case of student placement problem, different degrees of preference correlations for the two sides of the matching population, the impacts of a change in preference correlations on stable matchings is still unexplored. A second gap in the matching literature is that it is lacking an explanation as to how individuals decide on which potential mates in the population they find acceptable as a spouse. Rather, the existing literature implicitly assumes that each individual is born with the knowledge about the set of unacceptable mates and this knowledge remains unchanged during mate search. Obviously, this assumption is too strong and also unrealistic. In practice, an individual can simply decide whether a potential mate is acceptable or not by checking whether the estimated ‘value’ of this potential mate, i.e., a real number reflecting the aggregated quality of all his/her traits, is above or below his/her own aspiration (goal) level; i.e., the average quality (mate value) of a spouse that he/she can hope to be matched with. Clearly, individuals who perceive themselves to be more (less) compatible with the high mate value profiles, must have higher (lower) aspirations. In the mate search literature, individuals base their mating aspirations on the likelihood of realizing them, about which they learn during a series of social interactions called dating. Individuals who are rejected (not found desirable) by their dates lower their self-esteem, consequently causing them to lower their aspiration (threshold) levels in selecting mates. Conversely, positive signals of desirability from their dates boost the self-esteems of individuals and cause them to raise their mating aspirations. Motivated by the aforementioned gaps in the stable matching theory, in this paper we will study how the stable matching outcome of the DA algorithm (where men propose and women accept/reject) is affected by the degree of correlation in the preferences of individuals and by the intensity of their learning about their aspirations (and correspondingly about the set of unacceptable mates) during a pre-matching stage. While our research question is, to the best of our knowledge, novel for the literature on stable matchings, a related question has already been studied in a strand of literature on mutual sequential mate search where the focus is only the random formation of ‘acceptable’ matchings consistent with some demographic patterns. One of the pioneering works in this literature is due to Todd and Miller (1999), from whom we will borrow some part of our model. Basically, the mate search model of Todd and Miller (1999) involves two phases, an adolescence phase followed by a mating (or matching) phase. In the adolescence phase, individuals randomly meet a number of dates of the opposite gender and after each meeting they mutually exchange information as to whether they have found each other acceptable; i.e., whether each individual finds the mate value of the interacted date above his/her own aspiration level. Using the exchanged information, individuals update their aspirations after each instance of dating, following an adjustment (updating) rule commonly adopted by the whole population. After the adolescence phase is over, individuals enter the mating phase, where they randomly interact with potential mates, and using their aspirations finalized at the end of the adolescence phase they decide on whether to make a proposal to any interacted individual. Individuals mutually proposing to each other become mated and they are removed from the pool of unmated individuals. The mating phase ends after a stage in which the pool of unmated individuals becomes empty or shrinks to a set in which no man and woman are mutually acceptable (and can form a mated couple). Todd and Miller (1999) used the mate search model outlined above to find socially desirable and simple mate search rules (or heuristics) that can be used by individuals in a given matching population to update their aspirations after each dating in the adolescence phase. To compare the performances of several search rules they proposed, they introduced three success measures, namely the mating likelihood measured by the number of mated pairs in the population, the mating balancedness measured by the average mate value of all mated pairs, and the (potential) stability of mating measured by the mean within-pair difference in mate value. In the past, several works have investigated whether mate search models similar to that of Todd and Miller (1999) can produce demographic patterns consistent with the real data (see, for example, Hills and Todd 2008; Todd et al. 2005; Simão and Todd 2003, among others), while some other works have dealt with the robustness of Todd and Miller’s model (1999) or with its improvement. For example, Shiba (2013) and Saglam (2014) studied whether the findings of Todd and Miller (1999) are robust with respect to a change in their assumption that the whole matching population uses the same adjustment rule in the adolescence phase. In another robustness study that is also related to ours, Saglam (2019) relaxed Todd and Miller’s (1999) assumption of homogenous (or perfectly correlated) preferences to study the effect of heterogeneity in preferences on the performances of several aspiration adjustment rules. In the same literature, a second study that is closely related to our paper is due to Saglam (2018), who proposed a new adjustment rule, called Take the Weighted Average with the Next Desiring Date (TWAND), and compared its performance to those of the rules proposed by Todd and Miller (1999) for mutual sequential mate search with random acceptable matchings. For his analysis, Saglam (2018) also added the speed of mating to the list of success measures considered by Todd and Miller (1999), and showed that even though his new rule, TWAND, does not perform well with respect to mating stability, it is more balanced than the majority of the adjustment rules of Todd and Miller (1999), and moreover “... in terms of mating likelihood almost as good as, and in terms of mating speed always better than, the most successful—yet also unrealistic—heuristic of Todd and Miller (1999), namely the Mate Value-5 rule, which assumes that individuals in the mating population completely know their own mate values before interacting with any date.” (Saglam 2018; p. 122) In this paper, we consider a mate search model integrating various structures in Todd and Miller (1999), Saglam (2019), and Saglam (2018) with the one-to-one matching model of Gale and Shapley (1962). Basically, our model involves two phases, namely the adolescence and matching phases. We borrow the adolescence phase from the mate search model of Todd and Miller (1999); however, instead of their (aspiration) adjustment rules we adopt the adjustment rule of Saglam (2018), TWAND, because of its aforementioned superior features. On the other hand, we borrow the matching phase in our model from Gale and Shapley (1962). Specifically, we assume that matchings in our model are obtained by the application of the DA algorithm (with men proposing); hence matchings are always stable in our model (with respect to the stability notion of Gale and Shapley (1962) described previously), unlike in the matching model of Todd and Miller (1999), where matchings are obtained through the mutual acceptable proposals of randomly meeting individuals in the matching phase and hence they are not necessarily stable. Also, we allow for, like in Saglam (2019), heterogenous as well as homogenous preferences to study—through some Monte Carlo simulations—the effects of the degree of correlation in preferences and the frequency of aspiration adjustments on the performance of the DA algorithm using four performance measures, three of which we borrow from Todd and Miller (1999) and the remaining one from Saglam (2018). The rest of our paper is organized as follows. Section 2 introduces our mate search model, and Sect. 3 presents our simulation results. Finally, Sect. 4 concludes.",3
57.0,2.0,Computational Economics,04 February 2020,https://link.springer.com/article/10.1007/s10614-020-09970-8,An Intelligent System for Insider Trading Identification in Chinese Security Market,February 2021,Shangkun Deng,Chenguang Wang,Mingyue Wang,Unknown,Unknown,Unknown,Unknown,,
57.0,2.0,Computational Economics,04 March 2020,https://link.springer.com/article/10.1007/s10614-020-09973-5,A New Appraisal Model of Second-Hand Housing Prices in China’s First-Tier Cities Based on Machine Learning Algorithms,February 2021,Lulin Xu,Zhongwu Li,,Unknown,Unknown,Unknown,Unknown,,
57.0,2.0,Computational Economics,14 March 2020,https://link.springer.com/article/10.1007/s10614-020-09972-6,Optimizing Algorithmic Strategies for Trading Bitcoin,February 2021,Gil Cohen,,,Male,Unknown,Unknown,Male,"First introduced to the world in January 2009, Bitcoin has dominated the new field called cryptocurrencies. The idea of issuing digital currency without a central bank or a single other administrator or intermediaries has fascinated investors around the globe. Since its introduction, Bitcoin’s value has risen and fallen with a volatility that has made it difficult for investors to achieve positive gains on their investment. As a result, investors have sought algorithms to help them customize their investment systems for better results. We investigated the usefulness of three well-established strategies for trading financial assets in achieving this goal: the relative strength index (RSI), the moving average convergence/divergence (MACD), and pivot reversal. Using data about daily Bitcoin prices from the beginning of April 2013 until the end of October 2018, we explored these strategies through particle swarm optimization. Our results demonstrate that the relative strength index produced poorer results than the buy and hold strategy. In contrast, the MACD and pivot reversal strategies dramatically outperformed the buy and hold strategy. However, our optimizing process produced even better results.",5
57.0,2.0,Computational Economics,14 March 2020,https://link.springer.com/article/10.1007/s10614-020-09974-4,Stationarity Statistics on Rolling Windows,February 2021,Joseph Ross,,,Male,Unknown,Unknown,Male,"Stationarity is a fundamental concept in time series modeling. In the approach of Box et al. (2015), for example, a stationarity test is applied to a time series to determine if some differencing operation should be performed. Once the series has been made stationary (e.g., by passing to the series of successive differences, or by seasonal differencing), one fits a model (e.g., an autoregressive one) to the transformed series. The fitted model is used to forecast the transformed series, and forecasts of the original series are obtained by inverting the transformations. A statistical test for level-stationarity and trend-stationarity of time series, now called the KPSS statistic, is proposed in the paper of Kwiatkowski–Phillips–Schmidt–Shin (Kwiatkowski et al. 1992), see especially page 165, equation (13); it serves as a complement to the unit root test of Dickey and Fuller (1979). In the particular implementation of automatic ARIMA modeling of Hyndman and Khandakar (2008, p. 10), for example, one uses repeated KPSS tests to determine the number of differencing operations to perform. Time series model selection and estimation is often conceived of as an offline procedure, perhaps with periodic “refreshes” on new batches of data. However, in many industrial applications (e.g., processing machine sensor data), the characteristics of the time series may not be stable over time, and both the choice of model and estimation of parameters may become “stale” somewhat quickly. Additionally, in these settings, it is not uncommon for a time series to report at 1-second intervals, and for the relevant historical data to be no more than a few weeks old (with the majority being much more recent). By contrast, forecasting practice often focuses on series spanning much longer durations and reporting much less frequently. In the M4 Competition announced in November 2017, for example, 95% of the 100,000 time series have a monthly (or coarser) frequency; hourly frequency is the finest in this dataset, and appears only 414 times (Makridakis 2017). This paper is a step towards making model refresh a fully online procedure. Practically it allows for a forecaster to automatically detect degradation of various modeling choices. More precisely, we devise an incremental–decremental method of calculating the KPSS stationarity statistic on rolling windows: we maintain the statistic on a rolling window of fixed (but arbitrary) size while requiring constant memory overhead and a constant amount of computation for each addition/removal of a point. This means the complexity has no dependence on the window size. In particular, we do not revisit every point in the window upon every arrival. While the KPSS statistic is the case we treat in detail, the computational methods apply to other approaches to stationarity testing (and time series analysis more broadly); see Sect. 5.2 for indications concerning unit root tests. The computational efficiency we gain is of minor significance if one is interested in forecasting hourly series for which there are several years of data; with even modest computing resources, it is feasible to revisit the entire series very many times upon the arrival of every point. To perform non-trivial time series modeling on several days’ worth of 1-second data, on the other hand, the computational efficiency is of great importance. The efficiency gains may also be interesting on a large database of infrequently sampled series. Applications Suppose s is a time series and let \(\Delta (s)\) denote the series of first differences of s. If the KPSS level-stationarity statistic of s (over some window) is small, one may reasonably use a rolling mean or exponentially weighted moving average (Brown 1956) to form forecasts. To detect anomalies, one may calculate, for example, the z-score of a point (relative to a trailing window of data, whose mean and standard deviation can easily be calculated in an incremental–decremental fashion). If the KPSS stationarity statistic of s is large but that of \(\Delta (s)\) is small, one would prefer linear projection or double exponential smoothing (Holt 2004) for forecasting, and compare forecasts to observed values to perform anomaly detection. (These proposals may be viewed as implementations of the Box–Jenkins methodology wherein the ARMA modeling is quite coarse.) Similar remarks apply if one replaces \(\Delta (s)\) with the result of applying seasonal differencing to s, or some composite transformation. Keeping around several candidate transformations and comparing the KPSS statistics of the resulting series to guide forecasting is a form of dynamic model selection. The present paper allows this to be carried out in the online setting. The referees suggested the present work has an application to (online) ARF(I)MA forecasting. Asymptotically, Lee and Schmidt (1996) shows the KPSS statistic can distinguish short memory from long memory stationary processes [e.g., I(d) for \(d \in (0, {1 \over 2})\)], and that in finite samples, the power of the test increases with the parameter d of the alternative hypothesis (Lee and Schmidt 1996, §4). This suggests the KPSS statistic can be used as a measure of the degree of fractional integration, and that the present method allows for changes in the degree of fractional integration to be dynamically incorporated into the forecasting methodology. The KPSS statistic is particularly suitable for this application; Diebold and Rudebusch (1991) shows that the Dickey-Fuller tests have low power against fractionally integrated alternatives. The referees also pointed out that incremental–decremental rolling window methods provide a framework for efficient computation of moving block bootstrap estimators (see, e.g., Kunsch 1989). This is reminiscent of (Cauwenberghs and Poggio 2001, §3), which develops an incremental–decremental method of training a support vector machine, then obtains efficient evaluation of the leave-one-out generalization performance as a consequence. Variants In addition to the unit root test of Dickey and Fuller (1979), we mention the unit root test of Leybourne and McCabe (1994) (see also Leybourne and HcCabe 1999) which shares with the KPSS test a formulation in which stationarity appears as the null hypothesis (see Sect. 5.2). There is now a sizable literature studying properties of the KPSS statistic, and proposing generalizations and modifications. We do not attempt to conduct a comprehensive review, but we point out some connections with the present work. There is a robust variant of the KPSS level statistic which replaces the partial sums of the residuals from the mean with partial sums of indicators (preserving just the sign of the residual from the median) (de Jong et al. 2007); this handles thick-tailed errors. A robust variant of the trend statistic appears in Pelagatti and Sen (2013), replacing least squares regression with (robust) Theil–Sen regression and using ranks. Our methodology does not seem to extend to these cases, as maintaining even the median on a rolling window cannot be accomplished with constant memory and computation on each add/remove. It would be interesting to obtain incremental–decremental expressions of approximations of these robust variants. The KPSS test is generalized to test the null hypothesis of zero mean stationarity in Hobijn et al. (2004); our methods apply to this case as well (see Remark 4.2). Whereas the original KPSS test involves a regression against a trend line (possibly forced to have slope zero), Phillips and Jin (2002) shows the KPSS test extends without change to regression against seasonal effects terms. Our methods can be extended to this setting since one can obtain closed form expressions for the regression coefficients, the regression residuals, and hence the squares of the partial sum residual process. One of the contributions of Hobijn et al. (2004) is to analyze the effect of using the quadratic spectral window in the KPSS statistic, and, for both windows, the effect of using the automatic bandwidth selection procedure of Newey and West (1994). It would be interesting to extend our methodology to other windows, and more speculatively, to dynamic bandwidth selection methods. Some of the difficulties in modeling stationarity are discussed in Carrion-i Silvestre and Sansó (2006), with a particular focus on the estimation of the long-run variance. Scope This project arose from the author’s work on a commercial software product that facilitates analytics on time series of metrics produced by computing infrastructure (e.g., CPU utilization) with the goal of monitoring the health of that infrastructure. The work described here has been incorporated into that product. We wish to acknowledge that our approach is motivated by the requirements of this streaming analytics system, and may be unnecessary in more standard (offline, smaller scale) settings. Aside from infrastructure and application performance monitoring, we expect that “internet of things” sensor data, as well as financial markets data, are likely to benefit from Box–Jenkins type modeling and to impose similar systems requirements. We also wish to emphasize that our contribution is computational rather than inferential. In particular, the ability to update the computation with every observation should not be interpreted as a proposal to declare (non-)stationarity on the basis of a single observation. A threshold for separating stationary from non-stationary series might be calibrated to reflect, for example, those combinations of intensity and duration of mean shift relevant for the particular series being monitored. In any case, our goal is to enable timely inferences of (non-)stationarity, i.e., to identify the observation that tips the balance in favor of a hypothesis, not to make inferences that are unduly influenced by one observation. Outline Section 2 describes the rolling window framework and requirements, establishes some notation, and offers several formulations of our approach. Section 3 derives an incremental–decremental solution to the rolling (least squares) regression problem which is needed for the calculation of the KPSS statistic. Section 4.1 defines the KPSS statistic (a quadratic functional of partial sums of residuals from a regression, appropriately normalized) and is followed by the algorithms for incremental–decremental calculations of both the level (Sect. 4.2) and trend (Sect. 4.3) variants of the statistic. Through Sect. 4, our estimate of the squared error (which appears in the “denominator” of the statistic) assumes the errors are independent and identically distributed, and we are able to handle nulls. In Sect. 5, we handle possible covariance among the errors under the assumption there are no null values in the series, using the Bartlett window with a fixed bandwidth (as in Kwiatkowski et al. 1992); and we also discuss how to extend our methods to unit root tests.",
57.0,2.0,Computational Economics,20 March 2020,https://link.springer.com/article/10.1007/s10614-020-09976-2,An Integrated Quasi-Monte Carlo Method for Handling High Dimensional Problems with Discontinuities in Financial Engineering,February 2021,Zhijian He,Xiaoqun Wang,,Unknown,Unknown,Unknown,Unknown,,
57.0,2.0,Computational Economics,20 March 2020,https://link.springer.com/article/10.1007/s10614-020-09979-z,Futures Hedging in CSI 300 Markets: A Comparison Between Minimum-Variance and Maximum-Utility Frameworks,February 2021,Qianjie Geng,Yudong Wang,,Unknown,Unknown,Unknown,Unknown,,
57.0,2.0,Computational Economics,23 March 2020,https://link.springer.com/article/10.1007/s10614-020-09978-0,Functional Fuzzy Rule-Based Modeling for Interval-Valued Data: An Empirical Application for Exchange Rates Forecasting,February 2021,Leandro Maciel,Rosangela Ballini,,Male,Female,Unknown,Mix,,
57.0,2.0,Computational Economics,30 March 2020,https://link.springer.com/article/10.1007/s10614-020-09977-1,R-Squared-Bootstrapping for Gegenbauer-Type Long Memory,February 2021,Yixun Xing,Wayne A. Woodward,,Unknown,Male,Unknown,Male,"The well-known discrete stationary models in time series include autoregressive (AR) and autoregressive-moving average (ARMA) models. Stationary ARMA and AR processes have autocorrelations \(\rho _{k}\rightarrow 0\) as \(k\rightarrow \infty \). Because of the exponential decay of the autocorrelations, AR(p) and ARMA(p, q) processes are short memory processes. The spectrum of an ARMA(p, q) process is \(P\left( f\right) =\sigma _{a}^{2}\frac{|\theta \left( e^{-2\pi if}\right) |^{2}}{|\phi \left( e^{-2\pi if}\right) |^{2}}.\) In contrast to short memory, a stationary discrete time series is long memory if \(\rho _{k}\) decays hyperbolically as \(k\rightarrow \infty \), e.g. \(\rho _{k}\sim k^{-\alpha }\) as \(k\rightarrow \infty \) for some \(0\le \alpha \le 1\). As a result, the autocorrelation of a stationary long memory processes is seen to decay much more slowly than in the ARMA case. Also, the power spectrum P(f) becomes unbounded for some frequency, \(f\in \left[ 0,0.5\right] \) (see Woodward et al. (2017)). The fractional integration (FI) and Gegenbauer models are the basic and most common models used to model long memory behavior. However, different from the FI model whose spectrum has an infinite peak at zero, the power spectrum of the Gegenbauer (GB(\(u,\lambda \))) model can be unbounded at any frequency between 0 and 0.5, i.e. \(f\in \left[ 0,0.5\right] \), as shown in Gray et al. (1989) and Woodward et al. (2017). To define the Gegenbauer process, the Gegenbauer polynomial is first defined in Definition 1.1. The Gegenbauer polynomial is defined by where \(\left( \alpha \right) _{n}=\frac{\varGamma \left( \alpha +n\right) }{\varGamma \left( \alpha \right) }\), and \(\lfloor x\rfloor \) denotes the integer part of x. \(X_{t}\) is said to be a Gegenbauer process if it satisfies where \(\sum _{j=0}^{\infty }|C_{j}^{\left( \lambda \right) }\left( u\right) |<\infty \). If \(X_{t}\) is invertible, then the Gegenbauer model is written where \(|u|\le 1\), \(\lambda \le 0.5\), \(\mu \) is the process mean, and \(a_{t}\) is the white noise. The theoretical spectrum of a Gegenbauer process is \(P(f)=\sigma _{a}^{2}[4(\cos 2\pi f-u)^{2}]^{-\lambda }.\) The pole of the spectrum appears at frequency \(f_{G}=\frac{\cos ^{-1}\left( u\right) }{2\pi }\) which is called the Gegenbauer frequency (G-frequency). The autocorrelation function of a Gegenbauer process satisfies \(\rho _{k}\thicksim k^{2\lambda -1}\cos \left( 2\pi f_{G}k\right) \) as \(k\rightarrow \infty \) if \(|u|<1\) and \(0<\lambda <0.5\), i.e. it has the appearance of a hyperbolically decaying cosine curve. A Gegenbauer ARMA (GARMA) model is the combination of Gegenbauer and ARMA components shown in Definition 1.3. As a result, a GARMA process allows both short and long memory behaviors, where the long memory pattern tends to dominate in the presence of the mixed behaviors for moderate to large lags (see Woodward et al. 2017). A GARMA(\(p,\lambda ,u,q\)) process is where roots of \(\phi (z)=0\) and \(\theta (z)=0\) all lie outside the unit circle while \(\phi (z)\), \(\theta (z)\) share no common factors. Part of the problem of identifying a model for a stationary time series is to determine whether to use a short or long memory model. Many model identification approaches are built on the basis of parameter estimators in the time, frequency, or wavelet domains. 
Geweke and Porter-Hudak (1983), henceforth GPH, initially designed a semi-parametric periodogram-based regression method to estimate the fractional integration parameter d of a FARMA model. Generally, they constructed the regression equation where \(I_{X}(\omega _{j})\) is the periodogram (that is the sample spectrum at the harmonic frequencies \(\omega _{j}=2\pi j/N\) where \(j=1,\ldots ,g\left( N\right) \), and \(g\left( N\right) \) is a function of the realization length with \(lim_{N\rightarrow \infty }g\left( N\right) /N=0\)). The appropriate choice of \(g\left( N\right) \) has been discussed in many articles and is usually in the form of \(\lfloor N^{\alpha }\rfloor \), i.e. the integer part of \(N^{\alpha }\). Geweke and Porter-Hudak (1983) considered \(g\left( N\right) =N^{0.5}\) the best choice. However, Hurvich et al. (1998) and Hurvich and Deo (1999) showed \(g\left( N\right) =O(N^{4/5})\) minimized the mean squared error of the estimator of d and increased the efficiency of test. The parameter d is estimated using the ordinary least squares (OLS) estimator in the simple linear regression of the periodogram on \(\ln \left[ 4\sin ^{2}\left( \omega _{j}/2\right) \right] \). Ford (1991) applied the regression estimator on various long memory models and achieved reasonable estimates for a GARMA process when u is known. Other estimators of the fractional parameter d include the GPH-Tapered (GPHT) estimator by Hurvich and Ray (1995) and smoothed log-periodogram regression (SPR) (see Reisen 1994; Reisen et al. 2001; Boutahar et al. 2007). Gray et al. (1989) traded the problem of maximizing the GARMA likelihood with that of maximizing an ARMA likelihood by defining \(W_{t}=(1-2uB+B^{2})^{\lambda }X_{t}.\) Note that \(\phi \left( B\right) W_{t}=\theta \left( B\right) a_{t}\) satisfies an ARMA model. A grid search procedure was implemented to search for the \(\lambda \) associated with the largest maximum likelihood. Whitcher (2004) suggested an approximate maximum likelihood estimator in the wavelet domain as an alternative. Boubaker (2015) then showed an improvement of the Whitcher estimator by applying a generalized variance portmanteau test. Some test statistics, other than the discussed estimators above, that are able to capture the long memory features have been proposed to distinguish long from short memory. Lo (1991) developed a non-parametric long memory test (for FI type long memory) based on a modified rescaled range statistic (MRR) that is defined as the ratio of the range and modified standard deviation 
Giraitis et al. (2003b) derived the asymptotic distribution of the R/S-type statistics including MRR, and KPSS, V/S under short and long memory hypothesis. The further empirical test results by Giraitis et al. (2003a) demonstrate the R/S tests are useful for FI models but could only detect long memory when the Gegenbauer frequency is less than 0.0071, i.e. essentially 0. Overall, the methods for Gegenbauer-type long memory (i.e. poles in the spectrum in (0, 0.5]) are very limited and not well developed so far. Our study focuses on developing a better test for Gegenbauer long memory detection. The paper is organized as follows. Section 2 introduces the R-squared statistic for the Gegenbauer-type long memory detection, and Sect. 3 describes the testing procedure and the simulation design. Section 4 discusses the simulation results of the R-squared-bootstrapping test. Additionally, we consider the application to real time series (the lynx data) in Sect. 5 and conclude in Sect. 6.",1
57.0,3.0,Computational Economics,15 May 2018,https://link.springer.com/article/10.1007/s10614-018-9812-y,Preface Special Issue of Computational Economics Commemorating the Birth Centennial of Herbert Simon,March 2021,K. Vela Velupillai,Ragupathy Venkatachalam,,Unknown,Unknown,Unknown,Unknown,,
57.0,3.0,Computational Economics,12 April 2018,https://link.springer.com/article/10.1007/s10614-018-9811-z,"Herbert Alexander Simon: 15th June, 1916–9th February, 2001 A Life",March 2021,K. Vela Velupillai,Ragupathy Venkatachalam,,Unknown,Unknown,Unknown,Unknown,,
57.0,3.0,Computational Economics,03 April 2018,https://link.springer.com/article/10.1007/s10614-018-9808-7,Reconsidering Herbert A. Simon’s Major Themes in Economics: Towards an Experimentally Grounded Capital Structure Theory Drawing from His Methodological Conjectures,March 2021,Edgardo Bucciarelli,Nicola Mattoscio,,Male,Female,Unknown,Mix,,
57.0,3.0,Computational Economics,04 January 2021,https://link.springer.com/article/10.1007/s10614-020-10081-7,Correction to: Reconsidering Herbert A. Simon’s Major Themes in Economics: Towards an Experimentally Grounded Capital Structure Theory Drawing from His Methodological Conjectures,March 2021,Edgardo Bucciarelli,Nicola Mattoscio,,Male,Female,Unknown,Mix,,
57.0,3.0,Computational Economics,03 February 2018,https://link.springer.com/article/10.1007/s10614-018-9798-5,"Big Data, Scarce Attention and Decision-Making Quality",March 2021,Tongkui Yu,Shu-Heng Chen,,Unknown,Unknown,Unknown,Unknown,,
57.0,3.0,Computational Economics,12 February 2018,https://link.springer.com/article/10.1007/s10614-018-9797-6,Human Problem-Solving: Standing on the Shoulders of the Giants,March 2021,Dharmaraj Navaneethakrishnan,,,Unknown,Unknown,Unknown,Unknown,,
57.0,3.0,Computational Economics,13 February 2018,https://link.springer.com/article/10.1007/s10614-018-9799-4,Performance Budget Planning: The Case of a Research University,March 2021,M. J. Druzdzel,J. R. Kalagnanam,,Unknown,Unknown,Unknown,Unknown,,
57.0,3.0,Computational Economics,21 February 2018,https://link.springer.com/article/10.1007/s10614-018-9803-z,Human and Machine Learning,March 2021,Ying-Fang Kao,Ragupathy Venkatachalam,,,Unknown,Unknown,Mix,,
57.0,3.0,Computational Economics,10 February 2018,https://link.springer.com/article/10.1007/s10614-018-9801-1,Information Processing and Moral Problem Solving,March 2021,Cassey Lee,,,Unknown,Unknown,Unknown,Unknown,,
57.0,3.0,Computational Economics,09 February 2018,https://link.springer.com/article/10.1007/s10614-018-9800-2,Why Herbert Simon Matters for Policymaking,March 2021,Patrick Love,,,Male,Unknown,Unknown,Male,"Speaking at a NAEC seminar in November 2017, Jean-Claude Trichet, President of the European Central Bank at the time of the crisis, reiterated the harsh criticisms he made in 2010 of the information and advice he was getting from economic models as the crisis unfolded. “When the crisis came, the serious limitations of existing economic and financial models immediately became apparent. Arbitrage broke down in many market segments, as markets froze and market participants were gripped by panic. Macro models failed to predict the crisis and seemed incapable of explaining what was happening to the economy in a convincing manner. As a policy-maker during the crisis, I found the available models of limited help. In fact, I would go further: in the face of the crisis, we felt abandoned by conventional tools. In the absence of clear guidance from existing analytical frameworks, policy-makers had to place particular reliance on our experience. Judgement and experience inevitably played a key role.” (Trichet 2017) The belief, described above, that securitisation had enabled a transfer of risk to investors who were well-placed to bear the risk had two major consequences. First, it resulted in an increasing interconnectedness across financial institutions, while the globalisation of the financial sector increased the connectedness of financial institutions across countries. Second, it gave the impression that the risk had diminished. It hadn’t, but the individual shares in the risk, and responsibility for managing it had become diffuse, creating a sense of security that proved illusory. The financial crisis spread rapidly around the globe and reached the real economy, resulting in dramatic drops in stock markets and decreases in business and consumer confidence affecting all economic operators. Financial institutions were unwilling to lend to each other, while households cut back their consumption and started saving more; access to credit became more difficult and more expensive, undermining corporate investment especially among small businesses. As Trichet pointed out at the OECD, the crisis showed how interconnectedness, one of the very strengths that had allowed the economy to expand, could be just as potent in provoking or aggravating its downfall. The crisis also revealed how the tools economists used were not good enough. Bill White, chair of the OECD’s Economic and Development Review Committee (EDRC) argues that the dominant school of economic thought prior to the crisis essentially modelled the national economy as a totally understandable and changeless machine (DSGE models). Moreover, the machine almost always operated at its optimal speed, churning out outputs in an almost totally predicable (linear) way, under the close control of its (policy) operators. But rather than being a machine, for White the economy should be viewed as a complex adaptive system, with massive interdependencies among its parts and the potential for highly nonlinear outcomes (White 2017). Several characteristics of complex systems are particularly relevant for understanding the crisis. First, all complex systems fail regularly; that is, they fall into crisis. There were 195 stock-market crashes and 84 depressions between 1860 and 2006. Moreover, the distribution of outcomes is commonly determined by a power law. Big crises occur infrequently while smaller ones are more frequent. There were big crises in 1825, 1873 and 1929, as well as smaller ones more recently in the Nordic countries, Japan and South East Asia. Second, the trigger for a crisis is irrelevant. It could be anything, perhaps even something trivial in itself. It is the system that is unstable. Governor Bernanke of the US Federal Reserve originally estimated that the losses from the subprime crisis would not exceed 50 billion dollars and they would not extend beyond the subprime market. Similarly, how could difficulties in tiny Greece in 2010 have had such far-reaching and lasting implications for the whole Eurozone? Its GDP was only around $305 billion compared with over $16 trillion for the EU as a whole Third, complex systems can result in very large economic losses much more frequently than a normal distribution would suggest. Moreover, large economic crisis often lead to social and political instability. And finally, such systems evolve in a path-dependent way and there is no equilibrium to return to. White is not the only expert on finance to champion a complexity approach. Andy Haldane, Chief Economist at the Bank of England reminds us that although there is no generally-accepted definition of complexity, “that proposed by Herbert Simon in The Architecture of Complexity—‘one made up of a large number of parts that interact in a non-simple way’—captures well its everyday essence. The whole behaves very differently than the sum of its parts. The properties of complex systems typically give rise to irregular, and often highly non-normal, statistical distributions for these systems over time. This manifests itself as much fatter tails than a normal distribution would suggest. In other words, system-wide interactions and feedbacks generate a much higher probability of catastrophic events than Gaussian distributions would imply.” (Haldane 2017). Haldane discusses Simon’s idea that for evolutionary reasons of survival of the fittest, ‘decomposable’ networks are more resilient and hence more likely to proliferate, although Haldane does not think the idea captures all we need to know to deal with economic and social change: “By decomposable networks, he meant organisational structures which could be partitioned such that the resilience of the system as a whole was not reliant on any one sub-element. This may be a reasonable long-run description of some real-world complex systems, but less suitable as a description of the evolution of socio-economic systems. ... if these hyper-connected networks do face systemic threat, they are often able to adapt in ways which avoid extinction. For example, the risk of social, economic or financial disorder will typically lead to an adaptation of policies to prevent systemic collapse. These adaptive policy responses may preserve otherwise-fragile socio-economic topologies. They may even further encourage the growth of connectivity and complexity of these networks. Policies to support “super-spreader” banks in a crisis for instance may encourage them to become larger and more complex. The combination of network economies and policy responses to failure means socio-economic systems may be less Darwinian, and hence decomposable, than natural and biological systems.” (Haldane 2017) Another Andy, Andy Lo, is particularly interested in the adaptive, evolutionary aspects. His latest book, Adaptive Markets: Financial Evolution at the Speed of Thought (Lo 2017) points out that while economic and financial crises throughout history exhibit many similarities, they also have many differences. In part this is due to adaptive human behaviour, both in markets and on the part of regulators, in response to previous crises. Lo theorises this in his “adaptive markets hypothesis”, according to which financial market dynamics are driven by our interactions as we behave, learn, and adapt to each other, and to the social, cultural, political, economic, and natural environments in which we live; and survival is the ultimate force driving competition, innovation, and adaptation. This builds on earlier work in which he argues that “by extending Herbert Simon’s notion of satisficing with evolutionary dynamics, much of what behavioralists cite as counterexamples to economic rationality - loss aversion, overconfidence, overreaction, mental accounting, and other behavioral biases - are, in fact, consistent with an evolutionary model of individuals adapting to a changing environment via simple heuristics.” (Lo 2004) Simon is also referred to in Rick Bookstaber’s fundamental critique of traditional economic modelling (Bookstaber 2017), where he comes back to Trichet’s problem of connectedness. Again, this builds on earlier work, where he reminds us that: “Connectedness measures how one action can affect other elements of a system. A simple example of connectedness is the effect of a failure of one critical node on the hub-and-spoke network of airlines. Dynamic systems also emerge from the actions and feedback of interacting components. Herbert Simon posed a fractal-like measure of system complexity by looking at the layering of hierarchy. That is, the depth to which a system is composed of subsystems, which in turn are composed of yet deeper subsystems.” (Bookstaber 2011). He sees four problems with conventional economic models. The first of these is computational irreducibility. You may be able to reduce the behaviour of a simple system to a mathematical description that provides a shortcut to predicting its future behaviour, the way a map shows that following a road gets you to a town without having to physically travel the road first. Unfortunately, for many systems, you only know what is going to happen by faithfully reproducing the path the system takes to its end point, through simulation and observation, with no chance of getting to the final state before the system itself. It’s a bit like the map Borges describes in On Rigor in Science, where “the Map of the Empire had the size of the Empire itself and coincided with it point by point”. Not being able to reduce the economy to a computation means you can’t predict it using analytical methods, but economics requires that you can. The second characteristic property is emergence. Emergent phenomena occur when the overall effect of individuals’ actions is qualitatively different from what each of the individuals is doing. You cannot anticipate the outcome for the whole system on the basis of the actions of its individual members because the large system will show properties its individual members do not have. For example, some people pushing others in a crowd may lead to nothing or it may lead to a stampede with people getting crushed, despite nobody wanting this or acting intentionally to produce it. Likewise no one decides to precipitate a financial crisis, and indeed at the level of the individual firms, decisions generally are made to take prudent action to avoid the costly effects of a crisis. But what is locally stable can become globally unstable. (And as the physicists have found, when you try to optimise a complex system, it becomes unstable.) The name for the third characteristic, non-ergodicity, comes from the German physicist Ludwig Boltzmann who defined as “ergodic” a concept in statistical mechanics whereby a single trajectory, continued long enough at constant energy, would be representative of an isolated system as a whole, from the Greek ergon energy, and odos path. The mechanical processes that drive of our physical world are ergodic, as are many biological processes. We can predict how a ball will move when struck without knowing how it got into its present position—the past doesn’t matter. But the past matters in social processes and you cannot simply extrapolate it to know the future. The dynamics of a financial crisis are not reflected in the pre-crisis period for instance because financial markets are constantly innovating, so the future may look nothing like the past. Radical (or Knightian) uncertainty completes Bookstaber’s quartet. As Keynes put it, “There is no scientific basis to form any calculable probability whatever. We simply do not know.” For Bookstaber, Knightian uncertainty, where we can’t know everything that would be needed to calculate the odds is in fact is the human condition. He puts it this way in his 2017 book: “As the example of my daily life suggests, it is absurd to think that we optimize. So economists tone down this critical assumption to say that we act as if we are optimizing. Or we optimize subject to our constraints on cognitive ability and available information ...The ‘subject to our constraints’ argument suggests Herbert Simon’s satisficing approach.” For Bookstaber, the reality of humanity means that a mechanistic approach to economics will fail.",1
57.0,3.0,Computational Economics,06 April 2018,https://link.springer.com/article/10.1007/s10614-018-9810-0,Proofs and Predictions in Human Problem Solving,March 2021,K. Vela Velupillai,,,Unknown,Unknown,Unknown,Unknown,,
57.0,4.0,Computational Economics,09 June 2020,https://link.springer.com/article/10.1007/s10614-020-10000-w,An Expanded Local Variance Gamma Model,April 2021,P. Carr,A. Itkin,,Unknown,Unknown,Unknown,Unknown,,
57.0,4.0,Computational Economics,12 June 2020,https://link.springer.com/article/10.1007/s10614-020-10001-9,On the Extension of the Kiyotaki and Wright model to Transformable Goods,April 2021,Sacha Bourgeois-Gironde,Marcin Czupryna,,,Male,Unknown,Mix,,
57.0,4.0,Computational Economics,22 June 2020,https://link.springer.com/article/10.1007/s10614-020-10005-5,Exploring Option Pricing and Hedging via Volatility Asymmetry,April 2021,Isabel Casas,Helena Veiga,,Female,Female,Unknown,Female,"Options and other derivatives provide investors with an insurance against a loss or a gain above a certain level. The option price depends crucially on a precise forecast of the underlying asset volatility, which is known to be time-varying over the time to maturity. Therefore, the assumption of constant volatility of the Black–Scholes model is unarguably unrealistic. On the other hand, stochastic volatility models, such as the Heston model, have been an important part of the financial derivatives literature for a few decades. They incorporate two sources of uncertainty: one over the underlying returns whose diffusion term might be that of a CIR model, and the other over its volatility. In addition, the shape of the returns innovation distribution, such as the size of its tails and skewness, should not be disregarded in an efficient risk management practice. In this paper, we assume that the standardised returns innovations follow either a normal distribution or a Student-t distribution. One important stylised fact in option pricing is the volatility “smirk”, which consists of higher market prices for in-the-money call prices (and out-of-the-money put prices) than those calculated from the Black–Scholes model. The volatility “smirk” is often modelled with continuous time SV models which allow for a negative correlation between the return level and its volatility, known as the leverage effect (see, e.g., Bakshi et al. 1997; Nandi 1998; Bates 2000; Chernov and Ghysels 2000; Pan 2002; Jones 2003). According to Christoffersen et al. (2009), the leverage effect is important for option pricing because it increases the probability of a large loss and, consequently, the value of in-the-money call prices and out-of-the-money put options. It leads returns to have a negative skew distribution and produces asymmetric volatility “smirks” (Renault 1997). The different responses of the volatility to negative and positive past returns have been actively investigated in the literature (see Black 1976; Christie 1982; Engle and Ng 1993, among others) and the main conclusions report that the asymmetric response of the volatility has extreme relevance for asset pricing, portfolio selection and risk management. For instance, Duan (1995) and Heston and Nandi (2000) show that an option contract may be mispriced if the volatility asymmetry is not accurately specified.Footnote 1 The literature on financial econometrics proposes several discrete time volatility models that incorporate asymmetry into the volatility specifications. These can be classified into two main families. The first family is composed of asymmetric GARCH models, such as the E-GARCH by Nelson (1991) and the GJR-GARCH by Glosten et al. (1993). The second family consists of SV models, where the volatility is a latent process and consequently non-observable, as it depends on information from the past and on a random innovation term. It has been shown that SV models provide more flexibility in simultaneously capturing the high kurtosis of financial returns and the high volatility persistence (Carnero et al. 2004). Several asymmetric SV models co-exist in the literature. Taylor (1994) and Harvey and Shephard (1996) propose the asymmetric autoregressive stochastic volatility (AARSV) model, whose asymmetric volatility is defined by the correlation between the disturbances of the asset returns and its volatility. Some applications and extensions of this model can be found in Asai and McAleer (2011), McAleer (2005), Yu et al. (2006), Asai (2008), Tsiotas (2012) and Yu (2012). On the other hand, Breidt (1996) and So et al. (2002) propose the threshold stochastic volatility (TSV) model, where the volatility level depends on whether past returns are positive or negative (see also Asai and McAleer 2006). More recently, Mao et al. (2020) propose a general ASV model which nests all the aforementioned parametric SV models. In the nonparametric setup, Yu (2012) proposes an ASV model, where the volatility asymmetry is time varying and depends nonparametrically on the type of news arrived to the market. This proposal is in spirit similar to that by Wu and Xiao (2002), but it differs from the latter because it does not assume an additive functional form for the volatility asymmetry and uses different nonparametric methods. In this paper, we study extensions of the AARSV and TSV for option pricing. Several studies have worked on the valuation of options with GARCH volatility, such as Christoffersen and Jacobs (2004) and references therein. The advantage is that volatility is observed one step ahead and therefore is easy to estimate, but the implications of using GARCH models for option valuation are not obvious. Note that option pricing is often based on SV models. Furthermore, as Heston and Nandi (2000) mention, not all GARCH specifications have closed-form solutions for option pricing and have to be evaluated through simulation, which is computationally intensive. Some studies that use simulation for option pricing are Bakshi et al. (1997), Benzoni (2002), Yu et al. (2006) and Stentoft (2011). On the other hand, Badescu et al. (2016) study the use of the basic symmetric autoregressive SV (ARSV) for option pricing and hedging and conclude that it outperforms standard GARCH models. Jiang and van der Sluis (1999) also show that allowing for stochastic volatility can reduce the pricing errors. In the present paper, SV models are estimated using the Just Another Gibbs Sampler (JAGS) algorithm developed by Plummer (2003). This software is publicly available and user-friendly, but since it is based on a single-move Gibbs sampling algorithm, it is not simulation efficient (see Meyer and Yu 2000; Yu 2005, 2012, for similarities to the BUGS implementation of SV models). Yet, Meyer and Yu (2000) and Yu (2005) find that the simulation-efficiency is less of a problem for ASV models than for the basic SV model. Our simulation study in Sect. 3 shows that the JAGS produces reliable parameter estimates of all the models in the paper. Our results show that considering volatility asymmetry in SV models does not affect the accuracy of option price forecasts and dynamic delta hedging for mid-cap equity options. However, it benefits the accuracy of option price forecasting and hedging cost effectiveness in the large-cap equity sector, especially for short maturities and during highly volatile periods, such as the one including the global financial crisis. In this case, the type of volatility asymmetry that fares the best is the one that is incorporated in the TSV model. When the aim is hedging, conclusions depend on moneyness and market volatility. For periods of low volatility, the AARSV is the model with the smallest hedging effective costs for in-the-money and at-the-money options, whereas for out-of-the-money options the TSV is better. Regarding periods of high volatility, the models that provide the lowest effective costs are the basic SV and the TSV models, especially for short maturities. As the time to maturity increases, the basic SV model with Gaussian errors is the favoured one, regardless of the moneyness. This paper contributes to the current literature on option pricing with stochastic volatility in several ways. First, we propose a novel Bayesian approach in this literature to estimate SV models. This approach is open for public access, user-friendly and easy to implement. Second, we explore the effect of two types of volatility asymmetry in forecasting option prices and dynamic delta hedging, and compare their performances with that of the basic ARSV model. Finally, we use the S&P 500 and S&P MidCap 40 series of daily returns to measure the effect of volatility asymmetry in two different size equity sectors and in two periods which correspond to different levels of market risk. This is of extreme relevance for investors, financial institutions and firms dealing with options, and to our knowledge, it has not been studied before. The paper is organised as follows. In Sect. 2, we introduce the stochastic volatility models for option pricing. Section 3 motivates the potential effects of volatility asymmetry on option pricing, and it presents the MCMC estimation methodology and the results from a simulation study to analyze the properties of the estimator in finite samples. In Sect. 4, all volatility models are estimated from the S&P 400 (mid-cap equity) and S&P 500 (big-cap equity) series. These parameter estimates of the SV models are used to forecast option prices, and the three model option price forecasts are compared with the model confidence set procedure. In addition, the delta hedging distributions of all models are compared visually. Finally, we provide conclusions and discuss further lines of research in Sect. 5.",2
57.0,4.0,Computational Economics,26 June 2020,https://link.springer.com/article/10.1007/s10614-020-10008-2,Multi-Factor RFG-LSTM Algorithm for Stock Sequence Predicting,April 2021,Zhi Su,Heliang Xie,Lu Han,,Unknown,,Mix,,
57.0,4.0,Computational Economics,29 June 2020,https://link.springer.com/article/10.1007/s10614-020-10002-8,Variance Swaps with Deterministic and Stochastic Correlations,April 2021,Ah-Reum Han,Jeong-Hoon Kim,See-Woo Kim,Unknown,Unknown,Unknown,Unknown,,
57.0,4.0,Computational Economics,01 July 2020,https://link.springer.com/article/10.1007/s10614-020-10009-1,Nonparanormal Structural VAR for Non-Gaussian Data,April 2021,Aramayis Dallakyan,,,Unknown,Unknown,Unknown,Unknown,,
57.0,4.0,Computational Economics,01 July 2020,https://link.springer.com/article/10.1007/s10614-020-10006-4,Two-Sided Matching with Indifferences: Using Heuristics to Improve Properties of Stable Matchings,April 2021,Christian Haas,,,Male,Unknown,Unknown,Male,"The allocation of resources based on non-monetary preferences is a widespread and important challenge in markets where monetary allocations are not desirable or possible. In such markets, participants express their valuations through preference rankings and an allocation mechanism determines which participants are being matched to each other based on these rankings. This general concept is applied in a variety of settings. In the US National Residents Matching Program (NRMP) and similar programs in Canada and Scotland, medical interns are allocated to hospitals by expressing their mutual preferences (Abdulkadiroglu and Sönmez 2013). In school choice problems, students and schools express their preferences over available options to determine student placement in cities such as Boston, Minneapolis, or Seattle (Roth and Sotomayor 1992; Abdulkadiroglu and Sönmez 2003). As not everyone can be allocated to their favorite choice, matching mechanisms are used to find allocations that satisfy certain desirable characteristics, such as stability, fairness, Pareto efficiency, or incentive compatibility. Further examples of preference-based matching include the allocation of (computational) resources being shared in a social setting (Caton et al. 2014), student-project allocations in university settings (Abraham et al. 2007), or the matching of mentors to mentees in professional development scenarios (Haas et al. 2018; Haas and Hall 2019). For the previously mentioned examples, and the allocation of resources based on preferences in general, the field of Two-Sided Matching is a successful and established approach to find such allocations. Similarly to monetary markets, one side provides resources (e.g., school or college places) and the other side wants to “consume” the available resources. In contrast to monetary markets, however, the allocation decision has to be derived based on preference rankings, not on a willingness to pay. That is, each side ranks participants of the other side in an ordinal ranking, where a higher rank indicates that a match would be preferred. Over time, a variety of different mechanisms have been developed for these matching markets that create allocations based on certain criteria. Some of these algorithms assume that preferences are strict, i.e., each participant is always able to provide a strict priority structure between two options. However, in many practical scenarios such a restriction can be overly prohibitive. For a variety of reasons participants might be indifferent between certain choices, and in markets with a large number of options the effort to rank each individual option separately and uniquely might be prohibitively expensive. In addition, legal reasons (e.g., affirmative action) can lead to providers (schools, hospitals, etc.) ranking applicants in separate indifference classes, thus creating ties in the preferences (Erdil and Ergin 2017). The existence of indifferences between choices has been observed in several scenarios. In the residents matching programs, hospitals were observed to be indifferent between (a subset of) medical interns (Irving and Manlove 2008). Students in a school choice mechanism in Amsterdam sometimes provide the same numerical score to multiple schools even though they are allowed to provide a unique ranking for each school (de Haan et al. 2015). Preferences from conferences that use a Two-Sided Matching to allocate reviewers to papers exhibit substantial indifferences (Mattei and Walsh 2013). Similarly, preferences collected from applications such as Mentor-Mentee Matching also provide evidence that a substantial number of participants can be indifferent between multiple options (Haas and Hall 2019). From an allocation perspective, the inclusion of indifferences and the addition to potentially incomplete rankings, i.e., participants not providing a ranking for each option, leads to computational and optimization challenges. If indifferences between options are allowed, finding an optimal allocation, in general, will be NP-hard and even difficult to approximate for certain allocation criteria (Halldórsson et al. 2007). On the other hand, while efficient mechanisms exist for finding certain types of solutions, the most general form of Two-Sided Matching relies on approximation algorithms or heuristics to find matchings with desirable properties due to the mentioned NP-hardness. Previously, a set of different approximation algorithms and local search procedures have been suggested for calculating allocations to the general Two-Sided Matching formulation when indifferences and incompleteness are allowed. While they offer guarantees about specific properties, e.g., the minimum number of matched participants, their actual (average) performance is unclear and they are designed for a specific combination of allocation objectives (such as finding a large stable match). Previous work shows that there can be many different allocations that satisfy the stability criterion but differ with respect to other criteria, and suggested mechanisms often focus on finding a stable allocation that optimize one or more other criteria. Finding allocations that are stable and improve upon other solutions with respect to additional criteria such as fairness or number of matched participants also directly translates into benefits for the participants in the matching market. To determine and evaluate a wider set of potential allocations, this article proposes the use of heuristic algorithms for calculating solutions to the general Two-Sided Matching problem. Specifically, it considers two heuristic approaches, a Genetic Algorithm (GA) and a Threshold Accepting (TA) algorithm, as well as their combination with respect to their ability to find allocations with improved properties in the case of complete or incomplete preferences with indifferences. They also offer the flexibility to include multiple objectives when calculating allocations. Heuristic approaches have been used for matching problems before, e.g., GAs for strict (Kimbrough and Kuo 2010) and incomplete preferences (Haas 2014), and GAs to find fair solutions (Nakamura et al. 1995). In contrast to previous work in this field, this article specifically considers the case of non-strict, not necessarily complete preferences. Overall, this article provides a systematic evaluation of the allocation properties provided by the suggested heuristic approaches compared to other alternative allocation mechanisms. Properties in this case refers to criteria such as stability, fairness, or number of matched participants. For the case of (both complete and incomplete) preferences with indifferences, the evaluation of the allocation properties shows that heuristics can find significantly improved allocations for generalized Two-Sided Matching problems by providing substantial improvements on secondary objectives such as the fairness or average matched rank characteristics of the solutions. The structure of the article is as follows. Section 2 overviews related work. Section 3 describes the general Two-Sided Matching formulation and defines matching properties, and Sect. 4 describes several Two-Sided Matching algorithms and heuristics. After introducing the simulation environment used to systematically evaluate the algorithms in Sects. 5, 6 and 7 present the evaluation results which show that in many circumstances heuristics can improve upon previous mechanisms, especially for objectives such as average matched rank. Finally, Sect. 8 discusses the findings and future work.",3
57.0,4.0,Computational Economics,02 July 2020,https://link.springer.com/article/10.1007/s10614-020-10007-3,Entropy of Graphs in Financial Markets,April 2021,Chun-Xiao Nie,Fu-Tie Song,,,Unknown,Unknown,Mix,,
57.0,4.0,Computational Economics,16 July 2020,https://link.springer.com/article/10.1007/s10614-020-10014-4,Testing for Time-Varying Properties Under Misspecified Conditional Mean and Variance,April 2021,Daiki Maki,Yasushi Ota,,Male,Male,Unknown,Male,"Many economic and financial time-series data have time-varying properties. Their properties are roughly classified into two types. One is a property about the conditional mean. Constant and/or autoregressive parameters change with time as time-varying properties for the mean. For example, Lundbergh et al. (2003) developed a time-varying smooth transition autoregressive model and illustrated empirical applications to U.S macroeconomic data. In addition, Holt and Balagtas (2009) explored the possibility of time-varying properties for the mean of U.S. meat demand. Another time-varying property is the conditional variance of the error term for a regression model. Variance of the error term (i.e., volatility) is frequently modeled by autoregressive conditional heteroskedasticity (ARCH). As introduced by Dahlhaus and Rao (2006), a property of the time-varying model for volatility is that the parameters of volatility models change with time. Amado and Teräsvirta (2013) and Kim and Kim (2016) recently provided a time-varying volatility model and its applications. Linearity tests are usually used to investigate time-varying properties. When we test for time-varying properties of the conditional mean, we assume a homoskedastic variance or a correctly specified linear variance process for the error term of the conditional mean. When we test for time-varying properties of the conditional variance, we assume a correctly specified linear process for a mean. However, it is difficult to know a priori which time-varying property for the mean or variance is present. This challenge implies that researchers may erroneously test for time-varying properties of the mean, although volatility actually has time-varying properties. Similarly, researchers may erroneously test for time-varying properties of the variance, although the mean actually has time-varying properties. In fact, Pitarakis (2004) and Perron and Yamamoto (2019) showed that a test for a change in regression coefficients (variance) that ignores or misspecifies the presence of change in variance (regression coefficients) causes poor statistical properties. For the influence of ARCH on inference of misspecified models, Lumsdaine and Ng (1999) showed overrejection of ARCH tests in the presence of misspecified conditional mean models. Van Dijk et al. (1999b) demonstrated size distortions of ARCH tests when a process has additive outliers. Balke and Kapetanios (2007) pointed out that spurious ARCH effects appear when nonlinearity of the mean is ignored. If we were to perform erroneous tests and fail to obtain reliability from the derived results, it would be difficult to construct and evaluate the correct model. Accordingly, it is important to clarify the influence of tests on the misspecified models. This study examines the influence of time-varying tests on misspecified conditional mean and variance. In particular, we clarify the statistical performance of tests for time-varying variance when a process has time-varying mean with homoskedastic variance. We also investigate statistical performance of tests for time-varying mean when a process has time-varying variance with a linear mean model. Some studies, including Lumsdaine and Ng (1999), Van Dijk et al. (1999a), and Balke and Kapetanios (2007), have investigated problems of misspecified models in ARCH tests. However, previous studies have not clarified the influence of time-varying tests on misspecified conditional mean and variance. As mentioned above, it is important to clarify time-varying properties correctly. This study uses a logistic smooth transition function to model time-varying mean and variance. This model smooths threshold or structural break models in time. The time-varying tests used in this study are based on the method introduced by Luukkonen et al. (1988a). They developed linearity tests using a Taylor series approximation to overcome the identification problem pointed out by Davies (1977, (1987). The simulation results in this study provide evidence that the asymptotic test for time-varying mean has size distortions when the conditional variance model is misspecified. However, the wild bootstrap method introduced by Liu (1988) improves the size distortions. In fact, Becker and Hurn (2009) provided evidence that the wild bootstrap improves size properties when the conditional mean is being tested. We can test for time-varying mean by using the wild bootstrap without depending on the form of volatility. When we test for time-varying variance in the presence of a misspecified conditional mean, asymptotic tests have large size distortions. The properties are not improved by the use of the bootstrap ARCH test proposed by Gel and Chen (2012) and the wild bootstrap. The results show that the wild bootstrap tests for time-varying mean are robust regardless of the misspecified conditional variance, whereas tests for time-varying variance do not perform well in the presence of misspecified conditional mean. The remainder of this paper is organized as follows. Section 2 presents the tests for time-varying mean and variance. Section 3 provides statistical properties of the tests for time-varying mean and variance in the presence of misspecified conditional models. Section 4 illustrates empirical applications. Finally, Sect. 5 summarizes and concludes. Value of the transition function with \(T=200\)",2
57.0,4.0,Computational Economics,18 July 2020,https://link.springer.com/article/10.1007/s10614-020-10016-2,Embedding Four Medium-Term Technical Indicators to an Intelligent Stock Trading Fuzzy System for Predicting: A Portfolio Management Approach,April 2021,Konstandinos Chourmouziadis,Dimitra K. Chourmouziadou,Prodromos D. Chatzoglou,Unknown,Female,Unknown,Female,"The aim of this paper is to propose a model consisting of a small number of coherent, trend following, medium-term technical indicators with a novel fuzzyfied trading strategy and an intelligent stock trading fuzzy system appropriately designed to accommodate and implement these two characteristics and output on a daily basis the percent of total portfolio’s assets that should be invested for the medium-term. The trend following technical indicators which have been chosen are two moving averages with different parameters, the classic MACD indicator and the directional movement in one of the many alternative setups for this indicator. Financial markets are complex systems involving people affected by many interrelated economic, political and psychological factors (Manahov and Hudson 2014; Lan et al. 2011) that cause many uncertainties in financial markets and, consequently, affects stock prices. Various techniques have been used for stock price forecasting (Dymova et al. 2012), while there were claims that markets are characterised by inherent nonlinearities (Saadi et al. 2006; Antoniou et al. 1997). As a result, relatively recently, a variety of artificial intelligence techniques have also been proposed, that can prevail over the obstacle of market noise and nonlinearity of prices (Chang et al. 2011) and constitute the basis of a trading system (Briza and Naval 2011). Soft computing techniques are nonlinear and can approximate complex systems (Melin et al. 2007). Therefore, they can capture the nonlinear behaviour of stock markets (Atsalakis and Valavanis 2009). In soft computing, computation, reasoning and decision making exploit, wherever possible, the tolerance for imprecision and uncertainty (Zadeh 1994), highlighting thus its usefulness for overcoming complex problems (Soto 2007). One of the soft computing techniques is fuzzy logic (Kumar and Ravi 2007), the basic principle of which is to “exploit the tolerance for imprecision, uncertainty and partial truth to achieve tractability, robustness, low solution cost and better rapport with reality” (Zadeh 1997, p. 123). Technical analysis studies historical prices and volumes in order to forecast future stock values. The forces of supply and demand and the changing attitudes of investors, tend to move stock prices in trends, which alter only when there are changes in these forces. Since all external economic, political and psychological factors are reflected in past prices, using appropriate technical indicators each one of which provides its own perspective of the market dynamics, it is sufficient to predict future prices. These indicators are created by a mathematical formula based on the past sequence of prices and they continually evaluate the strength of the prevailing trend identifying any changes (Cheng et al. 2010; Bao and Yang 2008; Apostolou and Apostolou 2004). Artificial intelligence techniques have been increasingly applied to technical analysis (Zhang and Maringer 2016) and, amongst these techniques, fuzzy models can be used in a satisfactory way to address issues such as subjective assessment (Bekiros 2010b), in order to provide an appropriate solution. Various scholars (Anbalagan and Maheswari 2015; Ko et al. 2014; Araque et al. 2008) use technical indicators which are calculated using closing prices only, thus important information contained in other daily price data, such as high and low, are not considered. We argue that the use of technical indicators like Directional Movement, which is calculated using high, low and close of daily prices, helps to make more comprehensive the information on which decisions are based and, they might improve the predictive power of the models. Various researchers use many technical indicators and argue that by combining the specific characteristics of each indicator they have a more global perspective of technical indicators’ outcome (Fadlalla and Amani 2014; Kara et al. 2011; Pereira and Tettamanzi 2008; Zhai et al. 2007). However, the simultaneous use of technical indicators with different properties and uneven characteristics might have unexpected results, since they might negate each other. We argue that a more careful selection of indicators with similar characteristics, but with different way of arriving to their outcome, ensures that a stronger and more justified prediction will be made. This paper uses a novel trading strategy, initially presented by Chourmouziadis and Chatzoglou (2016). In order to empirically test this strategy, it was chosen in the fore-mentioned paper to use an amalgam of different short term technical indicators. On the contrary, in this paper it has been chosen to use well established trend following technical indicators, in a medium-term perspective. It was decided to use these indicators, although predictability in financial markets is short-lived, which is more true with such widely used indicators. Τhey were, therefore, a good candidate to investigate the performance of the proposed strategy, and even more so in a medium-term perspective, in which had not been tried this strategy so far. This novel trading strategy is based on the classic crossover strategy of technical analysis, where a buy (or sell) signal is generated when prices are becoming higher (or lower) than the technical indicator (Papailias and Thomakos 2015; Powers and Castelino 1991). However, Alfi et al. (2006) notes that price dynamics is affected by their distance from a moving average and, therefore, the previous classic strategy is extended by an observation made by Papoulias (1990), according to whom, when the space between price and technical indicator is maintained broad or becomes broader, the strength of the current move of the market is reaffirmed, while when this space shortens (creates a stenosis) the current move is approaching to end. Thus, the initial classic strategy was differentiated to a new strategy, which was further fuzzyfied with the notion of the degree of certainty, as follows (Chourmouziadis and Chatzoglou 2016). When prices are becoming higher (lower) than the technical indicator, then a buy (sell) signal is generated (If Cl > Ti => Buy, If Cl < Ti => Sell). This signal remains valid till the opposite signal is given. Further, as the distance (stenosis) between the price and the technical indicator narrows progressively, the degree of certainty of the present order (buy or sell) decreases as well, while increases the degree of certainty that the opposite order (sell or buy) will occur eventually. On the contrary, when the space between any of the technical indicators and the price increases, the degree of certainty of the present order increases as well, while decreases the degree of certainty that the opposite order will occur soon enough. This paper contributes to the literature in a number of ways. First, it uses a small number (four) of well known, trend following, medium-term technical indicators, which are rarely used together. Although that a) there are some reports suggesting that each one of these technical indicators separately produces returns that fall short from the buy and hold (B&H) strategy (Colby 2003), b) scholars claim that efficient technical indicators are kept secret (Pinto et al. 2015), c) predictability in financial markets is short-lived because of the self-destruction process (as discussed by Timmermann and Granger 2004), while portfolio returns are 58% lower post-publication (McLean and Pontiff 2016) and, moreover, d) the common practice is to use many indicators (Lee et al. 2014; Zarandi et al. 2009), the specific four very common indicators have been chosen as predictors in this research in order to support the claim that the careful choice of a small number of technical indicators with similar, but complimentary, characteristics, can make the difference and increase dramatically the acquaint performance. Before their use, all chosen technical indicators’s parameters have been adjusted for increased performance in medium-term periods. Furthermore although that most indicators are calculated only with the close of the daily data (da Costa et al. 2015), in order to avoid missing the information contained in the high and low of the daily prices, they are also used for the calculation of one of the chosen indicators (directional movement). Second, although the interpretation of technical analysis techniques is subjective (Dymova et al. 2012), the use of technical indicators as mathematical calculations should be precise (Metastock Professional 2002). This paper suggests a creative way to overcome these controversial tendencies, by transforming a classic strict trading strategy to a novel diversified strategy with a fuzzyfied approach, through the use of an appropriately designed fuzzy system. This strategy is differentiated from previous studies in that each buy/sell signal is a result of the comparison between the distance of the price and the relevant technical indicator, not in a purely mathematical way but with a more subjective and fuzzy way, by attaching to each signal a degree of certainty. Thus, the performance of technical indicators is maximized, while at the same time the final outcome of the model maintains the fuzzy and subjective characteristics of technical analysis. The proposed model was tested using the general index of the Greek (Athens) stock market, with 3.879 daily data (over 15 years), a long enough period to include a variety of market conditions. Αn additional tool was developed in order to check, identify and correct all errors of the historic data (Chourmouziadis 2004).",9
57.0,4.0,Computational Economics,20 July 2020,https://link.springer.com/article/10.1007/s10614-020-10012-6,Data-Based Automatic Discretization of Nonparametric Distributions,April 2021,Alexis Akira Toda,,,Male,Unknown,Unknown,Male,"This paper studies the following problem, which applied theorists often encounter. A researcher would like to calibrate the parameters of a stochastic model. One of the model inputs is a probability distribution of shocks, which is to be approximated by a discrete distribution. Due to computational considerations, the researcher would like this distribution to have as few support points (nodes) as possible, say five. Given the data of shocks, how should the researcher calibrate the nodes and probabilities of this five-point distribution? While there are many established methods for discretizing stochastic processes with Gaussian shocks such as Tauchen (1986), Tauchen and Hussey (1991), and Rouwenhorst (1995),Footnote 1 discretizing non-Gaussian distributions remains relatively unexplored. While models with Gaussian shocks such as the long-run risk models (Bansal and Yaron , 2004) remain popular due to analytical tractability, it has become increasingly common in economics to study models with non-Gaussian shocks. For example, the rare disasters model (Rietz 1988; Barro 2006; Gabaix 2012) uses rare but large downward jumps to explain asset pricing puzzles. Toda and Walsh (2019) calibrate a three-point distribution from dividend growth data. One issue with discretizing non-Gaussian distributions is how to calibrate them. If we have a parametric density, it is possible to discretize it using the Gaussian quadrature as in Miller and Rice (1983) or the maximum entropy method as in Tanaka and Toda (2013, 2015) provided that we can compute some moments. However, it is not obvious how to obtain an N-point distribution that approximates the data well without imposing parametric assumptions. Because the degree of freedom of an N-point distribution is large (\(2N-1\), because there are N nodes, N probabilities, and the probabilities need to add up to 1), providing an automatic discretization method is valuable because it removes the arbitrariness of calibration. Given the data, this paper proposes a simple method for automatically calibrating a discrete distribution with a specified number of grid points. The method is based on the observation that to compute the nodes and weights of the N-point Gaussian quadrature with some weighting function using the Golub and Welsch (1969) algorithm, one only needs to know the moments of the weighting function up to order 2N. Therefore a natural way to discretize a nonparametric distribution is simply to feed the 2N sample moments into the Golub-Welsch algorithm. Since this method does not involve optimization (it is a matter of solving for the eigenvalues/vectors of a sparse \(N\times N\) matrix), the implementation is easy and fast.Footnote 2 As applications, I discretize the U.S. historical data on aggregate consumption growth and stock returns and solve an asset pricing model and an optimal portfolio problem with constant relative risk aversion utility. I consider two cases in which the investor uses the nonparametric and normal densities. I show that when the investor incorrectly believes that the consumption growth/stock returns distribution is lognormal, the equity premium shrinks by up to 8% and the stock portfolio is overweighted by up to 17% because he underestimates the probability of recessions/crashes. These examples show that the choice of the calibration method may matter quantitatively. The closest paper to mine is Miller and Rice (1983), who use the Gaussian quadrature to discretize general parametric distributions. My method is different because it concerns the discretization of nonparametric distributions estimated from data. Tanaka and Toda (2013) consider the discretization of distributions on preassigned nodes by matching the moments using the maximum entropy principle, and Tanaka and Toda (2015) prove convergence and obtain an error estimate. Farmer and Toda (2017) consider the discretization of general non-Gaussian Markov processes by applying the Tanaka-Toda method to conditional distributions. In one of the applications, they discretize a nonparametric density on a preassigned grid by approximating it with a normal mixture. Since computing the nodes and weights of Gaussian quadrature does not require optimizing over parameters (unlike the maximum likelihood estimation of normal mixture parameters or solving the maximum entropy problem), my method is easier and faster to implement, and the grid is chosen endogenously. On the other hand, the Farmer and Toda (2017) method can discretize general Markov processes, whereas the proposed method in this paper is designed to discretize a single distribution.",3
57.0,4.0,Computational Economics,20 July 2020,https://link.springer.com/article/10.1007/s10614-020-10013-5,Predicting Stock Price Using Two-Stage Machine Learning Techniques,April 2021,Jun Zhang,Lan Li,Wei Chen,,,,Mix,,
57.0,4.0,Computational Economics,23 July 2020,https://link.springer.com/article/10.1007/s10614-020-10018-0,Extreme Wavelet Fast Learning Machine for Evaluation of the Default Profile on Financial Transactions,April 2021,Paulo Vitor de Campos Souza,Luiz Carlos Bambirra Torres,,Male,Unknown,Unknown,Male,"The financial market has peculiar characteristics about the way of acting within the branch of risk analysis. Features linked to problems with people who do not meet their financial obligations affect markets, and various types of business-related processes (Saunders and Thomas 1997). Efficient analysis of the profile of customers who carry out financial transactions can assist in preventing non-compliance with future payment obligations. Financial institutions are continually suffering from a lack of payment on bonds, which ultimately leads to an increase in the basic direct consumer interest rate. Much of that interest rate charged on financial market products such as overdraft is directly connected to the risk of non-compliance with financial obligations. To avoid major problems with various products in the economic market, companies need to modernize the risk analysis of financial transactions, allowing possible fraud profiles to be detected, allowed this transaction to be prevented (Wang 2010). Some personal data are involved in credit card transactions and online shopping performed on the internet. Data protection and techniques (Zhao et al. 2018) to prevent these types of information from being disseminated incorrectly help to reduce fraud in the financial system. In several countries around the world, credit card transactions worry financial institutions because, due to high unemployment (especially in emerging nations), violation of passwords and financial lack of control of the population, credit card passwords have been stolen and used in illicit transactions products that cause inconvenience to customers and responsible card management companies. Failure to pay a credit card leads to high-interest payments, lowering the consumer’s purchasing power and increasing the financial risk of credit card transactions (de Sá et al. 2018). Defining a dataset for judgments about the customer’s profile is not trivial, but has already been done by Yeh and Lien (2009). That dataset has several standard day-to-day features of financial transactions involving credit cards, where customers of such operations and elements have been labeled and defined as credible or not. This kind of information makes it possible to prevent a potentially risky agreement for financial firms. Nevertheless, because of the sheer volume of high dimensionality data of the problem, these tasks are not so easy to do by humans. Expert systems can aid in data processing through machine learning or artificial intelligence techniques. Several machine learning techniques were used to define fraud profiles in the financial market. Works like Thomas et al. (2017), Koutanaei et al. (2015) and Khanbabaei and Alborzi (2013) are examples of practical applications using data mining concepts and artificial intelligence for definitions of credit scores. However, because of the complex nature of the problem and the large volume of data processed by financial companies online, digital solutions must be fast and with an acceptable level of accuracy. Even in the literature, many works do not present the time-execution of the approaches, making them efficient at the level of correctness, but entirely out of the practice of the companies for not having answers in real time. Artificial neural network training models with multiple layers can help to define user profiles of credit cards, but updating network structure algorithms can become slow and hamper the necessary responses to take preventive measures, especially with methods based on backpropagation. Created in mid-2006 by Huang et al. (2006), the Extreme Learning Machine (ELM) presented itself as an algorithm that performs a parametric definition and for the estimation of synaptic weights of the models practically and efficiently in training models with a single hidden layer. The generation of random weights in the ELM hidden layer and the use of the least-squares concepts for the definition of the final weights of the network allow multi-layer models to perform responsibilities in a practical, fast and a single step. Nevertheless, this training model needed to be tweaked to more effectively solve problems with a massive data flow. In 2012, Huang et al. (2012) developed a way to calculate the weights of the output layer using a normalized parameter, allowing the network responses to be performed more quickly, especially when dataset used in training of the intelligent model is high. The definition of the weights through the concepts of Lagrange allowed the training of models that use the ELM to be carried out of faster form. An interesting factor that has been the subject of several types of research is the kind of activation functions of the neurons and their impact on the model’s output. They can define whether neurons will be more significant to the analyzed problem. With the advent of Big Data, new activation functions were designed for artificial neurons, supporting the accuracy and time to solve problems. Subsequently, as the issues of risk analysis change according to the profile of the clients and consequently in the data accessible to train the models, it is intended to make the models of neural networks more adaptable to complex datasets. The wavelet transform is a technique that allows the decomposition of functions, both in the frequency domain and in the time domain (Daubechies 1990), allowing its analysis at a different frequency and time scales. This type of function allows separate frequency input data to be used, allowing these frequencies to be applied to define the weights of the hidden layer of an ELM, changing the paradigm of random definition of the weights for a representation of the synaptic weights of the neurons, allowing them to store part of the frequency domain information of the training data. This paper proposes to create a better solution for this complicated situation, the use of artificial neural networks in multiple layers where the training algorithm is the ELM introduced by Huang et al. (2006) and altered by the same author in 2012 Huang et al. (2012), using a regularizing factor. As the ELM hidden layer weights, we intend to use the wavelet transform (Daubechies 1990) to perform a definition of the weights according to the characteristics of the database, allowing the models to be more representative of their database. Finally, several activation functions can be used in the hidden layer, seeking to tailor the answers to quickly and accurately seeking to identify whether or not there is a financial risk in the transaction of a customer’s credit card. The model will be tested through the database provided by Yeh and Lien (2009), which is commonly employed in credit risk analysis problems. The main contribution of this paper is the use of a feature extraction technique to define model weighting processes, seeking to act continuously and efficiently in the classification of fraud patterns, merging the nature of dataset and the speed training to solve complex problems in a short time. The main contribution of this paper is to seek the creation of an intelligent model with a high degree of precision in its responses, and that is capable of acting with the ability to respond in real-time, which is one of the most significant complicating factors in identifying credit card fraud. The paper is organized as follows: The next section (Sect. 2) presents the concepts that guide the research and in Sect. 3 the proposal of an ELM-based intelligent algorithm for detecting fraud in credit card transactions. Section 4 shows the tests to prove the feasibility of the model proposed in the paper, highlighting the characteristics of the database used and the configurations of the models submitted to the tests of customers profile’s detections. Finally, Sect. 5 presents the conclusions relevant to the proposed approach.",1
57.0,4.0,Computational Economics,23 July 2020,https://link.springer.com/article/10.1007/s10614-020-10029-x,Option Pricing Model Biases: Bayesian and Markov Chain Monte Carlo Regression Analysis,April 2021,Sharif Mozumder,Taufiq Choudhry,Michael Dempsey,Male,Male,Male,Male,"The pricing errors in the Black–Scholes (BS) model are systematically related to moneyness and maturity. Moneyness biases include familiar smiles, smirks, and skews in which implied volatility varies across strike prices of otherwise similar options. The maturity bias is generally that of rising implied volatilities with maturity. Option pricing models aimed at extending the Black–Scholes–Merton (BSM) framework (Black and Scholes 1973; Merton 1973) have incorporated (a) pure jump, (b) jump diffusion, (c) stochastic volatility, and (d) GARCH volatility, aimed at providing a more complete explanation of option pricing. The popularity of Bayesian and Markov Chain Monte Carlo (MCMC) methods in option pricing models is evident in various applications.Footnote 1 MCMC methods provide a means of estimating models for which classical methods of solution, such as the Generalized Method of Moments (GMM) and Maximum Likelihood, are too computationally demanding. In this paper, we show how regressions based on MCMC methods provide information on the variability of regression coefficients, and the extent to which biases in option pricing are reflected in replications. We find that the MCMC regression provide a useful tool in checking the effect of non-normal models empirically over normal models. In addition, MCMC is useful in estimating nonlinear models with high-dimensional integrals and many latent variables. To the best of our knowledge, this is the first paper to utilize a MCMC regression method to study the option pricing model biases. We consider four representative option pricing models which are considered as alternative to the Black–Scholes–Merton (BSM) framework. These are (a) pure jump Lévy models (Geman 2002; Carr et al. 2002), (b) jump-diffusion models (Kou 2002), (c) stochastic volatility models (Heston 1993), and (d) the GARCH approach (Heston and Nandi 2000). In pure jump models, jumps in the underlying asset are of various sizes, and such jumps are made sufficiently frequent as to capture diffusion. Here, we consider Lévy models of the Variance Gamma (VG) class. The VG model introduces skewness and kurtosis through a stochastic time changing feature of asset pricing. Excess kurtosis in such models may result from the implicit stochastic volatility induced by time changing (Geman 2002). The jump-diffusion models allow the movement of underlying assets to be sufficiently small as to capture diffusion, with occasional discrete jumps of various magnitudes superimposed on the diffusive paths. Secondly, we consider the finite activity jump-diffusion Lévy model-double exponential model (JD-DE) proposed by Kou (2002). Thirdly, in the case of stochastic volatility, we use the continuous time stochastic volatility model (SV) of Heston (1993) as well as Heston’s stochastic volatility (SV) model, which is a discrete time stochastic volatility model that uses the CIR process (Cox et al. 1985) for the stochastic element of volatility. Finally, we use the closed form GARCH model (CFG) of Heston and Nandi (2000), which uses a GARCH (1, 1) structure to update daily volatilities. We compare the distinction between normal and non-normal models based on the difference between model and market prices. If the difference is less than one we identify such biases as systematic in ‘moneyness’ and ‘maturity’, whereas if such difference is more than one but less than five, the deviation is assumed to be unexpected as an outcome of unsystematic ‘maturity’ and ‘moneyness’ biases in the model. We find that the regression methodologies cannot effectively distinguish the difference in systematic biases between normal and non-normal models. In case of unsystematic biases, even though the explanatory power of the regression models are similar, the MCMC regressions identify the variability of the estimated parameters, which can be interpreted as a metric to decide on estimation performance in replication. Nonetheless even small advantage from enhanced prediction accuracy is of importance especially in a domain of arbitrage and of taking substantially large positions. Therefore this study is not merely of academic importance but have relevance in practical applications too. The paper is structured in the following way. Section 2 deals with the regression methodologies. Section 3 considers data and calibration issues. Our empirical findings are discussed in Sect. 4. Section 5 concludes.",
57.0,4.0,Computational Economics,23 July 2020,https://link.springer.com/article/10.1007/s10614-020-10020-6,Gender and Bubbles in Experimental Markets with Positive and Negative Expectation Feedback,April 2021,Zhou Lu,Te Bao,Xiaohua Yu,,,Unknown,Mix,,
57.0,4.0,Computational Economics,24 July 2020,https://link.springer.com/article/10.1007/s10614-020-10021-5,How Connected is Too Connected? Impact of Network Topology on Systemic Risk and Collapse of Complex Economic Systems,April 2021,Aymeric Vié,Alfredo J. Morales,,Male,Male,Unknown,Male,"As highly complex and networked systems, the properties of economies are characterized by the behavior and interdependencies of their components (Kremer 1993; Bar-Yam 1997; Hidalgo et al. 2007; Barrat et al. 2008). Whether they arise from investments, trade or supply chains, interdependencies are increasingly important in contemporary economic systems, and fundamental for risk assessment and evaluation (Schweitzer et al. 2009). Interconnections enable the diversification of outputs, improve efficiency of economies, and increase the growth of economic complexity (Hidalgo et al. 2007). However, at the same time, they also introduce paths for risk contagion and generate large-scale vulnerabilities to systemic failure (Harmon et al. 2010, 2011). Strategic interdependencies, or complementarities, can generate aggregate volatility (Jovanovic 1987). Given the current context of increasing international trade, financialization and globalization in economies, it is crucial to understand the effects of connectivity on networked economies and its relationship to economic collapse. Traditional economic studies focus on explaining collapse by the contribution of different factors such as bankruptcy (Battiston et al. 2007), bank loans (Stiglitz and Greenwald 2003), inter-bank credits (Allen and Gale 2000), and changes of asset prices (Kiyotaki and Moore 1997). Risk contagion in supply chains and production networks has seen important analyses on credit contagion (Jorion and Zhang 2009), default contagion in financial networks (Elliott et al. 2014), liquidity risk (Cifuentes et al. 2005), and development disruption (Brummitt et al. 2017). How idiosyncratic shocks propagate at the macroeconomic level, and the structure of production networks has been studied notably by Ciccone (2002), Levchenko (2007), Jones (2011) and Levine (2012), a literature surveyed by Carvalho (2014). Roukny et al. (2018) show that interconnections in bank systems through credit contracts and subject to correlated external shocks constitute a source of uncertainty in systemic risk assessment. Additional studies have indeed emphasized on the need for understanding the impact of network structure on the probability of collapse (Schweitzer et al. 2009; Iyer et al. 2013; Albert et al. 2000). Battiston et al. (2012b) emphasized the identification of important nodes through feedback centrality with debt ranking. Billio et al. (2012) established important econometric statistics to measure connectedness, and evaluate system risk in financial sectors. Recently, Elliott et al. (2020) linked supply network formation and fragility in this context. These studies show an inherent relationship between the structure of the network and its robustness and vulnerability to selected attacks and random errors independently of their nature. Stressing the systemic complexity of economic networks may contribute to the design and implementation of policies leading to higher diversity and efficiency without undermining the robustness of economic systems (Schweitzer et al. 2009; Battiston et al. 2012b). In this paper we develop a model to show that while the creation of interdependencies among economic agents is fundamental for the growth of economic complexity, it also amplifies the risk of collapse during adverse conditions, and we characterize how the structure of the interdependencies shape the macroeconomic variables of the system. We show that the structure of interconnections among economic agents increases the fragility of economic systems despite an apparent improvement of their production complexity. We explore multiple ways in which systems can be interconnected. The first one is the density of the network, which refers to the number of connections that are drawn among agents independently. The second one is the network centralization, which refers to the emergence of highly connected nodes that bridge across large parts of the network. The third one is synthesized models of supply chain networks, which create instances of typical multilayer supply chains. Finally, we apply our models to international trade and supply chain networks created using real data. The results on realistic simulations and real data are consistent with the more generalized network framework. The networks based on density and centralization simplify and generalize previous literature specifications, focusing on a large variety of theoretical and empirical networks. Studying network density and centralization allows to account for this variety of structures in an unified framework. While different in nature, these directions show how nodes can become increasingly dependent on one another, either directly or indirectly (through secondary connections). We found that the transition to collapse is universal and independent of a specific network structure, including those that are built from real data. Instead the transition results from the accessibility of nodes one to another and the contagion of their failure. We put an emphasis on the impact of network structure over systemic risk and aggregate productivity, across many different network structures, leaving out of this paper the question of network formation in the presence of shock. In the context of production webs, see Levine (2012), Rostek and Weretka (2015), Acemoglu and Azar (2017) in endogenous production networks, Brummitt et al. (2017); Baqaee (2018) for an influential model of cascading failures in production networks, Bimpikis et al. (2019), Yang et al. (2019). Cabrales et al. (2017), Elliott et al. (2018), Erol et al. (2018), Erol and Vohra (2018), Jackson and Pernoud (2019) studied network formation in finance. More specifically, Kranton and Minehart focused on buyer-seller networks, and Gale and Kariv (2009) on intermediation relationships. Link formation with contagion of risk was studied by Blume et al. (2011), who early identified the trade-off at the heart of our model: “each agent receives benefits from the direct links it forms to others, but these links expose it to the risk of being hit by a cascading failure that might spread over multi-step paths”, concluding that optimal networks were situated right beyond a phase transition in the behavior of cascading failure. In a related approach, we take a broader look at the impact of network structure on cascading failures, without restricting our results to the identification of optimal networks. The architecture of economic networks is crucial to study their efficiency and vulnerabilities to systemic failure (Jackson et al. 2017), and full financial integration may not be desirable as it creates new risks (Stiglitz 2010). For example, if a vital firm within a supply chain suddenly ceases to exist, all producers linked to the failing element become unable to produce their output. The present analysis aims at quantifying both robustness and performance of production networks for various levels of individual node failure, and for various degrees of density and centralization in the network. Buldyrev et al. (2010), Tang et al. (2016) and Yang et al. (2019) studied different transmission mechanisms in supply chains, interdependent networks, giving rise to phase transitions. Interconnections may under certain conditions increase the complexity of production. However, as we introduce a non-zero probability of failure, the expected diversity and productivity decreases, leading the way to economic collapse. Even in most standard models of network production such as Acemoglu et al. (2012), such fragilities exist (Elliott et al. 2020), and motivated associations between physics of self-organized criticality and economics in the building of “sandpile” macroeconomic models (Scheinkman and Woodford 1994). There is a large literature on network robustness to internal failure and external attacks on nodes or edges. Previous studies have mainly focused on two particular network topologies: the Erdos–Rényi random graph (Erdos and Rényi 1960) and Barabási’s scale-free network (Barabási and Albert 1999; Barabási and Bonabeau 2003). Albert et al. (2000) studied error tolerance and attack impact, notably testing both web robustness to targeted attacks on well connected nodes, and to removal of a given fraction of nodes. Crucitti et al. (2003) study network robustness to failure and targeted attacks. Iyer et al. (2013) likewise analyze how interconnections structure evolves with the removal of vertices, for a variety of networks types. Lorenz et al. (2009) developed a general framework to describe systemic risk with cascading failures processes in networks through node fragility. Pichler et al. (2018) investigated the issue of systemic risk in the context of efficient asset allocation in the form of a network optimization problem. Caccioli et al. (2018) recently provided a thorough review of research in network models of financial systemic risk. Buldyrev et al. (2010) extended the framework of network cascading failure analysis to the case of interconnected networks transmitting failure from one to another. In the spirit of Albert et al. (2000) who focused on two models: the Erdos–Rényi random network model (Erdos and Rényi 1960) and the scale-free web (Barabási and Albert 1999; Barabási and Bonabeau 2003), the present paper extends the investigation on the robustness of networks models to a more general framework, including the transition from random to scale-free networks and further centralization, as well as the effects of density of connections. Various indicators of network robustness, and by consequence its fragility, have been extensively used in the literature. Identification of important nodes in contagion processes as occupied a central position in research on these topics (Billio et al. 2012; Battiston et al. 2012b; Thurner and Poledna 2013). The communication capacity of the network after nodes removal has been analyzed by Crucitti et al. (2003). Albert et al. (2000) used the average shortest path length among nodes in the network as a indicator of failure reachability. Iyer et al. (2013) and Callaway et al. (2000) analyzed robustness as percolation efficiency on networks. Lorenz et al. (2009) used the fraction of stable nodes after removing the failed ones as a measure of systemic risk. Rather than providing an indicator of fragility, we show the space of possible behaviors of networked production systems in terms of diversity of outcome and probability of collapse for different scenarios regarding conditions to failure and structure of interdependencies. Collapse may be framed as a comparison of the current state of the system with respect to a reference one. We define the reference state as the situation of autarky or network-free environment, where agents have no interaction with each other. A production below such reference state could be considered as collapse, i.e. a systemic failure of the network to achieve the network-free production levels. Because the reference state is defined without knowledge of the networked structure of the system, collapse is interpreted in our model as the inability of the system to achieve the autarky production level. Our results can be generalized and are consistent with other definitions of collapse. This article is organized as follows. Section 2 describes the network generation algorithms and the production model. Section 3 presents the results of model simulations in terms of productivity and collapse probability across multiple network topologies. Their discussion, implications and relations to previous literature are provided in Sect. 4. Section 5 concludes on the impact of network connectivity structure on global production and risk of failure in economic systems.",3
57.0,4.0,Computational Economics,27 July 2020,https://link.springer.com/article/10.1007/s10614-020-10015-3,Computational Modeling of Non-Gaussian Option Price Using Non-extensive Tsallis’ Entropy Framework,April 2021,Gangadhar Nayak,Amit Kumar Singh,Dilip Senapati,Unknown,Male,Male,Male,"In the present era, the study of financial markets have extensively emerged as a cornerstone for economic growth and prediction. The dynamics of financial markets are uncertain and stochastic in nature due to insufficient information flow among the market participants. The advancements in data acquisition equipment and computational techniques facilitate us to understand the intrinsic dynamics of stock price which help for a better prediction and accurate pricing. AraúJo and Ferreira (2013) proposed a morphological-rank-linear evolutionary technique for stock market prediction which does not follow the random walk principle. Efendi et al. (2018) have presented a new approach for stock market forecasting with the help of random auto-regression time series model. Several authors (Nayak et al. 2020; Verousis et al. 2018; Eholzer and Roth 2017) have applied different techniques to characterize the well-known stylized facts associated with the high-frequency stock price data sets. It has been confirmed from the high-frequency trading (Hasbrouck 2018; Seddon and Currie 2017), that the empirical data of stock return exhibits the power-law (Riyal et al. 2016; Singh and Karmeshu 2014) which creates an interest among various researchers to define a precise model to capture the dynamics of the real data. In this perspective, Gerig et al. (2009) proposed a mixture model for the return distribution to observe the tail behavior through Student’s t-distribution which is associated positive degrees of freedom (df). However, the discrete values of ‘df’ cannot describe the tail fluctuation of high-frequency data. Thus, it becomes necessary to apply the non-extensive concept (Singh and Karmeshu 2014; Singh et al. 2015; Tavayef et al. 2018) to observe the tail behavior of the stock return. As a result, Borland (2002) and Borland and Bouchaud (2004) have used the non-extensive Tsallis’ framework in their corresponding stochastic differential equation (SDE) to capture the long range memory effets. Stanley (2003) and Buchanan (2012, (2013) have noticed that all types of stock market exhibit power-law in the tail region. Pan and Sinha (2008) examined the same for Indian market, whereas, multifractal detrended fluctuation analysis (MF-DFA) has been performed by Kumar and Deo (2009) to analyze Indian market. Jaynes (1957) and Tsallis (2004) have proposed two different entropy concepts where the maximization of Shannon entropy unable to describe tail fluctuation of stock returns. However, maximization of entropy with parameter q, successfully describes the the tail behavior of underlying stock returns. It is significant to observe the convergence of Tsallis entropy to Shannon entropy as q approaches to 1 (Mukherjee et al. 2019; Bebortta et al. 2020). Thus, Senapati and Karmeshu (2016) have proposed the q-Gaussian distribution subject to entropy optimization for intra-day stock returns (IDR) in high-frequency trading. They have also validated their proposed distribution with six intra-day high-frequency stock return data sets. As the proposed return distribution captures the tail fluctuation, it is worthwhile to calculate the CCDF for the proposed return distribution. Hence in this paper, we present a rationale mathematical expression for the CCDF in terms of Hypergeometric2F1 function to characterize tail fluctuation of underlying returns through different values of the non-extensive parameter ‘q’. We have estimated the parameter‘q’ from empirical data by minimizing the generalized symmetric Jensen Shannon (JS) measure between the return distributions and empirical histograms of six different high-frequency data. For the sake of convenience, this article also presents a precise algorithm for the estimation of parameter parameter‘q’ . Since option price relies on the underlying stock price, we have derived the underlying stock distribution in terms of q-lognormal distribution. For ‘q’ equals to one, this stock distribution also converges to the well-known log-normal distribution. Consequently, we have successfully computed the corresponding non-Gaussian option price. It is interesting to observe, our option pricing model also coincides with the famous Black–Scholes option pricing model (Black and Scholes 1973) as q tends to 1. The remaining of this paper is organized as follows: Sect. 2, describes the computation of non-Gaussian q distribution using the maximum Tsallis’ entropy framework. Section 3, provides a brief description for the computation of Kullback–Leibler (KL) and JS measure. The next section, deals with the procedure regarding the estimation of ‘q’ parameter for the Powershares QQQ of NASDAQ 100 index high-frequency data set. Section 5 examines the tail fluctuation in five different stocks viz., SPDR S&P 500 ETF, General Motors, Ever-source Energy, Coca-Cola and NQ Mobile Inc., listed in various stock indexes. We provide the stock price distribution and the option pricing model in Sect. 6. Eventually, we conclude this paper in Sect. 7.",6
57.0,4.0,Computational Economics,27 July 2020,https://link.springer.com/article/10.1007/s10614-020-10024-2,An Approximation Scheme for Option Pricing Under Two-State Continuous CAPM,April 2021,Ali Safdari-Vaighani,Davood Ahmadian,Roja Javid-Jahromi,Male,Unknown,Unknown,Male,"The capital asset pricing model (CAPM) was introduced on the model of portfolio choice by Markowitz (1952) and was developed by Sharpe (1964) and Lintner (1975) which marks the birth of asset pricing theory. The CAPM describes that the risk of a stock should be measured relative to a comprehensive market portfolio, and offers powerful predictions about how to measure risk and the relation between expected return and risk. Asset’s beta in CAPM interprets how to measure the sensitivity of the asset’s return to variation in the market return. The Black–Scholes model Black and Scholes (1973) is the most well-known model for valuation of options. It has been assumed that underlying assets follow the geometric Brownian motion, and due to the constraints such as constant volatility and log-normal distribution of returns can not interpret the empirical observations that comes from stock markets. To improve the performance of the model many extensions have been introduced. One popular extension is to relax the assumption that volatility parameters is constant. The local stochastic volatility models introduced by Dupire (1994) and Derman and Kani (1994) to manage market smile on equity derivatives. The stochastic alpha, beta, rho (SABR) model, developed by Hagan et al. (2002) describes the volatility smile of financial instruments. The two-state CAPM introduced by Fridman (1994) in which beta is proportional to the unobservable two-state Markov chain and asset price enable to switch high and low volatility regimes. Fouque and Tashman (2012); Fouque and Kollman (2011) present the Stressed-Beta model as a two-state CAPM, where the beta is a two-valued function and dependent to the asset’s price. In fact, in a Stressed-Beta model, the volatility of the stock becomes stochastic, driven by the market. In this paper we are not dealing with estimation of beta parameter although it is an important issue in financial practice. However our assumption is that we have estimation of beta by historical returns on the asset and market index. We will focus on the option pricing in presence of beta model and examine the sensitivity of the European option price to the asset’s beta. The aim of this paper is to present the PDE formulation including initial and boundary conditions for option pricing where the underlying assets price follow the two-state CAPM, and develop the radial basis function partition of unity method (RBF-PUM) to approximate the solution of the arising PDE problem. The numerical experiments confirm that proposed model capture market volatility skew while it would not reflex various volatility structures generally. Radial basis function approximation methods have successfully applied for both European and American option pricing in one and two asset cases (Fasshauer et al. 2004; Pettersson et al. 2008). More recently, RBF approximation approach have also been combined with partition of unity (PU) method (Babuška and Melenk 1997) which allows for local adaptivity, produce sparse differentiation matrices, high order of the convergence rate with easy implementation in higher dimensions (Larsson and Heryudono 2016; Shcherbakov and Larsson 2016; Safdari-Vaighani et al. 2015). In this paper, we investigate the capability of RBF-PUM as a robust approximation method for numerical solution of PDE problem resulting of option pricing under two-state CAPM. In the next section we describe two-state CAPM, and partial differential equation for European option pricing with relative initial and boundary conditions in Sect. 3. In Sect. 4, we give an overview of RBF-PUM and differentiation matrix for spatial discretization of the time dependent PDE problem. In Sect. 5, the implementation of the RBF-PUM approximation method for European option price has been explained. Numerical experiment and computational aspect of the approximation are presented in Sect. 6.",1
58.0,1.0,Computational Economics,20 June 2021,https://link.springer.com/article/10.1007/s10614-021-10141-6,Introduction to the Special Issue on Agent-Based Computational Economics,June 2021,Christopher S. Ruebeck,Troy Tassier,,Male,Male,Unknown,Male,,
58.0,1.0,Computational Economics,04 March 2019,https://link.springer.com/article/10.1007/s10614-019-09887-x,The Emergence of Money: Computational Approaches with Fully and Boundedly Rational Agents,June 2021,Zakaria Babutsidze,Maurizio Iacopetta,,Male,Male,Unknown,Male,"The scholarship that puts individualism at the center stage of analysis interprets the emergence of institutional arrangements as a spontaneous phenomenon resulting from human interactions (Schotter 1981; Greif 2006; Acemoglu and Robinson 2006). One of the most durable social institutions is money. Menger (1892) with his influential work on the origin of money argued that some goods are gradually accepted by everyone. Menger emphasized that although money has, in some cases, been introduced through legislation, in many instances it has emerged as an unplanned outcome of individual interactions and has subsequently been disseminated into the society as practice and custom: “When the relatively most saleable commodities have become ‘money’, the event has in the first place the effect of substantially increasing their originally high salableness. ... On the other hand, he who brings other wares than money to market, finds himself at a disadvantage more or less” (1892, p. 250). The first modern attempt to formalize Menger’s observation was conducted by Kiyotaki and Wright (1989; KW henceforth). They characterized the conditions for the existence of a Nash equilibrium in which a high-storage cost object is accepted as money in a society with heterogeneous agents that trade in decentralized markets. In line with modern mainstream economics, in their model individuals are fully informed about what other people do, and on the current state of the economy. An important finding of their analysis is that, as long as the rate of returns across goods (or equivalently the storage costs) is not too large, it is possible that people will select an equilibrium where a low-return object acts as means of payment. This paper investigates the emergence of money from two different methodological points of view. One is based on KW’s characterization of an agent as an individual who makes decisions on the basis of the current and expected future evolution of the economy. Our methodological framework, however, is more general, in that money can emerge spontaneously as the economy evolves towards a long run equilibrium. In the second methodological approach, agents do not have perfect foresight. Boundedly rational agents make choices through a classifier system. In particular, following Marimon et al. (1990; henceforth MMS), we study the choices of individuals by assigning scores to their actions on the basis of an algorithm that tracks past trading and consumption choices as well as trade contingencies. Hence, agents are thought of as having a limited capacity to take actions on the basis of forecasted future states of the economy. By combining these two approaches we can go further in determining the conditions for the emergence of money, compared to previous studies. A number of scholars have tested the idea that a high-cost storage commodity is used as money through computer experiments with artificial agents and through laboratory experiments with actual human subjects: MMS explored the question of whether artificially intelligent agents can learn to play strategies that induce the emergence of money; Başi (1999) developed a learning algorithm that allows for imitation and found that the presence of imitation either speeds up convergence to the Nash equilibrium, or leads every agent of the same type to undertake a suboptimal behavior. A similar question is tested in a number of controlled laboratory experiments using humans (Brown 1996; Duffy and Ochs 1999, 2002). Matsuyama et al. (1993), Wright (1995), Luo (1999) and Sethi (1999) approached the issue through evolutionary dynamics. Duffy (2001) studied the fitness of a hypothetical learning algorithm against the outcome of laboratory experiments. Yet, these experimental studies concluded that, contrary to what KW suggested, when artificial or real people are asked to make decisions in such a trade environment, the low-return object is rarely accepted as money. The previous studies have two shortcomings. First, the experimental literature has not directly compared the results on the emergence of money with the KW dynamic Nash equilibria, but rather against stationary equilibria. Yet, the experiments have, as will be clarified below, an important dynamic dimension. We aim to advance the debate on such a comparison by proposing a numerical algorithm that is able to compute the dynamic equilibrium in a KW type of model. Second, the experimental literature has thus far not clarified what behavioral features may block the emergence of money in situations where this would be observed if agents were to coordinate on a Nash equilibrium. To fill this gap, we use a variant of the MMS classifier system applied to the specific KW environment. Such a system starts from the premise that individuals can hardly discern across options on the basis of payoffs associated to future contingencies, as it would be expected for rational-forward looking agents. Instead, agents compare the value of current options using the observed stream of payoffs that followed past actions undertaken in similar circumstances. Individuals favor actions that, in retrospect, have been shown to be advantageous and penalize those that have not. MMS did not find support for the notion that people can coordinate on a set of strategies that induce the emergence of money. While this could be a realistic conclusion, it is still interesting to understand if there are scenarios in which emergence is observed in computational experiments. The extraordinary expansion of computer power gives us an opportunity to review the conclusions of MMS that were based only on a few anecdotal experiments. Furthermore, because we use two numerical methodologies, we can compare the evolution of the boundedly rational-agent economy with that of the fully rational-agent economy and therefore reason which behavioral traits can hinder the emergence of money. It has been argued that the “procedural rationality” (Simon 1976) is the most appropriate way to study the behavior of not-fully rational agents in an artificial setting. However, “procedural rationality” does not give clear computational implementation guidelines. MMS implemented the model through classifier systems in which agents only react to the environment using pre-defined procedures. Agents cannot re-define the procedures during the course of the model run. While such a feature greatly reduces the computational power requirements, one may wonder if it has been an important reason for the lack of success of the MMS classifier system. The aim of this paper is twofold. First, by concentrating on the transitional path to the steady-state equilibrium, we show instances of the emergence of money. In particular we can determine exactly when a commodity emerges as money (or loses such a status) along the transition path. The emergence of money comes with a radical change in the trading behaviors of some individuals—a switch from a fundamental to a speculative trading strategy or the other way around. This dynamic phenomenon has been neglected in previous studies, as they have mostly focused on steady state equilibria. Second, working with two computational approaches allows us to critically assess these methodologies with regard to modeling complex interactive systems. We perform a detailed examination of the more complex, boundedly rational agent model. We are able to drastically simplify the previous implementation of the KW model using reactive boundedly rational agents. We find that, although this approach can produce the main observational discriminant (emergence of money) in some instances, the implementation of classifier systems has limited capabilities. To overcome these limitations, we argue that economists should take advantage of the current abundance in computational power and advances in machine learning to model cognitive computational agents to represent boundedly rational economic actors. The remainder of the paper is organized as follows. Section 2 presents the methodology with fully rational agents. It first derives steady state equilibria and then investigates transitional paths. Section 3 implements the environment in boundedly rational agent setting. It investigates, in detail, the conceptual problems that arise in implementing procedurally rational behavior through reactive agent systems. Section 4 presents a short discussion on possible ways to improve current methodologies with artificial agents by using advances in computer science and inferential statistics. The last section concludes the study.",1
58.0,1.0,Computational Economics,05 October 2019,https://link.springer.com/article/10.1007/s10614-019-09918-7,Spatially Heterogeneous Vaccine Coverage and Externalities in a Computational Model of Epidemics,June 2021,Myong-Hun Chang,Troy Tassier,,Unknown,Male,Unknown,Male,"Despite the availability of highly effective and inexpensive vaccines, many infectious diseases persist throughout the world. As an example, influenza and pneumonia consistently rank among the top ten causes of death in the U.S. (CDC 2017a) yet coverage rates for these and other vaccines are relatively low. In the US only 40–50% of individuals choose to be vaccinated for influenza each year and slightly more than 20% of adults are vaccinated for pneumonia (CDC 2017b). Measles, Mumps, and Rubella (MMR) vaccination rates are much higher, partly because of mandated vaccination for school age children in many areas. Yet still, between 5 and 10% of parents forgo recommended MMR vaccines for their young children and many seek exemptions to mandatory vaccination requirements (CDC 2017c). Economic incentives help to explain part of the low vaccination coverage: Given the rarity with which many of these infectious diseases cause death or extreme illness in otherwise healthy people, many judge the risks of forgoing a vaccination to be low and do not bother to be vaccinated even though the vaccines have a minimal monetary cost (Geoffard and Philipson 1997). In many cases it is likely a time cost that proves more prohibitive than the monetary cost which is one reason why many larger firms offer influenza vaccines to be distributed at work facilities. In addition to the cost effects of vaccines, there is a growing anti-vaccine movement which has been perpetuated by many well-known public figures (Olpinski 2012). Despite the lack of scientific evidence to support the claims, or in many cases scientific evidence in opposition to the claims, the anti-vaccine movement remains strong especially in some locales. This vaccine opposition often has a localized distribution that plays out in vaccine denial being especially high in particular localized areas. As a result, researchers have found a great deal of spatial heterogeneity in rates of vaccination coverage (Carrel and Bitterman 2015; May and Silverman 2003; Lieu et al. 2015). The anti-vaccine movement and spatial heterogeneity in vaccine coverage has sparked a wide spread discussion in academics and the general press.Footnote 1 Most of these discussions focus on decreases in overall vaccination rates and the accompanying increase in risk of infection (Feikin et al. 2000; Atwell et al. 2013). Other researchers examine the demographic correlates of vaccine choice and try to identify factors that lead to anti-vaccine sentiment, vaccine refusal, and simply lower levels of vaccination among some groups. Factors investigated include: distrust of vaccine safety; poverty and lack of easy access to healthcare; misunderstanding of both the benefits of vaccines and the potential risks of avoiding vaccination; and demographic factors (McNutt et al. 2016; Birnbaum et al. 2013; Sugarman et al. 2010; Gust et al. 2004). Outside of research in public health, medicine, and public policy, there is a growing interest in economics on the welfare implications and externalities inherent in vaccinations and epidemic processes (Gersovitz 2013; Philipson 2000; Chen and Toxvaerd 2014). Most recently, economists have begun to more thoroughly consider the effect of group structure and vaccine choice (Galeotti and Rogers 2013 and 2015). Our research continues to expand this area of interest. Despite the above discussed interest in spatial vaccine coverage and distribution, there exists a lack of research explicitly exploring this area (apart from interest in a decrease in the overall vaccination rate). In this paper we take observed spatial heterogeneity as given and examine how the spatial heterogeneity itself, holding overall vaccination levels constant, can generate unique features of an epidemic process. These features depend strongly upon the interaction structure within the population. The combination of these two factors is a unique area that has not been explored in the literature to date. We develop an agent-based computational model of an epidemic process that occurs across regions. In the model we create spatial disparities in vaccination rates. The population in our model exists in a discrete set of non-overlapping regions with varying levels of vaccination coverage across the regions. In order to isolate the effect of only spatial heterogeneity we change the variance in the vaccination rates across regions while holding the population level vaccination rate constant. This isolation of the variance apart from changes in overall vaccination levels allows us to more fully explore and understand the implications for spatial differences in vaccine coverage. This is a unique feature of our research which allows us to isolate and examine potential externalities that occur across regions with differing levels of vaccine coverage and with different rates of interaction between the regions. The most direct result of our model shows that spatial heterogeneity by itself can increase epidemic size (on average) when the population has a sufficiently high rate of vaccine coverage. We find this effect to be most pronounced when agents have more inter-regional connections. These results are consistent with expectations. However, we also find that this spatial heterogeneity can decrease epidemic size in special cases where vaccination levels are sufficiently low and there are sufficiently few (but some) connections between regions. We then use our model to to identify inter-regional externalities created by differing vaccination rates as we change the contact structure of the model. When we increase connections between high and low vaccine neighborhoods, as one would expect, we find that unvaccinated individuals in the high vaccine neighborhoods have increased risk of infection (a negative externality), while those in low vaccine neighborhoods have decreased risk of infection (a positive externality). The magnitude of these externalities, relative to each other, determines whether increasing connections between high and low vaccine areas will ultimately increase or decrease the average epidemic size. Holding overall vaccination levels, neighborhood heterogeneity in vaccine coverage, and the percentage of inter-regional contacts constant we find that magnitude of the externalities relative to each other, depends upon the overall level of vaccinations in the population and, specifically, how close the population is to reaching herd immunity. When the overall vaccination rate is sufficiently high, near or above levels that produce herd immunity, allowing more interconnections between the high and low regions can lower epidemic size. Conversely, when vaccine levels are sufficiently below the level needed for herd immunity, allowing more interconnections between high and low level regions significantly increases epidemic size. All of the results stated above depend on the non-linearity of the relationship between vaccine coverage and epidemic size. But, this relationship depends intricately on both the heterogeneity in vaccine coverage across regions and the interaction structure in the population. The interaction structure in particular creates non-trivial difficulties in analytically isolating the effects that we find. We use an agent-based model to better understand the complex relationships between interaction structure, heterogeneous vaccine coverage, and epidemic size. In economics, agent based models are most frequently used to incorporate into theoretical models elements of agent heterogeneity, agent adoption or learning, or situations of complex agent interactions. Frequently these models examine the underlying dynamics of the model and often emphasize out of equilibrium dynamics (either as a model result or on the path to an equilibrium). Other uses of agent based models include comparative statics. Agent based models allow one to fluidly change parameter settings and to parsimoniously examine how those changes in parameters result in changes in model outcomes through theoretical “computational experiments” (Miller and Page 2007). The current paper is an example of this feature of agent based modeling. Because epidemic processes frequently depend on interaction structures that are not tractable analytically, agent-based computational models similar to the one implemented in this paper are used to understand epidemic process (Epstein 2009; Eubank et al. 2004). These models are used in a variety of ways. In some instances, computational models are used to make real-time predictions as an epidemic progresses (Tizzoni et al. 2012). Most, however, attempt to better understand the dynamic processes involved in an epidemic spreading on a complex network and on how behavioral or interaction based changes can modify these processes.Footnote 2 For the purposes of our paper, agent-based computational models are ideal. This modeling paradigm allows us to easily change assumptions about regional vaccination coverage and inter-regional interactions in a precise and controlled manner and to examine the comparative statics properties of changes in vaccine coverage heterogeneity and interaction structure. In doing so we can isolate the effect of these variables and gain a greater understanding of their effect. The most unique contribution of our paper is the explicit study of vaccine coverage heterogeneity apart from changes in overall vaccination coverage. As described above, researchers have observed variance of regional vaccination rates. The results presented in this paper demonstrate that this phenomenon by itself, apart from changes in overall vaccination rates, has significant importance for understanding the economics of disease transmission. To better understand the degree of variation in vaccination rates, next we provide an example of geographic vaccination coverage data. This will make clear how wide ranging vaccination coverage rates are across regions of the U.S. Following this example we describe our formal model that will be used to examine the effect of this heterogeneity. In Sect. 4, we describe the set of computational experiments used to isolate and understand the effects of vaccination coverage heterogeneity. We describe our results in Sect. 5. Particularly we show the changes in epidemic size that result from changes in the spatial distribution of vaccine coverage and how this interacts with changes in the number of inter-regional connections in the contact structure of the model. We also show the magnitude of positive and negative externalities between high and low vaccination regions. The magnitude of these externalities, relative to each other, depend crucially on how close the population is to reaching herd immunity and on the interaction structure of the population.",2
58.0,1.0,Computational Economics,27 July 2020,https://link.springer.com/article/10.1007/s10614-020-10017-1,Endogenous Shared Punishment Model in Threshold Public Goods Games,June 2021,Gabriela Koľveková,Manuela Raisová,Vladimír Gazda,Female,Female,Male,Mix,,
58.0,1.0,Computational Economics,09 October 2019,https://link.springer.com/article/10.1007/s10614-019-09917-8,Microconsistency in Simple Empirical Agent-Based Financial Models,June 2021,Blake LeBaron,,,,Unknown,Unknown,Mix,,
58.0,1.0,Computational Economics,28 April 2020,https://link.springer.com/article/10.1007/s10614-020-09984-2,Plant location decisions in the ethanol industry: a dynamic and spatial analysis,June 2021,Jason Wood,James Nolan,,Male,Male,Unknown,Male,"Ethanol is a used as a gasoline additive and is blended into all gas sold in the U.S. and Canada. As of 2016, ethanol production in Canada was approximately 1.8 billion liters (USDA 2013). This production is spread across the country. For example, in the Canadian province of Saskatchewan, five wheat based plants produce a total of 345 million liters of ethanol per year (Mmly) (Canadian Renewable Fuels Association 2012). With respect to land use, the Saskatchewan plants require approximately 855,000 acres of farm land in order to satisfy their input demands. The choice of past and future ethanol plant locations will affect not only industry sustainability, but also broader societal issues like rural revitalization.Footnote 1 To this end, current industry orthodoxy assumes plant location is primarily driven by transport cost minimization, while the weight losing nature of ethanol production means that plants have tended to locate near areas of feedstock production (Huang and Levinson 2008). In addition, there is limited prior research examining ethanol industry location decisions at the plant level, and the research that has been done examines mostly non-spatial aspects of ethanol operations. We speculate that future plant location decisions will be driven equally by transportation factors as well as chosen policy direction and market interactions with end users. This research seeks to identify key factors influencing ethanol plant location in a major production region (the Canadian province of Saskatchewan) possessing spatial characteristics that are representative of much of the industry across North America. Due to the inherent individuality and complexity of plant location decisions in this industry, we develop an agent-based simulation model of the decision process. The simulation framework allows us to generate the significant drivers of past and future ethanol plant location decisions, while also permitting the estimation of a counterfactual scenario of interest, namely the removal of government subsidies to the industry. Overall, the model generates results (predictions) about the evolution of plant sizing and industry concentration in the ethanol sector across North America.",
58.0,1.0,Computational Economics,22 May 2020,https://link.springer.com/article/10.1007/s10614-020-09989-x,Realizable Utility Maximization as a Mechanism for the Stability of Competitive General Equilibrium in a Scarf Economy,June 2021,Tongkui Yu,Shu-Heng Chen,,Unknown,Unknown,Unknown,Unknown,,
58.0,2.0,Computational Economics,23 July 2020,https://link.springer.com/article/10.1007/s10614-020-10010-8,Censored Nonparametric Time-Series Analysis with Autoregressive Error Models,August 2021,Dursun Aydin,Ersin Yilmaz,,Male,Male,Unknown,Male,"In the statistical literature, researchers use the term “right-censored observation” for a unit’s failure time that is only known to exceed a detection limit. Generally, the measurements collected over time are observed with data irregularities, such as censoring due to a threshold value. Ordinary statistical methods cannot be applied directly to such observations, especially for time-series data. As is known, time-series measurements are often auto correlated and analyzed by modeling autocorrelations via their appropriate autoregressive structures. Box and Jenkins (1970) presented the first study dealing with time-series analysis within a parametric framework. However, although parametric approaches are highly useful for analyzing time-series data, they can produce biased estimates or lead to wrong conclusions, especially for censored data. When using time series, we may encounter severe problems using data with censored or auto-correlated errors. During the last decade, many techniques have been proposed for dealing with such problems in which the dependent variable is subject to censoring. Our view is that these techniques are fundamentally divided into parametric and nonparametric methods, depending on the estimated autocorrelation function. Here, we focus only on the nonparametric approaches and try to discern which will provide a better estimation of auto-correlated censored data. Suppose we consider a nonparametric time-series regression model with autoregressive errors for censored response observations at time t, given by where \(Y_t\) represents a stationary time series, and its prediction depends on the explanatory variable \(X_t\), and f is an unknown smooth function giving the conditional mean of \(Y_t\) given \(X_t\). In addition, \(e_t\), defined in (1), is a stationary autoregressive error term generated by where \(\phi\) =\(\ \left( {\phi }_1,\ldots ,{\phi }_p\right) ^{\top }\) is the vector of the autoregressive coefficients and \({\varepsilon }_t\) represents independent and identically distributed random variables with zero mean and variance \({\sigma }^2\). Model (1) does not contain lagged \(Y_t's\) and has auto-correlated error terms. This makes it an appropriate model for the regression analysis of certain kinds of time-series data. Our objective is to estimate both the unknown function f (.) and the autoregressive structure in (1) by nonparametric methods using censored time- series data. There are numerous studies suitable for the estimation approaches of f (.) in a nonparametric regression model based on censored data (Zheng 1984; Dabrowska 1992; Kim and Truong 1998; Yang 1999; Cai and Betensky 2003; and Aydin and Yilmaz 2017). As noted, while there are extensive studies on estimating nonparametric models with censored responses, the literature on censored time-series response data is limited. Examples of these works include Zeger and Brookmeyer (1986), Park et al. (2007), and Wang and Chan (2017). Problems with censored time- series data are commonly solved using data augmentation techniques. Several researchers, including Robinson (1983), Parzen (1983), Tanner (1991), Hopke et al. (2001), and Park et al. (2007, (2009), have used this technique for regression models with autoregressive errors when censored response observations are considered. Note that both imputation and augmentation methods are addressed by several authors for missing values, excluding time- series observations. For example, see the studies of Rubin (1996), Dempster et al. (1977), Heitjan and Rubin (1990), and Meng (1994). In addition, there are several nonparametric estimation methods for obtaining the autocovariance function in the literature, such as Hall and Patil (1994), who suggested a nonparametric estimation method for estimating the autocovariance function based on kernel smoothing (KS). Elogne et al. (2008) estimated the autocovariance function with interpolation techniques, and there are several similar studies: Glasbey (1988), Shapiro and Botha (1991), Sampson and Guttorp (1992), Bjørnstad and Grenfell (2001), and Wu and Pourahmadi (2003). In the literature, there are essentially two approaches to handling censoring. One is to eliminate the censored values, and the other is to use the censored data points as observed. However, the study of Park et al. (2007) demonstrates that both approaches yield biased and inefficient estimates. It is possible that the performance of the parameter estimation can be improved by using datasets that have a lower censoring rate (Helsel 1990). However, such methods are often not applicable, and outcomes depend heavily on rigid parametric model assumptions. As indicated above, even though some numerical solutions have been proposed in the literature to cope with the problem of censored responses in autoregressive error models, there are no studies making inferences for censored time-series models in terms of nonparametric approaches. By contrast, we propose nonparametric estimation procedures for time-series containing right-censored observations, instead of using parametric approaches. Thus, our study is remarkably different from other similar studies used in the statistical literature. In this paper, we consider three nonparametric approaches, smoothing spline (SS), kernel smoothing (KS), and regression (penalized) spline (RS), for estimating an autoregressive time-series regression model with right- censored observations. Note that these nonparametric approaches cannot be applied directly to censored observations, and a data transformation is required to estimate the censored response observations. To overcome this problem and to get stable solutions, we used a data augmentation method, namely a the Gaussian imputation technique. This data transformation method, which is a modified extension of ordinary Gaussian imputation, is used to adjust the censoring response variable in the setting of a time series. We also compare the performances of the smoothing methods with the benchmark AR(1) model. To the best of our knowledge, such a study has not been conducted. It should be noted that the best estimation of the censored lifetime observations \(Y_t\) that depend on an explanatory variable \(X_t=x\) could be expressed as the conditional mean of the response \(E (Y_t \vert X_t=x )=f(x)\), which minimizes the quantity Normally, it is not necessary that function f be linear, and the conditional variance is homoscedastic, but the error term indicated in model (1) is generally presented as follows As indicated above, the minimization of Eq. (3) is carried out through three smoothing methods, the \(SS,\ KS\), and RS methods. The primary purpose of this study is to estimate a right-censored time series non- parametrically and provide consistency in the estimation with the help of the data augmentation method for censored observations. One of the important points of data augmentation is to estimate the covariance function (Park et al. 2009). Here, this function is estimated with a nonparametric Nadaraya-Watson estimator. With this method, there is no need to describe the prior distribution of the time- series data because data augmentation is achieved using the nonparametric method. The paper is organized as follows. Section 2 introduces the thory of the censored autoregressive model and algorithm of Gaussian imputation to make data augmentation. In Sect. 3, the three smoothing methods are explained, and their modifications are illustrated according to censored data. Section 4 involves the evaluation measurements for the three modified smoothing techniques. Furthermore, an estimation of the covariance functions of the estimators are expressed. To obtain empirical results, a detailed simulation study is done in Sect. 5. Also, two real-data applications with cloud and unemployement datasets are realized in Sect. 6. Finally, conclusions and discussion are presented in Sect. 7.",1
58.0,2.0,Computational Economics,04 August 2020,https://link.springer.com/article/10.1007/s10614-020-10023-3,Modeling Economic Activities and Random Catastrophic Failures of Financial Networks via Gibbs Random Fields,August 2021,Levent Onural,Mustafa Çelebi Pınar,Can Fırtına,Male,Male,,Mix,,
58.0,2.0,Computational Economics,06 August 2020,https://link.springer.com/article/10.1007/s10614-020-10026-0,"Wage Inequality, Labor Market Polarization and Skill-Biased Technological Change: An Evolutionary (Agent-Based) Approach",August 2021,Patrick Mellacher,Timon Scheuer,,Male,Male,Unknown,Male,"Rising wage inequality is a widely discussed phenomenon, especially for the USA where it has been observed since the 1980s (see Autor 2014). A number of possible explanations have been proposed and investigated, including globalization (see Helpman 2017), the worker composition of firms (high-wage workers more often work in high-wage firms), as well as the rise of very large firms (see Song et al. 2018) and the diminished power of trade unions (Card 2001; Pontusson 2013). By the end of the last century, however, the hypothesis that new technologies are skill-biased to the benefit of higher-skilled workers (see Autor et al. 1998) became the “standard explanation” (Acemoglu 2002). Acemoglu (1998, 2002) delivers a theoretical framework for this observation. He assumes that new technologies can be designed in a way that they fit the currently available skills (so-called directed technological change).Footnote 1 In the beginning of the 2000s, however, this view was challenged. Autor et al. (2003) argue that the effect of computing technologies is twofold: they complement non-routine tasks (which are typically performed by high-skilled, but also low-skilled labor) and substitute routine tasks (typically performed by medium-skilled workers). For example, computers support the work of scientists and (at this point) do not threaten the existence of their jobs, whereas travel agents were hit hard by the ability of customers to book their flights and hotels on the internet. Goos and Manning (2007) introduced the term “job polarization” to describe that medium-skilled workers are most affected by substitution as their jobs are automated and vanish. On the other hand, low-skilled as well as high-skilled jobs (or, as they put it, “lousy and lovely jobs”) are on the rise. This result was confirmed repeatedly for the U.S. and Europe (Goldin and Katz 2007; Goos et al. 2009; Autor and Dorn 2013). Autor and Dorn (2013) and Autor (2015) speak, more broadly, of a polarization of the labor market and show that the polarization in employment was accompanied by a polarization of wages. Whether or not this trend will continue remains to be seen. Among others, Brynjolfsson and McAfee (2011), as well as Frey and Osborne (2017) suggest that new technologies become more and more sophisticated by the minute which might lead to a situation in which low- and high-skilled jobs are increasingly threatened as well. Meanwhile, the idea of directed technological change was developed further. Acemoglu and Restrepo (2018) develop a framework in which directed technological change may lead to excessive automation that lead to wages which are relatively too high compared to the rental rate of capital (e.g. due to labor market rigidities). In the most sophisticated version of this model to date, a potential skill-mismatch between low and high-skilled labor is highlighted. To fully reap the benefits of new technologies, but also to decrease inequality, the skill-level of the population has to be increased (see Acemoglu and Restrepo 2018a). We want to contribute to the understanding of the relationship between wage inequality, polarization and skill-biased technological change by putting forward an approach that combines two aspects of technological change: technical and economic feasibility. Following Schumpeter ([1934] 2012), we distinguish between inventions and innovations. Inventions open up new possibilities to produce a certain (old or new) good. But they will only become economically relevant if somebody (an entrepreneur or established firm) innovates by actually using this possibility and that only occurs if it is profitable to do so. The innovation process therefore crucially depends on prices: how much do the inputs cost and how much can a firm charge for the outputs? In other words, just because a technology becomes invented, i.e. technically feasible, it doesn’t mean that it will lead to an innovation, as only economically feasible technologies will be used. For example, fast food restaurants increasingly rely on self-service kiosks instead of human cashiers. Those kiosks substitute for (unskilled) labor, as well as land (as kiosks take up much less space). While this technology is available worldwide, it is more likely to be used in countries and areas where (unskilled) labor and land are relatively expensive, as the investment only then pays off.Footnote 2 From this point of view, a potential skill-bias of technological change may arise both in the invention, as well as in the innovation process. It is much easier to invent a machine that replaces human cashiers than a technology that substitutes doctors. But doctors are very expensive, so there is a huge incentive to automate at least some of the tasks they perform. For a start, artificial intelligence is thought to play a big role in radiology in future (Chartrand et al. 2017). Our approach thus combines both the perspectives of Autor et al. (2003), who emphasize the technical skill-bias of computing technology, as well as Acemoglu (1998, 2002) who assumes that the direction of technological change (and thus, a potential skill-bias) is determined (at least partly) by supply and demand. We also assume that technological change adds up—the invention of self-service kiosks, for instance, depended on continuous improvements that made computers ever smaller and cheaper—and thus is path-dependent. If this is the case, the trajectory of technological change may not easily be reversed, even if the direction of future innovations changes. To model our approach, we replicated the core model of the Keynes + Schumpeter agent based model family (Dosi et al. 2006, 2008, 2010, 2013, 2015, 2017) which seems to be perfectly suited as it features endogenous growth that is based on an invention, innovation and imitation process at the firm level. We tried to follow the reference model very closely and avoid major deviations. We then modified it by introducing heterogeneous labor in the form of three types of workers which assume different roles in the production process and are differently affected by technological change. One may argue that technological change could be implemented using a much simpler model. But we assume that the technological development, the goods and the labor markets are interdependent and therefore require a general analysis. We chose agent based modeling, as it allows us to study (a) technological change and inequality, which are inherently heterogeneous processes and (b) how technological change and the labor markets are shaped by equilibrating, but also by disequilibrating forces. Instead of creating a new model from scratch, we chose to follow an established one and only change it modestly to make our point. As we concede in Sect. 5.3, we are not fully satisfied with every single mechanism that we implemented. In the future, we plan to further improve on the model, but do so on a step-by-step basis to be able to grasp the effects of every small change. On the one hand, our model reproduces important insights from the previously cited well-known general equilibrium models concerned with the impact of technological change on heterogeneous workers, as well as from agent based models (see Sect. 5.1). On the other hand, it produces some distinct results which seem to be a promising starting point for further research (see Sects. 5.2 and 5.3). By the time we set up our model, a number of general agent-based models involving technological change and heterogeneous skills were published that we are aware of (see Dawid and Delli Gatti 2018, for a literature survey of recent macroeconomic ABMs). Dosi et al. (2019a) introduce skill level as a continuous variable (acquired by on-the-job-training) into the original K + S model and thus also includes endogenous technological change, which however is not skill-biased. The Eurace@Unibi model (see e.g. Dawid et al. 2018) and Eurace Simulator (see e.g. Ponta et al. 2018) offer two-fold heterogeneity: Workers have a certain general skill-level (which may be upgraded by policies as in Dawid et al. 2009) and specific skills, acquired by on-the-job training. The former model has been used to study the relationship between technological change and inequality under skill-heterogeneity as the authors assume complementarity between capital productivity and skill level. The latest technologies can only be fully productive if they are operated by workers who have acquired a certain skill level (see Dawid et al. 2018). Technological change in the Eurace@Unibi and the Eurace Simulator models is therefore in a way skill-biased towards higher-skilled workers. In contrast to our model, however, technological change is exogenously given and thus not influenced by the conditions at the labor market (i.e. it is not directed). The skill-bias also monotonically favors higher-skilled workers and thus does not account for the aforementioned observation by Goos and Manning (2007), Autor et al. (2003) and others that technological change in fact affects medium-skilled workers most. A model by Silva et al. (2012) features heterogeneous labor in the form of routine and non-routine workers. Although this distinction seems to be inspired by empirical work a la Autor et al. (2003), their model also produces wage inequality in the absence of routine-biased technological change. Georges (2017) describes a model emphasizing the effects of product innovation in the consumption good sector on inequality between two types of workers. Naturally, both models do not feature polarization tendencies, since they confine themselves to two types of workers. Finally, Caiani et al. (2019) feature a segmented labor market like ours, as well as endogenous technological growth. Caiani et al. (2018) expand their analysis and specifically analyze whether wage inequality fosters economic growth or not. Technological growth in their model is not skill-biased and higher wages for the lower classes of society actually encourages growth as aggregate demand is strengthened. Crucially, they assume that technological change does not affect the capital-labor ratio, which is assumed to be constant, but only capital productivity. The same is true for the Eurace@Unibi model. As further elaborated below, we make the exact opposite assumption, which leads us to different results. Agent-based models are also fruitfully employed to study the interrelationship between innovation, growth and inequality in the absence of heterogeneous labor. Vallejos et al. (2018) present an agent-based model, which is able to replicate the empirical tendencies towards wealth inequality in the United States. Palagi et al. (2017) analyze the effects of income inequality on macroeconomic performance. Neves et al. (2019) offer an agent-based interpretation of the “The Race between Man and Machine” emphasized by Acemoglu and Restrepo (2018) and Acemoglu and Restrepo (2018b) and, like the latter models, also uses a task-based framework and analyzes the counteractive effects of the discovery and automation of tasks. Our contribution is threefold: (1) We show that the well-known Keynes + Schumpter computational model can be modified with modest adaptions to produce wage inequality and job, as well as wage polarization—tendencies that are observed in the real world. (2) We model explicitly labor market policies that could be used by governments and trade unions trying to decrease wage inequality. We show that trying to set artificially high wages for low-skilled workers has the unintended consequence that technological change becomes skill-biased, which results in a higher unemployment rate for low-skilled workers. As discussed above in the context of self-service kiosks, there is reason to believe that such an unintended consequence could also be triggered in the real world. (3) We offer a Schumpeterian view of skill-biased technological change that distinguishes between technical and economic skill-biases. The economic skill-bias in our model represents an alternative to the mainstream approach to directed technological change by Acemoglu (1998) and his subsequent work and has immediate policy implications. In our model, highly skilled workers benefit from the fact that their supply is limited, which would be a disadvantage in Acemoglu’s models. In our model, a sustainable decrease of wage inequality is only possible if many low- and medium-skilled workers are upskilled. But if there is a conflict of interest, upskilling policies are much less likely to be implemented. The rest of the paper is organized in the following way: The next section gives an overview of the model and describes the sequence of events. The third section discusses the behavior of the agents in the model in detail. We then introduce different labor market institutions and policies that we are experimenting with. In the fifth section, we show how we validated our model and present the results of our labor market experiments. The final section concludes. The initial parameters of the model are displayed in “Appendix”.",7
58.0,2.0,Computational Economics,28 August 2020,https://link.springer.com/article/10.1007/s10614-020-10027-z,How Robust is Robust Control in Discrete Time?,August 2021,Marco P. Tucci,,,Male,Unknown,Unknown,Male,"A characteristic “feature of most robust control theory”, observes Bernhard (2002, p. 19), “is that the a priori information on the unknown model errors (or signals) is nonprobabilistic in nature, but rather is in terms of sets of possible realizations. Typically, though not always, the errors are bounded in some way…. As a consequence, robust control aims at synthesizing control mechanisms that control in a satisfactory fashion (e.g., stabilize, or bound, an output) a family of models”.Footnote 1 Then “standard control theory tells a decision maker how to make optimal decisions when his model is correct (whereas) robust control theory tells him how to make good decisions when his model approximates a correct one” (Hansen and Sargent 2007a, p. 25). In other words, by applying robust control the decision maker makes good decisions when it is statistically difficult to distinguish between his approximating model and the correct one using a time series of moderate size. “Such decisions are said to be robust to misspecification of the approximating model” (Hansen and Sargent 2007a, p. 27).Footnote 2 Tucci (2006, p. 538) argues that “the true model in Hansen and Sargent (2007a) … is observationally equivalent to a model with a time-varying intercept.” In the sense that, unless some prior information is available, it is impossible to distinguish between the two models by simply observing the output. Then he goes on showing that, when the same worst-case adverse shock and objective functional are used in both procedures, robust control is identical to the optimal control associated with time-varying parameters, or TVP-control, only when the transition matrix in the law of motion of the parameters is zero. He concludes that this decision maker implicitly assumes that today’s worst-case adverse shock is serially uncorrelated with tomorrow’s worst-case adverse shock. This is a relevant conclusion because it applies to a robust control set up widely used in economics. Moreover, as commonly understood, the robust control choice accounts for all possible kinds of persistence of worst-case shocks, which may take a very general form. Then, it is not immediately obvious why they look linearly independent when the decision maker cares only of the induced distributions under the approximating model and is indifferent between utility processes with identical induced distributions. Namely, when He/She is assumed having preferences defined by using a single constraint, or penalty, on the adverse shocks. At this stage however, it is unclear if this result holds when a more general framework is considered. For instance, when the decision maker has a different constraint for each type of adverse shocks. This may be the case when He/She is looking for decisions robust to perturbations in a situation where parts of the state vector are unobservable. Then two types of statistical perturbations are considered. One that distorts the adopted model conditional on the knowledge of hidden state and the other that distorts the distribution of the hidden state. This decision maker is sometimes referred to as the non-“probabilistically sophisticated” decision maker and contrasted with the “probabilistically sophisticated” decision maker, who considers only one kind of perturbation, described above.Footnote 3 Alternatively robust control may be applied to situations where the decision maker wants to be “immunized against uncertainty” related to unknown structural parameters as in Giannoni (2002, 2007).Footnote 4 The goal of this paper is to carry on the comparison between TVP-control and robust control for a much larger class of models in discrete-time.Footnote 5 Namely, the case of a non-“probabilistically sophisticated” decision makers who want to make decisions robust with respect to unstructured uncertainty à la Hansen and Sargent, i.e. a nonparametric set of additive mean-distorting model perturbations. And the class of models where uncertainty is related to unknown structural parameters. This is a necessary step to determine if Tucci’s (2006) result holds only in the simplest case or is valid for a much larger class of models. In the former case it may be treated as an interesting special case of limited, or no, practical relevance because it does not affect the most commonly used robust control frameworks. In the latter it is a clear indication that further investigation, outside the scope of the present work, is needed to see how large is the uncertainty set associated with these frameworks. In other words, how strong is the ‘immunization against uncertainty’ provided by the linear-quadratic robust control set up widely used in economics in discrete-time.Footnote 6 The remainder of the paper is organized as follows. Section 2 reviews the simplest robust control problem with unstructured uncertainty à la Hansen and Sargent. An example of a non-“probabilistically sophisticated” decision maker is discussed in Sect. 3. In Sect. 4 both problems are reformulated as linear quadratic tracking control problems where the system equations have a time-varying intercept following a mean reverting, or ‘Return to Normality’, model and the associated TVP-controls are derived. Then the optimizing model for monetary policy used in Giannoni (2002, 2007) is presented (Sect. 5). Section 6 reports some numerical results and the main conclusions are summarized in Sect. 7. For the reader’s sake, the major result of each section is stated as a proposition and its proof confined to the “Appendix”.",
58.0,2.0,Computational Economics,26 August 2020,https://link.springer.com/article/10.1007/s10614-020-10028-y,Estimating the Unrestricted and Restricted Liu Estimators for the Poisson Regression Model: Method and Application,August 2021,Kristofer Månsson,B. M. Golam Kibria,,Male,Unknown,Unknown,Male,"In empirical applications in economics when modeling the gene expression data, the dependent variables are often in the form of non-negative integers of counts, for example, in analyzing the determinants of the number of patents by a firm or in biostatistics. In such a situation, a common distributional assumption for the dependent variable \( y_{i} \) is that it follows a Poisson distribution. More specifically, it is distributed as \( Po\left( {\mu_{i} } \right) \), where \( \mu_{i} = \exp \left( {{\mathbf{x}}_{{\mathbf{i}}} '{\varvec{\upbeta}}} \right) \), \( {\mathbf{x}}_{{\mathbf{i}}} \) is the ith row of \( {\mathbf{X}} \) which is a \( n \times \left( {p + 1} \right) \) data matrix with p explanatory variables and \( {\varvec{\upbeta}} \) is a \( \left( {p + 1} \right) \times 1 \) vector of coefficients. The parameter for the unrestricted Poisson regression model is usually estimated using the iterative weighted least squares (IWLS) algorithm as: where \( {\hat{\mathbf{W}}} = diag\left[ {\hat{\mu }_{i} } \right] \) and \( {\hat{\mathbf{z}}} \) is a vector where the ith element equals \( \hat{z}_{i} = \log \left( {\hat{\mu }_{i} } \right) + \frac{{y_{i} - \hat{\mu }_{i} }}{{\hat{\mu }_{i} }} \). Hence, it is an iterative maximum likelihood (ML) procedure. As has been shown by Kaçiranlar and Dawoud (2018), Månsson et al. (2012) and Türkan and Özel (2016) among others this estimator is sensitive to multicollinearity. A common solution to this problem is applying the ridge regression method (Hoerl and Kennard 1970). Another widely used method is applying the Liu type estimator introduced by Liu (1993) and introduced in the context of the Poisson regression model by Månsson et al. (2012). This method has certain advantages over the ridge regression method since it is a linear function of the biasing parameter d. Therefore, this method has become more popular during recent years (see, for example, Akdeniz and Kaciranlar 1995; Kaciranlar 2003). The unrestricted Liu type estimator for the Poisson regression model is defined by Månsson et al. (2012) as:
 where \( 0 \le d \le 1 \). Different versions of this estimator for count data models have been considered by Asar and Genç (2017), where a two parameter estimator is considered, Asar (2018), where the author discussed different perspectives on this estimator and Türkan and Özel (2018) in which the authors considered a jack-knifed estimator. A review article with detailed literature review is written by Algamal (2018). In some situations, the coefficient vector \( {\varvec{\upbeta}} \) is suspected to belong to a linear sub-space: where R is an qxp matrix of full rank with q < p and r is an qx1 vector. Under this restriction, the restricted estimator is more efficient than the OLS estimator. This method has been considered by several authors for the linear regression model (Alheety and Kibria 2009; Ehsanes Saleh and Golam Kibria 1993). The restricted maximum likelihood estimator (RMLE) of β suggested by Kibria and Saleh (2012) is given by: where \( {\mathbf{\hat{C} = X'\hat{W}X}} \). However, the issue of multicollinearity still exists since the inverse of \( {\hat{\mathbf{C}}} \) is instable as it is near singular. Therefore, the estimated coefficient vector has a high variance. As a solution one can use a combination of the Liu type estimator in Eq. (2) and the restricted estimator in Eq. (4). Kaçiranlar et al. (1999) have already done this and it has also been used by many others (Kibria and Saleh 2012; Li and Yang 2010; Yang and Xu 2009). Also for the logit model, Şiray et al. (2015) suggested the following estimator: where \( {\hat{\mathbf{F}}}_{{\mathbf{d}}} {\mathbf{ = }}\left( {{\mathbf{X'\hat{W}X + I}}} \right)^{{{\mathbf{ - 1}}}} \left( {{\mathbf{X'\hat{W}X + }}d{\mathbf{I}}} \right) \). This estimator has later been considered in Månsson et al. (2016), Wu and Asar (2017) and Varathan and Wijekoon (2018). The purpose of our paper is to introduce the restricted estimator and the restricted Liu type estimator in the context of the Poisson regression model. We also introduce some new estimators for the shrinkage parameter d. The new methods are mainly evaluated using a simulation study where one can clearly see the benefits of applying a restricted estimator. The rest of the paper is organized as follows: Sect. 2 gives the statistical methodology and estimation technique. A simulation study has been conducted in Sect. 3 and Sect. 4 analyzes real life data to illustrate our findings. Some concluding remarks are given in Sect. 5.",8
58.0,2.0,Computational Economics,24 July 2020,https://link.springer.com/article/10.1007/s10614-020-10030-4,A Markov Decision Process Model for Optimal Trade of Options Using Statistical Data,August 2021,Ali Nasir,Ambreen Khursheed,Faisal Mustafa,Male,Unknown,Male,Male,"Options provide the ability to sell or purchase a principle asset at a specific price (strike price) within a certain period (Jiang et al. 2019). One of the major challenges raised concerning the trading of options is to find the option’s ‘fair price’ (Ciosek and Silver 2015). This challenge can be dealt with by implementing an optimal exercise strategy that is the feedback rule that directs when to exercise an option at a strike price to maximize the accumulative return. Determination of such a feedback rule that works is not straight forward because many factors affect the dynamics of the strike price. For example, the fluctuations in strike price for a share of a company, e.g., Microsoft may depend upon the variations in operational expenses of the company or changes in the net profit. This implies that the determination of a ‘good’ strike price requires the analysis of the correlation between the price of an option and the relevant factors or ‘features’. Further discussion on these features and their effects on the strike price is deferred until after the mention of selected existing approaches for option price estimation. There are many models available for option pricing mainly focusing on European options, American options, and path-dependent options (Johansen et al. 2014; Huang and Lu 2018; Krznaric 2016). Researchers revealed that European options bear a fixed exercise boundary on the expiration date however, American-style options are traded more extensively (Van Der Hoek and Elliott 2012; Duan 2000). In European options, the method of pricing is based on assumptions of the probabilistic model governing the evolution of underlying assets (Ang and Timmermann 2012). However, in the case of American options, several numerical and analytical methods were introduced but they have not provided a satisfactory solution for accurate price estimation. Early efforts made to price options are the binomial lattice model introduced by Cox et al. (1979) and the finite difference method by (Brennan and Schwartz 1977). These methods are simple to implement but have practical limitations. Then Boyle (1986) introduced a Binomial model in which a middle price jump incorporated in a price tree was analyzed. But due to its limited application in considering multiple underlying assets under American options, investors started to refrain from relying on this method. Further, Broadie and Detemple (1997) found a trinomial model that dominated the binomial model because of both accuracy and speed but still proved inefficient in its practical outcome. Later, the most prominent breakthrough in pricing American options through Monte Carlo was found by Longstaff and Schwartz (2001). Its efficient performance has gained much attention but later on, several improvements have been suggested (Merton et al. 2002; Stentoft 2004; Zhao 2018). One of the most commonly used models is the Black–Scholes equation which is widely used because of its easiest computable form to predict the options prices however, this method involves some critical problems such as asymmetric leptokurtic feature and volatility smile, which are intolerable for practitioners (Rubinstein 1985; Hogarth and Makridakis 2015; Krznaric 2016; Macbeth and Merville 1979; Tsuzuki 2014; Shi et al. 2016). Other models include Brownian motion and related jump-diffusion which are considered to predict any continuous-time financial model but they failed to predict volatilities and skewness (Jobert and Rogers 2006). The above discussion establishes two things, first, there is substantial research available for estimating or predicting options price, second, none of the existing research methods focus on the factors affecting the options price or handling of uncertainty. These factors (or features), as discussed earlier in this section, have been represented implicitly in the statistical data by previous work related to the options price but an explicit treatment of such features is missing. Therefore, we propose a model in this paper that incorporates the uncertainty and the features affecting the option price in predicting the fluctuations in the price. Another difference in our proposed model compared to the existing literature is that our proposed model can be solved using dynamic programming for the calculation of optimal decision policy regarding the trade of options. Our model is based on the Markov Decision Process (MDP) framework. MDP frameworks have been used for complex decision making under uncertainty for over half a century. The MDP model consists of five elements, i.e., a set of states, a set of available decisions, a state and decision dependent reward or cost function, a set of state transition probabilities, and a discount factor. One of our major contributions in this paper is that we have developed a topology that is used to represent an options trading problem as the MDP. Other contributions involve the analysis and evaluation of the proposed technique. The intuition behind our proposed approach is that the options trading problem can be formulated as the optimal stopping issues that are related to the Markov Decision Processes (MDP) (Bertsekas 1995). Practically, by assuming complete knowledge of the MDP, a dynamic decision-making method can be used to determine the optimal policy (Bertsekas and Tsitsiklis 1995). Therefore, in this paper, we contribute to the options literature by revealing the Markov modeling technique leads to the optimized American options trade decisions. We also aim to analyze and eliminate the risk associated with options investment to find out the ways to deal with the uncertainty of the market trends. Our approach will be to use a discrete-time Markov chain with a finite transition rate. Particularly, this paper presents the mathematical model and simulation-based examples to demonstrate how to implement and utilize the proposed model. There is an extensive literature on using Markov chain model to predict the option prices. Jobert and Rogers (2006) used the classical results from the Wiener–Hopf factorization of the Markov process to predict the prices of options based on Markov modulated assets. Duan (2000) approximated the prices of underlying assets by a finite state, Markov Chain, and found that the matrix can increase the dimension of the Markov chain to get better numerical results. Simonato (2011) studied a numerical approach to American option pricing and used the transition density of the process to target the jump-diffusion under the Markov chain model. He and Zhu (2016) obtained an analytical formula to price European options with the help of the Markov chain model by proposing a new hybrid model with the volatility in the Henston model. Nonetheless, the previous literature shows that Markov chain model has been widely applied in the finance literature though there is less attention paid to use this model in predicting the American option future prices (Robinson and Aria 2018; Maruotti et al. 2018; Yu et al. 2018). For American options, the exercise boundary is unknown so the technique of pricing is crucial. Therefore, our study focuses on the prediction of American regular option prices by applying a discrete-time finite Markov decision process. Furthermore, the unique aspect of our work is to include the statistical data related to the features affecting options prices along with the statistical data of the options prices for the calculation of relevant conditional probabilities. The remainder of this study proceeds as follows. Section 2 covers the definition of the problem and our mathematical model as the MDP. Section 3 describes how the proposed model can be solved using dynamic programming to calculate the optimal policy. Section 4 presents the data description and empirical results. Finally, concluding remarks are given in Sect. 5.",2
58.0,2.0,Computational Economics,28 August 2020,https://link.springer.com/article/10.1007/s10614-020-10031-3,Computing Macro-Effects and Welfare Costs of Temperature Volatility: A Structural Approach,August 2021,Michael Donadelli,Marcus Jüppner,Christian Schlag,Male,Male,Male,Male,"There is near unanimous scientific consensus that climate change affects human health, behavior, and activity (Patz et al. 2005; Deschênes and Moretti 2009; Zivin and Neidell 2014; Cattaneo and Peri 2016) and has a negative impact on economic development (Stern 2007; Hsiang and Meng 2015). Over the past decades, the economic risk of climate change has been quantified by means of the so-called Integrated Assessment Models (IAMs). In this class of models climate change effects (costs and benefits) are captured via damage functions. IAMs easily allow to relate climate variables (e.g., temperature, sea-level rise, rainfall, \({\text {CO}}_{2}\) concentration) to economic welfare. However, even if widely used, IAMs have been subject to severe criticism. Above all, IAMs have been questioned to have no empirical supports (Pindyck 2013; Diaz and Moore 2017). Moreover, Pindyck (2013) argues that the use of IAMs as a climate change policy tool faces a major problem: “the modeler has a great deal of freedom in choosing functional forms, parameter values, and other inputs, and different choices can give wildly different estimates of the social cost of carbon and the optimal amount of abatement”. In other words, he points out that IAMs can deliver any result one desires. In the end the crucial flaws of IAMs make them “close to useless as tools for policy analysis” Pindyck (2013) (pag. 860). Another important issue is that IAMs focus exclusively on level effects rather than on growth effects. However, distinguishing between level and growth effects is of first order importance. Actually, the effects on the growth rate compound over time are more quantitatively important than effects on the level of output (Dell et al. 2012; Pindyck 2013; Colacito et al. 2019). Finally but equally importantly, existing climate change models (and in particular IAMs) have paid little attention on other macroeconomic variables (e.g. productivity growth, investment growth, labor supply and capital accumulation) other than GDP (see, for example, Revesz et al. 2014). To address some of the issues associated with the use of IAMs to examine the economic costs of climate change, more recent analyses have incorporated empirical evidence indicating that rising temperature levels have a negative impact on the real economic activity (Dell et al. 2012; Du et al. 2017; Colacito et al. 2019) into Dynamic Stochastic General Equilibrium (DSGE) models (Bansal et al. 2016; Donadelli et al. 2017). Empirical findings and quantitative model-based results confirm that a rise in the average temperature level has a negative impact on the growth rate of key macroeconomic aggregates (e.g., productivity, investment and consumption growth) and equity prices. However, as also pointed out by Diaz and Moore (2017), both standard IAMs and recently developed temperature-related DSGE models typically estimate the effects of equilibrium changes in average temperature levels (or in average rainfall levels), but not necessarily the effects of extremes (persistent heatwaves) or stochastic variability (storm surges). The impact of climate change may be actually the result of variations in both the mean and the standard deviation of climate drivers (Rind et al. 1989; Mearns et al. 1996). By focusing exclusively on changes in the mean, the overall and true impact of climate change on human activity could be seriously underestimated (Katz and Brown 1992; Schär et al. 2004). For example, fluctuations in climate conditions at the inter-annual time scale represent important drivers to capture extreme weather events such as multi-year droughts (Peel et al. 2005) and water scarcity (Veldkamp et al. 2015). In this respect, Elagib (2010) and Ito et al. (2013) show that intra-annual temperature variability is associated with extreme air temperature. Moreover, changes in the intensity of extreme weather events, such as heatwaves, are highly sensitive to shifts in intra-annual temperature variability (Fischer and Schär 2010). Thus, other than mean values, the dynamics of volatility in climate drivers may be relevant for the understanding of extreme events, and, consequently, for the impact of climate change on the real economic activity (Brown and Lall 2006). In other words, climate change as well as weather variability are both partly to blame. Supporting the view that volatile climate conditions matter, some studies have examined the relationship between weather dynamics and fluctuations in consumer spending. Heavy rain, snow, and other extreme events are factors forcing people to stay home. This in turn would lower sales (Parsons 2001). Broadly, the idea is that highly volatile weather conditions may impact consumption decisions (Starr-McCluear 2000; Lazo et al. 2011). Another stream of research argues that the effect of weather on consumer spending is mediated by mood. High variability in weather conditions has a negative impact on mood. For instance, Spies et al. (1997) and Murray et al. (2010) empirically observe that people in good mood tend to be more willing to buy consumer goods than those in bad moods. In a similar spirit, another branch of literature finds instead that stock market anomalies may be the consequence of relevant weather factors (Saunders 1993; Kamstra et al. 2003; Cao and Wei 2005). For example, Kamstra et al. (2003) and Garrett et al. (2005) find that seasonal weather effects (such as the number of daylight hours in a day) tend to influence investors’ risk-aversion. Taken together, existing empirical evidence support the existence of two channels through which an adverse shock in climate conditions may affect economic factors. The first one operates via the destruction of capital through adverse weather events dampening innovations, output, and productivity (Fankhauser and Tol 2005; Stern 2013). The second one operates via the influence that weather has directly on people mood. Actually, a bad mood due to extreme weather translates into consumption spending (Spies et al. 1997; Murray et al. 2010) and equity investments (Kamstra et al. 2003; Cao and Wei 2005). Loosely speaking, more volatile weather conditions lead to a higher probability of extreme events, which in turn implies stronger effects on investment and consumption plans. Motivated by this evidence, we examine the effects of volatility in weather conditions on macro-variables and asset prices. Specifically, we investigate both empirically and theoretically whether shifts in the volatility of temperature affect aggregate productivity, economic growth, welfare, and equity prices. While the majority of climate change studies examine the effect of rising temperatures on real economic activity, to the best of our knowledge there is no study focusing on temperature volatility and its macroeconomic effects. With this paper we aim to fill this gap. Monthly data on UK temperature for the period 1659–2015 are employed to build an intra-annual temperature volatility index. Since both relatively low and relatively high temperature volatility may be harmful, our temperature volatility index is represented by the absolute deviation from an annual benchmark volatility value (i.e., historical average intra-annual volatility observed in the pre-industrial revolution era).Footnote 1 We then employ data on TFP (macro-aggregates, stock market, risk-free rate) for the period 1800–2015 (1900–2015). Empirically, we study the effect of changes in temperature volatility via Granger causality and standard VAR analyses over different historical periods (i.e., 1800–1900, 1900–1950, and 1950–2015). We find that the sign of the causality going from temperature volatility to TFP growth changes over time. By focusing on the 19th century, we find no evidence of a causality between temperature volatility and productivity growth.Footnote 2 For the period 1900–1950, we instead find a significant positive unidirectional causality from temperature volatility to productivity growth. In the post-war period, the direction of the causal effect remains unaltered but not the sign which becomes negative (i.e., temperature volatility has a negative effect on productivity). On one hand, this change in the direction of the causality could be due to the different sectoral structure characterizing the UK economy in different eras. On the other hand, it can be driven by the increasing number of extreme weather events observed over the last three decades. VAR investigations confirm that the way in which temperature volatility affects TFP growth is not constant over time. During the period 1800–1900 no significant effects are detected. Differently, the temperature volatility has a positive (negative) effect over the period 1900–1950 (1950–2015). These results are robust to the inclusion of macroeconomic and financial variables. Over the period 1950–2015, temperature volatility is also found to be detrimental for equity valuations. A battery of cross-sectional asset pricing tests suggest then that temperature volatility shocks command a significant and positive risk premium. This set of novel empirical facts is rationalized by means of a production economy featuring long-run macro and temperature volatility risk. More precisely, we calibrate the model to match (1) the drop in TFP growth generated by a temperature volatility shock and (2) main UK temperature statistics. Note that we chose the post-war sample since we find the strongest adverse climate economic effects in this period. This is in line with existing evidence documenting negative temperature effects in the post-war period (see e.g., Dell et al. 2012; Colacito et al. 2019). In our production economy a temperature volatility shock gives rise to a negative response of productivity, macroeconomic quantities, and equity valuations, consistent with our novel empirical evidence. In addition, in the model temperature volatility risk commands a positive risk premium. Welfare costs of this type of risk are substantial and amount to 9% of the agent’s consumption bundle in our benchmark scenario. Moreover a rise in temperature volatility is found to have long-lasting negative effects on output and labor productivity growth. Over a 50-year horizon, a single one-standard deviation shock reduces both cumulative output and labor productivity by about 1.0 percentage points (pp). In an economy featuring capital depreciation risk, welfare costs of temperature volatility risk increase when depreciation shocks are positively correlated with temperature volatility shocks, meaning that higher climate variability results in an increasing occurrence of natural disasters, which destroys capital faster. If we allow for adaptation to climate uncertainty by assuming a positive correlation between temperature volatility shocks and long-run productivity shocks (i.e., the economy immediately responds to temperature volatility shocks by boosting productivity), temperature volatility risk produces welfare gains and a drop in the equity risk premium. Needless to say, this evidence suggests that “adaptation” plays a crucial role in reducing the economic costs associated to temperature volatility risk. Our benchmark production economy features capital and labor dynamics. For reasons of robustness, we also study the macro and welfare effects of temperature volatility shocks in an endowment economy. By calibrating the model to match the main consumption dynamics and the empirically observed impact of temperature volatility shocks on consumption growth, we find qualitatively and quantitatively similar results. The rest of this paper is organized as follows. In Sect. 2, we review the related literature. Section 3 presents the main empirical findings concerning the effects of temperature volatility shocks on macro and financial aggregates. In Sect. 4, we describe our production economy featuring temperature volatility risk. Section 5 presents the quantitative results. To shed light on the robustness of the quantitative implications of temperature volatility shocks, we analyze an endowment economy in Sect. 6. Section 7 concludes.",12
58.0,2.0,Computational Economics,29 July 2020,https://link.springer.com/article/10.1007/s10614-020-10033-1,Accelerating FHS Option Pricing Under Linear GARCH,August 2021,Haibin Xie,Xinyu Wu,Pengying Fan,Unknown,Unknown,Unknown,Unknown,,
58.0,2.0,Computational Economics,19 August 2020,https://link.springer.com/article/10.1007/s10614-020-10034-0,On a Bivariate Hysteretic AR-GARCH Model with Conditional Asymmetry in Correlations,August 2021,Cathy W. S. Chen,Hong Than-Thi,Manabu Asai,Female,,Male,Mix,,
58.0,2.0,Computational Economics,26 August 2020,https://link.springer.com/article/10.1007/s10614-020-10037-x,A Generalized Time Iteration Method for Solving Dynamic Optimization Problems with Occasionally Binding Constraints,August 2021,Ayşe Kabukçuoğlu,Enrique Martínez-García,,Female,Male,Unknown,Mix,,
58.0,2.0,Computational Economics,12 September 2020,https://link.springer.com/article/10.1007/s10614-020-10041-1,Forecasting Volatility for an Optimal Portfolio with Stylized Facts Using Copulas,August 2021,Aida Karmous,Heni Boubaker,Lotfi Belkacem,Female,Unknown,Male,Mix,,
58.0,2.0,Computational Economics,10 September 2020,https://link.springer.com/article/10.1007/s10614-020-10043-z,Pricing European Option Under Fuzzy Mixed Fractional Brownian Motion Model with Jumps,August 2021,Wei-Guo Zhang,Zhe Li,Yue Zhang,,,,Mix,,
58.0,2.0,Computational Economics,20 September 2020,https://link.springer.com/article/10.1007/s10614-020-10044-y,Parallel Extended Path Method for Solving Perfect Foresight Models,August 2021,N. B. Melnikov,A. P. Gruzdev,B. C. O’Neill,Unknown,Unknown,Unknown,Unknown,,
58.0,2.0,Computational Economics,17 September 2020,https://link.springer.com/article/10.1007/s10614-020-10046-w,Foreign Currency Power Option Pricing Based on Esscher Transform,August 2021,Wenhan Li,Cuixiang Li,Mengna Wang,Unknown,Unknown,Unknown,Unknown,,
58.0,3.0,Computational Economics,20 June 2021,https://link.springer.com/article/10.1007/s10614-021-10142-5,Computational Aspects of Sustainability,October 2021,George E. Halkos,Kyriaki D. Tsilika,,Male,Female,Unknown,Mix,,
58.0,3.0,Computational Economics,21 June 2019,https://link.springer.com/article/10.1007/s10614-019-09902-1,Towards Better Computational Tools for Effective Environmental Policy Planning,October 2021,George E. Halkos,Kyriaki D. Tsilika,,Male,Female,Unknown,Mix,,
58.0,3.0,Computational Economics,18 November 2019,https://link.springer.com/article/10.1007/s10614-019-09948-1,Performance Management of Supply Chain Sustainability in Small and Medium-Sized Enterprises Using a Combined Structural Equation Modelling and Data Envelopment Analysis,October 2021,Prasanta Kumar Dey,Guo-liang Yang,Konstantinos Evangelinos,Female,Unknown,Male,Mix,,
58.0,3.0,Computational Economics,03 December 2019,https://link.springer.com/article/10.1007/s10614-019-09952-5,Evaluation of Urban Competitiveness of the Huaihe River Eco-Economic Belt Based on Dynamic Factor Analysis,October 2021,Malin Song,Qianjiao Xie,,Female,Unknown,Unknown,Female,"As hubs of population agglomeration, cities include residential, industrial, and commercial areas, and have administrative jurisdiction over residential areas, streets, hospitals, schools, office buildings, commercial properties, squares, parks, and other public green spaces and facilities. China’s urbanization has accelerated rapidly in recent years. According to data released by the National Bureau of Statistics, China’s urbanization rate reached 57.35% in 2016, a significant increase from 36.22% in 2000. China’s urbanization has led to the development of urban economies. As resource limitations have led to intensifying competition between cities, urban competitiveness has come under scrutiny. While urban competitiveness can be taken as the comparative advantages of one city over others, it is also the ability of a city to optimally allocate its resources to improve its development, including urban potential competitiveness (urban resource management), urban core competitiveness (urban industry), urban comprehensive competitiveness (urban environment and management), and urban future competitiveness (urban development strategy). Urban development refers to a city’s economic structure, culture, policy, and institutions and is systematic, differentiated, dynamic, open, and relative in nature. The State Council’s Huaihe River Eco-economic Belt Development Plan issued in November 2018 calls for the strategic orientation of “three belts in one region” and urges resource-based cities’ transformation by adjusting policies to develop the ecological economy, strengthen the implementation and management of general land use planning, and accelerate urbanization and agricultural modernization. Therefore, urban competitiveness is central to the construction of the Huaihe River eco-economic belt and the development potential of its cities. Urban competitiveness is the concrete embodiment of a city’s comprehensive strength. Cities with strong competitiveness have great potential for development while the opposite is true for cities with weak competitiveness. Thus, it is of great significance for development in the Huaihe River basin and the construction of the eco-economic belt to comprehensively evaluate and understand the competitiveness of its cities. In addition, the evaluation of urban competitiveness is important in formulating appropriate development strategies to promote the overall strength of the Huaihe River eco-economic belt and the coordinated development of central China. Research on urban competitiveness has mainly focused on three aspects: the connotation of urban competitiveness (what is urban competitiveness); how to evaluate urban competitiveness; and the construction of a theoretical model on urban competitiveness (which model is suitable for research and how to build the model). Urban competitiveness includes the economic, social, and environmental aspects of a city. Therefore, urban competitiveness must be comprehensively considered from the perspectives of urban economic institutions, environmental quality, and quality of life. With the rapid development of China’s economy, science, and technology, cities are continuously developing. Different from economic development, urban competitiveness reflects the development capacity of a city. Therefore, if the evaluation of urban competitiveness is carried out solely for a specific year, the result will be one-sided and will not accurately reflect a city’s potential development capacity. Thus, a dynamic evaluation model should be selected to analyze the changing trend of urban competitiveness from a dynamic perspective. Based on existing literature, this paper takes the Huaihe River eco-economic belt as the research object, constructs appraisal models of urban competitiveness, and adopts the dynamic factor analysis method to evaluate the competitiveness of 26 cities in the Huaihe River eco-economic belt. The competitiveness of each city is compared and ranked to analyze the influencing factors of urban competitiveness and understand the status and dynamic and static variation trends of each city. This research could help direct decision-making in the construction of the Huaihe River eco-economic belt. This paper is divided into five sections. The first introduces the research topic, that is, how to measure and evaluate the urban competitiveness of Huaihe River eco-economic belt and the significance of urban competitiveness evaluation. The second reviews relevant literature and summarizes the measurement and evaluation methods of urban competitiveness. The third introduces the index system and method to measure the urban competitiveness of the Huaihe River eco-economic belt using dynamic factor analysis. The fourth presents the empirical test results of the theoretical model using sample data. The fifth is the summary and conclusion, explaining the significance of the research results and direction for future research.",15
58.0,3.0,Computational Economics,29 January 2020,https://link.springer.com/article/10.1007/s10614-019-09962-3,MOLES: A New Approach to Modeling the Environmental and Economic Impacts of Urban Policies,October 2021,Ioannis Tikoudis,Walid Oueslati,,Male,Male,Unknown,Male,"This paper presents the structure, architecture and a real-world application of the Multi-Objective Local Environmental Simulator (MOLES), i.e. OECD’s new urban Computable General Equilibrium (CGE) model with selected microsimulation features. The model is tailored to assess the performance of local and national policy instruments that target transportation, energy consumption from mobile sources, and land use in urban areas. Among others, these instruments include: zoning regulations, public transportation subsidies, fuel and kilometer taxes, regulations in vehicle ownership and use, congestion pricing, as well as vehicle registration and circulation fees. MOLES assesses these interventions from multiple perspectives. That uncovers the potential trade-offs between different policy objectives, such as environmental performance, economic efficiency, as well as fiscal and distributional balance. Moreover, the model is designed to capture possible synergies between urban planning and transportation policies. The need for an urban environmental model with solid microeconomic foundations emerged from the evolution of city as the primary domain of innovation and growth, and as the locus where some of the most urgent environmental challenges will have to be addressed. Urbanization did not abruptly become one of the cornerstones of twentieth century’s vast economic expansion. For decades, the uninterrupted economic growth fueled—and was fueled by—a constant increase in urban population and the land uptake of cities. Slowly, a series of interrelated challenges that exert pressure on the ability of cities to generate prosperity emerged. Local air pollution and climate change are two characteristic examples of these challenges. Rough measures by the OECD (2010, 2015a, b) reveal that 60–80% of the global CO2 emissions are generated in cities.Footnote 1 Furthermore, cities span the areas where the largest part of air-pollution is generated and dispersed, exposing considerable fractions of population to it. To mitigate the negative welfare effects of the above, existing policies that affect economic activity in urban areas have to be streamlined, and new ones have to be conceptualized and developed. The need to examine environmental policies from an urban perspective is expected to grow, as cities expand and the associated challenges intensify. In 2050, about 70% of the world population, a number that raises to 85% for OECD countries, will be living in urban areas. The global urban land cover is projected to steeply increase: from 603 thousand km2 it has been in 2000 to over 3 million km2 in 2050 (Angel et al. 2011). The trends show that such a growth is likely to take the form of a major expansion of low-density suburban fabric into the countryside.Footnote 2 The resulting urban development pattern, widely referred to as urban sprawl, has serious environmental repercussions.Footnote 3 The increase in transport-related energy consumption, caused primarily by the longer distances private vehicles have to traverse in sprawled cities, is the most important of these consequences.Footnote 4 Moreover, urban expansion is highly likely to exacerbate the problem of air pollution for two reasons. The first reason is that urbanization implies higher rates in the ownership and more intense use of private vehicles. That relationship is particularly strong in developing countries, where urbanization is accompanied by a rapid increase in real income (Dargay 2001; Eskeland and Feyzioglu 1997).Footnote 5 The second reason is that the overall increase in urban population implies that more people are going to reside in areas where the concentration of various air pollutants is high. In turn, this implies that the expected exposure of an average person to air pollutants is going to increase, even if the concentration levels of these air pollutants within cities remain at today’s levels. The OECD (2012) estimates that the worldwide mortality due to particulate matter (PM2.5 and PM10) is expected to increase from 1 million in 2000 to over 3.5 million in 2050. Moreover, OECD (2016) projects that, in the absence of more stringent policies, the number of annual premature deaths due to air pollution will increase from 3 million people in 2010 to 6–9 million in 2060. The associated monetized cost will raise from USD 3 trillion in 2015 to USD 18–25 trillion in 2060, with the most affected areas being those densely populated with high concentrations of PM2.5.Footnote 6 The model we present in this paper enables the detailed examination of policies that may be part of the answer to these challenges. MOLES is unique in that it combines the internal consistency of a Computable General Equilibrium (CGE) model with the useful details of a microsimulation model. As a CGE model, it represents the value flows between various sectors and stakeholders in a micro-founded way. Unlike standard CGE models, however, MOLES abstracts from detailed representations of industrial production sectors. Instead, it models with high resolution the behavior of the real estate development sector, transportation providers and urban households, as these agents determine the locational and mobility patterns in an urban area.Footnote 7 That is, MOLES differentiates housing markets by location and residential type. It represents urban public transportation by various modes and private transportation by various vehicle types. That resolution allows the examination of policies affecting population density, urban structure and mobility patterns. The CGE nature of MOLES facilitates the exploration of feedback effects of land-use policies on transportation demand and of transportation policies on the long-run urban development.
 The microsimulation elements incorporated in MOLES introduce behavioral margins of high environmental and economic importance. More specifically, in MOLES the decision of whether to own a private vehicle from a certain class of vehicles (e.g. internal combustion engine, electric) is endogenous. Individuals decide the optimal number of trips, their type (i.e. commuting, shopping and leisure) and their destination locations. They also schedule the departure time of these trips across different types of days (working days, bank holidays) and across different periods of one day (e.g. on-peak, off-peak). Most important, travel demand is modelled separately from the ownership decision. That is, individuals select which of the scheduled trips will be realized by a private vehicle, public transportation, soft mobility (i.e. walking or bike), or combinations of them. In each case, mode choice satisfies the constraints imposed by vehicle ownership, household location and trip destination. The aforementioned elements enable the analysis of policies that aim to promote greener forms of urban transportation. They also facilitate the assessment of policies that alter the within-day travel patterns, by moving part of the traffic from the on-peak to less congested time intervals. To illustrate clearer the structure of the model, we apply it to area of Auckland. This is the biggest and most rapidly growing city of New Zealand, the country’s largest agglomeration of knowledge-based industries and one of the most important cities in the region of South Pacific. The rapid population growth and the stringent urban planning regulations have given rise to a series of challenges to economic and environmental sustainability of the region. First, the steep increase in the demand for residential floor space and the regulatory barriers to housing supply contributed to a substantial rise in housing costs. In the 6-year period between 2011 and 2017, the real house prices in Auckland grew by 70%Footnote 8; For the period 1985–2018, real housing prices rose by more than 200%. That growth is not aligned with the corresponding real growth of the average wage, which in New Zealand was approximately 35% for the period 1990–2015 (OECD 2019a, b). If that misalignment continues uninterrupted, it may undermine housing affordability, posing a threat to long-term prosperity. Second, the rigidity of density regulations in real-estate development has contributed to more urban sprawl and car dependency. Urban sprawl implies large distances between the key points of economic activity, such as residences, jobs, shopping hubs and major leisure attractions. These distances are covered almost exclusively by private vehicles. Furthermore, the low suburban density implies that only a limited portion of the costs associated with the provision of frequent public transportation services will be recovered by fare revenue. Therefore, public transportation is hard to thrive outside the city’s inner core, as satisfactory levels of service can be realized only with substantial budget deficits, especially outside the peak hours. Using a counterfactual experiment, we explore the environmental, fiscal and welfare impacts from a reform that promotes a massive switch to public transportation. That reform induces a drastic increase in the kilometer cost of car use, by substantially increasing the fuel tax by NZD 0.60/l and the flat kilometer charge by NZD 0.40/km. It also increases the annual fixed costs of car ownership by raising the circulation fees. The policy experiment offsets the negative effects of the above measures on private welfare by introducing a massive subsidy to public transportation fares and through a considerable relief from traffic congestion. We find that the above package eliminates 45% of the city’s transport-related greenhouse gas emissions and generates an annual fiscal surplus of approximately NZD 600 per capita. We show that the annual welfare impact of the reform is considerable (NZD 545 per capita), even without accounting for the social value of carbon reduction or recycling the fiscal surplus in the economy. Using insights from the literature of optimal taxation and proposed estimates for the social cost of carbon, we derive an “all-in-one” estimate for the annual welfare gain of the reform. That amounts to NZD 1035, or 2.1% of the per capita net income. Using the spatial properties of the model, e.g. housing prices, we shed light into the potential distributional impacts of the policy. We identify an increase in the average land and housing prices, which stems from substituting away from transportation in favour of housing expenditure. However, that price increase does not occur in a spatially uniform manner. Instead, prices grow much more in areas that possessed an initial locational advantage. The reason is that the steep increases in the kilometer and gasoline taxes, i.e. two key components of the package, reinforce these initial locational advantages. That implies a substantial divergence of housing prices across space, a result that may have ample distributional consequences, which depend on the initial distribution of land among different population cohorts. These consequences may hamper the public acceptability of the reform, calling for spatially varying compensatory measures. The paper is organized as follows. Section 2 provides a detailed documentation of MOLES, as the latter is adjusted to the needs and idiosyncratic characteristics of Auckland. Section 3 presents the solution algorithm of the model. Section 4 displays the benchmark equilibrium and the counterfactual experiment, and examines the welfare, environmental and fiscal impact of the policy change. Section 5 summarizes. The accompanying Technical Appendices provide additional information, chart flows, descriptions of the solution algorithms and pseudocode. Finally, for a more generic presentation of MOLES that is disengaged from any application, the interested reader is referred to Tikoudis and Oueslati (2017).",4
58.0,3.0,Computational Economics,02 January 2020,https://link.springer.com/article/10.1007/s10614-019-09963-2,Pollution and Health Effects: A Nonparametric Approach,October 2021,George Halkos,Georgia Argyropoulou,,Male,Female,Unknown,Mix,,
58.0,3.0,Computational Economics,30 July 2020,https://link.springer.com/article/10.1007/s10614-020-10025-1,Making Predictions of Global Warming Impacts Using a Semantic Web Tool that Simulates Fuzzy Cognitive Maps,October 2021,Athanasios Tsadiras,Maria Pempetzoglou,Iosif Viktoratos,Male,Female,Male,Mix,,
58.0,3.0,Computational Economics,26 October 2020,https://link.springer.com/article/10.1007/s10614-020-10045-x,A Guide on Solving Non-convex Consumption-Saving Models,October 2021,Jeppe Druedahl,,,Male,Unknown,Unknown,Male,"Multi-dimensional consumption-saving models with adjustment costs or discrete choices are typically hard to solve numerically due to the presence of non-convexities. Starting from value function iteration (VFI), which is simple and straightforward, but inherently slow, this paper provides a guide on reducing computational time in such models using three layers of optimization. In the first layer, I use that many consumption saving models have a nesting structure such that the continuation value can be efficiently pre-computed and the consumption choice solved separately before the remaining choices. I refer to this as the nested value function iteration (NVFI). In the second layer, I use that the nested consumption problem under weak assumptions can be solved efficiently by using an extension of the endogenous grid method (EGM) originally developed by Carroll (2006) for one dimensional models.Footnote 1 I refer to this as the nested endogenous grid method (NEGM). This step relies on a one-dimensional version of the multi-dimensional upper envelope algorithm developed in Druedahl and Jørgensen (2017). In the third layer of optimization, I use that the pre-computation of the continuation value needed in both NVFI and NEGM can be computed efficiently by introducing a novel loop reordering reducing the number of computations and improving memory access. I refer to the improved solution methods as NVFI+ and NEGM+. To study the implications for speed and accuracy of the proposed optimizations, I use a buffer-stock consumption-saving model with non-durable and durable consumption and adjustment costs similar to Berger and Vavra (2015). I show that NEGM+ relative to VFI provides a speed-up factor of almost 50 for a given level of accuracy. The speed-up is reduced to a factor of 15 without the optimized interpolation approach. NVFI provides a speed-up factor of about 10.5, increasing to 13.5 with the optimized interpolation approach. These results are furthermore obtained when using a non-robust version of VFI, where the underlying non-convex optimization problems are solved with a local solver without any multi-start, which therefore could end up in local maxima. I discuss how this speed-up can be expected to vary with both the model specification and implementation choices. Similar results are obtained for an extended benchmark model with two durable stocks adding both a state and a choice to the model. Code libraries to implement the proposed solution algorithms are provided in both a C++ version, and a somewhat slower, but more accessible Python version.Footnote 2 Related literature Existing extensions of the endogenous grid method does either not consider non-convexities (Barillas and Fernández-Villaverde 2007; Hintermaier and Koeniger 2010; White 2015; Ludwig and Schön 2018) or restrict attention to the one-dimensional case (Fella 2014; Iskhakov et al. 2017). This paper adds to the previous extensions of the endogenous grid method by simultaneously handling multi-dimensional models and non-convexities. The only other paper, which provides a EGM algorithm to solve multi-dimensional models with non-convexities is Druedahl and Jørgensen (2017). Their \({\hbox {G}}^{2}\)EGM algorithm, however, requires that the equation system of stacked discrepancies in the first order conditions and post-decision state equations have a unique solution. This can be cumbersome to determine for a given model, and small model changes can alter whether \({\hbox {G}}^{2}\)EGM is applicable or not (see their discussion in Sect. 5.4). By comparison, the NEGM only uses the first order condition for consumption, and it is thus more generally applicable. As an example, the benchmark model used in this paper cannot be solved by \(\text {G}^{2}\text {EGM}\). The modularity of NEGM is an additional benefit relative to \({\hbox {G}}^{2}\)EGM. The algorithms for computing the upper envelope and speeding up the interpolation step presented in this paper can be re-used across different models with little (or no) modification. The code thus becomes more generic and easier to understand and debug. For models where both \({\hbox {G}}^{2}\)EGM and NEGM can be applied, the former will most likely be faster, if the fast interpolation scheme proposed in this paper is used, because it completely avoids any VFI step. However, \({\hbox {G}}^{2}\)EGM typically requires computing additional value function derivatives, and the multi-dimensional upper envelope is more costly than the one-dimensional upper envelope used in NEGM. I show that in the case of the model from Druedahl and Jørgensen (2017) the \({\hbox {G}}^{2}\)EGM is only about 25% faster than NEGM in their baseline specification, but somewhat more accurate. In specifications with uncertainty where numerical integration is required the result can flip. The idea of pre-computing the continuation value, which EGM fundamentally relies on, is well-known in the operations research and engineering literature. For infinite horizon models it is thus common to iterate on a Bellman equation in the post-decision value function; see in particular Van Roy et al. (1997), Powell (2011) and Bertsekas (2012).Footnote 3 The idea of using pre-computations to limit the costs of numerical integration was also proposed by Judd et al. (2017) in an algorithm where the value function is approximated by a parametric function. The idea of reformulating the model using its nesting structure is similar to the general idea of plan factorization discussed in Ma and Stachurski (2018). Structure The paper is henceforth structured as follows. Section 2 presents the solution methods in light of a specific model. Section 3 presents speed and accuracy results comparing the proposed solution methods with VFI. Section 4 presents the general model class where the proposed solution methods can be used. Section 5 provides a comparison with \({\hbox {G}}^{2}\)EGM. Section 6 concludes.",2
58.0,3.0,Computational Economics,29 September 2020,https://link.springer.com/article/10.1007/s10614-020-10048-8,Statistical Validation of Multi-Agent Financial Models Using the H-Infinity Kalman Filter,October 2021,G. Rigatos,,,Unknown,Unknown,Unknown,Unknown,,
58.0,3.0,Computational Economics,19 September 2020,https://link.springer.com/article/10.1007/s10614-020-10049-7,Social Influence of Competing Groups and Leaders in Opinion Dynamics,October 2021,Catherine A. Glass,David H. Glass,,Female,Male,Unknown,Mix,,
58.0,3.0,Computational Economics,01 October 2020,https://link.springer.com/article/10.1007/s10614-020-10052-y,The Valuation of Weather Derivatives Using One Sided Crank–Nicolson Schemes,October 2021,Peng Li,,,,Unknown,Unknown,Mix,,
58.0,3.0,Computational Economics,28 September 2020,https://link.springer.com/article/10.1007/s10614-020-10053-x,Coalition Feature Interpretation and Attribution in Algorithmic Trading Models,October 2021,James V. Hansen,,,Male,Unknown,Unknown,Male,"Shapely values have been applied in market trading research for some time. One of the foremost purposes has been the analysis of portfolio risk (Mussard and Terraza 2008). More recently, as data sets have grown in size and complexity, advances in machine learning (ML) technology have unlocked new and promising possibilities. Rida (2019) applies ML to credit scoring, with encouraging results. The study’s XGBoost ML ensemble attained better results than by methods conventionally used by commercial banks. Central to this improvement was the integration of SHAP values to determine feature importance, which enabled users to interpret and improve the resulting model. Other benefits included promoting trust in the model, hypotheses of causal relationships, and assessing when legal requirements were being satisfied. This is noteworthy, as limitations on interpretability have been a constraining factor in the use of powerful methods such as deep learning and ensemble models in banking, insurance, healthcare, and other industries (Lundberg and Erion 2018). The issue has been underscored by the growing availability of big data, which has boosted the potential for complex models (Ribeiro et al. 2016). This has incentivized an accelerated interest in developing robust methods for interpreting machine learning models and measuring feature importance (Bach 2015; Hall and Gill 2018; Jansen 2018; Koshiyama et al. 2020; Lipovetsky and Conklin 2001; Shrikumar et al. 2017; Shrikumar 2016). Yet, it is shown in (Lundberg and Lee 2017) that all these methods can be inconsistent. That is to say, the features that are actually most important may not always be given the highest feature importance score. In fact, a model can change such that it relies more on a given feature, yet the importance estimate assigned to that feature decreases. This casts doubt on any comparison between features and impedes the goal of conjoining interpretability with high levels of accuracy. Fortunately, recent research has advanced a resolution, which is the focus of this paper. Our contribution is a novel integration of computational research on cooperative game theory, with algorithmic trading. Algorithmic trading is of interest because it utilizes computer programs to automate selected elements of trading strategy. Algorithms pursue optimization throughout the investment process, from asset allocation to idea-generation, trade execution, and risk management. The informational advantage of ML is the ability to analyze large quantities of data in real time. The capabilities of ML have grown rapidly due to the increase in computing power at lower cost, accompanied by advances in ML methods for analyzing complex datasets (Rida 2019). An extensive literature search found just two papers related to the use of SHAP values in financial trading. In addition to Rida (2019), Koshiama et al. (2020) reviewed artificial intelligence and ML applied to capital market trading. Their appraisal of over 120 research papers supports the postulate that ours is the first study to fuse ML and SHAP to analyze overlapping returns, back-testing, and profit-and-loss metrics. Because of the sparse literature, we have omitted a separate section on that topic. The remainder of the paper proceeds with Sect. 2, which provides a short review of the basic concepts relevant to the SHAP method. Section 3 outlines the fundamentals of decision tree SHAP theory. Section 4 introduces our schema for SHAP-supported algorithmic trading and its results. A summary and concluding remarks comprise Sect. 5.",
58.0,3.0,Computational Economics,09 October 2020,https://link.springer.com/article/10.1007/s10614-020-10055-9,Pricing Exotic Option Under Jump-Diffusion Models by the Quadrature Method,October 2021,Jin-Yu Zhang,Wen-Bo Wu,Zhu-Sheng Lou,,,Unknown,Mix,,
58.0,3.0,Computational Economics,15 October 2020,https://link.springer.com/article/10.1007/s10614-020-10056-8,An Integral Equation Representation for Optimal Retirement Strategies in Portfolio Selection Problem,October 2021,Junkee Jeon,Hyeng Keun Koo,Zhou Yang,Unknown,,,Mix,,
58.0,3.0,Computational Economics,13 November 2020,https://link.springer.com/article/10.1007/s10614-020-10070-w,On the Solution of the Black–Scholes Equation Using Feed-Forward Neural Networks,October 2021,Saadet Eskiizmirliler,Korhan Günel,Refet Polat,Female,Male,Unknown,Mix,,
58.0,4.0,Computational Economics,04 April 2020,https://link.springer.com/article/10.1007/s10614-020-09980-6,A Unifying Model for Statistical Arbitrage: Model Assumptions and Empirical Failure,December 2021,Jeff Stephenson,Bruce Vanstone,Tobias Hahn,Male,Male,Male,Male,"Statistical arbitrage is a highly quantitative trading and investment strategy that had its humble beginnings in the ubiquitous pairs trading methodology made famous by Morgan Stanley in the 1980s (Gatev et al. 2006). The initial incarnation of pairs trading sought to identify and exploit temporal market inefficiencies between two related stocks. Though the complexity of the methods and models used in pairs trading has grown considerably, the essence of the approach remains unaltered; a pair of securities which are exposed to similar market forces or share some statistical relationship are identified, with positions being entered when the securities’ prices diverge significantly from one another. The method seeks to capture profit by opening a short position in the overvalued security and a long position in the undervalued security, expecting that the temporary divergence will reverse in the future and allow prices to converge once more to their historical equilibrium. The declining profitability of statistical arbitrage, as observed by Gatev et al. (2006) and confirmed by Do and Faff (2010), represents a considerable challenge to our understanding of the phenomenon, and its practical considerations. While a broad subset of the literature is devoted to understanding the determinants of statistical arbitrage profitability, relatively few papers consider its incidence of failure. Despite overwhelming empirical evidence of the cause of such failure, namely non-convergence of candidate paired securities, the implicit assumptions of statistical arbitrage have not been challenged by academia. This paper aims to clarify the implicit assumptions of statistical arbitrage, which have until now been obfuscated by the competing approaches to the identification and exploitation of such phenomena. The transparency of the implicit model reveals four contributions to the academic understanding of statistical arbitrage, specifically: the distance approach is not only sub-optimal, it potentially ignores a large segment of arbitrage opportunities; the distance, cointegration and time series approaches are unified under a single implicit model that can be explicitly expressed as an Ornstein–Uhlenbeck (OU) process with stationary mean; arbitrage opportunities that are exploited under the implicit model will eventually fail within finite time; an OU process with time-varying mean better reflects the salient features of the empirical literature, and produces superior returns to the conventional implicit model. Section 2 begins with a review of the literature, emphasising the declining profitability of statistical arbitrage and investigations of its cause. Section 3 reveals the implicit model used by arbitrageurs, addressing its limitations and revisiting its assumptions to propose a more appropriate model. Section 4 concludes and offers potential avenues for future research.",
58.0,4.0,Computational Economics,13 April 2020,https://link.springer.com/article/10.1007/s10614-020-09981-5,A New Dynamic Mixture Copula Mechanism to Examine the Nonlinear and Asymmetric Tail Dependence Between Stock and Exchange Rate Returns,December 2021,Kuang-Liang Chang,,,Unknown,Unknown,Unknown,Unknown,,
58.0,4.0,Computational Economics,27 April 2020,https://link.springer.com/article/10.1007/s10614-020-09982-4,Reinforcement Learning in a Cournot Oligopoly Model,December 2021,Junyi Xu,,,Unknown,Unknown,Unknown,Unknown,,
58.0,4.0,Computational Economics,02 May 2020,https://link.springer.com/article/10.1007/s10614-020-09987-z,Research on the Effects of Institutional Liquidation Strategies on the Market Based on Multi-agent Model,December 2021,Qixuan Luo,Yu Shi,Handong Li,Unknown,,Unknown,Mix,,
58.0,4.0,Computational Economics,09 May 2020,https://link.springer.com/article/10.1007/s10614-020-09985-1,A Computational Approach to Uncovering Economic Growth Factors,December 2021,Mohsen Ahmadi,,,Male,Unknown,Unknown,Male,"Due to the rapid development and quick return on investment and significant impact on economic growth, Knowledge-Based Economy(KBE) is considered as one of the most important economic issues in the last two decades (Kukuła 2013). The efficiency of the knowledge-based economy requires a definition of the mechanisms and identifying the factors on production and application of knowledge and with the correlation of these factors with each other, increasing the performance of other sections is also provided (Korres 2016). Economists surveyed effective factors of KBE on economic growth, some suggest that business indicators and financial supports of government are the main factors in a small company (Nia and Kojoori 2015). Also, the factor of ability to work in groups is suggested as KBE important economic indicator based on a pharmaceutical industry Mehralian et al. (2013). The factor of business environment, innovation systems, human resources, communications infrastructure (Chen 2008), quality of institutions on the human development Balcerzak and Pietrzak (2017), human capital Oluwatobi et al. (2020), sociology of individual knowledge and creativity Ogundeinde and Ejohwomu (2016), knowledge-related policies Barkhordari et al. (2019) in some researches are suggested. Bozbura et al. (2007) have evaluated the quality indicators of human capital and subsets of the organizational index with the use of fuzzy ANP in Turkey. They showed that the four indicators are respectively “using the results of knowledge application”, “employees’ skills index”, “sharing and reporting knowledge”, and “succession rate of training programs” have a higher priority than the other defined indicators of human capital in Turkey. Since AHP development procedure, many applications have been published, such as portfolio selection (Islam et al. 1997) facility location selection (Kahraman et al. 2003) reverse logistics (Barker and Zabinsky 2011) and etc. Research on the effect of KBE indicators of economic growth is developed. Some studies used artificial intelligence approached to model this effects (Ahmadi and Taghizadeh 2019; Ahmadi et al. 2019). Moreover, services as a factor of welfare lead researchers to introduce the service-based economy in market economies (Morel 2015). There are some methods about decision making in KBE, For instances, factors analysis (Shabrina and Silvianita 2015), fuzzy AHP (Bozbura et al. 2007; Wang and Chin 2011a, b), Pythagorean fuzzy multiple-criteria (Zeng et al. 2016), intuitionistic fuzzy (Xu and Hu 2010). Economic isolation and the sheer emphasis on self-sufficiency may contribute to the economic growth of a country, but as the experience of leading countries in knowledge-based economies, such as Japan, Korea (Lee 2016) and Singapore (Gerke and Evers 2018), suggests, this approach in international relations. It will be based on the development of the knowledge economy. The extravagance of any country with a view to enjoying its competitive advantages at the level of international relations, the growth of knowledge and technology transfer speeds as well as economic interactions in the field of commercialized technology, are undoubtedly among the key parameters in creating a positive international environment. It is based on the growth of the knowledge economy. The rapid growth and development of human knowledge have led to not only changing scientific information and tools day by day but sometimes changing laws and assumptions as well, so individuals in every society must also adapt to the pace of scientific change and evolution. Raise their academic level to play a more prominent role in building the infrastructure of a knowledge-based economy. Prioritizing KBE based on economic growth factors is one of the important challenges. It provides economists in making a decision for controlling growth factors. Now, in this paper, we tried to prioritize KBE indicators and study its effects on economic growth. We used principal component analysis (PCA) for feature reduction. Also, we prioritize indicators with the use of logarithmic fuzzy preference programming in the period 1993-2013 in Iran based on time series data.",17
58.0,4.0,Computational Economics,20 May 2020,https://link.springer.com/article/10.1007/s10614-020-09994-0,"Trust and Social Control: Sources of Cooperation, Performance, and Stability in Informal Value Transfer Systems",December 2021,Claudius Gräbner,Wolfram Elsner,Alex Lascaux,Male,Male,Male,Male,"Many financial transactions in emerging economies are arranged on an informal institutional basis. This implies that they cannot be monitored or enforced by the legal authorities and their official regulation is possible only to a limited extent. Examples of these informal financial activities include Rotating Savings and Credit Associations, interlinking agricultural loans, informal value transfer systems (IVTS), and other ‘nonmarket institutions’ (Besley 1995), which are governed by the informal rules that are learned, internalized, and reinforced by the group members. But until now, “very little is known about the mechanisms used by these groups to ensure that members abide by their obligations” (Anderson et al. 2009). Here we seek to explore the governance mechanisms accounting for the emergence, stability, and often surprising success of one of the most significant informal financial institutions, which is involved in informal money transfer around the world and is called Hawala. People use Hawala to transfer cash from one country to another (see Fig. 1 for an illustration): a sender of money approaches an intermediary called hawaladar, handles him a sum of money, and receives a remittance code for transfer to the final recipient. The hawaladar contacts another hawaladar in the target area and informs him about the amount of money to be transferred and a remittance code. The final receiver of money then contacts the second hawaladar, reproduces the remittance code, and receives the money (we explain the functioning of hawala in more detail in Sect. 2). Such a transaction lasts only several hours (or days in case of very remote territories) and hawaladars charge only small commission fees ranging from 2 to 5 percent of the amount transferred. After completing a transfer all traces of the transaction are removed. Estimates of the amount of money transferred through Hawala range from 100 billion dollars (Razavy 2005; Schneider 2010; Schramm and Taube 2003) to as much as 680 billion dollars per annum (Shehu 2004). It is, therefore, considered one of the most important IVTS worldwide (Catrinescu et al. 2009; Rusten Wang 2011). Illustration of the operation of Hawala Given the informality and legal unenforceability of financial claims among Hawala participants, the obscurity and impenetrability of the system’s workings, and abundant opportunities for swindling clients and partner hawaladars out of their money, an important question arises: How does Hawala stabilize the expectations and coordinate the behavior of its participants so as to deter opportunistic defection? The existing literature discusses two major stabilizing mechanisms preventing intermediaries’ opportunistic behavior: generalized trust and social control (Bijlsma-Frankema and Costa 2005; Costa and Bijlsma-Frankema 2007; Das and Teng 2001, 1998; Piccoli and Ives 2003; Sasaki and Uchida 2013). Until now, however, a number of questions remain unresolved in this literature. In this paper, we try address four of them: How should trust and social control be operationally defined and operationalized? Does trust or social control carry a larger relevance for the functioning of IVTS? Do trust and social control relate to each other as substitutes or complements? How do trust and control relate to boundary conditions, such as group size or interaction density? Although the most recent studies on Hawala (Fazlur Rahman 2019; Redin et al. 2014; Sharif et al. 2016) add some points to the discussion of trust/control dualism in IVTS, the research questions stated above still remain unanswered. We address these questions by introducing the—at least to our knowledge—first agent-based representation of Hawala. An agent-based approach seems intuitive since it allows us to study Hawala in a generative way (see Epstein 1999) and to study the effects of out-of-equilibrium dynamics and shocks—aspects that have so far received little attention in the literature. Since our operationalization of trust and social control is generic, our results generalize beyond the example of Hawala and can be applied to IVTS in general. Our model specification is mainly built upon qualitative evidence, and the outline of the model focuses on the fundamental principles. Thus, our contribution is mainly analytical and we hope that further empirical applications of the model, as well as its refinement through new fieldwork, will improve our understanding of Hawala. But such more complex applications should be left for future research (see also Sect. 7). At this point we introduce the model as a first agent-based representation of hawala that directly portrays the fundamental mechanisms of this value transfer system, that helps to rigorously define and distinguish trust and social control and that enables us to study the behavior of the system out-of-equilibrium. This allows us to contribute a number of theoretical insights to the literature on Hawala, the most important of which we want to preview as follows: First, both trust and social control are necessary but not sufficient for the successful operation of Hawala. Second, their relation follows a particular temporal structure, with trust being especially important for the emergence and social control for the stabilization of the system. Third, in our dynamic analysis we find that out-of-equilibrium shocks on the trust and control level of the population can have long-lasting effects. Finally, we identify population size, interaction density and the forgiveness of the agents as relevant conditions, which, together with trust and social control, are sufficient for an efficient operation of the IVTS system. These results, which are discussed in more detail in Sect. 6, complement and extend existing game-theoretical treatments of the subject by directly representing and quantitatively analyzing the important mechanisms regulating IVTS. At the same time, we hope to inspire new empirical research that considers the important parameters identified by our model, and that produces (qualitative and quantitative) data that can then be used to verify and refine the model further. The remainder of this paper is structured as follows: Sect. 2 describes the key features of Hawala and its functioning. Section 3 elaborates on trust and social control as potential factors of emergence and stabilization of IVTS and informal exchange systems more generally. Section 4 presents our operationalization and the model, which is, to our knowledge, the first agent-based investigation of hawala. Section 5 summarizes the results of our computational experiments, with a broader discussion ensuing in Sect. 6. Section 7 concludes the paper and suggests implications for further research. The dynamics of our model, a more extensive sensitivity analysis as well as additional information about Hawala is provided in the online supplementary material. The program code is openly accessible on GitHub.Footnote 1",1
58.0,4.0,Computational Economics,21 May 2020,https://link.springer.com/article/10.1007/s10614-020-09990-4,Does Capacity Utilization Predict Inflation? A Wavelet Based Evidence from United States,December 2021,Pejman Bahramian,Andisheh Saliminezhad,,Male,Unknown,Unknown,Male,"The available economic models (e.g., the prominent, traditional Phillips curve and New Keynesian models) imply a short-run relationship between inflation and unemployment or output. Similar to the unemployment rate and gross domestic product (GDP) as the popular output proxy, capacity utilisation has the capability of measuring the current level of economic activities. With considering labour as an element of mentioned indicators, the capacity utilisation rate is a wider measurement of economic activities than the unemployment rate; further, information provided by monthly capacity utilisation is more contemporaneous than the quarterly GDP. Capacity utilisation is the rate at which an enterprise or a country uses its established productive capacity. According to Crotty (2002), capacity utilisation is often used in reporting the functional relationship between the feasible output (that could be produced) and the actual output (that is, actually produced), provided the capacity of such equipment installed is optimally engaged. Capacity utilisation illustrates the overall growth and demand in an economy, and it is often a key indicator for the Consumer Price Index (CPI). An increase in capacity utilisation often leads to a rise in prices, thereby creating demand-pull inflation as aggregate demand outweighs its (aggregate) supply. Indeed, the capacity utilisation rate has an important role as an indicator of inflationary pressure. Bagshaw (2015) states that there is a relationship between capacity utilisation and market demand; if demand grows, capacity utilisation would rise, and if demand weakens, capacity utilisation would slacken. Thus, considering the capacity utilisation rate–inflation nexus is of particular importance, because it signals inflation pressures. The link between capacity utilisation and inflation is coming out of some simple economic opinions. In terms of unused capacity, competition among producers restrains the rise in prices and keeps the prices down. Whereas, in the presence of capacity constraints, competition increases and prices rise. An industry with capacity constraints may face increase in prices from suppliers that encounter pressure from excess demand. Therefore, sectors operating at high capacity utilisation rates are subjected to both costs and prices rising together. In the 1970s, economists started deducing the ability of capacity utilisation to forecast inflation and it is still used by economists today. An economy operating at a high percentage of its productive capacity has likely more inflationary pressure than one operating at a low percentage of capacity. In those terms, the majority of economic analysts, together with the policy makers in the Federal Reserve Bank (Fed), have been monitoring inflation to discern some connections between capacity utilisation and inflation (McElhattan 1978, 1985; Bauer 1990; De Kock and Nadal-Vicens 1996; Emery and Chang 1997; Dotsey and Stark 2005). The above-mentioned analysts expected the possibility of capacity utilisation to serve as a useful leading indicator of inflation. 
Garner (1994) detected a stable connection between capacity utilisation and the future Consumer Price Index (CPI) in the manufacturing sector of the United States. The author concluded that capacity utilisation remains a reliable leading indicator of inflation. Moreover, he realised that, between 1964 and 1993, even factors such as openness, rising business investment or rapid technology changes could not weaken the relationship. 
Emery and Chang (1997) examined the stability of the relationship between capacity utilization and inflation over time. Their empirical results showed that capacity utilization and the future growth rate of inflation (inflation growth) were positively and significantly correlated from 1967 to 1996. Their finding holds for Consumer Price Index (CPI) and Producer Price Index (PPI) as the two different measurements of inflation. By testing for a break in their relationship, the authors found significant parameter instability, beginning in 1983. Their empirical results indicated that capacity utilization had significant predictive power for CPI before 1983, while this forecasting ability diminished after 1983. In terms of PPI, however, evidence indicated that the relationship between the variables was stronger before 1983. Although the strength of this relationship also declined after 1983, the authors empirically proved that there was a significant positive predictive relationship between capacity utilization and PPI, specifically for the forecasting horizon of 6 months.
 Similarly, Gordon (1994), Cecchetti (1995), Corrado and Mattey (1997), Stock and Watson (1999), Brayton et al. (1999) and Nahuis (2003) found the positive and significant utilisation–inflation relationship and concluded that the ability of capacity utilisation is better than the unemployment rate in predicting inflation. Against the foregoing literature, the evidence showed conflicting results in the mid-1980s. Shapiro et al. (1989) showed the insignificant impact of high-capacity utilisation on prices, which can sometimes be negative. Recently, Dotsey and Stark (2005) studied the predictive power of capacity utilisation on inflation, and they did not find a robust relationship. Their linear forecasting model implied an unstable connection between capacity utilisation and inflation over time, because capacity utilisation had forecasting power in the early 1980s, but lost this ability late in that decade. Most recently, Ahmed and Cassou (2017) detected a reversed relationship. Inflation measured by core CPI, preferably, comprises forecasting ability for capacity utilisation. Their empirical results affirmed that a one percent increase in the inflation rate is associated with a 0.004 rise in capacity utilisation in the long run. Moreover, they found that in the short run, changes in the inflation rate do Granger-cause capacity utilisation, whereas the reverse does not hold. The authors asserted that “the lack of Granger causality from capacity utilisation to inflation casts doubt on the older view that capacity utilisation could be a leading indicator for future inflation”. It is worth noting that several popular explanations were put forward as possibilities for the change in the utilisation–inflation relationship, which included advancements in technology and globalisation. In connection with the point previously mentioned, Aiyagari (1994), Finn (1995) and Bansak et al. (2007) investigated the effects of technological change on capacity utilisation, whereas Gamber and Hung (2001) and Dexter et al. (2005) found a significant impact of international trade through a downward pressure on U.S. inflation, which might result in an obscured utilisation–inflation relationship in the 1990s. Given the controversial conclusions in the relevant articles, a study that presents a deeper understanding of the true nature of the linkage between capacity utilization and inflation should be a matter of significance and of great interest to macroeconomic policymakers. In this regard, to complement the existing literature on the utilisation–inflation relationship, this study revisits the causal nexus between the variables through time and frequency-domain analyses. We extend our sample data into the new millennium covering both recession and recovery periods to provide a better understanding of the relationship between the variables. Unlike many scholars who have investigated this linkage by applying conventional time-domain methods (e.g., linear Granger causality test), the main contribution of the present paper is the application of wavelet analysis in the time and frequency domains. To the best of our knowledge, this is the first study to apply wavelet coherence and phase analysis to explore the dynamic dependence (simultaneous assessment of the co-movement and causal relationships) between the capacity utilisation rate and the inflation rate in the time and frequency domains. Wavelet coherency and phase differences simultaneously detect the time variation as well as the frequency fluctuation (across different frequencies) of the causalities among the variables. Therefore, we can identify high-frequency (short-term) and low-frequency (long-term) patterns between the two series. At the same time, the direction of the causality between the variables can also be illustrated at different moments in time. Besides, the time-frequency representation of the linkage among the variables emerging from wavelet analysis (wavelet transform) provides a detailed picture of localized periodicity in the nexus of variables. The local nature of wavelet analysis (particularly phase analysis) allows the examination of transient dynamics, synchronization and delays between oscillations of the two variables in a nonlinear framework. Accordingly, this approach controls for potential nonlinearities and structural breaks in the relationship between the two variables (see: Bloomfield 2004; Aguiar-Conraria et al. 2008; Antonakakis et al. 2018), whereas linear Granger causality tests are likely invalid, owing to the misspecification in the linear framework.Footnote 1 Using wavelet analysis, which permits the examination of the linkages between capacity utilisation and inflation across different frequencies and timescales, our empirical findings indicate a significant changing pattern in the dynamic linkage between the variables in the time-frequency domain. A positive causal relationship running from capacity utilisation to inflation is found, mainly at high frequencies (shorter-term). However, during the medium- and long-term frequency cycles, the causal linkages among the series do not follow a distinctive pattern. Our time- and frequency-varying findings also show that in various subperiods (including the Great Moderation and even the Great Recession), capacity utilisation can serve as a leading indicator of inflation. Although this link was found to be unstable over the entire sample, it still provides a new insight for policymakers who seek to model inflation accurately. The layout of the rest of the paper is as follows: Sect. 2 deals with the methodology; Sect. 3 presents a description of the data and provides the empirical results. Finally, Sect. 4 concludes.",4
58.0,4.0,Computational Economics,02 June 2020,https://link.springer.com/article/10.1007/s10614-020-09995-z,Implementing Convex Optimization in R: Two Econometric Examples,December 2021,Zhan Gao,Zhentao Shi,,Male,Unknown,Unknown,Male,"Equipped with tremendous growth of computing power over the last few decades, we econometricians endeavor to tackle high-dimensional real-world problems that we could hardly have imagined before. Along with the development of modern asymptotic theory, computation has gradually ascended onto the central stage. Today, discussion of numerical algorithms is essential for new econometric procedures. Optimization is at the heart of statistical estimation, and convex optimization is the best understood category. Convex problems are ubiquitous in econometric textbooks. The least square problem is convex, and the classical normal regression is also convex after straightforward reparametrization. Given a linear single-index form, the Logit or Probit binary regression, the Poisson regression and the regressions with a censored or truncated normal distributions are all convex. Another prominent example is the quantile regression (Koenker and Bassett 1978). With the advent of big data, practitioners attempt to build flexible models that involve hundreds or even more parameters in the hope to capture complex heterogeneity in empirical economic studies. Convex optimization techniques lay out the foundation of estimating these high-dimensional models. Recent years witnesses Bajari et al. (2015), Gu and Koenker (2017) and Doudchenko and Imbens (2016), to name a few, exploring new territories by taking advantage of convexity. To facilitate practical implementation, Koenker and Mizera (2014) summarize the operation in R by MOSEK via Rmosek to solve linear programming, conic quadratic programming, quadratic programming, etc. R is open-source software, MOSEK is a proprietary convex optimization solver but it offers free academic licenses, and Rmosek is the R interface that communicates with MOSEK. MOSEK specializes in convex problems with reliable performance, and is competitive in high-dimensional problems. This paper complements Koenker and Mizera (2014)’s work. We revisit two examples of high-dimensional estimators, namely Su et al. (2016)’s classifier-Lasso (C-Lasso) and Shi (2016)’s relaxed empirical likelihood (REL) by Rmosek. Other than Monte Carlo simulations, we also replicate a real data application that examines China’s GDP growth rate (Chen et al. 2019). These exercises highlight two points. Firstly, the R environment is robust in numerical accuracy for high-dimensional convex optimization and Rmosek takes the lead in computational speed. Second, we showcase the ease of creating new econometric estimators—often no more than a few lines of code—by the code snippets (in the supplement due to space limitations). Such convenience lowers the cost of turning an idea into a prototype, and enables researchers to glean valuable insights about their archetypes by experimenting new possibilities. Replication code and supplementary materials are hosted at https://github.com/zhan-gao/convex_prog_in_econometrics. The supplement provides the details of the data generating processes (DGP) of the simulation, additional results of the empirical application, and code snippets of convex optimization formulation.",
58.0,4.0,Computational Economics,06 June 2020,https://link.springer.com/article/10.1007/s10614-020-09998-w,"Examining Inferences from Neural Network Estimators of Binary Choice Processes: Marginal Effects, and Willingness-to-Pay",December 2021,Steven M. Ramsey,Jason S. Bergtold,,Male,Male,Unknown,Male,"Hanemann (1984) provides an often-used foundation for modeling binary choice survey data when the underlying behavioral process is grounded in economic utility theory. Following Hanemann (1991), consider an individual who derives utility from consumption of some good or service, such as an environmental amenity. Let q denote the supply of the good, service or amenity; I the individual’s income; and s vector of variables representing the consumption of other market commodities, prices, demographic characteristics and individual attributes. The individual’s indirect-utility function is given by Va(q, I, s) where a is an index denoting the amount of q being consumed. Consider the situation where the individual is faced with the opportunity of increasing consumption of q from q0 to q1. If the increase in q costs C, the individual will pay the amount if: The individual’s maximum WTP—equal to the compensating-variation measure of the change in q—is found where \(V_{1} \left( {q_{1} ,I - C,{\mathbf{s}}} \right) = V_{0} \left( {q_{0} ,I,{\mathbf{s}}} \right)\) (Hanemann 1991). In practice, the individual’s decision to pay C is observable, but their utility contains unobservable components and is treated as stochastic (Hanemann 1984). Thus, the individual’s indirect utility is decomposed as: where va is the observable utility component and \(\varepsilon_{a}\) is an IID random variable with zero mean (An 2000; Hanemann 1984). From this perspective, the individual’s response can be viewed in a probabilistic framework, where the probability an individual will pay $C to increase q is given by: or where p represents the probability that the offer is accepted, \(\Delta {\text{v}} = v_{1} ( \cdot ) - v_{0} ( \cdot )\) is the utility difference, and \(\eta = \varepsilon_{0} - \varepsilon_{1}\). Based on this, Eq. (4) can be written as: where \(F_{\eta } ( \cdot )\) is the cumulative distribution function (cdf) of η (Hanemann 1984). Thus, as stated by Hanemann (1984, p. 334), “if the statistical binary response model is to be interpreted as the outcome of a utility-maximizing choice, the argument of \(F_{\eta } ( \cdot )\) must take the form of a utility difference [i.e., Δv]” and so provides a criterion for determining whether a statistical model is compatible with utility maximization (Hanemann 1984). Once Δv has been specified, the modeler need only specify \(F_{\eta } ( \cdot )\), which is dependent upon the assumed distributions of ε0 and ε1.Footnote 2 A weakness of this approach is that the researcher has to make an assumption about the distribution of the stochastic term, which is usually unknown (Cosslett 1983). Because the researcher only observes the response by the individual to a proposed cost of $C to increase q, the response should be empirically viewed as a Bernoulli random variable with parameter p, which represents the probability of a response of “yes” or “accept” (Powers and Xie 2008). Let yi denote the response by the ith individual, where Assume that yi is dependent upon a m × 1 vector of unknown explanatory factors, xi, via the following relationship: where \(F_{\eta } ( \cdot ):R \to [0,1]\) (a transformation function), \({\mathcal{I}}\left( {{\mathbf{x}}_{i} ;\varvec{\beta}} \right):R^{m} \to R\) (a predictor or index function), and β is a m × 1vector of unknown parameters (Amemiya 1981; Davidson and MacKinnon 1993). Common choices for \(F_{\eta } ( \cdot )\) are the logistic and standard normal cdfs. The choice of which functional form to use for the index and transformation functions concerns the parameterization of the contemporaneous dependence between yi and xi (Spanos 1999). Given that researchers have the ability to vary the functional form of \({\mathcal{I}}({\mathbf{x}}_{i} ;\varvec{\beta})\), Amemiya (1981) states that the importance of having \(F_{\eta } ( \cdot )\) correctly specified is lessened: if one can approximate \({\mathcal{I}}({\mathbf{x}}_{i} ;\varvec{\beta})\) for a given choice of \(F_{\eta } ( \cdot )\), then \(F_{\eta } ( \cdot )\) need only satisfy the conditions of a transformation function. As compelling as this argument is, a particular choice of \(F_{\eta } ( \cdot )\) may not give rise to a proper statistical model in that the conditional-Bernoulli distribution based upon \(F_{\eta } ( \cdot )\) cannot be derived from a proper joint-density function. A choice of \(F_{\eta } ( \cdot )\) that does allow for the approximation of \({\mathcal{I}}({\mathbf{x}}_{i} ;\varvec{\beta})\) is the logistic cdf (Bergtold et al. 2010). Thus, one way of weakening the functional-form (and distributional) assumptions is to employ semi-nonparametric (SNP) estimation methods within the logistic-regression framework. SNP methods are semi-distribution free approaches that avoid restricting \(F_{\eta } ( \cdot )\) and/or \({\mathcal{I}}({\mathbf{x}}_{i} ;\varvec{\beta})\) in Eq. (7) by trying to estimate the compound function \(F_{\eta } \left[ {{\mathcal{I}}({\mathbf{x}}_{i} ;\varvec{\beta})} \right]\) (Cooper 2002). Following Cooper (2002), the modeler can replace \(F_{\eta } ( \cdot )\), \({\mathcal{I}}({\mathbf{x}}_{i} ;\varvec{\beta})\), or both with a flexible SNP functional form. Results from Gabler et al. (1993) and Horowitz (1993) suggest that SNP estimation may help in avoiding model misspecification due to an incorrect functional form. A SNP approach may be advantageous if the predictor function is not easily specifiable or it is highly nonlinear. Kay and Little (1987) show that specification of index functions linear in the variables may often be statistically misspecified and only arise under very narrow statistical grounds. Based on these findings, Arnold and Press (1989) question many of the binary-choice models presented in the literature that utilize index functions that are linear in the variables. A SNP estimator that can be found throughout the dichotomous-choice literature is that of Creel and Loomis (1997), which estimates the compound function \(F_{\eta } \left[ {{\mathcal{I}}({\mathbf{x}}_{i} ;\varvec{\beta})} \right]\) using a flexible-Fourier functional form. This estimator has been used to value the reduction of risk exposure to hazardous waste (Creel and Loomis 1997), to estimate farmer premiums for conservation adoption (Cooper and Signorello 2008), and was extended to a multivariate-discrete choice by Cooper (2003) to examine farmers’ willingness to adopt bundles of conservation practices. Hermite-polynomial approaches similar to that of Gallant and Nychka (1987) have been used, for example, to estimate WTP for water supply improvements (Arouna and Dabbert 2012) and the willingness of producers to use eco-labels (Chang 2012). The distribution-free estimator of Klein and Spady (1993) has been used to estimate WTP for sanitation improvements (Adriano et al. 2011) and the valuation of time (Bastin et al. 2010; Fosgerau 2005, 2006). One SNP approach, however, that is yet to be widely applied in the area of discrete choice modeling is feed-forward back-propagation artificial neural networks, which provides a potentially powerful SNP tool for modeling dichotomous-choice CV models. Furthermore, this approach can be extended to many other binary-discrete-choice modeling frameworks.",5
58.0,4.0,Computational Economics,09 June 2020,https://link.springer.com/article/10.1007/s10614-020-09996-y,A Statistical Analysis of Global Economies Using Time Varying Copulas,December 2021,Emmanuel Afuecheta,Saralees Nadarajah,Stephen Chan,Male,Unknown,Male,Male,"A copula is used to specify dependence between two or more random variables. A bivariate copula is a function \(C : [0, 1]^2 \rightarrow [0, 1]\) that satisfies \(C (u, 0) = C (0, v) = 0\), \(C (u, 1) = u\), \(C (1, v) = v\), \(\partial C (u, v)/\partial u \ge 0\) for all u and v, and \(\partial C (u, v)/\partial v \ge 0\) for all u and v. The concept of copulas was introduced by Sklar (1959). Since then many parametric, non-parametric and semi-parametric models have been proposed for copulas, including methods for constructing models for copulas. Applications of copulas are too numerous to list. There are practical situations, where the dependence between variables could itself be a variable: for example, the dependence between financial market indices versus time, dependence between competing risks versus time, and so on. In these situations, the models for copulas can be parameterized to depend on time. This gives rise to what is referred to as time varying copulas. There are also theoretical foundations for time varying copulas, see Patton 2006. The concept of time varying copulas is a very recent one. The first application appears to be that due to Vaz and de Mendes (2005). They model the dependence between stock indices of Argentina and Brazil. Subsequent applications have included: call option on the better performer of Shanghai and Shenzhen stock composite indices (Zhang and Guegan 2008); dependence between Dow Jones Industrial Average and DAX indices (Ausin and Lopes 2010); dependence of Chinese and US stock markets with other financial markets (Hu 2010); test for the presence of increases in stock market interdependence after the 1997 Asian financial crisis (Manner and Candelon 2010); dependence between the US and UK short-term interest rates (Bu et al. 2011); dependence between the Chinese market and other international stock markets (Wang et al. 2011); comovement between representative warrants and their underlying stocks in China’s stocks market (Zeng and Xu 2011); dependence between daily log returns of the DJI and NASDAQ (Almeida and Czado 2012); dependence between crude oil spot and futures markets (Chang 2012); dependence between bank and insurance equity prices (Chen and Gunther 2012); dependence between competing risks with multiple degradation processes (Wang and Pham 2012); dependence between the Brent crude oil price and stock markets in the Central and Eastern European transition economies (Aloui et al. 2013); dependence between stock market indices of France, Germany, Spain, Italy, Portugal, Netherlands, Ireland, and Japan (Berger 2013); dependence of agricultural price and production indices of Thailand (Sriboonchitta et al. 2013); dependence structure between the stock prices of West Texas Intermediate crude oil, natural gas and heating oil (Ghorbel and Trabelsi 2014); dependence between crude oil futures and natural gas futures (Lu et al. 2014); dependence between stock market indices from the US, the UK, Brazil and Mexico (Silva Filho et al. 2014); dependence among foreign exchange networks (Wang et al. 2014); dependence between gold prices and exchange rates (Yang and Hamori 2014). An excellent survey of time varying copulas can be found in Manner and Reznikova (2012). The applications of time varying copulas have covered a wide range of scope: Vaz and de Mendes (2005) consider two stock indices and model their dependence using the Gaussian and t copulas; Zhang and Guegan (2008) consider two stock indices and model their dependence using the Gaussian, t, Gumbel, Frank and Clayton copulas; Ausin and Lopes (2010) consider two stock indices and model their dependence using one copula; Hu (2010) considers seven stock indices and model their dependence using the Gaussian and Clayton copulas; Manner and Candelon (2010) consider eight stock indices and model their dependence using the Gaussian copula; Bu et al. (2011) consider two indices and model their dependence using the Clayton copula; Wang et al. (2011) consider seven indices and model their dependence using the Gaussian, Clayton, Gumbel and Joe copulas; Zeng and Xu (2011) use the Gaussian copula; Almeida and Czado (2012) consider two stock indices and model their dependence using the Clayton, Gumbel and Gaussian copulas; Chang (2012) considers two stock indices and model their dependence using the Clayton, Gumbel, Gaussian and mixture copulas; Chen and Gunther (2012) consider two stock indices and model their dependence using the Clayton copula; Wang and Pham (2012) consider two stock indices and model their dependence using the Gaussian, Clayton, Plackett, Frank, Gumbel, t and Joe copulas; Aloui et al. (2013) consider six stock indices and model their dependence using the Clayton, Gumbel, Gaussian, t and Tawn copulas; Berger (2013) considers eight stock indices and model their dependence using the Gaussian, t and extreme value copulas; Sriboonchitta et al. (2013) consider two stock indices and model their dependence using the Gaussian, t, Clayton, Gumbel, Joe and Frank copulas; Ghorbel and Trabelsi (2014) consider three stock indices and model their dependence using the Gaussian, t, extreme value and nested copulas; Lu et al. (2014) consider two stock indices and model their dependence using the Gaussian, t, Clayton and Gumbel copulas; Silva Filho et al. (2014) consider four stock indices and model their dependence using the Gaussian, t, Clayton and Gumbel copulas; Wang et al. (2014) use the t copula; Yang and Hamori (2014) consider four stock indices and model their dependence using the Gaussian and Clayton copulas. Some other papers focusing on dependence between different economies are: Low et al. (2013) examining the use of multi-dimensional elliptical and asymmetric copula models to forecast returns for portfolios with 3–12 constituents; Low et al. (2016a) showing that an application of copulas using marginal models and incorporating dynamic features such as autoregression, volatility clustering, and skewness to reduce estimation error in comparison to historical sampling windows; Rad et al. (2016) examining and comparing the performance of three pairs trading strategies using daily US stock data and copula methods; Low (2017) applying the Clayton canonical vine copula to model asymmetric dependence across a portfolio of assets. See also Adler and Kritzman (2007), Chua et al. (2009), DeMiguel et al. (2009), and Low et al. (2016b, c). The aim of this paper is to illustrate an application of bivariate and multivariate time varying copulas for all global economies, to give reasons for temporal variations and to give predictions of value at risk. We consider the dependence between stock indices of ten different countries: the United States of America, Canada, the United Kingdom, Germany, China, Japan, Brazil, Argentina, South Africa and Nigeria. These countries cover the major economies in all of the six continents. We model the dependence using ten different copulas: the Gaussian copula, the t copula, the Ali–Mikhail–Haq copula due to Ali et al. (1978), the Clayton copula due to Clayton (1978), the Farlie–Gumbel–Morgenstern copula due to Morgenstern (1956), the Cuadras–Augé copula due to Cuadras and Augé (1981), the Marshall–Olkin copula due to Marshall and Olkin (1967), the cubic copula due to Durrleman et al. (2000), the Gumbel copula due to Gumbel (1960), and the Joe copula due to Joe (1993). The emphasis will be on the modeling of the dependence, and not on the modeling of the marginal distributions of the stock indices. The latter will be modeled non-parametrically as explained in Sect. 3. The contents of this paper are organized as follows: Sect. 2 describes the data; the ten copulas and estimation issues are described in an ""Appendix"" and Sect. 3; the results of fitting the copulas, their discussion and predictions are given in Sect. 4; finally some conclusions are noted in Sect. 5.",
58.0,4.0,Computational Economics,09 June 2020,https://link.springer.com/article/10.1007/s10614-020-09997-x,The \(\alpha\)-Tail Distance with an Application to Portfolio Optimization Under Different Market Conditions,December 2021,Han Yang,Ming-hui Wang,Nan-jing Huang,,Unknown,Unknown,Mix,,
58.0,4.0,Computational Economics,18 June 2020,https://link.springer.com/article/10.1007/s10614-020-09999-9,A Time Series Framework for Pricing Guaranteed Lifelong Withdrawal Benefit,December 2021,Nitu Sharma,S. Dharmaraja,Viswanathan Arunachalam,Male,Unknown,Unknown,Male,"Variable annuities (VAs) are life insurance products that combine features of insurance and securities investments. In a VA, usually, a policyholder pays a single premium at the beginning. Then, this premium is invested in one or several mutual funds chosen by the policyholder himself from a variety of different mutual funds. The embedded options provided by an insurer can be categorized as the guaranteed minimum living benefit (GMLB), and the guaranteed minimum death benefit (GMDB) (Hardy 2003; Ledlie et al. 2008). Four main options that offer some guaranteed minimum living benefit are guaranteed minimum income benefit (GMIB), guaranteed minimum accumulation benefit (GMAB), guaranteed minimum withdrawal benefit (GMWB), and guaranteed lifelong withdrawal benefit (GLWB) (Piscopo and Haberman 2011). The first two options, GMIB and GMAB, offer a guaranteed minimum amount, irrespective of the account value, at the maturity of the contract. With GMIB, this guarantee is applicable only if the insured annuitize the account value, at the time of maturity. In GMWB, the insured can withdraw a pre-specified amount at fixed, regular intervals until the maturity of the contract. These withdrawals are independent of the sub-account fund value. Therefore, in case the account value diminishes to zero before the maturity, the insured can continue to withdraw the guarantees. GLWB is a lifelong version of the GMWB option, in which the policyholder can withdraw a pre-specified guaranteed amount at fixed, regular intervals till the time he is alive. If the account value becomes zero during the lifetime of the insured, he can still continue to withdraw the guaranteed amount until his death. The guaranteed amount under GMWB and GLWB options can be either static(constant) or dynamic(varying) depending upon the withdrawal strategy chosen by the insured. Such riders are congruous for risk-averse investors. As a result, VAs with a minimum guarantee feature is an alluring alternative for such investors. Additionally, as the baby boomers approach retirement, the demand for annuities and savings products will continue to increase (Condron 2008). Therefore, the fair valuation of VA products is compulsory. There have been many models introduced to value the VA products with some embedded options. A modelling framework for valuation of VA was introduced by Bauer et al. (2008), where they considered pricing of VA with GMDB, GMIB and GMAB riders. Krayzler et al. (2016) gave closed-form formulas for the pricing of GMAB and GMDB riders. They have considered a GBM model for the stock price dynamics with non-constant interest rates and volatility. However, this does not enable them to capture the leptokurtic behaviour of stock returns. Bacinello et al. (2011) evaluated different kinds of living and death guarantees (GMDB, GMIB, GMAB, GMWB) under both the static and mixed approaches. Since the introduction of GLWB rider in 2004, GLWB riders continue to be the most popular type of GMLB option in the VA market [according to a research article by LIMRA (Drinkwater et al. 2014)]. Despite the continued popularity, the literature for the valuation of GLWBs is very limited. In this direction, Piscopo and Haberman (2011) has given a theoretical model for the pricing and valuation of GLWB option embedded in the VA products. They have assumed the sub-account fund to be GBM with constant drift and volatility. Similarly, Dai et al. (2008) and Peng et al. (2012) have assumed GBM for the fund value process of a VA embedded with a GLWB/GMWB option. Forsyth and Vetzal (2014) developed an implicit partial differential equation (PDE) method for valuing GLWB option. They assumed the risky asset follows a Markov regime-switching process. Choi (2017) computed the indifference price of the VA contract with a GLWB rider using the concept of equivalent utility. They modelled the risky asset by a GBM model with a constant rate of return and constant volatility. Assuming fund value to follow GBM with constant volatility is not always realistic. Also, the returns have a leverage effect; volatility is not only time-varying, but the future volatility is asymmetrically related to past innovations. The unexpected negative returns influence future volatility more than unexpected positive returns (French et al. 1987). The GBM model, with or without stochastic interest rate, cannot capture the leverage effect and the volatility clustering effect in the stock returns. The volatility clustering effect in returns can be captured by the autoregressive conditional heteroscedastic (ARCH) and the generalized ARCH (GARCH) models formulated by Engle (1982) and Bollerslev (1986) respectively. However, ARCH and GARCH approaches have failed to capture asymmetric features of the stock returns (Ericsson et al. 2016). The family of asymmetric GARCH models can capture this stylized feature. We used some of the popular asymmetric GARCH models in our analysis. These include the exponential GARCH (E-GARCH) model by Nelson (1991), Glosten–Jagannathan–Runkle GARCH (GJR-GARCH) model by Glosten et al. (1993) and Threshold GARCH model by Zakoian (1994). Apart from taking into account the volatility clustering effect of stock returns and leverage effect, these time-series models are also discrete. Since discrete cash flows involved in the VA contract are incorporated in these models, they may be considered better models. In the direction of valuation of VA products using asymmetric GARCH models, Ng et al. (2011) used these models to develop a valuation model for the investment guarantees: GMDB and GMAB. They have shown that it is not possible to capture the stylized facts present in the equity index (Nikkei 225) by a GBM model and hence concluded that an E-GARCH model could provide more realistic modelling. The prices obtained by them are higher than those obtained by using GBM method for GMDB and GMAB guarantees. The work by Ng et al. (2011) incentivizes us to use GARCH type models for the valuation of the recently most popular living guarantee which enjoys a significant stake in the VA sales, i.e., GLWB (Drinkwater et al. 2014). Hence, this paper presents a fair valuation model for the GLWB guarantee. In this article, we have considered GJR-GARCH, E-GARCH, and T-GARCH models for modelling stock volatility. The models mentioned above capture all the “stylized” facts present in stock returns. The appropriate model for the considered data is chosen based on several standard criterion’s values. Following Siu-Hang Li et al. (2010) and Ng et al. (2011), we obtained a risk-neutral measure for the proposed model of risky asset. To simplify the model, we considered a static withdrawal strategy with a constant guarantee withdrawal amount over time. Then using the risk-neutral measure, we obtained an implicit equation in the fee. We solved some numerical examples to obtain the break-even fee using the implicit equation. For analysis purposes, we consider three different markets: the Japanese market, the US market and the World market. The first two markets are chosen based on the selling history of VA products. Moreover, to see the pricing for a global index, we considered the third market. The Japanese market has a circumscribed history, where the sale of VA products begins in 1999, whereas the US market is the oldest one in selling VA products. We obtained fee value for the three datasets under different scenarios with three different models, namely asymmetric GARCH model, standardized GARCH model and GBM model. As observed, the GBM model underestimates the prices for the guarantee. Whereas, the standardized GARCH model overestimates the prices. We also performed sensitivity analysis concerning different model parameters. Our results were consistent with the results conveyed by Quittard-Pinon and Randrianarivony (2011) for a GMDB guarantee. The rest of the paper is organized as follows: Sect. 2 comprises finding the stylized facts present in the stock returns. Section 3 gives a description of the asymmetric models and designates the corresponding risk-neutral measure. Section 4 contains the pricing model for the valuation of GLWB. Section 5 consists of selecting the asymmetric GARCH models, obtaining fee using Monte Carlo simulations and sensitivity analysis of the fund value concerning various parameters. Section 6 consists of the concluding remarks suggesting some possible future work.",4
58.0,4.0,Computational Economics,26 June 2020,https://link.springer.com/article/10.1007/s10614-020-09983-3,"Matlab, Python, Julia: What to Choose in Economics?",December 2021,Chase Coleman,Spencer Lyon,Serguei Maliar,Male,Male,Male,Male,"We perform a comparison of Matlab, Python and Julia as programming languages to be used for implementing global nonlinear solution techniques. We consider two popular applications: a neoclassical growth model and a new Keynesian model. Our overall experience with each language was comparable, though it is useful to recognize that each language has its specific strengths. Both Julia and Python are open-source which facilitates transparency and reproducibility. Julia outperforms both Matlab and Python in algorithms that require numerical solvers or optimizers. Python benefits from an active community and strong package ecosystem which makes finding the right tools easy. Finally, Matlab benefits from extensive documentation, technical support, and various built in toolboxes. The running times were not significantly different across three languages considered. Ultimately, the best choice of programming language depends on various factors including: model size, solution complexity, ease of writing, and co-author preferences. Regardless of the programming language chosen, our experience shows that transitioning between each of the three languages we consider should not require a substantial learning effort. Our paper is motivated by the growing interest among economists in global nonlinear solution methods. Value function discretization is the most well-known example of a global nonlinear solution method, but there are a variety of other iterative and Euler-equation based methods that differ in the way they approximate, interpolate, integrate and construct nonlinear solutions. These methods solve economic models on grids of points that cover a relevant area of the state space, and they generally produce more accurate solutions than perturbation methods which typically build approximations around a single point.Footnote 1 Also, global methods are useful for solving some models in which perturbation methods are either not applicable or their applications are limited; see Taylor and Uhlig (1990) and Judd (1998) for reviews of the earlier methods; and see Maliar and Maliar (2014) and Fernández-Villaverde et al. (2015) for the reviews of newer literature.Footnote 2 Finally, the recent developments of numerical techniques for dealing with high-dimensional data has greatly extended the class of models that can be studied with global solution methods. In particular, it is now possible to construct global nonlinear solutions to economic models with hundreds and even thousands of state variables that are intractable with conventional global solution methods (such as value-function discretization) due to the curse of dimensionality.Footnote 3 However, global nonlinear solution methods are more difficult to automate than perturbation methods, and their implementation requires more substantial programming from researchers. In particular, there is still no consensus in the profession on what software to use for implementing global solution methods, unlike for perturbation methods where a common choice is the Dynare or IRIS platforms. One important aspect of the implementation of a global solution method is the choice of programming language to use. Traditionally, Matlab is used in economics, although some researchers have also used Fortan and C. Recently, Python and Julia have begun to see a more widespread use in the economics literature. In this paper, we pursue two goals: First, we provide a comparison between global solution methods implemented in Matlab, Python and Julia in the context of two popular applications: a neoclassical growth model and a new Keynesian model. Our goal is to help economic researchers choose the programming language that is best suited to their own situation, and, if needed, help them transition from one programming language to another. The readers can see and compare how the same algorithm is implemented in different languages and, as a result, will understand some of the unique aspects of each language and choose the one which fits their needs.Footnote 4 The implementation, structure and number of lines of code are similar across all three languages. Second, we provide a carefully documented collection of routines for Matlab, Python and Julia that can be used for constructing global nonlinear solutions with a special emphasis on modern techniques for problems with high dimensionality. In particular, our code includes routines for constructing random and quasi-random grids, low-cost monomial integration methods, approximating functions using complete polynomials, as well as routines for computing the numerical accuracy of solutions. Much of this code is generic in a way which allows it to be easily portable to other applications including problems with kinks and occasionally binding constraints. Our examples are solved using a variety of solution techniques including: conventional policy function iteration, conventional value function iteration, an Euler equation method, the endogenous grid method of Carroll (2005), and several variants of the envelope condition method of Maliar and Maliar (2015). We also include examples of parallelization in the three languages considered. Aruoba and Fernandez-Villaverde (2015) is closely related to our work. In their paper, the authors compare 18 different programming languages by implementing the same algorithm—value-function discretization—to solve the stochastic neoclassical growth model and by reporting the CPU time needed to solve the model. They find that the choice of a programming language plays an important role in computation time: the fastest language in their study (C++) solves the model over 450 times faster than the slowest language (R). The present work extends Aruoba and Fernandez-Villaverde (2015) in three important ways: First, our comparison is more representative of and relevant to the modern numerical analysis in economics. To be specific, their code for value function discretization consists of arithmetic operations and loops, while our value-iterative and Euler equation methods involve also numerical integration, interpolation and regression routines, including sparse and quasi-Monte Carlo grids and derivative free solvers tractable in problems with high dimensionality. Second, our code is written in a way that exploits the strengths of each language considered, while Aruoba and Fernandez-Villaverde (2015) use the same implementation in all languages. In particular, we speed up the Matlab code by replacing some of the loops with vectorized computation, and we speed up Python by using the “just-in-time” compiler. As a result, we do not observe such huge differences across the three considered languages as those reported in their paper—the running times were roughly similar across the three languages considered.Footnote 5 Finally, in addition to the speed comparisons, we also discuss some key features of each language that might be useful for economists. Furthermore, for the new Keynesian model, we modify the method proposed in Maliar et al. (2015) to operate on random and quasi-random grids covering a fixed hypercube instead of operating on the high probability area of the state space. The Matlab code developed in the present paper achieves an almost a 60-time speed up over the original code, while still producing highly accurate solutions. Our code is sufficiently fast to be used for the estimation of a moderately-large new Keynesian model with 8 state variables. It takes us just a few seconds to construct the solution, including the model with active zero lower bound on the nominal interest rate in all three languages. Finally, we should mention an additional important factor for the choice of the programming language, which is a specific collection of packages and libraries designed for solving economic models. In fact, for those researchers who are deeply interested in a specific model, the choice between the languages may amount to the choice between the packages that are most suitable for the problem that they are trying to solve. Matlab users have access to the Dynare and IRIS perturbation software as well as a large collection of routines developed by macroeconomists over the last decades, for example, a value function iteration toolkit by Robert Kirby vfitoolkit.com; the PHACT toolbox for solving heterogenous-agent models by SeHyoun Ahn, Greg Kaplan, Benjamin Moll, Thomas Winberry and Christian Wolf github.com/gregkaplan/phact; a collection of routines for solving large scale problems developed by Lilia Maliar and Serguei Maliar lmaliar.ws.gc.cuny.edu/codes; etc. Python users can benefit from the Heterogeneous Agents Resources and toolKit (HARK) developed by the team led by Christopher Carroll econ-ark.org. In turn, Julia users have access to the HetSol toolkit developed by Michael Reiter elaine.ihs.ac.at/~mreiter/installhetsol.txt. Some developers provide both Python and Julia versions of their software, for example, the QuantEcon site administrated by Tom Sargent and John Stachurski and the Dolo platform for constructing global solutions developed by Pablo Winant github.com/EconForge/dolo. In turn, we provide identical routines for all three languages, so that our users do not face the choice between Matlab, Python and Julia but can use the language that is the most suitable for the given problem. The rest of the paper is organized as follows. In Sect. 2, we describe the three languages, Matlab, Python and Julia. In Sect. 3, we outline seven algorithms we implement to solve a variant of the standard neoclassical stochastic growth model. In Sect. 4, we present an algorithm for solving a medium-scale new Keynesian model with a zero lower bound on nominal interest rates and describe the results of our analysis. Finally, in Sect. 5 we conclude.",3
58.0,4.0,Computational Economics,16 October 2020,https://link.springer.com/article/10.1007/s10614-020-10058-6,Correlated at the Tail: Implications of Asymmetric Tail-Dependence Across Bitcoin Markets,December 2021,Stelios Bekiros,Axel Hedström,Gazi Salah Uddin,Male,Male,Male,Male,"Predicting Cryptocurrency price movements is one of the most challenging tasks an investor would embark on—thanks to the lack of a strong asset pricing theory, dominance of a largely unregulated market structure, and instrumental role of an implicit value of investors’ sentiments. Recent research in this regard demonstrates that a large part of variance in a cryptocurrency market, such as Bitcoin, is due to the sensitivity of investors to macroeconomic performances of a country (e.g., Corbet et al. 2018; Cheah et al. 2018; Gillaizeau et al. 2019). A common practice is to treat observed pattern of price movement as an ‘information set’, which an investor uses to ‘predict’ as his next strategy of investment. But, this information set conceals unaccounted for noisy signals arising out of, for instance, dynamic movements in macroeconomic fundamentals (representing economic parameter-driven sentimental values) from other markets. Eventually, a component of this ‘information set’ specific to a market, becomes a common component across other markets, because noises generally display transmissive and transformative effects (Gillaizeau et al. 2019). The problem most often neglected is that whilst it is the entire dynamic path of a cryptocurrency price and associated factors that determine the information set, but inference is based only on the centre of the distribution. There are essentially two ways to understand cross-market dynamic correlation: first, a systemic approach (such as estimation within a vector autoregression with/without long-memory), where interdependence across markets is assumed, but not modelled (Cheah et al. 2018). Yet, using this approach, one would be able to shed light on the ‘average’ dynamic effect, while being silent on what is happening on the other part of the distribution of this relationship. The second approach, which we propose in this paper, is a full-distributional approach where focus is laid on each part of the distribution of the variable; in our case, it is a study of a quantile-based dynamic correlation structure at various parts of the distribution of a cryptocurrency price. Previous studies on the Bitcoin market is mainly focused on the application of several different types of methods and its implication on the price discovery, volatility modelling, directionality via causality, and through the application of daily or high frequency trading data. First, price discovery is important concept in the financial modelling on Bitcoin market (Brandvold et al. 2015) and the long memory application via daily and trading data mechanisms (Phillip et al. 2018). Second, the study focused on the application of different GARCH-types modelling or Spillover approach in Bitcoin markets (Gillaizeau et al. 2019; Corbet et al. 2018; Symitsi and Chalvatzis 2018; Katsiampa, 2017; Guesmi et al. 2019). Third, several studies focused on the cointegration and directionality and dependence approach in Bitcoin Market by utilizing Causality (Cheah and Fry 2015). Fourth, recent studies focused on the interdependence on the Bitcoin and several measures of uncertainties (Al Mamun et al. 2020) and Hedge or Safe Haven properties with respect to major asset classes (Kang et al. 2020) and multifractal properties with respect to high frequency data (Stavroyiannis et al. 2019). To understand, assume that there are two markets for a cryptocurrency, viz., Bitcoin, traded for instance, in market A and market B with distinct exchange rates. Assume also that an investor—due to his pursuit of profit—will invest in a market that holds greater promise of return than the other. Each market is governed by macroeconomic and socio-political dynamics. Hence, the value of Bitcoin in that market is primarily a function of macroeconomic conditions, among others. Denote Bitcoin price in market A at time t as P
At
 = f(M
At
, P
At−1
; error
At
). Similarly, for market B, it is given by P
Bt
 = f(M
Bt
, P
At−1
; error
Bt
). Both Pt−1 Mt make the information set (It). Since, like many asset prices, Bitcoin price reflects heavy tail, the distribution in the tail, depicts heterogeneous behaviour. For instance, the tail distribution of P
At
 at time t = i and that of P
Bt
 at t = j where i is not equal to j, may depict heterogeneous correlation structure. By modelling such a heterogeneity one would be able to gather complete information about the directional prediction pattern of one market over the other at different parts of the distribution of the tail. A further implication is that since ‘fat tailed’ distributions depict implicit ‘herd behaviour’ (generated by asymmetric and incomplete information plus bounded rationality of agents), the same asset traded in two different markets can depict different herd dynamics. It is only when one is able to fully characterise the correlation of this ‘herd’ dynamics, it is possible to create an exhaustive information set (It) that will be used to predict the dynamic path of one over the others. A quantile-based estimation of directional predictability (in contrast to the conventional mean-based estimation of spillover effects, such as Corbet et al. 2018) is useful in this regard. This paper fills a gap in the literature and is the first one to propose a complete characterisation of tail dependence across cryptocurrency market. Thus, our purpose is two-fold: (i) to lend credible value of directional predictability of a cryptocurrency, (ii) to design optimal investment strategy by evaluative distributional patterns of correlations at the tail. A theoretical expectation is that a dynamic correlation between the currency in market A and B, for instance, will be heterogeneous over the entire range of the distribution. By modelling such a heterogeneity one would be able to gather complete information about the directional prediction pattern of one market over the other at different parts of the distribution of the tail. We model directional predictability across markets over the entire distribution of prices and appears to be the first one to propose a complete characterisation of tail dependence across cryptocurrency market. Cryptocurrency market is very closely associated with the traditional financial markets and other asset classes, hence, has imminent economic implications. Financial markets are subject to certain monetary regulations and their behaviors can more or less be explained by asset pricing theory. However, the cryptocurrency market behaviors cannot yet be satisfactorily explained by traditionally existing real asset pricing theories and herein presents a stiff challenge with far reaching economic implications, primarily due to the huge volatilities associated with cryptos. This issue is heightened as investors might substitute an asset market regulated stock with this highly volatile cryptocurrency market and thus innately increase the systemic risk of the economy as a whole. One of our main contributions is the development of a cross-quantilogram analysis of cryptocurrency which envisions a predictive power not only for the asset market but also for the real economy as well. Thus, we present a comprehensive method to envision the dynamic tail risk behavior between cryptos and other asset classes which would help investors to formulate better trading strategies and policy makes to focus on specific scenarios that would reduce economy wide systemic risk. The current paper aims to this nascent literature by studying directional predictability and their dynamic stable pattern across Bitcoin markets, exchanged in various currencies. To investigate further, in Sect. 2 we briefly present the cross-quantilogram approach. Section 3 presents data and discusses estimation results (along with robustness exercise). Section 4 concludes with the main findings and their implications for practitioners and policy.",4
58.0,4.0,Computational Economics,20 October 2020,https://link.springer.com/article/10.1007/s10614-020-10051-z,Ranking Countries and Geographical Regions in the International Green Bond Transfer Network: A Computational Weighted Network Approach,December 2021,George Halkos,Shunsuke Managi,Kyriaki Tsilika,Male,Male,Female,Mix,,
59.0,1.0,Computational Economics,30 October 2020,https://link.springer.com/article/10.1007/s10614-020-10064-8,Job Mobility and Wealth Inequality,January 2022,J. M. Applegate,Marco A. Janssen,,Unknown,Male,Unknown,Male,"The US job mobility rate, describing the extent to which employees move between employers, is at an all time low after declining for decades, and this decline has important consequences. Job changing is understood to improve productivity by matching workers to more suitable employment and by promoting innovation through the inter-firm exchange of experience (Eriksson and Lindgren 2008; Helsley and Strange 1990; Breschi and Lissoni 2009). Changing jobs is also understood to increase wages by providing workers with opportunities to negotiate higher salaries (Gottschalk 2001). Why has job changing become less frequent over the past several decades? Suggested causes include a need to retain employer-provided health insurance, an aging population, the rise of dual-career households, declining entrepreneurship, a decline in middle-skill jobs, burdensome occupational licensing requirements or skill supply and demand mismatches. Yet econometric studies do not provide strong support for any of these explanations (Hyatt 2015; Molloy et al. 2017). Another possible explanation is that changing jobs incurs costs on the part of the employee, such as gaps in income, training expenses or relocation costs, and these costs are funded by the employee by spending savings or borrowing through loans [also suggested by (Bhaskar et al. 2002)].Footnote 1 A broad exploration of the employee cost burden in the reallocation process is missing from the literature, perhaps in part because of the difficulty in quantifying such costs. A further consideration is that this decline could be the result of a complex interaction of several factors. The decrease in job mobility is contemporaneous with decreases in household savings (Guidolin and La Jeunesse 2007), increases in household debt (Getter 1996), and a stagnation of wages (Donovan and Bradley 2018).Footnote 2 Could a decrease in savings and an increased debt burden be impacting the ability of workers to take advantage of wage and productivity improving job opportunities, thus further impeding their ability to accrue savings? Some evidence suggests this could be the case. Owing more on a mortgage than the market value of the house has an impact on job mobility, to the extent that people take lesser jobs in order to avoid the costs of moving (Brown and Matsa 2016)Footnote 3. The decline in US savings rates strongly correlates with increased credit availability (Carroll et al. 2019), suggesting that households are substituting debt for savings. Barba and Pivetti claim evidence of the substitution of loans for actual wages (2008), further supported by findings of sharp increases in the use of consumer credit applied to necessitous spending, where households borrow to make regular purchases, which in turn may lead to liquidity traps that make future saving difficult (Pollin 1988; Sullivan et al. 2001; Weller 2007; Eggertsson and Krugman 2012).Footnote 4 Could mobility, wages and debt interact to generate a negative feedback loop, which differentially applied across a population, be one of the mechanisms driving wealth inequality? Informed by the evidence presented above, we propose that if pursuing improving work opportunities requires some amount of financial capital, then individuals without savings either miss out on wage increasing opportunities or resort to borrowing, which impedes their future ability to save, and that this dynamic may be a driver of wealth inequality.Footnote 5,Footnote 6 Thus we wish to explore a complex interaction between savings, lending and wages to explain job mobility and its consequences for wealth. Kirman (2011) defines economic complexity as agent interactions generating phenomena at the macroeconomic level that do not coincide with observations at the microeconomic, so in that spirit we have developed a stylized multi-agent model, the Emergent Firms (EF) model, to explore the emergent effects of individual work choices in the context of job change costs, savings and lending. We indeed find that if pursuing a job opportunity incurs costs, then having financial capital matters, and without it, and especially in the presence of debt, agents are limited in their ability to fully participate in the stylized economy. The strength of the relationships found in the model may generate testable hypotheses (Griffin 2006) as well as justify efforts to seek techniques and datasets to demonstrate these complex feedback effects more explicitly.",2
59.0,1.0,Computational Economics,30 October 2020,https://link.springer.com/article/10.1007/s10614-020-10066-6,Development of Intelligent Stock Trading System Using Pattern Independent Predictor and Turning Point Matrix,January 2022,Yoojeong Song,Jae Won Lee,Jongwoo Lee,Unknown,,Unknown,Mix,,
59.0,1.0,Computational Economics,03 November 2020,https://link.springer.com/article/10.1007/s10614-020-10063-9,Deep Learning Based Hybrid Computational Intelligence Models for Options Pricing,January 2022,Efe Arin,A. Murat Ozbayoglu,,Male,Unknown,Unknown,Male,"Financial options are among the most versatile derivative products that allow the investors to control their risks in varying market conditions. However, unlike most other financial instruments, there is some ambiguity in the option valuation process. Hence, there have been some variations between the real option values and the calculated option prices through well-known models like Binomial or Black–Scholes (Gultekin et al. 1982). Black and Scholes (1973) is a model constructed by Fischer Black and Myron Scholes and became the most commonly used formula in option pricing with some adjustments (Bodie et al. 2008). With the introduction of the formula, options trading excessively increased (MacKenzie 2006). Even though there are some known problems, like volatility smile, overall the model has a good approach on options pricing and widely adapted by the financial industry (Bodie et al. 2008). Meanwhile, slight differences can be observed in the BS formula output and the actual option prices, especially at both ends of the option premium value curve. Also depending on the volume, significant variations can occur in real-time pricing. So, using BS especially for the deep out-of-the-money options can be dangerous, since volume of these risky options are generally low and the options at that end is the smiling part of the BS where the pricing performance is relatively poorer. As a result, a better option pricing, especially at the tail ends might be quite helpful. Since more and more people started opening online brokerage accounts and involved in electronic trading, more accurate options pricing became a necessity. As better valuations are achieved, more accurate and more profitable investments can be available. Furthermore, with better pricing in options, the market dynamics and overall volatility also might become more robust and resilient to market manipulations. Meanwhile, computational intelligence, in particular deep learning models lately have shown to provide better prediction performances in various financial application areas. However, there was still a lack of deep learning studies that were focused on options (Ozbayoglu et al. 2020) which motivated us to work on developing such models. In light of all these information, our motivation and research objectives in this study are two-folds: Firstly, we wanted to develop hybrid deep learning models for options pricing. Then, secondly, we wanted to achieve better pricing performance when compared with BS formula under all working conditions without ambiguous volatility definitions. Throughout the paper, our focus was to outperform BS (Black–Scholes) model for the European type options trading through the use of deep neural network (DNN) based approaches. The main objective of our study was to be able to develop option pricing models without depending on external implied volatility estimations. For this purpose, S&P 500 ETF (SPY) options are used and compared to corresponding BS formula outputs that use volatility as annualized 20 intraday returns. As a result of our research and development efforts, we developed several hybrid DNN models. Our main contributions in this study were the developed novel DNN classifiers which had multi-stage topologies for various option scenarios. We also provided a thorough analysis for performance evaluation. Classifier-Pricer structured multi stage DNN based topology with optimized parameters gave the best result under the conditions of European style options for the stock closing times, while we used volume and open interest values. The results indicate the volatility smile was the main problem of the BS formula and the proposed model structure becomes more useful if risky tail-end (out-of-money) investments occur more frequently in the market. The performance evaluation results show that better option pricing models can be developed using DNN and other computational intelligence techniques. The details of our analysis are explained in the following sections. The structure of the paper is as follows: After this brief introduction, related works in this field will be analyzed, then methodologies used along with their results will be explained and finally future works and conclusions will be given.",6
59.0,1.0,Computational Economics,04 November 2020,https://link.springer.com/article/10.1007/s10614-020-10062-w,Multivariate Cointegration and Temporal Aggregation: Some Further Simulation Results,January 2022,Jesús Otero,Theodore Panagiotidis,Georgios Papapanagiotou,,Male,Male,Mix,,
59.0,1.0,Computational Economics,23 November 2020,https://link.springer.com/article/10.1007/s10614-020-10072-8,Implementing Maximum Likelihood Estimation of Empirical Matching Models,January 2022,Baiyu Dong,Yu-Wei Hsieh,Xing Zhang,Unknown,,,Mix,,
59.0,1.0,Computational Economics,23 November 2020,https://link.springer.com/article/10.1007/s10614-020-10074-6,Bayesian Analysis of Realized Matrix-Exponential GARCH Models,January 2022,Manabu Asai,Michael McAleer,,Male,Male,Unknown,Male,"Estimation and forecasting time-varying co-volatilities between assets plays an important role in asset pricing, portfolio selections, and risk management. Estimating conditional covariance matrices via the multivariate models of the class of generalized autoregressive conditional heteroskedasticity (GARCH) is a popular approach (e.g., see the survey paper by McAleer 2005). Over the past two decades, realized measures of volatility have received unprecedented attention in the academic literature on modeling and forecasting of stock market returns volatility. In the traditional literature on GARCH models, Engle and Gallo (2006) and Shephard and Sheppard (2010), among others, incorporated realized measures for modeling and forecasting volatility. In addition, Hansen et al. (2012) suggested the realized GARCH framework, which provides a structure for the joint modeling of returns and realized measures of volatility. By extending the aforementioned work, Hansen and Huang (2016) developed the realized exponential GARCH (EGARCH) model, which is an extension of Nelson’s (1991) EGARCH model. The former only use the information contained in the realized co-volatility matrix, and the models in both studies can be improved by considering the difference between realized and unobservable co-volatility matrix. Using information on multiple asset returns and corresponding realized covariance matrix measure, we consider a multivariate extension of Hansen and Huang’s (2016) realized EGARCH model. For the model specification, two features of Nelson’s (1991) EGARCH model are that it accommodates asymmetric effects and guarantees the positive value of conditional volatility via the exponential transformation. For asymmetric effects from asset returns to future volatility, Hansen and Huang (2016) consider the second-order approximation of Hermite polynomials, rather than the absolute value function of standardized returns, as used in Nelson (1991). We consider a multivariate version of this type of specification and develop news impact curves following Engle and Ng (1993). Turning to the positive definiteness of conditional co-volatility, we use the matrix-exponential transformation in Chiu et al. (1996) and Kawakatsu (2005). We also incorporate measurement errors of realized volatility and co-volatility measures in the specification of the new realized matrix-exponential GARCH model. Although Bauer and Vorkink (2011) and Asai and McAleer (2015) developed alternative matrix-exponential models, they are not extensions of Hansen and Huang (2016). Recently, several authors such as Hansen et al. (2014), Bollerslev et al. (2018), Gorgi et al. (2019), and Xu (2019) suggest using return vectors and realized covariance measures simultaneously for estimating conditional covariance matrix. Hansen et al. (2014) developed the realized beta GARCH model by accommodating the model of realized correlation. Bollerslev et al. (2018) proposed the realized semicovariance GARCH model by decomposing the realized covariance into three sums, that is, outer-products of positive returns, those of negative returns, and the cross products of positive and negative returns. Gorgi et al. (2019) suggested the realized Wishart-GARCH model based on a score-driven co-volatility matrix. Xu (2019) considers the DCC-HEAVY model as an extension of the HEAVY model. Apart from these works, we accommodate matrix-exponential transformation to develop a multivariate extension of Hansen and Huang’s (2016) realized EGARCH model by accommodating matrix-exponential transformation. As discussed in Virbickaite et al. (2015), the Markov Chain Monte Carlo (MCMC) technique is popular for Bayesian estimation of univariate and multivariate GARCH models. Using the Bayesian MCMC method, Vrontos et al. (2003) estimated several bivariate ARCH and GARCH models, and found that maximum likelihood (ML) estimates of the parameters were different from their estimates of the posterior means. As the differences can be caused by the non-normality of the parameter distributions, Vrontos et al. (2003) suggest cautious interpretation of the ML estimation method. For this reason, the Bayesian MCMC technique is considered herein so that non-normal posterior distributions are obtained. Recently, Ausín et al. (2014), Jensen and Maheu (2013), and Virbickaite et al. (2015) suggested using Bayesian non-parametric methods as an alternative to the parametric Bayesian approach. The important difference between these methods is on prior distributions. These authors considered infinite mixtures of Gaussian distributions with a Dirichlet process (DP) prior over the mixing distribution, which results in DP mixture models. This approach is left for future research. The remainder of the paper is organized as follows. Section 2 introduces the new realized MEGARCH model and its news impact curve. Section 3 provides a detailed explanation of the MCMC algorithm for the Bayesian estimation. Section 4 provides an empirical example for three stocks traded on the New York Stock Exchange. Section 4 compares five types of symmetric and asymmetric models, using the deviance information criterion of Spiegelhalter et al. (2002) and the Frobenius norm of forecast error. In Sect. 4, we also examine the MCMC estimates of the parameters, and present the news impact curves for describing effects from stock returns to future (co-)volatility of its own and other assets. Finally, some concluding remarks are provided in Sect. 5.",4
59.0,1.0,Computational Economics,30 November 2020,https://link.springer.com/article/10.1007/s10614-020-10076-4,Economic Categorizing Based on DFT-induced Supervised Learning,January 2022,Ray-Ming Chen,,,Unknown,Unknown,Unknown,Unknown,,
59.0,1.0,Computational Economics,03 January 2021,https://link.springer.com/article/10.1007/s10614-020-10080-8,Optimality Between Time of Estimation and Reliability of Model Results in the Monte Carlo Method: A Case for a CGE Model,January 2022,Tetsuji Tanaka,Jin Guo,Baris Karapinar,Male,Female,Unknown,Mix,,
59.0,1.0,Computational Economics,03 January 2021,https://link.springer.com/article/10.1007/s10614-020-10082-6,Finite Sample Lag Adjusted Critical Values of the ADF-GLS Test,January 2022,Peter S. Sephton,,,Male,Unknown,Unknown,Male,,1
59.0,1.0,Computational Economics,04 January 2021,https://link.springer.com/article/10.1007/s10614-020-10061-x,Solving High-Dimensional Dynamic Portfolio Choice Models with Hierarchical B-Splines on Sparse Grids,January 2022,Peter Schober,Julian Valentin,Dirk Pflüger,Male,Male,Male,Male,"A common approach to solve dynamic portfolio choice models in discrete time is dynamic programming, iterating over the value function backwards in time. Starting from the known value function at final time T, the value function is approximated on a state space grid, assuming that the state space is continuous. To determine the next iterate of the value function at each grid point at time \(T- 1\) we have to solve an optimization problem that depends on the value function of the previous iterate at time T. When a tensor product approximation is used, this approach suffers from the curse of dimensionality as the number of grid points of the approximation grows exponentially with the dimensionality of the state space. In addition, solving for the current value function iterate at a grid point relies on an accurate solution of the underlying optimization problem. When the portfolio choice is continuous, e.g., choosing the investment amount in stocks, bonds, etc., the computation of the optimal solution can be greatly accelerated by gradient-based optimization routines if the gradient of the objective function is available. Recently, sparse grids have been successfully employed to break the curse of dimensionality in high-dimensional dynamic models (Brumm and Scheidegger 2017; Judd et al. 2014; Schober 2018; Winschel and Krätzig 2010).Footnote 1 A standard d-dimensional tensor product grid on the unit hypercube \([0,1]^d\) with mesh size \(2^{-n}\), \(n \in \mathbb {N}\), and no points on the boundary contains \(2^{n} - 1\) grid points per coordinate direction and thus \(\mathscr {O}(2^{nd})\) points in total, growing exponentially with the dimensionality d. In contrast, a regular sparse grid with the same mesh size contains only \(\mathscr {O}{(2^{n} n^{d-1})}\) points. The error of the sparse grid approximation of a function with homogeneous boundary conditions using piecewise linear basis functions is \(\mathscr {O}(2^{-2n} n^{d-1})\) with respect to the \(L^2\) and \(L^\infty \) norm if the approximated function has bounded mixed second derivatives (Bungartz and Griebel 2004; Zenger 1991). This is only slightly worse than the corresponding error \(\mathscr {O}(2^{-2n})\) for the case of full tensor product grids. In higher dimensions, even regular sparse grids need too many grid points for a sufficiently accurate approximation when solving high-dimensional dynamic models (Brumm and Scheidegger 2017). Fortunately, for approximations in the standard piecewise linear basis, the hierarchical structure of sparse grids allows for spatially adaptive refinement of the grid by inserting the 2d children of only certain leaves in the hierarchical structure. Spatially adaptive refinement has successfully been employed to solve high-dimensional dynamic models by Brumm and Scheidegger (2017) and  Schober (2018). Unfortunately, approximations of the value function using the standard piecewise linear basis are not continuously differentiable and, hence, have discontinuous gradients. This poses a problem to gradient-based optimization techniques, which rely on a twice continuously differentiable approximation of the objective function to ensure convergence (Schober 2018). Global polynomial approximations have shown to work well with value function iteration and continuous choices for solving dynamic economic models (Cai and Judd 2015; Judd et al. 2014) as they are globally smooth. Smolyak’s formula can be used to construct sparse grid approximations (Barthelmann et al. 2000) on global polynomial bases, which can be refined adaptively with regard to specific dimensions of the state space (Judd et al. 2014) and with regard to the hierarchical surpluses, i.e., locally adaptively (Stoyanov 2017). Value function iteration with the use of gradient information to approximate the value function more accurately with global polynomials is also possible (Cai and Judd 2015). However, B-splines are much more flexible than global polynomials (Valentin and Pflüger 2016; Valentin 2019). While global polynomial approximations are bound to certain grid structures to avoid Runge’s phenomenon or similar issues, B-spline basis functions can be employed on any nested spatially adaptive grid hierarchy. They allow for simultaneous local- and degree-adaptive refinement (hp-adaptivity), implying that one could use a smaller or larger mesh size and/or degree of the B-spline basis functions in certain regions of the state space, e.g., to resolve kinks. In addition, the local basis functions are faster to evaluate than the conventional global polynomial basis functions. Approximations with B-splines of cubic degree (or higher) are twice continuously differentiable, and readily supply smooth and explicit approximations of both, the value function and the gradient. Compared to approximating the derivatives with finite differences, the optimization is not only more accurate but also significantly faster, especially when the number of optimization variables is large (Valentin 2019). B-splines have thus proven useful for computing numerical solutions to numerous dynamic models when finding the root of the gradient is required (Chu et al. 2013; Habermann and Kindermann 2007; Judd and Solnick 1994; Philbrick and Kitanidis 2001). In total, three issues with discrete time dynamic programming for dynamic portfolio choice models with continuous choices emerge: the curse of dimensionality, the lack of spatial adaptivity, and the lack of continuous gradients. It is apparent that current economic literature deals with these issues only in isolation, e.g., by combining sparse grids with global polynomial basis functions, or using sparse grids with non-smooth local linear basis functions to allow for spatial adaptivity. These approaches are hence computationally inefficient in accurately solving high-dimensional dynamic portfolio choice models or any high-dimensional dynamic economic model that requires smooth approximations or gradient-based optimization. This paper is the first to address all of these issues at once by combining hierarchical B-splines with sparse grids to approximate the value function and its gradient. Thus, we enable accurate and fast numerical solutions using gradient-based optimization while still allowing for spatial adaptivity (Pflüger 2010; Valentin and Pflüger 2016). The hierarchical grid structure allows us to develop an algorithm that uses the local adaptivity similar to Brumm and Scheidegger (2017) and Schober (2018), but interpolates the value function and its gradient with a B-spline basis. Therefore, we create a sparse grid for the value function, for which we interpolate the value function in the piecewise linear basis. We then refine the grid using the standard hierarchical surplus-volume-based refinement criterion. Finally, we interpolate the value function with hierarchical B-spline basis functions on the spatially adaptively refined sparse grid. We focus our study on the numerical accuracy of our approach. Therefore, we choose a dynamic portfolio choice model with multiple stocks, one bond, and consumption. For buying and selling the stocks, linear transaction costs have to be deducted. The resulting optimization problem is high-dimensional in terms of the state space, stochastic sample space, and choice variables. Hence, this problem is especially suited for a complexity analysis. At the same time, this model is similar to models from a vast strand of state-of-the-art literature on dynamic portfolio choice, e.g., Barberis and Huang (2009), Cocco et al. (2005), De Giorgi and Legg (2012), Horneff et al. (2010), Horneff et al. (2008), Hubener et al. (2016), Hubener et al. (2014), Inkmann et al. (2011). Consequently, our approach can be generalized to a broad class of dynamic portfolio choice models with only minor modifications. Dynamic portfolio choice models with transaction costs have been studied economically, e.g., by Abrams and Karmarkar (1980), Kamin (1975), Liu and Loewenstein (2002), Magill and Constantinides (1976), and extensively numerically by Cai (2009), Cai and Judd (2010), Cai et al. (2015), Cai et al. (2020). The latter report computational times and economic solutions for higher-dimensional transaction costs problems. They employ polynomial interpolation with only few polynomial nodes and parallelization to solve these problems with and without consumption using value function iteration in discrete time. Cai et al. (2020) present convergence results and computational times for the three-dimensional transaction costs problem with consumption and numerical errors for the four-dimensional problem using complete Chebyshev polynomials to approximate the value function. However, only we have employed spatially adaptive sparse grids to the transaction costs problem, which induces optimization on continuous choices (Schober 2018). We also apply local adaptivity to compute the optimal policies from the solution to the underlying optimization problem as earlier suggested by us (Schober 2018) and Brumm and Grill (2014). A complexity analysis reveals that cubic B-splines save more than one order of magnitude in computational effort compared to the state of the art with the linear basis (Brumm and Scheidegger 2017) and/or finite difference approximations of the gradient on regular sparse grids. Using spatially adaptive refinement of the optimal policy, we solve the problem for up to five dimensions where highly accurate solutions on regular sparse grids would require hundreds of thousands of grid points, and are thus no longer suitable. Here, spatially adaptive refinement allows a comparably low base resolution in the solution process of the value function, and, in a second step, adds grid points in the optimal policies where required. By this, we obtain low reported unit-free Euler equation errors for the transaction costs problem. The rest of this article is structured as follows: In Sect. 2, we define the general class of dynamic portfolio choice models for which our approach is applicable. Section 3 introduces hierarchical B-splines on spatially adaptive sparse grids, leading to the definition of hierarchical weakly fundamental not-a-knot splines. Algorithms for solving dynamic portfolio choice models with B-splines on spatially adaptive sparse grids are discussed in Sect. 4. We analyze the complexity and demonstrate the numerical accuracy of our approach solving the transaction costs problem in Sect. 5 before concluding in Sect. 6.",1
59.0,1.0,Computational Economics,13 March 2021,https://link.springer.com/article/10.1007/s10614-021-10105-w,Correction to: Solving High-Dimensional Dynamic Portfolio Choice Models with Hierarchical B-Splines on Sparse Grids,January 2022,Peter Schober,Julian Valentin,Dirk Pflüger,Male,Male,Male,Male,"The original publication has been updated. In the original publication of this article, under the Introduction heading section, the corrections to the second paragraph’s inline equation were not incorporated. The author’s additional corrections have also been incorporated. The publisher apologizes for the error made during production.",
59.0,1.0,Computational Economics,04 January 2021,https://link.springer.com/article/10.1007/s10614-020-10068-4,Connectedness in International Crude Oil Markets,January 2022,Niyati Bhanja,Samia Nasreen,Aviral Kumar Tiwari,Unknown,Female,Unknown,Female,"Crude oil prices are determined by the complex embodiment of factors that operate both regionally and globally. Several implications have sprouted especially related to the crude oil price benchmark co-movements. In energy economics literature, this has led to the emergence of two schools of thought. A strand of literature led by Adelman (1984) argues that the international crude oil markets are globalized and behave like a one great pool. Some studies like Weiner (1991) hold the contrary view. Weiner (1991) counter argues the idea of globalized crude oil markets and supports regionalization in the crude oil markets. Theoretically, both the arguments are hinged on sound theoretical underpinnings. According to Law of one price, a commodity similar in all respects should be sold at one price globally when converted into same currency under zero transportation condition. Crude oil varieties are similar in certain characteristics and differ in other characteristics like quality. In terms of heaviness and sourness, the West Texas Intermediate (WTI) is of the best quality. Other crude oil varieties though different, differ by miniscule degrees of heaviness and sourness. Transportation costs too forms the miniscule of costs, as crude oil is generally transported through ships in bulk quantity. One can therefore find the attributes of both globalization and regionalization in the crude oil markets. The advocates of globalization argue that there always occurs an active arbitrage in the international crude oil markets. Hence, the movements of crude oil prices in one market tend to move the crude oil prices in other markets at least in the long run. The supporters of regionalization argue that the transaction costs and crude oil quality differential often blur the co-movement of one crude oil market with the other market. The argument of crude oil markets behaves like one great pool or they behave, as a set of regionalized markets, is an unsettled issue. Empirical studies using different methodologies lend mixed support to the globalization–regionalization debate. The study by Gulen (1999) uses cointegration to test the globalization–regionalization hypothesis and concludes that crude oil prices co-move and converges in the long run rejecting the regionalization hypothesis. Hammoudeh et al. (2008), for example, uses Enders and Siklos (2001) threshold cointegration to test the co-movement between crude oil pairs. The results indicate that the short run adjustments towards long run equilibrium is asymmetric. The presence of non- linear cointegation suggests that the crude oil markets are globalized. Fattouh (2010) applies threshold unit root test on the spreads between crude oil pairs and finds that the spreads display mean reverting process. The stationary nature of crude oil price spreads thus indicates that the crude oil markets to be globalized. Reboredo (2011) using Copulas tested the extreme dependence between four regional benchmarks. For weekly data of WTI,Footnote 1 Brent, Dubai and Maya crude, the study observes significant dependence of crude oil varieties in upper and lower tails supporting the globalization hypothesis of crude oil markets. Some studies also lend support to the regionalization perspective. Fattouh (2011) justifies the regionalization characteristics of crude oil prices and argues that the WTI being a water borne crude often suffers from storage and logistic constrains. The prices of WTI, therefore, deviate from the other benchmarks at times. Liu et al. (2015) also support the arguments of recent regionalization to the factors highlighted by Fattouh (2011). One thread of literature exclusively attempts to investigate whether crude oil prices and their respective spreads have undergone any structural change. Lee et al. (2010) finds that the structural break has occurred in the WTI price during 1999. Chen et al. (2015) test structural change in the spreads between WTI and Brent and confirm a break during 2010. The break has been attributed to number of factors particular to the WTI crude oil supply centers at Cushing Oklahoma. The factors among others include logistic and storage constraints driven by excess supply of shale oil. Some studies, in order to ascertain, which crude oil prices serve as benchmarks, analyze the causality in crude oil pairs. Lin and Tamvakis (2001) find that disturbance in Brent is followed by the subsequent changes in the WTI. Using 32 crude oil varieties, Wlazlowski et al. (2011) show that the crude oil prices are driven by conventional benchmarks like Brent and WTI. The study by Montepeque (2005) shows that Brent and WT act as a benchmark for Russian Urals and Mars-American sour respectively. The more recent studies have put effort in understanding the price dynamics of global crude oils over time frequencies. In the study, Naccache (2011) has developed the case for analyzing crude oil prices at different time frequencies rather at conventional aggregate level. He argued that oil shocks essentially originate at different sources making its persistence time–frequency dependent. For instance, fire mishap in the refineries may last for few days, while strikes of workers may continue for months. In similar fashion, imposition of embargo for years and emergence of new source of energy or innovation in production technique most likely to have impact on price over a longer horizon. Assessing the oil price dynamics at disaggregated frequency level, therefore, has logical ground. Bhanja et al. (2018) in their recent study has worked in the similar line assessing the price integration of two major benchmarks—WTI and Brent. The study using various instruments of wavelets and test of club convergence, observed the crude prices to be synced over higher time horizons (long run); while fragmented in the short run. The empirical literature on the connectedness of crude oil market is summarized in Table 1. The enormous literature available on international crude oil markets is abstract and continues to surprise many energy economists. On several issues, the debates continue to persist and remain unsettled. One such debate is whether crude oil markets behave like one big market or like a set of fragmented markets. To shed light on this issue, this paper will investigate if the world’s major benchmark crude oil markets are integrated, hence globalized, using the latest data. This paper augments the existing debate on globalization of crude oil markets by applying firstly, the Bayesian inference of dynamic correlation in multivariate factor stochastic framework introduced by Kastner et al. (2017). One of the main problems with time-series data is the curse of dimensionality that is the higher dimensional dependency structures of time-series. In this method, this issue is solved by introducing low dimensional latent factor structure that effectively reduce the number of parameters at a viable amount. Bayesian Markov Chain Monte Carlo (MCMC) algorithms are used to obtain reliable statistical inferences. Using interlinking but mathematically equivalent parametrizations, this technique significantly improve the mixing of draws obtained from the posterior distribution, particularly for the factor loadings matrix. This method is easy to implement for a large set of data, convenient to extend and fully automatic in the sense that turning parameters do not require adjustment manually. Secondly, the family of multivariate Generalized Auto-regressive Conditional Heteroscedasticity (GARCH) models and Vector Error Correction (VEC) models are not capable enough to quantify spillovers in adequate details. In order to better measure the effects of volatility spillovers in crude oil markets, Diebold and Yilmaz (2012) (hereafter DY) time domain and Barunik and Krehlik (2018) (hereafter BK) frequency domain spillovers are used. Both methods provide the measures of spillovers or connectedness with latter measuring the connectedness at different time horizons. The main contribution of this study is the application of Bayesian inference of dynamic correlation introduced by Kastner et al. (2017), the identification of dynamic connectedness (both over time as well as across time horizons) through rolling sample windows and visualizing the network connectedness of international crude oil prices using network structure diagrams. The results of this study will provide useful information for hedgers and buyers (arbitragers) importing crude oil from the different international oil markets. While more globalized crude oil markets will offer lesser arbitrage opportunities for oil traders, lesser globalized crude oil markets the vice versa.",5
59.0,1.0,Computational Economics,06 January 2021,https://link.springer.com/article/10.1007/s10614-020-10078-2,Predicting Firm-Level Bankruptcy in the Spanish Economy Using Extreme Gradient Boosting,January 2022,Matthew Smith,Francisco Alvarez,,Male,Male,Unknown,Male,"Recent developments in regulatory requirements from central banks and governments have caused credit risk management to take a leading role amongst practitioners. The Basel agreement requires financial institutions to limit the amount of risk-weighted assets that a bank can hold, this affects the type and number of loans that a bank can issue in relation to its capital. Therefore, it has become more apparent to the banking industry that new and improved, more innovative ways should be adopted in order to identify counter-party default risk amongst its corporate clients early on. On a more economic scale the ability to adequately shift financial resources from an ailing firm to more positive recipients will help clear up inefficiencies within the financial lending sectors. Forecasting the failure of an organisation is an important economic and financial challenge, failure to adequately manage financially distressed firms within a timely manner in an economy can have profound negative economic consequences. Additionally the insolvency procedure can stretch across many years. Hernandez-Tinoco and Wilson (2013) found that UK firms have an average time gap of 1.17 years from the events which caused the firm to go bankrupt and the date in which bankruptcy was filed. Theodossiou (1993) found that firms in the US fail to provide financial accounts 2 years prior to bankruptcy. This suggests that firms feel the struggle of financial distress before filing for bankruptcy and it is therefore critical for financial institutions to identify these firms early on. This paper addresses the issue of new and innovative bankruptcy prediction models by applying a Machine Learning algorithm in order to better classify and distinguish bankrupt firms from non-bankrupt firms. We apply our model over 4 years of financial accounts, aiming to identify financially distressed firms early on. We aim to capture the rarity of financially troubled firms in an economy by using an imbalanced dataset and finally, we aim to capture the imperfectness of financial data through the inclusion of extreme and missing values as information. Moreover we show that Machine Learning models need not give just a simple black-box prediction but can be interpretable through a number of ways as in traditional regression models. We finally compare our results with a series of other Machine Learning models, notably a Support Vector Machine (SVM), Neural Network, Logistic Regression, Random Forest and Light Gradient Boosted Machine (LightGBM). The higher predictive accuracy and interpretability are just two ways why the model proposed in this paper is considered one of the most prominent Machine Learning models currently in use and therefore would be well posed for the problem of bankruptcy prediction. The problem of bankruptcy prediction is well suited for classical binary Machine Learning classification problems. A company can either be in a state of bankruptcy or not. The companies with the status of bankruptcy are described as the positive class since we wish to positively identify these firms. The negative class are the non-bankrupt firms, which are included such that the model can learn the differences between the two classes. When a model correctly predicts that a company is in a state of bankruptcy, then it is called a True Positive (TP). Conversely when a model correctly predicts that a company is in a state of non-bankruptcy, then it is called a True Negative (TN). The case when a model predicts that a company is bankrupt but the actual status is non-bankrupt is called a False Positive (FP or Type I error). Finally, the case when the model predicts that a company is non-bankrupt but the actual status is bankrupt is called a False Negative (FN or Type II error). The values TP, FP, FN, TN conform, by rows, a \(2 \times 2\) matrix denoted as the confusion matrix. The first challenge corresponds to analysing the cost of extending the forecasting horizon. Predicting bankruptcy early is an important factor in any lending policy. We make predictions for firms that went bankrupt using data 1, 2, 3 and 4 years prior to bankruptcy, in all cases using the whole dataset (including firms that remained active for the whole period). A number of statistics based on a confusion matrix have been computed, all of them with a common—and expected—message: extending the forecasting horizon impairs future predictions. The relevant question, of course, is to quantify this impairment through relevant statistics. A representative statistic of the performance of any classification algorithm is the Area Under the Curve (AUC) or the Area Under the Precision-Recall curve (AUPRC). The AUC takes a value of 1 for perfect classification, that is, 0 Type I and II errors, while it takes a value of 0.5 for pure random guessing.Footnote 1 The AUPRC takes value 1 for a perfect model and 0 for a poor model and is more informative than the AUC for imbalanced data sets. Our model achieves AUC values ranging from 0.84 to 0.74 and AUPRC values ranging from 0.66 to 0.42 for 1–4 year prior prediction respectively. It is worth noting that these values can be understood as optimal in some sense. Roughly, the goal of Extreme Gradient Boosting (XGBoost) is to push the limit of computational resources for boosted tree algorithms. XGBoost minimizes numerically a loss function which contains a number of parameters which need to be optimised. For this we carried out a grid search on a parameter space in order to minimise prediction errors on an in-sample test set using 10 fold cross validation, which is briefly discussed in more detail later. This implies that we located the parameter values which maximised the in-sample test AUC, AUPRC and minimise some other loss functions. We report our final analysis and statistics on a held-out test set. Next, we analyse which variables have a higher impact on the likelihood of bankruptcy. As mentioned, the XGBoost model minimizes a loss function. More precisely, the algorithm selects a loss minimizing collection of trees, denoted by \(\phi \). Each tree in \(\phi \) is a step function that assigns a score to each firm depending on the firm’s characteristics. The overall score of the firm is just the sum of these scores across all trees in \(\phi \). There is a standard monotonic function that maps overall scores into a probability of bankruptcy and, finally, the firm is predicted to be bankrupt whenever that probability overtakes a certain threshold, which we define. This scheme provides a natural environment to identify the most important variables in predicting bankruptcy. On average, across firms, the variables with the highest marginal contribution to the overall score are the ratio Total Liabilities to Total Assets (TL.TA), the logarithm of Total Assets (logTA), the logarithm of Sales (logSALES), Current Liabilities to Financial Expense (CL.FinExp) and Earnings Before Interest and Tax (EBIT.FinExp). Our analysis indicates that there are slight variations depending on the forecasting horizon: when we take 1 year prior predictions the most relevant characteristic is TL.TA, while for longer horizons it turns out to be logTA or logSALES. Perhaps more importantly, the across-firm variability is essential for any lending policy. The algorithm allows us to compute marginal impacts of each variable within each firm. A case study is provided for each quadrant of the confusion matrix. Generally, we illustrate the contribution of each variables changes from one firm to another in a way that is highly non-linear but roughly well captured by \(\phi \). Our final step is to compare XGBoost to other Machine Learning models. The key difference between the models is not non-linear vs linear—although this is an important characteristic of the models—but on how to deal with complexity. The loss function to be minimized under the XGBoost algorithm has two terms: the prediction errors and the overall complexity of \(\phi \), respectively.Footnote 2 The parameters which help control the complexity are decided by the practitioner, through domain knowledge and cross validation, otherwise default parameter values are given. Once the parameters are chosen, the algorithm starts from a very simple tree structure and recursively adds trees as to minimise an objective function up to a maximum number of trees, again usually determined at the cross validation stage.Footnote 3 To summarize, for a parametrized loss function, the XGBoost model automatically evaluates whether each increment of complexity pays off in terms of error prediction improvement. This automatism is absent in logistic models and other Machine Learning models. The rest of the paper is organized as follows. Section 2 positions this paper within the existing literature, in Sect. 3 we present the data, Sect. 4 discusses the methodology, Sect. 5 presents the main results, interpretation and case studies, Sect. 6 compares the performance to other Machine Learning models. Finally, Sect. 7 concludes the paper.",5
59.0,1.0,Computational Economics,11 January 2021,https://link.springer.com/article/10.1007/s10614-020-10071-9,Dynamic Metafrontier Malmquist–Luenberger Productivity Index in Network DEA: An Application to Banking Data,January 2022,Pooja Bansal,Aparna Mehra,Sunil Kumar,Female,Female,,Mix,,
59.0,1.0,Computational Economics,11 January 2021,https://link.springer.com/article/10.1007/s10614-020-10084-4,"Predictor Choice, Investor Types, and the Price Impact of Trades on the Tokyo Stock Exchange",January 2022,Ryuichi Yamamoto,,,Male,Unknown,Unknown,Male,"Stock markets have experienced a series of bubbles and crashes in the past few decades. Recent examples include the dot-com bubble from 1997 to 2000, which was followed by the crash in 2000/2001. Subsequently, we observed a boom around 2006, but major stock indices declined considerably in value after September 2008, triggered by the Lehman shock. Several analytical frameworks have been suggested for understanding the mechanisms of the apparent price dynamics. For example, Shiller (1981) and LeRoy and Porter (1981) imply that the phenomena are characterized by large and persistent stock price deviations from fundamental economic indicators. Brock and Hommes (1998) introduce a theoretical agent-based model wherein boundedly rational agents select their expectation rules from fundamentalist and trend-following predictors. Brock and Hommes (1998) and the extended theoretical works by several researchers demonstrate that shocks to economic fundamentals trigger asset mispricing and that this mispricing is magnified as the fraction of trend-followers increases in the market. The stock price reverts to the fundamental price because the market is dominated by fundamentalists. Therefore, theoretical agent-based models successfully explain the bubble-and-burst mechanism.Footnote 1 However, agents’ usage of the strategies differs among models employed to generate empirical features in financial markets. For example, the types of agent-based models developed by Brock and Hommes show that agents trade through dynamic predictor selection between fundamental and technical rules. Agents in the models, such as those by Chiarella et al. (2009), form price forecasts via a linear combination of fundamental and technical predictors, but the weights assigned to each predictor do not vary over time. Certain papers, such as Farmer and Joshi (2002), demonstrate that the trend-following indicator is the main generator of the empirical features in financial markets. Hommes (2006) concludes that dynamic heterogeneous agent models are able to generate the stylized facts, and contrarians also act as a destabilizing force, moving the price away from the fundamental value. By contrast, Szafarz (2012) theoretically concludes that the fundamental strategy independently causes excess volatility in financial markets. Several empirical studies have investigated whether stock investors actually use either a fundamentalist or a trend-following strategy and switch them over time.Footnote 2 However, none of the empirical studies has identified which types of investors independently use (1) a fundamental or (2) a technical predictor or which ones (3) switch them over time, nor which strategy has the greatest price impact and thus significantly explains the empirical feature in financial markets. It is important to examine which theoretical agent-based model has the most explanatory power for the empirical stylized facts in financial markets. We investigate this question by employing a unique dataset of a monthly panel that includes the transaction history of 11 types of buy-side investors on the Tokyo Stock Exchange from July 2002 through June 2013 whose stock capitalization exceeded 3 billion Japanese yen. The following 11 types of investors are included in our sample: those in charge of managing (1) proprietary trading; and funds placed in trusts by (2) individual investors, (3) foreign investors, (4) security companies, (5) investment trusts, (6) life or postal life insurance entities, (7) city or local banks, (8) trust banks, and (9) financial institutions other than (6), (7), and (8); (10) industrial corporations, and (11) other corporations. Our sample investors are fairly large and have a total trading volume that represents 87% of total market volume. Therefore, we are able to analyze the price impact of our investors’ trades. We first empirically identify whether our 11 types of investors use fundamental and/or technical predictors. If certain types of investors make significant use of both predictors, we proceed to examine whether they switch the rules over time. In the first step of identifying fundamental and/or technical predictors, we consider different versions of fundamental and technical predictor rules because of the model uncertainty in the formation of expectations. In particular, we formulate linear forecasting rules and rules using recursively updating parameters for fundamental and technical predictors. Our setup allows investors to have heterogeneous information sets and sophisticated expectations. We first determine the lags and whether or not they recursively update the parameters for each type of investor. We find that linear forecasting rules fit better than recursively learning rules do for all investor types in both fundamental and technical predictors, although the lag lengths differ across investor types. We next attempt to identify whether each type of investor uses fundamental and/or technical rules. When a certain investor significantly uses both rules, we investigate the significance of dynamic predictor selection for the investors by applying a maximum likelihood approach developed by Branch (2004). Ultimately, we determine whether each type of investor uses only one type of predictor or follows a switching or non-switching rule when they use both. The selected strategies are further analyzed to measure the price impact of trades by investor type. This study focuses on the fundamental and technical predictor rules for two reasons. First, our main objective is to validate the theoretical agent-based models that have successfully explained the mechanisms of several stylized facts in real financial markets, such as the bubble-and-burst mechanism, clustered volatility, and fat tails of return distribution.Footnote 3 Second, the two strategies are often used by investors in real financial markets, as demonstrated in survey studies of financial market participants such as those by Lui and Mole (1998) and Menkhoff and Taylor (2007). We demonstrate that life or postal life insurance entities, trust banks, industrial corporations, and other corporations (branches of foreign companies located in Japan or corporations related to governments, employee stock ownership, or labor unions) employ dynamic predictor selection between fundamental and technical rules. We also show individual investors, security companies, and investment trusts as fundamentalists, while foreign investors are trend-followers, and investors involved in proprietary trading and other financial institutions are contrarians. Furthermore, trades by all our investors have significant impacts on prices. Our findings offer significant support to several kinds of agent-based models that theoretically explain empirical phenomena regarding price dynamics in real financial markets, such as the short-term boom in the Japanese stock market that occurred around 2006 and the crash after September 2008 triggered by the Lehman shock. Our results suggest that a combination of the investor types behind trades and their predictor choices is a major determinant of price deviation from the fundamental value. This study contributes to the literature on agent-based finance by simultaneously identifying specific types of investors and their trading strategies that have (or do not have) a price impact. These contributions occur in the following context. First, several agent-based models with a fundamental or technical predictor or dynamic predictor selection have successfully replicated stylized facts in financial markets. Several empirical studies have investigated not only whether stock investors actually switch fundamentalist and trend-following strategies over time but also whether dynamic predictor selection explains asset price dynamics.Footnote 4 However, none of the empirical studies has identified the types of investors and investor strategies that do and do not explain stylized facts in financial markets. Empirical studies do not identify which types of agent-based theories best explain empirical phenomena in financial markets. We fill this gap with our unique dataset that allows us to investigate strategies and price impacts according to investor type. Our first contribution will assist financial investors and policy makers not only to understand the fundamental mechanism of price formation in actual financial markets but also to develop ideas for stabilizing financial markets or improving risk management in them. Second, we conduct not only an empirical test of the theory of Brennan and Cao (1996) but also an out-of-sample test of the evidence in Fama and French (1988), who use US data, by using a dataset on institutional investors in the Japanese market. Brennan and Cao (1996) demonstrate that traders who have a price impact, tend to be contrarians, whereas traders who do not have a price impact tend to adopt a trend-following strategy. Fama and French (1988) show that stock price momentum can be correlated positively in a short-to-intermediate horizon but negatively in a long horizon. We find that, among the 11 types of investors in our sample, short-term investors (such as foreign investors) who have a price impact tend to be trend-followers, whereas long-term investors (such as trust banks) who have a price effect tend to be contrarians. Therefore, we provide evidence contradicting the hypothesis by Brennan and Cao (1996) but supporting the view of Fama and French (1988). The remainder of the paper is organized as follows. Section 2 introduces the transaction data used in this paper. Section 3 develops an empirical model and details our estimation methodology; the section also presents empirical results on which strategies our investors actually employ and measures the price impacts of the investors. Section 4 summarizes our findings and presents a conclusion to this paper.",3
59.0,1.0,Computational Economics,11 January 2021,https://link.springer.com/article/10.1007/s10614-021-10092-y,The Cross-Shareholding Network and Risk Contagion from Stochastic Shocks: An Investigation Based on China’s Market,January 2022,Yun Feng,Xin Li,,,,Unknown,Mix,,
59.0,1.0,Computational Economics,10 August 2021,https://link.springer.com/article/10.1007/s10614-021-10160-3,Bidirectional Risk Spillovers between Exchange Rate of Emerging Market Countries and International Crude Oil Price–Based on Time-varing Copula-CoVaR,January 2022,Liang Wang,Tingjia Xu,,,Unknown,Unknown,Mix,,
59.0,2.0,Computational Economics,17 January 2021,https://link.springer.com/article/10.1007/s10614-020-10091-5,Evidence for Novel Structures Relating CSR Reporting and Economic Welfare: Environmental Sustainability—A Continent-Level Analysis,February 2022,George Halkos,Stylianos Nomikos,Kyriaki Tsilika,Male,Male,Female,Mix,,
59.0,2.0,Computational Economics,18 January 2021,https://link.springer.com/article/10.1007/s10614-020-10075-5,Using Double Frequency in Fourier Dickey–Fuller Unit Root Test,February 2022,Yifei Cai,Tolga Omay,,Unknown,Male,Unknown,Male,"Using Fourier functions to approximate structural breaks is gaining great attention in unit root tests since the seminal work of Gallant (1981). Following studies can be seen as Becker et al. (2006), Enders and Lee (2012a, b) and Rodrigues and Taylor (2012).Footnote 1 However, assuming single frequency in trig functions is too strict since the structural breaks caused by sudden events are unpredictable and asymmetrically located. The impacts of those unexpected events will make economic indicators asymmetrically react at different magnitude. Since the Fourier components are periodic functions, using single frequency cannot achieve satisfied approximating precision. Thus, employing double frequency in trig functions is more plausible to capture breaks which are asymmetrically located. Specifically, Fig. 1 plots the nominal crude oil price which is collected from the Energy Information Administration (EIA). There are frequent structural breaks which are asymmetrically located. To describe the deterministic trend, we utilize double frequency method and single frequency method as well. Obviously, the deterministic trend of double frequency method (\(k_s, k_c\)) is more fitted to the oil price. In fact, the single frequency method by choosing \(k=2\) cannot provide satisfied approximation to the sudden changes after 2000. Furthermore, the price before 2000 is comparatively stable around the mean, the single frequency method cannot mimic the trend. In this paper, we first propose a new Dickey–Fuller (DF) unit root test by utilizing double frequencies in Fourier components to approximate potential unknown structural breaks. This new test can be viewed as an updated version of previous ones to solve the problems of asymmetrically located structural breaks without ignoring the symmetric case. We first develop the asymptotic theory for the double frequency Fourier Dickey–Fuller unit root tests. Moreover, we solve the so-called Davies Problem through an updated data-driven method. By using Monte Carlo simulations, the proposed test has good finite sample size performance. In addition, the simulation results suggest that the newly proposed test gains more power than traditional methods (using single frequency) when the breaks are located at the beginning and end of the sample. Moreover, the proposed test statistics have better power performance than traditional single frequency method in smooth breaks. In empirical analysis, we utilize the newly proposed unit root test to examine relative commodity price. Existing studies like Kellard and Wohar (2006), Ghoshray (2011, 2018) and Winkelried (2016) have paid particular interests upon examining the unit root in relative commodity prices measured by Grilli and Yang (1988).Footnote 2 In this study, our attention is put on a new historical dataset which is measured by Harvey et al. (2010) in the main text.Footnote 3 In comparison with the single frequency method, we find more relative commodity prices reject the unit root hypothesis, indicating the relative prices are stationary around a deterministic trend obtained by using double frequency Fourier function. The rest of the paper is organized as follows. Section 2 constructs the double frequency Fourier Dickey–
Fuller unit root test. Section 3 presents the results of finite sample simulations. Section 4 is an empirical application to relative primary commodity prices. The last section concludes the paper.",16
59.0,2.0,Computational Economics,18 January 2021,https://link.springer.com/article/10.1007/s10614-020-10089-z,Macro-Regional Economic Structural Change Driven by Micro-founded Technological Innovation Diffusion: An Agent-Based Computational Economic Modeling Approach,February 2022,Zhangqi Zhong,Lingyun He,,Unknown,Unknown,Unknown,Unknown,,
59.0,2.0,Computational Economics,27 January 2021,https://link.springer.com/article/10.1007/s10614-021-10093-x,Deviation-Based Model Risk Measures,February 2022,Mohammed Berkhouch,Fernanda Maria Müller,Marcelo Brutti Righi,Male,Female,Male,Mix,,
59.0,2.0,Computational Economics,06 February 2021,https://link.springer.com/article/10.1007/s10614-021-10099-5,Determining the Flat Sales Prices by Flat Characteristics Using Bayesian Network Models,February 2022,Volkan Sevinç,,,Male,Unknown,Unknown,Male,"The need for residence is an indispensable need for people. Therefore, residence prices have always been a topic of interest in daily life and in various branches of science. In addition, residence prices in a country are one of the factors affecting the economy of the country. Estimating residence prices is very important for different purposes such as local or foreign people who want to buy a house for investment or dwelling in, banks providing mortgage credits, construction companies and real estate agents. Certain properties of a residence play a decisive role in the formation of it sales price. Although there are many factors that affect the sales price of a residence, there is no general agreement on how and to what extent these factors affect the price of the residence. Thus, researchers have been trying to build various models to estimate the sales price of a residence based on its various aspects. Hedonic price models, which are based on the consumer theory suggested by Lancaster (1966), have been being used widely for this purpose for a long time. Hedonic pricing identifies the factors and characteristics affecting the price of an item. Thus, hedonic price models estimate the sales price of a residence considering both its internal characteristics and some external factors that are likely to affect its price. House price estimation with hedonic pricing models is quite popular and repeated in many studies using data belonging to various countries. According to Sirmans et al. (2005), results provided by hedonic pricing models depend mostly on location and time. Moreover, the value that a buyer appraises for various properties of a residence varies considerably according to the social, cultural, economic, climatic conditions of the place where the residence is located. However, hedonic price models assume that buyers have the same amount of desire towards the same attributes of a property at the same level, which is not very possible. Bayesian networks however have some advantages in determining the effects of the variables. They have also some advantages over the regression models like hedonic pricing. For example, in regression models, regressors, which are used to estimate the dependent variable, are called independent variables, which are expected to be uncorrelated, and possible dependencies among them are ignored. Any change in the value of an independent variable affects only the dependent variable and not any other independent variables in the model. This way of modeling the sales price of a residence based on its various attributes may not be very realistic. Indeed, for example, when the number of bathrooms increases in a residence, it directly causes an increase in the total area of the residence. Bayesian network models, on the other hand, consider the dependency relations among all the variables in the model unless some restrictions are imposed. As the variables exist in a network structure they all can influence each other at various strengths. Another disadvantage of the regression models is that they are not as flexible as Bayesian network models. That is, regression models are additive models taking the sum of the independent variables and equalizing it to the dependent variable. After fitting a regression model, adding some extra independent variables that are found to be significant, may dramatically change the structure and even the accuracy of the model. Bayesian models, however, as they mainly consider the interdependencies among the variables rather than the sum of them, are not that highly affected by new variables being added into the model. Bayesian networks also do not suffer from over-parameterization problem very much like regression models do. Another advantage of Bayesian network models is that they can handle missing data more easily than regression models. In the literature, the number of studies on residence sales prices and containing a Bayesian approach is very few. In a study by Hui et al. (2010) hierarchical Bayesian approach was used to estimate the price of residences in Hong Kong. Giudice et al. (2017) developed a residence price estimation model based on the Bayesian approach. They used Markov Chain Hybrid Monte Carlo method for estimating the house prices in Naples, Italy. Liu et al. (2018) combined a manually formed causal Bayesian network depending on the expert opinion and a hedonic model to estimate the residence sales price in Nanjing, China.",2
59.0,2.0,Computational Economics,13 February 2021,https://link.springer.com/article/10.1007/s10614-021-10098-6,How Successful are Energy Efficiency Investments? A Comparative Analysis for Classification & Performance Prediction,February 2022,Haris Doukas,Panos Xidonas,Nikos Mastromichalakis,Male,Male,Male,Male,"Based on recent estimates by the International Energy Agency (IEA), investments in energy efficiency need to accelerate, as spending on renewable power will have to be doubled by 2030. More specifically, to meet sustainability goals and targets, low-carbon investment would need to increase two-and-a-half times by 2030, with its share rising to 65% (IEA 2019). However, the main hurdle remains the difficulty of sourcing the initial investment (Schlein et al. 2017), as project developers are struggling to finance their plans. This happens because investment institutions lack the knowledge required to evaluate and appreciate energy efficiency investments and therefore, they often seem reluctant to finance them. In this respect, this paper aspires to support investment institutions towards predicting the energy efficiency investments’ performance, enabling the identification of these investments that foster sustainable growth, while also having an extremely strong capacity to meet their financial commitments by reaching the expected performance targets (Xidonas et al. 2015; Doukas 2018). The presented approach aims to reduce the time and effort required, to enhance transparency and efficiency on decision making for investment institutions and therefore to stimulate greater volumes of energy efficiency investments. More specifically, the purpose of our analysis is to estimate, verify and compare prediction models that assess the probability of success of an energy investment, apply these models to the control sample to determine the most accurate model for prediction and finally, interpret the results. There are various approaches to properly estimate the probability of success of an investment in the financial world. Among them, the most widely used are the so-called classification models. These models use the main performance indicators of an investment as input, allocating a weight to each of them that reflects its relative importance in forecasting their probability of success (Gurný and Gurný 2013). They are associated with problems in which a set of alternatives are assigned to predefined homogenous groups. This type of problem is usually referred to as classification or sorting, depending on whether the groups are nominal or ordinal (Zopounidis and Doumpos 2002). We can group these models into three categories: (a) discriminant analysis; (b) regression models; (c) non-parametric models (Sironi and Resti 2007; Greene 2008). In the present study, we cover all the above categories, adopting linear discriminant analysis (LDA), ordinal logit, ordinal probit, support vector machines and k-nearest neighbors models, which are widely used in sorting problems that involve multiple categories (Louzada et al. 2016). The methods are applied in a crucial real-life problem, which is the identification of attractive energy efficiency investments. This application is the added value of the study, since, to the best of our knowledge, there is no such relevant approach applied to assist financial institutions increase their deployment of capital in energy efficiency. The data used are based on the Energy Efficiency De-risking Project (DEEP) database. We exploit data from 3004 energy efficiency projects in buildings from 6 countries across Europe (DEEP 2018). The data have been made available by financial institutions and investment funds, national and regional authorities, as well as energy efficiency solution providers, from the Energy Efficiency Financial Institutions Group (EEIFG), which was established in 2013 by the European Commission Directorate-General for Energy (DG Energy) and United Nations Environment Program Finance Initiative (UNEP FI). The sample contains data from the period 2006–2019, of which approximately 80% will be used as training sample. The rest of the paper is organized as follows: In Sect. 2, we analyze the underlying problem setting. In Sect. 3, we describe the methodological aspects of the applied models and in Sect. 4 we present the empirical testing procedure and discuss on the obtained results. Finally, concluding remarks are provided in Sect. 5.",6
59.0,2.0,Computational Economics,22 February 2021,https://link.springer.com/article/10.1007/s10614-021-10095-9,Bayesian Estimation of Economic Simulation Models Using Neural Networks,February 2022,Donovan Platt,,,Male,Unknown,Unknown,Male,"Recent years have, to some extent, seen the emergence of a paradigm shift in how economic models are constructed. Traditionally, a need to facilitate mathematical tractability and limited computational resources have led to a dependence on strong assumptions,Footnote 1 many of which are inconsistent with the heterogeneity and non-linearity that characterise real economic systems (Geanakoplos and Farmer 2008; Farmer and Foley 2009; Fagiolo and Roventini 2017). Ultimately, the Great Recession of the late 2000s and the perceived failings of traditional approaches, particularly those built on general equilibrium theory, would lead to the birth of a growing community arguing that the adoption of new paradigms harnessing contemporary advances in computing power could lead to richer and more robust insights (Farmer and Foley 2009; Fagiolo and Roventini 2017). Perhaps the most prominent examples of this new wave of computational approaches are agent-based models (ABMs), which attempt to model systems by directly simulating the actions of and interactions between their microconstituents (Macal and North 2010). In theory, the flexibility offered by simulation should allow for more empirically-motivated assumptions and this, in turn, should result in a more principled approach to the modelling of the economy (Chen 2003; LeBaron 2006). The extent to which this has been achieved in practice, however, remains open for debate (Hamill and Gilbert 2016). While ABMs initially found success by demonstrating an ability to replicate a wide array of stylised facts not recovered by more traditional approaches (LeBaron 2006; Barde 2016), their simulation-based nature makes their estimation nontrivial (Fagiolo et al. 2019). Therefore, while the last decade has seen the emergence of increasingly large and more realistic macroeconomic models, such as the Eurace (Cincotti et al. 2010) and Schumpeter Meeting Keynes (Dosi et al. 2010) models, their acceptance in mainstream policy-making circles remains limited due to these and other challenges. The aforementioned estimation difficulties largely stem from the simulation-based nature of ABMs, which, in all but a few exceptional cases,Footnote 2 renders it impossible to obtain a tractable expression for the likelihood function. As a result, most existing approaches have attempted to circumvent these difficulties by directly comparing model-simulated and empirically-measured data using measures of dissimilarity (or similarity) and searching the parameter space for appropriate values that minimise (or maximise) these metrics (Grazzini et al. 2017; Lux 2018). The most pervasive of these approaches, which Grazzini and Richiardi (2015) call simulated minimum distance (SMD) methods, is the method of simulated moments (MSM), which constructs an objective function by considering weighted sums of the squared errors between simulated and empirically-measured moments (or summary statistics). Though MSM has been widely applied in a number of different contextsFootnote 3 and has desirable mathematical properties,Footnote 4 it suffers from a critical weakness. In more detail, the choice of moments or summary statistics is entirely arbitrary and the quality of the associated parameter estimates depends critically on selecting a sufficiently comprehensive set of moments, which has proven to be nontrivial in practice. In response, recent years have seen the development of a new generation of SMD methods that largely eliminate the need to transform data into a set of summary statistics and instead harness its full informational content (Grazzini et al. 2017). These new methodologies vary substantially in their sophistication and theoretical underpinnings. Among the simplest of these approaches is attempting to match time series trajectories directly, as suggested by Recchioni et al. (2015). More sophisticated alternatives include information-theoretic approaches (Barde 2017, 2020; Lamperti 2018a), simulation-based approaches to maximum likelihood estimation (Kukacka and Barunik 2017), and comparing the causal mechanisms underlying real and simulated data through the use of SVAR regressions (Guerini and Moneta 2017). In addition to the development of similarity metrics, attempts have also been made to reduce the large computational burden imposed by SMD methods by replacing the costly model simulation process with computationally efficient surrogates (Salle and Yildizoglu 2014; Lamperti et al. 2018). Many of the above metrics have also been applied in the context of the related problem of model selection, where the output generated by various candidate modelsFootnote 5 is compared to empirically-observed data and the model associated with the lowest (highest) dissimilarity (similarity) score is predicted to be the most appropriate description of the empirical data-generating process. In particular, Franke and Westerhoff (2012) propose several variants of a novel financial heterogeneous agent model and make use of a moment-based approach to select the best performing candidate. Using more sophisticated information-theoretic techniques, Barde (2016) pits several prominent ABMs against each another and traditional time series models in a comprehensive series of head-to-head tests. Finally, Lamperti (2018b), who employs similar techniques, compares several variants of the Brock and Hommes (1998) model.Footnote 6 Interestingly, the aforementioned approaches are all frequentist in nature, with Bayesian techniques not generating much interest up until very recently.Footnote 7 This is perhaps rather surprising, given the plethora of Bayesian methods available for dynamic stochastic general equilibrium models (Fagiolo and Roventini 2017). The first major Bayesian study was conducted by Grazzini et al. (2017) and makes use of a relatively simple kernel density estimation-based likelihood approximation. This was later followed by the work of Lux (2018), who employs sequential Monte Carlo methods. While this investigation does make some attempts at Bayesian estimation, the vast majority of the experiments conducted still adopt a frequentist paradigm. In what appears to be a modest paradigm shift, the last year has seen a number of interesting new contributions. These include the work of Delli Gatti and Grazzini (2019), which applies similar techniques to those developed by Grazzini et al. (2017) to a medium-scale macroeconomic model,Footnote 8 the work of Lux (2020), which applies the methodology considered by Lux (2018) to additional Bayesian examples involving several small-scale financial models, and finally the probabilistic programming study conducted by Bertschinger and Mozzhorin (2020), which makes use of the Stan language to perform a calibration and model selection exercise on similar models to those considered by Lux (2020). While the estimation literature has certainly been growing, it still suffers from a number of key weaknesses. Perhaps the most significant of these is a lack of a standard benchmark against which to compare the performance of new methods. As a result, most new approaches have traditionally only been tested in isolation and comparative exercises have been relatively rare. For this reason, we compared a number of prominent estimation techniques in a previous investigation (Platt 2020) and found, rather surprisingly, that the Bayesian estimation procedure proposed by Grazzini et al. (2017) consistently outperformed a number of prominent SMD methods in a series of head-to-head tests, despite its relative simplicity. We therefore argued that more interest in Bayesian methods is warranted and suggested that increased emphasis should be placed on their development. Additionally, it is also worth noting that while the approaches of Lux (2020) and Bertschinger and Mozzhorin (2020) may achieve a modest degree of success when applied to small-scale models, they cannot be readily applied to models of a larger scale due to issues of computational tractability, a weakness not shared by the approach of Grazzini et al. (2017), which we found was able to achieve some success when confronted by a large-scale model of the UK housing market. In line with the above findings and recommendations, we now introduce a method for the Bayesian estimation of economic simulation modelsFootnote 9 that relaxes a number of the assumptions made by the approach of Grazzini et al. (2017) through the use of a neural network-based likelihood approximation. We then benchmark our proposed methodology through a series of computational experiments and finally conclude with discussions related to practical considerations, such as the setting of the method’s hyperparameters and the associated computational costs.",7
59.0,2.0,Computational Economics,22 February 2021,https://link.springer.com/article/10.1007/s10614-021-10102-z,Revisiting the Merton Problem: from HARA to CARA Utility,February 2022,Guiyuan Ma,Song-Ping Zhu,,Unknown,,Unknown,Mix,,
59.0,2.0,Computational Economics,24 February 2021,https://link.springer.com/article/10.1007/s10614-021-10106-9,A Regression-Based Calibration Method for Agent-Based Models,February 2022,Siyan Chen,Saul Desiderio,,Unknown,Male,Unknown,Male,"Unlike mainstream economic models, taking agent-based models (ABMs) to the data is still an unresolved issue. Generally speaking, agent-based (or also ‘multi-agent’) models are analytical frameworks used to describe and analyze complex systems populated by many heterogeneous and interacting units (i.e. the agents). The application of ABMs to economics has given birth to the so-called agent-based computational economics (ACE), which is basically the use of computer simulations to study evolving artificial economies composed of many autonomous interacting agents such as consumers and firms (Judd and Tesfatsion 2006). Applications are uncountable and range from the study of single markets, such as the financial market and the labor market, to the study of an entire multi-market economy. Formally speaking, ABMs are systems of stochastic difference equations that in general cannot be analytically solved because of their complexity. Once designed, therefore, an AB model must be coded and turned into a software, initialized and then simulated with the help of a computer for a given number of time steps. Then, from simulations the modeler obtains statistics that can be analyzed in order to assess the model’s properties, which in general will depend on the initial conditions, on the parameter values and on the stream of random numbers used along the simulation (for a deeper theoretical treatment see e.g. Delli Gatti et al. 2011, 2018). An important phase of the model analysis is its empirical validation, that is the comparison of the model with empirical data aimed at evaluating its degree of realism (Fagiolo et al. 2007). There are essentially three types of validation (Judd and Tesfatsion 2006; Bianchi et al. 2007):  Input or ex ante validation, ensuring that the model characteristics exogenously input by the modeler (such as agents’ behavioral equations, initial conditions, random-shock realizations, etc.) are empirically meaningful and broadly consistent with the real system being studied through the model. Descriptive output validation, assessing how well the model output matches the properties of pre-selected empirical data. Predictive output validation, assessing how well the model output is able to forecast properties of new data (out-of-sample forecasting). The most common procedure is descriptive output validation, which is carried out by visually and statistically evaluating the model’s ability to replicate (qualitatively and/or quantitatively) some stylized facts such as for instance firm-size distributions (as in Bianchi et al. 2007, 2008) or macroeconomic co-movements (as in Delli Gatti et al. 2011). Recently, more advanced methods to validate ABMs have been put forth. For example, Guerini and Moneta (2017) propose to compare the causal structures implied by a structural VAR model estimated on both real and simulated data, while Lamperti (2018) introduces a new measure of divergence to quantify the similarity between the dynamics of real-world time series and the dynamics of model-generated time series. Related and instrumental to validation is calibration (or also ‘estimation’), which is the process of choosing particular values for the model parameters in order to make the model output as realistic as possible.Footnote 1 As in general ABMs are complicated models that cannot be analytically solved, their parameters cannot be calibrated through direct estimation as it happens for regression models. Hence, ABMs are commonly calibrated by indirect methods such as indirect inference (Gourieroux et al. 1993) and the method of simulated moments (McFadden 1989), which are special cases of the simulated minimum distance approach (Grazzini and Richiardi 2015). Many different variants of calibration through indirect methods exist in ACE literature, including among the othersFootnote 2 those proposed by Gilli and Winker (2003), Bianchi et al. (2007, 2008), Fabretti (2013), Recchioni et al. (2015), Grazzini and Richiardi (2015) and Lamperti (2018). In essence, however, indirect methods always consist of three phases (Richiardi 2018):  Choosing a set of statistics \(S=(S_1,S_2,\ldots ,S_q)'\) computable both on real data (\(S_r\)) and on simulated data (\(S_m\));Footnote 3 Choosing a metric \(d(\cdot )\) to measure the distance between \(S_r\) and \(S_m\), that is the degree of realism of the model; Choosing by simulations those model parameters that minimize the distance \(d(S_r,S_m)\). All the three phases of the above scheme present their own issues, but the one that is more problematic from the computational point of view is the minimization procedure. For most of ABMs, in fact, the minimization of the distance cannot be achieved by solving a system of first-order conditions. Calibration therefore would require in principle a point-by-point exploration of the parameter space of the ABM aimed at calculating by simulations the value of the objective function \(d(S_r,S_m)\). In this way the modeler can find the “best” parameter vector, that is the one associated to the smallest distance. If the model is simple, a complete exploration of the parameter space can be done at reasonable computational costs. However, for most of ABMs this is hardly the case, as two difficulties arise. A first difficulty encountered during this exploration is created by the typical over-parametrization that characterizes ABMs and the ensuing high-dimensionality of the parameter space. For example, if in a model there are just 10 relevant parameters and each parameter can take on 10 different values (a rather simplifying assumption indeed), then the parameter space is composed of \(10^{10}\) parameter vectors. But a second difficulty adds to the search for the best parameter vector: the interference of randomness on the model output. Given in fact the set of statistics S, the statistic \(S_m\) computed on the output of a single simulation of the model will be in general a function of the model parameters \(\theta \) belonging to the parameter space \(\Theta \), of initial conditions \(I_0\) and random numbers r. Ignoring for simplicity the initial conditions (which can be done if the model is ergodic), we can write Hence, when computing the distance \(d(\cdot )\) between real and simulated data, also this will be a function of the parameters and the random numbers: Definition (2) highlights the second difficulty afflicting calibration, that is the influence of the numbers r on the magnitude of distance d. The typical way of getting rid of this influence is to perform Monte Carlo simulations. Basically, the model needs to be simulated k times with the same parameter vector but with different streams of random numbers, obtaining k statistics \(S_m(\theta ,r_k)\), so that the Monte Carlo average \({\hat{S}}_m(\theta )=k^{-1}\sum _k S_m(\theta ,r_k)\) can be taken. Finally, the distance d is computed using \({\hat{S}}_m(\theta )\), from which the influence of random numbers has been possibly averaged out. Clearly, computing Monte Carlo averages for each parameter vector \(\theta \in \Theta \) is computationally demanding, if anyway workable (in the example above it would require \(k\cdot 10^{10}\) simulations of the model). Hence, in order to reduce the computational burden approaches to calibration have been developed that explore only suitable sub-sets of the parameter space, where the sub-set can be endogenously selected via optimization procedures based on search algorithms (Nocedal and Wright 1999, is a good reference to the vast literature about this topic). Because of the typical complexity of ABMs, however, a major drawback of the application of search algorithms is that they can easily get stuck into local solutions. In other words, the solution depends on the starting point of the search. Hence, this approach may still require many simulations of the model because the researcher needs to repeat the search for different starting points. For instance, Recchioni et al. (2015) is a recent application of a gradient-based search algorithm to the calibration of a financial agent-based model requiring thousands of simulations. Another strategy adopted to avoid the complete exploration of the parameter space is to employ genetic algorithms as for instance in Fabretti (2013). Also genetic algorithms, however, are computationally demanding. A different approach to calibration that avoids to simulate a computational model (including ABMs) for the whole parameter space is based on the concept of meta-modeling. Meta-modeling, mainly used in sensitivity analysis, is the process of approximation of an unknown complicated relationship between input factors (the parameters \(\theta \)) and model output (the statistics \(S_m\)) with a simpler one of known shape (Saltelli et al. 2008, ch. 5). In this paper we will present a novel calibration method based on the global sensitivity analysis procedure proposed by Chen and Desiderio (2018, 2020), which belongs precisely to the meta-modeling approach. Our method requires the estimation of an auxiliary regression meta-model (the model of a model, also called ‘emulator’) for the statistics of interests by running the ABM only for some parameter vectors. Once estimated, the meta-model can be used to calculate approximated statistics \({\hat{S}}_m\) for the whole parameter space without further simulations of the original computational model. This approach bears strong resemblance to indirect methods and consists of four phases:  Choosing a set of statistics \(S=(S_1,S_2,...,S_q)'\) computable both on real data (\(S_r\)) and on simulated data (\(S_m\)); Choosing and estimating a meta-model \(S_m=MM(\theta )\) for the relationship between the model parameters \(\theta \) and \(S_m\); Choosing a metric \(d(\cdot )\) to measure the distance between \(S_r\) and the fitted values \({\hat{S}}_m\) of the meta-model; Choosing those model parameters that minimize the distance \(d(S_r,{\hat{S}}_m)\). The advantage of such a method is that the ABM is simulated only for a limited number of parameter vectors at point 2 in order to estimate the meta-model. Calibration procedures based on meta-modeling such as ours are therefore intended to increase the speed of the calibration process at the expenses of its accuracy, because the statistics \(S_m\) are approximated by the fitted values of the meta-model. Hence, calibration through meta-modeling makes specially sense for computationally demanding large-scale ABMs, in which case some accuracy may well be sacrificed in exchange of a gain in speed. To our knowledge, there are only three examples of calibration through meta-modeling in agent-based economics: Salle and Yildizoglu (2014), Barde and van der Hoog (2017) and Bargigli et al. (2020), basically all sharing the same methodology. The novelty introduced by our method, and also the essential difference with respect to the one used in the above-mentioned works, is its sampling strategy for the parameter vectors, which allows to eliminate the noise caused by the random numbers without resorting to Monte Carlo replications. In this way the computational burden is greatly reduced. Moreover, our sampling strategy is very simple and can in principle be applied to any other technique based on meta-modeling. Another important feature of the method is its simplicity of implementation, as it is based on simple econometric techniques that are at the average economist’s reach. The paper continues as follows: the calibration method is explained in Sect. 2, and in Sect. 3 it is applied as example to calibrate two parameters of the ABM presented in Chen and Desiderio (2018, 2020). Section 4 concludes.",5
59.0,2.0,Computational Economics,04 March 2021,https://link.springer.com/article/10.1007/s10614-021-10103-y,New DTW Windows Type for Forward- and Backward-Lookingness Examination. Application for Inflation Expectation,February 2022,Aleksandra Rutkowska,Magdalena Szyszko,,Female,Female,Unknown,Female,"This study investigates consumer inflation expectations’ forward-lookingness using a dynamic time warping (DTW) algorithm. Expectations are private agents believes regarding economic situation. Their formation and properties are the centre of interest for central banks. They are the driving force of transmission of central bank signals into economic results (Woodford 2003). During the post-crisis era of low inflation, expectations play an even more important role as the standard interest rate transmission remains ineffective. Numerous studies investigate whether the properties of expectations have changed since the Great Recession and their implications for policymakers (Ehrmann 2014; Łyziak and Mackiewicz-Łyziak 2014; Łyziak and Paloviita 2018). When non-specialists form their expectations about inflation, they may consider past inflation only or forecast inflation on the basis on numerous forward-looking factors. The notion of forward-lookingness is closely related to rationality of expectations. The rational expectation hypothesis was introduced to economics by J.F. Muth (1961). It gained recognition after a seminal papers by R.E Lucas (Lucas 1972, 1976) and brought the revolution in macroeconomics. The rational expectations hypothesis is by far the most common assumption applied in macroeconomic modelling and analysis. This remains true regardless the obvious empirical evidence that the hypothesis does not hold. The simplest description of the rational expectations hypothesis states that economic agents expectations are the same as the forecasts of the model being used to describe economy. The model reflects adequately economic system and relations. Consequently, private forecasts are, on an average, equal to realization of the variable. The intuition behind the hypothesis is far away from econometrical approach to incorporation of past values in forecasting. The rational expectations story is that economic agents, including consumers, could ignore past information about inflation and refer only to the value of future inflation. They are believed to know economic model as well as policy makers do. There is no need to stick to past inflation values while express their forecasts. The rational expectations are thus fully forward-looking – focused on the future, and equal, on an average, to actual inflation realization. Backward-looking expectations remains in opposition to rationality: they stick to past inflation. Information content of expectations:- forward- or backward-looking is a primary concern of our study. Having in mind that that description holds some simplification, we refer to the former approach as to forward-looking expectations and we call the latter—backward-looking. Unlike existing literature, our study is firstly methodological. We propose an alternative method for assessing the degree of expectations’ forward-lookingness. Our search for a novel solution is motivated by the shortcomings of the existing approaches. The standard procedure estimates hybrid specification of expectations following economic theory and intuition about the information content of private forecasts. However, its application can be questioned due to the properties characteristics of the time series (expectations are quite often non-stationary) and the results’ robustness (they are estimator-dependent and react strongly to small adjustments in the research period). With the findings of previous studies, our own experience with hybrid specification estimations, and the topic’s relevance to central banks in mind, we have decided to apply an alternative approach—dynamic time warping (DTW) to assess the degree of expectations’ forward-lookingness. The DTW technique originates from speech recognition where it founds numerous applications (Itakura 1975; Myers et al. 1980; Rabiner and Juang 1993; Rabiner et al. 1978; Sakoe and Chiba 1978; Benkabou et al. 2018). In time series analysis, DTW is an non-parametric technique for measuring the similarity or distance between two temporal sequences which may vary in time or speed. DTW application in economics is rare despite its advantages: it does not impose assumptions on the time series properties or the lag structure. To our best knowledge, few examples of DTW application for economic analysis are available in the literature. DTW was used to detect recessions (Raihan 2017) and clustering of business cycles (Franses and Wiemann 2018), similarity networks among 35 currencies in international foreign exchange markets (Wang et al. 2012), and commodity prices’ co-movements (Śmiech 2015). Arribas-Gil and Müller (2014) present a pairwise dynamic time warping and show its application opportunities to online auction data. Apart from a methodological contribution to the research on expectations, our study provides an alternative understanding of forward-lookingness (FL) and backward-lookingness (BL) of expectations as DTW allows for different perspectives while searching for similarities in time series. We compare our findings using theoretical approaches to forward- and backward-looking expectations with the results using modified assumptions about horizons of information incorporated. The DTW algorithm provides both: a distance measure that is insensitive to local compression and stretches and the warping, which optimally deforms one of the two input series onto the other. DTW solves the problem of local time shifting in time series. Thus, it does not assume constancy of delays over time. This paper offers a practical solution but is not about proposing a new forecasting method. As expectations are forecasts by non-specialists, we do not wish to forecast forecasts. We would like to assess their properties in a proper way. From the policy-maker point of view, the value added arising from the recognition of expectations FL and BL is far enough for policy analysis purposes, inc. inflation modelling. It allows for determining inflation equation (new-Keynesian Phillips curve or its hybrid specification) that better replicates empirical evolution of inflation. It could be useful for calibrating parameters of inflation equation. The summary of the value added of our paper is as follows: we build up on an existing literature on the assessment of expectations properties. This methodological novelty is extended by the provision of an alternative understanding of forward- and backward-lookingness. Moreover, we offer the modification of DTW algorithm that allows for tackling this specific problem. Our sample covers consumer expectations derived from the European Business and Consumer Surveys held under the auspices of European Commission. We present DTW for seven monetary areas: Croatia, the Czech Republic, Hungary, Poland, Romania, Sweden, and the UK. This sample covers economies for whom we conducted previous examinations using standard methodology. Thus, we are able to compare the DTW results with our previous findings and that of the others. Apart from our need for comparing results, we find that the economies that we cover function within the European Union monetary policy (price stability as priority, high degree of central bank independence). The research period covers 2001 to mid-2018. The rest of the paper proceeds as follows. Section 2 presents the materials and methods. In this section we briefly outline the standard methodology that estimates the degree of expectations’ FL and the DTW technique, in detail. The next section describes the results for both versions of the algorithm and juxtaposes them with the standard estimations of FL. The last section provides the conclusions.",3
59.0,2.0,Computational Economics,08 March 2021,https://link.springer.com/article/10.1007/s10614-021-10107-8,Inferring Causal Interactions in Financial Markets Using Conditional Granger Causality Based on Quantile Regression,February 2022,Hong Cheng,Yunqing Wang,Tinggan Yang,,Unknown,Unknown,Mix,,
59.0,2.0,Computational Economics,15 March 2021,https://link.springer.com/article/10.1007/s10614-021-10100-1,Option Pricing by the Legendre Wavelets Method,February 2022,Reza Doostaki,Mohammad Mehdi Hosseini,,,Male,Unknown,Mix,,
59.0,2.0,Computational Economics,15 March 2021,https://link.springer.com/article/10.1007/s10614-021-10109-6,Numerical Simulation of Non-cooperative and Cooperative Equilibrium Solutions for a Stochastic Government Debt Stabilization Game,February 2022,Z. Nikooeinejad,M. Heydari,J. Engwerda,Unknown,Unknown,Unknown,Unknown,,
59.0,2.0,Computational Economics,16 March 2021,https://link.springer.com/article/10.1007/s10614-021-10110-z,Quantum Computing and Deep Learning Methods for GDP Growth Forecasting,February 2022,David Alaminos,M. Belén Salas,Manuel A. Fernández-Gámez,Male,Unknown,Male,Male,"Forecasting GDP growth with great precision is vitally important for policy-makers, central banks, and private economic agents since they use the macroeconomic forecasts to determine fiscal and monetary policies and plan future operating activities. On the other hand, the selection of suitable GDP growth forecasting methods has been of huge interest among researchers and academics (Kapetanios et al. 2016). In the most recent literature, some GDP growth forecasting models stand out (Carriero et al. 2019; Carriero et al. 2019; Claveria et al. 2019; Kapetanios et al. 2016; Marcellino et al. 2016; Clark and Ravazzolo 2015; Ferrara et al.  2015; Schorfheide and Song 2015). Most of the existing literature on GDP growth has been focused on developed economies, mainly the United States and Europe. The evidence is limited to emerging countries. These models have shown that vector autoregressions and Bayesian vector autoregressions are widely used in empirical macroeconomics to predict macroeconomic and financial variables (Carriero et al. 2019; Marcellino et al. 2016; Schorfheide and Son 2015). So, the existing models have used different methods for GDP growth forecasting, especially statistical methods, and some computational methods as an evolutionary algorithm have been developed (Claveria et al.  2019), but no quantum computing or deep learning methods have been applied. For this reason, the literature demands a new GDP growth forecasting model, specifically in terms of new models that provide a better fit in scenarios globally and compare different methods to achieve more accurate results (Carriero et al.  2019). Besides, although the explanatory capacity of these models is significant, they still have certain limitations related to their levels of precision and their exclusive focus on groups of developed countries. To contribute to the accuracy capability of GDP growth forecasting models, in the present study a comparison of methodologies to forecast GDP growth has been analyzed and, as a consequence new models that will generate better forecasts of behavior in the future of GDP growth. These models can predict in all countries, also achieving accuracy levels above 93 %. These models have been constructed from a sample of 70 countries (47 emerging countries and 23 developed countries). Quantum computing and deep learning methods have been applied in the construction of the GDP growth forecasting models to compare them and determine the highest accuracy model. Specifically, the quantum computing methods are Support Vector Regression Chaotic Quantum Bat Algorithm, Quantum Boltzmann Machines, and Quantum Neural Networks. For its part, the Deep Learning methods are Deep Recurrent Convolution Neural Network, Deep Belief Network, Deep Neural Decisions Trees and Deep Learning using Support Vector Machines. The Deep Neural Decisions Trees model has obtained the highest levels of precision. We make at least three further contributions to the literature. First, we consider explanatory variables for predicting GDP growth, testing the importance of these variables which have not been considered so far. It has important implications for policymakers, who will know which indicators provide reliable, accurate, and potential macroeconomic forecasting. Second, we improve the forecasting accuracy concerning that obtained in previous studies with innovative methodologies, concluding that the deep learning methods predict macroeconomic forecasting better than quantum computing methods, although the last ones have obtained very good results. Third, our study has studied GDP growth globally, and so not restricted to developed countries. It is interesting for those responsible for the economic policies of any country in the world. This study is structured as follows: Sect. 2 provides a literature review of empirical research on GDP growth forecasting. Section 3 sets out the methodology used. Section 4 provides details of the data and the variables used in the study. Finally, Sect. 5 analyses the results obtained. The article concludes by stating the conclusions of the study and its implications. The existing literature has mainly focused on predicting GDP growth in the United States (Batchelor and Dua 1992; Batchelor and Dua 1998; Stock and Watson 2002; Clements and Galvão 2008; Marcellino 2008; Clark 2011; Clark and Ravazzolo 2015; Barsoum and Stankiewicz 2015; Carriero et al. 2019). On the other hand, a large majority of studies have developed predictions of GDP growth for the euro area and in specific countries of Europe, mainly for Scandinavian economies (Bergstroöm 1995; Camba-Mendez et al. 2001; Hansson et al. 2005; Martinsen et al. 2014; Smets et al. 2014; Kapetanios et al. 2016; Marcellino  et al. 2016; Carriero et al. 2019; Claveria et al. 2019). For their part, Ferrara et al. 2015) developed GDP growth prediction models in 19 OECD countries. On the other hand, Stock and Watson (2003), and Kuzin et al. (2013) investigated the prediction of GDP growth in developed economies, specifically Canada, France, Germany, Italy, Japan, the United Kingdom, and the United States. From another point of view, taking into account the predictive variables, the most widely used indicators in the literature to forecast GDP growth have been the financial variables such as long-term interest rate on government bonds, short-term interest rate on government bonds, real effective exchange rate index, and S&P Index (Carriero et al.  2019; Carriero et al. 2019; Schorfheide and Song 2015; Smets et al. 2014; Stock and Watson 2003). Also, various studies use other financial variables, such as broad money, money supply (Kapetanios et al. 2016; Koop 2013; Stock and Watson 2003). Finally, recent literature also uses macroeconomic predictors, such as industrial production, exports and imports goods services, trade, labor force, and unemployed rate (Claveria et al. 2019; Barsoum and Stankiewicz 2015; Ferrara et al.  2015; Kuzin et al. 2013). Among them, Stock and Watson (2003) determined that predictors as interest rate, long-term and short-term interest rate on government bonds, exchange rate, money supply, and broad money contain very useful information to forecast GDP growth. Regarding the methods used, a considerable number of researchers have applied statistical methods for GDP growth forecasting, highlighting Vector autoregressions (Hansson et al.  2005; Koop 2013; Clark and Ravazzolo 2015; Ferrara et al. 2015; Schorfheide and Song 2015; Kapetanios et al. 2016; Marcellino et al. 2016; Carriero et al. 2019), and Vector autoregression with stochastic volatility (Clark 2011; Smets et al. 2014; Diebold et al. 2017; Carriero et al. 2019). On the other hand, the authors Barsoum and Stankiewicz (2015) and Kuzin et al. (2013) developed Mixed-Data Sampling (MIDAS) regression models to forecast GDP growth. Among them, Carrierio et al. (2019) and Clark (2011) conclude that adding stochastic volatility to BVARs substantially improves the real-time accuracy of density forecasts, reducing mean squared errors. In turn, these models also improve the precision of point forecasts. For their part, Barsoum and Stankiewicz (2015) concluded that MIDAS models open new possibilities for researchers to use available data of different frequencies in forecasting and to address the possible problem of delays in the publication of macroeconomic variables. Some studies have used statistical models to assess trends in the macroeconomic situation through surveys (Batchelor and Dua 1998; Martinsen, Ravazzolo and Wulfsberg, 2014). Finally, few previous studies have developed computational techniques. Claveria et al. (2019), applied evolutionary computation, a branch of artificial intelligence, implementing evolutionary algorithms, to forecast the evolution of GDP. These authors concluded that using this technique, they significantly improve the precision of macroeconomic predictions. Lastly, regarding the level of precision reached in GDP growth forecasting’ literature, the range moves in 60–70 % (Batchelor and Dua1992; Hansson et al. 2005; Clark 2011; Kuzin et al.2013; Martinsen et al. 2014; Smets et al. 2014; Ferrara et al. 2015; Schorfheide and Song 2015). With a higher level of precision range (70–80 %), we find the research by Barsoum and Stankiewicz (2015), Gambetti and Giannone (2013), Clark and Ravazzolo (2015), Kapetanios et al. (2016), Marcellino et al.  (2016), Diebold et al. (2017), Carriero et al. (2019), Claveria et al. (2019). Table 1 shows a summary of this literature. In this table we can highlight, on the one hand, the autoregression methodology, as the most used in recent previous literature and, on the other hand, the United States´prevalence as a sample of countries.
",14
59.0,2.0,Computational Economics,19 March 2021,https://link.springer.com/article/10.1007/s10614-021-10108-7,The Benefits of Fractionation in Competitive Resource Allocation,February 2022,Jonathan Lamb,Justin Grana,Nicholas O’Donoughue,Male,Male,Male,Male,"Competitive resource allocation problems are ubiquitous. The canonical example is the case of two military colonels that must allocate a limited number of troops across different battlefields. The colonel that allocates more troops to a battlefield wins the battlefield and the goal of each colonel is to win as many battlefields as possible. This now century-old abstraction of a competitive resource scenario is known as the Colonel Blotto Game (Roberson, 2006; Gross & Wagner, 1950). Despite its original militaristic framing, the Colonel Blotto game has found wide application in both the social science and engineering communities. For example, social scientists have used Blotto games to understand political campaigning strategies (Snyder, 1989), research and development investments (Golman & Page, 2009) and marketing strategies (Friedman, 1958). The engineering community has adopted Blotto games to model computer network attacks (Min et al., 2017; Labib et al., 2015), communication network formation (Shahrivar & Sundaram, 2014), infrastructure resilience (Ferdowsi et al., 2017), spectrum allocation (Chien et al., 2019) and tax policy (Crutzen & Sahuguet, 2009). Although the framing of the Blotto game is intuitive, simple perturbations to the basic game often pose significant analytical challenges. For example, games where players have different numbers of resources (Roberson, 2006) or the battlefields have different values (Thomas, 2018) have been solved only relatively recently. One important perturbation that has received scant attention is the case where players have different types of resources. This perturbation of the Blotto game is as ubiquitous as the standard formalism. For example, military commanders must allocate different resources such as people and equipment, firms must decide how to allocate different types of employees to different tasks, advertisers must allocate different product placement mechanisms (commercials and endorsements, for example) over time and space, and political campaigners must determine how to allocate funds, volunteers and managers. Despite its wide prevalence, there has yet to be a systematic study of such a resource allocation scenario. To address this gap, we analyze what has been called a multi-resource Blotto game (Behnezhad et al., 2017). In the game, the players allocate platforms across a set of battlefields. Each platform has a set of capabilities. Generally, a platform may have more than one unit of each capability. Each unit of a capability carried by a platform is called a resource. The winner of a battlefield is a function of the joint allocation of resources and capabilities to a battlefield and the players’ goal is to win as many battlefields as possible. Consider, for example, the scenario where two aerial combat colonels must allocate their resources among three different battlefields. One colonel has a fleet of ten large aircraft, each with five missiles. Another colonel has 50 small autonomous aerial vehicles (drones), with one missile per drone. The winner of the battlefield is the colonel that allocates more total missiles to the battlefield. In this case, the platforms are the aircraft, missiles are the capability, and missile capacity is the number of resources for each platform. However, the multi-resource Blotto game is more general than simply capturing aggregation of resources on platforms. For a more complex example, consider two colonels that now must allocate their resources between two theaters. Specifically, each colonel must allocate ten battleships and fifteen airplanes. Each battleship has five missiles and is equipped with radar. Each airplane has two missiles and is likewise equipped with radar, but in addition carries a camera to conduct surveillance. In this case the theaters are the battlefields. Battleships and airplanes are platforms. The capabilities are missiles, radar and camera. The resources for each battleship are five missiles and one radar while the resources for each airplane are two missiles, one radar and one camera. The winner of the theater is some function of the joint allocation of missiles, radars, and cameras. For example, the function might be “the player that allocates more missiles to a theater, provided they allocate at least two radars and one camera, is the winner.” Applications of the multi-resource Blotto games are not confined to the military domain. For example, consider two political parties that are lobbying for and against a particular bill in several counties. Each party seeks to have their side of the bill supported in as many counties as possible. To sway voters, the party allocates orators, pamphlet distributors and journalists among the counties. However, each “platform” (orator, pamphlet distributors and journalists) affects voters in each county differently and the winner of each county is some function of the joint allocation of orators, pamphlets and journalists. In yet another example to further reinforce the importance of multi-resource Blotto games, consider two pharmaceutical companies that are competing to develop new drugs. The two companies are investing to either find a cure for baldness or a cure for halitosis. Both companies have equipment, scientists and lab technicians to allocate between researching the two conditions. Scientists can design three experiments per year and can execute one experiment per year. Technicians can assist in two experiments per year, provided there is a scientist leading the experiment. Equipment is used to carry out experiments where any piece of equipment can carry out either three baldness experiments or two halitosis experiments per year. This is a case where each side has three different types of resources that can be allocated to different experiments in search for a cure. The “target” is interpreted as “finding a cure” and the winner of the targets is a stochastic function of the allocation of scientists, technicians and equipment. Payoff functions in multi-resource Blotto games need not be homogenous or symmetric across players. Strategic competition between great powers could be described as a multi-resource Blotto game where two nations seek advantage across multiple geographies in different adversarial spheres such as economic, cultural, political, and military dominance. To gain regional advantage, nations develop influence programs that act on one or more spheres of activity. Each region may be relatively more susceptible to specific kinds of influence or combinations of influences, or have different strategic value to each player. In this work, we use the general multi-resource Blotto game to determine the potential benefits of having resources spread across many platforms versus having resources aggregated on fewer platforms. Specifically, we examine the equilibrium payoffs as a function of the platforms’ degree of fractionation. We “overload” the term fractionation to take on different meanings depending on the context. In the case where each force only has one capability, we define fractionation as the number of resources per platform (the number of missiles per aircraft, in the example above). In the case of multiple capabilities, we define the degree of fractionation as the number of number of unique capabilities per platform. We refer to this second type of fractionation as fractionation of heterogenous resources (FHR). As shorthand, we refer to the force that is more fractionated (fewer resources and capabilities per platform) as the fractionated force and the force that is less fractionated as the aggregated force. We vary the key parameters—the number of battlefields and total number of resources—to fully characterize the potential benefit of having a fractionated force. To limit the scope, we examine how the most fractionated force performs against varying types of aggregated forces. Specifically, we first consider the case where there is only one capability and determine how a force with one resource per platform (fractionated) performs against a force with several resources per platform (aggregated). We augment the model to include the case where platforms are imperfect and subject to error. We then analyze the potential benefits of FHR in scenarios with multiple capabilities. That is, we examine whether it is better to have several different capabilities aggregated onto one platform or several platforms each with a unique capability. Finally, we analyze under what (if any) fixed and variable costs the adoption of a fractionated force is justifiable. Of course, we do not cover the complete parameter space of the very general multi-resource Blotto game. For example, we do not consider the case where forces have mixed degrees of fractionation. In reference to the aerial example, this means we don’t consider the case where forces have large aircraft and drones. Furthermore, we do not consider the case where both forces’ platforms have multiple capabilities but a different number of resources per platform. Again, in reference to the aerial example, this means we do not consider the case of a force that has platforms with three missiles and three cameras per platform against a force that has ten missiles and ten cameras per platform. While these are interesting extensions, our goal is to specifically disentangle the benefits of fractionation and therefore, we simplify the experiments in a way that best highlights the impacts of fractionation and eliminates other confounding factors. The enabler of this work is the recently developed algorithm for solving multi-resource Blotto games (Behnezhad et al., 2017). The key to the algorithm is to transform the zero sum game into a flow problem on a layered graph. The authors show that with this representation, the game can be solved with \(O(N^{2c}K)\) constraints where N is the number of platforms, c is the number of different platform types and K is the number of battlefields. This allows us to solve for equilibrium payoffs in games where players have on the order of 50 platforms and up to nine battlefields. Thus our numerical results cover enough of the parameter space to draw robust conclusions regarding the benefits of fractionation. In addition to the vast literature on Blotto games and its variants (Roberson, 2006, 2010; Gross & Wagner, 1950; Roberson & Kvasov, 2012; Hart, 2008; Schwartz et al., 2014; Rinott et al., 2012) our work is related to the work on multi-activity contests (Arbatskaya & Mialon, 2010, 2012). In multi-activity contests, players contribute costly resources in order to win a contest. The winner of the contest is a stochastic function of the players’ joint contributions. While this is similar to the multi-resource Blotto game, it has two main differences. First, and most crucially, multi-activity contests often only have one battlefield and therefore players do not have to allocate resources across different spaces. Secondly, multi-activity contests usually assume players’ contributions are continuous, which precludes an analysis of fractionation since continuous pure strategy spaces are infinitely divisible for both players.",1
59.0,2.0,Computational Economics,30 March 2021,https://link.springer.com/article/10.1007/s10614-021-10104-x,Cap and Trade Versus Carbon Tax: An Analysis Based on a CGE Model,February 2022,Jin-Feng Zhou,Dan Wu,Wei Chen,,Male,,Mix,,
59.0,2.0,Computational Economics,03 April 2021,https://link.springer.com/article/10.1007/s10614-021-10115-8,A New Strategy for Short-Term Stock Investment Using Bayesian Approach,February 2022,Tai Vo-Van,Ha Che-Ngoc,Thao Nguyen-Trang,,,,Mix,,
59.0,3.0,Computational Economics,04 May 2021,https://link.springer.com/article/10.1007/s10614-020-10087-1,A Wiener–Kolmogorov Filter for Seasonal Adjustment and the Cholesky Decomposition of a Toeplitz Matrix,March 2022,D. Stephen G. Pollock,Emi Mise,,Unknown,Female,Unknown,Female,"This paper introduces the SEADOS computer program for the seasonal adjustment of economic data. The program allows the user to specify the seasonal-adjustment filters in view of the periodogram of a de-trended data sequence. The periodogram enables an assessment to be made of the range of the frequencies of the elements that contribute to the seasonal fluctuations and which might be eliminated from the data. Therefore, the program provides more flexibility than does a progam in which the procedures follow a fixed prescription or in which they are determined by the estimated parameters of a statistical model. Both of the latter approaches have been followed by central statistical offices. In the past, the predominant methods within statistical agencies have been those of the venerable X-11 procedure of Shiskin et al. (1967) and its derivatives. These methods prescribe a collection of moving-average filters that are attributable to Henderson (1916, 1924). The X-11 program has been fully documented in a monograph of Ladiray and Quenneville (2001). Recently, model-based methods have gained favour. These are represented, primarily, by the highly competent TRAMO–SEATS program of Augustin Maravall—see Gómez and Maravall (2001) and Caporello and Maravall (2004). This program follows the prescriptions of Hillmer and Tiao (1982) regarding the canonical decomposition of time series affected by seasonal and cyclical variations. The model-based methods are in accordance with a dominant opinion amongst economists that economic investigations should be conducted within the context of well-defined models of economic activities that are to be estimated from the available data. A problem that affects the prevalent methods of seasonal adjustment is that they nullify completely only the elements at the seasonal frequency and its harmonics. The seasonal fluctuations may comprise elements at adjacent frequencies that also need to be removed from the data. A testimony to this problem has been provided by McElroy and Roy (2017), who have described a way of detecting residual seasonal effects in seasonally adjusted data. The issue has also been addressed by Findley et al. (2005). The problem can arise in consequence of a variety of data anomalies that affect the regularity of the seasonal fluctuations. These include calendar effects, holidays, strikes and other untoward events. Methods for dealing with such irregularities by adjusting the data directly have been described, recently, by Attal-Toubert et al. (2018) and by Ladiray (2018). One way of eliminating a wider band of elements in the vicinities of the seasonal frequencies, which is described in this paper, is to create offset filters that are targeted at frequencies on either side of the seasonal frequencies. Then, such filters can be applied in series with a central filter that is targeted at the seasonal frequencies. In the next section of the paper, a model is provided for a trended economic data sequence affected by seasonal fluctuations. The model gives rise to a Wiener–Komogorov filter, described as the basic filter, that is aimed at extracting the principal seasonal elements from a de-trended version of the data sequence. The section that follows is devoted to the derivation of a practical finite-sample version of this filter. The filter employs an algorithm for the Cholesky decomposition of large matrix with a small number of centralized diagonal bands. The algorithm is presented in Sect. 4, together with its Pascal code. Some additional algorithms that are required in implementing the filter are provided in Sect. 5. The effects of the basic filter, as represented by its frequency response function, are described in Sect. 6, where a variant of the filter, aimed at estimating a trend-cycle function, is also provided. It is shown that the basic filter and its variant are capable of closely mimicking the filters of a conventional model-based method of seasonal adjustment. In Sect. 7, an empirical comparison is made of the effects of the basic filter and those of a triple filter that encompasses a range of frequencies adjacent to the seasonal frequencies. In the concluding section, which follows, questions are raised concerning the appropriate definitions of the seasonal component and of the seasonally adjusted data.",
59.0,3.0,Computational Economics,05 June 2021,https://link.springer.com/article/10.1007/s10614-021-10112-x,A Bootstrap Method to Test Granger-Causality in the Frequency Domain,March 2022,Matteo Farnè,Angela Montanari,,Male,Female,Unknown,Mix,,
59.0,3.0,Computational Economics,15 April 2021,https://link.springer.com/article/10.1007/s10614-021-10113-w,"A Comparative Analysis of Parsimonious Yield Curve Models with Focus on the Nelson-Siegel, Svensson and Bliss Versions",March 2022,Ranik Raaen Wahlstrøm,Florentina Paraschiv,Michael Schürle,Unknown,Female,Male,Mix,,
59.0,3.0,Computational Economics,11 April 2021,https://link.springer.com/article/10.1007/s10614-021-10114-9,\(\ell _{1}\) Common Trend Filtering,March 2022,Hiroshi Yamada,Ruoyi Bao,,Male,Unknown,Unknown,Male,"The \(\ell _{1}\) trend filtering, which was developed by Steidl et al. (2006), Steidl (2006), Kim et al. (2009), Tibshirani (2014), and Guntuboyina et al. (2020), enables us to extract a continuous piecewise linear trend of univariate time series.Footnote 1 Figure 1 illustrates a continuous piecewise linear trend. The filter and its variants have been subsequently applied in various fields, including astronomy (Politsch et al. 2020), climatology (Khodadadi and McDonald 2019), economics (Yamada and Jin 2013; Yamada and Yoon 2014; Winkelried 2016; Yamada 2017; Klein 2018), electronics (Suo et al. 2019), environmental science (Brantley et al. 2019), finance (Mitra and Rohit 2018), and geophysics (Wu et al. 2018). The \(\ell _{1}\) trend filtering is defined by replacing the squared \(\ell _{2}\)-norm penalty of the Hodrick–Prescott (HP) (1997) filtering with the \(\ell _{1}\)-norm penalty.Footnote 2 It is notable that, even though the modification seems to be somewhat minor, the \(\ell _{1}\) trend filtering provides a continuous piecewise linear trend, whereas the HP filtering provides a smooth trend. In econometrics, such a continuous piecewise linear trend was dealt with by Perron (1989) and Rappoport and Reichlin (1989) and it reflects the idea that ‘economic events that have large permanent effects are relatively rare’ (Hamilton 1994). Thus, it is possible to say that the \(\ell _{1}\) trend filtering is a method to obtain the trend considered by Perron (1989) and Rappoport and Reichlin (1989). Although the \(\ell _{1}\) trend filtering can estimate a continuous piecewise linear trend of univariate time series, it cannot estimate a common continuous piecewise linear trend of multiple time series. In this paper, we develop a statistical procedure that enables us to estimate it, which is a multivariate extension of the \(\ell _{1}\) trend filtering. To explain more precisely, let \(y_{i,t}\) be an observation of a univariate time series i at t, where \(i=1,\ldots ,n\) and \(t=1,\ldots ,T\), and suppose that it has a continuous piecewise linear trend \(x_{i,t}\). As stated, the \(\ell _{1}\) trend filtering can be applied for estimating \(x_{i,t}\) from \(y_{i,t}\). In this paper, we consider the situation such that \(x_{i,t}\) can be expressed as where \(x_{t}\) is a continuous piecewise linear trend and \(a_{i}\) is a loading coefficient. Given that (1) can be represented as even though \(y_{1,t},\ldots ,y_{n,t}\) commonly have \(x_{t}\), their linear combination \(\beta _{1}y_{1,t}+\cdots +\beta _{n}y_{n,t}\) no longer has \(x_{t}\) if \([\beta _{1},\ldots ,\beta _{n}]'\) is a vector that belongs to the orthogonal complement of the space spanned by \([a_{1},\ldots ,a_{n}]'\). Hatanaka and Yamada (2003) referred to it as ‘co-trending.’Footnote 3 In this paper, by extending the \(\ell _{1}\) trend filtering, we develop a novel method to estimate \(x_{t}\) and \(a_{i}\) from \(y_{i,t}\). Recall that \(i=1,\ldots ,n\) and \(t=1,\ldots ,T\), where n (resp. T) represents the number of univariate time series (resp. observations). We refer to the novel filtering method as ‘\(\ell _{1}\) common trend filtering.’ We provide an algorithm for estimating this and a clue to specify the tuning parameter of the procedure, both of which are required for its application. We also (i) numerically illustrate how well the algorithm works, (ii) provide an empirical illustration, and (iii) introduce a generalization of our novel method. Here, we remark that (2) is not an unlikely model of trends in macroeconomic time series but has strong relevance. To explain more precisely, let \(y_{1,t},\ldots ,y_{n,t}\) be macroeconomic time series in natural logarithms and \(e_{1,t},\ldots ,e_{n,t}\) be such that Let \(g_{i,t}=\Delta y_{i,t}(=y_{i,t}-y_{i,t-1})\). Accordingly, \(g_{i,t}\) for \(i=1,\ldots ,n\) represent the growth rates of the original time series. Then, (2) and (3) are equivalent to with initial conditions such as \(y_{i,1}=a_{i}x_{1}+e_{i,1}\) and \(\Delta y_{i,2}=a_{i}\Delta x_{2}+\Delta e_{i,2}\), where \(b_{t}=\Delta x_{t}-\Delta x_{t-1}\) and \(v_{i,t}=\Delta e_{i,t}-\Delta e_{i,t-1}\). Recall that \(\Delta g_{i,t}\) in (4) denotes the difference of growth rates of variable i at t. Given that \(x_{t}\) in (2) is a continuous piecewise linear trend, only a few of \(b_{3},\ldots ,b_{T}\) are not equal to 0. We may regard such nonzero \(b_{t}\)s in (4) as occasional permanent shocks that shift growth rates of multiple time series simultaneously and \(a_{i}\) for \(i=1,\ldots ,n\) in (4) represent individual reaction coefficients of the time series. A typical example of such occasional permanent shocks is the oil price shock in 1973. It is natural to consider that, at the time, the growth rates of macroeconomic time series changed simultaneously with their own reaction rates. This paper is organized as follows. Section 2 introduces the novel filtering method and provides its reduced-rank-regression (RRR) representations. Section 3 discusses a numerical computation method for \(x_{t}\) and \(a_{i}\) in (1). Section 4 provides a clue to specify the tuning parameter of the procedure required for its application. Section 5 numerically illustrates how well the novel statistical procedure works. Section 6 includes an empirical illustration. Section 7 mentions a generalization of our method. Section 8 concludes the paper. Notations Let \({\varvec{y}}_{i}=[y_{i,1},\ldots ,y_{i,T}]'\), \({\varvec{x}}_{i}=[x_{i,1},\ldots ,x_{i,T}]'\), \({\varvec{x}}=[x_{1},\ldots ,x_{T}]'\), \({\varvec{Y}}=[{\varvec{y}}_{1},\ldots ,{\varvec{y}}_{n}]\in \mathbb {R}^{T\times n}\), \({\varvec{I}}_{m}\) is the identity matrix of order m, \({\varvec{J}}=[{\varvec{0}},{\varvec{I}}_{T-2}]\in \mathbb {R}^{(T-2)\times T}\), \({\varvec{a}}=[a_{1},\ldots ,a_{n}]'\), and \({\varvec{D}}\in \mathbb {R}^{(T-2)\times T}\) be the second order difference matrix such that \({\varvec{D}}{\varvec{x}}_{i}=[\Delta ^{2}x_{i,3},\ldots ,\Delta ^{2}x_{i,T}]'\). Explicitly, \({\varvec{D}}\) is the \((T-2)\times T\) Toeplitz matrix of which the first and last rows are \([1,-2,1,0,\ldots ,0]\) and \([0,\ldots ,0,1,-2,1]\), respectively. In addition, let Finally, for a vector \({\varvec{\gamma }}=[\gamma _{1},\ldots ,\gamma _{m}]'\), \(\Vert {\varvec{\gamma }}\Vert _{2}^{2}={\varvec{\gamma }}'{\varvec{\gamma }}=\sum _{t=1}^{m}\gamma _{t}^{2}\), \(\Vert {\varvec{\gamma }}\Vert _{1}=\sum _{t=1}^{m}|\gamma _{t}|\), \(\Vert {\varvec{\gamma }}\Vert _{\infty }=\max \{|\gamma _{1}|,\ldots ,|\gamma _{m}|\}\), and, for a matrix \({\varvec{\Gamma }}\in \mathbb {R}^{r\times s}\) whose (i, j) entry is denoted by \(\gamma _{ij}\), \(\Vert {\varvec{\Gamma }}\Vert _{\mathrm {F}}^{2}=\sum _{i=1}^{r}\sum _{j=1}^{s}\gamma _{ij}^{2}\). A small note (i) The null space \({\varvec{D}}\) is identical to the column space of \({\varvec{\Pi }}\) and accordingly \({\varvec{D}}{\varvec{\Pi }}={\varvec{0}}\), (ii) \({\varvec{\Psi }}\) is a right inverse of \({\varvec{D}}\), i.e., \({\varvec{D}}{\varvec{\Psi }}={\varvec{I}}_{T-2}\) (Paige and Trindade 2010), (iii) \({\mathsf {det}}({\varvec{X}})=1\) and thus \({\varvec{X}}\) is nonsingular, and (iv) given (1), we have \([{\varvec{x}}_{1},\ldots ,{\varvec{x}}_{n}]=[a_{1}{\varvec{x}},\ldots ,a_{n}{\varvec{x}}]={\varvec{x}}{\varvec{a}}'\in \mathbb {R}^{T\times n}\).",2
59.0,3.0,Computational Economics,26 April 2021,https://link.springer.com/article/10.1007/s10614-021-10116-7,Method for Improving the Performance of Technical Analysis Indicators By Neural Network Models,March 2022,Yong Shi,Bo Li,Wei Dai,,Male,,Mix,,
59.0,3.0,Computational Economics,01 May 2021,https://link.springer.com/article/10.1007/s10614-021-10117-6,Analytically Pricing European Options under a New Two-Factor Heston Model with Regime Switching,March 2022,Sha Lin,Xin-Jiang He,,,,Unknown,Mix,,
59.0,3.0,Computational Economics,28 April 2021,https://link.springer.com/article/10.1007/s10614-021-10120-x,The Dynamic Volatility Connectedness Structure of Energy Futures and Global Financial Markets: Evidence From a Novel Time–Frequency Domain Approach,March 2022,Ehsan Bagheri,Seyed Babak Ebrahimi,Stelios Bekiros,Male,Male,Male,Male,"Globalization has a significant impact on different aspects of human lives. Nowadays, countries have more economic relations than in the past. An example of this phenomenon is more integration among financial markets; this is true not only for developed countries but also for the financial markets of developing countries. However, more connectedness can encourage vulnerability because of spillover risks of other markets. The 2008 financial crisis, which began with the United States and quickly propagated to other financial markets, led to severe financial, economic, and social repercussions around the world. In many studies, fluctuations of energy prices have been regarded as a source of shock to the economy of countries and financial markets (Kilian, 2008). Also, there are various indications of the impact of financial markets on shocks caused by energy price changes (Ghosh et al., 2020; McCarthy & Orlov, 2012; Shahzad, Arreola-Hernandez, et al., 2018; Shahzad, Hernandez, et al., 2018; Wang & Wang, 2019). Due to the importance of identifying market risks and the dangers that could threaten the macroeconomy of a country, many researchers and policymakers have measured the interconnections between markets and examined the intensity of their interdependence. Besides, many investors are interested in markets that are less subject to spillover risks of other markets in order to reduce their risks, particularly during a crisis. The primary aim of this article is to measure the connectedness of the energy futures markets and other financial markets. For measuring connectedness, different methodologies can be adopted. To this end, we use frequency connectedness introduced by Barunik and Krehlik (2018) which measures the magnitude and direction of connectedness over time and different frequencies. This paper provides new empirical evidence to two distinct research areas. Firstly, we measure frequency connectedness of energy futures markets and other financial markets to be familiarized with the nature of the connectedness of energy futures and different markets. It is good to known whether they have short-term, medium-term or long-term connectedness. In addition, we improve Barunik and Krehlik methodology using HVAR in order to mitigate the problems of high dimensionality and to enhance the accuracy of our findings. To the best of our knowledge, it is the first study measuring time–frequency volatility connectedness among energy futures, major international stock markets, and currencies concurrently via HVAR. We use volatility that illustrates the flow of information in favor of traders who are seeking concrete hedging methods. The findings of this article can help policymakers, portfolio managers, and investors who want to hedge risks, and other researchers interested in the relationships between different markets, especially the energy futures markets. The rest of this paper proceeds as follows: Sect. 2 reviews the related literature. In Sect. 3, we present the empirical methodology. In Sect. 4, the most significant empirical results and the findings are discussed. Ultimately, Sect. 5 concludes the paper.",4
59.0,3.0,Computational Economics,26 April 2021,https://link.springer.com/article/10.1007/s10614-021-10121-w,A Mellin Transform Approach to the Pricing of Options with Default Risk,March 2022,Sun-Yong Choi,Sotheara Veng,Ji-Hun Yoon,Unknown,Unknown,Male,Male,"In general, derivatives traded on stock exchanges are considered to be no default risk. However, several derivatives such as currency options, options on precious metals, interest rate swaps, or credit default swaps, still have been traded in over-the-counter(OTC) market where there is no the exchange or clearing corporation. Therefore, many buyers of the related derivatives are vulnerable to the default risk. In other words, the option holder is always apt to have counterparty credit risks since the option writer of the counterparty may not fulfil the appointed contracts at the maturity. So, we need to take account of the default risk of the contingent claim for pricing the options on the defaultable instruments in such OTC market. the option is called a ‘vulnerable option’ when the option buyer is exposed to the option writer’s default risk which is an example of a counterparty risk. Financial crises such as the Global Financial Crisis, Eurozone crisis and COVID-19 pandemic have brought the issue of counterparty risk to academic researchers and practitioners. We also note that counterparty risks are an important risk factor recognized by Basel III. Especially, the recent economic crisis caused by the COVID-19 pandemic has increased the importance of the default risk on the derivatives. According to BIS report,Footnote 1 the gross market value of OTC derivatives rose from $11.6 trillion to $15.5 trillion during the first half of 2020. Correspondingly, gross credit exposures reached $3.2 trillion at end-June 2020, which is the largest increase since 2009. The largest growth indicates that many holders of the option contracts should be seriously aware of the default possibilities. The pricing of European vulnerable options has been already proposed by Johnson and Stulz (1987). Since then, several works have been reported by researchers on the pricing of vulnerable options. For example, Klein (1996) derived a formula for European vulnerable option when a proportion of nominal claim is paid out in default depending on the amount of other liabilities as well as the assets of the counterparty. In that study, the value of the asset of the counterparty is assumed to follow a geometric Brownian motion. Hung and Liu (2005) obtained a vulnerable option pricing formula under an incomplete market condition. The advantage of the formula is that it can evaluate nontraded assets underlying vulnerable options. Yang et al. (2014) assumed that volatility of the asset value follows a fast mean-reverting Ornstein-Ulenbeck (OU) process and derived an analytic formula for European vulnerable option. Yoon and Kim (2015) derived an explicit closed form formula of European vulnerable option with stochastic interest rates following the Hull-White interest rate process. In this study, we evaluate the price of vulnerable options based on the stochastic of elasticity of variance (SEV) model introduced by Kim et al. (2014). The SEV model forecasts market’s volatility more accurately than the constant elasticity of variance (CEV) model so that investors can employ a dynamic investment strategy reducing the risk more effectively. In particular, it is a useful model for extraordinary volatility behavior which would take place in a financial crisis. Refer to Kim et al. (2015). The model consists of two separate scales in such a way that there are the following important merits. The first one is that the number of necessary parameters for calibration is minimized while the model well reflects the essential characteristics of the underlying asset and the firm value of the option writer. The second one is that singular perturbation technique can be implemented to extract essential information for the properties of the option price. The SEV model has been used to the pricing of other types of derivatives. For example, Yoon and Park (2016) and Yoon et al. (2013) evaluated turbo warrant options and perpetual American options, respectively. It could be hybridized with a pure stochastic volatility model such as the Heston model (1993) as done by Choi et al. (2016). A Mellin transform is an integral transform that can be thought of as the multiplicative version of the two-sided Laplace transform. The Mellin transform can be a very useful tool for the transformation of partial differential equations (PDEs) as described in Brychkov (1992). In general, the Mellin transform technique, if it is plausible for the pricing of a given option, would not require the complexity of the calculation as appeared in the probabilistic approaches. The Mellin transform technique has been used for pricing financial derivatives in Panini and Srivastav (2004), Panini and Srivastav (2005), Brychkov (1992), Frontczak and Schöbel (2010), Frontczak (2013), Jeon et al. (2016), Jeon et al. (2017) and Guardasoni et al. (2020). In this study, we first employ singular perturbation technique to obtain PDEs for the vulnerable option price and then exploit the double Mellin transform introduced by Krapivsky and Ben-Naim (1994) to derive an analytic solution for each PDE. The double Mellin transform provides an explicit closed form formula. Model calibration in financial engineering is as important as the model itself. Calibration is equivalent to identifying the parameter values so that the model can reproduce market prices as accurately as possible. The speed as well as the accuracy of calibration is crucial because practitioners use the calibrated parameters to price complicated derivatives and to work out trading strategies. A main contribution of this paper is that our option price formula is given in a relatively simple closed form explicitly. It is the Black–Scholes price plus correction terms that can be calculated by simply taking derivatives of the Black–Scholes formula. In this sense, our pricing formula is better than the well-known formula under the Heston model introduced by Heston (1993), one of the most popular stochastic volatility models for pricing derivatives, which is given analytically but requires a numerical method to compute the complex integral. Actually, the semi-analytical pricing formula for the European vanilla options by Heston model obtained by the use of Fouier transform techniques comes up with the instability and a great deal of computing time in obtaining the solutions, including the problem of the model calibrations. So, the existence of the closed solution for the option pricing is very important because it directly affects the computing time in implementing the option’s data fitting (or the calibration of option’s model) and the numerical stability related to the pricing accuracy. This paper shows that the SEV model with the Mellin transform leads to a better analytical solution for pricing derivatives than the Heston model with the Fourier transform in terms of the stability (or the accuracy) and the computing time. The structure of the paper is organized as follows. Section 2 gives a brief review of the SEV model. In Sect. 3, we apply multiscale asymptotic analysis to constructing an explicit approximation of the European vulnerable option price. The explicit analytic solution is given by the Melling transform technique in Sect. 4. Section 5 is committed to investigating the sensitivity of the vulnerable option value with regard to the model parameters and verifying the accuracy and efficiency of the approximate formula. Section 6 provides concluding remarks.",
59.0,3.0,Computational Economics,10 June 2021,https://link.springer.com/article/10.1007/s10614-021-10122-9,Credit Scoring Model Based on HMM/Baum-Welch Method,March 2022,Badreddine Benyacoub,Souad ElBernoussi,Mohamed Ouzineb,Male,Female,Male,Mix,,
59.0,3.0,Computational Economics,23 June 2021,https://link.springer.com/article/10.1007/s10614-021-10123-8,Inaccurate Value at Risk Estimations: Bad Modeling or Inappropriate Data?,March 2022,Evangelos Vasileiou,,,Male,Unknown,Unknown,Male,"The role of the financial system is to allocate resources from lenders/investors to borrowers/enterprises in an efficient way in order to maximize welfare for all (lenders/investors, borrowers/enterprises, and society). A significant aspect of this flow of funds is how the financial system allows risk to be shared and who bears it (Allen et al. 2004). Therefore, the protection of lenders/investors is vital to the stability of the financial system and largely relies on the ability of lenders/investors to accurately assess the financial risk in their transactions/decisions. For these reasons, the Committee of European Securities Regulators (CESR) introduced a set of regulations (CESR 2010) which focus on financial markets, and the Basel Committee on Banking Supervision (BCBS) issued recommendations (Basel Accords I, II, and III) which focus on the banking industry. The dominant statistical measure for estimating financial risk is the Value at Risk (VaR). The VaR of an investment is an estimation of the loss that will not be exceeded, with a given significance level, at a specific timeframe. It is a percentile, usually 1% or 5%,Footnote 1 of a profit/loss distribution which presents some differences/variances depending on the model the researcher/risk analyst uses, e.g. when the Historical VaR model is applied, VaR is the percentile of the actual x-day past returns; under the Delta Normal VaR model, it is the percentile of the normalized distribution of the last x-day returns (Jorion, 2007). The first widely known and complete VaR system was Risk Metrics (Longerstaey & Spencer, 1996). The need for financial risk information and an increased interest by policy makers on financial risk issues and VaR, in combination with advances in financial econometrics and computer science over the last decades, have led to significant advances in VaR modeling: Monte Carlo (Berkowitz et al. 2011), the popular GARCH family models (Angelidis et al. 2004; Degiannakis et al. 2012; Engle, 2004), and the Markov Switching Regime models (Billio & Pelizzon, 2000). However, some contemporary suggestions, such as Fuzzy VaR, Expected Shortfall models with elliptical distributions (Moussa et al. 2014, and Extreme Learning Machine (Zhang et al. 2017), are too complex to be implemented (explained) in the financial industry by (to) non-mathematicians and computer scientists. The oxymoron is that despite strict legislation and significant advances in financial econometrics, several crises have emerged throughout the world in the last decades: USA savings and loan crisis (1989–1991), Asian financial crisis (1997–1998), Russian financial crisis (1998), Argentine crisis (1999–2002), Global financial crisis (2007–2008), European Sovereign Debt Crisis (2010-today) etc. Advanced VaR models show increased accuracy in forecasting risk. So, why are there so many crises? A reason could be that, in practice, the most popular models applied in the financial industry are the conventional and simplest VaR models: the Historical, the Variance–covariance (or Delta Normal), and the Monte Carlo Simulation.Footnote 2 Conventional models generate low VaR estimations during the days before a crisis and high VaR estimations when a crisis has already emerged (Vasileiou & Pantos, 2020; Vasileiou & Samitas, 2020).Footnote 3 However, the purpose of VaR should be to timely and accurately inform market participants that a stress period is approaching. To offer an explanation as to why the advantages of financial econometrics have not been adequately utilized in the industry, some scholars suggest that wider use of advanced models in the financial industry is hindered by the complexity and the increased cost that such advanced systems involve (Vasileiou, 2016). Eminent scholars highlight the mathematical complexity issue in their work: Fama (1995) notes that in some cases contemporary models are extremely complex for non-mathematicians, and Ross (1993, p. 11) characteristically describes such a case using the phrase: “If you torture the data long enough, it will confess to any crime”. Therefore, this study’s main objective is to suggest a methodology that could bridge this theory–practice gap and improve the accuracy of VaR estimations without the need for mathematical complexity. Our goal is to provide investors with accurate, representative, and easy to understand and analyze VaR estimations. This way, investors will be adequately informed of the risk they bear when they invest in a financial asset. This will help address a long-standing problem in financial markets and promote financial stability. In order to achieve our goal, we do not use complex mathematics in the output stage, but we filter the data in the input stage and select the most appropriate data set. In some way, we try to listen to what the data want to say to us, as opposed to torturing the data. We tested our assumptions using the easiest to apply and communicate VaR model, the Delta Normal (or Variance–Covariance) model (Jorion, 2007). We examined the world’s most significant stock market, the US stock market and particularly the S&P500 Index, for the period 2000–2020. During this period, several crises emerged: (a) the dot.com crisis in early 2000, (b) the crisis of 2007–2008, and (c) the crisis of the COVID-19 pandemic in 2020. The empirical results confirm our assumptions: the data filtering procedure significantly improves the accuracy of Conventional Delta Normal VaR (CDNVaR) estimations without the use of complex mathematics. The rest of the paper goes as follows: Sect. 2 analyses the motivation for this study, Sect. 3 presents the theoretical framework of the newly suggested estimation procedure, and Sect. 4 provides empirical evidence of the accuracy of the CDNVaR model and the backtesting results under the current legislation. Section 5 presents the theoretical framework of the new approach/model and empirically shows that the new model is not only significantly more accurate than the CDNVaR models, but also that its estimations are comparable to advanced models such as the widely applied GARCH(1,1) model. Section 6 concludes the study.",2
59.0,3.0,Computational Economics,03 September 2021,https://link.springer.com/article/10.1007/s10614-021-10182-x,Correction to: Inaccurate Value at Risk Estimations: Bad Modeling or Inappropriate Data?,March 2022,Evangelos Vasileiou,,,Male,Unknown,Unknown,Male,"Unfortunately the author’s given name and family name were interchanged in the electronic version of this article. The article was updated and the name is now correctly displayed in the order given name followed by family name, i.e. Evangelos Vasileiou. The original article has been corrected.",
59.0,3.0,Computational Economics,12 July 2021,https://link.springer.com/article/10.1007/s10614-021-10124-7,Best Subset Selection for Double-Threshold-Variable Autoregressive Moving-Average Models: The Bayesian Approach,March 2022,Xiaobing Zheng,Kun Liang,Dabin Zhang,Unknown,,Unknown,Mix,,
59.0,3.0,Computational Economics,22 May 2021,https://link.springer.com/article/10.1007/s10614-021-10125-6,Dependence and Systemic Risk Analysis Between S&P 500 Index and Sector Indexes: A Conditional Value-at-Risk Approach,March 2022,Shoukun Jiao,Wuyi Ye,,Unknown,Unknown,Unknown,Unknown,,
59.0,3.0,Computational Economics,26 May 2021,https://link.springer.com/article/10.1007/s10614-021-10126-5,Corporate Bankruptcy Prediction Using Machine Learning Methodologies with a Focus on Sequential Data,March 2022,Hyeongjun Kim,Hoon Cho,Doojin Ryu,Unknown,,Unknown,Mix,,
59.0,3.0,Computational Economics,13 May 2021,https://link.springer.com/article/10.1007/s10614-021-10129-2,High Frequency and Dynamic Pairs Trading with Ant Colony Optimization,March 2022,José Cerda,Nicolás Rojas-Morales,Werner Kristjanpoller,Male,Male,Male,Male,"The quantitative trading strategy known as Pairs Trading has become increasingly popular since the mid-1980s. This strategy has been widely used by traders and hedge funds to improve investment performance. Previously, the algorithms based on this strategy were not generally accessible to researchers or the public due to the proprietary nature of the area (e.g., High Frequency Data). The result of the inaccessibility was a limited amount of published research in the field. However, a surge of interest in the subject has driven increased research attention (Khandani and Lo 2011 and Huck 2010). Following this increase in research interest, studies in this area have been performed empirically and have studied High Frequency Trading indirectly via observed trading behavior (Cartea and Penalva 2011). Krauss et al. (2017) cautiously stated that the gap between academic finance and the financial industry is progressively evolving. In this study, we use the method of statistical arbitrage, which is defined as an investment strategy that exploits mathematical models to generate returns from systematic movements in the prices of securities. An important aspect of statistical arbitrage is obtaining positive and superior results to the market via a neutral transaction ledger (Pole 2007). As shown by Miao (2014), statistical arbitrage can be used to identify when a gap arises from inefficiencies in the market. Pairs Trading is widely assumed, in the literature, to be the ”ancestor” of statistical arbitrage. Krauss (2017) explained that although academic research of this strategy is still small compared to research focusing on other strategies (e.g., Contrarian and Momentum strategies), Pairs Trading research is gaining momentum in five research areas: Distance, Cointegration, Time-series, Stochastic control and a bucket of approaches that he labels ’other approaches’. Three categories within the ’other approaches’ bucket include: Machine Learning and Combined Forecasts; Copula; and, Principal Components Analysis (PCA) (Krauss 2017). Two articles of note within the context of Machine Learning and Combined Forecasts respectively are Huck (2009) and Huck (2010). These studies propose combinations of Machine Learning (ML) and the Combined Forecasts with three stages: Forecasting; Outranking; and, Trading. In the Forecasting stage, Neural Networks are applied to develop the forecast. In the stages of Rank and Trading, a Multi-Criteria Decision Method (MCDM) called ELECTRE III is used to improve performance. Another method applied in this context is the Copula Approach that, according to Krauss (2017), can be classified as either: Return-Based Copula Method or Level-Based Copula Method. These approaches have been applied in studies that analyze High Frequency Forex data (Chu 2018). With respect to the third group of the so-called other approaches, Avellaneda and Lee (2010) proposed the use of Principal Components Analysis (PCA) to create eigenportfolios and make them part of a multifactor model from which their residual is modeled as an Ornstein-Uhlenbeck process that is then used to generate a buy-sell signal. Once the signals are obtained, the buy-sell decision is determined by comparing it to fixed thresholds that are determined empirically. To maximize the long-term benefit, it is necessary to correctly set the thresholds. For example, if the values of the thresholds are too similar, less time will be required to complete the operation and it will be less beneficial than if the values of the thresholds were more different (Zeng and Lee 2014) . High Frequency Trading (HFT) is a type of algorithmic trading, meaning that it uses technological tools to automate trading decisions. The trading decisions are characterized by the relationship between the speed and profit of each decision (Cartea and Penalva 2011). One option for applying HFT is in the Forex, the most liquid market in the world, with a turnover of $5.1 trillion Dollars per day in 2016 and $6.6 in 2019 according to Triennial Central Bank Survey (Bank for International Settlements (2019)), with 24/5 operations and a large amount of leverage. The volume of activity, time of operations, and leverage allows traders to respond almost immediately to fluctuations in the foreign exchange market (Forex 2016 and Investopedia 2018). However, the leverage not only accentuates the characteristic liquidity of this market, but also increased risk (Investopedia 2018). In finance, there are a lot of studies and applications that use exact methods. Because solving NP-hard problems traditionally require too much computing time, there has been a surge of research involving metaheuristics. This field of study provides efficient solutions that are close to optimal and offer reasonable computing times. This makes the use of these algorithmic methods reasonable since many financial decisions must be made in a very short time frame, such as minutes or seconds (Dorigo and Blum 2005). In this study, the methodology presented by Avellaneda and Lee (2010) is applied to investment decision-making in the Forex market via High Frequency Ant Colony Optimization. This algorithm is a nature-inspired metaheuristic used to solve hard Combinatorial Optimization (CO) problems (Soler-Dominguez et al. 2017) and is used in this study to find the optimal thresholds of the Pairs Trading dynamically, in terms of decisions performed every 15 min with low compute time. The objective is to obtain better precision and performance than the base case of fixed thresholds. In the current study, we expand on Avellaneda and Lee (2010) who applied their approach in the US equities market by investigating a context with greater daily trading volume over longer daily trading periods since the Forex trades 24-h whereas the US equities market is open for only 8-h. The novelty offered by the approach used here is that of the Ant Colony Optimization to optimize Pairs Trading thresholds. To the best of our knowledge, this study is unique for its use of Pairs Trading in the Forex market via High Frequency Trading as well as a PCA approach. Finally, a metaheuristic algorithm is proposed to better address the investment strategy based on Pairs Trading given the high frequency of the data to which it is applied, i.e., Ant Colony Optimization of Pairs Trading (ACO-PT), which was inspired by ACO-FRS (Fernández-Vargas and Bonilla-Petriciolet 2014). This study improves the use of Ant Colony Optimization metaheuristics when applied to a trading strategy and also contributes to the expansion of the literature regarding High Frequency Trading algorithms. The results of this work demonstrate that ACO-PT can be efficiently used in deep markets. The remainder of this paper is organized as follows: In Sect. 2, the methodology is described in detail. In Sect. 3, our approach is presented. In Sect. 4, the data used in the experiments is explained. In Sect. 5, the empirical results are presented. Finally, in Sect. 6, the conclusions of the study are presented and ideas for future studies are also given.",2
59.0,4.0,Computational Economics,20 April 2022,https://link.springer.com/article/10.1007/s10614-022-10260-8,Deep Learning for Financial Engineering,April 2022,Mu-Yen Chen,Arun Kumar Sangaiah,Erol Egrioglu,Unknown,Male,Male,Male,"Financial operations are generally related to huge amounts of cash flow with risks and uncertainties attracting much research efforts for the development of sophisticated quantitative models to manage these financial risks. “Financial engineering” is a term coined with the help of modern information technologies. Financial engineering is a cross-disciplinary field for analysts to optimize and analyze various kinds of financial decision making such as risk management, financial portfolio planning, forecasting, trading, hedging, fraud detection, and other applications. Today, the field of financial engineering has successfully integrated a wide range of quantitative analysis disciplines, such as mathematics, statistics, time series, stochastic process, data mining, and artificial intelligence. More and more evidence is indicating that the financial environment is not ruled by mathematical distributions or statistical models. In the field of computer science, attempts have been made to develop financial engineering models using soft computing approaches in order to build up more flexible financial engineering models. Bio-Inspired computing is a way of developing computer systems by taking ideas from the biological world. Many of the ideas taken from natural processes have been applied to machine learning and deep learning, leading to new developments in artificial intelligence. The pursuit of Bio-Inspired computing technology is a recent trend in which technologies, including artificial immune systems, particle swarms, ant colony, bacterial foraging, artificial bees, harmony search, nano computing, multi-objective, dynamic, and large-scale optimization are applied to daily life. The Bio-Inspired computing could be realized and construct simple systems which are able to evolve into more complex ones.",6
59.0,4.0,Computational Economics,22 January 2022,https://link.springer.com/article/10.1007/s10614-021-10213-7,National Governance Differences and Foreign Bank Performance in Asian Countries: The Role of Bank Competition,April 2022,Sheng-Hung Chen,Feng-Jui Hsu,,Unknown,Unknown,Unknown,Unknown,,
59.0,4.0,Computational Economics,04 October 2021,https://link.springer.com/article/10.1007/s10614-021-10192-9,Enterprise Intelligent Audit Model by Using Deep Learning Approach,April 2022,Rui Ding,,,Male,Unknown,Unknown,Male,"The main purpose of auditing is to extract, analyze and study the financial data of the audited entity, so as to supervise the accuracy, rationality and efficiency of the financial revenue and expenditure of the entity (Hamawandy et al., 2021). Currently, there is increasingly fierce competition among large enterprises and high operation risk of enterprises. It requires internal audit to mine meaningful data information for enterprises from multiple perspectives, so that enterprises can better cope with the competitive pressure from external enterprises (Saeed et al., 2020). The objective of an internal audit is not only to correct the defects and predict risks, but also to promote the operation and management. The data information output by the audited enterprise are compared with the internal and external enterprise data, such as the historical data of the enterprise and the data of industries. The results can help the enterprise managers to grasp the current competitive position of the audited enterprise in time, understand the differences between themselves and competitors, and make better decisions and plans based on internal audit data (Al-Tarawneh et al., 2020; Yang, 2020). The development of science and technology makes it easy to find financial fraud in the environment of information and big data. Various financial problems are not easy to be discovered in massive data. The traditional audit technology based on sample extraction cannot find these problems or meet the accuracy requirements of audit work in the current environment (Zhang et al., 2021). It is essential to optimize auditors’ thinking mode and change the audit mode to adapt to the changes of audit in the big data environment and make audit play its role. A new intelligent audit technology for computer audit is created as the gradual development of artificial intelligence (AI) technology. The use of AI technology to analyze and study the massive data in the audit project can help auditors quickly grasp the overall situation of the audited enterprise, and discover the connections and rules contained in the audit data. Auditors can perceive the unusual economic business according to the above clues, and predict the future development of the audited enterprise based on the output data (Belanche et al., 2019; Hu et al., 2021). The deep learning technology in the field of AI adopts the hierarchical structure of neural network, including several hierarchical networks composed of input layer, multiple hidden layers and output layer. Its characteristics are self-learning and data training (Sun et al., 2020). In deep learning, the feature of the initial signal is changed layer by layer in the hidden layer, and the change process is not supervised by human beings. The feature of the object in the original space is removed to another feature space, and the hierarchical feature is automatically and intelligently learned to facilitate better visual analysis of features (Bjornson & Giselsson, 2020; Jin et al., 2021). The brand-new research results have been acquired in deep learning, especially in the field of image recognition. These new results are also useful to the text and image data of big data and financial sharing services. Moreover, the information scale in the financial sharing service center is large, which is also compatible with the data scale of the deep learning network (Ly et al., 2019; Son et al., 2020). Therefore, through the classification, processing and storage analysis of enterprise intelligent audit data, an automatic encoder using data compression algorithm is proposed according to the deep learning theory to analyze the audit data. The main research contribution is to apply genetic algorithm to the weight optimization of deep learning network. Enterprise intelligent audit is conducted through the horizontal and vertical comparison of enterprise financial information dataset. It suggests that the enterprise intelligent audit model proposed under the financial sharing mode has practical reference value for the data mode transformation of enterprise intelligent audit.",3
59.0,4.0,Computational Economics,20 November 2020,https://link.springer.com/article/10.1007/s10614-020-10073-7,A New Bootstrapped Hybrid Artificial Neural Network Approach for Time Series Forecasting,April 2022,Erol Eğrioğlu,Robert Fildes,,Male,Male,Unknown,Male,"Artificial neural networks (ANNs) can be used to obtain forecasts for linear or non-linear time series. Many types of artificial neural networks have been proposed in the literature. The findings of studies about the performance of ANNs for forecasting purpose vary from study to study. There is no consensus about the reasons behind the success or failure of ANNs performance on forecasting problem. In early research, Gorr et al. (1994) stated that ANN can (1) automatically transform and represent complex and highly non-linear relationships and (2) automatically detect different states of phenomena through independently variable data patterns and switch on/off model components as appropriate. Besides these good properties, Gorr et al. (1994) emphasised that ANNs have several limitations, mostly noticeable in ‘explanation research’ (causal modelling and hypothesis testing) but not when used in forecasting. This occurs because ANN models are non-linear in the model coefficients and the normal probability models are not applicable. As a result of this, they do not have parametric statistical properties based on the t and F distributions. In this study, these problems are focused on and a new method proposed to solve them. Additional to these problems, the mean square error function used as the loss function in estimating the weights in ANNs is multi-modal, so the optimization algorithms suffer from the local optimum traps. The outputs of optimization methods are therefore not stable. The problem has been partially alleviated by using an artificial bee colony algorithm as an artificial intelligence optimization technique. In time series analysis, it is expected that a forecasting method provides forecasts, prediction intervals for forecasts, and hypothesis tests such as input significance, linearity and nonlinearity. These are important aspects of modelling to provide as simple a model as possible to conform to the data. Many studies have not considered input significance tests, model selection or model adequacy. Researchers have focused more on point estimations in ANNs. In an ANN approach, determining inputs, the number of hidden layer nodes, activation function types and network architecture affect network performance. Moreover, inputs to the networks should influence the outputs. Determining relevant inputs have usually been identified by trial and error or from theoretical information about the data from the literature. The alternative is to develop statistical hypothesis tests for an ANN to determine input variables and appropriate functional forms to include. This viewpoint is supported by Anders and Korn (1999) who suggested that statistical analysis as described below should become an integral part of neural network modelling. (The terms given in parenthesis corresponding the meaning in the statistics literature.) Input significance test This test is needed to see which inputs are relevant to produce an output or outputs in ANN (Variable Selection) Non-linearity test This test is needed to decide where to apply an ANN to the data, otherwise a linear alternative modelling method should be used. Architecture tests These tests are needed to establish if the network has a linear part or an additional non-linear part. These tests provide evidence that using the ANN architecture proposed has the potential to be useful in forecasting (Model Selection, e.g. RESET tests) Weights significance tests These tests are needed for pruning the ANN, eliminating unnecessary hidden connections (Parameter Significance Tests). 
Applying these tests is no easy task. Moody (1994) and Moody and Utans (1994) developed input selection and architecture selection approaches. Researchers faced some important problems for proposing these tests. Anders and Korn (1999) wanted to carry out parameter inference in a neural network based on an asymptotic normal distribution but they emphasise that the parameters of ANN are at least locally unique. To guarantee this, it is necessary to ensure that a given network model contains no irrelevant hidden units. One solution in the literature is to use model selection criteria to determine network architecture, the inputs and number of hidden nodes. Anders and Korn (1999) stated that criteria are not theoretically justified for over-parameterized networks, e.g. networks with irrelevant hidden units, even if the neural network model encompasses the true structure. Anders and Korn (1999) proposed strategies based on Terasvirta et al. (1993) using hypothesis tests and network information criteria but their method still suffered from the aforementioned problems. To summarize Refenes and Zapranis (1999) stated that the following situations cause unwanted results in ANNs. The omission of relevant variables as inputs in the ANN. Inclusion of irrelevant variables employed in the ANN Measurement errors in inputs and targets Incorrect specification of the architecture Inadequacies of the model selection and training algorithm, trapping local optimums 
Refenes and Zapranis (1999) emphasised that consistent estimators can be obtained from ANNs if they satisfy the properties of convergence and uniqueness. This can be achieved by making a good decision for determining inputs, and the architectures. The second problem requires estimating standard error for the parameters in the ANN. It is not easy to obtain theoretically a formula for the standard errors in ANN. In discussing the second problem Zapranis and References (1999) proposed using a local bootstrap technique to make hypothesis tests in an ANN but because of the presence of local minima and the sensitivity of the training algorithm to initial conditions, resampling schemes tend to overestimate sampling variation. Their approach used derivate based training algorithm and this algorithm can be easily trapped in local optima. White (1989), Lee et al. (1993), Terasvirta et al. (1993) have proposed a hypothesis test method for the nonlinearity of time series. These studies only focused on a multilayer perceptron. Yolcu et al. (2019) proposed linearity and non-linearity hypothesis test methods by using particular ANN type and forecasting accuracy was improved by using bootstrap methods, suggesting a potential route forward. Bootstrap methods can be used to develop hypothesis tests in ANNs if we use an efficient learning algorithm by avoiding local optimum traps. Hypothesis tests and other statistical inferences can be made easily for non-linear of non-parametric models by using bootstrap methods as they delivering distributional estimates of components of the ANN. They have been employed to forecasting methods, for example, Masaratto (1990) discussed bootstrap confidence intervals for an autoregressive model and a residual-based approach was employed in the study. Lam and Veall (2002) compared analytic and bootstrap prediction intervals and they found that bootstrap prediction intervals performed better in Monte Carlo experiments. Dantas et al. (2018) proposed a new forecasting method based on bootstrap aggregation. They combined clustering and bagging in exponential smoothing methods. They found that their method outperforms many methods in the literature for M3 and CIF competition data sets. Bootstrap methods also used for artificial neural networks in the literature. Tiwari and Chatterjee (2010a, b) papers use a bootstrap method to improve the forecasting accuracy of MLP. Kourentzes et al. (2014) proposed an ensemble operator for bootstrap approaches in ANNs. They proved that their operator is better than the mean ensemble operator. Barrow and Crone (2016) proposed a “crogging method” for ANNs: The method is very similar to bagging. In this study, the results on real and simulated series demonstrated significant improvements in forecasting accuracy especially for short time series and long forecast horizons. Politis and Dimitirs (2016) obtained interval forecasts from ANNs by using bootstrap approaches. Yolcu et al. (2017) obtained confidence intervals for forecasts with an SMNM-ANN. Szafranek (2019) proposed bagged artificial neural network method for forecasting inflation data set. Yolcu et al. (2019) proposed linearity and non-linearity hypothesis test methods by using special ANN type and they improved forecasting accuracy by using bootstrap methods. In summary, ANNs has been shown to produce good forecasting performance for some type of time series but ANNs cannot automatically produce statistical distributional results for forecasts and model coefficients. In contrast, statistical results can be obtained from other linear or non-linear statistical forecasting methods and this is a deficiency of ANNs but ANNs have better forecasting accuracy for some nonlinear time series in the literature. Most recently, the results of the M4 competition (Makridakis et al. 2018) compared accuracy on some methods and the two winning methods used hybrid combinations of ML methods with statistical models. Such comparisons aim to provide researchers and practitioners: Fildes (2020) commented on the M4 competition results that “the results certainly should guide the short-list” of methods to consider. Out-of-the-box, ML methods did not perform well. As a result of these findings, proposing new ANN methods can be useful for forecasters if they provide statistical distributional results and more accurate forecasts. The main focus of this study is providing this kind of ANN approach. In this study, a new hybrid artificial neural network is proposed. New methods for testing input significance, linearity and non-linearity in this new ANN are proposed. Hypothesis tests are realized by using the residual bootstrap method to take care of time series serial dependency. Moreover, the forecasting accuracy is improved by using bootstrap methods in the new ANN. The new ANN is trained by an artificial bee colony algorithm for avoiding local optimum traps. The proposed method provides the following advantages for users: Empirical distributions and confidence intervals for forecasts Empirical distributions and confidence intervals for weights of ANN A method to test linearity and nonlinearity A method to test input significance More accurate and confidential forecasts 
In the second section, a new hybrid artificial neural network is introduced. In the third section, the training algorithm of the HANN is introduced. In the fourth section, the bootstrapped HANN method is introduced. Moreover, input significance, linearity and nonlinearity hypothesis test methods are introduced in this section. In the fifth section, the performance of the proposed method is investigated by using stock exchange data sets and M4 yearly competition data set. In section six, the obtained results are discussed. The proposed method is shown to perform well on both data sets compared to benchmark approaches.",11
59.0,4.0,Computational Economics,24 February 2021,https://link.springer.com/article/10.1007/s10614-021-10101-0,The Use of Machine Learning Combined with Data Mining Technology in Financial Risk Prevention,April 2022,Bo Gao,,,Male,Unknown,Unknown,Male,"At present, with the integration of global economy, the capital of all kinds of enterprises is active in the capital market, which leads to extremely fierce market competition (Yu 2017). In 2008, the U.S. subprime mortgage crisis triggered global financial market turbulence. Throughout the crisis, although the root cause is from the real estate industry, especially the weak supervision of subprime mortgage, it reflects the lack of financial risk management of many enterprises (Nguyen 2018). From 2009 to 2011, the credit rating of Greece and Italy in Europe was lowered, which caused a serious debt crisis in European society (Bekiros et al. 2020). As a country with a large financial volume, China fully seized the opportunity and led the comprehensive recovery of the real economy to achieve catch-up and surpass in the development (Ha et al. 2016). It is of great significance for China to study and establish its own financial early warning system, whether from the perspective of the supervision and management of national regulatory departments and financial management institutions at all levels, or from the interests of financial markets at all levels, listed companies and their creditors and debtors (Yang et al. 2020). A complete set of enterprise financial risk evaluation index system has been established to objectively evaluate the financial risk status of the enterprise as a whole, which has become an important means of enterprise risk management and control, and has gradually developed into various fields of market economy (Li et al. 2019). The establishment of enterprise risk prevention and control system is conducive to national macroeconomic decision-making, enterprise loan financing, and the improvement and development of existing financial risk management theory (Oláh et al. 2019). Therefore, the study of enterprise financial risk prevention has great practical value. The financial risk prevention model based on the traditional statistical analysis method often contains a series of assumptions, and the amount of calculation is large (Li et al. 2020). With the continuous development of artificial intelligence (AI) and data mining technology, more and more big data information is imperceptibly affecting people's lives (Vinuesa et al. 2020). The financial risk of an enterprise is often accompanied by a great change of an indicator. Therefore, many scholars have studied how to explore the rules among indicators and find out the key factor indicators, so as to distinguish the different financial crisis stages of enterprises (Sosnovska and Zhytar 2018). Among them, Khemakhem and Boujelbene (2018) built an enterprise risk prediction model by using artificial neural network and decision tree method. The study shows that profitability, repayment ability, solvency, credit reporting duration, guarantee, company size, loan quantity, ownership structure and the duration of corporate banking relationship are the key factors to predict default (Khemakhem and Boujelbene 2018). Khemakhem et al. (2018) established the relationship between enterprise finance and risk by means of AI and data statistics. It is found that the combination of AI and statistical technology is very promising for default rate management and can provide accurate credit risk assessment (Khemakhem et al. 2018). Kim et al. (2020) evaluated the operation of enterprises by using deep belief network and designed a suitable network framework. The results show that the method can analyze and process enterprise data well, and the performance of risk prediction is good (Kim et al. 2020). By using decision tree, support vector machine and Bayesian classification method, Vaghfi and Darabi (2019) found the direct relationship between financial risk and inflation as well as the negative impact of non-exercise ratio, stock yield and operating cash flow ratio on financial distress. To sum up, deep learning and data mining have applied to enterprise financial risk in many researches. However, some problems are still outstanding, such as the risk system studied is not complete and the indicators are not detailed enough. Therefore, under the background of AI, deep learning technology and data mining method are combined and applied to the financial risk analysis of listed companies, so as to propose an analysis method of financial risk prevention based on interactive mining. This model is used. On the one hand, the method of testing data preprocessing is of great significance to improve the accuracy of data mining, which can increase the credibility of model prediction. On the other hand, it can well predict the future financial risk of the enterprise, and give reasonable suggestions according to the prediction results of the model. The innovation of this exploration lies in the following aspects. (1) Based on the problem of enterprise financial risk, an interactive mining method based on association rules is established. This method is based on the theory of enterprise financial risk analysis, some index parameters which are closely related to the enterprise financial risk are established, a specific discussion around their correlation is carried out, and the risk analysis model is determined, which is not available in the related research. (2) Through the combination of deep learning and data mining technology, the empirical research of enterprise financial risk analysis is realized. Moreover, through the selection of samples and indexes and the reconstruction of database, the financial risk of enterprises is analyzed, and the relevant early warning scheme is given. The rest of the research is organized as follows. In the method part, first, the principle of back propagation neural network (BPNN) and data mining technology is introduced. Then, according to the principle of analytical hierarchy process, the indicators of financial risk are analyzed. According to the above indicators and weight distribution, the financial risk assessment model is constructed. Then, the methods of data collection and analysis, model test and simulation are listed to further determine the effectiveness of the model.",8
59.0,4.0,Computational Economics,17 April 2021,https://link.springer.com/article/10.1007/s10614-021-10118-5,Intelligent FinTech Data Mining by Advanced Deep Learning Approaches,April 2022,Shian-Chang Huang,Cheng-Feng Wu,Meng-Chen Lin,Unknown,,Unknown,Mix,,
59.0,4.0,Computational Economics,09 January 2021,https://link.springer.com/article/10.1007/s10614-020-10088-0,Predicting Business Risks of Commercial Banks Based on BP-GA Optimized Model,April 2022,Qilun Li,Zhaoyi Xu,Jiacheng Zhong,Unknown,Unknown,Unknown,Unknown,,
59.0,4.0,Computational Economics,17 August 2021,https://link.springer.com/article/10.1007/s10614-021-10180-z,Innovative Risk Early Warning Model under Data Mining Approach in Risk Assessment of Internet Credit Finance,April 2022,Min Lin,,,,Unknown,Unknown,Mix,,
59.0,4.0,Computational Economics,07 January 2021,https://link.springer.com/article/10.1007/s10614-020-10079-1,Financial Sequence Prediction Based on Swarm Intelligence Algorithms of Internet of Things,April 2022,Jinquan Liu,Yupin Wei,Hongzhen Xu,Unknown,Unknown,Unknown,Unknown,,
59.0,4.0,Computational Economics,16 February 2022,https://link.springer.com/article/10.1007/s10614-021-10229-z,Analysis of Internet Financial Risks Based on Deep Learning and BP Neural Network,April 2022,Zixian Liu,Guansan Du,Han Ji,Unknown,Unknown,,Mix,,
59.0,4.0,Computational Economics,05 August 2021,https://link.springer.com/article/10.1007/s10614-021-10172-z,Analysis of Early Warning of RMB Exchange Rate Fluctuation and Value at Risk Measurement Based on Deep Learning,April 2022,Chunyi Lu,Zhuoqi Teng,Yuantao Fang,Unknown,Unknown,Unknown,Unknown,,
59.0,4.0,Computational Economics,05 November 2020,https://link.springer.com/article/10.1007/s10614-020-10069-3,A Computational Model to Predict Consumer Behaviour During COVID-19 Pandemic,April 2022,Fatemeh Safara,,,Female,Unknown,Unknown,Female,"Coronavirus disease (COVID-19) is a severe respiratory syndrome, which first recognized in Wuhan, China in December 2019 (Kucharski et al. 2020). The initial symptoms of COVID-19 are dry caught, fever, and tiredness which are common for a number of respiratory diseases. Then, aches and pains, sore throat, loss of smell and taste would be presented. In the next stages of the disease, serious symptoms such as pneumonia and sever respiratory diseases and even heart failure would be occurred. Several research papers are published on COVID-19 from different views such as the transmission speed, drug and vaccine production, general health and psychological problems, however, consumer behaviour has received less attention. With the advent of internet, knowledge-based economies such as online shopping was emerged long time ago. Machine learning methods could be used to extract implicit knowledge from the online shopping sites’ logs (Ahmadi 2020). Industries and businesses use the knowledge to better understand the consumer behavior, and opportunities and threats correspondingly. However, with the outbreak of COVID-19, online shopping attracted more attention. We have to stay at home and observe self-Isolation and social distancing because the transmission speed of the disease is very high. In particular, for elderly and people with background disease such as respiratory diseases, heart (Ivanov 2020) conditions, hypertension, diabetes, multiple sclerosis (MS), and cancer, we have to pay special attention and provide specific facilities. These conditions considerably affect the consumers shopping behavior. Nowadays, big projects such as Siri Apple, Eco Amazon, Google, Facebook, and Microsoft employ machine learning techniques to automatically analyze their consumers shopping behavior and provide appropriate service for them (Wu et al. 2019). However, using machine learning methods is not restricted to projects with huge research budgets. Small and Medium Enterprises (SMEs) could benefit from these methods to improve their efficiency as well. Beside the spread of COVID-19, different important reasons could be presented to show the appropriateness of machine learning methods for SMEs (Zeng et al. 2019). For example, considering urban traffic management issues, using online tools for online shopping and online marketing is on the rise (McDonald et al. 2019). In addition, online catalogs helps price and quality comparison (Ballestar et al. 2019). The required time for buying a product could be reduced considerably as well. Recognizing the consumer behaviors and providing proper service in a very short time is an important issue in any e-commerce application (Souri et al. 2019). Sometimes users try to buy a product, however, they cannot add the selected product to their shopping cart or they have problem with paying for it. Therefore, consumers leave the website without buying. This is an important issue for many e-commerce retailers (Vanneschi et al. 2018). In a successful e-commerce application, understanding consumer behavior requires identifying reasons that encourage consumers to buy from the websites, however, identifying consumer behavior and encouraging factors is difficult (Agrawal et al. 2018). Machine learning techniques use data analytic tools to discover behavioral patterns of consumers (Patil and Rao 2019). Increasing consumer satisfaction with online shopping, and improving the accuracy of consumer need prediction have always been the main challenges in e-commerce. Machine learning and data mining methods such as classification, clustering, association rule mining, and pattern recognition techniques have been effectively employed in previous researches for predicting influential factors on e-commerce (Khodabandehlou and Rahman 2017). Problems raised with COVID-19 added the importance of paying attention to consumer satisfaction during online shopping. Therefore, a model to predict consumer behaviour in online shopping is proposed using machine learning techniques. Contributions of the method proposed could be summarized as follows: A predictive model is presented to anticipate consumer behavior in online shopping in an e-commerce environment in COVID-19 era. The accuracy of predicting consumer behaviors in online shopping is increased through Bagging and Boosting of the classifiers. 
The rest of the paper is organized as follows: Sect. 2 provides related works published about the prediction of customer behavior. Section 3 presents the proposed method for predicting consumer behavior. Section 4 provides experimental data, experimental setup and results. Finally, Sect. 5 concludes the paper.",21
59.0,4.0,Computational Economics,03 June 2021,https://link.springer.com/article/10.1007/s10614-021-10132-7,A Novel ARMA Type Possibilistic Fuzzy Forecasting Functions Based on Grey-Wolf Optimizer (ARMA-PFFs),April 2022,Nihat Tak,,,Male,Unknown,Unknown,Male,"Time series forecasting is a commonly studied tool that has many applications throughout the various fields of economics. There is therefore a great deal of effort to improve the predictive power of extant models. Whereas exponential smoothing (ES) Gardner (1985) and autoregressive integrated moving average (ARIMA) Box et al. (2015) are two commonly used probabilistic models, they have strict assumptions about the time series, namely, that data are stationarity. This poses a hurdle, as the time series in economics generally do not satisfy the assumptions. Alternative methods have thus been proposed to address this issue in the literature. Artificial intelligence and fuzzy sets based forecasting methods are some of the more commonly studied topics able to handle time series that do not satisfy the assumptions. Fuzzy sets have been frequently used to predict time series over the last decades. Researchers have proposed a number of methods based on different points of view of fuzzy sets, with fuzzy time series (FTS) and fuzzy inference systems (FIS) being two widely used forecasting techniques. FTS was first introduced and used to forecast the enrollment of a university by Song and Chissom (1993a) and (1994). FTS models were later extended and detailed by Song and Chissom (1993b). Chen (1996) also improved the accuracy of forecasts when historical data were not accurate in his study. Integrating problem-specific heuristic knowledge with Chen’s model, Huarng (2001) proposed heuristic models. The forecasting ability of these early studies on FTS attracted the attention of researchers to this field. Consequently, FTSs have gained application as powerful forecasting tools in a wide range of fields. Recently, parsimonious FTS was proposed by Gao and Duru (2020). Chen et al. (2019) introduced an FTS model based on proportions of intervals to improve the prediction ability of FTSs. Another recent study involving an improved fuzzy functions based FTS model was introduced by Vovan and Lethithu (2020). Several studies have shown FTS models to be powerful instruments that are widely used for time series prediction problems. Defining fuzzy systems and linguistic terms, Zadeh (1973) introduced yet another powerful tool for time series prediction, the fuzzy inference systems (FISs). FISs commence with the fuzzifying of crisp numbers of inputs, after which the application from the precedent to the consequent continues as the fluctuating operator (AND / OR). Then, the results are aggregated according to a rule established by an expert opinion. Finally, FISs end with defuzzification of the outputs. In short, FISs take inputs and process them to produce outputs based on predefined rules. Owing to its performance in predicting and easy-to-use structure, FISs have been used in many fields. Takagi-Sugeno type FISs Takagi and Sugeno (1985), Mamdani-Assilian type FISs Mamdani and Assilian (1975), and adaptive neuro fuzzy inference systems (ANFIS) Jang (1993) are several well-known and widely used FISs. Still, the main challenge for FISs is to define the rules. Though ANFIS was proposed as a rule-based inference system, it later began to adapt the fuzzy c-means clustering approach to define the rules. Thus, ANFISs have been widely used in recent studies to predict time series. Sarıca et al. (2018) proposed an ANFIS model employing an autoregressive model. A Dynamic Neurons based neuro fuzzy inference system was introduced by Samanta et al. (2019) for time series forecasting. Deo et al. (2019) used ANFIS to forecast sub-tropical photosynthetically active radiation. Azad et al. (2019) proposed intelligent algorithms to improve ANFIS performance in precipitation modelling. In recent years, greater attention has been paid to T1FFs, which were proposed by Türkşen (2008) as a non-rule based fuzzy inference system for classification and regression problems. The idea behind T1FFs was to include a new variable that quantifies the impact of all variables’ observations in the input matrix. In adding such a variable, the concept of enhancing the predictive accuracy expanded the use of T1FFs. A significant amount of research has demonstrated that studies based on T1FFs (Baykasoğlu and Maral 2014; Türkşen 2009; Çelikyılmaz and Türkşen 2007) have successful outcomes. The aforementioned advantages of T1FFs have led researchers to use T1FFs for time series prediction. Seeking to improve the forecasting accuracy of T1FFs, Beyhan and Alci (2010) incorporated an ARX structure in T1FFs. However, they were unable to find the best model for their proposed method. Later, Aladag et al. (2014) developed a forecast model able to be used to determine the best model using the autoregressive (AR) model. To increase the prediction performance of T1FFs, Bas et al. (2020) adapted intuitionistic FCM instead of FCM. Another study that ignores noises was proposed by Tak (2020b), who employed possibilistic FCM in T1FFs. Bas et al. (2019) used ridge estimates instead of least square estimates in T1FFs to overcome the issue of multicollinearity. In their study, Kizilaslan et al. (2020) improved the prediction ability of T1FFs based on ridge regression and intuitionistic FCM. Another study employing intuitionistic FCM based on hesitation margin in T1FFs was proposed by Yolcu et al. (2020). Recurrent fuzzy functions in which the coefficients of the model were estimated using particle swarm optimization were introduced that also employed an moving average model Tak et al. (2018). Eventually, they succeeded in showing that their proposed method was able to produce more promising results. Goudarzi et al. (2016) proposed an interactively recurrent fuzzy functions with a multi objective learning benchmark for chaotic time series prediction. Tak (2020c) employed intuitionistic FCM in recurrent T1FFs to improve the calculation time and forecasting ability of his early work Tak et al. (2018). The literature on T1FFs has shown that T1FFs have gained more attention as a result of their rule-free characteristic and easy-to-use structure. In our study, we propose a novel way of modifying T1FFs with autoregressive moving average model (ARMA), possibilistic FCM and grey wolf optimizer (GWO) Mirjalili et al. (2014) to ignore coincident cluster centers and to minimize the non-derivative objective function. Previous studies have shown GWO to be a promising tool in dealing with time series prediction benchmarks. Several forecasting benchmarks in which GWO adapted have recently been proposed by Tak (2020c), Ma et al. (2019), Koc et al. (2019), Dehghani et al. (2019), Tak (2020a). The outstanding performance of GWO has led us to employ GWO in the proposed method to obtain model coefficients. The proposed ARMA type possibilistic fuzzy forecasting functions (ARMA-PFFs) and the corresponding pseudo code will be introduced in Sect. 2. Numerous stock indices are evaluated to reveal the prediction accuracy of the proposed ARMA-PFFs in Sect. 3. Finally, Sect. 4 covers the conclusions and arguments of the proposed method.",1
59.0,4.0,Computational Economics,20 June 2021,https://link.springer.com/article/10.1007/s10614-021-10136-3,Adaptive Trading System of Assets for International Cooperation in Agricultural Finance Based on Neural Network,April 2022,Guangji Tong,Zhiwei Yin,,Unknown,Unknown,Unknown,Unknown,,
59.0,4.0,Computational Economics,30 August 2021,https://link.springer.com/article/10.1007/s10614-021-10143-4,Financial Performance Analysis with the Fuzzy COPRAS and Entropy-COPRAS Approaches,April 2022,Yüksel Akay Ünvan,Cansu Ergenç,,,Female,Unknown,Mix,,
59.0,4.0,Computational Economics,29 January 2021,https://link.springer.com/article/10.1007/s10614-020-10090-6,Using Machine Learning Approach to Evaluate the Excessive Financialization Risks of Trading Enterprises,April 2022,Zhennan Wu,,,Unknown,Unknown,Unknown,Unknown,,
59.0,4.0,Computational Economics,22 January 2022,https://link.springer.com/article/10.1007/s10614-021-10226-2,Kelly-Based Options Trading Strategies on Settlement Date via Supervised Learning Algorithms,April 2022,Mu-En Wu,Jia-Hao Syu,Chien-Ming Chen,Unknown,Unknown,Unknown,Unknown,,
59.0,4.0,Computational Economics,05 September 2021,https://link.springer.com/article/10.1007/s10614-021-10189-4,The Impact of News Sentiment Indicators on Agricultural Product Prices,April 2022,Jia-Lang Xu,Ying-Lin Hsu,,Unknown,,Unknown,Mix,,
59.0,4.0,Computational Economics,12 October 2020,https://link.springer.com/article/10.1007/s10614-020-10050-0,Blockchain-Based Cryptocurrency Regulation: An Overview,April 2022,Satya Prakash Yadav,Krishna Kant Agrawal,Leonardo Mostarda,Male,,Male,Mix,,
59.0,4.0,Computational Economics,20 June 2021,https://link.springer.com/article/10.1007/s10614-021-10111-y,A Two-Dimensional Sentiment Analysis of Online Public Opinion and Future Financial Performance of Publicly Listed Companies,April 2022,Meng‐Feng Yen,Yu‐Pei Huang,Yueh‐Ling Chen,Unknown,Unknown,Unknown,Unknown,,
59.0,4.0,Computational Economics,03 March 2021,https://link.springer.com/article/10.1007/s10614-020-10086-2,The Training of Pi-Sigma Artificial Neural Networks with Differential Evolution Algorithm for Forecasting,April 2022,Oguzhan Yılmaz,Eren Bas,Erol Egrioglu,Unknown,Male,Male,Male,"Artificial neural networks, one of the most widely used methods of artificial intelligence, is a very simple imitation of the human nervous system. The most basic element of the human nervous system is nerve cells called neurons. McCulloch and Pitts in (1943) set the artificial mathematical model of a biological neuron for the first time and this model underlies the various artificial neural network model even today. Generally, many artificial neural networks have three layers as input, hidden and output layer and these layers reveal the architecture of an artificial neural network. Besides the running process of each layer differs from an artificial neural network model according to an artificial neural model. Artificial neural networks have been extensively used for pattern recognition, clustering, classification and forecasting task in recent years. Feedforward neural networks (FFNNs) are the popular form of artificial neural network models that can perceive and approximate computational models using their advanced parallel layered structure (Ojha et al. in 2017). Multilayer perceptron (MLP) is a special form of FFNNs. Unlike the single perceptron model, MLP has the hidden layer(s) structure. The input layer receives the incoming data and sends it to the hidden layer. Then the incoming information is transferred to the next layer. The number of hidden layers varies depending on the problem, at least one and adjusted according to the need or the problem structure. The output of each layer becomes the input of the next layer. In artificial neural networks literature, MLP models have been successfully used and applied in time series forecasting. MLP is employing one or several hidden layers in the artificial neural networks and generally, many FFNNs models including MLP work with additive neuron model (units). Besides, some artificial neural networks use multiplicative neuron model such as multiplicative neuron model artificial neural networks proposed by Yadav et al. in (Yadav et al. 2007). MLP is a type 1st order neural network which can effectively carry out inner products which are then weighted and summed before passing through the non-linear threshold function. The other way to overcome the restriction to linear maps is to introduce higher-order units to model nonlinear dependences (Giles and Maxwell in (Giles and Maxwell 1987), Giles et al. in (Giles et al. 1988)). Higher-order neural networks (HONNs) are a type of FFNNs which provide nonlinear decision boundaries, therefore offering a better classification capability than the linear neuron (Guler and Sahin in 1994). HONNs use higher combinations of inputs. HONNs do not only contain additive or multiplicative units but contain both units at the same time. HONNs also allow the inputs to be used by duplicating them. Unlike many artificial neuron networks, Pi-Sigma artificial neural networks (PS-ANN) one of the HONNs proposed by Shin and Gosh in (1991) have a very different architecture than other artificial neural networks due to using both additive and multiplicative structure. In the literature, there are many studies about PS-ANN for different aims especially forecasting. Ghazali and Jumeily in (2009) used PS-ANN for financial time series prediction. Husaini et al. in (2014) used PS-ANN for temperature forecasting in Batu Pahat. Husaini et al. in (2011) showed the effect of network parameters of PS-ANN for temperature forecasting. Husaini et al. in (2012) used PS-ANN for a one-step-ahead temperature forecasting. Nayak et al. in (2015) proposed a novel chemical reaction optimization based on PS-ANN for nonlinear classification. Mohamed et al. in (Mohamed et al. 2016) used batch gradient method for training of PS-ANN with a penalty. Nayak in (2017) used PS-ANN based on genetic algorithm and particle swarm optimization for exchange rate prediction. Bas et al. in (2016) proposed a high order fuzzy time series method based on PS-ANN and determine the fuzzy relations with PS-ANN. Dash et al. in (2018) used a PS-ANN based on evolutionary algorithms for gold price prediction. Deepa et al. in (2018) used a PS-ANN based on bioinspired swarm intelligence optimization algorithm for multimodal tumour data analysis. Akram et al. in (2019) proposed an improved PS-ANN with error feedback for physical time series prediction. Nayak in (2020) used a fireworks algorithm for the training of PS-ANN for modelling and forecasting chaotic crude oil price time series. Panda and Majhi in (2020) used improved spotted hyena optimizer with space transformational search for the training of PS-ANN. Kocak et al. in (2020) proposed a new fuzzy time series method based on an ARMA-type recurrent PS-ANN. Pattanayak et al. in (2020) proposed a multi-step-ahead fuzzy time series forecasting by using hybrid chemical reaction optimization with PS-ANN. Nayak and Ansari (2020) used cooperative optimization algorithm in PS-ANN for stock forecasting. In this paper, the training of PS-ANN is performed by differential evolution algorithm (DEA) uses DE/rand/1 mutation strategy for time series forecasting. The performance of the proposed method (DEA-PS-ANN) is evaluated on two well-known data sets in ANN literature and compared with many studies in ANN literature. The rest of the paper is as follows: Section two is about PS-ANN and DEA. The proposed DEA-PS-ANN is given in section three. The application results are given in section four and finally, section five is about conclusions and discussions.",14
59.0,4.0,Computational Economics,08 February 2022,https://link.springer.com/article/10.1007/s10614-022-10235-9,GPS data Mining at Signalized Intersections for Congestion Charging,April 2022,Wang Yu,Zhang Dongbo,Zhang Yu,,,,Mix,,
59.0,4.0,Computational Economics,27 January 2021,https://link.springer.com/article/10.1007/s10614-021-10096-8,An Identification Algorithm of Systemically Important Financial Institutions Based on Adjacency Information Entropy,April 2022,Linhai Zhao,Yingjie Li,Yenchun Jim Wu,Unknown,Unknown,Unknown,Unknown,,
59.0,4.0,Computational Economics,20 April 2022,https://link.springer.com/article/10.1007/s10614-022-10253-7,Optimizing Financial Engineering Time Indicator Using Bionics Computation Algorithm and Neural Network Deep Learning,April 2022,Zeyu Wang,Yue Deng,,Unknown,,Unknown,Mix,,
60.0,1.0,Computational Economics,20 June 2021,https://link.springer.com/article/10.1007/s10614-021-10130-9,Towards Crafting Optimal Functional Link Artificial Neural Networks with Rao Algorithms for Stock Closing Prices Prediction,June 2022,Subhranginee Das,Sarat Chandra Nayak,Biswajit Sahoo,Unknown,Unknown,,Mix,,
60.0,1.0,Computational Economics,06 July 2021,https://link.springer.com/article/10.1007/s10614-021-10131-8,A Machine Learning Approach to Detection of Trade-Based Manipulations in Borsa Istanbul,June 2022,Nurullah Celal Uslu,Fuat Akal,,Male,Male,Unknown,Male,"In stock markets, manipulation can generally be defined as transactions made with the aim of giving a misleading impression to investors or creating a misleading market with respect to the stocks traded in capital markets. In the literature, it is possible to find different definitions of manipulation, but the main focus of all those definitions is behavior towards directing investors to buy or sell a security or keeping the price of securities at an artificial level within the scope of external interventions in the functioning of the markets. Those who carry out such manipulative transactions that cause price changes in stocks create artificial prices in the market, and investors make a profit while investors lose. Actions designed to interfere with price mechanisms by preventing the free interaction of supply and demand, to deceive people to trade in a security, or to keep the price of securities at artificial levels are evaluated as manipulative (Fischel & Ross, 1991). Such manipulations also reduce investor confidence in the market (Ogut et al., 2009). In the securities market, stock trading transactions are now carried out over the Internet, and millions of transactions are made in the market in a single day. Innovations in information technologies cause differentiation of manipulative transactions in the markets and the emergence of new and creative methods daily. With such developments, the numbers and types of transactions causing market disruption are also increasing. Allen and Gale classified manipulations into three different categories, which are action-based manipulation, knowledge-based manipulation, and process-based manipulation. Action-based manipulation includes manipulative transactions that can change the current prices of securities and, therefore, the value of the company. Information-based manipulation, on the other hand, evaluates information that may affect the value of capital market instruments, providing false, misleading, and unsupported information or spreading news or avoiding disclosing information (Allen & Gale, 1992). An example of this type of manipulation is a group of investors first buying a stock and then spreading positive rumors about the firm and selling those stocks at a certain level of profit (Mei et al., 2004). Trade-based manipulation, which is the subject of this study, is defined as transactions performed by buyers or sellers, orderers, cancelers, order takers, or those performing account activities in order to give a false or misleading impression of capital market instruments' prices, price changes, supplies, and demands (Capital Market Law, 2012). All types of manipulation, as the biggest obstacles for investors to invest securely in capital markets, prevent fair stock formation and harm investors by aiming to deceive them. Therefore, manipulation cases are closely monitored in the stock market and sanctions are applied to manipulators. Manipulation is a very important issue in the securities market and it is crucial to detect it. Since trade-based manipulation generally has the appearance of a legal stock market transaction alone, this type of manipulation is difficult to detect (Manavgat, 2008). As there are few such studies in the literature, and especially few studies on the detection of trade-based manipulations, in our study, we used supervised machine learning classification techniques to detect trade-based manipulations in the daily data of 20 manipulated stocks between 2010 and 2015 in the Istanbul Stock Exchange (BIST). The remainder of this article is organized as follows: Section 2 provides a brief review of manipulation in the stock market and the corresponding detection methods. Section 3 gives information about the data used in this study. Section 4 is the main section of this work, which evaluates the proposed machine learning-based model for the detection of trade-based market manipulation; promising performance is reported here. Finally, Section 5 discusses potential improvements and presents the key results of our research. At the end of this section, the conclusions of the paper and recommendations for future work are presented.",5
60.0,1.0,Computational Economics,02 July 2021,https://link.springer.com/article/10.1007/s10614-021-10133-6,Need to Meet Investment Goals? Track Synthetic Indexes with the SDDP Method,June 2022,Lorenzo Reus,Rodolfo Prado,,Male,Male,Unknown,Male,"Multistage stochastic programming (MSP) is a well-known framework for modeling large-scale problems under uncertainty, and has been widely used in several fields, including asset allocation problems.Footnote 1 MSP solutions have the quality of enabling planning not only for today’s positions, but also for future portfolio rebalancing, depending on the evolution of market events and prices. That advantage avoids increases in turnover (which leads to higher transaction costs) and augment the probability of meeting objectives by measuring them under possible future outcomes. One of the main complexities of solving MSP is known as the “curse of dimensionality”, which happens when working with a high-dimensional state space. In this case, it becomes intractable to find the optimal solution by computing the Bellman function in each state. The number of possible allocations and scenarios to consider for future asset prices increases exponentially with the time horizon, or more specifically, to the number of rebalancing periods. Thus, high dimensionality is present in both, long-term asset and liability management (ALM) problems and short-term problems with frequent rebalancing. Pereira and Pinto (1991) propose a method for handling large-scale MSP, called the Stochastic Dual Dynamic Problem (SDDP). The SDDP approximates the Bellman function of the dynamic programming model with linear cuts, instead of computing its exact value in every state. This method has mainly been used in electricity generation planning and the extraction of natural resources. However, its use in financial applications is recent and incipient. In this paper, we show how the SDDP can be applied to a particular asset allocation problem. We implement a model that allows the inclusion of transaction costs, short-selling and time-dependent returns in the Julia SDDP.jl package created by Dowson and Kapelevich (2017). In this way, we illustrate how one might work with the SDDP without needing to build the method from scratch, as happened before the irruption of open-source programming languages. We also solve an instance involving 28 rebalancing periods, which to the best of our knowledge, no other research has done before with the SDDP method. The objective of our model is to generate dynamic allocation policies that aim to achieve user-defined goals, rather than maximizing profits or the expected utilities of investors’ wealth as previous studies do. This approach might be suitable for pension fund managers that must meet future pension commitments, or insurance companies that must cover insurance claims. Wealthy investors or non-profit institutions with big endowments (e.g. universities) might prefer stable returns over time, even if this comes with lower expected profits in the long term. We also propose a novel multistage stochastic dynamic programming model using the idea of index tracking as a way to achieve managers’ goals. For this reason, the allocation strategy aims to track user-defined indexes, rather than a real index, as previous studies do. Our allocation policies are not intended to reduce the error experienced with common tracking measures. Instead, we follow the idea of previous research solving the Enhanced Index Tracking Problem (EITP), so as to outperform the index. In our study, we do this by tracking the negative deviations of the cumulative returns of the replicating portfolio. Figure 1 illustrates how tracking cumulative returns differs from following one-period returns. Suppose the goal is to replicate an index with a fixed return of r at each time period. If the cumulative returns are below those of the index (case A), then the portfolio needs to achieve a return that is higher than r in the following periods, in order to track the cumulative returns of the index. The opposite happens when the cumulative returns are above those of the index (case B). In this case, the portfolio can afford to achieve returns that are lower than r, as long as the cumulative returns remain higher than those of the index. The paper is structured as follows. In the next section, we provide a brief literature review related to applications of the SDDP method and the index tracking problem. In Sect. 3, we explain the SDDP method and the proposed model for tracking predefined returns. Then, in Sect. 4, we test and analyze the performance of the model based on market data involving 6 assets and 28 rebalancing periods (7 years with quarterly rebalancing). Finally, we discuss the main conclusion and possible improvements that can be made in future research. Tracking an index with constant one-period return r. A and B represent cases where the cumulative returns of the portfolio are below and above those of the index respectively. Our tracking measure only considers negative deviations from the cumulative returns of the index (case A)",
60.0,1.0,Computational Economics,26 June 2021,https://link.springer.com/article/10.1007/s10614-021-10138-1,The Effect of Including Irrelevant Alternatives in Discrete Choice Models of Recreation Demand,June 2022,John N. Ng’ombe,B. Wade Brorsen,,Male,Unknown,Unknown,Male,"Estimating a discrete choice model using stated preference data requires choosing a choice set. Choice set misspecification occurs when irrelevant alternatives are mistakenly placed into the choice set or mistakenly excluded. Decision makers may naturally exclude certain alternatives because they are unaware of them or because one of the attributes generates substantial disutility. It is well known that parameter and welfare estimates are sensitive to choice set definitions (Li et al., 2015; Peters & Adamowicz, 1995; Swait & Ben-Akiva, 1987; von Haefen & Domanski, 2018). Parameter sensitivity is especially of concern in applications with a huge number of feasible alternatives, which includes models of location choice in applications to real estate and recreation demand, because the researcher often subjectively defines the choice set (Guevara & Ben-Akiva, 2013; McFadden, 1978; von Haefen & Domanski, 2018). The literature on choice set misspecification offers several approaches to deal with irrelevant alternatives. One practice common in recreation demand applications is to exclude alternatives that exceed a certain distance or price (determined by the researcher) from each decision-makers’ home. Including distant sites affects parameter estimates, although this influence may diminish past some threshold (Parsons & Hauber, 1998; Whitehead & Haab, 1999). Several papers examine parameter sensitivity to the inclusion of various substitute site/activity combinations in discrete choice models of recreation demand (Haab & Hicks, 1997; Jones & Lupi, 1999; Parsons et al., 2000; Pramono & Oppewal, 2012). These papers treat choice set definitions as exogenous—i.e. the decision maker’s choice set is assumed to be known deterministically. However, a growing number of papers treat choice set definition endogenously by modeling choice set formation in one stage and the selection of an alternative in a second stage (Haab & Hicks, 1997; Li et al., 2015; Manski, 1977; Swait & Ben-Akiva, 1987; Thiene et al., 2017). These papers conclude that ignoring choice set formation leads to biased parameters and biased welfare estimates. This article extends research into choice set formation in discrete choice models by measuring the bias from two plausible forms of choice set specification. Our motivation comes from the practice of excluding recreational sites that the researcher thinks are too far for a decision-maker to reasonably access (for example, day-trip sites more than 200 one-way miles from an individual’s home). Specifically, we examine two types of choice set formation: (1) every choice set includes the full set of alternatives and (2) each choice set is formed from a stochastic selection process that is a function of choice attribute levels, thereby making choice set formation endogenous. The two cases correspond to the two estimators of discrete choice models that we assess. Our primary objective is to measure the bias when an observable attribute (e.g. travel cost to recreation site) affects the probability that an alternative appears in the choice set. In doing so, we estimate two models: (1) the independent availability logit (IAL) model that incorporates choice set formation as a stochastic process and probabilistically imputes all possible choice sets and (2) the conditional logit (CL) model which assumes the choice set includes all alternatives. The literature of revealed preference models has a variety of models that differ from the CL and IAL. These include random parameters logit (RPL), latent class model (LCM), scaled multinomial logit (SMNL) and others. In this research, we focus only on the CL and IAL models. These two models are common in agriculture (Chang et al., 2009), economics (Wichmann et al., 2016; Cha & Melstrom, 2018), transportation (Alizadeh et al., 2018; Habib et al., 2013) and other discrete choice studies (Hensher et al., 2005). Our approach is illustrated in Fig. 1. Modeling strategy for Monte Carlo experiments While actual data are often used to demonstrate potential bias in recreation demand models, we conduct Monte Carlo experiments as well as empirical analyses using real data. The experiment simulates a random utility maximization (RUM) problem in which individuals choose from many recreational sites with varying attributes. Monte Carlo analysis allows us to measure bias when the model and choice set are misspecified, and test how well recreational choice set definition heuristics can do in reducing this bias. The real-world application is location choice in recreational fishing demand. We find that both the CL and IAL models perform well when their own assumptions underlie the data generating mechanism. However, both models perform poorly under the alternative’s assumptions. Our empirical results show that the IAL model approximates the attribute-based cutoff well and fits the data more appropriately than the CL model. These findings have important implications for researchers studying small-sample properties of CL and IAL specifications and when it is recommended to use the two models. The rest of the paper is arranged as follows. The next section details the CL and IAL models while Sect. 3 presents a Monte Carlo simulation study. Section 4 presents an empirical application and Sect. 5 concludes the paper.",
60.0,1.0,Computational Economics,18 June 2021,https://link.springer.com/article/10.1007/s10614-021-10139-0,Maximum Likelihood Estimation Methods for Copula Models,June 2022,Jinyu Zhang,Kang Gao,Qiaosen Zhang,,,Unknown,Mix,,
60.0,1.0,Computational Economics,20 June 2021,https://link.springer.com/article/10.1007/s10614-021-10140-7,The Geometry of the World of Currency Volatilities,June 2022,Gueorgui S. Konstantinov,Frank J. Fabozzi,,Unknown,Male,Unknown,Male,"Foreign exchange (FX) volatility is an important determinant in currency trading. Menkhoff et al. (2012) show that modelling volatility innovations determines the impact on currency strategies (most notably on carry trades). Currency pairs trading is a well-known strategy for portfolio managers and traders, as pairwise carry, value and momentum strategies are implemented based on significant statistical and economic variables. FX options are not only an active risk management tool, but also represent an active portfolio management strategy as Pojarliev and Levich (2008) identify. One of the first academic works on pairs trading in the equity market was by Elliot et al. (2005), who investigated an equilibrium approach and defined profits as convergence to equilibrium. One of the most well-known works appeared the following year by Gatev et al. (2006) who investigated pairs trading and found that it generated high abnormal returns. On a portfolio level, they found that diversification is achieved in a portfolio consisting of roughly 20 trades. In another study, Engelberg et al. (2009) documented that the decline in liquidity positively affects pairs trading in stocks because pairs trading is a contrarian strategy. As we observe from the literature, pairs trading is an important issue that is affected by liquidity, volatility and correlations, and market (systematic) risk. Currencies are an asset class that is enormously relevant in a multi-asset context and pairs trading is a source of return as contrarian strategies often take place in the market. Questioning the success of pairs trading, Do and Faff (2010) report that it is a risky arbitrage strategy which works for the U.S equity market. A comprehensive review of the mechanics and relevant issues in pairs trading is provided by Vidyamurthy (2004). In this paper, we propose a framework to capture a portfolio’s currency risk using empirical data on currency pairs. As Neely and Weller (2013) found, currency strategies are highly successful compared to equities. However, compared to bonds and currencies, which underlie the interest rate mechanism, equity and foreign exchange strategies are separated as investment approaches in order to be successful. This applies to pairs trading. Extending the equilibrium triangle notion introduced by Margrabe (1993) and Geman and Souveton (1997), we apply the implied volatility approach to spot transactions and simple trading instead of derivative instruments only. Specifically, we show how currency pairs depend on volatilities and correlations that can be visualized as triangles in a two-dimensional coordinate plane. Then we rotate the triangles generating complex rotational objects. This allows us to consider possible realizations of changing implied volatilities and correlations in a portfolio context. Using the rotation of the currency triangles, we define a factor that captures both implied volatility and implied correlation dynamics that we label gravity radius factor. We derive the factor from the distance travelled by the center of gravity of the currency triangle. This is the intuition behind triangle rotation. Using the gravity radius factor makes it is possible to derive a weighted portfolio factor that comprises the single gravity radiuses of all currency pairs in a portfolio. We show that portfolio managers can use this factor as a combined correlation and volatility driven risk indicator. To do this, we compare the dynamic of our factor to the well-known metrics like the turbulence index suggested by Kritzman and Li (2010) and Kritzman et al. (2011). Previous research investigated the importance of volatility indexes for risk and sentiment analysis. For example, Whaley (2009) explained the purpose of the CBOE Market Volatility Index (VIX) as a risk indicator for the stock market. Moran (2014) argued that volatility indexes are used for asset allocation and market sentiment indicators. With our approach, we go a step beyond and combine both implied correlation and volatility. We try not to identify idealized stylized facts about currency correlations and volatilities, but rather introduce a factor that captures empirical data, which changes as new information arrives in the market. We propose a framework based on empirical currency market data rather than on idealized economic conditions. Because our factor incorporates correlations and volatilities of currencies in a portfolio, we find similarities to the financial market turbulence metric of Kritzman et al. (2012) and Kinlaw and Turkington (2014). However, we show that the gravity radius factor we construct behaves differently than the turbulence metric identified in previous research. Specifically, the gravity radius factor allows for instant data and forward-looking volatility estimations to determine the magnitude of the gravity radius factor both on a single pairs-trade level and on portfolio level. Similar to gravitational property of physical objects, the factor can be small but not zero. Our results show that for an equally weighted portfolio of nine currency pairs, the gravity radius factor behaves similar to the Deutsche Bank FX Volatility Index in the sample range from January 2000 to August 2020. This index is widely used as a proxy for the FX risk in the currency market. However, our portfolio factor spikes and converges much more dynamically than equity and currency volatility indexes. The gravity radius factor reflects changes in both correlations and volatilities. Like the well-known volatility indexes, our factor exhibits left-tail skewness. A rise in correlation is associated with higher levels of the gravity radius factor; lower correlations between the currency pairs is associated with lower values for this factor. This behavior is similar to market conditions with low volatility and large risk appetite by investors. Similarly, an increase in volatilities results in a higher value for the gravity radius factor, which incorporates changes in correlation values and vice versa. As a consequence, the gravity radius factor can be applied to estimate correlation and volatility risk on a historical basis, monitor risk, predict pairs trading, and estimate a factor that captures dynamically both correlation and volatility changes on a single-security level and on a portfolio level. The paper is organized as follows. In Sect. 2 we explain the mechanics of currency pairs trading, showing how to derive volatilities in Euclidean space, deriving the gravity radius factor after triangle rotation, and explaining how changing implied correlations and volatilities affect the gravity radius factor. In Sect. 3 we show how the gravity radius factor for single currency pairs can be incorporated at a portfolio level. Specifically, we demonstrate that the gravity radius factor captures both changes in implied correlations and implied volatilities. In Sect. 4 we provide robustness analysis.",1
60.0,1.0,Computational Economics,22 July 2021,https://link.springer.com/article/10.1007/s10614-021-10145-2,Is Deep-Learning and Natural Language Processing Transcending the Financial Forecasting? Investigation Through Lens of News Analytic Process,June 2022,Faisal Khalil,Gordon Pipa,,Male,Male,Unknown,Male,"Accurate forecasting of returns is crucial for individual investors, investment banks and corporate investment managers. It is also equally important for investors to foresee the returns accurately and design the investment or trading strategies keeping in view all relevant aspects of forecasting. For many years stock market forecasting studies have been emphasizing the volatility models. Few studies inculcate the role of technological forecasting i.e. artificial intelligence. The efficient market hypothesis (EMH) is proposed by Fama (1998) is under criticism, because the proposed model is in contrast to the behavioral finance concept (Kahneman & Tversky, 1979; Kahneman, 2003; Shefrin, 2008). It has been much debated and considered as a limitation of EMH that this model is not considering the role of investor’s sentiments and their behavioral aspect. Technological advancements and inventions of the new artificial intelligence-based model are reshaping the method of forecasting (Wang et al., 2018; Kuo & Huang, 2018; Makridakis et al., 2018). Normally, the artificial intelligence-based model takes previous stock prices and other variables into account but news analytic consideration is less researched. In this specific context news analytic is in the early stages and needs advancements for better forecasting and efficiency of intelligent trading systems. In the last decade, soft computing methods and techniques have grown rapidly that entice researchers to explore more sophisticated techniques for the stock market and time-series predictions. Time series financial modeling has a long history and time-series data is characterized by hidden relationships, high uncertainty, and unstructured in nature. To estimate the behavior of financial time series there are two types of models are available; linear model and non-linear models. Whereas, linear models are affected by techniques like Box Jenkins and Kalman filters, piece-wise regression, and Brown’s exponential smoothing. All these theories are turning data into the linear functions. However, recent evidence shows that financial markets behave in a non-linear fashion. In addition to these problems, there are other factors that intact with financial markets like, general economic conditions, political events, news, and investor’s psychology that makes a stock market prediction so difficult (Cheng & Chan, 2018; Huang et al., 2007). To address these issues, artificial intelligence has been evolved as a very good technique due to its learning, generalization, and non-linear behavior to overcome these problems and to give better forecasting (Makridakis et al., 2018; Li & Ma, 2010). In this connection, the most relevant techniques are; Recurrent Neural networks, Neural Networks, fuzzy logic, and genetic algorithm (Hiransha et al., 2018; Ergen et al., 2017; Nelson et al., 2017; AlFalahi et al., 2014). Artificial Neural Networks model are pretty good with flexibility and adaptability to learn from changes and previous trend in a given set of input and predicts the trends based on network training. There is a fair deal of evidence that exists in the literature that models that based on artificial neural networks outperform the traditional time series model, for example, see Adebiyi (2012), AlFalahi et al. (2014), Trippi and DeSieno (1992), Correa et al. (2009) and Hansson  (2017). There are many soft-computing techniques available under the umbrella of artificial intelligence but finding appropriate techniques is very important to get accurate forecasting results. Study of Li et al. (2018) and Atsalakis and Valavanis (2009) can be referred here each has surveyed more than 100 articles by researchers who have used fuzzy logic, genetic algorithms, and neural networks and recurrent neural network as modeling techniques in their studies. It is evident from these articles that mostly researcher have used feed-forward neural networks (FFNN), currently, some studies use Recurrent Neural Networks(RNN) multilayer perceptron (MLP) to forecast the stock markets (Arora et al., 2019; Pawar et al., 2019). This survey study also testifies the magnitude of the importance of non-conventional tools for stock market prediction. For the stock market prediction process we cannot rely upon past stock prices and some other variable but we need to embed the impact of market news to achieve maximum accuracy. In the prediction process, it can be very tedious for managers to focus on every news that just pops up and align their investment strategies. A human being can miss much information and even information can be out of his reach as well. So, here natural language processing (NLP) techniques come into play. So, there is an urgent need to automate the news analysis process based on NLP technique so that the investment manager and the corporations can be benefited as well as AI-based predictive models can be supplied with more relevant information instead of just past prices. Natural language processing is a subfield of AI where Algos and deep learning model tries to make computers understand language intuitively near to the human level (Nadkarni et al., 2011). A human being has evolved from thousands of year training to understanding the emotion and feeling of language elicits but computers are struggling with the help of deep learning and AI-based models. In this study, we have used the NLP model (see Fig. 1) with the help naive Bayes classifier to process the raw information that is parsed out of many sources. These sources include mainstream media, print media, social media news feeds, blogs, investors’ advisory portals, expert’s opinions, brokers updates, web-based information, company’ internal news and public announcements regarding policies and reforms. Detail of the news analytic and sentiment analysis can be seen in Sect. 3.1.2. Many studies propose soft computing techniques for better and most of the researches have focused on the comparison of traditional time series stock prediction models and artificial neural embedded network models. This study contributes to the existing body of knowledge in the following ways: Normally, studies use news information and stock price data for indices. Apart from other motivations to choose indices for the prediction process, one benefit is that data collection and aggregation is relatively easier because of its ready availability. However, collecting news information for each company individually and make meaningful sentiments for that stock is challenging. However, this study focuses on individual-level stock and news information that makes this study bit challenging because not only news from all possible sources need to accumulate but also company internal news is also taken care. For example, the company changes the top echelon due to any reason or decides to change the level of dividends, any commentary on ‘hashtags’ is not covered by prominent media sources but still, they impact upon the prediction. Secondly, this study is emphasizing NLP techniques and the way how to raw news text can be used for sentiments building processes. So, NLP based models are simply efficient in extracting emotion, feelings, and sentiments out of a raw text. Thirdly, this study not using simple neural networks for predictions process but Long Short Term Memory (LSTM) model based upon the newly developed and highly proven performance in different fields. LSTM models are specifically designed to remember the long-term dependencies. A point that makes it different is mostly, LSTM model is supplied with past stock prices as an input to predict the future price of the stock, however, this study has used sentiments, extracted with help of NLP techniques, to predict the stock price and it is evident from results that model with sentiments has significantly increased the accuracy of the model. This study will be generically beneficial to all institutional and individual investors, all kinds of traders, portfolio managers, and specifically for short-term and long-term investors who invest in the equity market, future marks, derivative and foreign exchange market. Natural language processing model The rest of the paper is divided into the following sections; Literature review, Methodology section that discusses data collection processes, sentiment index development process, NLP techniques, and implementation of the study model. Then comes results and their interpretations and finally the conclusion of the study.",3
60.0,1.0,Computational Economics,24 July 2021,https://link.springer.com/article/10.1007/s10614-021-10146-1,A Computational Analysis of the Tradeoff in the Estimation of Different State Space Specifications of Continuous Time Affine Term Structure Models,June 2022,Januj Amar Juneja,,,Unknown,Unknown,Unknown,Unknown,,
60.0,1.0,Computational Economics,24 July 2021,https://link.springer.com/article/10.1007/s10614-021-10147-0,Menu Optimization for Multi-Profile Customer Systems on Large Scale Data,June 2022,Jeyhun Karimov,Murat Ozbayoglu,Erdogan Dogdu,Male,Male,Unknown,Male,"Graphical User Interface (GUI) is an important component of interactive computer applications with multiple user profiles (i.e., users exhibiting different usage behaviors and patterns). Interactive web portals, Automated Teller Machines (ATMs), smart phone applications are among many of these applications. There is an opportunity to customize user interfaces, especially user menus, of these systems, for different user profiles based on the usage behaviours so that user satisfaction, service quality, and profitability can be increased. For example decreasing transaction completion time benefits many users of ATMs and the banks that own those ATMs. This problem, in general, is called “menu optimization” or “menu customization” problem Goubko and Danilenko (2010). A naive glimpse to the menu optimization problem may render the problem, erroneously, as a trivial one. For example, determining the most clicked menu items and positioning them in top menu screens, is a quick solution Jain (2012). In fact, such a heuristic may reduce the average completion time provided that frequently used items are not already at the upper layers of menu hierarchy. However, there are several constraints that prevent such a heuristic to be implemented freely. For example, there is an upper limit on the number of items that can be placed in a single screen, due to screen space and/or the ergonomics of GUI design Al-Saleh and Bendak (2013). Besides those, menu items cannot be arbitrarily grouped under a single menu item because they may not be related to each other (semantics) Danilenko and Goubko (2013). Indeed, usability is one of the main factors affecting the quality of software systems McCall et al. (1977); Mayer et al. (2016). Therefore, in optimizing menus ensuring usability is vital. For example, the structure of the menu cannot contain any ambiguous placement of menu items. Hence, optimizing a menu structure necessitates taking many constraints into consideration which accordingly requires construction of a well defined mathematical optimization framework. Furthermore, abstractions mapping constraints arising due to ergonomics, semantics, resource limitations, and functional behaviors should be captured by concise and efficient mathematical representations to be used in the optimization framework. Optimization of ATM menu design is a topic investigated in the literature from various perspectives and for maximizing different objectives Zhang et al. (2012). Our main contribution in this study is the creation of a generic mathematical optimization model which is guaranteed to uncover the optimum design among many possible alternatives under the given constraints within the defined variable space. We transformed the ATM menu optimization problem into a network flow problem by treating the menu items as vertices of the graph, links between the menu items as edges, and the users’ navigation among the vertices as the flows. The objective is to position the menu items in such a way that the total weighted sum of flows on the vertices is the achievable minimum without violating the usability, ergonomics, and resource constraints. All parameters of the model are derived from a large database of actual ATM logs. Since our optimization model is a Mixed Integer Program (MIP), it is rather straightforward to extend its applicability to many GUI menu optimization problems other than the ATM menu optimization problem by customizing the parameters, constraints, and objective function. To the best of our knowledge, such optimization framework has never been constructed in the context of ATM menu optimization in the literature. One exception is an earlier study where the initial design and performance analysis of our MIP-based solution has been introduced Karimov et al. (2015b). In this study we improved our model and significantly expanded our performance evaluations. Nevertheless, in this study we seek answers to the following research questions, the answers of which also represent our novel contributions enumerated as follows:  How can we unearth the behaviors and actions of a huge number of users buried in ATM logs to create efficient abstractions and parameters to provide input to a mathematical programming framework? How can we transform the ATM menu optimization problem into an equivalent network flow problem that can be modeled as an MIP model? How can we embed the ergonomics, usability, and resource constraints into the MIP model? Can we construct a heuristic model that has much lower complexity than the exact model? What is the extent of the improvements brought by the MIP model in comparison to simple heuristics like moving up the highly accessed menu items to the upper layer menu screens? The rest of the paper is organized as follows. Literature review is presented in Sect. 2. We present the system model in Sect. 3. Experiments are provided in Sect. 4. Conclusions are drawn and open questions for future research are given in Sect. 5.",1
60.0,1.0,Computational Economics,22 October 2021,https://link.springer.com/article/10.1007/s10614-021-10148-z,Numerically Pricing Nonlinear Time-Fractional Black–Scholes Equation with Time-Dependent Parameters Under Transaction Costs,June 2022,M. Rezaei,A. R. Yazdanian,S. M. Mahmoudi,Unknown,Unknown,Unknown,Unknown,,
60.0,1.0,Computational Economics,09 July 2021,https://link.springer.com/article/10.1007/s10614-021-10149-y,House Prices as a Result of Trading Activities: A Patient Trader Model,June 2022,Ralf Korn,Bilgi Yilmaz,,Male,Female,Unknown,Mix,,
60.0,1.0,Computational Economics,14 July 2021,https://link.springer.com/article/10.1007/s10614-021-10150-5,A Neural Network Approach to Value R&D Compound American Exchange Option,June 2022,Giovanni Villani,,,Male,Unknown,Unknown,Male,"R&D investments are considered an important driving force for the growth of the modern economy. For the analysts, is very important to value these investments considering their uncertainty. As it is well-know, R&D projects are characterized by the sequentiality of their investments and by the flexibility to realize the production investment at any time before the expiration time of R&D innovation. In this scenario, the real option approach can capture these aspects, unlike the Net Present Value (NPV) and the Internal Rate of Return (IRR) that underestimate R&D projects. In particular, the R&D projects can be considered as compound American exchange option (CAEO) in which both the gross project value and the investment cost are uncertain. Papers that deal with exchange option valuation are Margrabe (1978), McDonald and Siegel (1985), Carr (1988), Carr (1995), Armada et al. (2007) and so on. In particular way, McDonald and Siegel (1985) value a simple European exchange option, Carr (1988) develops a model to price a compound European exchange option while Armada et al. (2007) propose a Richardson extrapolation in order to value a simple American exchange option. These models consider that assets distribute ”dividends” that, in real options context, are the opportunity costs if an investment project is postponed Myers (1977). However, the analytical computation of CAEO is more difficult and it is convenient to implement a numerical method. Numerical approximation is therefore an important task as witnessed by the contributions of Tilley (1993), Barraquand and Martineau (1995), Broadie and Glasserman (1997). The first goal of our paper is to implement a Monte Carlo methodology in order to value a CAEO applied in the context of R&D investment. To realize this objective, based on Cortelezzi and Villani (2009) and Villani (2014), we present the Least Square Monte Carlo (LSM) proposed by Longstaff and Schwartz (2001) in order to value the CAEO. Despite this approach is valid in term of accuracy, the time required for the simulation of this kind of option is very long. Consequently, the second aim of this paper is to build a neural network architecture based on Back Propagation (BP) system using the simulation results as “targets” in the learning phase. As there is not a market valuation of CAEO, the advantages of this approach are first of all the speed and the accuracy in the computations and, after that, the possibility to extend the trained neural network to value any R&D investment project. To highlight our method, we compare BP approach with Radial Basis Function (RBF) and General Regression (GRNN). The computing power has allowed nonlinear methods to become applicable to modeling and forecasting a host of economic and nancial relationships. Neural networks, in particular, have been applied to many of these empirical cases. For instance, Aminian et al. (2006) compare the predictive power of the linear regression model against the fully generalized nonlinear neural network, with the improvement exposing the degree of nonlinearity present in the relationship investigated. Their study uses neural networks as an efcient nonlinear regression technique to assess the validity of linear regression in modeling nancial data. Andreou et al. (2006) show that the artificial neural network models with the use of the Huber function outperform the ones optimized with least squares. Eskiizmirliler et al. (2020) approximate the unknown function of the option value using a trial function, which depends on a neural network solution and satisfies the given boundary conditions of the Black–Scholes equation. Arin and Ozbayoglu (2020) develop hybrid deep learning based options pricing models to achieve better pricing compared to Black-Scholes. The results indicate that the proposed models can generate more accurate prices for all option classes. RBF method as a meshless technique is suggested to solve time fractional Black–Scholes model for European option pricing problem Golbabai et al. (2019). The literature that studies the real option in the neural network context is not very extensive. For instance, Ma (2016) based on real options method to construct a petroleum exploration and development projects, select the appropriate option pricing method and instance data analyzed by gas exploration and point out that the application of real option method can effectively improve the investment project evaluation. Moreover, Taudes et al. (1998) propose to use neural networks to value options approximating the value function of the dynamic program showing for each mode of operation the current state as input and yielding the mode to be chosen as output. The paper is organized in this fashion. Section 2 presents the structure of an R&D investment and its evaluation in term of real option while, Sect. 3, illustrates the valuation of CAEO using the LSM approach. Moreover, the implementation of the neural network architecture is realized in Sect. 4 and some numerical applications are proposed in Sect. 5. Finally, Sect. 6 concludes.",1
60.0,1.0,Computational Economics,15 July 2021,https://link.springer.com/article/10.1007/s10614-021-10151-4,DeepValue: A Comparable Framework for Value-Based Strategy by Machine Learning,June 2022,K. J. Huang,,,Unknown,Unknown,Unknown,Unknown,,
60.0,1.0,Computational Economics,24 September 2021,https://link.springer.com/article/10.1007/s10614-021-10194-7,Optimized Machine Learning Algorithms for Investigating the Relationship Between Economic Development and Human Capital,June 2022,Erdemalp Ozden,Didem Guleryuz,,Unknown,Female,Unknown,Female,"The last decade has been in an era of innovation. The conditions of competition have been increasingly difficult; hence the necessity of innovation and offering diversity became a priority, and the innovation and diversity ended up producing high value-added products. As the engine of innovation, R & D has become momentous, affecting countries' economic production power and development. It is also crucial to develop and design new products and production methods and develop economic processes and organization methods. Human capital is directly linked to the R&D activity and competition intensity of an economy (Canton et al., 2005). The development of human capital ensures the formation of innovative developments, and these developments pave the way for new products. Thus, this situation indirectly supports economic development. The transformation of human capital can cause an increase in the number of innovative products and entrepreneurs. Also, human capital has become an essential factor in determining the competitive edge in international economies. Thus, the economic development of a country can be indirectly achieved through innovation and competition. Many studies in the literature emphasize the vital role of innovation for economic development and growth (Lucas, 1988; Romer, 1986; Solow, 1956). However, the current problem from the past to the present, measuring the impact of factors affecting human capital on economic development, is still a black box. Therefore, the impacts of human capital on economic development through innovation and competition require further research. There are two major reasons for this situation. The first is whether the existing human capital in a country plays a decisive role in forming an economic development and predicting this development. Second, since this relationship is inherently sophisticated and nonlinear, econometric models may be powerless to explain this relationship. Especially the most significant progress that might be a source for the solution of the letter problem is the ML method. The ML method, which sheds light on many problems, has recently found its way into analyzing economic development theories (Islam, 2013; Kilian & Vigfusson, 2013; Kouziokas, 2020; Long, 2010; Milačić et al., 2017). With ML methods, the relationships between inputs and outputs can be handled and examined nonlinearly, and superior predictive results can be obtained with fewer data. We propose the ML approaches to explain this nonlinear relationship better and predict long-term economic development with human capital indicators. To that end, in this study, ML methods that can be applied to this relationship have been developed, and a new perspective is provided by comparing the success of these developed models with a classical econometric model. Although it does not entirely indicate the extent of countries' economic development Gross Domestic Product (GDP) per capita is the most studied determinant to measure countries' economies and production. Developed countries generally have high GDP per capita if they do not have a strategically important presence due to their geographical location. The countries that make higher added value products must have higher human capital (Melnikas, 2008). Although higher human capital is a widely used and vogue word, its measurement is not exactly precise. According to Europe 2020 reports (EuropeanCommision, 2020), higher human capital is required to create higher value-added (HVA), which can only occur with a higher education level, which is correlated to the budget allocation of countries on education. Economic theories and empirical studies imply that R & D creates technological innovation, which increases the current stock of knowledge. Therefore, accumulated knowledge enhances human capital quality, creating a more competitive, productive, and more balanced and sustainable economy in the long run (OECD, 2009). R & D increases the catalytic role of innovation and enables companies (which are the backbone of a country) to follow the advanced developments by improving the ability to transfer technology (Griffith et al., 2004). Ulku (2004) compares that member and non-member OECD countries, and the results support the strong interaction between innovation (patent stock as an indicator) and GDP per capita (Ulku, 2004). The OECD report (2000) focused on analyzing sophisticated products and learning innovation of their processes. The transition to more innovative knowledge-intensive industries and services has been demonstrated to play a significant part in improving economic performance and social welfare (OECD, 2020). Based on economic theory, a non-declining marginal product of capital must be reached to achieve sustainable economic growth. In neo-classical economic growth models such as Ramsey (1928), Solow (1956), or Swan (1956), the long-term growth rate of total capital depends on exogenous technological improvement and the growth rate of the population. The idea behind this assumption is that the problem of potential reductions in capital returns can continually offset due to technological development. However, since these earlier growth theories take technological change and population growth as exogenous, they are severely criticized for not determining economic growth (Ramsey, 1928; Solow, 1956; Swan, 1956). Romer (1986) and Lucas (1988), who had a significant impact on Economic Theory, in the production function, the new growth theory made considerable efforts to 'endogenize' the technological change. In these studies, which are generally called ""endogenous growth theory,"" it is accepted that technological change will provide a sustainable and balanced economic development. Besides, these growth models provide a favorable framework for examining the impact of technological transformation in economic growth and development and understanding critical issues connected to innovative design, efficiency, and R&D (Howitt, 2010; Lucas, 1988; Romer, 1986). There are still some problems that exist in economic development and human capital. While some studies treat human capital only as a production factor and examine its effect in explaining the surplus factor, other studies have used endogenous growth models based on human capital. However, considering these models as complementary to each other will help to reach more valuable results, especially in terms of examining the relationship between human capital and economic development (Cakmak & Gümüs, 2005). Based on the R&D and technological development idea underlies the rate and direction of innovation to determine scientific progress. Therefore, technological development is a crucial force behind innovative activities (Nemet, 2009). Besides, R&D studies importantly determine the engine of technology development. Numerous academic studies in the literature confirm R&D's positive impression on economic development and growth for a long time (Bassanini & Scarpetta, 2003; Konrad & Wahl, 1990; Soete, 1981). Studies also emphasize the vital role of innovation in acquiring growth and development, which are sustainable and balanced and switch to an HVA and higher-income economy (OECD, 2013). Shrivastava et al. (2016) concluded that innovation would be beneficial in achieving sustainable development. More importantly, according to the World Bank's (2003) research, it has been argued that efforts focused on technological development are an essential input for economic growth and poverty alleviation (Shrivastava et al., 2016; Watson et al., 2003). The relation between economic development and the factors affecting human capital, and the direction of the trend that may occur over the years has been analyzed with econometric models. In this study, in addition to conventional econometric methods, new analyzing methods as machine learning will be applied to investigate the mentioned relationship to whether these new methods are better to explore this relationship and trend. Some critical indicators for industry and scientific studies can be predicted with better performance by designing new machine learning techniques. Besides, Artificial intelligence methods are also applied in many work areas, such as forecasting gross domestic product (GDP), agriculture, energy, health care, and transportation management (Abdel-Nasser & Mahmoud, 2019; Abdullah & Zeng, 2010; Costantino et al., 2015; Rahman et al., 2020). Estimates of GDP are of great importance for financial management and planning. Many studies estimate the GDP using classical estimation methods and artificial intelligence-based estimation methods when analyzing the literature (Mladenović et al., 2016). In previous studies, many methods have been used to estimate GDP, these methods can be divided into two main classes: statistical methods (Islam, 2013; Salma et al., 2020), and artificial intelligence-based methods (Chong & Pu, 2006; Huang et al., 2020; Sokolov-Mladenović et al., 2016; Yoon, 2020; Zainun et al., 2010). Table 1 summarizes the papers that include the prediction of GDP in previous studies using Machine learning models. Du (2009) proposed a chaotic time series prediction model based on online weighted SVM to enhance the prediction accuracy. In this method, changing data is used to create a dynamic model of the system. The system model is dynamically renewed online. The developed method was used to estimate GDP. The results showed that the suggested method could be quickly applied and perform well in terms of robustness and precision in time series estimation (Du, 2009). Long (2010) designed a hybrid model that combined genetic algorithm and support vector machine model (GA-SVM) for GDP prediction. The genetic algorithm has reached the optimum parameters of SVM to accurate results. Anhui's total GDP data from 1989 to 2007 were used to compare GA-SVM and radial basis function neural network (RBFNN) models' prediction performance. As a result of the study, the developed GA-SVM model was preferable to RBFNN (Long, 2010). Yu et al. (2010) developed the genetic algorithm tuned SVM model to make a successful GDP forecast. The study designed a prediction model that integrates a multi-scale chaotic search and genetic algorithm to determine SVM kernel parameters. The study results show that predictive accuracy improves with the optimized parameters (Yu et al., 2010). Islam (2013) tried to predict the 2008 Great Recession by using Support Vector Regression (SVR) and feedforward neural networks. Macro-economic data between 1978 and 2005 is used, and compared performances of developed models with ARIMA, which was previously applied in the literature. The paper shows that SVR had less variance value during the test period (Islam, 2013). Colkesen et al. (2016) used Gaussian process regression (GPR) and SVM to produce a landslide susceptibility map in Tonya, Turkey. The researchers compared these two methods with logistic regression (LR). GPR and SVR have similar prediction results, and these results are approximately 18% better than LR (Colkesen et al., 2016). Marjanović et al. (2016) proposed Extreme Learning Machine (ELM) model to estimate GDP based on CO2 emissions. The developed ELM model results were compared via statistical metrics the results of genetic programming and artificial neural network models by simulating. According to the simulation results, it has been observed that ELM has adequate performance for GDP estimation (Marjanović et al., 2016). Sokolov-Mladenović et al. (2016) presented the prediction model to estimate GDP growth rate using two different types of ANN, namely backpropagation -ANN (BP-ANN) and ELM-ANN. As a result of the study, the estimation accuracy of ELM-ANN based on trade data was better than BP-ANN (Sokolov-Mladenović et al., 2016). Milačić et al. (2017) used extreme learning machine (ELM) and artificial neural network (ANN) models to predict the growth rate of GDP. Economic growth estimation has been made by considering the added value in GDP agriculture, manufacturing. The reliability of computation models was measured using simulation results and various statistical indicators. According to the results, ANN with ELM learning methodology has been shown to have better results in GDP estimation practices against backpropagation (Milačić et al., 2017). Roman et al. (2019) investigated the ease of changing the kernel process to adaptive rather than prioritizing it during the optimization process. They made six kernel suggestions and tested the two kernel functions with the determined parameters. As a result, kernel functions with parameter optimization give better outcomes (Roman et al., 2019). Kouziokas (2020) developed a novel SVM kernel that combines the benefits of various machine learning methods to acquire advanced predictive results, called weighted-SVM. The paper obtained significantly improved forecasting results with the new kernel by joining a factor associated with the weight matrix produced by the particle swarm optimization with the bayesian optimized SVM kernel (Kouziokas, 2020). Salma et al. (2020) recommended a novel model that can predict the impact of various indicators on GDP growth data for Bangladesh. Firstly, they performed the stepwise linear regression model and compared it with Gaussian Process, Random Tree, Multilayer Perceptron, and Random Forest. As a consequence of the study, the stepwise linear regression model provides significant performance with the smallest prediction error in estimating Bangladesh's per capita GDP and evaluates determinants for a potential increase (Salma et al., 2020). This study has estimated GDP per capita by considering capital investment indicators with MLR, and BT-SVM, BT-GPR methods. Using machine learning algorithms with the optimization of kernel function parameters for estimation has begun to draw attention in the literature. However, hyperparameters are determined by the developer manually in most of the previous studies. This study's difference from other studies is that we optimize the parameters of various kernel functions of SVM and GPR methods using Bayes' theorem. The proposed optimization method searches the identified solution space and finds the best hyperparameter values. The searching of model-specific parameters for the used dataset is necessary to get better estimation results. For this reason, Box constraint (c), Bias, Epsilon (ɛ), Kernel scale values of each kernel function for SVM are optimized based on the Bayes theorem. Similarly, for the GPR model, length scale (σL), Signal standard deviation (σF), Beta, and Sigma values for all Kernel Functions are optimized with Bayes' theorem. All calculations were made with Matlab 2019b.",3
60.0,2.0,Computational Economics,22 July 2021,https://link.springer.com/article/10.1007/s10614-021-10152-3,Sectoral Impacts of International Labour Migration and Population Ageing in the Czech Republic,August 2022,Martin Stepanek,,,Male,Unknown,Unknown,Male,"The population in the Czech Republic, similarly to many other countries in Europe, has been going through unprecedented changes in the recent years, characterised principally by ageing, a result of long-term below-replacement fertility rates and decreasing mortality, and international migration, especially following the 2004 enlargement of the European Union. The median age in the Czech Republic increased from 35.3 in 1990 to 41.4 in 2015 (United Nations 2017), while the average life expectancy increased from 71.4 in 1990 to 78.6 in 2015, resulting in a shift in the old-age dependency ratio—the number of retired to working-age individuals—from 19.3 to 26.9% in that time period (World Bank, 2018). In addition, the number of foreign nationals with various forms of residence statuses increased from just over 80,000 in 1993 to 425,000 in 2008 and 515,000 in 2018, while the number of Czech nationals living in another EU country reached approx. 125,000 in 2018 (European Commission, 2019; MICZ, 2019). The exact effects of demographic changes and international migration on the economy and society in both the receiving country and the country of origin are difficult to estimate, not only because a large proportion of the relevant data, such as the share of spending in the receiving country, are often not collected, and may range from changes in wages (Nagarajan et al., 2016; Ratha et al., 2011) and tax revenues (Martinsen & Pons Rotger , 2017; Vargas-Silva , 2015) to human capital accumulation (Beine et al., 2008; Dinkelman & Mariotti, 2016) and innovation (Bosetti et al., 2015,Capello & Lenzi, 2019). In the context of the Czech Republic, population ageing has affected particularly the pension system, which will require a substantial reform in order to maintain a consistent pension-wage ratios in the future, and the labour force, which has been shrinking both in absolute and relative terms within the population as a whole. Net immigration, on the other hand, may help to counterbalance these effects by bringing additional working-age population in the economy. Which effects will dominate and what will be the impact on the economy and the society as a whole? Saczuk (2013) argues that the need of replacement migration—use of international migration to offset population ageing (see e.g. Craveiro et al., 2019; Bou-Habib , 2018)—is questionable at best and may be a purely academic exercise given the political situation in Europe. Still, understanding the potential implications of demographic changes conditional of various migration scenarios is essential for academics, businesses and policy-makers alike. Yet such an impact assessment is difficult due to data limitations and requires new methods of forward-looking analysis able to combine the various exogenous changes while enabling detailed insights into the potential outcomes. This study develops a dynamic Overlapping Generations Computable General Equilibrium (OLG–CGE) model used to evaluate the micro- and macroeconomic impacts of population ageing and international migration in the context of the Czech Republic. This is done through computer simulations of scenarios assuming the baseline projected population ageing patterns as well as a return of a large number of Czechs working abroad and a sudden outflow of foreign workers. The model is calibrated in a high level of detail using the latest population and productivity estimates for 350 population groups differing in their education, occupation and sectoral affiliation, providing an accurate representation of the Czech economy and its internal economic mechanisms. The analysis clearly outlines the extent of the underlying population changes, characterised by a projected 5.4% decrease in the overall population size and over 6% decrease in the effective labour supply in the next 30 years (keeping the default retirement age unchanged), and 4-year increase in the average age. This results in higher unit labour costs and a shift towards production relying more heavily on capital across all industries and industrial sectors being affected by various levels of initial increase and subsequent decrease in demand for their products and services. As a consequence, the Czech economy would see lower economic growth and competitiveness at the international markets. The next section provides a brief overview of the relevant literature and sources for this study, followed by a description of the simulation model, its calibration, the three modelled scenarios with varying levels of migration, and an overview of the simulation results across a range of variables with a concluding summary.",1
60.0,2.0,Computational Economics,15 July 2021,https://link.springer.com/article/10.1007/s10614-021-10153-2,Economic Policy Uncertainty Index Meets Ensemble Learning,August 2022,Ivana Lolić,Petar Sorić,Marija Logarušić,Female,Male,Female,Mix,,
60.0,2.0,Computational Economics,26 July 2021,https://link.springer.com/article/10.1007/s10614-021-10154-1,The multiColl Package Versus Other Existing Packages in R to Detect Multicollinearity,August 2022,Román Salmerón Gómez,Catalina B. García García,José García Pérez,Male,Female,Male,Mix,,
60.0,2.0,Computational Economics,22 July 2021,https://link.springer.com/article/10.1007/s10614-021-10155-0,Bayesian Estimation of Agent-Based Models via Adaptive Particle Markov Chain Monte Carlo,August 2022,Thomas Lux,,,Male,Unknown,Unknown,Male,"Over the last decade, agent-based models in economics have reached a state of maturity that brought the tasks of statistical inference and goodness-of-fit of such models on the agenda of the research community. Using mostly relatively simple models of financial markets, a variety of statistical tools have meanwhile been developed to this end (cf. Lux and Zwinkels, 2018, for a review of this literature). Most available papers have used a frequentist approach adopting either likelihood-based algorithms or simulated moment estimators. In contrast, Bayesian estimation approaches can be found in very few papers only. This is surprising insofar as Bayesian methods have become the dominating paradigm in estimation of contemporaneous dynamic general equilibrium models in macroeconomics (cf., Herbst and Schorfheide, 2015). At the time of writing, to my knowledge only three contributions exist applying a Bayesian methodology to agent-based models: Grazzini et al. (2017), Lux (2018) and Bertschinger and Mozzhorin (2021). Grazzini et al. (2017) use both Markov Chain Monte Carlo (MCMC) and Approximate Bayesian Computation (ABC) to estimate the posterior of the parameters of a medium-scale macroeconomic model with heterogeneous expectations. In contrast to traditional MCMC, the ABC algorithm uses auxiliary measures of fit rather than the likelihood to sample from the posterior distribution (cf. Sisson et al., 2007). Since the likelihood is often not available in analytical form in agent-based models, the authors adopt a kernel density estimator to approximate this component of the MCMC algorithm. Lux (2018) applies both frequentist and Bayesian methods for estimation of two basic financial ABMs. All his implementations of various estimators are based on an approximation of the likelihood using the concept of a particle filter as introduced in the statistical literature by Gordon et al. (1993) and Kitagawa (1996). This approach presumes a state-space representation of the underlying model (which is natural for many ABMs). A particle filter approximation is initiated by sampling the initial values of the particles for the hidden states of the model from their unconditional distribution. The collection of particles is, then, updated via sampling-importance-resampling over the length of the available time series of the observable variables. Andrieu et al. (2010) show that when using the particle filter within MCMC the resulting Markov chain converges to the posterior under very general conditions on the structure of the likelihood and the proposal density. More generally, they demonstrate that using an unbiased estimate of the likelihood (which is, for instance, the case with the particle filter), leaves the equilibrium distribution of the posterior of the MCMC chain unchanged.Footnote 1 One advantage of using a particle filter approximation of the likelihood within an MCMC algorithm is that together with estimation of the posterior distribution it also allows for filtering of the time variation of the unobservable variables which are often at the center of interest in ABMs (e.g. agents’ expectations, opinions or strategies). One problem of traditional MCMC and PMCMC alike is finding a parametrization that leads to a reasonable acceptance probability for new draws from the proposal density. While with an analytical likelihood this requires an appropriate choice of the proposal density and its parameters, in PMCMC the number of particles enters as another factor that influences the acceptance rate as it determines the extent of approximation error of the likelihood. Most empirical applications first run a number of trials to determine an appropriate specification for the proposal density before turning to estimation proper. Doing so, Lux (2018) identifies scenarios with conventional acceptance rates for the three-parameter model of Alfarano et al. (2008), but admits that he had been unable to find a satisfactory combination of proposal density and number of particles for a second model with four parameters based upon Franke and Westerhoff (2012). He states that in all trial runs, the acceptance rate remained extremely low. While this would not impede the statistical validity of the approach as established by Andrieu et al. (2010), it makes its implementation impractical as very long simulation runs would be required to compensate for a low acceptance rate. The very problem of low acceptance rates is addressed by the third entry on this topic in the recent literature by Bertschinger and Mozzhorin (2021). These authors use so-called Hamiltonian MCMC for Bayesian estimation of two agent-based models. Hamiltonian MCMC attempts to find regions with high probabilities of acceptance by introducing an auxiliary momentum variable (cf., Neal, 2011). Adopting the principle of joint preservation of volume in conservative dynamic systems, the dynamics of the parameters and their associated momentum variables is modeled as a Hamiltonian system which when used to draw new proposals of the parameters, should select these approximately along an iso-line of equal probability and, thus, should guarantee high acceptance rates. Unfortunately, this approach requires numerical derivatives of the log probability density to implement the Hamiltonian dynamics as a conservative system of differential equations. This is particularly problematic in the present setting: First, in most agent-based models with a finite number of agents, the probabilistic elements in the individuals’ behavior will make any statistics derived from the underlying process non-smooth. For instance, if agents’ behavior is described by probabilities to switch from one behavioral alternative to another, even with a fixed sequence of random numbers, a discrete change will happen at a certain value of any one of the parameters of the model. Second, the same type of discrete changes happens in the particle filter in the resampling step when the acceptance of any particle to the new population is decided via multinomial draws based on their relative likelihood (implying that the Hamiltonian approach is generally not computable with a particle filter approximation). The first type of complication is not relevant for the models explored in Bertschinger and Mozzhorin (2021) since these are either based on a limiting case with an infite number of agents or on a dynamic process with two uniform groups of agents without consideration of individual agents per se. The examples in Lux (2018), however, do consider a finite number of autonomous agents and, thus, the elegant approach of Hamiltonian MCMC appears unfeasible because of the lack of smoothness of the numerical approximation of the likelihood. To make MCMC work, we, therefore, have to choose an alternative route: adaptive adjustment of the parameters of the proposal density. The rest of the paper proceeds as follows: The next section introduces the principles of PMCMC together with algorithms for the adaptive choice of the proposal density and delayed rejection. Delayed rejection generates a second proposal from a different transition kernel if the first one is rejected, and, thus, helps to increase the acceptance rate of the Markov Chain. Section 3 provides a short outline of the theoretical asset-pricing models we use as test cases for adaptive PMCMC. Section 4 provides Monte Carlo results on the performance of the algorithms, and Sect. 5 presents empirical results obtained for data of three major stock markets. Section 6 provides conclusions.",9
60.0,2.0,Computational Economics,30 July 2021,https://link.springer.com/article/10.1007/s10614-021-10156-z,Bayesian Estimation of the Skew Ornstein-Uhlenbeck Process,August 2022,Yizhou Bai,Yongjin Wang,Xiaoyang Zhuo,Unknown,Unknown,Unknown,Unknown,,
60.0,2.0,Computational Economics,31 July 2021,https://link.springer.com/article/10.1007/s10614-021-10157-y,Portfolio Correlations in the Bank-Firm Credit Market of Japan,August 2022,Duc Thi Luu,,,,Unknown,Unknown,Mix,,
60.0,2.0,Computational Economics,11 November 2021,https://link.springer.com/article/10.1007/s10614-021-10158-x,"Bounded Rationality, Group Formation and the Emergence of Trust: An Agent-Based Economic Model",August 2022,Jefferson Satoshi Kato,Adriana Sbicca,,Male,Female,Unknown,Mix,,
60.0,2.0,Computational Economics,28 July 2021,https://link.springer.com/article/10.1007/s10614-021-10159-w,A Finite Difference Scheme for Pairs Trading with Transaction Costs,August 2022,Zequn Li,Agnès Tourin,,Unknown,Female,Unknown,Female,"Generally, trading models with transaction costs are reputed difficult to analyze and cannot be solved in closed form (see or instance Curato et al. 2017). In particular, for Markov stochastic control type problems, the Partial Differential Equation approach leads to nonlinear and possibly degenerate Hamilton–Jacobi-Equations that require robustly designed numerical approximations. Our motivation in this paper is to demonstrate how these problems can be solved, at least in low dimensions, by using monotone Finite Difference methods that are known to converge to the viscosity solution of the Hamilton–Jacobi equation (see Barles and Souganidis 1991). A significant obstacle to a broad application of the theory of monotone approximations has been the lack of general procedure that would enable one to produce systematically a monotone numerical scheme for any Markovian control problem. In this article, we take a step in the direction of filling this gap, by designing a procedure for the derivation of a monotone Finite Difference scheme for the transaction costs term in the Hamilton–Jacobi equation. The basic idea is rooted into an earlier paper (Rouy and Tourin 1992), in which the second author had proposed a similar method for the eikonal equation, in geometrical optics. Generally, it consists in substituting first-order upwind Finite Differences into the Hamilton–Jacobi–Bellman operator, followed by the derivation of an analytical formula for the solution of the optimization problem, in terms of the forward and backward Finite Differences. The main model we consider in this paper is an extension of the pairs trading model proposed in Tourin and Yan (2013), incorporating constraints on the gross market exposure and transaction costs. Given a fixed time horizon and a portfolio of two cointegrated assets, the agent trades the spread between the two assets and the trading strategy is defined as the possibly negative portfolio weight maximizing the expected exponential utility derived from terminal wealth. Next, we only consider market-neutral strategies and we also impose lower and upper bounds on the number of shares of each asset held, in order to prevent a run-up in leverage whenever a large deviation from equilibrium occurs, ensuring the existence of a non trivial global solution for an arbitrary time horizon. Next, trades incur transaction costs comprised of explicit transactions fees and commissions and the implicit cost due to slippage. These costs are modeled as a function of the trading rate and respectively added or subtracted from the observable asset price at the time when a buy or a sell order enters the market. Since we are unable to solve this stochastic control problem in closed-form, we need to implement a numerical approximation to compute the optimal trading strategies. To this end, we apply our methodology to the successive derivation of monotone Finite Difference schemes for three different price impact models. The first two simply correspond respectively to the standard linear and square root cost functions; in the third one, we also include a linear market impact term affecting the observable market price for all the agents. Besides, we also explore in Sect. 4 the case without constraints and compute the corresponding trading policies by solving analytically the Riccati equation together with the set of linear Ordinary Differential Equations characterizing the trading policies. Note that in this simplified model, there is a possibility that the optimal value process may converge to 0 in finite time (see Benth and Karlsen 2005; Tourin and Yan 2013). Finally, we combine our monotone Finite Difference schemes with a Monte Carlo sampling method to illustrate the computed trading strategies, study their behavior as the number of replications and the time horizon increase,and analyze the effects of transaction fees and slippage on their performance. We refer to the books by Fleming and Soner (1993) and Pham (2009) for a general introduction to optimal stochastic control and its applications in Finance and to chapter 13 in Touzi (2013), for an introduction to monotone Finite Difference schemes. Furthermore, a general convergence result of a monotone scheme toward the viscosity solution of the Hamilton Jacobi Equation has been proven in Barles and Souganidis (1991). The application of stochastic control to pairs trading originated in the work of Mudchanatongsuk et al. (2008). Further models of time-consistent pairs trading strategies were developed recently in Chiu and Wong (2011), Tourin and Yan (2013), Ngo and Pham (2014), Lei and Xu (2015), and Leung and Li (2015). Besides, Hogan et al. (2004) provided a definition of the concept of statistical arbitrage and empirical measures to test the presence of opportunities. The standard Merton problem with constraints on portfolio proportions has been studied in Cvitanic and Karatzas (1992), Kogan and Uppal (2002). Furthermore, Grossman and Vila (1992), Cuoco (1997) considered constraints on the absolute amount invested in each asset. The first immediate market impact model was proposed in Almgren and Chriss (2001). Empirical market impact studies were carried out in Potters and Bouchaud (2002), Lillo (2003), and Almgren et al. (2005). The first model incorporating a square root market impact in the rate of trading can be found in Almgren (2003). A numerical approximation of a nonlinear market impact term in an optimal execution model, where the execution strategies are characterized as the solution of a nonlinear constrained optimization problem and computed by the Sequential Quadratic Programming (SQP) and the Generating Set Search (GSS) algorithms, was developed in Curato et al. (2017). Although these two algorithms are computationally intensive, they are still only able to locate local minima. Primbs and Yamada (2018) formulated a pairs trading problem with a mean-variance criterion and proportional transaction costs, in discrete time, as a convex quadratic program and also solved it by a suboptimal method, known as Model Predictive Control. Finally, an optimal portfolio optimization problem with linear and quadratic costs was proposed in Liu et al. (2017) and its solution was derived by using asymptotic expansions for small trading costs. Additional results for general nonlinear costs were recently obtained for a portfolio of assets following geometric Brownian motions in Guasoni and Weber (2015). The rest of the paper is organized as follows: in the next section, we start introducing the trading strategies; in Sect. 3, we formulate a stochastic control problem with state constraint and derive the corresponding Hamilton Jacobi Equation. In Sect. 4, we briefly lift the state constraint and we reduce the corresponding stochastic problem to solving a Riccati equation and two linear Ordinary Differential Equations. In Sect. 5, we describe our general method for deriving a Finite Difference scheme and we apply it to construct an approximation of our pairs trading problem. In Sect. 6, we briefly investigate an extension of our main model incorporating a price impact term. In Sect. 7, we present the numerical and simulation results we obtained.",3
60.0,2.0,Computational Economics,13 October 2021,https://link.springer.com/article/10.1007/s10614-021-10161-2,A Fitted L-Multi-Point Flux Approximation Method for Pricing Options,August 2022,Rock Stephane Koffi,Antoine Tambue,,Male,Male,Unknown,Male,"In finance, an option is a contract which gives to the holder the right but not the obligation to buy (call) or to sell (put) an asset at a specific price (strike) at a certain date in the future (expiry date). We have two main types of options which are European and American options. European options are options that can be exercised only at expiry date while American options can be exercised anytime before the expiry date. This flexibility of exercising American options leads to solve an optimal stopping time problem in the Black–Scholes framework which incorporates the early exercise. Many studies focused on the pricing problem of American options were conducted and the linear complementary problem approach was quite popular for pricing American options (see Kovalov et al. 2007; Zhang et al. 2009; Wang et al. 2006; Topper 2005). This approach brings us to solve linear complementary problem stated as follows (see Topper 2005): where \({{\cal L}}\) is the following Black–Scholes operator with r is the risk free interest, V is the option value at time \(\tau \), \(V^{\star }\) is the payoff, \(\tau =T-t\) with t and T respectively the instantaneous and maturity time. For \(i,j=1,\ldots ,n,\,S_i \) represents the asset i price, \(\sigma _i\) represents the volatility of asset i, \(\rho _{ij}\) represents the correlation between the assets i and j. Furthermore, (Wang et al. 2006) proposed a power penalty method to solve the linear complementary problem for pricing American options. The power penalty problem is formulated as follows: where \(\beta \) is the penalty parameter and k is the power of the method. Let us notice that, when we take the penalty parameter \(\beta =0\) in (3), we get the Black–Scholes Equation for pricing European options, with the operator \({{\cal L}}\) defined in (2). However, the power penalty problem (3) can not be solved analytically, therefore numerical methods are required for its resolution. Nevertheless, the Black–Scholes operator (2) is degenerated when the stock price approaches zero. This degeneracy can affect the accuracy of the numerical method used for the resolution. To tackle this problem, several methods have been proposed. The fitted finite volume method, proposed by S.Wang in Wang (2004) whereby a rigorous proof of convergence is provided, appears to be more attractive. Moreover, the fitted finite volume method has been used for the resolution of the two dimensional second order Black-Scholes PDE followed by the convergence proof in Huang et al. (2006). In spite of the fact that the fitted finite volume methods perform well for the resolution of the Black-Scholes PDE, they are only of order 1 with respect to asset price variable. Besides, the fitted O-Multi-Point Flux Approximation (O-MPFA) method has been proposed in Koffi and Tambue (2019a) to overcome the degeneracy problem of the Black–Scholes PDE. It has been shown that the O-MPFA is more accurate than the classical fitted finite volume method by Wang (2004). However, the O-MPFA is heavy (9 points stencil method) and for more general grids, the convergence rate of the O-MPFA method may decrease (see Aavatsmark 2007). In this paper, we focus on the L-MPFA method which is based on the approximation of a linear function gradient defined over a given triangle and the continuity of flux through the edges of this triangle. Indeed, the L-MPFA method is a 7 points stencil method while the O-MPFA is a 9 points stencil method. This shows that the O-MPFA method can be computationally more expensive than the L-MPFA method. Moreover, for more general grids, the order reduction in convergence rate is larger for the O-MPFA than the L-MPFA (see Aavatsmark 2002). Thereby, to approximate the solution of the second order Black-Scholes operator, we couple the L-MPFA method with the upwind methods (first and second order). Besides, the degeneracy of the Black-Scholes operator (2) is handled by the fitted finite volume,(Wang 2004), when the stock price is approaching zero. The L-MPFA method coupled with the upwind methods (\(1{st}\) and \(2{nd}\) order) is used to approximate the solution of (3) when the Black-Scholes operator is not degenerated. We call fitted L-MPFA method that combination of the L-MPFA method and the fitted finite volume method. Numerical simulations show that the new fitted L-MPFA method is more accurate than the fitted O-MPFA method developed in Koffi and Tambue (2019a) and the standard fitted finite volume method developed in Huang et al. (2006). Note that in dimension one, the L-MPFA method and the O-MPFA method are identical and are well known as Two Point Flux Approximation (TPFA). The rigorous convergence proof of the fitted TPFA for pricing options is provided in Koffi and Tambue (2019b). Note that the standard extension in high dimension of TPFA while keeping the two point flux approximation is very complicated on general grid and can only converge on M-orthogonal grids for simple diffusion problems (see Tambue 2016). Note that L-MPFA schemes are very different to O-MPFA schemes and the current paper is not a simple extension of the work in Koffi and Tambue (2019a). The key differences can be summarized in the following points: Here, we have considered American and Europeans options rather than Europeans options in Koffi and Tambue (2019a). Note that pricing American put options is very complicated as the exact solution does not exist even for constant coefficients. The American options pricing here is modeled by the nonlinear degenerated parabolic PDEs (4). We can observe that the current novel fitted L-MPFA is very robust for such degenerated nonlinear parabolic PDEs, where we have coupled with implicit time stepping methods. The corresponding nonlinear algebraic equations have been solved using a modified Newton method as the corresponding functions are not differentiable. The L-MPFA schemes presented here in two dimensional domain only have one additional cell added to the TPFA schemes, while still being able to handle general grids. For more general grids, the order reduction in convergence rate will be lower for the L-MPFA schemes than the O-MPFA schemes and few oscillations are expected here in the non-monotone parameter regions. The paper is structured as follows. In Sect. 2, we present the power penalty problem with the corresponding initial and boundary conditions. The spatial discretization of the linear operator is developed in the Sect. 3. Details on the L-MPFA method of the diffusion term discretization are provided. The convection term is discretized using the upwind methods (\(1{st}\) and \(2{nd}\) method). At the end of the Sect. 3, the novel fitted MPFA method is provided. The \(\theta -\) Euler method is used for the time discretization method in the Sect. 4. Numerical experiments are presented for the different numerical methods in Sect. 5. The conclusions of our study are drawn in the last Section.",1
60.0,2.0,Computational Economics,05 October 2021,https://link.springer.com/article/10.1007/s10614-021-10162-1,Maximum Likelihood Estimation for the Asymmetric Exponential Power Distribution,August 2022,Mahdi Teimouri,Saralees Nadarajah,,Male,Unknown,Unknown,Male,"Modelling data using heavy tailed distributions has a long history and becoming increasingly popular in economics and finance. A great deal of effort has gone into investigating the applications and properties of heavy tailed distributions in both fields of study. One of the most important classes of heavy tailed models that accommodates both of tail thickness and skewness parameters is the \(\alpha \)-stable distribution. This distribution was introduced in terms of its characteristic function (chf). Let \(S(\alpha ,\beta ,\sigma ,\mu )\) denote an \(\alpha \)-stable random variable with parameters \(\alpha , \beta , \sigma \) and \(\mu \). The chf of \(S(\alpha ,\beta ,\sigma ,\mu )\) is given by Nolan (1998): where \(i^{2}=-1\) and \(\mathrm{sign(u)}\) is the sign function that takes values -1 and +1 when \(u < 0\) and \(u\ge 0\), respectively. The distribution is characterized by four parameters: tail thickness \(\alpha \in (0,2]\), skewness \(\beta \in [-1, 1]\), scale \(\sigma \in {\mathbb {R}}^{+}\), and location \(\mu \in {\mathbb {R}}\). If \(\alpha =2\), we have a normal distribution that has the lightest tail thickness. The tail thickness increases when \(\alpha \) decreases. The Cauchy distribution is obtained when \(\alpha =1\). For \(\alpha =1/2\) and \(\beta =1\), we have the Lévy distribution that is totally asymmetric (skewed) to the right. The direction and degree of asymmetry are adjusted by the parameter \(\beta \); for instance, if \(\beta =1\) (or \(\beta =-1\)), we have a totally skewed to the right (or left) \(\alpha \)-stable distribution. If \(\beta =0\), we have the symmetric \(\alpha \)-stable distribution. The parameter \(\sigma \) is not the standard deviation since the second moment of the \(\alpha \)-stable distribution is not finite for \(\alpha <2\). If \(\alpha <1\) and \(\beta =1\), then the support of the \(\alpha \)-stable distribution would be \((0,\infty )\), i.e., \(P(Y >0)=1\). For this reason, \(S(\alpha /2,1,\sigma ,\mu )\) is called a positive \(\alpha \)-stable random variable. (Devroye 2009) Let W denote a polynomially tilted \(\alpha \)-stable random variable with tilting parameter equal to 1/2. The probability density function (pdf) of W is given by for \(w > 0\), where \(f_{P}(\cdot )\) denotes the pdf of \(P\sim S\left( \alpha /2, 1, \left[ \cos (\pi \alpha /4)\right] ^{2/\alpha }, 0\right) \). We use \(W\sim {PT}(\alpha )\) to denote a polynomially tilted \(\alpha \)-stable random variable with tail thickness \(\alpha \in (0, 2]\) and pdf given in (1). We refer readers to Fama (1963), Mandelbort (1963), Fama (1965), Rachev and Mittnik (2000) and Rachev (2003) for comprehensive accounts of theory and applications of the \(\alpha \)-stable distribution. Although the class of \(\alpha \)-stable distributions possesses fascinating properties, it has two main drawbacks. The first one is the lack of closed form expression for the pdf except for the three particular cases noted. As the second one, since the pdf of the \(\alpha \)-stable distribution has no closed from expression, computing the maximum likelihood (ML) estimator is mathematically intractable and computationally difficult. Many if not most numerical algorithms for computing ML estimates are based having an expression for the pdf. These two issues are the main obstacles for the use of the \(\alpha \)-stable distribution in practice. Therefore, several efforts have been made to find a class of distributions that possesses important characteristics such as tail thickness and skewness. For this purpose, a variety of asymmetric distributions (e.g. Azzalini, 1985; Fernandez et al., 1995; Mudholkar & Hutson, 2000; Eugene et al., 2002) have been introduced in the literature. The class of asymmetric exponential power (AEP) distributions introduced in Fernandez et al. (1995) accommodates both tail thickness and skewness parameters. Let Y denote an AEP random variable. Its pdf is given by where \({\varvec{\theta }}=(\alpha ,\sigma ,\mu ,\epsilon )\) for \(0<\alpha \le 2\), \(\sigma \in {{{\mathbb {R}}}}^{+}\), \(\mu \in {{\mathbb {R}}}\), \(-1<\epsilon <+1\), and for \(u > 0\). The cumulative distribution function (cdf) of Y is given by where for \(\nu >0\). The parameters \(\alpha \), \(\sigma \), \(\mu \) and \(\epsilon \) control tail thickness, scale, location, and skewness, respectively. The direction and degree of asymmetry are controlled by \(\epsilon \); for example, we have a totally skewed to the right distribution as \(\epsilon \rightarrow 1\), a totally skewed to the left distribution as \(\epsilon \rightarrow -1\), and a symmetric distribution if \(\epsilon =0\). It is worth noting that the AEP distribution can be regarded as a generalization of the epsilon skew normal distribution introduced by Mudholkar and Hutson (2000). The AEP distribution has received much attention in economics and its properties have been studied in several papers (e.g. Theodossiou, 2015; Komunjer, 2007; Hsieh, 1989; Nelson, 1991; Duan, 1999; Ayebo & Kozubowski, 2003; Christoffersen et al., 2010; Zhu & Zinde-Walsh, 2009; Zhu & Galbraith, 2010; DiCiccio & Monti, 2004). The aim of this paper is to propose an expectation–maximization (EM) algorithm for computing the ML estimates for the parameters of the AEP distribution. As we know, when the ML estimator has no closed form expression, the ML estimate is obtained by maximizing the log-likelihood function through an iterative algorithm such as Newton-Raphson. Unfortunately, there is no guarantee that a Newton-Raphson algorithm will converge even when the initial values are the true parameters. The ML method based on iterative methods may sometimes fail to converge. Dempster et al. (1977) proved that the EM algorithm for finding the ML estimate always converges. As an application, we perform a robust linear regression modelling with errors following the AEP distribution. The EM algorithm is employed for estimating the regression coefficients and their asymptotic standard errors. This paper is organized as follows. The EM algorithm is proposed in Sect. 2. Section 3 is devoted for determining initial values, robustness analysis with respect to the initial values, determining the stopping criteria, and demonstrating the performance of the EM algorithm through simulation and analyzing a real data. Robust linear regression analysis is described in Sect. 4. We conclude the paper in Sect. 5.",
60.0,2.0,Computational Economics,30 July 2021,https://link.springer.com/article/10.1007/s10614-021-10163-0,Communication and Learning: The Bilateral Information Transmission in the Cobweb Model,August 2022,Eran Guse,M. C. Sunny Wong,,Male,Unknown,Unknown,Male,"Communication is one of the fundamental human behaviors that are of great interest to economic researchers. Daily decisions are often made based on information obtained by communicating with friends or acquaintances or watching the news. While the additional information can be potentially useful, miscommunication can commonly occur for various reasons: inattention, lack of interpretational skills, or inadequate delivery from the speaker. The imperfect communication process may produce uncertainty or volatility in the economy, confounding people to make optimal decisions (Granato et al., 2008, hereafter GGW). As a result, people have incentives to enhance effective communication and improve comprehension via learning. This paper investigates how communication and learning mechanisms can affect forecast behaviors and accuracy in a self-referential dynamic market. Game theorists and experimental economists have extensively investigated how communication, considered a signaling process, can promote coordination (Schelling, 1960).Footnote 1 Farrell (1987, 1988) examines the effect of structured costless communication on coordinating the equilibrium among players in a simple sequential game—the Battle of the Sexes—with complete information. Players are allowed to make nonbinding announcements about their intentional decisions. Farrell considers two possibilities: Only one player can send a message of his/her intention (this is called a one-way communication), and both players can send messages to each other (a two-way communication). The author shows that one-way communication efficiently coordinates players” beliefs and reaching equilibrium. On the other hand, two-way communication may result in miscoordination. However, multiple rounds of two-way communication are more effective than single rounds (Cooper et al., 1989, 1992; Rabin, 1990, 1994). Researchers have been further motivated to develop various experimental designs to study the impacts of communication on coordination effectiveness. Cooper et al. (1989, 1992) present experimental evidence on nonbinding and preplay communication in coordination games.Footnote 2 Consistent with Farrell’s (1987, 1988) findings, Cooper et al. show that the Pareto-dominant equilibrium is more likely to achieve with one-way communication than the no communication circumstance in the cooperative coordination game (CCG). Interestingly, two-way communication possibly increases the likelihood of coordination failures. The authors further consider an alternative game, called the simple coordination game (SCG). They find that two-way communication consistently leads to the Pareto-dominant Nash equilibrium, while one-way communication does not (Rabin, 1990, 1994). Ellingsen and Östling (2010) extensively investigate how communication can facilitate coordination in a larger class of games with one-way and two-way communication. They find that one-way and two-way communication promote coordination: When both players make multiple levels of thought processes, they coordinate and achieve the Nash equilibrium (Crawford, 2003). Furthermore, Fonseca and Normann (2012) design an experiment investigating how communication influences pricing behaviors in a Bertrand oligopolistic market. The authors show that the communication process helps firms yield higher profits. They also conclude that communication facilitates collusive pricing mechanisms and conflict mediation. There are more recent surveys presenting the literature on the effects of communication on coordinating behaviors. For example, Camerer and Weber (2012) and Kriss and Weber (2013) discuss the literature on communication in organizational economics with experiments. Brandts et al. (2019) contribute a survey of communication literature on laboratory experiments. While the research on communication continues to flourish in game theory and experimental economics, the tools for analyzing the interaction between communication processes and economic outcomes remain limited in the mainstream literature (Babe, 2018). An alternative approach to communication, considered an information acquisition process, slowly enters mainstream economics, particularly macroeconomics and computational economics. While the substantive ideas can be found in now-classic works in the political economy literature, such as Lazarsfeld et al. (1944), the modern communication and information acquisition literature is relatively young. For example, financial economists have studied explanations for herding behavior in which rational investors demonstrate some degree of behavioral convergence (Devenow and Welch, 1996). Studies in monetary economics investigate how information diffusion influences economic forecasting behavior (Romer and Romer, 2000; Muto, 2011). In the literature of political economy, Lazarsfeld et al. (1944) and Katz and Lazarsfeld (1955) suggested a two-step flow of communication model where the information (political ideas) flow from the media to the “opinion leaders,” and then from the opinion leaders to the less-informed population called the “issue publics.” Inspired by Lazarsfeld et al. (1944), Granato and Krause (2000) investigate the flows of information acquisition empirically. Based on monthly surveys of inflation expectations, the authors find that educational differences contribute to asymmetric information diffusion. The expectations of more educated groups shape the expectations of the less educated groups. Recent studies in macroeconomics and computational economics have focused on information acquisition through a specific learning mechanism called adaptive learning. The adaptive learning approach allows individuals to learn stochastic processes by updating their forecasts (expectations) over time as new information becomes available (Evans and Honkapohja, 2001).Footnote 3 The theory of adaptive learning has been recently implemented to study macroeconomic policy (GGW, 2008; Muto, 2011; Evans and McGough, 2018; Mitra et al., 2019). Researchers also investigate different scenarios of adaptive learning processes under a widely-studied model known as the cobweb model. The cobweb model was initially introduced to explain fluctuations of market equilibrium where the supply of a single good is affected by a production lag.Footnote 4 It has been comprehensively studied in the dynamic macroeconomic literature (see Muth, 1961; Arifovic, 1994; Evans and Honkapohja, 2001; Branch and McGough, 2008; Pfajfar, 2013; Evans and McGough, 2020). It is assumed that there are n firms in a competitive market producing a homogeneous product. Since the production process takes time, firms need to decide the output level before the actual market price can be observed. As a result, the optimal production level is determined by maximizing firms” expected profits according to their (rational or nonrational) expectations of the next period’s market price. Previous studies of the cobweb model show that a unique equilibrium exists when firms (agents) form either homogeneous or heterogeneous expectations (Evans and Honkapohja, 1996). Several versions of the cobweb model have been studied in the context of adaptive learning literature. For example, Evans and Honkapohja (1996) allow N types of agents to form different expectations with different parameter estimates in the same structural forecasting rule. Giannitsarou (2003) studies the stability of rational expectations equilibria (REE) under heterogeneous adaptive learning in an economy with a homogeneous structure and finds that the stability conditions under heterogeneous learning are not necessarily the same as those under homogeneous learning. Pfajfar (2013) introduces the assumptions of information frictions and heterogeneity in expectations formation to examine the equilibrium properties in the cobweb model. The author finds that intrinsic heterogeneity exists in the model where “the forecasting rules are not equally costly and do not exhibit identical performance in the long run” (p.1434). While acquiring information through the adaptive learning process is reasonable, the vast majority of studies assume that individuals gather their information to forecast independently (Evans and Honkapohja, 1996; Honkapohja and Mitra, 2003; Guse, 2005, 2014; Branch and Evans, 2007). This independent forecasting process is rather a strong assumption, which does not allow agents to interact with each other. This assumption holds only if all agents obtain identical information. However, due to information heterogeneity (Granato and Krause, 2000; Pfajfar, 2013), it is natural to believe less-informed individuals have incentives to acquire advanced information to improve their forecast accuracy. In fact, information acquisition can be costly (Stigler, 1961; Aidt, 2000).Footnote 5 Individuals may consider acquiring advanced information through the communicative process. To date, few attempts have been made to investigate the role of information diffusion in the expectations formation in the cobweb model. GGW (2008), which examine the process of information interactions across firms in the cobweb model, is an exception. The authors present an interactive cobweb model that allows for both information heterogeneity and information diffusion. They assume that leading firms (leaders) make initial forecasts based on exogenous information, while following firms (followers) form forecasts by observing leaders” expectations with the possible interpretation or measurement errors. We call this one-way communication process the uni-directional information diffusion (UID). This interactive economy with the information transmission from leaders to followers results in a possibility of up to three equilibria and expands the parameter space of potential learnable equilibria. Equally important, the authors discover the “boomerang effect” where followers” inability to observe leaders” expectations correctly causes a reduction in leaders” forecasting efficiency. GGW (2008) point out that the boomerang effect occurs because leaders fail to realize that the followers” interpretation errors create excess volatility. GGW (2008) have put forward the hypothesis that social interactions influence expectations formation and market equilibria. However, their model has a critical limitation: The UID process prohibits leaders from acquiring further information in the economy. Intuitively, suppose the leaders are aware of their forecasting accuracy decline due to the information transfer. In that case, they should have incentives to observe the followers” behavior to improve their forecast accuracy. Realistically, effective communication should be a two-way process: sending messages and receiving feedback. This paper investigates equilibrium properties, learnability, and forecast accuracy in an economy under a more logical mechanism of information diffusion. We develop a two-way information exchange procedure called the bi-directional information diffusion (BID) process. This two-way communication process allows the leaders to further revise their expectations by observing the followers” forecasts after the followers receive the leaders” initial expectations. Under the BID process, we find a possibility of up to five equilibria in the model. For some parameter values, the space of at least one learnable solution is expanded. More interestingly, we show that the BID process improves forecast efficiency for both leaders and followers. The boomerang effect disappears if leaders correctly observe followers” forecasts to revise their expectations. This BID process is particularly important in the literature of macroeconomics and monetary policy. Fisher and Robertson (2016) point out that the market responses to the Federal Reserve’s information provide “a wealth of information to policymakers, ... and those generally interested in monetary policy.” Muto (2011) investigates the expectational stability conditions of REE in a standard New Keynesian model, where the public forms expectations based on the central bank’s forecast in the process of adaptive learning. The author shows that the central bank must respond to the public’s inflation expectations more aggressively than the level suggested by the Taylor Principle to ensure the model is expectational stable (Geiger and Sauter, 2009). This paper is organized as follows. Section 2 describes the interactive cobweb model. We first briefly discuss the uni-directional information diffusion process suggested by GGW (2008). Section 3 presents a modified interactive model with the process of BID, where leaders can further revise their expectations by observing followers” forecasts after followers receive the initial expectations from leaders to form expectations. In Sect. 4, we adopt a numerical approach to study the number of steady states and the stability conditions of the model. We also demonstrate the dynamics of equilibrium using simulations. Section 5 analyzes the forecast accuracy for both leaders and followers under the BID’s process, and Sect. 6 concludes.",
60.0,2.0,Computational Economics,05 August 2021,https://link.springer.com/article/10.1007/s10614-021-10164-z,Estimation of Expected Shortfall Using Quantile Regression: A Comparison Study,August 2022,Eliana Christou,Michael Grabchak,,Female,Male,Unknown,Mix,,
60.0,2.0,Computational Economics,25 July 2021,https://link.springer.com/article/10.1007/s10614-021-10165-y,Averages: There is Still Something to Learn,August 2022,José Dias Curto,,,Male,Unknown,Unknown,Male,"The traditional ways to deal with outlying observations in empirical Economics and Finance is to exclude them (by trimming or winsorizing), or by computing statistics robust to outliers: the median and inter-quartile range, for example. Due to their importance, however, there are situations where the exclusion of the observations is not reasonable and may even be counterproductive. Suppose that we compute the average of stock prices of companies listed in the “Information Technology” sector of Standard and Poor’s 500 (S&P 500), including Amazon, Google, Microsoft and Apple, among many others. As the stock prices of Amazon and Google are much higher in comparison to the others, would it make sense to exclude these two companies from the analysis or should we give them a very low weight to compute the average? In economic terms, maybe neither of these options seems a reasonable decision because Amazon and Google are third and fifth in the list of the 10 largest components of the S&P 500. An alternative way would be to keep the two companies in the data set and to compute a measure that does not penalize the higher observations as much as the median, harmonic and geometric averages. The adjusted median, the measure proposed in this paper, meets that purpose because while the influence of higher data points is not as high as in the arithmetic average, it nevertheless gives more weight to the higher observations than the median and the other two averages. The role of an average is to represent a data set meaningfully, and the decision to choose the appropriate average to represent the central tendency of a distribution is an old and yet highly topical matter, in view of what was written by Coggeshall a long time ago (1886, p. 84). His contention was that the mean commonly employed by the economist is not a real quantity at all, but is a quantity assumed as the representative number of others that differ from it to a greater or lesser extent. Its fictitious character renders it possible to choose from among different values, and thus among different methods of finding it. Even in terms of mental images, the word average can lead to different meanings, reinforcing the doubts when we think on it (Kaplan et al. 2010). When asked to compute an average,Footnote 1 many practitioners assume the arithmetic mean is what is called for. However, and very often, they are not aware that better alternative approaches are available to capture the central tendency of a distribution (Coggeshall 1886), namely the geometric and harmonic means. Knowing which one to use for your data means understanding their differences. For example, in the case of ratios the choice of averaging method does matter, and sometimes the much less familiar harmonic mean provides a more logical approach to averaging the ratio between two magnitudes (Agrrawal et al. 2010). For square contingency tables, Nakagawa et al. (2020) proposes, as an alternative to the weighted arithmetic mean to represent the degree of departure from the marginal homogeneity, a measure which is expressed as a weighted geometric mean of the diversity index. The averages, and the central tendency measures in general, continue to be used in most of scientific fields. In Economics and Finance, e.g., Gan et al. (2020), Wellalage and Fernandez (2019) and Cheuk and Vorst (1999). In Education, e.g., Assari et al. (2020). In Mathematics, Statistics and Econometrics, e.g., Maki and Ota (2020), Kolahdouz et al. (2020), Priam (2020), del Barrio et al. (2019) and Gou et al. (2019). In Climate Change, e.g., Lyu et al. (2020). In Complexity Systems, e.g., Wu et al. (2020). In Psychology, e.g., Wenzel and Kubiak (2020). In Logistics, e.g., Choi et al. (2019), just to mention a few. In view of their importance, the main purpose of this paper is to clarify differences regarding averaging methods. The contribution we make is fivefold. First, we show in Sect. 2 that the harmonic mean is equivalent to a weighted arithmetic average, where the weights are inversely proportional to the original values, and they are computed in such a way that the contribution of each value to the final average is exactly the same. Thus, the weights compensate the original values to make the contribution of each value equal to the final average. We also generalize the new formula, taking the harmonic and arithmetic averages as particular cases. Second, different central tendency measures give different interpretations of the center of a distribution. In Sect. 2.2 we show that the median is the center of the distribution in terms of the observations counted, no matter the value of the observations. The arithmetic mean is such that the absolute deviations to the right of it are compensated by the absolute deviations on its left. So, the center is defined in terms of the absolute deviations (or distances) between each value and the arithmetic average. The geometric average defines the center of the distribution in terms of the compounding relative (percentage) deviations. The negative deviations in relative terms are balanced by the positive ones. Finally, the harmonic mean defines the center of the distribution in order that the weighted deviations on its left compensate the weighted deviations on its right, and the weights are inversely proportional to the original values. Third, the traditional central tendency measures do not properly handle outlying observation. The arithmetic average is dominated by outlying observations. The insensitivity of the geometric and harmonic averages to outliers can obscure large values that may be consequential. Finally, the median does not use all available data and can be misleading with regard to distributions with a long tail because it discards so much information. Due to the drawbacks of traditional measures, we propose a modified one—the adjusted median—in Sect. 3. The adjusted median originates an intermediate value between the median and the arithmetic average, giving more weight to the higher observations than the median and the other two averages. However, the contribution of each value to the final result is not exactly the same as in the harmonic average. Monte Carlo simulation shows the intermediate position of the adjusted median. We also propose a simple measure of skewness, taking the median as the reference. Fourth, to compute and compare the measures based on real economic data, we use the daily stock price of 56 companies listed in the “Information Technology” sector of the S&P 500. The data set includes Amazon, Google, Microsoft and Apple, among many others. We show that the adjusted median represents the center of the daily stock price distribution without excluding or giving a very low weight to the outlying observations. Finally, we provide the R code to perform the calculations arising in this study. The outline of the paper is as follows. First, we review the traditional averaging methods suggested in statistics textbooks, and used by academic researchers and practitioners. We also discuss some particularities of the means leading to a better interpretation and understanding. A new measure, the adjusted median, is proposed in Sect. 3. Monte Carlo simulation studies are performed in Sect. 4 to show the location of this measure and a real data example is also considered in Sect. 5. Finally, we present our concluding remarks.",1
60.0,2.0,Computational Economics,14 June 2022,https://link.springer.com/article/10.1007/s10614-022-10283-1,A Comprehensive Study of Market Prediction from Efficient Market Hypothesis up to Late Intelligent Market Prediction Approaches,August 2022,Amin Aminimehr,Ali Raoofi,Amirhossein Aminimehr,Male,Male,Unknown,Male,"Predictability of stock market has been argued for a long time. Before the claim of Fama, there were different researches held by various scholars about the predictability of stock market, but 1965 was the milestone in the literature of this area in finance. In that year, Fama and Samuelson stated their own beliefs on the behavior of financial time series. Fama, for the first time, suggested the term “efficient market”, and stated that price time series in strong efficient markets follow a random walk process. At the same year, Samuelson suggested the martingale stochastic process instead of random walk for financial time series behavior. Besides the differences of their point of view, they both concur on unprofitability of predictions in efficient stock markets especially with the advantage of historical data; see Fama (1970) and Samuelson (2013). Accordingly, studying the evidence of efficient market hypothesis (EMH) to determine the degree of efficiency in different markets is highly recommended in prediction surveys. Although many researchers have applied various methodologies with the aim of studying market efficiency; see Abeysekera (2001), Dias and Peters (2020), Dickinson and Muragu (1994), Dima and Miloş (2009), Gordon and Rittenberg (1995) and Pele and Voineagu (2008) this investigation does not seem to be straightforward. Surveys held on EMH may have lacked considering the nonlinear behavior of data; see Basu (1977), Busse & Clifton Green (2002), Jensen (1978) and Rosenberg et al. (1985), as a result they may have falsely accepted the efficiency of market in different levels. In this regard, a research tested the random-walk hypothesis with volatility-based specification test. They found strong evidence for rejecting random walk hypothesis in weekly data of CRSP stock market. They also stated the evidence of long-term memory and predictability to some extends; see Doan and Lo (1988) and Lo (1991). Furthermore, Broock et al. (1996) developed another test called BDS with the aim of effectively studying independently and identically distribution (IID) in the behavior of data. This test could more effectively study financial time series and their behavior with a non-linear approach, as a result studying EMH in different researches became deeper. Despite the developments in the approaches assessing market efficiency, there are still many discussions in this regard. From 1980, a group of researchers developed the idea of investors behavioral effects on market movements. This idea was first witnessed by Shiller (1981) while studying the movements of Standard & Poor (S&P) return after every time a new information was supplied in the market. He could observe excess unexplained volatility (by the literature of classic EMH) after each release of information. In this regard, in 2004, Lo explained a new definition for efficiency in financial markets with the term adaptive market hypothesis (AMH). In this definition, as investors behavior is not considered rational but a combination of complicated psychological factors; see Barber and Odean (2000) and Shleifer (2000), there may be periods of time that market spends on adapting to new circumstances; see Lo (2004). This branch of literature, nowadays, is also continued with researches on investor’s sentiments. After the developed notion by Lo, approaches upon studying the efficiency of market changed. It seems that by the developments in the machine learning knowledge extraction algorithms, or the availability of quantitative and qualitative data like what is available on social media, attitudes towards the level of market efficiency has changed. Accordingly, the efficiency of those markets that were confirmed may not still be confirmed. As a result, there is a rich literature devoted to the area of finance which takes use of mathematics and statistics with the aim of modeling and predicting financial time series. These methods vary from early statistic and econometric methods up to the late machine learning (ML) and evolutionary based optimization techniques; see e.g. Kanas and Yannopoulos (2001), Kim (2003, 2006), Park et al. (2010), Rather et al. (2015) and Han et al. (2020). There are many studies aiming at modeling financial time series with different frontier methods, however it should be kept in mind that financial predictions should be performed consciously and cautiously by the pre-studies conducted on data. Thus, in this research attempts are made to predict S&P 500 index with the latest advanced Deep Learning (DL) methods. The novelty of this research is that it’s emphasize is on the pre-estimations of data and the process of feature engineering. The remainder of the paper is organized as follows. In Sect. 1.2 the literature of financial predictions using machine learning methods and their benefits against classic methods is summarized. Section 2 explains the materials including the statistical tests and methods used in the study. Section 3 is related to the studies on the characteristics of the used data. Section 4 explains the methodology and the conducted experiments for validating different feature engineering methods. Section 5 is devoted to explaining the metrics and statistical test used for validating the composed models. Section 6 exposes the results and discussions. Finally, Sect. 7 concludes the study. Artificial Neural Networks (ANN) are a class of complex computing models inspired from human’s neural network system, and are a class of ML algorithms. As human’s brain can solve many complex problems by learning the underlying relations and patterns between variables, ANN’s are expected to solve many problems that earlier methods couldn’t. This simulation of the biological process of thinking and solving has been used in different subjects for about 80 years; see Adya and Collopy (1998), McCulloch and Pitts (1943) and Tsaih et al. (1998). Besides, ANNs have been even successful in predicting cycles that included crisis periods with an accuracy margin error of bellow 5% Yavuz et al. (2015). Although many believe that ANNs unconditionally can perform better than conventional statistical methods in modeling financial time series, inconsistent results of recent literatures; see McAleer and Medeiros (2011) in using them on financial markets has led to controversy amongst scholars. Inconsistent results in the success of ANN’s against earlier methods is due to various reasons such as inappropriate model implementation (for example model selection and hyperparameter selection), and neglecting to study the characteristics of data. Nowadays, with the developments of artificial intelligence (AI) in Deep Neural Network’s and model implementing techniques, hopes are increasing to solve the first problem. Keeping in mind that the answer to the second problem lies in studying the data generating process (DGP). It seems that in some cases when there is not a linearity and non-linearity specification on data, controversial results occur. For example, in some cases while ANN’s are struggling with unnecessary complexity of data, linear nature in the relation of features and target value, makes linear classic models more effective in modeling them. This problem can be prevented by an adequate study on linearity and non-linearity characteristic of data which results in a more coherent model selection. Eğrioğlu and Fildes (2020) have considered this specification through an input significance test in their research. After all, it is obvious that conventional statistical methods have many limitations in studying financial data. Problems such as high number of computations, losing degree of freedom, facing redundant and omitted variables, and multicollinearity between input features. In financial predictions, due to the high number of variables, there is a need for a clear and strong feature selection approach to avoid the problems mentioned above. Unfortunately, classic models lack an effective procedure of feature selection too. consequently, scholars used to select features either by chance, or by calculating linear correlation and maybe earlier literatures recommendations. For example, Hajizadeh et al. (2012) mentioned in their paper that the exogenous input variables for their hybrid model is selected from the recommendations of some earlier researches. Besides, stock market is a dynamic system that may change its dependency pattern through time and in different economic situations, therefore, there must be a suitable feature engineering approach for a better prediction. Moreover, although GARCH family models are known for their non-linear modeling capability, they are designed for modeling volatility instead of return and cannot sufficiently capture long term dependencies in data. Therefore, classic models seem to be insufficient at modeling financial time series. As a result, many scholars have inevitably switched to more complicated methods which can better capture long-term memory. Fortunately, along with the developments of big data and data science, new procedures are suggested to deal with the problems that earlier methods couldn’t. For example, there are various methods proposed for feature engineering before deploying prediction models. Within feature engineering approaches, there are some methodologies concentrating on the curse of dimensionality. Feature extraction and feature selection are two branches of feature engineering with the aim of dimensionality reduction. The main difference between these two approaches is that in feature selection, subsets of the original features are provided by the method, but in feature extraction brand new feature are created regarding to the initial features. Random Forest (RF); see Tin Kam (1995) and Principal Component Analysis (PCA); see Gastpar et al. (2006) respectively are well known examples of each feature engineering methods introduced. In addition, lately, intelligent feature engineering is performed by deep neural networks like Recurrent Neural Network’s (RNN) and Convolutional Neural Networks (CNN); see LeCun and Bengio (1998). In another words, deep neural networks have provided a feature extraction procedure in addition to their mapping functionality through the training process. Conventional simple neural networks seem to not provide this feature extraction process, as a result many scholars have switched to CNN’s and RNN’s like Long Short-Term Memory (LSTM); see Hochreiter and Schmidhuber (1997) networks for their studies. Literature of finance and deep learning have had many contributions in recent years. Various methods of DL models have penetrated through diversified subfields of finance such as algorithmic trading, risk assessment, fraud detection, portfolio management, asset pricing, financial statement analysis, text mining, behavioral finance, etc.; see Ozbayoglu et al. (2020). In Table 1 there is a list provided of recent literatures of machine learning and more specifically DL methods used in finance. With the advanced developments in programming languages especially python and its specific libraries of data science such as Sikitlearn, Pytorch, Tensorflow, Scipy etc., running various types of models have become extremely convenient. Hence, the main concern of this paper is not to seek for a new model or to create one, but to manage a unique methodology that completes the earlier researches of financial forecasts. The main aim of this paper is twofold. First is to sufficiently study the characteristics of financial time series and then better incorporate explanatory variables in predictions. The stated criteria are lacking in many researches dedicated to stock prediction, however studying them results in more reasonable, accurate and reliable predictions. In this research, first, investigations are conducted to gain a primary perception on the characteristics of the studied data which gives an insight on the process of data generation. Through this step EMH is accordingly studied on the S&P 500 data. Next, attempts are made to improve the accuracy of a specific ML model (LSTM), experiment by experiment, by modifying the procedure of feature selection, feature extraction and denoising. Indeed, the model is primarily fed with a variety of input variables and then features are extracted with different methods. The experiments in this paper are as follows: 1. PCA-LSTM 2. RF-LSTM 3. Deep LSTM 4. Wavelet-Deep LSTM The flowchart in Fig. 1 shows the overview of the second part of the methodology. Experiments regarding to pre-processing methods (color used)",1
60.0,3.0,Computational Economics,06 August 2021,https://link.springer.com/article/10.1007/s10614-021-10166-x,A Valid and Efficient Trinomial Tree for General Local-Volatility Models,October 2022,U Hou Lok,Yuh-Dauh Lyuu,,Unknown,Unknown,Unknown,Unknown,,
60.0,3.0,Computational Economics,07 September 2021,https://link.springer.com/article/10.1007/s10614-021-10167-w,Portfolio Selection Using Multivariate Semiparametric Estimators and a Copula PCA-Based Approach,October 2022,Noureddine Kouaissah,Sergio Ortobelli Lozza,Ikram Jebabli,Unknown,Male,,Mix,,
60.0,3.0,Computational Economics,19 November 2021,https://link.springer.com/article/10.1007/s10614-021-10169-8,Exploring Statistical Arbitrage Opportunities Using Machine Learning Strategy,October 2022,Baoqiang Zhan,Shu Zhang,Xiaoguang Yang,Unknown,,Unknown,Mix,,
60.0,3.0,Computational Economics,09 August 2021,https://link.springer.com/article/10.1007/s10614-021-10170-1,Euro Area Deflationary Pressure Index,October 2022,Luca Brugnolini,Giuseppe Ragusa,,Male,Male,Unknown,Male,"Central banks strive to keep inflation stable. Optimal monetary policies aimed at reaching the price stability objective ultimately depends on medium term inflation forecasts. It is not then surprising that central banks employ many resources to build and maintain forecasts of the inflation path at different horizons. This effort notwithstanding, inflation is one of the hardest macroeconomic variables to forecast, and it is rare to find models that consistently outperform subjective forecasts given in surveys of professional forecasters (Ang et al., 2007; Wright, 2009). The literature has been leveraging the increasing availability of macroeconomic data to construct larger and richer models in an attempt to improve the quality of inflation forecasts. Richer models means dealing with the potential overfitting of densely parameterized models by regularization techniques. Regularization can be achieved by adding a penalty term to the objective function being optimized to estimate the parameters. For instance, Inoue and Kilian (2008) show that the ridge regression and LASSO perform relatively well to forecast inflation when many predictors are used. Bayesian methods are particularly useful in achieving regularization. Imposing informative priors on parameters or averaging different models as in Bayesian Model Averaging are effective ways of limiting the scope for overfitting and yet consider larger models. Giannone et al. (2014) use a vector autoregressive model coupled with Minnesota priors to produce short term inflation forecasts. Wright (2009) and Koop and Korobilis (2012) have shown that Bayesian Model Average may lead to large improvements in forecast performance relative to alternative approaches. Factor models help achieve a good compromise between using information arising from several indicators and keeping the model parsimonious. Stock and Watson (1999) show how using factors from real activity indicators improve forecast based on model inspired by the conventional unemployment rate Phillip’s Curve. That article spawned a large literature which uses macroeconomic factors extracted from a large pool of variables which include real and financial variables to forecast inflation (see, e.g., Forni et al., 2003). We focus on forecasting the probability that euro area inflation will fall into one of three intervals. We employ an ordered multinomial model augmented with macroeconomic variables to directly forecast the probability that the expected euro area HICP price index inflation rate (12-month percent changes) over the next 12 and 24 months will be less than 1.5 percent, exceed 2 percent, or be between these two values. Assessing whether the probability that inflation will move above (or below) a certain target zone over some horizon is interesting from a policy perspective. Policymakers are interested in knowing whether and by how much the inflation target will be missed. The argument can be heuristically formalized with a simple forward-looking Taylor rule (Bernanke & Woodford, 1997; Clarida et al., 2000) as in Eq. (1). where \(i_{t}\) is the central bank interest rate, \(\pi _{t}\) is the inflation rate, \(\pi ^{*}\) is the inflation target, \(\phi _{\pi }\) is the reaction coefficient, and \({\mathbb {E}}_{t}\equiv {\mathbb {E}}\left( \cdot |\Omega _{t}\right) \) is the conditional expectation operator given the information set at time t (\(\Omega _{t}\)). The Taylor rule determines the central bank interest rate direction in response to price deviations from the target. When \({\mathbb {E}}_{t}\pi _{t+h}>\pi ^{*}\) the central bank raises the interest rate, and cuts when \({\mathbb {E}}_{t}\pi _{t+h}<\pi ^{*}\). Hence, given \(\phi _{\pi }\), the point forecast is necessary and sufficient to know the magnitude of the interest rate adjustment. However, it is sufficient but not necessary to identify the direction of the policymakers’ action. What is necessary is to know whether inflation will be above or below the target, and, indeed, this information is readily assessed through probabilities.Footnote 1 A model for inflation levels will provide information about both the direction and the magnitude of the miss and it comes with the costs of being more susceptible to large forecast errors. Modeling directly the probability target misses is likely to have small forecasting errors since the focus is only on the direction of the miss and not on its magnitude. We use a large number of macroeconomic variables as predictors. These variables do not enter directly into the model. Instead, we use a limited set of factors (principal components). This approach limits the dimensionality of the model and yet lets the model use up-to-date, relevant information. Although we use relatively few principal components, the number of parameters of the model is still too large compared to the number of observations available. To avoid overfitting, we estimate the ordered probit model by imposing informative priors on the parameters. The pseudo-out-of-sample exercise used to assess the quality of the probability forecasts shows that the inclusion of these factors improves the model’s forecastability, especially at the longest of the horizons we consider. The index proposed in this paper is closely related to the St. Louis Federal Reserve Price Pressures Measure of Jackson et al. (2015). That index measures the probability with which the US inflation rate over the next 12 months exceeds 2 percent. There are several differences between the approach followed here and that of Jackson et al. (2015). First, we focus on deflationary rather than inflationary pressure. This difference is only in the focus as we can turn our attention to inflationary pressure by looking at the probability of inflation being higher than 2%. Second, they consider a shorter period (12 months), and they construct the index by averaging one-period ahead probabilities. We estimate the probability directly in a Bayesian framework that allows us to simultaneously shrink the parameters and derive a measure of uncertainty robust to model misspecification. Also related to this paper is the literature centered on the idea that forecasting the magnitude of macroeconomic variables is as relevant as forecasting their direction (Galbraith & Norden, 2012). Several papers have tackled the problem of forecasting the probability of recession (Estrella & Hardouvelis, 1991; Estrella & Mishkin, 1998; Berge, 2015; Liu & Moench, 2016) or developed early-warning signals (Kaminsky et al., 1998; Kaminsky & Reinhart, 1999; Reinhart, 2002). The remainder of the paper is structured as follows. Section 2 describes the order probit models and the estimation procedure. Sections 3 and 4 detail the data used in our application and the procedure used to select and construct the factors. Section 5 reports the results of our pseudo-out-of-sample exercise. Section 6 concludes.",
60.0,3.0,Computational Economics,28 July 2021,https://link.springer.com/article/10.1007/s10614-021-10171-0,Tail Risk Early Warning System for Capital Markets Based on Machine Learning Algorithms,October 2022,Zongxin Zhang,Ying Chen,,Unknown,,Unknown,Mix,,
60.0,3.0,Computational Economics,12 September 2021,https://link.springer.com/article/10.1007/s10614-021-10173-y,Disentangling Shareholder Risk Aversion from Leverage-Dependent Borrowing Cost on Corporate Policies,October 2022,Mateus Waga,Davi Valladão,Thuener Silva,Male,Unknown,Unknown,Male,"Corporate finance structural models struggle to explain the interaction between debt, dividend, and investment policies under risk-aversion. On the one hand, continuous-time models incorporating utility-based manager’s risk aversion can endogenously choose risky debt and dividend policies but fail model investments realistically. On the other hand, existing discrete-time models better characterize investment and dividend policies but are inept on representing risk-aversion as well as debt and equity issuance costs. Besides that, very few works can handle risk-aversion as part of their framework. So far, no model could realistically incorporate investments, dividends, and debt policies considering leverage dependent borrowing cost and other financial frictions in a fully dynamic risk-averse framework. This lack of realism and the poor treatment of risk-aversion explain why these models are detached from empirical works, such as Strebulaev and Yang (2013). For continuous time models, Carlson and Lazrak (2010) develop a static choice of debt and a dynamic policy for the asset risk-return profile under the simplifying assumption that average returns increase proportionally to volatility levels. On the dynamic side, Bhamra et al. (2010) and Herranz et al. (2015) incorporate debt dynamics under unrealistic assumptions and conclude that leverage ratios increases with risk aversion. This unintuitive insight is a consequence of borrowing rates assumed fixed, i.e., independent on the firm’s leverage ratios. Furthermore, these three continuous-time models fail to represent corporate investment dynamics suitably. Bhamra et al. (2010) and Carlson and Lazrak (2010) do not model investment explicitly, and Herranz et al. (2015) oversimplifies the investment dynamics assuming constant returns to scale. All these unrealistic assumptions lead to misleading or unreliable managerial insights for practical purposes. Existing discrete-time investment models are flexible in their formulation and allow us to understand the dynamic effects of investment and dividend policies but are poor in treating debt and equity issuance polices. Studies like Gomes (2001) and Hennessy and Whited (2005) introduced dynamic models that endogenously choose investment and debt levels. The use of risky debt on dynamic models is studied on Hennessy and Whited (2007) and in Titman and Tsyplakov (2007), in which they implement an “endogenous default"" strategy when its equity value equals zero, Lambrecht and Myers (2017) proposed a dynamic model that takes into account agent-principal problems, in which the objective function is to maximize the utility function of managers. Li et al. (2016) using a dynamic contracting model quantified the cost of lost financial flexibility, and claimed that it is approximately the same magnitude as the tax benefit of debt. To jointly address these decisions within an optimal policy is a challenge that current models cannot deal. Optimization models are needed even considering the computational challenges due to their dynamic and uncertain nature. Simplified models dealing with the determination of these optimal policies have been relatively successful in measuring how financial frictions affect the decisions of companies. However, these models rely on strong assumptions, e.g., that agents are risk-neutral. Market interest rates are independent of the firm’s leverage, preventing a comprehensive study, and understanding of firms’ behavior in more realistic settings. This issue is well described in Strebulaev and Whited (2012): We close this section by pointing out that although dynamic models of investment and financing are rich in their treatment of dynamic effects, they are less rich in their treatment of fundamental reasons behind the existence of financial frictions. For example, in these model debt and equity issuance costs are specified exogenously, so that the firm is powerless to influence its own cost of external finance. In addition, the form of financial contracts (usually equity and one-period debt) is exogenous. Finally, most (but not all) of these models are implicitly specified under the risk-neutral measure, which implies that one cannot disentangle the effects of risk from the effects of financial frictions on corporate policies. The model we propose is set to give a step forward on fulfilling the gaps pointed out by Strebulaev and Whited (2012), in particular, to disentangle the effects of risk aversion from financial frictions (such as leverage-dependent borrowing costs) on corporate policies. Our model also brings new light to the zero/low leverage puzzle described in many empirical works. As pointed by D’Mello and Gruskin (2014), empirical results show much lower levels of debt than predicted by traditional models. Minton and Wruck (2001) found out that conservative firms (another word for risk-averse firms) follow a pecking order style financial policy. They also found out that low leverage is largely transitory, in other words, dynamic. El Ghoul et al. (2018) reveals that firms are more likely to employ a zero-leverage policy in countries with more conservative cultures (again, more risk-averse cultures). Strebulaev and Yang (2013) argue that Dividend-paying zero-leverage firms pay substantially higher dividends and are more profitable. They also conclude that family-owned firms and firms with smaller boards are more likely to have zero debt. Traditional oversimplified closed-form models cannot explain why some firms have very low (or none) leverage. Our results show that the key to understanding this phenomenon is the aggregated effects of risk aversion and a realistic borrowing cost modeling. In this work, we develop a dynamic model incorporating risk aversion as well as endogenously determined leveraging costs. The firm faces uncertain revenue and debt costs follow an increasing convex function. We consider a recursive certainty-equivalent discount-dividend formulation that parameterize risk aversion with a single scalar parameter defining the convex combination between the expected value and the Conditional Value at Risk (CVaR) as proposed in Street (2010). The nested risk-averse certainty equivalent used in our study is practical, as it is based on a largely used measure of risk in industry practices, and induce to time-consistent decisions as shown in Rudloff et al. (2014). We reformulate the model into a computationally tractable model and solve it with a Markov-chained Stochastic Dual Dynamic Programming algorithm, a stable and long stand technique for solving multistage stochastic programming models, first proposed by Pereira and Pinto (1991). The use of these cutting-edge operations research techniques allows us to present a more realistic framework than the traditional closed-form solution for oversimplified continuous-time dynamic models as we can model borrowing costs as a piecewise linear function. Therefore, we summarize our main contributions as follows: to develop a computationally tractable risk-averse dynamic model that co-optimizes investment, dividend, and debt policies under financial frictions; to incorporate endogenously determined leveraging costs by explicitly representing the agent’s access to different credit rates due to the market risk perception; to consider financial frictions such as tax shield, costs of equity issuance, and asset fire sale; to bring a possible explanation for a long-standing question in the financial literature: the low leverage puzzle. The fundamental arguments are introduced in Sect. 2. We use intuitive and simple examples to demonstrate how our proposal works and what are its main benefits. In Sect. 3, we propose the dynamic model where we detail the objective function as a risk-adjusted shareholder value, as well as decisions on investing (or divesting), issuing debt, holding cash or paying dividends. Also, we present the considered financial frictions such as tax shield, equity issuance costs and asset fire-sale discount. In Sect. 4, we present the computationally tractable reformulation which include a set of auxiliary linear inequalities to represent the piecewise linear functions of the model. In Sect. 5, simulations of the resulted model illustrate the behavior of risk-averse financial and investment optimal policies and Sect. 6, concludes.",
60.0,3.0,Computational Economics,17 August 2021,https://link.springer.com/article/10.1007/s10614-021-10174-x,Feature Screening in High Dimensional Regression with Endogenous Covariates,October 2022,Qinqin Hu,Lu Lin,,Unknown,,Unknown,Mix,,
60.0,3.0,Computational Economics,22 August 2021,https://link.springer.com/article/10.1007/s10614-021-10175-w,Indicator Selection of Index Construction by Adaptive Lasso with a Generic \(\varepsilon \)-Insensitive Loss,October 2022,Yafen Ye,Renyong Chi,Xiangyu Hua,Unknown,Unknown,Unknown,Unknown,,
60.0,3.0,Computational Economics,29 August 2021,https://link.springer.com/article/10.1007/s10614-021-10176-9,Swarm Intelligence Based Hybrid Neural Network Approach for Stock Price Forecasting,October 2022,Gourav Kumar,Uday Pratap Singh,Sanjeev Jain,Unknown,Male,Male,Male,"Stock price time series exhibit complex non-linear, chaotic, highly volatile and dynamic behavior with irregular movements and hence, it is considered as highly unpredictable (Fama, 1970). Forecasting stock price time series is one of the most emerging areas of research and challenging tasks for researchers and financial analyst as stock market plays a crucial role in the financial growth of country as well as global economic status because economic developments of the countries are effected by the various financial activities (Lin et al., 2012). Fluctuation in stock market is governed by various macro-economic and micro-economic factors like political stability, government policies, general economic status, organization’s growth, investor’s expectations, global economic conditions, investor’s psychology etc. (Haleh et al., 2011; Menkhoff, 1997). However, the exact factors that have greatest influence on the stock market are not known. The only information that is available from the stock market is the prices. The Stock price time series that can be represented as: xt = {xt ∈ R| t = 1, 2, 3….N} is the set of prices recorded at regular interval of time t. The major focus of stock price time series forecasting approaches is to forecast the future prices of the series on the account of the regular pattern present in current prices of the series itself. Some previous approaches that have attempted the forecasting of stock market are fundamental analysis (Nassirtoussi et al., 2011), technical analysis (Menkhoff, 1997), traditional time series forecasting techniques (Zhang et al., 2008) such as autoregressive moving average (ARMA) (Box et al., 2015), exponential smoothing (ES) (Wang et al., 2012), autoregressive integrated moving average (ARIMA) (Adebiyi et al., 2014; Ariyo et al., 2014; Yao et al., 1999), autoregressive conditional heteroskedasticity (ARCH) and generalized autoregressive conditional heteroskedasticity (GARCH) (Guresen et al., 2011a; Kristjanpoller & Minutolo, 2018). However the stock price time series data are highly volatile, possess complex non-linear behavior, highly noisy, dynamic and chaotic in nature (Si & Yin, 2013). Hence, traditional time series forecasting methods have a limitation that they cannot capture the complex non-linear behavior of stock markets. In recent years, computational intelligence (CI) and nature inspired optimization based hybrid models have been proposed for stock price time series forecasting. Computational intelligence (CI) is an evolving computation approach which mimics the thinking capability of human brain to learn and generalize in an environment of uncertainty and imprecision for solving complex real world problems (Ibrahim, 2016). The popular CI approaches that are collectively used to create hybrid models include artificial neural network (ANN), fuzzy Logic (FL), genetic algorithm (GA) and various nature inspired algorithms (NIA) such as particle swarm optimization (PSO), differential evolution (DE), ant colony optimization (ACO), artificial bee colony (ABC), bacterial foraging optimization (BFO) etc. ANN is self-organizing, data-oriented and auto-adaptive approach which can be effectively applied to forecast the stock price as it cannot incorporate complex model structure and have an ability to capture the non-linear relation between input and output without prior assumption about data (Guresen et al., 2011b; Lu et al., 2009). Due to capability of ANNs in order to deal with non-linear data and uncertainty, it is widely used for forecasting stock price time series (Liu & Wang, 2012). Feed-forward neural networks (FFNN) and recurrent neural network (RNN) are two most frequent used artificial neural networks technique for stock market forecasting (Atsalakis & Valavanis, 2009). Yan (2012) employed a fusion of generalized regression neural network (GRNN) model in order to automatic search of design parameters of ANN for time series forecasting problem. Zhou et al. (2016) et al. proposed a hybridization of dendritic neuron model and phase space reconstruction to predict the financial time series and obtained promising result. Gao et al. (2018) applied six nature inspired learning algorithms including PSO and GA to train the dendritic neuron model for solving 14 problems. Kim (2006) combined ANN with GA for selecting the feature instances and optimizing the inter-neuron connectionist weights and achieved the improved predictive ability of proposed model and reduced the training time. Gong et al. (2018) used a multiobjective learning algorithm namely multiobjective evolutionary algorithm (MOEA) to optimize the recurrent neural network (RNN) and termed the model as multiobjective model-metric (MOMM) learning which simultaneously optimizes the network structure, data representation and the time series model separation for time series approximation and classification. The biggest challenge in ANNs is defining its architecture in terms of number of nodes in input layer (input features), number of processing units in hidden layers and output layer as well as tuning the parameters of neural network such as connection weight and bias of the network, appropriate activation functions, learning rate etc. so as to obtain accurate result. Currently the architecture of ANNs have been defined manually by trial and error approaches or by human experts which is cumbersome process that takes lot of time and prone to error. Hence, there is growing need of automatic search of optimal neural network architecture. Various authors have attempted the automated search of neural network architecture (Elsken et al., 2018a). Han et al. (2016) proposed adaptive PSO to automatically optimize the network structure and weights of radial basis function neural network (RBFNN) simultaneously and verified the effectiveness of proposed model with five different problems including two time series prediction problems. Wang and Kumbasar (2019) applied PSO and big-bang-big crunch (BBBC) optimization techniques to tune the parameters of a hybrid system obtained by combining interval type-2 fuzzy system (IT2FS) with ANN and demonstrated the capability of proposed model by approximating two time-varying non-linear system. Kapanova et al. (2018) applied GA to develop an automatic approach that search for the optimal network architecture of ANN for fitting a specific function. Sakshi and Kumar (2019) combined the pruning of redundant weights and GA for optimizing the parameters of ANN terming the model as neuro-genetic technique and observed that the proposed model shows the fast convergence, short training time and higher success rate. Defining the number of nodes in input layer and output layers are problem specific. However, the number of nodes in input layer can be obtained by selecting appropriate features. Selecting and reducing the dimension of input feature space for stock market forecasting is a challenging task. Dimensionality can be reduced either by feature selection or feature extraction techniques (Webb, 2003). In soft computing, high dimensionality in dataset may result in the problem of overfitting, high computational complexity and reduction in the performance of the model. Hence, by reducing the dimensions of feature space one can overcome these problems. Dimensionality reduction aims at representing the feature space with lower features while preserving the most of the information present in original dataset. Feature selection techniques minimize the feature space by excluding the irrelevant features and choosing the appropriate one. Various feature selection approaches have been used to enhance the interpretability of the model (Kumar et al., 2016; Guyon & Elisseeff, 2003). Selecting the number of neurons in hidden layers is also a key challenge in ANNs because there is no well-defined method for defining the optimum number of neurons in hidden layers of ANNs. For this purpose researchers generally employ trial and error approaches. Selecting the too few or too many hidden neurons can cause the problem of underfitting or overfitting respectively, hence degrading the accuracy of forecasting model. In this study, we consider two issues; first one is automatic searching the network architecture i.e. number of neurons in input layer (optimum set of features) as well as number of neurons in hidden layer of FFNN. The performance of correctly defined model depends upon the selection of appropriate input variables i.e. number of nodes in input layer. Hence, in this study, key features are selected using swarm intelligence technique. Another challenging problem considered in this work is tuning the initial parameters (weights and bias) of FFNN. Updating the weight and bias in ANNs according to predefined criteria is known as training or learning of neural networks. Training in ANNs is governed by minimizing the loss function like mean squared error between actual and predicted value averaged over all training samples. Most of training methods such as conjugate gradient, Quasi-Newton method, gradient descent, Newton method, and Levenberg–Marquardt (LM) are gradient based (Basheer & Hajmee, 2000). Among the learning algorithms, a second order technique like LM (More, 1978) method can be successfully used to obtain more accurate result. The LM method is powerful optimization technique used in ANNs as it has a capability to speed-up the learning procedure and convergence of the networks. It uses back-propagation (BP) algorithm in which gradients are computed and propagated iteratively from last layer of the network to first layer until the error between actual and predicted value reached to minimum level or other stopping criteria such as number of epochs are satisfied (Hagan & Menhaj, 1994). Despite being good performance of LM method in some areas, it has two drawbacks: firstly, it gets trapped in local minima if the loss function is multimodal or/and non-differentiable (Yao, 1999). Secondly, it leads to the problem of vanishing gradient if network has more than three layers (Bartlett & Downs, 1990). These shortcoming of LM and ability of swarm intelligence based optimization techniques to deal with combinatorial and continuous optimization problems have motivated us to use nature inspired and swarm intelligence based methods to determine the optimize set of connection weights and bias of neural networks. Due to weakness of LM to converge locally, it can be shown that optimization of network parameters such as weight and bias are strongly dependent on initial random value. If the initial parameters are located in local search space, there is a chance that networks get trapped in local solution. The local convergence problem can be overcome by applying global search techniques for training neural networks. Considering the drawback of local convergence of LM, in this article we present the combination of particle swarm optimization (PSO) which is a stochastic swarm intelligence based optimization techniques and feed-forward neural network (FFNN) trained with Levenberg–Marquardt back-propagation (LMBP) method for forecasting stock market indices. At the initial stage, PSO is employed to find the initial value of weight and bias of network to reduce the search space. After search space gets minimized, then the obtained weights and bias are assigned as initial parameters for FFNN algorithm. In certain scenario input data has a wide range of values which reduces the efficiency of the FFNNs. Hence, to scale the data into small range, the data transformation approach such as min–max normalization technique is applied. In this paper, we proposed a two stage swarm intelligence based hybrid intelligent mechanism that can forecast the 1-day-ahead, 5-days ahead and 10-days ahead close price of stock market by utilizing various technical indicators. The proposed model is developed by combining the discrete particle swarm optimization (DPSO), particle swarm optimization (PSO) and Levenberg–Marquardt (LM) algorithm for training the feed-forward neural networks (FFNN). In this study, we obtain the joint optimization of topology as well as initial parameters of FFNN in two stages simultaneously. In the first stage, we employ DPSO in order to obtain the optimal topology (number of neurons in input layer and hidden layer) of the network due to its capability to handle the binary variables and in the second stage we employ PSO for evolving the initial weight and bias of FFNN. Finally, the obtained weights and bias and optimal number of neurons in input (optimal feature subset) and hidden layers are used to train the FFNN by using LM algorithm. The proposed approach is named as DPSO-PSO-FFNN. This paper also performs the comparison of the forecasting capability of proposed model with regular FFNN, Elman neural network (ENN) (Ren et al., 2018), adaptive network-based fuzzy inference system (ANFIS) (Jang, 1993) and another hybrid model obtained by combining evolutionary technique such as binary coded genetic algorithm (BCGA) and real coded genetic algorithm (RCGA) with FFNN and termed the model as BCGA-RCGA-FFNN. The predictive ability of proposed model has been verified by employing it to forecast the close price of five stock market indices namely Nifty 50, Sensex, S&P 500, DAX and SSE Composite Index for multiple-horizon (1-day ahead, 5-days ahead and 10-days ahead) forecasting. The major contributions of this work are summarized as under: Since the only information available in the stock market is daily open, high, low and close prices and volume of share traded. We have created the pool of technical indicators from the available prices and volume by using TA python library. Feature selection is key challenge in soft computing domain. The first issue considered in this study is automatic selection of optimal subset of technical indicators from pool of technical indicators in order to reduce the network size and to attain higher accuracy. We employ the DPSO to obtain the optimal feature subset due to its capability to deal with combinatorial optimization. To automatically determining the optimal number of neurons in hidden layer and optimizing the initial weights and bias of FFNN simultaneously, a swarm intelligence based hybrid ANN model termed as DPSO-PSO-FFNN is developed. The key objectives of the proposed model are, firstly to reduce the risk of being stuck in local minima by gradient based Levenberg–Marquardt (LM) learning algorithm, secondly to overcome the problem of overfitting and underfitting by automatically selecting the optimum number of neurons in hidden layer by DPSO and finally, to attain the higher prediction accuracy in stock price time series forecasting domain. The major contribution of this study is to automatically searching the optimal feature subset, optimal number of neurons in hidden layer and optimal initial weights and bias in FFNN simultaneously, instead of manual trial and error approach. To develop another evolutionary computation based hybrid ANN model termed as BCGA-RCGA-FFNN by combining binary coded genetic algorithm (BCGA) and real coded genetic algorithm (RCGA) with FFNN for automatic search of reduced feature subset, number of neurons in hidden layer and parameters of FFNN simultaneously, in the same stock price time series forecasting domain in order to perform the comparative analysis. The experimental results verify the superiority of DPSO-PSO-FFNN in comparison to BCGA-RCGA-FFNN. The rest of paper is structured as follows: Sect. 2 presents the review of previous work. Section 3 introduces the framework for creating the proposed model. Section 4 describes the evaluation metrics and experimental setup. Section 5 demonstrates the experimental results and analysis. Finally, Sect. 6 presents the conclusion.",5
60.0,3.0,Computational Economics,20 August 2021,https://link.springer.com/article/10.1007/s10614-021-10177-8,A Nash Equilibrium for Differential Games with Moving-Horizon Strategies,October 2022,Enrico Saltari,Willi Semmler,Giovanni Di Bartolomeo,Male,Male,Male,Male,"Our paper aims at introducing moving (or receding) horizon strategies in a differential game. We propose a setting where players predict the effects of their actions and those of their opponents on a finite-moving horizon and define an equilibrium concept consistent with this assumption. Moving-horizon strategies are implemented by using nonlinear model predictive control (NMPC) techniques. It should be noted that, in our context, we do not view this approach as an approximation tool to solve complex dynamic problems.Footnote 1 We interpret it as a tool to formalize a specific kind of a strategic interaction. To emphasize this aspect, we label our equilibrium concept as the NMPC Nash Equilibrium. In a nutshell, the NMPC Nash Equilibrium incorporates strategies based on moving-horizon strategies, i.e., in each instant of time, each player maximizes her utility subject to some dynamic constraints taking as fixed her policy horizon; given the strategies of the others, the equilibrium is such that no player has an incentive to change her strategy unilaterally. Formally, the players’ problems only involve the repetitive solution of an optimal control problem at each sampling instant in a receding time horizon fashion. Everything being equal, different lengths of the time horizon imply different NMPC Nash Equilibria and, consequently, different dynamics of the relevant variables. By focusing on the lengths of the time horizon, we propose two alternative economic interpretations for the NMPC Nash Equilibrium.  Observing that economic agents often make decisions under limited information, we can assume that they respond imprecisely to the continuously available information: there is a cost of information arising with longer horizon (Hebert & Woodford, 2017), or they have a limited information processing capacity (Grüne et al., 2015; Woodford, 2018). A shorter horizon is then interpreted as a measure of inattention. Bounded rational or inattentive players base their behavior on strategies built on short (moving) time windows; conversely, the behavior of rational or attentive players can be rationalized by strategies based on longer horizons. The assumption that agents would neglect distant-horizon payoffs has also a foundation in the political economy literature. James Buchanan considered the issue of policy horizons as one of the pillars of his thought. In the debate with Robert Barro about the present effects of the future burden of public debt, Buchanan assumed that taxpayer’s expectations have a limited horizon, criticizing Barro’s ultra-rationality hypothesis (Buchanan, 1976). Therefore, we can distinguish between present-centric players with short-horizon perspectives and far-sighted players who adopt strategies based on longer horizons. Our approach is naturally related to the studies on inattention, where agents respond by using a finite horizon sampling to the continuously available information (Sims, 2003; Reis 2006a, 2006b). However, similarly to Grüne et al. (2015), we interpret inattention in a slightly different way from these authors. We assume that agents neglect payoffs after a particular horizon, whereas rational inattention. e.g., assumes that agents are uncertain of the current state of the world. Hebert & Woodford (2017) give a deeper explanation of the rational inattention theory by including explicitly information cost, justifying the sequential forward-looking information sampling behavior for finite time. Bounded rationality, as proposed by Woodford (2018), is also related to our approach in that it assumes that economic agents, due to their limited ability to solve complex decision problems over an infinite horizon by backward induction, use forward planning with a finite-horizon. Our method is probably akin to beta-delta preferences (Laibson, 1997) and neglected risks (Gennaioli et al., 2012), which have solid foundations in the behavioral literature.Footnote 2 A one-time period horizon NMPC model is very much like beta equal to zero in a beta-delta model. In this respect, our approach could be dubbed “neglected times,” which is in some sense very similar to neglecting states, which would be another proper name for the neglected risks theory. Neglecting distant-horizon payoffs can be interpreted as a rational reaction to uncertainty or a limited commitment ability. Being interested in being re-elected, politicians have a limited temporal horizon: when they are uncertain about remaining in office are induced to adopt relatively short-sighted policies (e.g., Alesina & Tabellini, 1990; Wittman, 1995; Chari & Cole, 1993; Smart & Sturm, 2013). Political economy models also often justify policymakers’ myopia by something like hyperbolic discounting (e.g., Persson & Svensson, 1989). Many authors have emphasized the effects of impatience and discount factor shocks on government’s and central bank’s behavior (Adam, 2011; Niemann, 2011; Niemann et al., 2013; Rieth, 2014). Along these lines, the NMPC Nash Equilibrium can be interpreted as a measure of policy uncertainty about the politicians/bureaucrats’ turnover or their limited commitment ability. On the methodological ground, two other papers are closely related to our study. A source of inspiration is Grüne et al. (2015), who surveyed the NMPC approach in economics and, as said, discuss the intuition behind the inattention as used in this paper. A second paper is van den Broek (2002), who introduces the idea of receding horizon in a scalar LQ differential game. Following a different interpretation, he attempts to endogenize the horizon length to minimize the distance between a feedback Nash equilibrium and the moving horizon solution. We extend van den Broek (2002) in two respects. i) We follow the approach developed by Grüne et al. (2015) so we are not restricted to the LQ context. ii) We consider a more general time structure where the state variables evolve in continuous time while the control ones are regularly adjusted and kept constant for a small finite interval. This formalization is more suitable for capturing iterations between policymakers, such as those typical, e.g., of policy games (cf. Sect. 3).Footnote 3 The remainder of the paper is organized as follows. Section 2 describes the basic strategy of NMPC in the case of a single policymaker and then it introduces an equilibrium concept suitable for strategic games where multiple players optimize by a NMPC algorithm. This Section also contains an algorithm to find he NMPC Nash Equilibrium. Section 3 shows how to apply the NMPC Nash Equilibrium by using a simple two- and three- player model in a well-known class of policy games. Section 4 concludes.",5
60.0,3.0,Computational Economics,26 August 2021,https://link.springer.com/article/10.1007/s10614-021-10178-7,A Pricing Method in a Constrained Market with Differential Informational Frameworks,October 2022,Ivan Peñaloza,Pablo Padilla,,Male,Male,Unknown,Male,"One of the most important problems in financial mathematics as well as in practice is how to price financial instruments such as options, futures, etc. It is well known that under certain assumptions, such as market completeness (a market without arbitrage opportunities where every contingent claim can be replicated), it is possible to find the price of financial instruments in a mathematically consistent way. However, in practice those hypotheses do not hold, and therefore, problems such as non-uniqueness of the price, arbitrage opportunities, hedging problems, etc., appear. In the real world, there is always a group of investors that has some hedging constraints on their portfolios since they do not have access to information and proprietary technology that institutional investors have. For this type of investors perfect replication of a derivative is usually not possible, and the market shows properties of incompleteness relative to these agents. Moreover, almost any market has institutional investors who combine technical analysis, fundamental analysis, and economics to assess potential investments on stock derivatives. Their decisions depend on their informational framework, and their perception of the probabilities of future macroeconomic events. When this happens, investors face the problem of what measure to use to price financial instruments. This type of environments has been a topic of interest for researchers in the field of mathematical finance, and some have created methods to deal with this problem. Some of the most common characterizations have been the existence of many martingale measures and hedging restrictions. When this happens, quantitative analysts face the problem of what criteria they have to use to pick up a probability measure to price the financial instruments they are working with. There are some works that have addressed this problem, but some of them offer a mathematical answer rather than a financial solution of how to price derivatives. For example, the minimal martingale measure proposed by Föllmer and Schweizer (1991); the minimax measure by Belini and Fritelli (2002) or the minimal distance martingale by Goll and Rüschendorf (2001). Most recently and as it is explained by Cheridito et al. (2016) there have been several works giving more financial sense about pricing derivatives under utility indifference arguments; however, utility based prices are personal and reflecting the preferences of a single agent. One way to describe this price is the following. Assume that we are in a market where there is a big institutional investor and many small investors. Let C be the payoff of a derivative with maturity at time T. Let \(P^{M}\) be the market price of the derivative (at time zero) in this micro-market. Let \(P^{{\bar{\lambda }}}(C)\) be the big agent’s price. This is the discounted expected value of the payoff of the derivative under the martingale measure \(Q^{{\bar{\lambda }}}\) such that the expected value of the financial indicators of the companies related with the derivative match their respective forecasts. More specifically with \({\bar{\lambda }}\) computed from subject to where \({\tilde{C}}\) is the discounted value of C under the numéraire; H is the Shannon’s information entropy; \(X_{i}\) is the i-th macroeconomic indicator that includes part of the information of the financial health of the stocks related with the derivative, and \({\bar{X}}_{i}\) is the respective forecast. Let \(p^{\alpha _{j}}(v_{0}^{(j)}, C)\) be the price for the \(\alpha _{j}\)-stereotyped small agent with an initial endowment \(v_{0}^{(j)}\) with \(j = 1,\ldots ,N\). If we compute these prices for each \(t = 0,1,\ldots , T.\), we can define \(P_{t}^{{\bar{\lambda }}}(C)\), and \(p_{t}^{\alpha _{j}}(v_{0}^{(j)}, C)\). What we propose is that the micro-market price should be of the form where and the error, \(\varepsilon _{t}\), must be analyzed using time series analysis to know what is the most accurate model for calibration and forecasting. Here \( {\mathbf {Y}}_{t} \) represents the coefficient of participation of each agent in the market. In theory, we should have \(\Vert {\mathbf {Y}}_{t}\Vert = 1\), but since there are errors that cannot be captured by \(\varepsilon _{t}\), we use the inequality instead. This approach can serve as a bridge between the theoretical economic models of market prices and the technical methods of how to find market prices of derivatives using black boxes. There have been some approaches that provide a good approximation. Abedinia et al. (2019) propose a model to find optimal offering and bidding strategies for large consumers in specific markets using stochastic hybrid approaches. Saeedi et al. (2019b) find a way through which decision makers can select as a risk-neutral strategy the most robust decision via robust optimization approach. Saeedi et al. (2019a) show a multiblock-neural network (NN) that is optimized by an algorithm to increase the training and forecasting capabilities for price and load prediction. Xu (2006) proposes new methods with financial intuition about how to price and hedge financial instruments through super-replication strategies. Consiglio and Giovanni (2008) show a mathematical model to determine the fair price of bonus and default options using constraints and super-replication via stochastic programming. Sirignano and Cont (2018) provide a Deep Learning approach to uncover evidence for exitence of a universal and stationary price formation mechanism relating the dynamics of supply and demand for a stock. Most of these models provide either a numerical approach or a theoretical financial intuition. However, we need to find a holistic approach where we not only know how different agents price derivatives, but how the macroeconomic events and other economic sectors can influence the price of financial instruments, and the a posteriori probability of occurrence of those interactions. This information will be useful for decision takers and organizations that base their strategies in the expectations on future financial events. We know that the prices of stocks options are not only affected by the underlying assets, but also by the diversity of informational frameworks, and the other stocks that are correlated with the underlying assets of the stock option. We use, expand, and create algorithms using the theory developed by El Karoui and Rouge (2000) and Brigo et al. (2004) to describe the different types of agents in this micro-market. To know how other stocks affect the price of stock options, we break down the volatility of the underlying asset into what we call components of volatility, parts of the volatility of the underlying that depend on other assets. In addition to that, we need to consider all this information together in a mathematical structure that let us understand where the weights \({\mathbf {Y}}_{t}\) come from, and how they change when agents choose prices by relations of preference, and macroeconomic information. For simplicity, we will assume that the function of the expected value of the market price is a linear function of the prices proposed by each one of the agents. This allow researchers to use many approximation methods. In particular, by using linear approximations under very small intervals (Taylor approximations) of time, we can assume 1.4 should hold. In practice, because of the lack of accuracy, forecasts are never made over long periods. Therefore, we can use the estimator of \({\mathbf {Y}}_{t}\) to make predictions of the price of the derivative over short periods.",
60.0,3.0,Computational Economics,25 August 2021,https://link.springer.com/article/10.1007/s10614-021-10179-6,Optimal Pricing of Climate Risk,October 2022,Thomas F. Coleman,Nicole S. Dumont,Alexey Rubtsov,Male,Female,Male,Mix,,
60.0,3.0,Computational Economics,22 September 2021,https://link.springer.com/article/10.1007/s10614-021-10181-y,Complementarity Modeling of a Ramsey-Type Equilibrium Problem with Heterogeneous Agents,October 2022,Leonhard Frerick,Georg Müller-Fürstenberger,Max Späth,Male,Male,Male,Male,"People are interested in future economic growth both in terms of national income but equally—or even more importantly—in terms of personal income. The acceptance of the free-market system will ultimately depend on the resulting long-run distribution of income and wealth rather than on allocative efficiency. This paper presents numerical experiments on the dynamics of income and wealth distribution in Germany. Moreover, it describes a computational approach to analyze this issue more flexibly compared to what is current practice in applied macroeconomics. The natural starting point for the analysis of income dynamics is the well-understood Ramsey-type growth model. In this model, a representative agent chooses a welfare-maximizing consumption path, given initial endowments and technology. The optimal consumption path coincides with a decentralized perfect-foresight competitive-market equilibrium as long as markets are complete. Aggregation is no issue in this setting. Heterogeneity comes into play when it is assumed that there are many agents that differ in their initial endowments and preferences. The decentralized market outcome of such an economy cannot be identified simply by solving an optimal control problem with a given objective function, as it is the case in the model with a single representative agent. Agents must forecast future prices to base their decisions on. However, their decisions in turn jointly determine future prices. It is this coherence-loop in aggregation that makes these models complicated. A broad literature addresses the heterogeneous agent growth model, following different methodological strategies. Most authors make assumptions so that aggregated variables still evolve as if a representative agent would act as the decision maker. An example is the seminal work by Caselli and Ventura (2000) and its slightly simplified textbook version; see Barro and Sala-i-Martin (1995), p. 120. Caselli and Ventura have shown that this aggregation is feasible as long as agents share CIES (constant inter-temporal elasticity of substitution) preferences with identical discount rates and a constant substitution rate between private and public consumption. Based on this, the authors follow a three-step procedure to deal with heterogeneity, see Caselli and Ventura (2000), p. 910: (1) solve the aggregated dynamics by constructing an adequate representative-agent model, (2) assume distributions on preference parameters and initial endowments, and (3) derive testable hypotheses. A similar approach is chosen by Turnovsky and García-Peñalosa (2008, 2013), who allow for heterogeneity in capital assets and labor productivity. Another strategy to solve the heterogeneous agent model is based on an approach by Krussel and Smith; see Krusell et al. (1998) and the overview by Heathcote and co-authors in Heathcote et al. (2009). Krussel and Smith use an approximate aggregation for which the mean of the wealth distribution is all what the agents need to predict future prices. As before, the distribution of assets does not matter for aggregated outcomes. Their approach is particularly suited for stochastic growth models. Heathcote applied a related methodology to analyze Ricardian equivalence in a heterogeneous agent model; see Heathcote (2005). He identifies a saving rule, which does not depend on distributional parameters but on aggregated variables only. In this paper, we follow a direct computational strategy, i.e., without analytical or approximate aggregation. This is different from what is done in the literature as it is both a ready-to-use and precise approach to model wealth distribution dynamics. As a consequence, we can cope with situations in which the distribution of assets does matter for aggregated outcomes. We use mixed complementarity problems (MCPs) as the main modeling tool to show that heterogeneous agents that cannot be easily solved analytically can be modeled effectively. Moreover, we achieve the ability to capture severe heterogeneity in the utility function and individual dynamics, which are far from the average. Even highly nonlinear functions do not impede our approach. The equilibrium model is obtained by the optimization problems of the heterogeneous households as well as by those of the production sector, which are coupled using suitably chosen equilibrating conditions for interest and wage rates. The resulting equilibrium model in time-continuous and discretized form is presented in Sect. 2 as an MCP. The relation between MCPs and variational inequalities (VIs) is then used in Sect. 3 to prove existence of equilibria by exploiting the classic theory of VIs. In Sect. 4, we apply our modeling in a setting similar to the one used by Caselli and Ventura (2000). In particular, we compute the income dynamics on a model calibrated on data for Germany. Based on this, we investigate the impact of a highly stylized form of capital market imperfection, where the returns on investment depend on initial wealth, assuming that poor agents have higher costs to generate returns then richer ones. To show the versatility of our approach, we then implement a policy intervention, where agents must choose an investment path such that their final wealth distribution meets a given policy target. Finally, we demonstrate a two-data-point calibration of preference parameters. The model then replicates two given base years, which we consider a substantial improvement in numerical modeling. The paper ends with some concluding remarks in Sect. 5.",
60.0,3.0,Computational Economics,25 August 2021,https://link.springer.com/article/10.1007/s10614-021-10183-w,Undirected and Directed Network Analysis of the Chinese Stock Market,October 2022,Binghui Li,Yuehan Yang,,Unknown,Unknown,Unknown,Unknown,,
60.0,3.0,Computational Economics,27 November 2021,https://link.springer.com/article/10.1007/s10614-021-10220-8,An Algorithm for the Pricing and Timing of the Option to make a Two-Stage Investment with Credit Guarantees,October 2022,Linjia Dong,Zhaojun Yang,,Unknown,Unknown,Unknown,Unknown,,
60.0,4.0,Computational Economics,04 February 2021,https://link.springer.com/article/10.1007/s10614-021-10097-7,The Relationship Between Economic Growth and Electricity Consumption: Bootstrap ARDL Test with a Fourier Function and Machine Learning Approach,December 2022,Cheng-Feng Wu,Shian-Chang Huang,Yung-Chih Chen,,Unknown,Unknown,Mix,,
60.0,4.0,Computational Economics,20 June 2021,https://link.springer.com/article/10.1007/s10614-021-10134-5,The Risk Early-Warning Model of Financial Operation in Family Farms Based on Back Propagation Neural Network Methods,December 2022,Zhigui Guan,Yuanjun Zhao,Guojing Geng,Unknown,Unknown,Unknown,Unknown,,
60.0,4.0,Computational Economics,10 June 2021,https://link.springer.com/article/10.1007/s10614-021-10135-4,The Impact of Financial Enterprises’ Excessive Financialization Risk Assessment for Risk Control based on Data Mining and Machine Learning,December 2022,Yuegang Song,Ruibing Wu,,Unknown,Unknown,Unknown,Unknown,,
60.0,4.0,Computational Economics,20 June 2021,https://link.springer.com/article/10.1007/s10614-021-10137-2,The Analysis of Credit Risks in Agricultural Supply Chain Finance Assessment Model Based on Genetic Algorithm and Backpropagation Neural Network,December 2022,Yingli Wu,Xin Li,Guangji Tong,Unknown,,Unknown,Mix,,
60.0,4.0,Computational Economics,27 June 2021,https://link.springer.com/article/10.1007/s10614-021-10144-3,Early Warning of Chinese Yuan’s Exchange Rate Fluctuation and Value at Risk Measure Using Neural Network Joint Optimization Algorithm,December 2022,Zhaoyi Xu,Yuqing Zeng,Shenggang Yang,Unknown,Unknown,Unknown,Unknown,,
60.0,4.0,Computational Economics,25 October 2021,https://link.springer.com/article/10.1007/s10614-021-10206-6,Dynamics of Firm’s Investment in Education and Training: An Agent-based Approach,December 2022,Jung-Seung Yang,,,Unknown,Unknown,Unknown,Unknown,,
60.0,4.0,Computational Economics,22 August 2021,https://link.springer.com/article/10.1007/s10614-021-10184-9,V-Shaped BAS: Applications on Large Portfolios Selection Problem,December 2022,Spyridon D. Mourtas,Vasilios N. Katsikis,,Male,Male,Unknown,Male,"Beetle Antennae Search (BAS) is a nature-inspired meta-heuristic optimization algorithm which is capable of effective global optimization and has been applied widely in many scientific fields in the last few years (see Li et al. (2020a, 2020b); Cheng et al. (2020); Gao et al. (2020); Khan et al. (2020a, 2020b); Katsikis et al. (2020); Khan et al. (2020); Katsikis and Mourtas (2020)). For instance, in Li et al. (2020b), a task offloading framework in fog computing networks is proposed based on enhanced contract net protocol and BAS. In this way, the major challenge for the fog computing network on how to successfully and rapidly offload the task to fog nodes is accomplished. In Gao et al. (2020), to optimize Elman neural network, a soft-sensor model based on BAS is introduced. Therein, the prediction accuracy of the conversion value of vinyl chloride monomer is increased, while real-time control of the production process of Polyvinyl chloride polymerization is realized. In this paper, the binary version of BAS (BBAS), which is described in Medvedeva et al. (2020), is modified by adding a V-shaped transfer function as presented in Mirjalili et al. (2014); Mirjalili & Lewis (2013), to solve the BBAS problem of trapping in local minima. In this way, we introduce the modified V-shaped transfer function-based binary BAS (VSBAS) algorithm, which is a more efficient version of BBAS in the case of large input data. VSBAS is compared against BBAS, the binary bat algorithm (BBA), the binary genetic algorithm (BGA) and the modified V-shaped transfer function-based binary particle swarm optimization (VPSO) on a Markowitz-based portfolio selection problem using real-world large data input. The portfolio selection problem (or portfolio optimization) alludes to the optimal distribution of budget on the available stocks according to some objective (see Ghorbel & Trabelsi (2014); Corsaro and Simone (2019); Dai (2019); Katsikis & Mourtas (2019a, 2019b)). A common objective is the expected mean-return maximization, and the risk (variance) minimization. This objective was presented not many decades ago from Markowitz’s Modern Portfolio Theory Markowitz (1952), and has been studied extensively ever since. Contemporary references on Markowitz-based portfolio selection problems are Kulali (2016); Kellner & Utz (2019); Platanakis & Urquhart (2020); Fahmy (2020); Trichilli et al. (2020); Puntsag (2020); Zhang et al. (2020). For instance, the authors of Trichilli et al. (2020) explore the optimization of the portfolio under the Hidden Markov model’s investor sentiment states and over a different time period. By using two methods, namely the Markowitz and Bayesian mean-variance, their findings show that the Bayesian efficient frontier of regular and Islamic stock portfolios is influenced by the state of sentiment of the investor and the time period. In Zhang et al. (2020), a Markowitz-based model is considered in a privacy-conscious manner for outsourcing to a public cloud. The proposed model uses encryption operations for location-scrambling and value-alteration that can protect the privacy well of its input/output. The results therein display that when solving the proposed model, the investor will obtain a tremendous amount of computational benefit and cloud complexity consistent in comparison with that of the original model. In recent years, many Markowitz-based portfolio selection problems have been proposed and studied, and they have been addressed using a variety of methods, including a multi-objective evolutionary algorithm (MOEA) in Branke et al. (2009), a standard solver (Cplex) in Canakgoz & Beasley (2009), an augmented \(\epsilon \)-constraint method (AUGMECON) in Xidonas & Mavrotas (2014a, 2014b), a multi-objective optimization genetic algorithm (MOO-GA) in Nobre and Neves (2019), a non-dominated sorting genetic algorithm (NSGA-II) in Akbay et al. (2020), a BAS algorithm in Katsikis et al. (2021). The proposed approach has several new features when compared to previous research on Markowitz-based portfolio selection problems in general. New features and similarities with the most relevant papers are presented in Table 1. The key contributions of this work could be summarized as follows: We introduce a modified V-shaped transfer function-based binary BAS (VSBAS) algorithm, W study a binary Markowitz-based portfolio selection problem, We modify the VSBAS algorithm to handle optimization problems with cardinality constraints, We present the VSBAS efficiency against BBAS, BBA, BGA and VPSO on a financial integer linear programming problem. We introduce applications based on the DJIA and CAC 40 datasets, as well as some of the most active stocks in the US market. The paper is structured as follows. Section 2 presents the VSBAS algorithm. Section 3 contains applications on a binary Markowitz-based portfolio selection problem and includes information for the MATLAB package which has been made available on GitHub to support the readability and computational value of this paper. The applications use real-world data and examine the efficiency of VSBAS algorithm against BBA, BBA, BGA and VPSO in different and large portfolios setups. Finally, in Sect. 4, the concluding comments are presented.",2
60.0,4.0,Computational Economics,07 September 2021,https://link.springer.com/article/10.1007/s10614-021-10185-8,Hedge Effectiveness of the Credit Default Swap Indices: a Spectral Decomposition and Network Topology Analysis,December 2022,Peter Sinka,Peter J. Zeitsch,,Male,Male,Unknown,Male,"Mitigating market risk is a primary concern for both derivative book runners and portfolio managers. Hedge effectiveness, or the ability to offset the risk in an open market position, is an active area of investigation. Markets, where hedge efficiency has been explored, include foreign exchange (Hill & Schneeweis, 1982), equities (Park & Switzer, 1995) crude oil (Ripple & Moosa, 2007), soft commodities (Dark, 2012; Johnson, 2008), jet fuel (Turner & Lim, 2015), inflation (Chang, 2013; Wang et al., 2011), longevity (Cairns et al., 2014) and electricity (Madaleno & Pinho, 2010). Within fixed income, studies assessing the ability to hedge interest rate or treasury exposures have been undertaken by Barone-Adesi and Carcano (2016), Briys and Pieptea (2006), Gay et al. (1983) and Young et al. (2004). Barone-Adesi et al. (2012), Ioannides and Skinner (1999) and Liu and Xie (2019) examined a variety of hedging instruments for corporate bonds. Interest in employing single name Credit Default Swaps (CDS) as a hedge to lower debt financing costs was studied by Ashcraft and Santos (2009). Similarly, the implications of banks buying CDS protection to reduce regulatory capital was studied by Shan et al., (2021). The ability of single names to function as a cross hedge for implied volatility exposures was studied by Da Fonseca and Gottschalk (2014). The CDS indices, CDX.NA.IG, CDX.NA.HY, iTraxx Europe and iTraxx Xover are the most liquid securities in the corporate credit market. CDX.NA.IG and iTraxx Europe have each traded the equivalent of USD 10–30 billion (BN) in notional per day since 2017. Volumes for CDX.NA.HY and iTraxx Xover have varied from USD 2–10 BN per day (DTCC, 2019). As highly liquid, standardized contracts, with tight bid-offer spreads, the idea of using the CDS indices to hedge bond or single name CDS portfolios can be traced back to Fung et al. (2008). The effectiveness of the CDS indices as a hedge for CDS index tranches was investigated by Cont and Kan (2011). Their results indicated that the market was incomplete and that a large proportion of the risk in the CDS market was un-hedgeable. Barone-Adesi et al. (2012) found that CDX.NA.IG has limited hedging ability for corporate bond portfolios. Motivated by XVA trading (Zeitsch, 2017), Chamizo and Cinca (2015) reported similar limitations when the indices were employed to hedge portfolios of CDS. Similarly, Dor and Guan (2017) reported that the efficiency of the CDS indices to hedge high yield portfolios was dependent on the state of the market. Underperformance increased in stressed markets. This study continues the investigation of the hedge effectiveness of the CDS indices. Here, 3 connected methodologies are applied. Initially, the diversification of the indices themselves is explored through a spectral decomposition. An initial finding questions the diversification of the traded indices. This leads to potential alternative CDS portfolios with theoretically higher hedge performance. The quality of the CDS indices as hedges is then quantified by comparing the efficacy of the traded indices to the hedge effectiveness of the alternate spectrally chosen CDS portfolios. In all cases, the traded indices consistently underperform the alternatives. The observed volatility reduction of the spectral hedges can be as much as an order of magnitude. Furthermore, as few as 2 CDS can match the performance of the printed index. To understand why, the network topology of the single name CDS is then analyzed. This identifies a single main cluster within the market, or ‘market effect’, that represents the majority of CDS. The names from the printed indices are invariably included in this market effect. Essentially, the correlated CDS from that main cluster all carry the same information. Subsequently, this is shown to only represent a small fraction of the price action – hence the underperformance of the printed indices. For the spectral decomposition, principal components analysis (PCA) is a well-established technique to decompose interrelated variables into uncorrelated components. Kim and Hawoong (2005), Rudin and Morgan (2006), Sensoy et al. (2013) and Yang et al., (2016, 2017) studied market diversity with PCA as a central technique. Intuitively, higher diversification equates to more accurate market replication and improved hedge performance. Since 2010, the Deposit Trust Clearing Corporation, (DTCC, 2019), has tracked the 1000 most liquid CDS globally based on traded volume and number of trades. These CDS are eligible for inclusion in the indices. The spectral decomposition is initially applied to the CDS from the first DTCC liquidity report spanning September 2010 to February 2011. It was employed to construct CDX series 16 and iTraxx series 15. Taking a pair-wise correlation matrix of the time series returns of the CDS closing prices, an iterative algorithm is applied that progressively reduces the number of portfolio constituents by seeking to decrease the noise present in the matrix. Amongst the indices, CDX.NA.IG is the standout with 328 available obligors. Consequently, the results for that index will be reported in detail, with summary results included for the other 3 indices. To establish a market-wide effect, the algorithm is also run on CDX.NA.IG.30, CDX.NA.HY.30, iTraxx Europe 29 and iTraxx Xover 29, from 2018. This spans the entire DTCC reporting period, as of the time of writing. The initial finding is that the PCA eliminates the majority of the CDS that were subsequently selected for each index at the roll. The result holds across all 4 indices, for both 2011 and 2018. For spectral portfolios with equivalent numbers of CDS, as few as 1/3 of the PCA selected credits were included in the final published index. Iterating the selection process, the PCA reduces each portfolio to as few as 2 names, as is the case for CDX.NA.IG.16. Due to the wholesale removal of the actual reference entities from the traded indices, the spectral decomposition provides a sequence of portfolios that theoretically offer greater diversification than the associated printed index. The hedge effectiveness of the traded indices versus the spectral alternatives is quantified through a Profit and Loss (P&L) analysis. The ability to hedge the original DTCC universe of CDS provides a common benchmark to assess the risk mitigation of the actual printed indices. An equally weighted portfolio, by notional, of the original candidate CDS from DTCC is constructed. The six-month period following the index roll i.e. March 21st to September 19th, 2011 is then taken. This was the window during which the newly printed indices were on-the-run. Firstly, hedge the DTCC exposure with the relevant index on a DV01 neutral basis. Secondly hedge the portfolio on a DV01 neutral basis with the PCA selected names. Several sub-portfolios of CDS are chosen based on the iterations of the spectral analysis. In effect, there are multiple macro hedged portfolios. The daily mark-to-market and P&L of the hedged portfolios are then calculated during the on-the-run period for all 4 indices. Quantifying the hedge effectiveness of the indices versus the possible alternatives is calculated by volatility reduction. This approach has been applied by Kerkhof et al. (2005), Kalotay and Abreo (2005), Alexander and Barbosa (2007) and Adesi-Barone et al., (2012, 2016). Put simply, the lower the volatility of the P&L of the hedged portfolio, the higher the value of the hedge. The calculations are then repeated for the on-the-run window for series 30 and 29, from March to September 2018. Lower volatility in the P&L of the PCA hedged portfolios would indicate that the spectral approach has increased hedge effectiveness, and that is what is found here, across both 2011 and 2018. In fact, the spectrally selected portfolios with similar numbers of obligors to the traded indices see volatility reductions of between 2 and 10 times that of the relevant index. The market properties that produce the large volatility reductions merit further investigation. Likewise, the market characteristics that allow 2 factors to replicate the price action of approximately 300 CDS are worth pursuing. As summarized by León et al. (2017), financial connectedness may be classified into two main categories: network approaches and non-network approaches. The spectral decomposition is an example of a non-network methodology. An alternative network approach, such as exploring the topology of the CDS, may offer further insights into the drivers of the market—and that is what is found here. Hierarchical Trees (HT) are a network topology approach that has proven useful in studying the number and nature of common economic factors within markets. Using such tree-based techniques, meaningful taxonomies of stocks, currencies, treasuries and other economic data such as electricity prices, gross domestic product and carbon emissions have been published (see for example, Aste et al., 2005; Caido & Crato, 2010; Cui et al., 2018; Devire & Deviren, 2016; Kantar et al., 2016; León et al. 2014; Tang et al. (2018), Wang & Xie, 2015, You et al., 2015). Crucially, both the spectral analysis and the network topology are derived from the same correlation matrix. The network topology therefore offers an alternative perspective for investigating the same correlations. The outstanding question is then: what aspects of the CDS market are being captured by the spectral decomposition beyond the number of trades and the traded volume? Calculating the HT for the various markets identifies a single main cluster, or ‘market effect’, that represents the majority of the applicable DTCC CDS. Essentially the clustered CDS from that main cluster all carry the same information. Following Tumminello et al. (2010), a Hierarchically Nested Factor Model (HNFM) is then constructed from the HT. By examining the factor loadings, it is shown that deleting these CDS does not significantly degrade the information retained from the market. Instead, it indicates that a small subset of CDS carry more information than the main market effect cluster. The remaining CDS are themselves connected by clusters of 1–3 reference entities. Again, the factor loadings indicate that removing CDS from those clusters is also possible as multiple names are carrying the same low information. The net result is that the CDS market can be replicated by 10–20 single name CDS. In the most clustered cases, that reduction can reach 2–4 CDS, which agrees with the P&L calculations. This result is new. In Sect. 2, an overview of the data is presented that includes the motivation for the choice of the indices and their importance in the fixed income market. The PCA methodology is presented with the required analytics for the valuation of single name CDS and index positions in Sect. 3. Full results of the spectral decomposition for CDX.NA.IG.16 are detailed in Sect. 4, as well as the P&L hedge results for each index in 2011. The equivalent calculations in 2018 are also summarized. To explain the hedge effectiveness results, the network topology is analyzed in Sect. 5 before conclusions are reached in Sect. 6.",1
60.0,4.0,Computational Economics,03 September 2021,https://link.springer.com/article/10.1007/s10614-021-10186-7,An Analytical Approximation Formula for Barrier Option Prices Under the Heston Model,December 2022,Xin-Jiang He,Sha Lin,,,,Unknown,Mix,,
60.0,4.0,Computational Economics,13 September 2021,https://link.springer.com/article/10.1007/s10614-021-10187-6,Global Optimal Consumption–Portfolio Rules with Myopic Preferences and Loss Aversion,December 2022,Jia Yue,Ming-Hui Wang,Nan-Jing Huang,,,Unknown,Mix,,
60.0,4.0,Computational Economics,04 September 2021,https://link.springer.com/article/10.1007/s10614-021-10188-5,Calibration of Agent-Based Models by Means of Meta-Modeling and Nonparametric Regression,December 2022,Siyan Chen,Saul Desiderio,,Unknown,Male,Unknown,Male,"The acknowledgment that real-world economies are intrinsically complex systems has pushed an increasing number of economists to turn in the last few decades to agent-based models (ABMs). ABMs are generally large systems of stochastic difference equations whose properties cannot be analytically understood because of their complexity. Consequently, ABMs are first translated into software and then analyzed numerically through computer simulations. Examples of applications of ABMs to economics (or agent-based computational economics, a.k.a. ACE, Judd and Tesfatsion 2006) are now uncountable and range from the study of single markets, such as the financial market and the labor market, to the study of entire multi-market economies. A crucial and still problematic phase of ABMs analysis is the assessment of their degree of realism. This phase is generally called empirical validation, which consists in comparing the model output with analogous empirical data (Fagiolo et al. 2007). Basically, there are three broad categories of empirical validation (Judd and Tesfatsion 2006; Bianchi et al. 2007):  Input or ex ante validation, ensuring that the model characteristics exogenously input by the modeler (such as agents’ behavioral equations, initial conditions, random-shock realizations, etc.) are empirically meaningful and broadly consistent with the real system being studied through the model. Descriptive output validation, assessing how well the model output matches the properties of pre-selected empirical data. Predictive output validation, assessing how well the model output is able to forecast properties of new data (out-of-sample forecasting). The most common typology of validation procedure is the descriptive one, which is carried out by evaluating the model’s ability to replicate some set of stylized facts. However, descriptive validation procedures are still eminently qualitative and consequently no consensus has emerged so far on the best way to evaluate ABMs.Footnote 1 Taking ABMs to the data is made tricky also by the estimation of the model parameters. The outcome of validation procedures depends in fact also on the model parameter configuration. As a consequence, to increase the model’s degree of realism the researcher may want to resort to some calibration procedure to choose optimal parameter values. Calibration is however one of the major challenges posed by agent-based models. The reason is that in general ABMs cannot be analytically solved and so it is not possible to obtain some closed-form equilibrium relationship depending on the model parameters (e.g. equilibrium GDP as function of the marginal propensity to consume) to estimate directly on real data through common statistical techniques.Footnote 2 Hence, because of the difficulties encountered in calibrating the parameters by direct estimation, indirect methods such as indirect inference (Gourieroux et al. 1993) and the method of simulated moments (McFadden 1989) have become popular to calibrate ABMs. Many different variants of calibration through indirect methods exist in ACE literature, including among the others those proposed by Gilli and Winker (2003), (Bianchi et al. 2007, 2008), Fabretti (2013), Recchioni et al. (2015), Grazzini and Richiardi (2015) and Lamperti (2018).Footnote 3 In spite of their differences, however, all indirect calibration methods feature the same three phases:  Choosing a set of real data \(D_r\) and analogous simulated data \(D_m\) produced by the model.Footnote 4 Choosing a metric \(d(\cdot )\) to measure the distance between \(D_r\) and \(D_m\), representing the degree of realism of the model. Choosing by simulations the model parameters that minimize the distance \(d(D_r,D_m)\). Such a general framework has been labeled simulated minimum distance approach (Grazzini and Richiardi 2015), of which the first two phases basically constitute the criterion used to validate the model, and the third phase is the very calibration process. The goal of this paper is precisely to propose a new indirect calibration method. Indirect calibration in principle would require repeated simulations of the ABM to compute the value of the objective function \(d(D_r,D_m)\) for each point of the parameter space. By the brute force of calculations the modeler can therefore find the minimum-distance parameter vector. Unfortunately, for most of ABMs a complete exploration of the parameter space cannot be carried out at reasonable computational costs because of well-known issues:  Complexity: a single simulation of a sufficiently large ABM may take a long time to be run even for a small number of periods. ‘Curse of dimensionality’: ABMs are usually over-parametrized, i.e. their parameter space is large. Randomness: normally ABMs are stochastic. Thus, Monte Carlo replications are needed to average out the noise caused by random numbers, that is for each point of the parameter space simulations must be repeated with different seeds of the random numbers generator. To overcome these problems, calibration methods are usually constructed around search algorithms that explore only sub-sets of the parameter space, like for instance genetic algorithms (e.g. as in Fabretti 2013) and gradient-based search algorithms (e.g. as in Gilli and Winker 2003; Recchioni et al. 2015). Because of the typical complexity of ABMs, however, a major drawback of search algorithms is that they may find a solution that is only a local minimum. This means that such methods may still require many runs of the model because to find the global minimum the search needs to be repeated for different starting points (and for different seeds of the random numbers generator). A totally different approach to calibration which allows to avoid the complete exploration of the parameter space is the one based on the concept of meta-modeling. Meta-modeling is the process of approximation of an unknown complicated relationship between input factors (the parameters or other initial conditions) and model output with a simpler one of known shape, called ‘meta-model’ (Saltelli et al. (2008), ch. 5). In the context of calibration a meta-model could be specified for example to approximate the relationship between the distance \(d(\cdot )\) and the ABM parameters. The advantage of this approach is that the distance is not computed point by point but is estimated using only a limited number of observed points of the parameter space, for which therefore the ABM needs to be simulated. Subsequently, the estimated meta-model is used to calculate an approximated distance \({\hat{d}}\) for the whole parameter space without further simulations of the agent-based model. Calibration procedures based on meta-modeling are therefore aimed to increase the speed of the calibration process at the expenses of its accuracy, because the distance is approximated by the fitted values of the meta-model. Hence, calibration through meta-modeling seems to be particularly suitable for computationally demanding large-scale ABMs, in which case a loss in accuracy is well compensated by gains in execution time. Moreover, methods based on meta-modeling have potentially also another merit: as in fact the approximated distance is smoother than the actual one, in general they have the tendency to eliminate local minima, which are one of the main problems afflicting methods based on search algorithms. Among the few examples of the meta-modeling approach in ACE literature are Salle and Yildizoglu (2014), Barde and van der Hoog (2017) and Bargigli et al. (2020), who employ a kriging meta-model estimated on a sample constructed by the nearly-orthogonal Latin hypercube method. Another one is Chen and Desiderio (2021), who propose a novel sampling strategy of the parameter space which allows to estimate meta-models without resorting to Monte Carlo replications. All of these works are based on parametric regression meta-models, which can be easily estimated by such standard regression techniques as OLS and GLS but which may provide a poor approximation of the distance if this is a highly non-linear function of the model parameters. Thus, to overcome such a limitation in the present paper we will introduce a new calibration method still based on the sampling scheme of Chen and Desiderio (2021), but with the crucial difference that we will not specify the functional form of the meta-model used to approximate the distance. The unspecified meta-model will be therefore estimated by nonparametric regression techniques, namely local polynomial estimation. As for the parametric case, the main advantage of our method over other calibration procedures is that the ABM is simulated relatively few times. In addition, its high degree of flexibility makes it fitter than the parametric counterparts when the distance function is highly non-linear. Clearly, the inevitable drawback is that it requires less common estimation techniques. To our knowledge, our method is the first of its kind. Another recent calibration method for ABMs based on nonparametric meta-modeling is Lamperti et al. (2018), which use machine learning algorithms in place of regression meta-models. Although very similar in spirit, our work is however totally different as for the technique adopted, and is probably of greater ease of implementation. Other recent works quite different from ours but also employing some typology of nonparametric estimation techniques are Kukacka and Barunik (2017) and Grazzini et al. (2017). The paper continues as follows: in Sect. 2 we briefly introduce nonparametric regression, in Sect. 3 we present the calibration procedure and in Sect. 4 we apply it to calibrate the ABM presented in Chen and Desiderio (Chen and Desiderio 2018, 2020). Section 5 concludes.",1
60.0,4.0,Computational Economics,06 September 2021,https://link.springer.com/article/10.1007/s10614-021-10190-x,"Generalized, Partial and Canonical Correlation Coefficients",December 2022,H. D. Vinod,,,Unknown,Unknown,Unknown,Unknown,,
60.0,4.0,Computational Economics,12 September 2021,https://link.springer.com/article/10.1007/s10614-021-10191-w,Towards a Validation Methodology for Macroeconomic Agent-Based Models,December 2022,Sebastiaan Tieleman,,,Male,Unknown,Unknown,Male,"Macroeconomic agent-based models (MABMs) are a promising new tool in the analysis of macroeconomic phenomena (Farmer and Foley 2009). The models do not rely on ex ante equilibrium assumptions which make them particularly suitable for the analysis of economic crises. MABMs have presented us with several methodological innovations compared to dynamic stochastic general equilibrium (DSGE) models. These same innovations, however, have also been the cause of critique on the use of MABMs. Most of this critique has been focused on how MABMs are validated (Fagiolo et al. 2007). The criticism is partially due to the relative novelty of the MABMs. A deeper understanding of the relationship between validation theory and the MABMs is required in order to judge the correctness of current validation practices. The goal of this paper is to present a methodology for the validation of MABMs. Such a methodology must be able to answer the following question: How do validation practices in MABM work to enhance model validity? Furthermore, I will show that such a methodology allows us to shed light on some more fundamental issues regarding our characterization of MABMs. The structure of this paper is as follows. First, I will provide an account of the foundations of model validation in general. I will present a definition of model validation and relate this to the concept of model domain, where model domain can be seen as the scope of the model. Second, I will introduce a classification scheme based on how models are validated based on the account by Barlas (1996) and Boumans (2009). An important distinction in this classification is the difference between mechanism and target validation. The model target is the phenomenon that the model is constructed to reproduce. Model mechanisms are the relationships between model entities generating the model target. Third, I will present the framework of the structure of complex systems with emergent properties by Baas and Emmeche (1997). Connecting these three elements, we can take steps towards a methodology for the validation of MABMs. The basis of this methodology will be to analyse how the structure of complex systems relates to how MABMs are validated in practice, and from there, situate MABMs within the classification by Boumans (2009). The analysis will imply that an insightful way to look at the validation of MABMs is to consider them as modelling multiple non-trivial levels that are subject to distinct forms of validation. The methodology of MABM validation that I will present reveals several fundamental insights into MABMs. First, it allows us to pinpoint what the mechanisms of MABMs are constituted by. The mechanisms at a lower level are distinct from, but are input to, the higher level mechanisms. Since mechanisms at different levels are validated in different ways I come to a specific characterization of MABMs within the classification of Barlas (1996) and Boumans (2009), that is, in some ways, distinct from other types of models in macroeconomics. Second, I will show that because the mechanisms of MABMs are validated in a direct way at the level of the agent, MABMs can be seen as a more realist view on modelling compared to the DSGE approach. With this paper, I will contribute to several strands of literature. First there is the literature that seeks to explicate some of the general issues that modellers run into the validation of MABMs, as well as discuss the up- and downsides of the different validation methods used in practice. The first publications in this series were Fagiolo et al. (2005) Fagiolo et al. (2007) and Windrum et al. (2007). Later, important updates followed to discuss new developments (Fagiolo and Roventini 2017; Gatti et al. 2018). In this series of papers the most commonly used validation approaches are put forward. In Fagiolo et al. (2005) the main approaches are qualitative simulation modelling, replication of stylized facts, empirical calibration and the history friendly approach. In qualitative simulation modelling, the relationship between the behaviour of the model and empirical data is only required in a qualitative dimension. That is, as long as the model behaviour is roughly in line with some qualitative empirically observed features, the model is considered valid (if A increases than B also increases etc.). Such models are most applicable for exploratory and experimental purposes. Replication of stylized facts is the approach in which the model is considered valid if it is able to reproduce a set of relevant (given the model purpose) stylized facts. Importantly, all of the model parameters are calibrated indirectly, meaning that they are quantified such that the model is able to reproduce the set of stylized facts. The empirical calibration approach is similar to the replication of stylized facts approach, in the sense that stylized facts are used to calibrate the model parameters. In addition, however, some of the parameters are calibrated directly. This entails that the empirical data used for the calibration concerns the individual relationship in which the parameter occurs, instead of comparing the output of the model as a whole. In the case of agent-based models, most of the parametrization occurs at the micro level, implying that, in direct calibration, empirical data at the micro level are used. This type of validation is considered to be a more strict type of validation. Finally, the history friendly approach, in which the validation criterion is to reproduce a precise data history, at least in qualitative terms. Often, this comes in the form of reproducing a specific time series. In this approach, there is an additional importance of matching the initial conditions of the model to the one’s observed in time series to be reproduced. Most recently Fagiolo et al. (2019) has been published, which provides us with the latest developments regarding validation techniques of MABMs. This includes the use of machine learning techniques to select regions of the model parameter space that exhibit interesting behaviour. Such computational techniques have become increasingly important as agent-based models have become more elaborate, and the increasingly important criterion of sensitivity analysis. Sensitivity analysis is an assessment of how robust certain model behaviour is to changes in the parameter space, or to changes in assumptions. An additional interesting development that is discussed in Fagiolo et al. (2019) is the validation approach described in Guerini and Moneta (2017). In this approach, a VAR-estimation is performed on the aggregated (macro) variables of the simulated data (generated by the agent-based model). The coefficients of this simulated VAR-estimation are then compared to the coefficients of the same VAR-estimation performed on empirical data. If the coefficients match in terms of their sign and, to some extent, their size, the model passes the validation test. This is an interesting approach because it deals with what is known as the conditional object critique originally put forward by Brock (1999). The core of this critique is that stylized facts may be too general such that multiple distinct models may be able to reproduce these stylized facts. The approach in Guerini and Moneta (2017) provides a more selective validation measure within this context. The publications discussed above have been critical in providing overviews of validation approaches in MAMBs in order to build a more standardized approach and are important in making clear in which directions new research should develop. This strand of literature, however, has not sought to provide in depth methodological foundations for the methods they present. It does list an overview of some deeper methodological issues in model validation, such as realism versus instrumentalism and underdetermination [see for example Windrum et al. (2007)], but does not go the step further and look at how these issues specifically apply to MABMs. This is, in turn, precisely what this paper aims to do. An alternative angle to consider the validation of MABMs is to construct a benchmark model, the performance of which, serves as a minimum criterion for model validation. In a.o. Caiani et al. (2016) and Lengnick (2013), for example, the aim has been to present a benchmark or baseline model. The difficulty with such a common benchmark may be that actual validation criteria may differ given various different questions models are built to answer. Benchmark models, therefore, seek to incorporate only features that are seen as essential for most models. If we look at Lengnick (2013), for example, we see that the incorporated features include households, firms and banks that behave and interact according to simple rules. Moreno et al. (2019) does not consider particular models but rather looks at setting up a benchmark for various agent-based model software platforms. It looks at which features are typically deemed essential for an agent-based model, such as the ability to incorporate interaction rules and to simulate large numbers of agents. By implementing these features in various agent-based modelling platforms, the performance is assessed by looking at the computational performance (such as execution time) of the platforms. An interesting avenue for future work would be to extend the approach of Moreno et al. (2019) from agent-based modelling platforms to agent-based models. The literature discussed above shows that there are a variety of approaches to model validation of macroeconomics agent-based models. Some approaches may be more suitable for particular types of models than others. For the purpose of this paper I will focus on the stylized fact (or indirect calibration) approach, because this is the approach that is most frequently used in practice (Fagiolo et al. 2019), and because more recent approaches such as Guerini and Moneta (2017) are ultimately extensions of this approach. In important building block in the methodology I will present is the literature that relates the type of model to the type of validation. This literature originates in the system dynamics modelling literature with Barlas (1996) as one of its most relevant publications. Boumans (2009) has specified this way of relating a model’s purpose and validation further by applying it to models in economics. This literature is rather unique in that it explores validation in the context of model purpose. I will use this classification to characterize what type of model MABMs are and what this implies for their domain of validation. As we will see, the notion of complex systems and complexity economics plays an important role in the analysis of this study. My understandings of the economy as a complex system have been strongly influenced by works such as Arthur (2013). Furthermore, the concepts of reductionism and emergence in the context of macroeconomic modelling will be relevant. Hoover (2015) and to some extent Gatti et al. (2011) are important contributions in this regard. This study will contribute by connecting the concepts of complexity and emergence to model validation. Finally, this contribution stands within a strand of literature looking at the fundamental methodology of agent-based modelling. Most well-known are Epstein (1999) and Epstein (2006), in which the idea of agent-based models as tools to generate explanation is brought forward. This also links to validation since a necessary condition within this concept is that agent-based models are valid only if they are able to generate an explanation starting from interacting agents. Furthermore, contributions such as Elsenbroich (2012) and Grüne-Yanoff (2009) have helped me to gain a deeper understanding of agent-based methodology in relation to explanation. An analysis of validation in the light of the fundamental methodology of agent-based models is not present in the current literature, however, which is where I hope to contribute.",1
60.0,4.0,Computational Economics,23 September 2021,https://link.springer.com/article/10.1007/s10614-021-10193-8,Does the Real Business Cycle Help Forecast the Financial Cycle?,December 2022,Fredj Jawadi,Hachmi Ben Ameur,Alexis Flageollet,Unknown,Unknown,Male,Male,"The forecasting of the dynamics of the financial market as a whole and asset prices in particular has always been at the center of the debate between eminent economists. Indeed, Eugène Fama, who is considered the father of modern financial theory, refers to his theory of market efficiency by suggesting the impossibility to forecast asset prices in the short-term. However, Robert Shiller, who shares the 2013 Nobel Prize in Economics with Eugène Fama, shows it is possible to predict asset prices in the long run. Accordingly, forecasting the dynamics of the financial sector has always been a challenge.Footnote 1 That is, in line with Shiller, significant attention has been paid to the analysis of the business cycle, as well as the financial environment. This is particularly relevant as the recent global financial crisis, induced by the US subprime crisis of 2007, has led to an economic downturn for most developing, developed, and emerging countries during 2008–2009. The interaction between the US real sectorFootnote 2 and the financial sector has also shown further evidence of significant connectedness in the context of the ongoing Covid-19 crisis. In the literature, numerous studies have investigated the impact of the financial sector on the business cycle (Cover & Mallick, 2012; Rafiq & Mallick, 2008; because the financial sector has always been considered as an important factor of production and a key economic growth driver. For example, Aikman et al. (2010) and Schularick and Taylor (2012) investigated the effect of credit on the real economy and Hatzius et al. (2010) and Benetrix and Lane (2011) measured the effect of the financial sector on the real business cycle. More recently, Drehmann et al. (2012) and Borio (2014) found further evidence of a significant relationship between the financial cycle and the macroeconomy, while Niemira and Klein (1994) developed channels to document the usefulness of financial (respectively economic) information to forecast the dynamics of the economic (respectively financial) cycle and to explain its fluctuations. Claessens et al. (2012) explained how financial and economic cycles interact, and showed strong linkages between these two cycles for 44 countries over January 1960–April 2010. In particular, they found that recessions relating to financial issues such as equity prices raids are longer and deeper than other recessions. Nevertheless, the recovery after asset price busts is weaker, whereas recoveries related to rapid growth in credit are stronger. Accordingly, the authors confirmed the effect of financial market development on the real economy. Moreover, Drehmann et al. (2012) empirically characterized the financial cycle while analyzing turning points and using frequency-based filters. They identified the financial cycle using the medium-term approach, and found that financial cycle peaks are close to financial crises, and the length of the financial cycle has augmented since 1985. Accordingly, they concluded that financial cycles are longer than economic business cycles. Further, the recessions of the business cycle are more profound when associated with the contraction phase of the financial cycle. Recently, Schüler et al. (2015) applied a multivariate spectral approach to characterize financial cycles, and identified common cycle frequencies across a set of indicators. Focusing on 13 European Union countries over 1970–2013, they showed that credit and asset prices share cyclical similarities. They also proved that the financial cycles appear long in upswing phases, with an important dispersion across countries. Interestingly, the concordance between financial and economic cycles appears only 2/3 of the time, whereas that for financial cycles is apparent across countries suggesting further evidence of high heterogeneity. Otherwise, the analyses of the usefulness of the information provided by the business cycle to forecast the financial sector are rather scarce. However, the exploration of this relationship is relevant because an investor can focus on economic conjectures before deciding to invest. Therefore, the causality from the economic conjectures to the financial sector might be different. On the one hand, at least two main macroeconomic variables (oil price and the US dollar exchange rate) have experienced important changes and volatilities over the last decade, which have impacted the level and the duration of the phase of the business cycle. These changes have affected the cost and level of production and thus, company behavior, as companies balance investment to deal with this new economic conjuncture. Accordingly, the unemployment rate and poverty have increased, resulting in a negative shock to consumer behavior and an increase in the saving rate per measure of precaution. On the other hand, most central banks (e.g., Fed, ECB, Bank of England) have switched to new central banking rules dominated by unconventional monetary policy (e.g., quantitative easing, qualitative easing, lender of last resort, purchase of public debt, important increase of the size of the balance sheet of central banks). This switch from conventional to unconventional monetary policy is due to the zero lower band effect, which occurs when the short-term nominal interest rate is near zero. At this interest rate level, there exists a liquidity trap and the central bank is less able to stimulate economic growth. These new central banking rules also implied massive injections of liquidity into the market and are expected to impact the financial sector as well. As such, all things being equal, these factors have directly or indirectly affected financial resource allocations and thus financial cycle fluctuations. This yielded a priori close time-varying correlations between the economic and financial cycles, giving credit to Minsky’s theories of cycles and the movement of the financial system from stability to fragility followed by crisis regarding the evolution of macro-economic indicators and credit conditions.Footnote 3 To fill this research gap, our study proposes to check whether economic indicators can help forecast the dynamics of the financial sector. Particularly, we propose to investigate the relationship between the financial and business economic cycles as a whole and also across different phases of these cycles. To this end, we estimate the financial turning points through an analysis of turning times and fluctuations of the economic cycle. In other words, we identify the drivers of the volatility of the economic cycles and test how far they impact the fluctuations of the financial cycle. Accordingly, our study contributes to the literature in different ways as follows. First, rather than using proxies for the economic and financial cycles as in previous studies, we carry out data analysis using an exhaustive list of macroeconomic and financial variables to specify the weighted factors through principal component analysis (PCA) to measure economic and financial cycles. Second, we apply the BBQ approach proposed by Bry and Boschan (1971) to identify the cyclical peaks and troughs for both the economic and financial indexes. Third, we identify for each index the number of cycles and distinguish the different phases of each cycle. Finally, we investigate the connectedness between the financial and economic cycles while estimating their bilateral dependency per phase. Our findings show evidence of significant connectedness between the economic and financial cycles, with an intensity of dependency that is time-varying regarding the phase of the cycle. This is in line with the findings of Barnett et al. (2015). The remainder of this paper is organized as follows. Section 2 presents the data and Principal Component Analysis PCA methodology. Section 3 presents the BBQ approach to computing cycles and identifying their turning points. Section 4 proposes a parametric modeling using Vector Autoregressive Model of the connectedness between the financial and real business cycles. Section 5 concludes the paper.",2
60.0,4.0,Computational Economics,17 September 2021,https://link.springer.com/article/10.1007/s10614-021-10195-6,How do Fines and Their Enforcement on Counterfeit Products Affect Social Welfare?,December 2022,Marta Biancardi,Andrea Di Liddo,Giovanni Villani,Female,Female,Male,Mix,,
60.0,4.0,Computational Economics,23 October 2021,https://link.springer.com/article/10.1007/s10614-021-10196-5,Correction to: The Cross-Shareholding Network and Risk Contagion from Stochastic Shocks: An Investigation Based on China’s Market,December 2022,Yun Feng,Xin Li,,,,Unknown,Mix,,
61.0,1.0,Computational Economics,24 September 2021,https://link.springer.com/article/10.1007/s10614-021-10197-4,A Novel High Dimensional Fitted Scheme for Stochastic Optimal Control Problems,January 2023,Christelle Dleuna Nyoumbi,Antoine Tambue,,Female,Male,Unknown,Mix,,
61.0,1.0,Computational Economics,27 September 2021,https://link.springer.com/article/10.1007/s10614-021-10198-3,Forecasting the Dynamic Correlation of Stock Indices Based on Deep Learning Method,January 2023,Jian Ni,Yue Xu,,,,Unknown,Mix,,
61.0,1.0,Computational Economics,05 October 2021,https://link.springer.com/article/10.1007/s10614-021-10199-2,A Study of the International Stock Market Behavior During COVID-19 Pandemic Using a Driven Iterated Function System,January 2023,Aman Gupta,Cyril Shaju,Kamal,Male,Male,Male,Male,"Financial markets all over the world witnessed sharp volatility during the ongoing COVID-19 pandemic. This is something the financial world had not seen earlier. Recessions in the last 75 years can usually be categorized as one of the three factors—financial bubbles, oil shocks, or policy mistakes. According to Deloitte U.S.A., the rapid economic deterioration of economies and stock markets amid the COVID-19 threat represents a new category: a global societal shock (Ranjen, 2020). We saw a sharp decline in the stock market from 19th February 2020 through 23rd March 2020. This crash was, however, recovered significantly quicker than expected, creating an almost ‘V’ shaped recovery. The economic impact of pandemics has been studied earlier, such as how HIV/AIDS impacted the economy discussed by Haacker (2004), while Santaeulalia-Llopis (2008) focused on the impact of the HIV/AIDS pandemic on development. Yach et al. (2006) discussed the costs of the global growth of obesity and diabetes. All of them conclude that these pandemics bring economic disruption, loss of employment, and loss of foreign direct investment which resulted in a global recession in the case of the COVID-19 pandemic (Goodell, 2020) also. A black swan event that is not related to the pandemic but resulted in similar behavior in the financial market was the financial crisis of 2008–2009, as discussed by Grout and Zalewska (2016). Berkmen et al. (2012) studied how an economic crisis affects different countries. The study concluded that the countries having more leveraged domestic financial systems, stronger credit growth, and short-term debt suffer the most economy-wise. The recovery of the market has also been tried using mathematical tests (Yanglin et al. 2020). The current pandemic crisis has forced many financial researchers to study its effects in a short time (Nicola et al. 2020; Zhang et al., 2020; Zaremba et al., 2020; Ali et al., 2020). Most of these studies use a statistical or mathematical modeling approach that sometimes is difficult to comprehend at first hand. We propose a novel method based on fractals, popularised by Benoit Mandelbrot (1982) in the last quarter of the last century that is becoming increasingly popular in all branches of sciences. With a little use of traditional statistical methods, we aim to investigate the impact of the novel coronavirus disease (COVID-19) on the financial markets worldwide using the driven iterated function system approach, which uses the concept of fractals and chaos game. Fractals have widely been used for the study of financial markets (Alves, 2019; Bianchi & Frezza, 2017; Kristoufek, 2013; Lux, 1998) but with complex methodologies. We use a simple and widely used concept of chaos game representation (CGR) of DNA sequences (Jeffrey, 1990; Almeida et al, 2001; Randhawa et al., 2020, Pratibha et al. 2020) and modify the CGR method to accommodate the financial data. The CGR method has been very successful in analyzing genome sequences but has not yet found applications in economics and finance. We, for the first time, use a modified CGR technique to analyze the financial market behavior and demonstrate its application to the index fund futures during the COVID-19 pandemic. In the next section, the data and the methodology used in this study are described in detail, followed by the presentation and discussions of the results obtained from this study in Sect. 3. The major outcome of the study is highlighted in the conclusions Sect. 4.",9
61.0,1.0,Computational Economics,13 October 2021,https://link.springer.com/article/10.1007/s10614-021-10200-y,DSGE-SVt: An Econometric Toolkit for High-Dimensional DSGE Models with SV and t Errors,January 2023,Siddhartha Chib,Minchul Shin,Fei Tan,Unknown,Unknown,,Mix,,
61.0,1.0,Computational Economics,18 October 2021,https://link.springer.com/article/10.1007/s10614-021-10201-x,Finding the Impact of Market Visibility and Monopoly on Wealth Distribution and Poverty Using Computational Economics,January 2023,Kashif Zia,Umar Farooq,Sakeena Al Ajmi,Male,Male,Unknown,Male,"The decades-old tradition of understanding the market-based economy and trade using equations-based linear and highly non-linear Dynamic Stochastic General Equilibrium (DSGE) modeling is still in practice. However, these approaches are mostly incapable of explaining the individual-level dynamics contributing to and influencing the global economic indicators (Gooding 2019; Stiglitz 2018). The disparities induced by the market-based economy are well established without concrete evidence of the reasons (Chappe and Semmler 2019). Therefore, recently, a debate has started to deviate from this tradition (the use of equations-based linear and non-linear approaches) and adopt a fresh perspective (Gräbner et al. 2019). Towards this, Gooding (2019) proposed using agent-based modeling to study the free market economy, in order to find out the conditions that could lead to a fairer society in terms of wealth distribution. This paradigm shift (from equation-based to agent-based modeling of the market-based economy) is needed for two important reasons. Firstly, the practiced theories and models have failed to provide sustainable solutions to the complex problems of wealth disparity, inflation, and poverty. This could be due to the nature of academic and research discourse, which is popular in this domain for many decades. Therefore, it is necessary to adopt a different perspective and recreate an understanding at a different level  (Tesfatsion 2002, 2003, 2006). Secondly, the multidisciplinary research in complexity science Downey (2018) (the knowledge and expertise of different domains together to tackle complex problems) has evolved as a promising science. According to Davis and Simmt (2003), complexity science is the branch of science, which deals in learning systems. These systems are understood in terms of adaptive behaviors that occur in response to interactions among agents. Recent years have witnessed a remarkable coherence in economic structures in terms of complexity. The new face of economic order seems to emerge from complex interactions constituting the evolutionary economic process. This has led to a phenomenon called economic complexity (Chen 2017; Kuznets 1995). According to the advocates of this paradigm shift, complexity science is capable of understanding market economy and trade at a different level Fei-Yue (2004), as it has successfully played its role in related domains (Mitchell 2009; Phelan 2001). Literature suggests that there are many weaknesses in traditional theories of economics. It is a known fact that free-market economy (and capitalism) offers benefits to the society, however, it has numerous side effects. Capitalism has, certainly, provided opportunities to strong individuals and it perfectly matches the accepted evolutionary principles. However, it is also true that it leaves the majority in pain. According to Gooding (2019), it is not sustainable. He claims that the most sustainable entity around us is nature, which is not exhibiting an individualistic selfish approach for supporting survival of the fittest but instead using a collective selfless approach for managing the total ecological system. The same is needed to reflect in economic theories and practices, where it is needed to explore the potential of connectedness and inclusion as a driving force instead. Humans have a very diverse behavior and their diversity comes in many different forms. Psychologically, they vary in terms of personality and cognitive behavior. Socially, they possess different beliefs, cultures, norms, and biases. Biologically, they are not only different genetically but also in terms of inherited characteristics. These differentiations are collectively called human factors (Chen 2017; Hamill and Gilbert 2016). However, the studies in economics reflect humans usually in homogeneous fashion, also known as representative agents. This trend must be changed now, as it is observed that these differentiations in humans exhibit a collective global behavior based on their factors-driven local interactions. Such a simplistic and at the same time complex (mainly due to inter-human interactions) behavior is wrongly associated with a representative agent. In complexity science, this phenomenon is called emergence. Unfortunately, the modeling techniques capable of incorporating agents of heterogeneous nature and demonstrating the emergent aggregate behavior based on local interactions are not well-established in economics (Chen 2017). There are, however, certain well-established interdisciplinary fields/domains, whose models are based on common micro-foundations, which are already emerging as contradictory concepts compared with conventional practices in economics. These research domains include behavioral economics (Akerlof 2002) and econophysics (Aoki 1998) emerged from psychology and mathematics, and agent based computational economics (Chen and Wang 2004) emerged from the field of computer science (Chen 2017). Complexity science offers a different way of thinking for systems with collective behavior. This collective behavior emerges from the interactions of localized units, which have usually very primitive behavior (Dent 1999). This collective behavior cannot be predicted in most of the cases, and it, therefore, often unfolds with surprising aspects (Allen et al. 2010). In the start, the physicists had significant resistance against this non-traditional anti-Newtonian science of complexity. They, however, finally recognized its importance. The adoption of complexity science in physical sciences has resulted in several interesting domains (Sayama 2015) such as distributed artificial intelligence (Gasser and Huhns 2014), complex adaptive systems (Holland 1992), self-organized systems and swarm intelligence (Blum and Merkle 2008). Economics is linked more tightly with the complexity of society and ecology than physics. Thus, it is the right time to adopt the concepts of complexity science for solving our current economic problems. Students of the Universities have worked out the possible changes into their syllabuses (Wall 2018). Research councils have started allocating huge funding for transforming the macro and micro economics (Raworth 2017). This is, perhaps, due to the blames on economic policies based on traditional linear and non-linear DSGE systems for financial crashes and poverty (Stiglitz 2018; Watkins et al. 2017). The best way, probably, to define the complexity science in economics is to compare it to the theory of linear and non-linear DSGE systems. These systems build models based on equations, which are used to represent reality. These equations are solved together to get an equilibrium. The systems are assumed to have a set of two equilibria: one representing the system before while the other after the solution. However, complex systems do not assume equilibrium and, thus, they are not composed of two states. These systems, instead, evolve over time and, hence, their histories do matter. As a result of continuous evolution, there are chances of abrupt changes in the system. A complex system adapts according to its state and response of global feedback. It might not only transform the cause-effect modality of a single local variable but also change the relationship between the variables. Consequently, the historically observed correlations might be invalid for predicting futuristic perspectives. Instead of the current research trend of using mathematical modeling for researching economics, the computational economics believes in experimental models. Gooding (2019) advocated that, for a fairer society, it is needed to focus more on applied computational economics (Miranda and Fackler 2004). However, it is premature to think of this paradigm shift as a game-changer. It depends on experimentally proving the perceived usefulness of the proposed paradigm, which considers economics a complex system rather than just a traditionally equation-based system. Complex systems can be modeled using the concept of multi-agent systems, where the agents interact with the environment or other agents but only within their locality. They get influenced by global level feedback. Agent-based systems evolve/emerge without knowing their direction. The future is, therefore, impossible to predict in these systems as in mathematical equations. The only thing, we could do is, to generalize the system with some known established properties of emergence (Chen 2017). Agent Based Modeling (ABM) is one of the most established techniques for studying complex systems. It is capable of modeling the complexities that emerge as a result of interactions among individuals. ABM possesses the strength required to build and analyze abstract, conceptual, and experimental models. The emerging field of computational economics is all about developing and experimenting with, these kinds of models for the field of economics (Hamill and Gilbert 2016). The use of agents in modeling has a rich past. They were first introduced in Physics in 1930. This has especially proved to be a valuable technique to model systems made up of people to discover the changing patterns under human behavioral laws (Chen 2017; Hamill and Gilbert 2016). ABM is also adopted in economics domain, as a part of the larger domain of computational economics (Gooding 2019). In computational economics, the agents are used to represent individuals, companies, or institutions. One of the most important characteristics of ABM is modeling in space and time (Hamill and Gilbert 2016; Chen 2017), which exhibits a great relevance for the systems that involve trading and other economy related concepts (Gooding 2019). This paper is an attempt to highlight the problems of a free-market economy and identify why traditional methods are unable to understand such complex systems. Towards this, we have developed and simulated a stack of agent-based models. Our models extend Gooding’s toy trader model (Gooding 2019) and the simple economy model (Wilensky and Rand 2011). The former represents a basic fair trading environment while the latter one relates complexity science to the theory of free-market economy. These extended models are more realistic due to the introduction of two major characteristics of the real-world economy. The first is about the variation in market visibility among the traders and the second is about the existence of a monopoly. The main objective of these extensions was to find the impact of market visibility and monopoly on wealth distribution, which are expected to lead towards social and economic inequality and, thus, ultimately result in poverty. This paper is organized as follows. The research domain and the problem undertaken in this study are presented in Sect. 1. Section 2 presents a detailed literature review along with the motivation and research gap. The agent-based models, developed in NetLogo, are investigated in Sect. 3 with an aim to find out the strength of computational economics. Section 4 presents the proposed models to overcome the limitations of existing models. Detailed evaluation of the proposed models and their comparison with the state-of-the-art models is also presented in Sect. 4. This paper ends with the conclusion and future directions presented in Sect. 5.",
61.0,1.0,Computational Economics,29 September 2021,https://link.springer.com/article/10.1007/s10614-021-10202-w,Multivariate Picture Fuzzy Time Series: New Definitions and a New Forecasting Method Based on Pi-Sigma Artificial Neural Network,January 2023,Eren Bas,Erol Egrioglu,Taner Tunc,Male,Male,Male,Male,"Picture fuzzy sets are a generalized form of fuzzy sets. The uncertainty of the time series is better handled by using picture fuzzy sets than fuzzy and intuitionistic fuzzy sets. Fuzzy time series was firstly defined based on fuzzy sets in Song and Chissom (1993). Fuzzy time series have been used to forecast many types of time series in the literature and produced the best results for some kind of time series such as stock exchange data sets in many studies. Despite some unsolved problems, many researchers focused on fuzzy time series methods. Fuzzy time series methods are methodologically improved but many researchers continue to ignore membership values in their methods by using only ranks of fuzzy sets. Although membership values are the most important contribution of fuzzy sets, the researchers did not use membership values for defining the fuzzy relations in their methods. Researchers focus more on improving the fuzzification step and fuzzy relation tables which were firstly used in Chen (1996). Various artificial optimization techniques and clustering methods have been also used in the fuzzification step. Recent literature about fuzzy time series is reviewed and summarized below. In the last three years, many new studies have been published about fuzzy time series. Cagcag Yolcu and Lam (2017) proposed a robust fuzzy time series approach. Their method is reflected in outliers in the time series. Singh and Dhiman (2018) proposed a multivariate fuzzy time series method based on granular computing. They did not use the data partition strategy and they did not use a test set to see the real performance of their method. Their method was run very well for in-sample data. Guan et al. (2018) proposed a fuzzy time series method based on neutrosophic sets. Their method is multivariate and based on fuzzy relation tables. They applied their method to Taiwan Stock Exchange Capitalization Weighted Stock Index (TAIEX) data set in the year 2004 and others. Abhishekh et al. (2018) proposed an intuitionistic fuzzy time series method based on intuitionistic fuzzy relation tables. They focused on in-sample performance and the test set were not used in the paper. Xian et al. (2018) proposed a fuzzy time series method based on an improved artificial fish swarm optimization algorithm for determining interval lengths. They applied their method to enrolment data and they showed the good performance of their method. Cheng and Chen (2018) proposed a fuzzy time series method and they emphasized some problems of fuzzy time series by omitting huge literature about the problems. Abhishekh Kumar (2019) proposed a two-factor fuzzy time series method. They focused on in-sample performance and the test set were not used in their paper. Ilieva (2019) proposed a new fuzzy time series method based on a fuzzy transition matrix. The transition matrix is not new in the literature. They did not make a comprehensive comparison to see their method performance. For a one-time series, they found competitive results. Bose and Mali (2019) presented a review of fuzzy time series methods. They classified fuzzy time series methods and the paper made an important contribution to fuzzy time series literature. Chen et al. (2019) proposed a fuzzy time series method. In their method, particle swarm optimization (PSO) was used to determine optimal interval lengths and fuzzy group relation tables are obtained. They applied their method to Enrollments data, Turkey spot gold time series data, and Belgium car accident data. Sadaei et al. (2019) proposed a multivariate fuzzy time series based on a convolutional neural network. In their methods, hourly time series data was converted into images to be input for the convolutional neural network. Jilani et al. (2019) proposed a fuzzy time series method for forecasting hospital emergency department attendance. Khan and Khan (2019) used the fuzzy time series method for forecasting CO2 emission and global temperature. They compared fuzzy time series performance with adaptive-network-based fuzzy inference systems (ANFIS) and artificial neural networks (ANN) methods. They founded that the performance of the fuzzy time series was significantly better than others. Guan et al. (2019) proposed a forecasting method based on neutrosophic soft sets. They converted observations of time series to neutrosophic soft sets and established relation tables based on these sets. Liu and Zhang (2019) proposed second-order fuzzy time series forecasting method. They proposed a new method and applied it to the stock exchange data set. They focused on in-sample performance and the test set were not used in the paper. Ahmadi et al. (2019) proposed a hybrid algorithm in multivariate forecasting. Dong et al. (2019) proposed a multifactor fuzzy time series method to predict the trend of aircraft control surface damage. Their method uses a fuzzy relation matrix like the first paper of fuzzy time series. The experimental results proved the feasibility and its high forecasting accuracy of the proposed method. Wei et al. (2019) proposed a hybrid method of empirical mode decomposition, fuzzy time series, long short-term memory, and multi-objective grey wolf optimization. They applied their method to wind speed and electrical power load time series. They compared their method with many deep ANN models. Their method was outperformed. Guo et al. (2019) proposed a new fuzzy time series method based on axiomatic fuzzy set theory. This study initiated a new kind of fuzzy time series method. In the paper, results for application to the TAIEX data set were given and the proposed method is better than some others. Bisht and Kumar (2019) proposed time series forecasting method based on the intuitionistic fuzzy set. They applied their method to stock exchange data sets such as TAIEX. They found better forecasting results but they focused on in-sample forecasting but other method’s performance was out of sample forecasting. This is a serious problem to see the real performance of their method. Vovan (2019) proposed a fuzzy time series method for forecasting the variation of the data. They applied their method to M3 competition results. Their method was better than some of the other methods in the competition but compared methods have not good performances in the competition. Zhang et al. (2019) proposed a multivariate fuzzy time series method for forecasting stock exchange data sets such as TAIEX. Their method is the hybridization of fuzzy c means, genetic algorithm, and multilayer perceptron. They focus on the estimation of variations. Taghizadeh and Ahmadi (2019) used principal component analysis—Tukey and ARDL bound to test the statistical and econometrical analysis of knowledge-based economy indicators affecting economic growth in Iran. Fan et al. (2020) proposed an intuitionistic fuzzy time series method. Their method is multiple inputs- multiple outputs forecasting method and they proposed a special clustering method in their algorithm. They applied their method to network traffic forecasting. Li and Yu (2020) proposed a new fuzzy time series based on fuzzy relation tables. They proposed a different type of fuzzy relations and hybridized two different algorithms in their method. They used a synthetic and a real-world time series to see the performance of the proposed method. Their method is better than some others in the literature. Ahmadi (2020) used logarithmic fuzzy preference programming for the economy growth-effected knowledge-based indicators. Panigrahi and Behera (2020) proposed a new fuzzy time series methods. Their methods are based on a modified average-based method for determining interval length and deep belief networks, support vector machines, and long short-term memory for defining fuzzy relations. The support vector machines-based fuzzy time series method has the best performance and the shallow network was better than deep alternatives in the paper. Silva et al. (2020) proposed a probabilistic fuzzy time series method. Their method presents point and interval forecasts and the distribution of forecasts. They calculate empirical probabilities for fuzzy sets but in probabilistic forecasting methods, probabilities are generally calculated for crisp forecasts. The papers’ contributions are very important. Some other papers produced probabilistic fuzzy time series methods before them but they omitted other papers: Egrioglu et al. (2016) and Yolcu et al. (2018). When the recent literature is examined, the following research findings and directions can be observed: Some authors only focused on the in-sample forecasting performance of the fuzzy time series method. This is criticized by the forecasters’ community. Deep artificial neural networks have been started to use fuzzy time series methods. The new methods will be based on advanced artificial neural network types. Enhanced fuzzy set types such as intuitionistic fuzzy sets, neutrosophic sets have been started to use in fuzzy time series methods. Using these sets creates new research directions for fuzzy forecasting methods. According to these findings and directions, the motivations of this method can be given below: Picture fuzzy sets are the general form of classical fuzzy sets and provide more information for modelling procedures. Multivariate forecasting methods can be useful for some kind of time series because some time series have casual relationships. Advanced artificial neural networks can provide more modelling capability for fuzzy time series methods. In this paper, we proposed a new multivariate picture fuzzy time series method. The proposed method is based on picture fuzzy sets. The new multivariate picture fuzzy time series and its forecasting model are firstly defined in this paper. The defining of fuzzy relations is made by Pi-Sigma artificial neural network (PS-ANN) as an advanced artificial neural network type. In the second section of the paper, picture fuzzy sets and picture fuzzy c-means are summarized. In the third section, particle swarm optimization is summarized. In the fourth section, brief information about Pi-Sigma artificial neural networks is given. In the fifth section, the required definitions for the proposed method are introduced. In the sixth section “multivariate picture fuzzy time series forecasting method” is introduced. In the seventh section, the application results are summarized and finally, the conclusion and discussion parts of the paper are given in the eighth and ninth section respectively.",1
61.0,1.0,Computational Economics,30 September 2021,https://link.springer.com/article/10.1007/s10614-021-10203-9,Multivariate Regime Switching Model Estimation and Asset Allocation,January 2023,Kai Zheng,Weidong Xu,Xili Zhang,Male,Unknown,Unknown,Male,"The state of financial markets often changes abruptly. Although some changes may be transient (jumpy), the fluctuations of asset price changes usually last for a longer period of time. For example, at the beginning of the 2008–2009 global financial crisis, the mean, volatility and correlation patterns of stock returns had undergone tremendous changes, which continued until the end of the crisis (Fonseca and Wang 2016). Changes in the behavior of fixed income, stock and foreign exchange markets, and many macroeconomic variables are in similar condition (Bhar et al. 2015). Some of them are reduplicative, such as recessions and expansions, while others are permanent, such as structural breaks. The Markov Regime Switching (MRS) model can capture these sudden changes in behavior, as well as the phenomenon that new patterns of prices and fundamentals will continue for a period of time after the changes (Ang and Timmermann 2012). Regime-switching models have been widely applied to financial modeling, including stock market return (Hardy 2001), risk management (Elliott and Siu 2010), option pricing(Wahab and Lee 2011), and pension funds (Siu 2012). The wide adoption of the MRS model in finance and economics is partly due to the fact that the regime switching observed are closely related to the various stages of the business cycle (Campbell 1999; Cochrane et al. 2005; Ielpo 2012; Ma et al. 2019; Lopes and Zsurkis 2019). The length and intensity of the cycle are irregular, and the duration of a complete cycle can range from one to twelve years. On the basis of the Black-Scholes pricing model, assuming that asset prices follow the log-normal regime switching process, the resulting pricing model can significantly improve the fitting effect (Hardy 2001; Duan et al. 2002). The MRS model can produce a large number of nonlinear effects through Gaussian Mixtures or other different types of distributions, as well as provide a good approximation of the yield generation process. However, the existence of regime switching in financial data also poses new challenges to traditional strategic asset allocation and other issues. Currently, the MRS model mainly adopt the maximum likelihood estimation (MLE) and the expectation maximization (EM) algorithm (Hamilton 1989) as estimating method. The Bayesian update process is used in the EM algorithm to infer the probability of the current regime based on all available information. The improved maximum likelihood method uses Gibbs Sampling to estimate the posterior distribution of the model (Albert and Chib 1993; Kim et al. 1999). The core of the above method is to find the parameter estimation to maximize the likelihood function. Due to the complexity of the model, current estimation methods have some shortcomings. First of all, the calculation of the likelihood function of the regime switching model is path-dependent, which leads to higher computational complexity of the optimization algorithm. Secondly, the likelihood function does not satisfy the convexity, so there may be multiple local optima, which may cause the optimization algorithm to fail to converge to its global optima. And the optimization algorithm is more sensitive to the selection of initial values, which may cause some difference in the optimization results. Finally, in the M step of the EM algorithm, the maximum conditional expectation is adopted to speed up the convergence speed of the likelihood function. However, it requires the probability distribution function to have good analytical properties, such as reversible, second-order derivable, etc. In this paper, we consider a more general framework of a multi-state regime switching model using multi-variate time series data. The multi-variate setting is a very important feature in financial market, especially in Asset Allocation. The multivariate MRS model on the dynamic correlation between cross markets and cross assets has been widely used. Different from the univariate Markov Regime Switching case, the number of parameters to be estimated in the MRS model is mainly determined by two factors, namely the number of states and the dimension of variables. When the model has m independent variables and K mechanisms, there are a total of \(K(m^2+3m+2K-2)/2\) parameters to be estimated. Therefore, with the respect to the number of states and independent variables, the number of parameters to be estimated in the model increases with the progression of \(O(K^2)\) and \(O(m^2)\), respectively . As the scale of the model increases, the shortcomings of traditional estimation methods are further magnified. In order to achieve the same estimation accuracy, the optimization algorithm put forward higher requirements on sensitivity and robustness. For multi-variate MRS model, an existing approach is to turn a multivariate problem into a univariate problem by finding proxy variables for the mechanism. Jiang et al. (2015) use univariate MLE method to calculate the states among 21 markets by estimating the new index which is weighted by cross volume. Mulvey and Liu (2016) used the states estimated by the S&P500 index and studies the allocation of stocks, bonds and commodity futures. Compared with direct estimation with multivariate data, the proxy univariate method can reduce the difficulty of model estimation by reducing the number of parameters to be estimated, and improve the efficiency of estimation and robustness. But the disadvantage is that it loses a lot of valid information, such as the variational correlation between variables over time. Zheng et al. (2019) proposed a spectral clustering algorithm for univariate MRS model, which could avoid the drawbacks of MLE mentioned above. In this paper, a spectral clustering method is proposed for estimating univariate Hidden Markov Model(HMM), which is proved to be more robust than the MLE method by numerical simulations. The proposed method depends on pairwise feature similarities based on local observations. This paper only considered the case where a single time series is governed by the hidden states. While in-sample fitting is considered in this paper, out-of-sample prediction is not discussed. Inspired by and extended from Zheng et al. (2019), this paper further proposes an improved spectral clustering algorithm for multi-variate MRS model (Multivariate SC-HMM) to improve the recognition accuracy and estimating accuracy of parameters. Therefore, in this paper, the research on the estimation method of the multi-variate MRS model and the proposed Multivariate SC-HMM provides new ideas for exploring the study of cyclical financial phenomena among multiple research objects, and is a very useful supplement to the existing empirical research. In order to test the efficient setting of the multi-variate MRS model and the proposed Multivariate SC-HMM algorithm, we consider a multi-asset portfolio allocation problem based on the Chinese financial market, to see whether the strategy based on regime switching can bring excess returns. China Economic growth has shifted from high-speed to medium-to-high speed. The financial market has continued to transform and upgrade (Singh and Roca 2020). In the long run, the trend of asset prices has demonstrated obvious phase characteristics, which means most of them are with the trait of structural instability, including structural changes (Wang et al. 2021). Therefore, regime switching is adopted for further study in the characteristics of asset returns and their correlations in different economic situations. Domestic research believes that there is a certain correlation between stocks and bonds in the Chinese market, and this correlation is time-varying. However, there is still controversy about whether the correlation coefficient is positive or not. In addition, Chinese financial market is marked with late start and individual investors have received little asset allocation and risk management education. Investors are often prone to behaving irrationally when the market changes. This paper focus on the research of the Chinese stock market through a MRS model, obtaining its return and correlation characteristics in different market conditions, and providing a state prediction method. By designing strategies without any other additional forecasting skills and studying the effectiveness of regime switching strategies in the Chinese market, the characteristics of asset returns and performance of the proposed method could be seen more directly. The main contributions of this article are twofold. First, we design an estimation algorithm for multi-variate MRS model based on spectral clustering method (Multivariate SC-HMM). The number of parameters to be estimated in the MRS model increases rapidly with the number of independent variables. Another difference between univariate and multi-variate MRS is that the dependent variables in the model can be related. Based on whether the dependent variables of the multivariate model are related or not, the paper designs different numerical experiments to find out its separate impact on the estimation results. Also, the paper redesigns the feature vector based on local correlation and find out the scalability of the algorithm on the multivariate model. In addition, the paper analyzes the algorithm’s recognition rate of the hidden state accuracy and robustness of the estimation result through the Monte Carlo numerical simulation method. Second, we develop some strategies asset allocation in Chinese stock market based on Regime Switching. The asset allocation strategy based on Regime Switching has been around for a long time, while there still exist disputes over whether it can obtain higher returns than static asset allocation strategies under realistic conditions in the academic circles. There are two reasons account for this. First, the strategy based on regime switching is excessively frequent, so that the transaction costs will offset part of the benefits brought by timing. Second, some research conclusions are not purely drawn from out-of-sample predictions or some of them have used additional prediction techniques in the regime switching process. Based on Chinese stock market, This paper established a three-state MRS model for the Chinese stock market and studies the time-varying characteristics of the return and correlation of the Chinese stock index. In addition, the paper designed a simple asset allocation strategy without additional forecasting skills, and the performance was compared with the 1/N strategy. And we also discussed the sensitivity of the actual asset allocation when transaction costs are considered. The structure of the paper is as follows. Section 2 introduces the feature vector construction and proposed algorithm (Multivariate SC-HMM) for multivariate MRS model. Section 3 gives the numerical analysis results on the algorithm with two model structure. Section 4 shows a three state MRS model result for Chinese stock market. Section 5 test the performance of a simple regime switching asset allocation strategy in Chinese market. Section 6 concludes the paper.",2
61.0,1.0,Computational Economics,02 October 2021,https://link.springer.com/article/10.1007/s10614-021-10204-8,Hedging the Risks of MENA Stock Markets with Gold: Evidence from the Spectral Approach,January 2023,Awatef Ourir,Elie Bouri,Essahbi Essaadi,Unknown,Male,Unknown,Male,"Stock markets play a key role in economic activity. They become the mirror of the health of each economy. When businesses are growing and the economy is expanding, investors are returning to the stock markets to take advantage of this expediency. Not only does the economy grows, but its fundamental structure changes across time. The changes in several economies and their opening to the international markets supported the increase in the frequency of shocks and crises in such markets. In looking for opportunities, many investors give attention to the potential risk of these actions, so to hedge, they diversify their portfolios and/or purchase derivative products that limit their losses. After the double COVID-19 crisis (supply and demand crisis) on the oil market and the fall in prices below zero, gold seems to always be a safe haven, and that it still retains its function as a store of value. Besides, due to a lack of futures market in MENAFootnote 1 economies we suggested a gold as hedge alternative in the stock market. As a part of their culture, gold occupies an important part of life in the MENA countries. The volatility and correlation dependency and interdependency for financial series among different markets are assumed to be time-varying due to the presence of shocks and structural changes. Under these circumstances, we investigate the dynamic relationship between two financial indicators (gold and stock markets) for selected MENA countries. While including gold in the portfolio reduces volatility, we can conclude that gold can provide diversification and hedging for investors. Using evolutionary spectral analysis, we can examine the time-varying relationship at different frequencies. The novelty of this study is two-fold. To our knowledge, we are the pioneers in studying the interdependence between gold and the stock market index for the MENA region as a whole. Our study extends also the related literature by using a non-parametric measurement that is based on the definition of the spectrum introduced by Priestley (1965, 1996) and which respects the properties emphasized by Loynes (1968). However, previous studies that introduced frequency analysis use wavelet theory. In the present study, we analyze the dynamic dependence between series. Our measure of time-varying coherence gives not only the dynamics of the correlation process but also indicates the frequencies at which they comove. Therefore we can determine the nature of the dynamic correlation process for a short run (high frequencies) and/or long run (low frequencies). The advantage of the frequency approach is that it detects the variability in the dependence process at different time scales. With this additional information, we can know which cycles are more relevant for each market. The use of frequency-domain allows us to distinguish between the properties of each cycle and to estimate the dynamic hedge ratio in each cycle. Using a nonparametric approach of spectral approach, we use seven possibilities to construct a hedged portfolio for each frequency. These frequencies correspond to a specific period T, which calculate by the ratio of \(2\pi /w\), where w is the studied frequency. The remainder of the paper is organized as follows. Section 2 presents the literature review for different hedging strategies and related analyses. Section 3 focuses on the methodology adopted in this paper. Section 4 presents data and analysis of empirical results. Section 5 concludes.",4
61.0,1.0,Computational Economics,01 November 2021,https://link.springer.com/article/10.1007/s10614-021-10205-7,Controlling Heterogeneous Structure of Smooth Breaks in Panel Unit Root and Cointegration Testing,January 2023,Tolga Omay,Perihan Iren,,Male,Female,Unknown,Mix,,
61.0,1.0,Computational Economics,03 November 2021,https://link.springer.com/article/10.1007/s10614-021-10207-5,Robust Portfolio Optimization Based on Semi-Parametric ARMA-TGARCH-EVT Model with Mixed Copula Using WCVaR,January 2023,Xue Deng,Ying Liang,,,,Unknown,Mix,,
61.0,1.0,Computational Economics,13 October 2021,https://link.springer.com/article/10.1007/s10614-021-10208-4,An Application of the IFM Method for the Risk Assessment of Financial Instruments,January 2023,Adrià Pons,Eduard Cristobal-Fransi,Jordi Vilaplana,Unknown,Male,Male,Male,"Customer experience has emerged as a new battleground in investment management. AI is changing how financial institutions attract and retain customers, and through this, offers the opportunity for firms to innovate and enhance the investor journey. What is certain now is that investment management firms can no longer rely solely on price and outperformance to attract investors. Firms that adapt their apps and integrate AI, data, and analytics into their bank solutions will be better placed to optimize and execute their product and content distribution strategies. On the other hand, human decisions are largely relied upon on how information is represented by third parties. According to Pompian (2017), the way investors think and feel affects their investment behaviors which is unconsciously influenced by past experiences and personal beliefs to the extent that even intelligent investors may deviate from logic and reason. Bollen et al. (2011) find that Twitter mood predicts subsequent stock market movements. Gilbert and Karahalios (2010) find that the level of anxiety of posts on the blog site Live Journal predicts price declines. Behavioral finance suggests that the investment decision-making process is influenced by various behavioral biases that encourage investors to deviate from rationality and make irrational investment decisions (Kumar and Goyal, 2015). Investors’ perceptions regarding the risk and return characteristics of a particular stock or the stock market are commonly assumed to be key drivers of their decision making (McInish and Srivastava, 1984; Antonides and der Sar, 1990). This means that investors must decide which risks to take and how much to take. While previous experiments have already shown that emotions can increase risk aversion (Kuhnen and Knutson, 2005, 2011; Knutson et al. 2008), our goal is to provide some prior knowledge of the likely risk scenarios that can face different type of investors (i.e. floor brokers, in-house traders, institutional traders), and to provide an algorithmic trading system for financial enterprises that intend to take full advantage of AI applying machine learning techniques to more accurately move its smart banking solutions from manual to semi-automated or fully automated processes and help their customers segments which have been traditionally underserved to become more rational and unbiased using the IFM method (inference function for margins) proposed in Xu (1996) and which is widely used on the financial industry. Precisely we have selected the assumption of skewed student-t distributed residuals on a GARCH(1,1) regression and copulas for the inter-dependencies, implemented by one of the biggest German banks (for disclosure contract, the name of the bank is confidential) because of its reliability. Models, whose only input are the historical prices of the forecasted securities. We believe that this method is not sufficient in certain market situations. The main purpose is the testing of the IFM method in different scenarios, where each scenario is composed of two simulations in two consecutive time intervals in the past (in normal market conditions and in specific market events that changed the course of securities). To assess the perpetuation of the risk extracted from both simulations. The risk measure used is the value at risk (Linsmeier and Pearson, 1996; Jorion, 1997), which estimates how much might a set of investments lose (with a given probability), in a set time period such a year. A Monte Carlo Simulation (Glasserman, 2003) with a GARCH process is performed in order to calculate the expected return, volatility and the value at risk (Wiener and Benninga, 1998), on both, portfolio and instrument level (Gueant 2012). The project gives an insight of the current research on the topic, followed by a detailed explanation of the method, the data-set used for the simulation, the results obtained, the discussion of the results and finally the conclusions for the future research of a new combined model.",1
61.0,1.0,Computational Economics,28 October 2021,https://link.springer.com/article/10.1007/s10614-021-10209-3,Unfolding Beijing in a Hedonic Way,January 2023,Wei Lin,Zhentao Shi,Ting Hin Yan,,Unknown,,Mix,,
61.0,1.0,Computational Economics,06 November 2021,https://link.springer.com/article/10.1007/s10614-021-10211-9,Diversification and Systemic Risk of Networks Holding Common Assets,January 2023,Yajing Huang,Taoxiong Liu,,Unknown,Unknown,Unknown,Unknown,,
61.0,1.0,Computational Economics,30 January 2022,https://link.springer.com/article/10.1007/s10614-021-10212-8,Are the Eurozone Financial and Business Cycles Convergent Across Time and Frequency?,January 2023,Dalia Mansour-Ibrahim,,,Female,Unknown,Unknown,Female,"The financial crisis of 2007 and the economic crisis of 2008 in the Eurozone (EZ) pointed out the relevance of studying financial disruptions simultaneously with economic recessions. Borio (2012) relates that recessions that coincide with contraction phase of the financial cycle are especially severe. Indeed, GDP tend to drop by around 50% more than otherwise when recessions coincide with the financial cycle’s contraction phase (Drehmann et al. 2012; Borio, 2012). The study of interactions between business cycles (BC) and financial cycles (FC) have thus, been more intensively studied after the occurrence of the 2008 economic crisis. According to Burns and Mitchell (1946), “Business cycles are a type of fluctuations found in the aggregate economic activity of nations that organize their work mainly in business enterprises [...] a cycle consists of expansions occurring at about the same time in many economic activities, followed by similarly general recessions, contractions, and revivals which merge into the expansion phase of the next cycle”. On the other hand, FC that will be studied in this paper are defined according to the definition of Borio (2012)Footnote 1 : “self-reinforcing interactions between perceptions of value and risk, attitudes towards risk and financing constraints, which translate into booms followed by busts. These interactions can amplify economic fluctuations and possibly lead to serious financial distress and economic dislocations”. In particular, Bernanke and Gertler (1999) and Kiyotaki and Moore (1997), suggested that BC and FCs interact when we consider the presence of financial frictions such as collateral constraints to borrow. Specifically, wealth and substitution effects can be amplified because of changes in access to external financing, including through the financial accelerator and related mechanisms (Claessens et al., 2012). Conversely, changes in the supply of external financing can affect corporations and households and thereby aggregate BC (Claessens et al., 2012). Hence, the interactions between BCs and FCs play an important role in shaping recessions and recoveries (Claessens et al., 2012). Our work relates to the literature studying the interactions between BCs and FCs within the Eurozone. Many studies focus on the BC convergence within the Eurozone (Hugues Hallett and Richter, 2006; Darvas and Szapáry (2008); Koopman and Azevedo, 2008). However, only a few of them studies the synchronization of FCs in the Eurozone (Merler, 2015; Schüler et al., 2015). And even a thinner part of the cycles convergence literature focuses on the synchronization between financial and BCs within the EZ (Oman, 2019; Hiebert et al., 2018). Overall, in spite of the rich research on cycles, empirical researches lack to understand the synchronicity between BC and FC within and between each country of the Eurozone. It either studies the interaction between BCs and FCs by focusing on whether it is the financial or macroeconomic variables that are leading, coincident or lagging indicators of the real economy excluding the Eurozone context (Helbling et al., 2011); or analyses the synchronicity of business (respectively financial) cycles between countries (Merler, 2015; Schuler et al., 2015; Coussin and Delatte, 2019). In the previous analysis, they mainly investigate the the synchronicity over time without evaluating the differences of synchronicity stemming from the frequency of the cycle. Yet, evaluating at which term cycles are synchronized would drastically improve the implementation of the monetary policy (Wälti, 2009). As noted by Mayes (2008), “simply looking at BCs loses much of the detail of the extent of co-movement in different frequency cycles within the euro area”. Their study focuses on the growth cycles of the core of the Eurozone in terms of frequency content and phasing of cycles; but it does not look at the Eurozone in terms of frequency content and phasing of BCs and FCs between and within Eurozone state members. Hence, findings suggesting that the Eurozone is synchronous (or not) have some shortcomings as they do not depict the short term, medium term and long term dynamics of BCs and FCs within and between countries. For instance, Aldasaro et al. (2020) distinguish two types of FCs. The Global FC defined by financial asset prices and capital flows and the Domestic Financial Cycle (DFC) characterized by credit and house prices. They found that the DFC is predominantly linked with medium-term cycles. In the context of the EZ, this is particularly important as the shock of 2007 led to different responses at different terms in the real economy of the EZ’s countries and revealed how much the economic and financial structures in the EZ were divergent. They are divergent not only in terms of amplitude but also through frequencies as they do not present the same development path (e.g. Greece and Germany). Hence, when the 2007 crisis occurred and was diffused asymmetrically in the EZ, the single monetary policy could not stabilize the economy as it should have. In other words, the symmetric financial shock of 2007 led to different response in the BCs of the EZ’s countries revealing how much the economic and financial structures were heterogeneous and unadapted to receive a unique monetary policy. As highlighted by Bayoumi and Eichengreen (1993), the Eurozone enclosed a core and a periphery group which were divergent. More precisely, if disturbances are distributed asymmetrically across divergent countries, there will be an occasion for an asymmetric policy response. And as the adoption of the euro leads to a loss of the monetary policy sovereignty, governments will no longer have the option of adopting a monetary policy which differs from that of the Union as a whole as a response to a country specific shocks. Insofar as monetary policy is useful for facilitating adjustment to disturbances, adjustment problems grew more persistent and difficult to resolve. Mundell (1961) preconized to have a high mobility of the production factors as a solution for asymmetric shocks absorption. But because of the language barrier, the strong national preferences and the different social systems, this solution is not applicable. Hence, the main issue for the EZ is its capacity to handle and absorb an asymmetric shock. Indeed, if the shock is symmetric, the European Central Bank (ECB) can intervene by implementing a common monetary policy which will suit all countries. But if a shock occurs in only one country and is not diffused in the others, or is asymmetrically diffused, the monetary policy cannot stabilize the economy as it should do. This paper addresses some of these gaps in the literature. I investigate the level of synchronicity of BCs and FCs within and between EZ state members using the wavelet approachFootnote 2 Maximum Overlap Wavelet Transform (MODWT) and Continuous Wavelet Transform (CWT). The wavelet analysis is a refinement of Fourier analysis and overcomes its shortcoming, as it allows one to take into account, both the frequency and the time variations of a time series simultaneously. More precisely, a cycle is decomposed into its shortest and longest frequencies over the years. Three main shortcomings of the literature are adressed here: (i) the convergence between BCs on the one hand and between FCs on the other hand between nine EZ’s state members; (ii) the convergence between BCs and FCs within nine Eurozone country; (iii) the convergence of BCs and FCs within and between state members of the Eurozone over time and across frequenciesFootnote 3. I thus address the following question: are the BCs and FCs in the Eurozone co-moving, synchronized and convergent over time and through frequencies? My paper is the first studying BC and FC’s convergence between and within Eurozone state members over time and through frequencies simulteanously. I contribute to the existing literature in several ways. First, I contribute to the very thin EZ’s FC literature, as my first objective is to evaluate the Eurozone convergence through FCs and BCsFootnote 4. Hence, I measure the DFCFootnote 5 in terms of property prices, private credit and credit-to-GDP ratio for the Eurozone (Borio, 2012) and establish the main interactions between FCs in the Eurozone. Secondly, I investigate the FCs and BCs at all frequencies and over time through the MODWT methodology following Crowley and Lee (2005); Crowley (2007). Thirdly, BCs have been a concern for two centuries and are still a major unsolved question in economy. In this paper, not only do I look into this issue in the context of economic changes in the Eurozone, but I also connect it to FCs. In other words, I estimate the classical BC of Burns and Mitchell (1946) and the NBER represented by the GDP and look at which term and when BCs are synchronized. Finally, I analyze the BC/FC interrelations within and between state members in order to establish if there is an intercountry and intracountry convergence of the FC/BC interaction. I find that there are strong divergences between cycles through frequencies at short term and after the occurrence of a crisis. In other words, the link between cycles changes over time but mostly across frequencies. I show that after the occurrence of a crisis, cycles tend to be less convergent in their trough phases during 8 quarters. I also find that there are a common BC and a common FC to the Eurozone state members but with divergent amplitudes. In terms of absorption of asymmetric shocks, it induces that there is structural divergences between state members and that the monetary policy cannot handle them because of the too strong differences in terms of amplitudes. My results confirm that FCs tend to be more correlated to BCs at medium term (Aldasoro et al., 2020; Drehmann et al., 2012; Borio, 2012). This paper is composed of five sections. Section 1 stresses the importance of studying cycles within the EZ. Section 2 presents the literature review. Section 3 develops the methodology and data used in this study. Section 4 shows some stylized facts of the Eurozone synchronization by distinguishing BCs and financial cycles. Section 5 analyses the empirical results.",1
61.0,1.0,Computational Economics,27 November 2021,https://link.springer.com/article/10.1007/s10614-021-10214-6,A Closed Form Solution for Pricing Variance Swaps Under the Rescaled Double Heston Model,January 2023,Youngin Yoon,Jeong-Hoon Kim,,Unknown,Unknown,Unknown,Unknown,,
61.0,1.0,Computational Economics,10 June 2022,https://link.springer.com/article/10.1007/s10614-022-10275-1,Classifying the Variety of Customers’ Online Engagement for Churn Prediction with a Mixed-Penalty Logistic Regression,January 2023,Petra P. Šimović,Claire Y. T. Chen,Edward W. Sun,Female,Female,Male,Mix,,
61.0,2.0,Computational Economics,09 November 2021,https://link.springer.com/article/10.1007/s10614-021-10215-5,Multiscale Multifractal Detrended Fluctuation Analysis and Trend Identification of Liquidity in the China's Stock Markets,February 2023,Ruzhen Yan,Ding Yue,Wei Gao,Unknown,,,Mix,,
61.0,2.0,Computational Economics,05 November 2021,https://link.springer.com/article/10.1007/s10614-021-10216-4,The Convergence Investigation of a Numerical Scheme for the Tempered Fractional Black-Scholes Model Arising European Double Barrier Option,February 2023,Y. Esmaeelzade Aghdam,H. Mesgarani,B. Farnam,Unknown,Unknown,Unknown,Unknown,,
61.0,2.0,Computational Economics,18 November 2021,https://link.springer.com/article/10.1007/s10614-021-10218-2,Recursive Computation of the Conditional Probability Function of the Quadratic Exponential Model for Binary Panel Data,February 2023,Francesco Bartolucci,Francesco Valentini,Claudia Pigini,Male,Male,Female,Mix,,
61.0,2.0,Computational Economics,18 November 2021,https://link.springer.com/article/10.1007/s10614-021-10219-1,Integrating Wavelet Decomposition and Fuzzy Transformation for Improving the Accuracy of Forecasting Crude Oil Price,February 2023,Faramarz Saghi,Mustafa Jahangoshai Rezaee,,Male,Male,Unknown,Male,"A prediction is a powerful tool in the process of planning anything. The prediction is divided into three categories including short-term, mid-term, and long-term. Of course, prediction never fits into reality and must be tried to reduce the forecast error to the minimum possible value. In a general split, prediction methods are divided into qualitative and quantitative methods. Qualitative methods of prediction are based on empirical knowledge and direct understanding and other related information and generally the opinions of the relevant experts are used in these methods. Some of the qualitative methods of prediction are the method of mental curve fit, the Delphi method, the opposite method, etc. Quantitative prediction approaches are based on information that is attainable from previous periods. These data are time series that should be reliable. Some of the quantitative methods of prediction are the moving average method, exponential smoothing method, artificial neural networks (ANN), etc. The time series model's primary target is for the prediction of the latest changes depended on the past price trend. Time series often include noise, which may be difficult to model and predict (Adhikari & Agrawal, 2014). Oscillations of the Crude oil prices are important affection for both fiscal experts and stock shares. Howsoever, this problem is one of the most recondite and hard for models owing to the oscillation of the crude oil value is sort of untidy, nonlinear, dynamic, and top elusive. Accordingly, the precision prospect of this type of time series is one of the most issues and across the most significant problems encountering energy area experts toward better determination in many directorial layers. ANN methods have characterized voluminous strength in modeling and predicting nonlinear and perplexing time series (Shabri & Samsudin, 2014). The contributions of this study are divided into two parts. First, the fuzzy transform is proposed to model the uncertainties in time series. In the second contribution, the wavelet decomposition and fuzzy transform are combined for improving the accuracy of forecasting and uncertainty modeling that this type of integration has not been provided in the previous studies. Integrating the mentioned proposed algorithms with various ANNs is used to forecast time series of natural gas prices. The wavelet transform has been used to reduce the noise of the time series. Also, the F-transform is applied for its potency of management uncertainty relevant to the fluctuation of data. Uncertainty is one of the important features in price time series. To model the uncertainty in the studied time series, the new heuristic fuzzy transform is used. Time-series are inherently uncertain which the fuzzy approach is used for importing uncertainty to the system. The rest of the paper is organized as follows: Sect. 2 presents a literature review of prediction in a different area. The used methodologies are proposed in Sect. 3. Section 4 presents the proposed approach. Section 5 provides the case study, the results, and the related analyses. Finally, the summary and conclusion are given in Sect. 6.",4
61.0,2.0,Computational Economics,25 November 2021,https://link.springer.com/article/10.1007/s10614-021-10221-7,A Method to Pre-compile Numerical Integrals When Solving Stochastic Dynamic Problems,February 2023,Karolos Arapakis,,,Unknown,Unknown,Unknown,Unknown,,
61.0,2.0,Computational Economics,25 January 2022,https://link.springer.com/article/10.1007/s10614-021-10222-6,Finite-State Markov Chains with Flexible Distributions,February 2023,Damba Lkhagvasuren,Erdenebat Bataa,,Unknown,Unknown,Unknown,Unknown,,
61.0,2.0,Computational Economics,26 January 2022,https://link.springer.com/article/10.1007/s10614-021-10223-5,"COVID 19 Pandemic, Socio-Economic Behaviour and Infection Characteristics: An Inter-Country Predictive Study Using Deep Learning",February 2023,Srinka Basu,Sugata Sen,,Unknown,Female,Unknown,Female,"Prediction about a disaster plays an important role to minimize the damage and economic losses. The importance of prediction increases infinitely when the disaster appears as unique and starts to spread at a fast rate. With the emergence of good quality high volume data, Artificial Intelligence (AI) has appeared as a potent tool of prediction. But in case of unprecedented disasters like COVID 19 minimizing the errors in multi-horizon prediction is one of the greatest challenges of AI based models. These challenges heighten further when the dynamic planning to mitigate the menace largely depends upon the static non-pharmaceutical interventions (NPI). With time this pandemic has exposed that in the absence of medical interventions non-pharmaceutical community measures can be the only policy initiatives which the societies can formulate to overcome the disaster. But the success of these NPI depends upon an array of socio-economic variables. Naturally the errors in predictions vary from counties to countries as the distribution of the influences of the stated variables varies between economies. So this work has tried to develop an AI based multi-horizon predictive model considering the simultaneous variability within the static explanatory variables as well as the longitudinal historical data of infection. Due to exceptional nature of the current disaster selecting the causal variables appeared as the additional challenge of this prediction. To that respect this effort has used an attention based deep neural network termed as Temporal Fusion Transformer (TFT) in selection of the relevance of the explanatory variables. For predictive activities TFT, unlike the existing statistical and machine learning models, combine the static variates, historical cumulative incidence count and derived temporal variates like days since first occurrence of infection to estimate the future temporal target cumulative incidence count. Apart from developing a predictive model this work has delivered country-wise long term future infection levels related to COVID 19 pandemic. The finding concludes that the inter-country infection related predictions vary widely over spatio-temporal variability and different socio-economic variables have different influences over this inter-country variability. Based on the similarity of the influencing explanatory variables the countries of North America, South America, Africa, Asia and Australia exhibit certain patterns. For example, it is observed that none of the North American countries are found in clusters where urban population, poverty, availability and accessibility to quality health care (expressed through Healthcare Availability and Quality Index or HAQI), human development status (expressed through Human Development Index or HDI) and sex ratio have taken most influential role to form the cluster. On the contrary, the South American and Asian countries are not found in the clusters where Trade Openness Index and HAQI have taken the most influential roles respectively while Australian countries are not found in the clusters where HAQI and HDI have played the most influential role to form the clusters. The first use of computations to establish the incidence of epidemics dates back to 1760 when Bernoulli used it to predict the life expectancy of French population (Marathe and Vullikanti 2013). But the actual improvement in the application of computation based techniques on the studies of epidemics started in the early years of 1900s. In 1911 Ronald Ross developed a model to study the spread of malaria with the help of a system of differential equations (Bacaër 2011a). Kermack and McKendrik furthered this model to a mass-action model (Bacaër 2011b). This development is regarded as the foundation of modern computational epidemiology. The process ultimately culminated in the emergence of Artificial Intelligence (AI) and Machine Learning (ML) as the foremost supports to the professionals and researchers related to development of drugs, improvement of diagnosis and health related policy formulation (Lalmuanawma et al. 2020). Naturally AI and ML based modeling has started to play a major role in predictive studies on COVID 19 pandemic. These types of modeling have also appeared as the main tool of social policy formulation on the background of COVID 19 pandemic. Ribeiro et. al has developed an AI based forecasting model on Brazilian data using Stacking-ensemble with support vector regression algorithm to predict short term cumulative incidence of infection (Ribeiro et al. 2020). Chakraborty and Ghosh have proposed a real time prediction model using wavelet-base forecasting and autoregressive integrated moving average time series analysis method. Using demographic data this model has tried to forecast the short term outcome of COVID 19 incidences (Chakraborty and Ghosh 2020). Zheng et. al. have used Improved Susceptible Infected (ISI) model instead of fixed infection rate to study the short term infection process of COVID 19. For developing a hybrid AI based model to incorporate effects of control measures they have embedded Natural Language Processing module and Long Short Term Memory (LSTM) network into the ISI model (Zheng et al. 2020). Yang et.al. have also used LSTM model to predict new infections. Here probability of transmission, incubation rate, probability of recovery or death and number of contacts have been used as explanatory variables (Yang et al. 2020). Li et al. (2020) have tried to develop a predictive model on the transmission process of corona infection using Gaussian distribution. This work has tried to infer the evolution trend of the COVID 19 infection history overtime to predict the future trend. Yan et al. (2020) have tried to develop effective bio-markers to estimate the risk of fatality using machine learning algorithms. To that respect they have used biological, demographic and clinical data of the sample respondents. The study used supervised XGBoost classifier to cluster the respondents with risk of death. Velásquez and Lara (2020) have tried to forecast infection, fatality and recovery rates with the help of Reduced-Space Gaussian Process Regression associated to Chaotic dynamic system using history data. Bertozzi et al. (2020) have tried to estimate the course of infection and the influence of stringency measures on the growth of infection with the help of three interconnected macroscopic models and time series data. Judging the usefulness of different models in different stages this study has interchangeably used exponential growth model, the self-exciting branching process and the susceptible-infected-resistant compartment model. They found that reproduction rate is variable over time and location. Incidentally it is found that most of the AI based predictive models on COVID 19 have failed to incorporate major socio-economic variables or in other words it can be said that predictive models purely based on socio-economic factors are almost absent. It is to be kept in mind that UNDP has mentioned that the COVID 19 pandemic is not a global health emergency but a human development crisis. They have concluded that higher the state of development, higher the level of preparedness to face the pandemic (Kovacevic and Jahic 2020). The above stated preparedness of the countries has differently been studied through an index called HAQI (Health care Access and Quality Index) (Lai et al. 2020). It is not only the human capital but also the social capital which plays an important role to mitigate the spread of infectionFootnote 1, (Chuang et al. 2015; Koh and Cadigan 2008). Interestingly, apart from the development divergence it is the variation in gender that has played a crucial role in the spread of current pandemic (Conti and Younes 2020). Some other demographic factors which have left a distinct mark in the rate of infection are population density and urbanization (Desai 2020). Recent data from World Health Organization have shown that the initial infection rates and trajectories of infection varied widely for the whole of the world Footnote 2. In the absence of pharmaceutical interventions it is undoubtedly the effectiveness of the socio-economic variables - which we discussed above, have created some nations more competent and some not. So at the end of this section it can be said that as the success of control measures depends purely on the socio-economic macro variables, AI based models with socio-economic variables as the explanatory variables are the need of the time. These models will be able to deduce the influence of individual socio- economic variables on the spread of infection at national levels. On the other hand to formulate acceptable social policies AI and ML based models with ability to deliver long-term predictions are also needed. Thus this work wants to substantiate on the following. To develop a deep neural network based multi-horizon predictive model for the spread of COVID 19 infection on the basis of static socio-economic macro variables as well as historical incidence record. To predict about 270 days ahead inter-country incidence of COVID-19 using the developed model. To locate the influence of individual socio-economic factors on the estimated country-wise predictions. To cluster the countries on the basis of influential explanatory variables and thus to search for intra-cluster and inter-cluster characteristics.",4
61.0,2.0,Computational Economics,01 December 2021,https://link.springer.com/article/10.1007/s10614-021-10224-4,Price Change and Trading Volume: Behavioral Heterogeneity in Stock Market,February 2023,Changtai Li,Weihong Huang,Wai-Mun Chia,Unknown,Unknown,Unknown,Unknown,,
61.0,2.0,Computational Economics,23 January 2022,https://link.springer.com/article/10.1007/s10614-021-10227-1,Bankruptcy Prediction using the XGBoost Algorithm and Variable Importance Feature Engineering,February 2023,Sami Ben Jabeur,Nicolae Stef,Pedro Carmona,Male,Male,Male,Male,"Assessing the risk of business failure has long been a major concern for researchers, bankruptcy practitioners, accountants and banks. Financial failure affects company survival and imposes high costs on bankers and other creditors, who may only partially recover their investments (Wruck, 1990). In general, failure refers to situations that lead to firm’s bankruptcy following payment default. Everett and Watson (1998) showed that the concept of failure is associated with three types of risk: risk related to the national economy, risk related to the firm’s industry, and risk that is unique to the business itself. Moreover, the process of failure may vary. As noted by Laitinen et al. (2014), firms that fail may decline quickly, gradually collapse, or operate poorly for a long period. When business failure is imminent, debtors (firms) can find shelter in the bankruptcy system. For instance, reorganization can encourage firm survival despite financial failure (Blazy & Stef, 2020; Stef, 2018, 2021). Unsurprisingly, corporate failure prediction is a serious problem in risk management, and it is treated carefully by banks and other financial institutions. Consequently, financial organizations must develop effective warning systems to predict bankruptcy (Liang et al., 2015). The emergence of big data, information technology, and social media provides an enormous amount of information about firms’ current financial health. When faced with this abundance of data, decision makers must identify the crucial information to build an effective and operative prediction model without sacrificing the quality of the estimated output. Achieving such a goal is enabled by feature selection, which is an important data preprocessing stage in machine learning models (Liang et al., 2015). In this paper, an improved Extreme Gradient Boosting (XGBoost) algorithm based on feature importance selection (FS-XGBoost) is proposed to predict corporate failure. The primary contributions of this paper are twofold. First, we combine a feature importance strategy with the XGBoost algorithm. Our estimates suggest that FS-XGBoost can provide enhanced accuracy and suitability in financial distress prediction. Second, we compare FS-XGBoost with seven machine learning models. We thus identify which classification methods are most sensitive to the feature selection technique. Therefore, our study offers banks and financial institutions guidelines in their search for suitable bankruptcy prediction models. This paper is structured as follows. Section 2 presents the literature review. Section 3 describes the methods used in our study. Section 4 presents our sample and the variables used in the empirical analysis. Section 5 summarizes and discusses the experimental results. Section 6 concludes.",18
61.0,2.0,Computational Economics,06 January 2022,https://link.springer.com/article/10.1007/s10614-021-10228-0,Use of Econometric Predictors and Artificial Neural Networks for the Construction of Stock Market Investment Bots,February 2023,Ciniro A. L. Nametala,Jonas Villela de Souza,Eduardo Gontijo Carrano,Unknown,Male,Male,Male,"One major challenge in stock markets is to forecast a given asset behavior along time. Asset prices are influenced by social, economic, and political factors (Kara et al., 2011), which increase prediction complexity. In fact, several theories, such as the Efficient Market Hypothesis (EMH) (Fama, 1969), suggest that any price forecasting technique is useless. These studies are based on the assumption that price variations have random behaviors, regardless the market in which the asset is being traded (Fama, 1969; Cootner, 1964; Eugene, 1972). On the opposite direction, some surveys provide evidences contesting market efficiency, stating that investors do not always act rationally (Shehata et al., 2021; Salman et al., 2018; Baker et al., 2019; Listyarti & Suryani, 2014; Tversky & Kahneman, 1981). Such works state that EMH would only prevail under certain conditions (Elder, 2002; Peters, 1991). Osborne (1962) is one of the first to analyze these market conditions, contributing to the dissemination of statistical tools to forecast financial time series. His work assumes that prices follow normal distributions with stable mean and finite variance. The normality assumption is also used as basis for several works, some of them considered as remarkable contributions to economy theory, such as: (i) Markowitz (1952), in which Markowitz proposed an investment risk mitigation model based on the diversification of portfolio assets; (ii) Sharpe (1964), in which Sharpe presented the Capital Asset Pricing Model (CAPM), a method that allows to analyze investments through the projection of expected profits; (iii) Black & Scholes (1973), in which Black and Scholes developed an option pricing model based on the premise that prices follow the trends included in Brownian movements and have constant volatility, and; (iv) Roll & Ross (1980), in which Ross proposed the Arbitration Pricing Theory (APT), which is a generalization of CAPM that models price impacts based on unexpected market events. However, the normality assumption is contested by other works, which state that it is not uncommon to identify asymmetries in the distribution of asset returns (Mandelbrot & Richard, 2010; Paulos, 2004; Turner & Weigel, 1992; Grossman & Stiglitz, 1980). In (Mandelbrot & Richard, 2010), the Dow Jones index variation from 1916 to 2003 was investigated, and the adherence to a normal distribution was not verified. In Paulos (2004), the authors claimed that price variations are better modeled by probability distributions with “fat tails” and high kurtosis, which means they are not well modeled by Gaussian distributions. In Turner & Weigel (1992), the authors analyzed the S&P 500 index between 1928 and 1989 and identified that the distribution of returns is not normal. Grossman & Stiglitz (1980) discussed that information is not equally available in time and integrity to investors, which generates asymmetries. In Kahneman (2003), Odean et al. (2007), a behavior pattern called the Disposition Effect is discussed. The authors argue that investors tend to keep assets that have depreciated with the hope they will return to better prices. References (Karsten, 2005; Lucchesi et al., 2015; Fischbacher et al., 2017; Weber & Camerer, 1998) emphasize the importance of the Disposition Effect and other behavioral patterns. Along time, the authors identified that financial time series have complex characteristics such as trends, seasonality, points of influence (atypical), conditional heteroscedasticity, and nonlinearity (Morettin & Toloi, 2006; Bueno, 2011). It motivated the study of methods that would be capable of dealing with these characteristics (Engle, 1982; Bollerslev, 1986). This field of study, which encompasses several other research areas such as economics, computing, engineering, and statistics is known as quantitative finance (James, 2017). Works in quantitative finance have increased significantly in recent decades (Lin et al., 2009; Bustos & Pomares-Quimbaya, 2020; Kumar et al., 2020). Among these approaches, hybrid methods have received considerable attention. For instance, (Pimenta et al., 2017) uses a Genetic Programming (GP) evolutionary algorithm combined with technical analysis (TA) to generate rules for buying and selling assets in the Brazilian stock market. Reference (Lam, 2004) integrates fundamental analysis (FA) and TA with Artificial Neural Networks (ANN) to perform price prediction in 364 S&P companies between 1984 and 1995. In Nayak et al. (2015), the authors proposed the hybridization of Support Vector Machines (SVM) and K-Nearest Neighbors (KNN) for predicting indices in Indian stock market. Ebadati & Mortazavi (2018) applied a hybrid method that combines a Genetic Algorithm (GA) and ANN to predict the price of the main index and five companies in the NASDAQ stock market. In Buyuksahin & Ertekin (2019), the authors provided a mixed approach involving Autoregressive Integrated Moving Average (ARIMA) and ANN to forecast the British pound/U$ dollar exchange rate. Bukhar et al. (Bukhari et al., 2020) used a Fractional ARIMA (ARFIMA) with a Long-Short Term Memory (LSTM) ANN to forecast the price of an Asian chemical company. In Ftiti et al. (2020), Ftiti & Jawadi (2019), Ftiti & Jawadi (2019), Hadhri & Ftiti (2019), Assessment of gold-oil markets (2016), advanced hybrid approaches were proposed to predict the dynamics of different financial time series in the American and European markets. Supported by such recent advances, we propose in this manuscript a hybrid method that combines ANN, econometrics, and decision committees to identify profitable opportunities in the Brazilian stock market. This strategy is endowed with some important features: Automatic choice and parameterization of several econometric methods and neural networks. Use of price values and price trends to identify profitable trading strategies. Automatic definition of stop prices (stop loss and take profit) for each asset, based on past return distributions. Adoption of multiple types of decision committees. Dynamic adjust of the sliding window size. Introduction of the concept of “confidence moments”, which estimates reliable predictors for the current time window. Proposal of a approach to model trends, namely Trend ANN. This manuscript is organized as follows: Sect. 1 introduces the subject of study; Sect. 2 presents a brief survey regarding quantitative finance in the Brazilian market; Sect. 3 describes the approach proposed; Sect. 4 reports the results and discussions; finally, Sect. 5 presents some conclusions.",1
61.0,2.0,Computational Economics,16 January 2022,https://link.springer.com/article/10.1007/s10614-021-10231-5,CO2 Emission Allowances Risk Prediction with GAS and GARCH Models,February 2023,Nader Trabelsi,Aviral Kumar Tiwari,,Male,Unknown,Unknown,Male,"The European Union (EU) Emissions Trading System (ETS) is the world’s largest organized financial market for CO2 Emission Allowances. In ETS, polluters can trade enough emission rights with another one who may have a surplus of them. Much like any market, the EUA price, at any given point in time, will result in supply and demand conditions. As this market works on the ‘cap and trade’ programs, the supply or the cap is pre-determined for 31 countries, but the allowance demand from energy-intensive installations across the EU (i.e., combustion plants, coke ovens, oil refineries, iron and steel plants, and factories making cement, brick, ceramics, glass, lime, pulp and paper) is rather flexible and can hardly be predicted. Thus, although the carbon market has been growing extremely fast, there is a consensus in the literature that there is an ambiguous picture of the EUA price formation.Footnote 1 Precisely, the EUA price determination and their “forecastability” are still open questions that continue to attract increasing interest from researchers. Indeed, a better understanding of the price formation of EUAs will help companies, among other things, to assess their business future risk, and policymakers to evaluate the performance of each national allocation plan to achieve compliance with their commitments under the Kyoto Protocol. In this sense, Benz and Trück (2006) describe the EUAs as a new class of assets with distinct characteristics, which demand efficient techniques to produce accuracy price forecasts. According to Mansanet-Bataller et al. (2007), Alberola et al., (2008a, 2008b), Chevallier (2009), Hintermann (2010), Hitzemann and Uhrig-Homburg (2013), among others, the long-term EUAs spot prices depend on several determinants such as the price of traditional energy products, weather conditions, and economic fluctuations. For Nazifi and Milunivich (2010), there are only significant short-run linkages between the price of EUAs and traditional energy prices (i.e., coal, oil, natural gas, and electricity). On the other hand, Wang et al. (2018) find significant time-varying spillover effects between returns and volatility of energy and EUA future markets. Moreover, Carnero et al. (2018) show that EUAs price changes can be explained by the dynamics of fuel prices.Footnote 2 While there are several studies performing multiple regression analysis to find how the EUAs prices are influenced by various independent variables, there are few other studies that go beyond the financial econometric analysis (Daskalakis et al., 2009; Benz & Trück, 2009; Hitzemann and Uhrig-Homburg, 2013; Gil-Alana, 2016; Müller et al., 1997). Their results also demonstrate the capability of models that capture the dynamic features of the financial series, such as volatility clustering, skewness and excess kurtosis. However, Paolella and Taschini (2008) believe we cannot predict carbon emission permits under EU ETS accurately. This paper adds to this literature by providing a statistical procedure that delivers the “best” model to forecast EUA’s price change. This issue is of paramount importance because a ""poor"" model would significantly affect the risk management strategies of a range of industrial companies, being direct CO2 emitters or consumers, as well as players in financial market including hedgers and portfolio managers. In fact, the participants in the EUA market have increased in number, and previous studies provide evidence for its utility for hedging portfolio risk (Zhang & Sun, 2016, among others). Clearly, this study offers support for the following idea: GAS model of Creal et al. (2013) and Harvey (2013) is the most appropriate model to use when one has to evaluate the volatility of EUA returns. The appropriateness of this model can be seen through its mathematical flexibility in solving the complicated dynamics characteristics of times series (i.e. asymmetry, long memory, etc.). Particularly, this requires to computing a score among different scales rather than solving a number of moment equations. The second contribution of this study is to evaluate the impact of different error distributional approximations on the accuracy of the predictive and forecasting results. Precisely, we assume that innovations are normal (norm), std (t-student), sstd (skewed t-student), Asymmetric t-Student with two decay parameters (ast) or Asymmetric t-Student with a left-tail decay parameter (ast1). To the best of the authors’ knowledge, this will be the first study that checks the appropriateness of GAS model and these distribution assumptions to predict EUA's price changes. The third contribution of this study is to focus on the comparison of GAS model with multiple General Autoregressive Conditional Heteroskedasticity (GARCH) models. For GARCH models, innovations are assumed to follow: Normal (Norm), t-Student (St), Skewed-t-Student (Sts), Generalized Error Distribution (GED) or Johnson’s Reparametrized SU (JSU) distributions. Such models are useful as they demonstrate their ability to capture several stylised facts of the financial time series. Moreover, as they have been applied successfully in several areas (i.e., default and credit risk modelling, stock volatility, and correlation modelling, modelling time-varying dependence structures, CDS spread modelling and questions relating to financial stability and systemic risk, modelling high-frequency data, etc.), It's worth noting that by scaling the score function appropriately, standard observation-driven models such as the GARCH models can be recovered. For that, only non-Gaussian-GARCH models are discussed in our analysis. Comparison between general GARCH models and GAS model is made so that researchers and policy makers are in better position to use evaluate the difference when traditional models are used as opposed to the advanced models. In addition to that to choose the best model we used three back-testing procedures namely Unconditional Coverage, Conditional Coverage, and Dynamic Quantile tests. In order to choose the best model among GARCH and GAS models, we implement several model selection criteria. More precisely, the best-fitted model with maximum likelihood (ML) will be validated by the minimum AIC (Akaike Information Criteria) and BIC (Bayesian Information Criteria) criterion. Our research is also centred on the prediction of the EUA extreme volatility using the Value-at-Risk (VaR) measure. To this end, we use the MCS approach to perform a comparison of a number of VaR forecasts. The MCS approach consists of a sequence of tests that permits to construct of a set of ""superior"" models. In other terms, GAS and GARCH models are compared in terms of their predictive ability, so that models that produce better forecasts of EUA’s VaR will have the highest ranks. Specifically, we assess whether GAS model leads to more accurate volatility forecasts of EUA return than any other alternative models. Contrary to other researchers, we do not use distinct phases (i.e., phase I, II and III). Note that the CO2 allowance trading periods of the EU ETS are usually referred to as “phases”. Our empirical results will be, however, identified from a vast amount of data (i.e., from 2005 to nowadays), which might make this study rather unique in the environmental literature. Results show the GAS model under skewed t-distribution has better volatility forecasts of EUA price changes compared to any other GARCH models. This model can detect common facts about conditional volatility, such as fat-tails, excess kurtosis, persistence of volatility, asymmetry and leverage effects. We find also the superiority of some GARCH types to find VaR predictions. Thus, we conclude that it is feasible to discriminate between the estimation methods based on an analysis of the VaR forecast accuracy. The remainder of the paper is organized as follows. Section 2 gives an overview of the regulatory design of the EU ETS. Section 3 presents a brief literature review on the formation of EUA prices and their determinants. Section 4 describes the data and the econometric models used in our analysis. Section 5 presents the main results, while Sect. 6 concludes.",4
61.0,2.0,Computational Economics,17 January 2022,https://link.springer.com/article/10.1007/s10614-021-10232-4,Valuation of Spark-Spread Option Written on Electricity and Gas Forward Contracts Under Two-Factor Models with Non-Gaussian Lévy Processes,February 2023,Farshid Mehrdoust,Idin Noorani,,Male,Unknown,Unknown,Male,"One of the most prominent features of the commodity market is the large number of long-term relationships. For instance, unlike the price of heating oil, the price of crude oil may move on a certain day, but not in the long-term. In other words, in the long-term the crude oil price is tied to the heating oil price in an equilibrium relationship. There is an extensive literature on modeling the price of a commodity as a non-stationary process. Schwartz and Smith (2000) proposed the sum of two factors for the logarithmic price, which the long-term level and the short-term deviation of equilibrium are modeled by a geometric Brownian motion and Ornstein-Uhlenbeck process with a zero mean, respectively. Farkas et al. (2017) provided a simple, usable, yet comprehensive model for a cointegrated commodity pricing system, so that by maintaining the horizontal structure of the views of previous approaches, their model allows for an arbitrary number of cointegration relationships. They show that the cointegration component allows capturing well known features of commodity prices. Benth et al. (2007) extended the proposed model by Schwartz and Smith (2000) in a multi-factor framework for the electricity spot price. A two-factor model based on two diffusion factors for the price of electricity forward contracts was presented by Kiesel et al. (2009). They calibrated the proposed model by option written on forward price and showed that their model fits the option implied volatility with long delivery period. The existing literature on electricity price modeling can be roughly divided into three categories. At one end of the spectrum are so-called full-cost production models. These rely on the knowledge of all production units, relevant operational constraints, and network transmission constraints. Pricing problems are then usually solved by complex optimization procedures (see Eydeland and Wolyniec, 2003). Although this type of model may provide insights and market forecasts in the short term, it is unsuitable for derivative pricing or valuation of physical assets due to its complexity for managing uncertainty. Other related approaches which have this weakness include strategic bidding models (see Hortacsu and Puller, 2008) and other equilibrium approaches (see Bessembinder and Lemmon 2000). At the other end of the spectrum are the reduced form models. These features are characterized by the external specifications of the electricity price, which is the forward curve (see Benth and Koekebakker, 2008 and Clewlow and Strickland 2000) or spot price (see Benth et al., 2008; Cartea and Figueroa, 2005; Deng, 2003, and Frikha and Lemaire, 2013) representing the starting point for the model. Reduced form models do not succeed in capturing the structure of the above-mentioned important dependence between fuel and electricity. Since, these models typically ignore fuel prices (i.g. gas) or refer to them as exogenously correlated processes. In addition, spikes are usually obtained only by inserting jump processes (see Benth and Schmeck, 2014; Hambly et al., 2009 and Benth et al., 2007) or regime switches (see Mehrdoust and Noorani, 2021; Janczura and Weron, 2012 and Huisman and Weron, 2003), so reduced form models offer little insight of these sudden price changes. Since the spot price in energy, especially, electricity is not trade-able asset, any probability measure being equivalent to the objective (or market) probability is the risk-neutral, and the underlying asset process does not need to be a martingale with respect to the risk-neutral measure, (see Benth and Sgarra 2012). According to the definition of forward price F(t, T) at time t with maturity date T, the electricity forward contracts can be evaluated as, \(F(t,T)={\mathbb {E}}^{\mathbb {Q}}[S_T|{\mathcal {F}}_t],\) where, S and \({\mathcal {F}}\) are the spot price and generated filtration by market information, respectively. Thus under any equivalent probability \({\mathbb {Q}}\), including the equivalent martingale measure for spot price (obviously, there are some integrability issues as well ignored here of the sake of argument), the energy forward price is a martingale. The definition of the forward price is not to choose the objective measure in the above conditional expectation. But doing so, we have a martingale, and our argument that the forward is tradeable and hence has to be a martingale is validated. In energy markets such as electricity and gas, options traded on exchanges are typically written on forward contracts for the delivery of underlying energy over a specified period. For example, in the Nord Pool energy market, calls and put options are traded for months under a forward contract with electricity delivery. Another important class of derivatives in the electricity market is the so-called spark-spread option. Such derivatives are written on different future prices for electricity and gas and it allows factory owners to hedge the undesirable movements in the gas and electricity market (see Carmona et al., 2013; Benth and Kettler, 2011; Benth and Saltyte-Benth, 2011; Benth et al., 2015 and René et al., 2013). Here, we price the saprk-spread option, such that the short-term dynamics modeled by an Ornstein-Uhlenbeck process driven by a Lévy process. The short-term factor of forward price evolution derived from the spot price may be negligible in the option price. Depending on the delivery period, short-term shocks in the spot price may disappear in forward dynamics thanks to the smoothing of the delivery period. This means that there must be a non-stationary part, which leads to the claim that the option price can be approximated well by a Black-Scholes formula for two-asset (2D Black-Scholes), since we deal with the spark-spread option written on electricity and gas. As Benth and Schmeck (2014) showed this issue for European option written on electricity forward contracts. In this paper, we prove a uniform exponential convergence of the spark-spread option price towards the one given by 2D Black-Scholes formula. This convergence is expressed based on the speed of mean reversion of the short-term stationary factor and the time left to delivery of the underlying forward. A common feature of gas and electricity markets is a sudden sharp price deviation, often referred to as a spike. For instance, the German energy market EEX shows a significant increase in negative prices, mainly due to wind energy production. More common positive spikes, which for example can be seen in the Nord pool market during winter season (see Mehrdoust and Noorani, 2021). It is also observed in gas markets that large price fluctuations occur due to cold weather (see Geman, 2005 for example). These large price fluctuations require models based on non-Gaussian stochastic process and Lévy processes, possibly time-inhomogeneous (see Benth et al. 2008). In this paper we evaluate the short-term of electricity and gas dynamics by an Ornstein-Uhlenbeck process driven by a Lévy process. In this way, spike or large-variations dynamics in the spot price can be included. In general, in the electricity dynamics, mean reversion and jump factors are significantly important, so that with increasing maturity time, mean reversion can be reduced the volatility smile, and jump size volatility can affect on the smile curve (see Nomikos and Soldatos, 2010). As we known, under the pure-jump Lévy processes, the market model is incomplete and the spark-spread option cannot be hedged perfectly. In general, when the forward price dynamics are modelled by jump processes, the hedging problem is an issue that needs to be addressed. In incomplete markets, there are many ways to hedge the option, where one soughts to find an underlying strategy that minimizes the risk of short-term options (see Cont and Tankov, 2004). In this study, we focus on the quadratic hedging strategy, which minimizes the \(L^2\)-distance between the payout from the spark-spread option and the hedging portfolio. We refer to Cont and Tankov (2004) and Benth and Schmeck (2014) for more information on this strategy in incomplete markets where the underlying spot price is defined as the exponential Lévy process. We can determine the quadratic hedging strategy of proposed market model and express this in terms of the price of the spark-spread option and its sensitivity to the infrastructure. As it turns out, we show that with increasing delivery time, the quadratic hedge decreases and the energy markets described by the two-factor models tend to the complete markets. The rest of this paper is organized as follows. In Sect. 2, we present data testing which includes normal probability test for spot returns, normal probability test for the filtered returns, autocorrelation test for returns and autocorrelation test for the deseasonality returns of the electricity Nord Pool market and Henry Hub gas market. Section 3 proposes a two-factor model with non-Gaussian Lévy noise in order to model the electricity and gas markets. Then we derive the dynamics of forward contracts and investigate the existence and uniqueness of the stochastic differential equation of forward contracts. In Sect. 4, we derive the spark-spread option price and prove that it is converges to the simple formula of 2D Black-Scholes. In Sect. 5, in order to hedge of spark-spread option we apply the quadratic hedging strategy and examine its convergence. In Sect. 6, the Lévy processes are considered as normal inverse Gaussian processes and independent compound Poisson processes. The model parameters are then estimated and all convergences are evaluated numerically. The conclusion of this paper is reported in Sect. 7.",3
61.0,2.0,Computational Economics,19 February 2022,https://link.springer.com/article/10.1007/s10614-022-10233-x,The Impact of Large Investors on the Portfolio Optimization of Single-Family Houses in Housing Markets,February 2023,Bilgi Yilmaz,Ralf Korn,A. Sevtap Selcuk-Kestel,Female,Male,Unknown,Mix,,
61.0,2.0,Computational Economics,19 February 2022,https://link.springer.com/article/10.1007/s10614-022-10234-w,Quantitative Macroeconomics: Lessons Learned from Fourteen Replications,February 2023,Robert Kirkby,,,Male,Unknown,Unknown,Male,,1
61.0,2.0,Computational Economics,20 March 2022,https://link.springer.com/article/10.1007/s10614-022-10236-8,"Correction to: Generalized, Partial and Canonical Correlation Coefficients",February 2023,H. D. Vinod,,,Unknown,Unknown,Unknown,Unknown,,
61.0,2.0,Computational Economics,24 November 2021,https://link.springer.com/article/10.1007/s10614-021-10217-3,Correction to: \(\ell_{1}\) Common Trend Filtering,February 2023,Hiroshi Yamada,Ruoyi Bao,,Male,Unknown,Unknown,Male,"The article “\(\ell_{1}\) Common Trend Filtering”, written by Hiroshi Yamada and Ruoyi Bao, was originally published electronically on the publisher’s internet portal on April 11, 2021 without open access. With the authors’ decision to opt for Open Choice the copyright of the article changed on October 18, 2021 to ©The Author(s) 2021 and the article is forthwith distributed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0. The original article has been corrected.",
61.0,3.0,Computational Economics,20 April 2023,https://link.springer.com/article/10.1007/s10614-023-10382-7,Editorial to the Special Issue on Game Theory,March 2023,Marta Biancardi,Giovanni Villani,,Female,Male,Unknown,Mix,,
61.0,3.0,Computational Economics,23 July 2019,https://link.springer.com/article/10.1007/s10614-019-09909-8,Optimal Abatement Technology Licensing in a Dynamic Transboundary Pollution Game: Fixed Fee Versus Royalty,March 2023,Hao Xu,Deqing Tan,,,Unknown,Unknown,Mix,,
61.0,3.0,Computational Economics,02 July 2020,https://link.springer.com/article/10.1007/s10614-020-10003-7,Non-Cooperative Bargaining with Unsophisticated Agents,March 2023,Kristal K. Trejo,Ruben Juarez,Alexander S. Poznyak,Female,Male,Male,Mix,,
61.0,3.0,Computational Economics,15 May 2021,https://link.springer.com/article/10.1007/s10614-021-10128-3,"Non-cooperative Mode, Cost-Sharing Mode, or Cooperative Mode: Which is the Optimal Mode for Desertification Control?",March 2023,Jiayi Sun,Deqing Tan,,Unknown,Unknown,Unknown,Unknown,,
61.0,3.0,Computational Economics,03 October 2022,https://link.springer.com/article/10.1007/s10614-022-10321-y,Computing Profit-Maximizing Bid Shading Factors in First-Price Sealed-Bid Auctions,March 2023,Paulo Fagandini,Ingemar Dierickx,,Male,Male,Unknown,Male,"In this paper we address the problem that bidders face in first price sealed bid (FPSB) auctions for common value goods: How much to shade their signals about the value of the object being sold. Countless firms are grappling with this problem every day. Two different disciplines, game theory and decision sciences, have taken totally different approaches to this problem. Excellent reviews of the different approaches to auctions can be found in Rothkopf (2007) and Lorentziadis (2016). We build upon the pioneering work of Rothkopf (1969) and Wilson (1984), who use non-Bayes Nash equilibrium models. Like them, we work on the classical mineral rights model, in which an indivisible good is auctioned in a first price sealed bid fashion among competitive bidders, who produce an unbiased estimate about its value, unknown to all of them at the moment of the auction. Like (Hubbard et al., 2013; Hubbard & Paarsch, 2014), we propose a computational method to address a broad range of realistic valuation and information scenarios. However, our paper differs in several aspects: (1) we focus on a common value component instead of a private valuations, (2) we consider an unbounded support for the signals, (3) we allow for naive bidders, and (4) we use a different methodology that enables firms to submit bids that maximizes ex-ante expected profits. Specifically, we derive first order conditions and compute a constant shading factor (SF) ex-ante of receiving the signal that (i) allows for a common value component as well as a firm-specific component in valuations, (ii) allows for differences in the accuracy of bidder signals, and (iii) allows for the introduction of non-rational bidders.Footnote 1 In real life, constant shading rules such as constant absolute markup and constant percentage markup are commonly used. Shachat and Wei (2012) find that bids and prices in laboratory experiments agree with game-theoretic predictions in English auctions, but not in First Price Sealed Bid auctions, where constant shading strategies are used more often. Rothkopf (1980), and Compte and Postlewaite (2012) provide discussions on why constant strategies, and shading before observing the signal, should approximate Bayesian equilibrium strategies when the prior is diffuse.Footnote 2 To test whether the SF results in Bayes Nash equilibrium bids, we use as benchmark the analytical solution to the symmetric problem with normally distributed noise (Hoernig & Fagandini, 2018, Levin & Smith, 1991),Footnote 3 and find that the SF exactly replicates those results when the prior is diffuse. In addition, to verify the SF in a broader range of scenarios, we compared it with results from “brute force” Monte Carlo simulations. In all cases, we found that results from the SF and “brute force” Monte Carlo simulations coincide. We also generalize Robert Wilson’s bias factor (BF) to obtain a measure of the Winner’s Curse. In short, the BF shades the bidder’s signal by the expected error of the signal conditional on winning. Therefore, this correction allows the bidder to obtain zero expected winning profits, avoiding the Winner’s Curse. To obtain positive expected winning profits, shading must exceed the BF by some margin. This margin depends on the number of rivals that a bidder is facing. The more bidders in an auction, the more aggressive a bidder must be in order to have a chance at winning. However, the more bidders in an auction, the more severe the Winner’s Curse, and therefore the bids should be more conservative. Optimal shading should take into account both the Winners’ Curse and the competitive effect (Thaler, 1988, p.192). To disentangle these two opposite effects we consider the BF as that part of the SF that takes care of avoiding the Winner’s Curse, while the remainder \((SF-BF)\) accounts for the competitive effect.Footnote 4 Finally, we allow for a subset of naive bidders, who follow a simple rule of thumb and shade their signals by an arbitrary fixed amount. Dyer et al. (1989) posit that even experienced bidders follow simple strategies when facing an auction that would work only in invariant environments. While our model allows us to set any fixed shading for these naive bidders we assume they are naive only in a limited sense, viz. that they do not account for the Winner’s Curse. Specifically, we assume that naive bidders shade their bids by \((SF-BF)\). That is, they do not account for the Winner’s Curse, but do properly account for the competitive effect and analyze their impact on the optimal bids of sophisticated bidders.Footnote 5 The presence of naive bidders in real life bidding problems cannot be denied. The notion of the Winner’s Curse was first discussed by three Atlantic Richfield engineers in a study of field data in the oil industry (Capen et al., 1971). The Winner’s Curse cannot occur when all bidders act rationally (Cox & Isaac, 1984). However, as Thaler (1988) stresses, bidding in a common value auction can be very challenging. Occurrence of the Winner’s Curse in common value auctions has been acknowledged for more than a half a century (Kagel & Levin, 2002), providing strong evidence for the presence of naive bidders. Dyer et al. (1989) document that in laboratory experiments even experienced executives in the construction industry, who are successful in their jobs, suffer from the Winner’s Curse. They suggest that industry specific learning and situation-specific rules of thumb, which could not be applied in a laboratory setting, may help them avoid overbidding in the field. Furthermore, experienced contractors do suffer unanticipated losses when bidding on a type of project they are not familiar with.Footnote 6 These findings indicate that in real life auctions it is not unlikely that a subset of bidders may be naive. We find that failing to account for the presence of naive bidders results in underbidding only in one case, when facing a single rival who is naive, and in overbidding in all other cases. Losses due to overbidding are particularly severe when the population of naive competitors is large. The paper is organized as follows: Starting with Sect. 2 we present a brief literature review for the main works related to this article. In Sect. 3 we present the model and introduce the shading factor. In Sect. 4, we present the bias factor. In Sect. 5 we show how shading factors react to specific asymmetries in the bidding population. Sect. 6 summarizes key conclusions and implications of our work and suggests avenues for future research.",1
61.0,3.0,Computational Economics,06 January 2023,https://link.springer.com/article/10.1007/s10614-022-10343-6,Correction: Computing Profit-Maximizing Bid Shading Factors in First-Price Sealed-Bid Auctions,March 2023,Paulo Fagandini,Ingemar Dierickx,,Male,Male,Unknown,Male,,
61.0,3.0,Computational Economics,16 March 2023,https://link.springer.com/article/10.1007/s10614-023-10372-9,Collaborative Innovation Strategy of Supply Chain in the Context of MCU Domestic Substitution : A Differential Game Analysis,March 2023,Yaxin Wang,Haoyu Wen,Yuntao Zhang,Unknown,Unknown,Unknown,Unknown,,
61.0,3.0,Computational Economics,22 October 2019,https://link.springer.com/article/10.1007/s10614-019-09937-4,An Evolutionary Game to Study Banks–Firms Relationship: Monitoring Intensity and Private Benefit,March 2023,Giovanni Villani,Marta Biancardi,,Male,Female,Unknown,Mix,,
61.0,3.0,Computational Economics,11 February 2022,https://link.springer.com/article/10.1007/s10614-022-10237-7,Extracting Rules via Markov Chains for Cryptocurrencies Returns Forecasting,March 2023,Kerolly Kedma Felix do Nascimento,Fábio Sandro dos Santos,Tiago A. E. Ferreira,Unknown,Unknown,Male,Male,"Recently, the cryptocurrency market has become popular among investors due the technology known as blockchain allows the distribution of data and records among market participants based on cryptographic protection (Stosic et al. 2019). There are currently more than 2000 cryptocurrencies on the market (CoinMarketCap 2020a; Cheng et al. 2019), the most popular being Bitcoin (BTC), first introduced in 2008 and with the largest market capitalization (Song et al. 2019). In the ranking of the top 100 cryptocurrencies, the four more important are Bitcoin (often in the first position), Ethereum (ETH), Litecoin (LTC), and Ripple (XRP). These four cryptocurrencies are considered the largest in the digital market. They account for about \(80\%\) of all cryptocurrency market capitalization and are often the focus of investigation in academic studies (CoinMarketCap 2020b). In this perspective, several researchers have endeavored to understand the dynamics of cryptocurrency time series better. Different approaches have been adopted with the aid of computational tools. For example, the multifractality techniques were employed to analyze the Bitcoin market (Takaishi 2018; Mensi et al. 2019; El Alaoui et al. 2019). It is also possible to find studies with the modeling of volatilities of some cryptocurrencies (Fakhfekh and Jeribi 2020; Yi et al. 2018; Phillip et al. 2019), and the use of a hybrid structure for forecasting volatility (Almeida et al. 2015; Bouri et al. 2019; Peng et al. 2018). Caporale et al. (2018) investigated the persistence in the time series of Bitcoin, Litecoin, Ripple, and Dash cryptocurrencies. The study results revealed a positive correlation between current and past values, with changes over time. Also, one can see the inefficiency of the studied markets, ensuring a certain degree of predictability into the series. Catania and Grassi (2017) analyzed more than 600 cryptocurrencies. They emphasized the main four (BTC, ETH, LTC, and XRP) and produced a dynamic model with the ability to use asymmetries and long memory in volatility; its forecast results were promising with the use of a robust filter where there is a variable asymmetry over time. Bouri et al. (2021b) analyzed the dynamic equicorrelation between twelve cryptocurrencies from 2015 to 2019. The authors noted that if Bitcoin, for example, suffers a serious failure, other cryptocurrencies may be affected. In addition, evidence suggests a time-varying mean return equicorrelation. It also points out that the integration of markets has its causes in the volume of business and measures of uncertainty. Schinckus et al. (2021) studied 1636 cryptocurrencies and observed densification of interconnections between virtual currencies. The authors suggested that cryptocurrencies presented short-term interdependence between them. The power of influence of each cryptocurrency varied year after year. Schinckus et al. (2020) related Bitcoin trading volume data to energy consumption to verify the impact on climate change. Their results revealed a significant and increasing positive influence between energy consumption and cryptocurrency activities. They also verified that the increase in energy consumption is linked to the increase in cryptocurrency activities. Naeem et al. (2021) used Multifractal Detrended Fluctuation Analysis (MF-DFA) to assess the asymmetric efficiency of the Bitcoin, Ethereum, Litecoin and Ripple series during the COVID-19 pandemic. The approach revealed that in downtrends, there is weaker multifractality than in uptrends. The two largest cryptocurrencies (Bitcoin and Ethereum) were the most affected. They presented a rapid recovery from the slide into inefficiency during the pandemic period. Bouri et al. (2021a) investigated seven cryptocurrencies between 2015 and 2020 and observed instability of the connectivity system under extreme events, such as COVID-19. The authors pointed out besides Bitcoin, other cryptocurrencies such as Ethereum contribute to the stability of the connection network in the cryptocurrency market. This fact indicates the importance of other cryptocurrencies, in addition to Bitcoin, must also be investigated during monitoring. Regarding forecasting, Markov chain models have been used for various types of phenomena, such as wind speed predictability (Song et al. 2014), the stock market (Kiral and Uzun 2017), water resources management (Rezaeianzadeh et al. 2016), and inflammatory breast cancer with bone metastasis (Fujii et al. 2019). Leaning on forecasting financial markets using Markov chains, Svoboda and Lukas (2012) used four Markov chain models to forecast trends in Prague stock indexes and analyze investment strategies. The authors used discrete state spaces to define the models and generate the transition probability matrices used in the forecast. Soloviev et al. (2011) performed the forecast with Markov chains for American, European, and Asian stock indexes. The results presented in the study are very favorable and reveal the efficiency of the model. There are several works on cryptocurrency forecasting in the literature (Sun et al. 2020; Chowdhury et al. 2019, 2020; Velankar et al. 2018; Catania et al. 2019) and a variety of Markov chain applications for forecasting different phenomena (Zhan-Li and Jin-Hua 2011; Liu et al. 2009; Chaudhuri et al. 2014; Carvalho and Moura 2015). However, our study is the first study that uses Markov chains from the first to the tenth order to extract rules to be used to forecast cryptocurrency returns. Namely, the advantages of our approach lie in the development of a simple model that is easy to use, understand and implement. We verified the stochastic dependence of the first to tenth-order models of the Markov chains, analyzing the adjustment errors and investigating the existence of dependence on higher orders in the process. About the disadvantages, the dependence was considered by directly relating the observations with each order of the Markov chains. Furthermore, as we increase the order of the Markov model, there is an increase in memory consumption, leading to a memory cost of \(\Theta (n^{m+1})\), i.e. the memory cost increases with the number of state transition n raised to power \(m+1\), where m is the Markov chains order. In this way, this prevents us from using higher orders in the models. In this sense, our observations are restricted only to the investigated orders, leading to statements regarding only those orders. Therefore, in this paper, the main contributions are as follows: extract rules from the dynamics of cryptocurrencies return time series via Markov Chains to determine possible future scenarios, analyzing memory dependence on the process dynamics. The first to tenth order models were used to assess which one provides the slightest forecast error for the time series. We use time series data for the four main cryptocurrencies by market capitalization: Bitcoin, Ethereum, Litecoin e Ripple. The rest of this paper is structured as follows: Sect. 2 presents a brief description of Markov chain models. Section 3 displays the evaluation metrics used, Sect. 4 describes the investigated data set. Sections 5 and 6 expose the data pre-processing, the experiment protocol, the empirical results, and analysis, respectively. Finally, Sect. 7 sets out the main conclusions.",4
61.0,3.0,Computational Economics,18 March 2022,https://link.springer.com/article/10.1007/s10614-022-10238-6,Pricing a Specific Equity Index Annuity in a Regime-Switching Lévy Model with Jump,March 2023,Yayun Wang,,,Unknown,Unknown,Unknown,Unknown,,
61.0,3.0,Computational Economics,18 February 2022,https://link.springer.com/article/10.1007/s10614-022-10239-5,Prediction of Loan Rate for Mortgage Data: Deep Learning Versus Robust Regression,March 2023,Donglin Wang,Don Hong,Qiang Wu,Unknown,Male,,Mix,,
61.0,3.0,Computational Economics,21 February 2022,https://link.springer.com/article/10.1007/s10614-022-10240-y,A Dynamic Mechanism Design for Controllable and Ergodic Markov Games,March 2023,Julio B. Clempner,,,Male,Unknown,Unknown,Male,"Mechanism design is a formal study of solution concepts for generating certain outcomes in a class of self-interested private-information games. It is a method for solving game theory issues that is based on engineering principles. The mechanisms assume that values are private and that incentives are given by monetary transfers, with the goal of achieving a Bayesian–Nash equilibrium. Because the players are rational, they want to maximize their own self-interest. Players are not motivated to offer correct information while acting in their own self-interest. Mechanism design established the existence of an incentive-compatible mechanism. A mechanism is said to be truthful (subject to an incentive-compatibility constraints) if a player always maximizes his rewards by declaring his true type, no matter what the other players do (declare). Computing an effective and efficient mechanism that maximizes the rewards is a challenge. The main goal is to design a mechanism for self-interested players such that at equilibrium the rewards are maximized (Bergemann & Said, 2011). Over the past decade there has been a large interest in understanding how to design mechanisms and Reinforcement Learning (RL) methods. Goldman and Zilberstein (2003) looked at mechanism that lead to the breakdown of a global challenge into smaller, more manageable problems that are solved each time the agents exchange data. Jain and Walrandb (2010) proposed a mechanism for auctioning bundles of multiple divisible goods in a network. Bergemann and Välimäki (2010), and Athey and Segal (2013) presented a dynamic approach of the Vickrey–Clarke–Groves considering quasilinear payoffs in which agents observe private information. Pavan et al. (2014) considers dynamical mechanism design in dynamic quasilinear environments where private information arrives over time. Sinha and Anastasopoulos (2017) suggested a mechanism design for a network where strategic agents, who are contesting for allocation of resources, are divided into fixed groups. Baumann et al. (2019) looked at how an external agent might encourage artificial learners to cooperate by delivering additional incentives and penalties based on the behaviors of the learners. Mguni (2019) to effectively calculate optimum incentive compatible mechanisms, he proposed a technique that integrates stochastic optimization and RL inside mechanism design. Clempner and Poznyak (2020b) provided a Bayesian method for games that took into account both the Bayesian model’s imperfect information and the incomplete information over the states of the Markov system and broaden the design theory to include mechanism design and the joint observer design. Grover et al. (2020) demonstrated a Bayes-adaptive RL algorithm based on model-based online planning. Senda et al. (2020) proposed a BRL model for robots based on an approximate parametric method, including live Bayesian estimation and planning for an estimated model. In a Bayesian environment, Kassab and Simeone (2020) presented a gradient descent approach for federated learning. Nolan et al. (2020) provided a parameter estimation method based on a classification job and utilizing artificial neural networks to conduct Bayesian estimation effectively. Using hierarchical Bayesian modeling, van Geen and Gerraty (2020) developed a method for deriving empirical priors. Vasilyeva et al. (2021) presented two distinct preconditioning methods: Machine learning technology and multiscale approach for model order reduction. Clempner and Poznyak (2021) suggested an analytical approach for calculating the mechanism design in the context of a paradigm in which participants in a non-cooperative Markov game with imperfect state knowledge pursue an average utility. For a class of controllable homogeneous Markov games, Clempner (2021a) developed a dynamic Bayesian–Stackelberg incentive-compatible mechanism in which many agents see private information and learn their behavior through a series of encounters in a repeating game, where it is presumed that leaders may commit to their disclosure approach and method in advance and influence followers’ behavior. This paper suggests a method to construct an incentive-compatible mechanism in a dynamic framework where players acquire knowledge of their preferences through repeated interactions. We propose an analytical method for computing Bayesian incentive-compatible mechanisms where the private information is revealed following a class of controllable Markov games. Our method incorporates a new variable that represents the product of the mechanism design, the strategies, and the distribution vector. We derive the relations to analytically compute the variables of interest. The introduction of this variable makes the problem computationally tractable. We use the notion of Bayesian–Nash equilibrium as the equilibrium concept for our game. For computing the equilibrium in Markov games, we employ an iterative approach conceptualized in two half steps: (i) the proximal approach, and (ii) the gradient method. This approach converts the game theory problem in a system of equations, which is an independent optimization problem calculated using a nonlinear programming solver. We show that the method convergence to the equilibrium point. We also consider a dynamic environment where players acquire knowledge of their preferences through repeated interactions and decisions are taken after several finite periods. The method involves a RL approach which computes the near-optimal mechanism in equilibrium with the resulting strategy of the game with high profit maximization. We implement a controller exploitation-exploration architecture for computing near-optimal policies, in which the controller, employing the Kullback–Leibler divergence among the distribution of the policies makes a trade-off between the exploration and the exploitation processes (Asiain et al., 2019). In our approach players learn by selecting the best-reply strategies their behavior in response to a mechanism that is computed in equilibrium. We show that the RL method converges. An interesting challenge is that for the objective of profit maximization there is no single optimal mechanism because there are multiple equilibria. For solving this problem, we employ the regularization Tikhonov’s approach, which is a popular technique to solve ill-posed of the minimization problem (Clempner & Poznyak, 2018a, 2018b). We show the convergence to a unique incentive-compatible mechanism and the equilibrium of the game. This yields a new and significantly improved results for different game theory problem domains and in addition, produces incentive-compatible mechanisms that match the equilibrium of the game. The approach seeks approximate to y by substituting the ill-posed problem given by \(\underset{y \in Y_{adm}}{\min } \zeta (y)\) by a penalized problem given by \(\zeta _{\delta }(y)= \zeta (y)+\tfrac{\delta }{2}\left\| y \right\| ^{2} \) such that \(\left\| \cdot \right\| \) denotes the Euclidean vector norm and the scalar \(\delta >0\) is known as the regularization parameter. The expression \( \frac{\delta }{2}\left\| y \right\| ^{2}\) penalizes the large values of y. In game theory, regularization plays a fundamental role in order to ensure the convergence to one of the Nash equilibria. The paper is organized as follows. Next section presents a description of the model. The analytical method for computing the mechanism and the equilibrium is suggested in Sect. 3. Section 4 describes the RL approach. Section 5 shows the convergence analysis. A numerical example is presented in Sect. 6. Section 7 concludes.",3
61.0,3.0,Computational Economics,21 February 2022,https://link.springer.com/article/10.1007/s10614-022-10241-x,Evaluation of Non-survey Methods for the Construction of Regional Input–Output Matrices When There is Partial Historical Information,March 2023,Cristian Mardones,Darling Silva,,Male,Unknown,Unknown,Male,"Computational economics focuses on using computers to analyze stylized or highly complex models that allow answering questions that are not easy to obtain with standard tools (Cogliano et al., 2021). For example, the Monte Carlo method is a computational technique that simulates possible results by substituting the values of parameters or variables about which there is uncertainty (Hall et al., 2020; Jang et al., 2020; Tanaka et al., 2021). Advances in computing power and the possibility of making more realistic assumptions have increased the use of simulation tools to solve different economic problems (Platt, 2021). Therefore, in this study, the Monte Carlo method is used to provide new evidence in the area of regional economics. In the regional economics literature, there is a long discussion about methods to elaborate input–output matrices (IOTs) at the regional level, including methods based on surveys, non-surveys, and hybrids (Lampiris et al., 2020).Footnote 1 The construction of regional IOTs is very relevant since it allows the analysis at the regional level of the productive structure, identify productive linkages, simulate policy impacts, obtain production, income, and employment multipliers, among others (Loizou et al., 2019; Zhang et al., 2019; Wang et al., 2018; Mardones & Hernández, 2017; Ramos et al., 2017; Nakano et al., 2018; Bonfiglio & Esposti, 2016; Chiquiar et al., 2017). In general, national IOTs are prepared for a specific year with a considerable lag since their construction involves relevant efforts to collect all the required information. However, regional IOTs are more challenging because the regional economy differs from the national economy in various aspects, such as the structure of production, size, trade relations among regions, and there is also limited availability of data at the regional level.Footnote 2 The construction of regional IOTs from survey-based methods is very demanding in terms of time and cost due to the high requirements of intersectoral information, so practically no countries are making efforts to construct these matrices with primary data.Footnote 3 However, there are other indirect methods simpler and/or less expensive, which have not been exempt from criticism and questions about their accuracy. Non-survey methods apply formulas that transform national IOTs to a regional context, while the hybrid method combines non-survey methods with primary and secondary data obtained through experts, economic surveys, and other reliable sources. Consequently, it is necessary to provide more empirical evidence to validate non-survey methods (Flegg et al., 2021; Lampiris et al., 2020; Mardones & Silva, 2021) or develop new hybrid methods to elaborate regional IOTs (Hiramatsu et al., 2016; Lahr et al., 2020; Többen & Kronenberg, 2015). Non-survey methods assume that the region and the country have identical production structures but require adjusting the regional input coefficients for intermediate inputs from other regions through so-called location quotients (LQs) (Kowalewski, 2015). The different location quotients proposed in the literature attempt to quantify the specialization of an economic sector at the regional level with respect to the same sector specialization at the national level (Lamonica & Chelli, 2018). However, it is a problem to know the best location quotient from observable data since countries regularly publish IOTs at the national level but do not elaborate regional IOTs.Footnote 4 Therefore, some previous studies have used Monte Carlo simulation to choose the best location quotient method, considering the different non-parametric and parametric formulas proposed in the literature (See Mardones & Silva, 2021; Lampiris et al., 2020; Bonfiglio, 2009; Bonfiglio & Chelli, 2008). The first location quotients that emerged to regionalize national IOTs were the simple location quotient (SLQ) and the cross-industry location quotient (CILQ), which used only sectoral production data at the regional and national level for their calculation. However, they were criticized by producing substantial overestimations of regional input coefficients and underestimating regional imports (Harrigan et al., 1980; Morrison & Smith, 1974; Stevens et al., 1989). The above is explained because the formulas used in these methods do not consider the region's relative size, underestimating the propensity to import from other regions. To solve the previous problem, Flegg et al. (1995) developed the Flegg location quotient (FLQ), which has the characteristic of increasing regional imports if the region is smaller. Later, Flegg and Webber (1997) generated a revised version of the FLQ formula. McCann & Dewhurst (1998) raised some concerns about the FLQ since the tendency to import is related to the region's size, but it is also important to consider regional specialization. For the above, Flegg and Webber (2000) developed the AFLQ formula that takes regional specialization into account, demonstrating that a measure of regional specialization in the location quotients does not produce more accurate estimates. More recently, Kowalewksi (2015) suggested an extended formula of the FLQ called the specific industry location quotient (SFLQ) that allows variations by economic sector. It is worth mentioning that the location quotients FLQ, AFLQ, and SFLQ are parametric since they are based on the SLQ and CILQ, regional and national production, and require determining a parameter δ that can adopt values between zero and one. This last parameter is not easy to determine since there are practically no true regional IOTs with which to make comparisons to find the appropriate values of δ. The empirical literature has tried to contribute to the discussion of which location quotient method is more appropriate, using the few available regional IOTs that are survey-based. Tohmo (2004) made a comparison of the regional input coefficients and output multipliers estimated from surveys applied in a region of Finland with those estimated through location quotients, showing that the SLQ and CILQ produce highly erroneous regional input coefficients and output multipliers, while the FLQ produces the best estimates in virtually all economic sectors. Flegg and Tohmo (2013) analyzed the use of location quotients to elaborate regional IOTs with data from 20 regions of Finland, determining that the FLQ and AFLQ achieve the best results. Specifically, the FLQ performs better than the AFLQ if the value of δ is between 0.2 and 0.3, but the AFLQ slightly outperforms the FLQ if the value of δ is greater than 0.3. Flegg et al. (2016) used data based on surveys from a province of Argentina to evaluate the behavior of the location quotient methods, establishing that a δ in the interval 0.3 to 0.4 is appropriate for the FLQ, while a value of 0.4 is suitable for the AFLQ. Lamonica and Chelli (2018) evaluated the performance of non-survey methods from multi-country IOTs for 41 countries with a classification of 35 economic sectors, concluding that the performance of the SLQ and CILQ decreases when applied to national IOTs with many coefficients close to zero, while the performance of the FLQ with δ equal to 0.2 and AFLQ with δ between 0.2 and 0.3 are not affected by this situation. Kowalewksi (2015) evaluates the performance of SFLQ using IOTs based on surveys from a German region; the results reveal that the optimal value of δ varies drastically between sectors, but if this parameter is between 0.11 and 0.17, very precise regional input coefficients and regional output multipliers are produced. Furthermore, a regression analysis concluded that highly concentrated economic sectors tend to require a higher value of δ. Flegg and Tohmo (2018) used survey data from 16 regions of South Korea to refine the application of the FLQ and SFLQ, and then, they used a regression model to estimate the parameter δ in each sector; their results show that the variation in the propensity to import from abroad plays a key role in determining the value of δ. All the previous studies that analyze regional IOTs based on surveys agree that the FLQ and AFLQ methods perform well when δ varies between 0.2 and 0.4. However, recently it has been shown that the SFLQ method generates better results. Despite the above, the choice of the parameter δ in the FLQ, AFLQ, and SFLQ formulas is an open question that has been an obstacle to successfully applying the parametric LQs methods (Flegg et al., 2021). The behavior of non-survey methods for constructing regional IOTs from multiregional IOTs has also been obtained with Monte Carlo simulation. Bonfiglio and Chelli (2008) show that the FLQ and the augmented Flegg location quotient (AFLQ) represent a significant improvement over the SLQ and CILQ, both in the precision of the estimates and in the generation of more stable errors. Bonfiglio (2009) generated multiregional IOTs from Monte Carlo simulation to determine an interval of values of the parameter δ within which it is more likely to find the best estimate for a region with the FLQ (δ = 0.3) and AFLQ (0.3 ≤ δ ≤ 0.4) method. Lampiris et al. (2020) compare European Union IOTs for the period 2010–2014 to the IOTs estimated by the FLQ, AFLQ, SLQ, and CILQ, concluding that the AFLQ and FLQ provide better results with δ from 0.1 to 0.3. Finally, Mardones and Silva (2021) determine with Monte Carlo simulation that the value of δ in the FLQ or AFLQ method is sensitive to the use of simulated or true sectoral production in each region. In the previous literature based on Monte Carlo simulation, the performance of non-survey methods has been evaluated with a uniform distribution between 0 and 1 to estimate the regional input coefficients, which are assumed to be true for the analysis. However, more realistic simulation scenarios are proposed in this study, taking advantage of some available information. Specifically, historical data on total regional technical coefficients (sum of regional input coefficients, inter-regional input coefficient, and imported input coefficient) of each region of Chile in 1996 is used to establish an upper limit for the uniform distribution.Footnote 5 Thus, it is intended that the evaluation of location quotient methods consider a regional economic structure more similar to the true structure of 1996 and then determine if this contributes to obtaining more precise results. Additionally, it should be mentioned that no study based on Monte Carlo simulation has evaluated the performance of the SFLQ method since it is relatively new and requires determining a key parameter (δ) that varies by region and economic sector. Therefore, this study proposes obtaining this parameter through an iterative process that allows minimizing the performance indicator most used in the literature (MRAD indicator). The main results of this study show that the parameterized location quotients with different δ by region and/or sector represent an effective improvement over other location quotients for obtaining output multipliers closer to the 'true' ones. Specifically, the SFLQ with different δ by sector and region is the best-evaluated non-survey method in all scenarios. The FLQ and AFLQ with different δ by region also provide good results in terms of performance. Regarding the three simulated scenarios, it is concluded that placing the evaluation in a more realistic context through the 'true' data of regional production and narrowing down the distribution of regional input coefficients increases the precision of non-survey methods. The present evaluation focuses only on location quotients methods since they are widely used in many empirical studies due to the low requirements for their calculation. Other non-survey methods rely on sophisticated optimization techniques or require more specific data not available in Chile, such as interregional trade flows. Therefore, this study aims to contribute to the regionalization of IOTs in countries with no data available to apply other non-survey methods easily.",3
61.0,3.0,Computational Economics,17 February 2022,https://link.springer.com/article/10.1007/s10614-022-10242-w,Accurate and Efficient Finite Difference Method for the Black–Scholes Model with No Far-Field Boundary Conditions,March 2023,Chaeyoung Lee,Soobin Kwak,Junseok Kim,Unknown,Unknown,Unknown,Unknown,,
61.0,3.0,Computational Economics,04 March 2022,https://link.springer.com/article/10.1007/s10614-022-10243-9,Dating Currency Crisis and Assessing the Determinants Based on Meta Fuzzy Index Functions,March 2023,Adem Gök,Nihat Tak,,Male,Male,Unknown,Male,"The financial crisis is the disruption in the financial market, usually associated with falling asset prices and debt insolvency, which undermines the capacity of financial markets to allocate capital by spreading across the financial system (Eichengreen & Portes, 1987). There are several types of financial crises; currency crises, banking crises, debt crises, balance of payments crises and inflation crises (Racickas and Vasiliauskaite, 2012). Currency crisis is the most common form of financial crises, which involves a speculative attack on the domestic currency leading central bank to defend the value of the domestic currency either by expanding large amount of foreign reserves or by sharply increasing interest rates (Claessens & Kose, 2013). The aim of the study is to use a novel method, which is called meta fuzzy index functions (MFIFs), to date currency crisis in Turkey over the period of January 1990 and October 2019 and to assess the determinants of currency crises.
 There are several papers in the literature for dating currency crises (Boonman, 2019; Ari and Cergibozan, 2016; Jacobs, 2004). The primary difference between MFIFs and these papers is that MFIFs combine the data on all currency crisis indices (CCIs) in functions instead of using just one method of currency indexing. Particularly, by using the fuzz c-means clustering algorithm, the MFIFs integrate various CCIs into functions for a certain dataset. We compile the best placed CCIs in the function, which we select to be the best one with greater effect size. Given the motivation for the study, there are several contributions of the paper to the corresponding literature. First, the paper introduces a novel approach to date currency crisis in Turkey. Second, it evaluates the determinants of currency crises. Last, it provides policy implications to prevent the occurrence of currency crises.
 Section 2 introduces the literature on financial crises experienced in Turkey over the period of 1990–2019. Section 3 represents the currency crisis indices used in the analysis to generate our currency crisis index. We dated currency crises for Turkey over the period of 1990M01-2019M10 with our novel technique in Sect. 4. In Sect. 5, we assessed the determinants of currency crises with logit model and we made a robustness check with another currency crisis index generated by principal component analysis (PCA). The results are emphasized and discussed in Sect. 6.",1
61.0,3.0,Computational Economics,12 March 2022,https://link.springer.com/article/10.1007/s10614-022-10244-8,Threshold Moving Approach with Logit Models for Bankruptcy Prediction,March 2023,Michaela Staňková,,,Female,Unknown,Unknown,Female,"Logistic regression (Logit) models are a very popular tool for classification (especially binary classification) in various fields of research. These models are widely used, for example, in the marketing field for segmentation, as in Zambrano-Rey et al. (2019), in quality management, as in Beneš and Hampel (2016), in the financial classification of countries as in Blašková et al. (2013) or for the purposes of bankruptcy prediction as in Vavřina et al. (2013). In our article, attention is focused on Logit models which are used to predict company bankruptcy. The assessment of a company’s financial situation has been a very important matter for decades in the academic or business fields of corporate finance. A deep understanding of a company’s financial situation is essential not only for the company’s top management, shareholders and creditors (see James, 2016) in supporting their decision processes, but also for other entities connected with the company. For example, company financial difficulties, especially for a number of bankrupt companies during the same period - can create serious problems, often such as unemployment. The unemployment rate represents a serious matter for the government. Therefore, there is a constant demand for increasingly accurate and stable instruments for predicting a company’s financial situation. From the second half of the twentieth century onwards, the issue of assessing the financial health of a company began to receive more attention. The problem of bankruptcy prediction is being investigated by a large number of scientists who have developed more or less successful prediction models over time. We are able to predict various future states of a company, but the most common is the prediction of bankruptcy. Bankruptcy is a clearly defined situation (as opposed to detecting financial distress or other risks). All models assessing a company’s financial situation are based on the assumption that companies have symptoms typical of bankruptcy for some time before this situation arises. These symptoms comprise, in particular, liquidity problems, problems with the volume of net working capital and problems with return on invested capital, etc. Using the company’s financial statements, many financial indicators that could predict a company’s bankruptcy can be defined. These financial indicators can be used to construct the model. Some authors use previously established procedures (more precisely, defined combinations of financial indicators) to calculate a bankruptcy score, see Zorn et al. (2017). However, the results of Borańska and Grzegorzewska (2018) show that if the own expert view on the selection of financial variables is incorporated in the evaluation of the company the predictive capabilities of the bankruptcy model are positively affected. There are also cases where a possible bankruptcy is derived from tracking the business cycle and contemporaneous unemployment rates. However, as a study by Wang et al. (2020) shows, in times of the Covid crisis, these relationships are ineffective for predicting bankruptcy, and it is therefore appropriate to define one’s own financial indicators and construct one’s own bankruptcy model. The oldest well-known bankruptcy models were based on multiple discriminant analysis (MDA) as in Beaver (1966), Beerman (1976), and Altman (1968). However, these models have been overturned and replaced by other approaches. The Logit model by Ohlson (1980) and the probit model by Zmijewski (1984) can be mentioned. These statistical approaches later gained competition from other methods such as neural networks, supporting vector machines (SVM) and decision trees (DT). Much research has been done to compare the classification capabilities of these methods, see for example Klepáč and Hampel (2017), Li et al. (2010), Sun and Li (2012), and Niknya et al. (2013). As found in the article (Staňková and Hampel, 2018), the prediction capabilities of individual methods can be strongly influenced (both positively and negatively) not only by the exact setting of the particular model, but also by the method for pre-processing variables. In the article Staňková and Hampel (2018), attention was paid to the Logit method together with the SVM and DT method. Based on the results of the study by Staňková and Hampel (2018), it can be stated that the Logit models excelled in comparison with other methods used in the terms of the stability of very good results in classification, especially for periods longer than one year before the bankruptcy. Previously, in studies such as Min and Lee (2005), models were estimated based on a large and balanced dataset. However, looking at any sector of economy, it is clear that active companies make up the majority of companies and bankrupt companies appear less frequently. In cases where the ratio of active to bankrupt companies is not balanced in the model, it is necessary to analyse the classification capabilities of the model in more detail. In studies such as Klepáč and Hampel (2017) and Staňková and Hampel (2019), the error rate of the model was independently assessed within the group of active and also within the group of bankrupt companies (so-called type I error and type II error). Because simply following the maximization of the total accuracy of the model via an imbalanced dataset can lead to unsuitable models in practice. According to Collell et al. (2018) class imbalance represents a major hurdle in the application of classification methods. Regardless of the chosen approach, unbalanced datasets are widely used today and with this the need for appropriate setting of these models grows. Studies like Galar et al. (2012), López et al. (2013), Zou et al. (2016), Li et al. (2016), and Sun et al. (2019) can be named in this area. It can be stated that this problem can be solved in various ways. According to Collell et al. (2018) studies can be divided into three main groups of approaches: cost-sensitive learning, rebalancing mechanisms, and threshold moving. Cost-sensitive and rebalancing methods require changes in the learning phase. According to Chawla et al. (2004), it is possible to use various procedures for rebalancing such a random oversampling with replacement, random undersampling, direct oversampling, directed undersampling, or oversampling with informed generalization of new sample. However, oversampling can result in overfitting for some models and undersampling can result in losing information invaluable to a model. According to Nguyen et al. (2010), the cost-sensitive procedure is widely used in decision trees. Under this procedure, the misclassification errors made by the model are not equal. It is possible to give higher misclassification costs for a minority class, while the aim of the cost-sensitive learning method is to minimize the total cost of classification. In this article, attention will be paid to finding the best threshold by the threshold-moving approach. In these approaches, a bankruptcy prediction model can be learned from the dataset with either the standard or modified class proportions and the model’s output is modified into a class label by applying a proposed threshold. According to Collell et al. (2018), this technique is undervalued and very little attention is paid to it. The main aim of this article is to evaluate the predictive capabilities of the constructed bankruptcy prediction model based on financial data on companies. It is an attempt to predict bankruptcy for a period of up to four years before the bankruptcy occurred. In order for the conditions to be as close as possible to the actual market situation, the ratio of active and bankrupt companies in the data set will not be balanced. The partial goal of this article is to find a suitable threshold for the classification of active and bankrupt companies using the Logit model based on data where bankrupt companies are less represented. Unlike most studies, a suitable threshold will be sought on the basis of empirically estimated ROC curves, individually for the period of one, two, three and four years before a company’s bankruptcy. In order to achieve robust results, Logit models will be estimated based on data from two manufacturing sectors, allowing a cross-sectoral comparison of results. The structure of the paper is as follows: The section Data and Methods describes the datasets, variables and models used. The section Measuring Prediction Quality covers the criteria used to measure the performance of the models and to find suitable thresholds. The empirical results are then presented and discussed. Finally, brief conclusions are provided.",3
61.0,3.0,Computational Economics,25 March 2022,https://link.springer.com/article/10.1007/s10614-022-10245-7,Modelling Sovereign Credit Ratings: Evaluating the Accuracy and Driving Factors using Machine Learning Techniques,March 2023,Bart H. L. Overes,Michel van der Wel,,Male,Male,Unknown,Male,"A sovereign credit rating is an evaluation of the credit risk of a country and gives an indication of the likelihood that the country will be able to make promised payments. These ratings have a large influence on the interest rate at which governments are able to issue new debt and thereby a big effect on government spending and the government deficit. Sovereign credit ratings are usually given by one of three credit rating agencies (CRAs): Moody’s, S&P, and Fitch. These agencies use a combination of objective and subjective factors to determine the rating, however, unfortunately, the exact rating methodology and the determining factors remain unknown. This lack of transparency has resulted in widespread criticism of the CRAs. They have, among other things, been accused of giving biased ratings Luitel et al. (2016), reacting slowly to changing circumstances Elkhoury (2009), and behaving procyclically Ferri et al. (1999). Getting an understanding of the rating methodology and the determining factors would be very helpful for governments, investors, and financial institutions. Governments would be able to anticipate possible rating changes, while investors and financial institutions could check if ratings deviate from what the fundamentals of a country imply. In order to get an understanding of the credit rating process, a model is needed that can predict the ratings, ideally with high accuracy. Research has, up until now, mostly focussed on modelling sovereign credit ratings using various forms of the ordered probit/logit (OP/OL) model, which assumes a particular functional form for the relation between a linear combination of the input variables and the continuous output variable, or other related models, see, for example, Cantor and Packer (1996), Dimitrakopoulos and Kolossiatis (2016), Reusens and Croux (2017).Footnote 1 These models allow for easy interpretation of the determining factors and prove to be fairly accurate, but come at the cost that the linear relation they assume might not always hold. A recent branch of research has therefore focussed on using machine learning (ML) techniques to model sovereign credit ratings (Bennell et al., 2006; Ozturk et al., 2015, 2016). Ozturk et al. (2015, 2016) show that ML models outperform linear models on predictive accuracy, sometimes by a large margin. Multilayer perceptron (MLP), classification and regression trees (CART), support vector machines (SVM), and Naïve Bayes (NB) are among the commonly used techniques. Where especially MLP and CART prove to be well suited for modelling sovereign credit ratings. However, getting an insight into the inner workings of the models and their determining factors is difficult. This paper focusses on obtaining the determining factors of four ML models used for sovereign credit rating; MLP, CART, SVM, and NB, the latter of which has, up until now, not been done for ML models in the sovereign credit rating setting. This will give insight into the way in which the ML models give the ratings and what variables are important in the process, lack of these insights has been the main weakness of the ML models to date. In order to obtain the determining factors, we use so called Shapley additive exPlanations (SHAP) Lundberg and Lee (2017). SHAP allow for the isolation of each variable’s effect and can pick up on non-linear relations, making them well suited for the interpretation of ML models. Getting an understanding of these more accurate models will help figure out the driving factors and methodologies for sovereign credit ratings, as interpreting a model is only useful when that model accurately represents reality. We contrast these approaches to an OL model, this allows for examining how different the insights are for Machine Learning methods compared to a more econometric approach. This study uses Moody’s credit ratings for a set of 62 developed and developing countries, such as Argentina, China, Germany and New Zealand, for the period 2001–2019, to train and evaluate the models. The explanatory variables are similar to those of Dimitrakopoulos and Kolossiatis (2016): GDP growth, inflation, unemployment, current account balance, government balance, government debt, political stability, regulatory quality, and GDP per capita. These variables are chosen because they proved to be important in the credit rating process in earlier studies (Cantor & Packer, 1996; Afonso, 2003; Butler & Fauver, 2006). We document that MLP is the most accurate model for sovereign credit ratings with an accurate rating prediction in a random split cross-validation of 68%, and 86% of ratings correct within 1 notch. Where the percentage within 1 notch indicates what fraction of the ratings given by the model does not deviate more than 1 class from the actual rating. CART follows relatively closely with an accuracy of 59%, and 76% correct within 1 notch. The other two Machine Learning techniques, SVM, with 41% correct and 59% within 1 notch, and NB, 38% correct and 61% within 1 notch, prove to be less accurate. OL significantly underperforms the best ML techniques, with correct predictions for only 33% of the observations, and 57% within 1 notch. Analysis of the determining factors shows some heterogeneity between the different modelling techniques. Nonetheless, regulatory quality and GDP per capita are very important explanatory variables in the two best performing models, being MLP and CART. The relation between these explanatory variables and the credit rating is as expected, with regulatory quality and GDP per capita having a positive influence on the credit rating. The structure of our paper is as follows. We begin by discussing the methodology used in this study in Sect. 2, directly followed by a discussion of the data in Sect. 3. Section 4 gives an overview of the results obtained in this study. Section 5 concludes.",3
61.0,4.0,Computational Economics,19 March 2022,https://link.springer.com/article/10.1007/s10614-022-10247-5,Short- and Long-Term Interactions Between Bitcoin and Economic Variables: Evidence from the US,April 2023,Lei Wang,Provash Kumer Sarker,Elie Bouri,,Unknown,Male,Mix,,
61.0,4.0,Computational Economics,31 March 2022,https://link.springer.com/article/10.1007/s10614-022-10248-4,A Dynamic Baseline Calibration Procedure for CGE models,April 2023,Johannes Ziesmer,Ding Jin,Christian Henning,Male,,Male,Mix,,
61.0,4.0,Computational Economics,01 April 2022,https://link.springer.com/article/10.1007/s10614-022-10252-8,The Slicing Method: Determining Insensitivity Regions of Probability Weighting Functions,April 2023,Martín Egozcue,Luis Fuentes García,Ričardas Zitikis,Male,Male,Male,Male,"Extensive empirical studies, mostly conducted by psychologists and behavioural economists, have shown that people weigh objective probabilities of outcomes in a nonlinear fashion. This observation deviates from some of the axioms of expected utility theory (von Neuman & Morgenstern, 1944) and thus has inspired modern theories that aim at better explanations of human behaviour (e.g., Dhami, 2016; Wakker, 2010). A special feature of some of these theories is that objective probabilities are transformed by probability weighting functions (pwf’s), which have close connections to weighted distributions in statistics and pricing in insurance (e.g., Furman & Zitikis, 2008, 2009). Numerous pwf’s have been proposed in the literature, and many studies have been devoted to exploring their properties (e.g., al-Nowaihi & Dhami, 2010; Fehr-Duda & Epper, 2012; Karmarkar, 1978; Stott, 2006; Wu & Gonzalez, 1996, and references therein). One of these properties is based on the likelihood insensitivity region, which reflects the tendency of people to have different perceptions of probabilities. Many empirical studies have argued that individuals’ decision weights are less sensitive to moderate probabilities and very sensitive to extreme probabilities. In other words, pwf’s tend to be flatter for intermediate probabilities and steeper near the endpoints of the probability interval (0, 1). This intermediate range of probabilities has given rise to several notions of likelihood-insensitivity regions. Based on this observed psychological trait, Tversky and Wakker (1995) proposed to use the class of cavex (i.e., concave-convex) pwf’s that has become a prevailing functional form in behavioural economics. In this context, the notions of lower subadditivity (henceforth simply called subadditivity) and upper subadditivity naturally arise and play important roles in decision making. In particular, using these notions, definitions of insensitivity regions have been given. The regions have been captured empirically (e.g., Abdellaoui, 2000; Bleichrodt & Pinto, 2000; Gonzalez & Wu, 1999; Kilka & Weber, 2001), although reliable analytical tools for determining them have been elusive. As far as we know, there has been one attempt in this direction in the economics literature (Wakker, 2010) in the form of a heuristicFootnote 1 technique that aims at finding the maximal region of subadditivity. It has been observed, and our own testing has confirmed, that the technique works well for many pwf’s that appear in the literature, but there are also examples when the technique fails to provide correct answers. Yet, as far as we are aware of, there has not been a comprehensive study to determine when the technique works. In the current paper, we shall shed light on this topic by proposing a method for determining the subadditivity regions of pwf’s. Determining subadditivity and insensitivity regions has important decision-making implications, as they are used to explain several relevant problems in economic analysis. For instance, Baillon et al. (2020) analyzed relations between insensitivity regions and risk underprevention. Several authors used insensitivity regions in the context of probability uncertainty and applied them to study decision-making problems such as the Ellsberg paradox (Abdellaoui et al., 2011; Baillon et al., 2018), the Allais paradox (Neilson, 2003; Neilson & Stowe, 2002), tort and contract laws (Posner, 2004), terrorism (Sunstein, 2003; Phillips and Pohl, 2020). The interest in insensitivity regions also arises when studying ambiguity aversion, called a-insensitivity (Dimmock et al., 2016). This is the tendency to treat subjective likelihood as fifty-fifty, and overweighting extreme events, as has been observed in several ambiguity aversion studies (Crockett et al., 2019; Dimmock et al., 2016). This feature is the analog of the cavex pwf’s that are usually encountered in risk analysis. It is also used to explain why source preferences between known and unknown probabilities and ambiguity aversion depend so much on the likelihood of events (Wakker, 2010), and it has been applied to study many topics in finance such as stock market participation (Dimmock et al., 2016) and asset pricing (Izhakian, 2020). The aforementioned studies and our consulting engagement to be described later in this paper have motivated our current research. The rest of the paper is organized as follows. Mathematical preliminaries and special properties of pwf’s are presented in Sect. 2, where we also recall the heuristic technique for determining subadditivity regions. Conditions under which the technique works are discussed in the same section, alongside several examples that illustrate the technique’s strengths and limitations. In Sect. 3 we introduce the slicing method (Theorem 3.1) for determining accurate subadditivity and thus likelihood-insensitivity regions, and we then illustrate the method by completing examples of Sect. 2. In Sect. 4 we derive subadditivity and insensitivity regions of several classical pwf’s that are frequently used in, or have arisen from, empirical studies. In Sect. 5 we consider an illustrative example that resembles those real-world cases that we have encountered during our consulting engagements. We shall see in the section that accurate determination of subadditivity and insensitivity regions is helpful when making decisions in real applications. In Sect. 6 we give a short summary of our main contributions with potential directions for future research, and in Appendix A we provide a user-friendly computer code for finding subadditivity and insensitivity regions. Throughout the paper, we use the acronyms and notation specified in Table 1, with their complete definitions, mathematical details, and references given later in the paper.",
61.0,4.0,Computational Economics,02 April 2022,https://link.springer.com/article/10.1007/s10614-022-10246-6,Resilient Control for Macroeconomic Models,April 2023,David Hudgins,Patrick M. Crowley,,Male,Male,Unknown,Male,"The triad of modern control systems properties includes resilience, robustness, and security (Huang et al., 2020). Resiliency, or in model terminology resilient control, refers to policy approaches aimed at restoring a system in response to unforeseen rare external incidentsFootnote 1 (or what some economists might refer to as “shocks”Footnote 2). If resiliency is the ability, ex-post, to rebound from the effects of an adverse incident, then robustness is the setting of macroeconomic policy, ex-ante, so as to encompass as many probable future scenarios as possible. One variation on robustness is minimax worst-case robust control which aims to formulate optimal policy that operates functionally under a given range of adverse disturbances. Neither resilience nor robustness are general properties of a system, however, since a system may be either resilient or robust to one set of disturbances, yet may be highly vulnerable to others (Mili, 2011). There is generally a tradeoff between the two control aspects of robustness and resilience. In order to achieve resilience, the policymakers use control policy resources to bring the system back into normal operation following the impact of a negative incident. Robustness, on the other hand, requires that policymakers utilize resources to obtain optimal performance in each specified operating state (Zhu & Basar, 2011).Footnote 3 Centralized systems are typically more robust but less resilient than decentralized systems, but may fail to respond to unexpected events, while decentralized systems are typically more resilient to unexpected incidents or attacks (Zhu & Basar, 2011). Lastly, security refers to the ability of the system to resist negative external events or malicious attacks (Huang et al., 2020). There are many examples of the tradeoff between resilience and robustness in macroeconomics, but here we just provide two. The severe economic downturn during the “great recession” of 2008–2009 was far outside the feasible predictions of common macroeconomic forecasting methods, such as DSGE models and the BVAR models (Lindé, 2018). Moreover, these models do not account for the asymmetry where the negative effects of recessionary gaps are far more damaging than the positive effects of unexpectedly strong growth. These two aspects demonstrate the need to seriously consider worst-case robust and resilient approaches when formulating policy. The economic decline in the second quarter of 2020 that resulted from the coronavirus lockdowns led to a projected 10% quarterly drop in real GDP, which represents the largest single-quarter drop since 1947. This decline is four times larger than the GDP decline in 1958 quarter 1, during the Eisenhower Recession, which previously was the worst quarter recorded.Footnote 4 This unprecedented 2020 economic decline represented an unexpected external incident that moved the economy away from equilibrium and beyond any forecasted projections. Since the economic system failed in terms of security and robustness, this incident demonstrates the need to examine resilience when formulating policy. The purpose of this paper is to develop a resilient control framework for obtaining optimal feedback fiscal and monetary policy responses under large negative external incidents. First, we construct a discrete-time soft-constrained LQ dynamic game under a worst-case design with multiple disturbances. This framework is especially applicable to large-scale macroeconomic tracking control models and wavelet-based control (WBC) models when the distribution of the disturbances is unknown, when there may be model misspecification, or when the policymakers are concerned with obtaining maximum disturbance robustness. Next, we simulate a WBC version of the model for the U.S. under the conditions that prevailed throughout the 2020 economic crisis. Within this context, we introduce a resilience feedback response and compare the case where policymakers counter against the external incident with the case when they do not. Under worst-case modeling, the policymaker is concerned about the effects on the economy if the reference model is misspecified and/or the probability distributions of the disturbances are unknown. Typically, robust models account for this by including additive disturbances that are determined through feedback from the entire state-space model (Leitemo & Soderstrom, 2008). Since the disturbances impact the model in the same way as multiplicative parameter uncertainty or omitted variables, robust control represents a general approach to model uncertainty (Hansen & Sargent, 2007; Leitemo & Soderstrom, 2008). The robust methods of Hansen and Sargent (2008) that use entropy to measure model misspecification in the time and frequency domains have been widely employed in macroeconomics; these methods, however, do not address the optimal tracking control problem. Minimax approaches to linear-quadratic (LQ) H∞-optimal control achieve robust designs by minimizing a performance index under the worst possible disturbances, where those disturbances maximize that same performance index. Although adaptive control and linear-quadratic Gaussian (LQG) models produce optimal policies when models are correctly specified, these polices can result in poor performance and instability when the model or disturbances are misspecified (Tornell, 2000). H∞-optimal control has been widely applied in engineering and economics, with many examples such as Basar and Bernhard (1991), Basar (1992), Hansen et al. (1999), Rustem and Howe (2002), Zakovic et al. (2007), Hansen and Sargent (2008), Hudgins and Chan (2008), Hudgins and Na (2016), and Hudgins and Crowley (2019). There are four limitations for using H∞-optimal control in economics. First, if the system is formulated incorrectly, then optimizing the wrong controller makes performance worse, instead of better. Secondly, many analyses model the interdependent prices and agent’s decisions separately, thus H∞-methods cannot be applied (Tornell, 2000). Thirdly, the computational solution can be difficult. Lastly, most of the macroeconomic policy forecasting models, such as Kendrick and Shoukry (2014), Taylor (1993), and the various central bank forecasting models are large-scale, with multiple equations and disturbances, where the policymaker’s preference for robustness varies across equations. Wavelet-based control (WBC) models, which contain variables that are wavelet-decomposed to account for cycles in the time–frequency domain, create additional equations and disturbances, thus further expanding the size of the model and the computational complexity (Crowley & Hudgins, 2015). But the ability to assign different disturbance weights across different targets, frequencies, and equations is particularly useful for policy modeling (Hudgins & Crowley, 2019; Leitemo & Soderstrom, 2008). In terms of the recent literature using these methods in macroeconomics, Hudgins and Na (2016) simulated a multiple-parameter design and a mixed stochastic/single parameter H∞-optimal control design for an aggregate model, and then Hudgins and Crowley (2019) was the first paper to integrate minimax robust optimal control policies and dynamic non-cooperative game theory with a WBC model. The optimal policy stance in robust control designs can be more cautious or more aggressive than in alternate model specifications (Bernhard, 2002). Hudgins and Na (2016) and Hudgins and Crowley (2019) find that fiscal policy is more aggressive under the minimax robust design. Onatski and Stock (2002) and Dennis et al. (2009) also find that robust monetary policy is relatively more active. These results contradict the less active monetary policy rules obtained in Zakovic et al. (2007) and Barlevy (2011), which uphold the Brainard principle (Brainard, 1967), which states that increased uncertainty decreases monetary and fiscal policy activism. Leitemo and Soderstrom (2008) partially reconcile the more aggressive versus more passive robust policies by finding that the type of external shock and source of misspecification determine whether monetary policy is more aggressive or more cautious. Resilient control is an ex-post incident concept that models the system’s ability to recover after negative events occur (Huang et al., 2020). Resilient control policies are designed to restore performance after robustness and security fail under unexpected adverse external incidents, so that the system can self-recover from the impact of the deterioration. Whereas the term “resilience” was originally studied in the early 1970s in the fields of ecology and psychology, resilience is now often used to describe the efforts of businesses, communities, and governments to enhance their ability to rapidly recover from crises such as natural disasters, terrorist incidents, or cyberattacks (Wei & Ji, 2010). Smith, Pereyda, and Gammel (2016) explain eight cybersecurity best practice for resilient control systems, where practices 4 – 7 are applicable to economic policy: have a contingency plan; patch, update, and maintain; don’t forget physical security; and learn from events. In terms of a contingency plan, the study points out that something will go wrong, so that it is not a matter of “if”, but rather “when”. Thus, a good control defender will always have a plan, or plans, to respond to a crisis. Wei and Ji (2010) give four properties of resilient industrial control systems: (1) the incidence of undesirable incidents can be minimized; (2) most of the undesirable incidents can be mitigated; (3) adverse impacts of undesirable incidents can be minimized; (4) ability to recover from the adverse impacts of undesirable incidents to normal operation in the shortest time. Yang and Sydnor (2012) stress the importance of viewing the resilient control system as a large coupled system designed by using multiple input-multiple output (MIMO) methods, such as optimal control, and H∞-control. That study emphasizes that robust control design improves the survivability of resilient control systems since it mitigates the adverse effects of modeling error and unpredicted disturbances under normal operating conditions, and thus reduces the chance of penetrating multiple defensive layers. Zhu and Basar (2011) develop a hybrid framework for a cyber-physical system that integrates a minimax worst-case continuous-time H∞-control system with a discrete-time stochastic uncertainty that models external incident shocks, or cyberattacks, that cause a change in the system operation at random times. In this framework, the defense against cyberattacks involves human decision-making, and thus involves a longer time-scale than the attacks. The dynamic-game model parameters yield a stationary saddle-point solution strategy where the defender is always defending, and the attacker should not be attacking. Our approach follows both Zhu and Basar (2011) and Yang and Sydnor (2012) by integrating resilient control within a robust optimal control model. To our knowledge, this paper is the first to explore resilient control within a macroeconomic context. Klimek, Poledna, and Thurner (2019) explore resilience in an econometric model impulse-response model, but not in an optimal control framework. Martini (2020) surveys the socio-economic literature on regional resilience, and explores the relationship between economic structure and regional resilience. Our procedure is designed for macroeconomic tracking models where the policymakers are interested in robustness to worst-case disturbances in the normal business cycle, and also achieving resilience to large negative incidents that might occur irregularly. This is therefore also the first paper to integrate wavelets and resilient control.",1
61.0,4.0,Computational Economics,08 April 2022,https://link.springer.com/article/10.1007/s10614-022-10250-w,Incentives for Research Effort: An Evolutionary Model of Publication Markets with Double-Blind and Open Review,April 2023,Mantas Radzvilas,Francesco De Pretis,Barbara Osimani,Male,Male,Female,Mix,,
61.0,4.0,Computational Economics,09 April 2022,https://link.springer.com/article/10.1007/s10614-022-10256-4,Personal Finance Decisions with Untruthful Advisors: An Agent-Based Model,April 2023,Loretta Mastroeni,Maurizio Naldi,Pierluigi Vellucci,Female,Male,Male,Mix,,
61.0,4.0,Computational Economics,10 April 2022,https://link.springer.com/article/10.1007/s10614-022-10249-3,Stock Price Formation: Precepts from a Multi-Agent Reinforcement Learning Model,April 2023,Johann Lussange,Stefano Vrizzi,Boris Gutkin,Male,Male,Male,Male,"Multi-agent systems (MAS) or agent-based models (ABM) have a long history as statistical tools in quantitative finance research. They are an active topic of research (Lipski and Kutner 2013; Barde 2015), especially to study price formation and general market microstructure. Key examples are the law of supply and demand (Benzaquen and Bouchaud 2018), game theory (Erev et al. 2014), order books (Huang et al. 2015), high-frequency trading (Way and Wellman 2013; Aloud 2014), cross-market structure (Xu et al. 2014), quantitative easing (Westerhoff 2008), market regulatory impact (Boero et al. 2015), or other exogenous effects (Gualdi et al. 2015) . Remarkably, over the years, financial MAS have captured specific patterns recurring virtually within any asset class and time scale: the so-called stylised facts. Stylised facts can be grouped in three main categories: i- non-gaussianity of price returns, ii- time decay of price auto-correlations and iii- volatility and volume clustering. More specifically, non-gaussian distribution of price returns are asymmetric, negatively skewed, and platykurtic  (Cristelli 2014; Cont 2001; Potters and Bouchaud 2001); secondly, time decay of price auto-correlations portrays a loss of arbitrage opportunity  (Cont 2001, 2005); thirdly, volatility and volume clustering means that large (small) jumps in prices and volumes are more likely to be followed by large (small) jumps  (Lipski and Kutner 2013). In particular, the latter stylised fact has long-range implications on the dynamics of meta-orders (i. e. slicing large trading orders into smaller ones so as to not unfavourably disrupt the market price by law of supply), especially wrt. the square-root impact law (Bouchaud et al. 2018) (growth in square-root of orders impact with traded volumes). This discussion over stylised facts is important to study economic microstructure, because they relate to the role of market memory (Cont 2005; Cristelli 2014), namely how past economic information is relevant to forecast future prices . If we consider the efficient-market hypothesis (Fama 1970; Bera et al. 2015), where market memory is constrained under certain strict assumptions, stylised facts may characterise the whole game-theoretic aspect of financial markets (Erev et al. 2014; Lux and Marchesi 1999), as recurring patterns can be discovered and analysed. In the past, the conceptual challenge of framing realistic agents surrounded the scientific pertinence of MAS with skepticism, especially in finance, with human agents(Gao et al. 2012). The strongest critic concerned the use of so-called zero-intelligence agents by previous generations of financial MAS (Gode and Sunder 1993). Agents were precast with specific trading rules dictating the way they trade, thereby overshadowing dynamics proper to behavioural game theory and decision theory underpinning real market activity. Calibration of the model was performed so as to capture the stylised facts (Preis et al. 2006; Chiarella et al. 2009; Leal et al. 2016). Moreover, this MAS design often needed to include a fraction of noise traders (Hanson 2011; Bartolozzi 2010; Preis et al. 2006; Farmer et al. 2005; Maslov 2000; Challet and Stinchcombe 2003) within the trading population to emulate price formation and basic business activity (LeBaron 2002; Xu et al. 2014; Sornette 2014), namely traders with random trading activity. Furthermore, the information processed by each agent for price forecasting was often in common to all other agents in the form of a board of technical indicators (da Costa Pereira et al. 2009). In contrast, in a more realistic emulation of price formation, each agent would source its own information autonomously (Grossman and Stiglitz 1980). The other two main well-known classes of models studying market dynamics in finance and economics are econometric models (Greene 2017) and Dynamic Stochastic General Equilibrium (DSGE) models. Both types of models have shown a variety of promising results, but they are meant to describe reality only coarsly (Farmer and Foley 2009), averaging over the heterogeneity of price microstructure. MAS therefore still show two major advantages over these models: i- as complex systems, MAS naturally display specific emergent phenomena (Bouchaud et al. 2019), and ii- they require fewer model assumptions (no gaussian distributions, no efficient market hypothesis (Fama 1970; Bera et al. 2015), etc.). As for their challenges, we can mention two specific conceptual limitations: i- modelling complex system heterogeneity (Chen et al. 2017) (e. g. individual investors, institutional portfolio managers), ii- discretionary framing of certain model parameters apart from possible empirical intuition (Platt and Gebbie 2018) (e.g. number of agents, time steps of each simulation run, size of agents’ portfolios...). This conceptual and methodological criticism has been challenged in the past few years by the notable progress of two fields : machine learning and cognitive science. In machine learning, multi-agent reinforcement learning (Silver et al. 2018) (RL) has yielded remarkable results with far-reaching applications in domains closely related to quantitative finance, such as decision theory and game theory (Lanctot et al. 2017). In cognitive science, behavioural decision-making research has benefited from important advances in behavioural economics and, more recently, neuroeconomics (Eickhoff et al. 2018), shading new light onto the cognitive mechanisms underlying financial phenomena (Frydman and Camerer 2016). As machine learning can imbed results from cognitive science (Lefebvre et al. 2017; Palminteri et al. 2015) and vice-versa (Duncan et al. 2018; Momennejad et al. 2017) , the interplay of these two fields becomes of special interest and relevance to financial MAS research. Moreover, there are further machine learning applications to finance (Ganesh et al. 2019; Hu and Lin 2019; Neuneier 1997; Deng et al. 2017) and order book models have become an active area of research (Spooner et al. 2018; Biondo et al. 2019; Sirignano and Cont 2019). The integration of all these advancements enable us to design a new generation of MAS stock market simulators  (Lussange et al. 2020) that outstrip former MAS, adding a whole new degree of realism in market microstructure emulation. More importantly, these novel MAS can quantitatively address crucial points of interest, such as agent information and learning, which are central to price formation (Dodonova and Khoroshilov 2018; Naik et al. 2018) and hence all market activity. We designed a MAS stock market simulator , named SYMBA (SYstème Multiagent Boursier Artificiel), where agents autonomously learn to perform price forecasting and stock trading by reinforcement learning, via a centralised double-auction limit order book. The primary purpose of SYMBA is to gain a deeper bottom-up understanding of the individual contributions of traders to the collective formation of the financial market. For this reason, its architecture differs from other types of financial MAS (Lee et al. 2007) whose purpose is purely trading performance enhancement. It also differs from other financial MAS studying the role of individual and social learning in the co-evolution of trading strategies (Kendall and Su 2003), which employ artificial neural networks. Moreover, it differs from other financial MAS aiming to replicate trading heterogeneity (Mota Navarro and Larralde 2017), as all their agents employ the same econometric forecasting tool. A full technical description of our model implementation is outlined in our previous work (Lussange et al. 2020), which includes, crucially, parameter selection and calibration performances with respect to real market data coming from 642 stocks from the London Stock Exchange daily quotes between 2007 and 2018. Because of its importance in this current work, we first present a general overview of reinforcement learning theory, in Sect. 2. Then, in Sect. 3, we summarise SYMBA’s general architecture and we report previous key results as supplementary material in Fig. 9, 10, 11 and 12. Among the large number of reinforcement learning algorithms available, as discussed in our previous work, we based our model on direct policy search, whose advantages are highlighted in Sect. 4 of the current paper. The two general aspects of greatest interest to our financial MAS are the impact of agent learning on price formation and multi-agent reinforcement learning. The former is motivated by the fact that price formation underpins all market activity; the latter by the important parallels between reinforcement learning and decision processes in neural circuitries (Dayan and Daw 2008) ,  (Lee et al. 2012) . Our current results in Sect. 4 therefore focus on two main aims: characterising agent learning and measuring policy heterogeneity among agents. Finally, in Sect. 5, we probe the trading characteristics of successful agents. As said, the greatest interest of financial MAS is to study and quantitatively gauge the impact of agent learning, which is at the heart of the price formation processes and hence of all market activity, and the motivation for multi-agent reinforcement learning for such a framework is motivated by the important parallels between reinforcement learning and decision processes in the brain (Dayan and Daw 2008).",
61.0,4.0,Computational Economics,13 April 2022,https://link.springer.com/article/10.1007/s10614-022-10258-2,Valuation of Standard Call Options Using the Euler–Maruyama Method with Strong Approximation,April 2023,Daniel Suescún-Díaz,Luis Eduardo Girón,,Male,Male,Unknown,Male,"Stochastic numerical methods are very useful techniques that allow the process of solving stochastic differential equations to be approached when they do not have a known exact analytical solution. These approximations can be made taking into account two approaches: Strong approaches and weak approaches. Stochastic numerical methods can be used to value financial options, i.e., financial instruments that grant the holder a right to buy or sell an asset at a specified price during a specified period or date. The option can be American if it can be exercised at any time within the contract period, or European if the option can be exercised only at the expiration of the contract. The exercise price or strike price of an underlying asset is the fixed price at which the option holder can buy -in the case of a call option- or sell -in the case of a put option-. The last day the option can be exercised is called the expiration date. The factors that influence the premium value of an option can be exogenous and endogenous. Exogenous factors are those determined by the market: The price and volatility of the underlying asset, the risk-free interest rate and the dividends generated by the underlying asset throughout the contract, while endogenous factors such as the duration of the option contract and the exercise or strike price are incorporated into the contract. The original Black–Scholes model (B–S) (1973) assumes ideal conditions for the asset market, from which they develop the formulas to value the theoretical price of a call option and a European put option when the stock does not pay dividends. Although the B–S model and its vanilla option pricing are formulated a single asset, it is possible to develop options where more than one underlying asset is involved. The B–S model presents two assumptions, the first, considers that the volatility of the underlying asset is constant, and the second, that the distribution of the returns on the assets is normal. However, multiple empirical investigations such as Bakshi, et al. (1997); Das and Sundaram (1999); Dumas, et al. (1998), where stochastic volatility and non-normal distributions of returns are considered, show little improvement compared to the B-S model. The main challenge in the options market is how to price an option fairly, in such a way that arbitrage can be avoided. Currently, various standard option pricing methods have been developed, such as the implicit finite difference method applying the Crank–Nicholson scheme to solve the partial differential equation of the B–S method with variable volatility obtained from a GARCH model (1, 1) Rana and Ahmad (2011), a method based on finite differences to solve partial integro-differential equations that describe the behavior of the option pricing under diffusion models with jumps, where it is assumed that jump sizes have a double exponential distribution Kwon and Lee (2011), the Fast Fourier transform (FFT) method for option pricing when the underlying asset follows the double exponential jump process with stochastic volatility and stochastic interest rate Zhang and Wang (2013), the FFT method for option pricing when the asset follows the double exponential jump process with stochastic volatility and stochastic intensity Huang et al.(2014), the Martingale method and fuzzy set theory for option pricing Nowak and Romaniuk (2014), the method that consists of the solution of a complex partial differential equation, which considers transaction costs and stochastic volatility Mariani, et al.(2015), the method that improves the compact scheme of higher order finite differences for option pricing in non-affine stochastic volatility models Shi, et al. (2016), the method that is based on the general equilibrium framework taking into account stochastic volatility, establishing a formula obtained by using the Fourier transform method Shi, et al. (2017), the method of change of regimes or states for option pricing Zhu, et al. (2012). Another method for the valuation of put options in the electricity market is proposed in Sheybani and Buygi (2017). A new multifactorial vulnerable european option pricing with three stochastic factors model (TSFM), spot price of the underlying asset that follows a geometric Brownian motion process, yield spreads, and the default-free unit discount bond that is modeled using a Ornstein–Uhlenbeck stochastic process with mean reversion Chaoqun, et al. (2019), constitutes a general model where the B–S model, the Hull and White model (H–W) (1995) and the Merton model (Merton, 1973) are considered particular cases. In this work, we use the Euler–Maruyama stochastic method with strong approximation to value an european call option of different volatility types and an european exchange option based on the prices of two underlying assets. The results are compared with those obtained by the B–S formula and the weak Euler–Maruyama method, additionally, they are also compared with some methods reported in the option valuation literature (Chaoqun et al., 2019; Hull & White, 1995; Merton, 1973). This document is organized as follows: the next section presents the theoretical considerations that support the formulas used to assess the different options considered in this work, subsequently, the proposed Euler–Maruyama stochastic method with strong approximation is developed. Finally, the most outstanding results and conclusions are presented.",2
61.0,4.0,Computational Economics,15 April 2022,https://link.springer.com/article/10.1007/s10614-022-10251-9,Analytic Method for Pricing Vulnerable External Barrier Options,April 2023,Donghyun Kim,Ji-Hun Yoon,,Unknown,Male,Unknown,Male,"Ever since the global financial crisis hit in 2007–2008, there have been growing concerns regarding the credit default risks of financial instruments in over-the-counter (OTC) markets. As there is no organized exchange to guarantee the promised payment, it is more likely that the default risk from the counterparties has put many investors at risk. In other words, in OTC markets, the unorganized stock exchange always makes the option holder bear counterparty credit risks because the option writer of the counterparty may not pay back the promised amounts before the expiration date. Because of the possibility of default, these options, which are subject to default risk, are called vulnerable options. After the global financial crisis, option pricing under credit default risks has been widely researched; numerous studies have been conducted on these vulnerable options. The vulnerable option pricing was first introduced by Johnson and Stulz (1987). Klein (1996) presented an analytic pricing formula by assuming that the option writer will have other liabilities in the capital structure and by considering the correlation between the option’s underlying asset and the default risk of the counterparty. Meanwhile, the options did not get any significant attention until the financial crisis brought on by the sub-prime mortgage crisis in the United States. However, the papers published before the global financial crisis provide the theoretical background necessary to conducting research on vulnerable option prices and give lots of readers who read the related researches many motivations to the pricing of the diverse vulnerable options under various financial models. In fact, many financial engineers or financial mathematicians have started focusing on studies on vulnerable options with keen interest again. For example, Hung and Liu (2005) and Yang et al. (2014) investigated the pricing of vulnerable options in an incomplete market, and Choi et al. (2019) and Jeon et al. (2016, 2017) examined the pricing of exotic options on assets with default risk. An external barrier option comprises two stochastic variables: an underlying asset and the state variable of the barrier. The option’s payoff is determined by the underlying asset, whereas the barrier level of the option is directly affected by the state variable. There are diverse types of state variables as stochastic processes, such as the stock price, interest rate, exchange rate, and stochastic volatility. A European-style single external barrier option was obtained by Heynen and Kat (2015). Kwok et al. (1998) derived analytic formulas for European options under one or multiple assets with external barriers, and Jeon and Yoon (2016a, 2016b) derived the pricing of external-chained barrier options with an external single or double barrier option using the probabilistic method. Carr (1995) used two extensions of the European-style up-and-out single-external-barrier call option to derive an analytical option-pricing formula. Kim et al. (2015) exploited Laplace transforms for pricing the external single or double barrier option under a regime-switching model with finite regimes, whereas Kim et al. (2021a) addressed the pricing of the external barrier option using the stochastic volatility model. Recently, Golbabai et al. (2019) studied the numerical analysis of the time fractional Black-Scholes model, and Golbabai and Nikan (2020) priced double barrier options in this model. In this study, we investigate the explicit closed solution for vulnerable external-barrier-option prices by using Mellin transform techniques and the method of images. For vulnerable options, it is difficult to find or analyze the solution of the prices because both the underlying asset and the market value of the option’s writer (or the counterparty) have to be considered simultaneously. In fact, Klein (1996) derived the explicit closed formula for the prices of vulnerable options using probabilistic techniques. However, the derivation of the closed formula using probabilistic methods is complex and presents mathematical challenges because it is necessary to determine the multivariate joint probability density functions for the case in which the dimension of the underlying assets is greater than two. In fact, as seen in Choi et al. (2021) and Yang et al. (2014), in the real market, extraordinary volatility behavior from the financial crisis and high volatility of a market could be closely related to not only default risk of option contracts but also the stochastic volatility (SV) or stochastic elasticity of variance (SEV) models. In addition, we were required to extend the standard vulnerable option to other types of options with the default risk to price diverse financial derivatives in the financial market. However, in case of the extended model or other types of options, deriving the solution of the vulnerable options using the probabilistic methods becomes extremely challenging compared with the standard vulnerable option. To resolve these problems, Yoon and Kim (2015) used partial differential equation (PDE) approaches and Mellin transform methods to obtain the explicit closed solution for the vulnerable option price. They verified that the analytic method based on Mellin transforms may be advantageous as it simplifies the problem significantly. In other words, the Mellin transform allows us to resolve the complexities in the calculation of the option pricing with probabilistic methods. Since then, there have been numerous studies on the vulnerable options under the extended models that reflect the real situation of the financial market and other types of vulnerable options, using the PDE methods (for example, Choi et al. (2019, 2021), Jeon and Yoon (2016a, 2016b), Jeon et al. (2017, 2018),  Kim and Koo (2016), Kim et al. (2021b)). The Mellin transform, which is an integral transform, is the multiplicative version of the two-sided Laplace transform. Paniniand Srivastav (2004, 2005) obtained the pricing formula of European and American vanilla, basket, and perpetual American options using the Mellin transform. Yoon and Kim (2015) found European vulnerable options under stochastic (Hull–White) interest rates, as well as under a constant interest rate, by implementing the double Mellin transform. Yoon (2014) took advantage of the Mellin transform to derive a closed-form formula for European options under a Black–Scholes framework with a stochastic interest rate. Moreover, Jeon et al. (2017) examined explicit-form formulas for vulnerable path-dependent options by utilizing double Mellin transforms, and Kim and Koo (2016) studied the valuation of exchange options with credit default risk by exploiting Mellin transform approaches. Meanwhile, the method of images is a crucial tool for solving the problem of the closed solution for the vulnerable external barrier option. The method of images closely pertains to the reflection principle of the expectation solution. The aforementioned Mellin transforms can only be applied to functions comprising ranges or domains. However, in the case of barrier options, the boundary condition exists at only one point. Therefore, we cannot apply the Mellin transform directly to the given PDEs. As demonstrated by Jeon et al. (2017), the use of the method of images on the given PDEs enables us to obtain the pricing formula of path-dependent options, including barrier options, more easily than the previous cases wherein the probabilistic method is used. Using the method of images, we can transform the PDE of the path-dependent options (for example, the barrier or lookback option) with two conditions (boundary and final conditions) into the PDE with one final condition in the total range of the underlying assets; then, we can derive the pricing formula of the options using Mellin transform approaches. Buchen (2001) used this method for the first time in option pricing, and Buchen and Konstandatos (2009) investigated double knock-out barrier-option prices under the risk-neutral measure using the same method. Moreover, the method of images for options has been applied to other types of financial problems by Choi et al. (2019), Jeon and Yoon (2016a, 2016b), Jeon et al. (2017, 2018) and  Kim et al. (2021a). The derivation of the explicit(closed)-form solution of the vulnerable external barrier option prices is the main contribution of our study in the economic perspective. The main reasons we should find the closed-form solutions in valuing the options in the financial market are as follows: First, deriving the explicit-form solutions enables us to obtain the stable results for the option values compared with other probabilistic numerical methods (such as the binomial tree method, trinomial tree method or Monte-Carlo simulation) or the Crank–Nicolson (CN) method. Second, the probabilistic approaches or the numerical method from the CN method require significant amount of time to evaluate the option prices in comparison with the closed-form solution. Even in the option’s calibration, when we conduct the option’s data fitting to evaluate the option’s implied volatility, if the solution of the option prices is unstable, then the price unstability will have a negative effect on the option calibration because of the irregular pricing errors. Thus, if we utilize the PDE methods and then derive the explicit-form solution for the option prices using mathematical techniques (multiple Mellin transform and the method of images), we can resolve the price unstability and the calculation time in pricing the options or in implementing the option’s data fitting. Therefore, the derivation or existence of the closed-form solutions for those in financial derivatives in the financial market is very crucial. The related and similar topics with the research are given by Bazhanov (2012), Choi et al. (2021) and Yoon and Kim (2021). In particular, in this article, while deriving the explicit-closed-form solution for the vulnerable external-barrier option, we consider three-dimensional-model dynamics extended by one more dimension, in contrast to the previous studies. To solve the PDE problems for the option price induced from the underlying asset model, most of which use the method of images, we extend the PDEs for the restricted domain of the barrier level to the related PDEs in the unrestricted region. Next, using the triple Mellin transform method on the PDE, we obtain the desired explicit solution for the vulnerable external barrier options, as expressed by the multivariate normal cumulative distribution functions (CDFs). It should be noted that the formulas of the explicit form for the option pricing using the triple Mellin transforms have been derived for the first time in our study. In other words, no reserach has been introduced using more than 3-dimensional Mellin transform approaches yet. Truly, from completing this problem, it is possible for the triple(3-dimensional) Mellin transform to be extended to the multi-dimensional Mellin trasform, and then we can obtain the closed-form solutions of the option prices under the multi-dimensional model dynamics (more than 4-dimentional model dynamics), which can be applied to the option pricing models with more realistic perspectives in the financial market or the problems of the portfolio optimization selection which are closely related to Asset Allocation Theory. The remainder of this paper is organized as follows. Section 2 presents the model dynamics for pricing the external barrier options on underlying assets with default risk and presents PDEs with regard to the option value. Section 3 presents the use of the triple Mellin transform and method of images to solve the PDEs of the vulnerable external barrier options. Section 4 presents an evaluation of the accuracy and efficiency of the explicit closed solution of the option price by comparing our solution with that of the Monte Carlo simulation and an investigation of the influence of model parameters on vulnerable external barrier options. Finally, in Sect. 5, we present our conclusions.",
61.0,4.0,Computational Economics,15 April 2022,https://link.springer.com/article/10.1007/s10614-022-10254-6,Multi–Scale Risk Connectedness Between Economic Policy Uncertainty of China and Global Oil Prices in Time–Frequency Domains,April 2023,Sheng Cheng,Wei Liu,Yan Cao,,,Male,Mix,,
61.0,4.0,Computational Economics,21 April 2022,https://link.springer.com/article/10.1007/s10614-022-10262-6,Bitcoin Price Prediction: A Machine Learning Sample Dimension Approach,April 2023,Sumit Ranjan,Parthajit Kayal,Malvika Saraf,Male,Unknown,Unknown,Male,"Bitcoin was introduced in 2008 by an unknown group of individuals using the name Satoshi Nakamoto and officially came into play in 2009 when its ASCII (American Standard Code for Information Interchange) text file was released as an open-source network. It was fabricated to solve the innate weakness of trustworthy transaction models and was initially elucidated as merely a peer-to-peer digital cash system. Today, it is the most valued cryptocurrency, trading in over 40 exchanges worldwide and undertaking 30 different currencies. As a currency, Bitcoin provides a novel opportunity for accurate price prediction due to its high volatility and relatively young age (McNally et al., 2018). Investors consider Bitcoin to be a kind of speculative investment much like web stocks. Several years of trading and increasing popularity of Bitcoin grabbed attention from across society, especially policymakers, and soon Bitcoin’s capitalisation touched its peak at 300 billion US dollars in 2017, almost resembling that of Amazon in 2016 (Yermack, 2015). The University of Cambridge 2017 research estimates show that there have been 2.9 to 5.8 million discrete users employing a cryptocurrency wallet, most of them using bitcoin. The value of Bitcoin reflects investors’ confidence in cryptocurrency (Mai et al., 2018). Bitcoin price fluctuations, due to its idiosyncratic volatility, have plagued investors since it began trading. It is quintessential to find ways to accurately forecast Bitcoin price changes. Previous studies on Bitcoin have predicted its price using two approaches: empirical analysis and analysis of machine learning algorithms. Although existing work has best-utilized machine learning methods for better and precise Bitcoin price prediction, only a few have focused on the feasibility of applying different modelling techniques to samples with varying dimensional features (Chen et al., 2020). This paper aims to predict Bitcoin price by using daily data and accessible high-frequency data, similar to stock market prediction (Madan et al., 2015). Since seasonality is absent in Bitcoin, machine learning models can be highly effective and useful in its price prediction. However, the selection of appropriate machine learning algorithms depends on the frequency and structure of the data, or else the complexity of such models could lead to overfitting. Previous studies have implemented well-known machine learning algorithms such as recurrent neural network (RNN), long short-term memory (LSTM), support vector machines (SVM), and random forest models. But they simply input data into models without differentiating between data frequency and sample size. Most research in this field concentrates only on accuracy when using machine learning algorithms while ignoring the sample dimension (Chen et al., 2020; McNally et al., 2018). This study strives to leverage suitable machine learning algorithms to sample dimensions for predicting Bitcoin prices. Motivated by Occam's razor theory (Gamberger & Lavrač, 1997) and making use of our daily and accessible high-frequency datasets, this paper carries out the following methodology. Firstly, we utilize two datasets, one with daily intervals and the other with 5-min intervals. Going forward, we perform feature engineerings such as high-dimension features for low-frequency data to steer clear of overfitting (Wind, 2014) and low-dimension features for high-frequency data (Rechenthin, 2014). Subsequently, we apply conventional statistical models like Logistic Regression, Linear Discriminant Analysis along with machine learning models (Random Forest, Support Vector Machine, XGBoost, Decision Tree, Quadratic Discriminant Analysis, and k-Nearest Neighbors) for comparing results. This paper proceeds as follows. We look at the literature review in Sect. 2, where we compare our subject matter with earlier studies. We elaborate on the data and methodology in Sect. 3, where we talk about the problem statement, source of data and feature engineering. In Sect. 4, we discuss the machine learning algorithms and appropriate evaluation indicators, and subsequently, in Sect. 5 exhibit our empirical results. Section 6 concludes the paper.",4
61.0,4.0,Computational Economics,22 April 2022,https://link.springer.com/article/10.1007/s10614-022-10255-5,Weighted-Average Least Squares (WALS): Confidence and Prediction Intervals,April 2023,Giuseppe De Luca,Jan R. Magnus,Franco Peracchi,Male,Male,Male,Male,"Many empirical studies in economics assume that the data are generated by a linear regression model where a distinction is made between ‘focus regressors’ and ‘auxiliary regressors’. The focus regressors are included because we believe the model is not credible without them or because they are the subject of our investigation, while the number and the identity of the auxiliary regressors is less certain. The parameters of primary interest are the coefficients on the focus regressors (the ‘focus parameters’), while the coefficients on the auxiliary regressors are treated as nuisance parameters. Instead of a single model for the data generating process (DGP), there is a ‘model space’ containing a finite but potentially large number of models, namely the unrestricted model that includes all auxiliary regressors, the fully restricted model that includes none, and all intermediate models. Adding auxiliary regressors tends to reduce omitted variable bias in estimating the focus parameters, but tends to increase sampling variability. Examples include studies concerning the determinants of economic growth (Sala-i-Martin et al. 2004; Magnus et al. 2010), risk premia (Sousa and Sousa 2019), product and labor market reforms (Duval et al. 2021), the impact of legalized abortion on crime (Donohue and Levitt 2001), and the relationship between body mass and income (Dardanoni et al. 2011). Model uncertainty can be approached via ‘model selection’ or via ‘model averaging’. In the model selection approach we attempt to find the ‘best’ model given the data, the model space, and a specific purpose (e.g., estimation of particular parameters or prediction of future outcomes). Given this best model, one then employs its estimates for the intended purpose. Like any other data-driven statistical decision, model selection is subject to sampling uncertainty which, if ignored, can lead to overestimate accuracy (Kabaila and Mainzer 2018). Typical examples are the classical pre-test estimator and post-selection estimators that select the model with the smallest value of some information criterion, such as the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC). More recently, considerable attention has been devoted to penalization estimators based on model sparsity and an absolute penalty criterion, such as the least absolute shrinkage and selection operator (LASSO), which address the sampling uncertainty problem by performing variable selection and regularization at the same time. These estimators typically require the choice of some ‘tuning’ parameters that control the trade-off between bias and variance. They also tend to be biased and to have nonstandard sampling distributions, so that inference based on the normal approximation can be misleading (Knight and Fu 2000; Claeskens and Hjort 2008). The second approach is model averaging. In contrast to model selection, one is not concerned with finding a ‘best’ model but with finding a ‘best’ estimator of the focus parameters or a ‘best’ predictor of the outcome. The (well-established) terminology is a little confusing because we don’t average over models but over estimators. In fact, one takes a weighted average of the estimators from all the available models, with data-dependent weights to account for the uncertainty associated with each model. There are many proposed model averaging estimators, typically obtained either from a Bayesian perspective (Bayesian model averaging: BMA) or from a frequentist perspective (frequentist model averaging: FMA). BMA weights can be interpreted as posterior model probabilities, while FMA weights are decreasing functions of some measure of predictive inaccuracy, such as Mallows’ \(C_p\) (Hansen 2007) or leave-one-out cross-validation (Hansen and Racine 2012). There also exist Bayesian-frequentist ‘fusions’, such as weighted-average least squares (WALS), introduced by Magnus et al. (2010), which is frequentist but with a Bayesian flavor. We refer to Steel (2020) for an extensive survey of the various types of model averaging estimators and their use in economics. Like for model selection estimators, most of these estimators tend to be biased and their sampling distribution is not well approximated by the normal distribution. Furthermore, there is increasing evidence that, even after correcting for bias, inference for model averaging estimators can be misleading if based on the normal approximation (see, among others, Claeskens and Hjort 2008; Hansen 2014; Liu 2015; and DiTraglia 2016). The finite-sample bias and variance of WALS have recently been analyzed by De Luca et al. (2021), who exploit results on the frequentist properties of the Bayesian posterior mean in a normal location model. The current paper extends their results to inference by proposing a simulation-based approach that yields re-centered confidence and prediction intervals using the bias-corrected posterior mean as a frequentist estimator of the normal location parameter. We assess its finite-sample performance by an extensive Monte Carlo experiment. To facilitate comparisons with the simulation study by Zhang and Liu (2019), we stay close to their framework and consider a finite model space that contains the true data-generating process (M-closed environment) but has little additional structure. Unlike Zhang and Liu (2019), who restrict attention to inference about a single auxiliary parameter, we consider inference about a single focus parameter, interpreted as the causal effect of a policy or intervention in the presence of a potentially large number of auxiliary parameters. This is likely to be the most interesting case for applied economists. We compare the performance of WALS point estimates and confidence intervals with the performance of several competing approaches, including least squares estimators for the unrestricted and fully restricted models, post-selection estimators based on AIC and BIC, Mallows and jackknife model averaging estimators, and one version of the LASSO (the adaptive LASSO). In addition, we discuss prediction intervals for the outcome of interest, which involves linear combinations of all focus and auxiliary parameters. The main conclusion of our Monte Carlo experiment is that, compared to other estimators, the coverage errors for WALS are small and confidence and prediction intervals are short, centered correctly, and allow for asymmetry. They are also easy and fast to compute. The remainder of this paper is organized as follows. Section 2 introduces the framework and briefly describes the estimators that we consider. Section 3 discusses how to construct confidence intervals for a single parameter of interest. Section 4 describes the Monte Carlo experiment. Sections 5–7 contain the simulation results, separately for point estimates (Sect. 5), confidence intervals (Sect. 6), and prediction intervals (Sect. 7). Section 8 concludes. There are two appendices. Appendix A formalizes the nine estimators introduced in Sect. 2, while Appendix B describes the algorithm for simulation-based WALS confidence intervals.",1
61.0,4.0,Computational Economics,24 April 2022,https://link.springer.com/article/10.1007/s10614-022-10261-7,A New Neural Network Approach for Predicting the Volatility of Stock Market,April 2023,Eunho Koo,Geonwoo Kim,,Unknown,Unknown,Unknown,Unknown,,
61.0,4.0,Computational Economics,25 April 2022,https://link.springer.com/article/10.1007/s10614-022-10263-5,Derivation and Application of Some Fractional Black–Scholes Equations Driven by Fractional G-Brownian Motion,April 2023,Changhong Guo,Shaomei Fang,Yong He,Unknown,Unknown,,Mix,,
61.0,4.0,Computational Economics,03 May 2022,https://link.springer.com/article/10.1007/s10614-022-10259-1,Auctions: A New Method for Selling Objects with Bimodal Density Functions,April 2023,Javier Castro,Rosa Espínola,Daniel Gómez,,Female,Male,Mix,,
61.0,4.0,Computational Economics,06 May 2022,https://link.springer.com/article/10.1007/s10614-022-10264-4,A New Stabled Relaxation Method for Pricing European Options Under the Time-Fractional Vasicek Model,April 2023,Mohamed Kharrat,Hassen Arfaoui,,Male,Unknown,Unknown,Male,"Pricing derivatives and especially options is one of the most popular problems in mathematical financial literature. For instance, European options are very popular in the worldwide financial markets. Over the last few decades, several papers investigated the problem of pricing options generated by different models using many methods for instance (Black & Scholes, 1973; Bensoussan, 1984; Heston, 1993; Kharrat, 2014). The most famous are the Black and Scholes model (Black & Scholes, 1973) and Heston model (Heston, 1993), which the first one rests upon the concept that the stock price of the underlying asset is log-normally distributed conditional on the current stock price with constant volatility. As compared to the case of the Black and Scholes model, where the volatility is constant, the Heston model (Heston, 1993) is more important since the volatility is stochastic, as the dynamics of the volatility is fundamental to elaborate strategies for hedging and arbitrage, a model based on constant volatility cannot explain the reality of the financial markets. But for the case where the interest rate is stochastic we are forced to use the Vasicek Model (Vasicek, 1977). So, pricing option under stochastic model is then more important and required. In the following we introduce the standard Vasiček model. Let \(S_{t}\) be the asset price generated by the following dynamic: and \(r_{t}\) be the interest rate process which follows the following process: where the volatility \(\sigma \) supposed to be constant, \( W_{t}^{S} \) and \(W_{t}^{r}\) are two correlated Brownian motion i.e. \(W_{t}^{S}=\sqrt{1-\rho ^{2}}B_{t}^{1}+B_{t}^{2}\) and \(W_{t}^{r}=B_{t}^{2}\) where B is Standard 2- dimensional Brownian motion and \( \rho \in \left] -1,1\right[ \) and the parameters \(\frac{-\mu _{1}}{\mu _{2}}\), \(-\mu _{2}\) and \(\eta \) represent respectively the long-term mean level, the speed of the reversion, and the volatility of the interest rate \(r_{t}\). Let \(\vartheta \) the price of European option, using the standard hedging and the application of Ito’s lemma we get: The fractional calculus is invested in several fields (Amit et al., 2019; Benchohra et al., 2011; Daftardar-Gejji & Bhalekar, 2008; Dumitru et al., 2020; Podlubny, 1999; Srivastava et al., 2020) and (Yu et al., 2011). For example, fractional derivation models have shown an ability to describe shape-memory materials better than full derivation models. When a material is purely elastic, it is described by an integer derivation of order zero while when it is purely viscous it is described by an integer derivation of order one. Immediately, we can describe a viscous-elastic material by a derivation between 0 and 1. This justifies the use of fractional derivation for this kind of material. So out of mathematical curiosity and to get closer to the reality of the financial market we find ourselves obliged to use models based on fractional derivatives. Recently, it has been integrated in the Mathematical finance field (Yu et al., 2011; Xiaozhong et al., 2016; Kharrat, 2021) especially designed to resolve the pricing option problem. For instance (Kharrat, 2018; Zhang et al., 2016) which are devoted for the evaluation of the European option. From this perspective, using the splitting method, we present a new resolution for the pricing European option under the fractional Vasicek model. The aforesaid method allowing to solve a mixed problem Parabolic/Hyperbolic by decoupling the parabolic and hyperbolic operators, (for more details see Arfaoui, 2020). A nonlinear mixed problem generated by two completely different operators, (Parabolic/Hyperbolic), can cause difficulties in the numerical simulations. During discretization, the splitting method makes it possible to treat each operator Parabolic and Hyperbolic by an adequate numerical scheme. This method preserves the numerical properties (stability, consistency, \(\cdots \)) of each scheme used for each operator. This new method allowed us to give relevant numerical results besides we found in the literature that the coefficient of correlation it’s always between \(-\,0.7\) and 0.7. With our new numerical method, we can extend the aforesaid coefficient between \(-\,0.9\) and 0.9. In the following definition, we present the Caputo time-fractional derivative. (Kilbas et al., 2006). The Caputo time fractional derivative of order \(\gamma >0\), \(a, \gamma , t \in {\mathbb {R}}\) can be defined as follows: When \(m=1:\) \(0<\gamma <1\), then the Caputo fractional derivative of order \(\gamma \) of the function f reduces to where \(\Gamma (\cdot )\) is the Gamma function given by This definition of fractional derivative is interesting, among other reasons, because it holds properties of the non-fractional derivatives as to make nullthe derivative of a constant (see Podlubny, 1999). In our work, we will use the definition when \(m=1\). (Erdelyi et al., 1981) The Mittag-Leffler function of one parameter is defined as: where \(\Gamma (\cdot )\) is the gamma function. (Erdelyi et al., 1981) The Mittag-Leffler function of two parameters is defined as: where \(\Gamma (\cdot )\) is the gamma function. The outline of this work is as follows. In Sect. 2, we introduce the time fractional Vasicek model. The Splitting method and the discretization of the Model are derived respectively in Sects. 3 and  4. In Sect. 5, we present the numerical analysis and discuss the stability of the solution. In Sect. 6, we present some numerical results.",2
61.0,4.0,Computational Economics,06 May 2022,https://link.springer.com/article/10.1007/s10614-022-10266-2,Investigating the Asymmetric Behavior of Oil Price Volatility Using Support Vector Regression,April 2023,Yushu Li,Hyunjoo Kim Karlsson,,Unknown,Unknown,Unknown,Unknown,,
62.0,1.0,Computational Economics,14 May 2022,https://link.springer.com/article/10.1007/s10614-022-10268-0,"A Multi-market Comparison of the Intraday Lead–Lag Relations Among Stock Index-Based Spot, Futures and Options",June 2023,Fei Ren,Mei-Ling Cai,Zhang-HangJian Chen,,,Unknown,Mix,,
62.0,1.0,Computational Economics,19 May 2022,https://link.springer.com/article/10.1007/s10614-022-10269-z,Spatio-Temporal Instrumental Variables Regression with Missing Data: A Bayesian Approach,June 2023,Marcus L. Nascimento,Kelly C. M. Gonçalves,Mario Jorge Mendonça,Male,,Male,Mix,,
62.0,1.0,Computational Economics,20 May 2022,https://link.springer.com/article/10.1007/s10614-022-10267-1,Reconstructing the Emergent Organization of Information Flows in International Stock Markets: A Computational Complex Systems Approach,June 2023,Paolo Massimo Buscema,Francesca Della Torre,Pier Luigi Sacco,Male,Female,Male,Mix,,
62.0,1.0,Computational Economics,23 May 2022,https://link.springer.com/article/10.1007/s10614-022-10270-6,Ensuring Mutual Benefit in a Trans-boundary Industrial Pollution Control Problem,June 2023,Ryle S. Perera,Kimitoshi Sato,,Unknown,Unknown,Unknown,Unknown,,
62.0,1.0,Computational Economics,24 May 2022,https://link.springer.com/article/10.1007/s10614-022-10271-5,Modeling Tail Dependence Using Stochastic Volatility Model,June 2023,See-Woo Kim,Yong-Ki Ma,Ciprian Necula,Unknown,Male,Male,Male,"Copulas represent a compelling alternative for capturing the dependence structure of a joint distribution. Copulas are applied widely in finance, in various fields such as portfolio optimization, risk management, and multi-asset derivatives pricing, etc. The most important advantage of this approach is that it provides a representation of the joint distribution if the marginal distributions are given. There are many studies on the properties of copulas and their applications. Cameron et al. (2004) studied the distribution of the difference between two non-negative integer-valued datasets by using copula functions. Choe et al. (2013) found relations between a copula and the Fokker-Planck equation, which is applied to derive the probability density for a stochastic process expressed by a stochastic differential equation and obtained a time-dependent copula equation by exploiting the Fokker-Planck equation. Kim et al. (2016) applied the truncated invariant Farlie-Gumbel-Morgenstern (FGM) copula to analyze the dependence between default intensities. Ma and Kim (2010) derived the joint Laplace transform of the jump-diffusion intensities by using copulas. Meneguzzo and Vecchiato (2004) studied multi-name credit derivatives such as collateralized debt obligation and basket default swap under the copula frameworks. Ma (2015) obtained the joint survival probability from the extended FGM copula based on the Cox intensity processes and applied this to compute credit default swap rates. Necula (2010) estimated the empirical and parametric copula functions from the dependency structure between two market indices and applied these results to compute the frontier of efficient portfolios. Guegan and Zang (2009) derived the price of bivariate contingent claims under the generalized autoregressive conditional heteroskedasticity model by exploiting the dynamic copula approach. Garcia-Jorcano and Muela (2020) used various copula models to examine the characteristic of Bitcoin as a hedge asset against the international market stock indices. Zhang and Zhao (2021) analyze the dynamic dependence between returns of crude oil and natural gas by exploiting time-varying geometric copulas. The Gaussian copula which generates the joint standard normal distribution is by far the most popular of the copulas. However, it has no tail dependency. To overcome this drawback in the context of stochastic volatility models, Fouque and Zhou (2008) showed that some tail dependence can be restored by adding a fast mean-reverting volatility factor. However, a one-time scale model cannot appropriately reflect that the impact of shocks accounting for heavy-tailed distributions tend to be short-lived, while the effects of business cycles, which explain volatility clustering, are more enduring. Hence, in order to capture these stylized facts, a volatility model should include at least two factors to express a well-separated time scale: one factor that controls the persistence of the volatility and another factor that handles the rapid reversion of volatility to its mean and contributes to the volatility of volatility (see Gallant et al. (1999), Chernov et al. (2003), and Adrian and Rosenberg (2008)). Recently, Jeon et al. (2021) developed a multiscale stochastic volatility for pricing of SPX and VIX options and their empirical study indicates that introducing a fast mean-reverting factor in the model contributes to a reduction of about 20% of the pricing errors compared with the corresponding single-scale model. The importance of multiscale stochastic volatility models has also been highlighted in other contexts such as pricing the variance swap (Kim and Kim, 2019), portfolio optimization (Fouque and Hu (2020) and Yang et al. (2020)) or pricing lookback style options (Deng, 2020). The main aim of our paper consists in developing a two assets multiscale stochastic volatility model, having quite a general specification, in order to obtain an analytically approximate formula, in terms of the Gaussian copula, for the joint transition density of the returns of the two assets. This paper is organized as follows. In Sect. 2, we introduce the general form the two-factor stochastic volatility model for two assets which consist of fast and slow mean-reverting factors. Moreover, we perform a validation of a specific formulation of the model by calibrating it to data regarding the daily and weekly returns of the S&P 500 and Dow Jones Industrial Average Indices. Next, in Sect. 3, we obtain, for the general specification of the model, the approximated Gaussian copula density by extending the framework of Fouque and Zhou (2008) to account for the assumption that the volatility is governed by two factors evolving on different timescales. Sect. 4 contains an empirical application regarding a rolling-window estimation of the parameters of the model using daily S&P 500 and DAX data.",
62.0,1.0,Computational Economics,02 June 2022,https://link.springer.com/article/10.1007/s10614-022-10279-x,A Deep Learning Based Numerical PDE Method for Option Pricing,June 2023,Xiang Wang,Jessica Li,Jichun Li,,Female,Unknown,Mix,,
62.0,1.0,Computational Economics,05 June 2022,https://link.springer.com/article/10.1007/s10614-022-10273-3,Predict Stock Prices Using Supervised Learning Algorithms and Particle Swarm Optimization Algorithm,June 2023,Mohammad Javad Bazrkar,Soodeh Hosseini,,Male,Unknown,Unknown,Male,"Today's world is a world of change and knowing what awaits us in the future can greatly contribute to the success of individuals and organizations. In the past, traders took a lot of risks and always looked for a partner to share their profits and losses with, and everyone shared in the profits and losses according to the amount of their capital. ""Stock exchange"" means an organized and formal capital market in which shares of companies and participation bonds are traded under certain terms and conditions. The stock exchange is a formal and reliable reference for attracting capital from individuals and the private sector to finance long-term investment projects. With the continuous development of society's economy, the emergence of capital markets in countries has increased rapidly, so the investor needs powerful and reliable tools to predict stock prices and the ability to buy the stock they need. Stock prices are basically divided into four categories: dynamic, nonlinear, nonparametric and chaotic (Oh & Kim, 2002). Stock price forecasting has been considered by many stock market participants and traders for many years. Even today, buying and selling small stocks of listed companies is attractive to many ordinary people who do not have much capital. Stock market indices fluctuate widely and affect the amount of investment people make. Stock traders are looking for ways to increase their capital gains by predicting future stock prices. Therefore, it seems necessary that appropriate, correct and science-based methods in determining the future price of stocks be presented to investors. In recent years, various methods for forecasting stock prices have been examined like the artificial neural network (ANN) method, which does not work properly due to many parameters in stock price forecasting and analysis (Tao et al., 2004). Support vector machine (SVM) is also mentioned as one of the efficient algorithms for stock price forecasting, which, unlike artificial neural networks, does not stick to local minimums and seeks optimal solutions globally while artificial neural networks are placed in local optimizations (Cherkassky & Ma, 2004). In support vector machine learning algorithm, regulator parameters and algorithm kernel parameters play an important role therefore, it is necessary to choose these parameters. The impact and importance of these parameters have been discussed in (Alvarez Meza et al., 2012). In this paper, we try to predict the stock prices of Amazon and several other companies using the support vector machine, which is one of the machine learning (ML) approaches. We also use the radial base function (RBF) for the support vector machine. Finally, we try to use the particle swarm optimization (PSO) algorithm to optimize the parameters in the algorithm so that we can get the best values for the algorithm and thus get the best prediction. Although many studies have been done in the field of stock price forecasting using RBF kernel support vector machine and particle swarm optimization algorithm, in none of them the accuracy of forecasting the proposed method has reached over 90% and at best, they were about 70 percent (Karazmodeh et al., 2013). In this paper, by accurately and step-by-step optimizing the variables C and \(\updelta \), as well as by selecting the appropriate parameters of the PSO algorithm, we were able to achieve above 90% accuracy in forecasting stock prices in all cases. Our main motivation for doing this research is to provide a reliable way to predict stock prices so that stock market participants and the public can safely invest in the stock market and contribute to the growth and prosperity of their community economy. Our contributions in this paper are as follows. First, we improve the support vector machine algorithm for stock price forecasting using the radial basis function and particle swarm optimization algorithm and second increase the stock price forecasting accuracy and reduce investment risk. The rest of the paper is as follows: Sect. 2 provides a brief summary of related stock price forecasting work. Section 3 describes and review the support vector machine algorithms, the radial base function, and the particle swarm optimization algorithm. Eventually, we create the proposed model in Sect. 4 and examine it. The results obtained and the evaluation of the model are done in Sect. 5, and finally in Sect. 6 we give a general conclusion of the proposed model.",4
62.0,1.0,Computational Economics,07 June 2022,https://link.springer.com/article/10.1007/s10614-022-10274-2,A Synthetic Data-Plus-Features Driven Approach for Portfolio Optimization,June 2023,Bernardo K. Pagnoncelli,Domingo Ramírez,Arturo Cifuentes,Male,Male,Male,Male,"Medium- and long-term investors normally keep an important fraction of their portfolios in liquid assets that trade in public markets, such as stocks, bonds, and commodities. Such investors include individuals—most likely saving for their retirement, or their children’s education—and institutions that are managed as a going concern and/or under an intergenerational equity concept (e.g., insurance companies, pension funds, endowments, family offices). The U.S. stock market is approximately $50 trillion in size. By way of comparison, the global pension funds manage a similar amount while the global insurance sector holds assets equivalent to roughly half that amount. Most pension funds and insurance companies are forced by their regulators to put a high percentage of their portfolios in publicly traded securities. Endowments, and to a lesser extent family offices, although generally unregulated, follow similar, albeit more relaxed, versions of this constraint. Therefore, developing investment strategies for portfolios consisting of publicly traded securities is a problem of practical importance that also has public policy implications. The purpose of this paper is to introduce a portfolio optimization framework aimed at serving the needs of these investors. Our framework is based on two key considerations. First, there is an extensive body of evidence showing that in reasonably efficient markets active portfolio management is a losing proposition, particularly over long time-periods, see Sharpe (1991), French (2008), Walden (2015), Malkiel (2017), Elton et al. (2019), Fahling et al. (2019) for examples and discussions. Consequently, our approach focuses on selecting the appropriate asset allocation weights and then, within each market segment we follow a passive (index-based) strategy. And second, we rebalance (recompute) the asset allocation weights regularly, but not frequently (for example, once a year). To put things in context we need now to go back to Markowitz’s seminal paper. Markowitz (1952) framed the portfolio selection problem as a formal mathematical optimization problem, and in doing so he gave birth to a fruitful subdiscipline within the financial engineering arena. The basic ideas behind the so-called mean-variance (MV) portfolios, namely, the benefits of diversification and the necessary trade-off between risk and return, have resisted well the test of time. It is with the practical application of these ideas that the difficulties arise. First, the correlation-of-returns matrix—an essential element of the MV formulation—has proven to be a challenge to determine. DeMiguel et al. (2009b) calculated that for a portfolio of 25 assets, at least 3,000 months of monthly returns would be required to estimate such matrix with any degree of confidence. Not surprisingly, solving for the MV portfolios often results in unstable solutions, as discussed in Ban et al. (2018). And second, these solutions often yield very concentrated portfolios. Perhaps more surprisingly, DeMiguel et al. (2009b) also concluded that none of the 14 MV-portfolios they considered could consistently beat the equally-weighted (referred to as “EW”) selection scheme based on out-of-sample performance. In another work, DeMiguel et al. (2009a) proposed to constrain the portfolio’s norm to deal with estimation errors, while Kolm et al. (2014) summarized thoroughly the advances made, and the remaining challenges, within the portfolio optimization arena. All in all, it is fair to say that in the second part of the previous century most portfolio optimization efforts were dominated by attempts to address these two difficulties. These attempts relied on robust optimization (see Xidonas et al. 2020 for a survey), regularization techniques (see Ban et al. 2018 and Pagnoncelli et al. 2021 for examples), the introduction of ad hoc constraints to the MV optimization problem (DeMiguel et al. 2009a studied a norm-constrained formulation), Bayesian techniques, see, e.g., the work of Black and Litterman (1992), the addition of risk-parity criteria to the optimization, as done by Bai et al. (2016), plus a number of numerical techniques (e.g., denoising, detoning as described by de Prado 2020) whose specific objective was to mitigate the error propagation that results from ill-conditioned matrices. Another salient element of these efforts was the dominant role played by parametric models to describe asset returns. (Needless to say, estimating the parameters of these models has often proved to be as challenging as estimating the entries in the correlation matrix.). And again, it is fair to say that no approach emerged as a clear winner in this contest. However, the new century brought some fresh air to the portfolio selection problem. This has been the result of incorporating ideas that have proved promising in other areas, rather than the developing of entirely new techniques. In this regard, we recognize three shifts in thinking.  The standard deviation-strictly speaking a measure of uncertainty, not risk-has been consistently losing ground as the preferred risk metric. The Value-at-Risk (VaR) and the Conditional-Value-at-Risk (CVaR), introduced in the late 1990s by Jorion (1996) and in 2000 by Rockafellar et al. (2000), respectively, have firmly established themselves as the risk metrics of choice (the CVaR is somewhat superior to the VaR since it satisfies the subadditivity condition, being a coherent risk measure according to Artzner et al. 1999). Although the academic community has been somewhat slow in making this transition, the risk management community, as well as most financial regulators and portfolio managers, have adopted these metrics, see Chang et al. (2019) for a discussion in the context of Basel III and Boonen (2017) for a discussion in reference to Solvency II. An important advantage of both, the VaR and the CVaR, is that by focusing on losses they are more in tune with the way investors think about risk. Advances in behavioral economics have shown that most investors are actually loss averse rather than risk averse. Thus, it is easier to articulate investors preferences in terms of the VaR or CVaR than in reference to the standard deviation of returns or the coefficients of some utility functions. The following example clarifies this point. Suppose an investor has $1 million in savings. She might describe her risk tolerance stating, for example, that “I want to be sure, that is, with a great deal of confidence, say 90%, that in a worst-case scenario I will not lose more than $50,000.” Or, “If I were to face any of those worst 10% scenarios, I would like my losses, on average, to be $100,000.” Evidently, it is not possible to express these specifications appealing to the standard deviation of returns. For the VaR and the CVaR is straightforward: (i) VaR\(_{0.90}\) = $50,000; and (ii) CVaR\(_{0.90}\) = $100,000. The correlation matrix, leaving aside the difficulties associated with its determination, is useful when dealing with linear relationships, and ideally, with normally distributed random variables. Recent approaches to improve the estimation of the correlation matrix include De Nard et al. (2021), which incorporates factor structure into the estimation process, obtaining better results on historical data than state-of-the-art competitors. The very recent work Agrawal et al. (2022) proposes a new estimator that imposes that the joint distribution of returns be multivariate totally positive of order 2, which outperforms the method proposed in De Nard et al. (2021). Unfortunately, assets returns’ distributions depart significantly from normality. Sklar (1959) published another seminal paper in which he introduced the copula concept, seven years after Markowitz published his famous paper on portfolio selection. Copulas quickly made their way into the world of scientific computations and eventually were adopted by financial engineering practitioners; today they are a standard tool in financial risk management. In short, a copula is a mathematical procedure that builds a multivariate distribution based on the marginal distributions of a group of individual random variables while incorporating their interdependence. This approach has proven to be superior to the standard linear correlation-based scheme characteristic of the Markowitz formulation. An important property of the copula approach is that it imposes no constraints on the marginal distributions. More to the point, the marginal distributions do not need to be normal. The Gaussian copula (there are many others) is the most widely used in finance. Part of its attractiveness comes from its numerical tractability, coupled with satisfactory empirical evidence. Recent advances in machine learning (ML) have demonstrated that the use of contextual information—which are also known as features, covariates, or independent/exogenous information—can improve significantly the out-of-sample performance. A case in point is Ban and Rudin (2019), where the authors propose a big data version of the classical newsvendor problem. In this instance, incorporating features to the optimization problem via kernel-based approaches has proved promising. In the context of portfolio selection, features refer to past information related to economic or financial variables different than returns (e.g., unemployment, consumer confidence). Nguyen et al. (2021) use distributionally robust optimization techniques to solve a portfolio problem with daily trades; features are taken from the celebrated Fama and French (1992) data set. In Dai and Kang (2021) the authors use L1 regularization to obtain sparse portfolios with better out-of-sample properties. We should notice, however, that a common challenge when applying ML algorithms to portfolio selection problems is the scarcity of data. This topic has been discussed extensively in a recent paper by Israel et al. (2020). In our case, as we explain in more detail later, we overcome this limitation by generating synthetic data using a Gaussian copula-based technique. It is worth noting that recent works have incorporated reinforcement learning to solve the problem, both in discrete (see Zhang et al. 2020) and continuous (see Wang and Zhou 2020) time. In summary, the method we propose—a single-step data-driven semi-parametric approach—incorporates the three above-mentioned elements. And when tested during the 2007–2021 time frame it showed very good out-of-sample performance (it beats consistently the EW approach), generated diversified portfolios, and produced coherent risk-return diagrams for a wide range of risk tolerance values. Given the low interest rates observed in the last years, there is a growing pressure on institutional investors, especially pension funds, to deliver satisfactory returns. These two factors combined are putting increasing pressure on reducing costs, and the strategy we propose herein responds to these demands since it is very low-cost (passive positions within each asset class plus once-in-a-year rebalancing), easy-to-implement, and it can be tailored to the investor’s risk preference. However, it should be clear that our approach is not suitable for high-frequency trading, or for investing in private markets where passive investment is not possible (venture capital, private equity, private debt, etc.).",
62.0,1.0,Computational Economics,10 June 2022,https://link.springer.com/article/10.1007/s10614-022-10277-z,"Exploring Uncertainty, Sensitivity and Robust Solutions in Mathematical Programming Through Bayesian Analysis",June 2023,Mike G. Tsionas,Dionisis Philippas,Constantin Zopounidis,Male,Unknown,Male,Male,"In optimization, the data is rarely, if ever, known with precision. That is, certain parameters of the mathematical programming problem are unknown with certainty and are subject to varying degrees of error, which is still an open problem for decision-makers/modelers. Approaches to addressing mathematical programming under uncertainty have been developed, dated back to the seminal works of Tintner (1955, 1960) in stochastic programming, the method of chance-constrained programming in the study of Charnes and Cooper (1963) and the technique of two-stage programming under uncertainty by Dantzig and Madansky (1961). Starting with the early study of Soyster (1973), a research field has emerged in robust optimization that seeks to find a solution that is feasible under all possible configurations of uncertainty in the data of linear programming (Ben-Tal & Nemirovski, 1998 and 1999; El Ghaoui & Lebret, 1997; El Ghaoui et al., 1999; Bertsimas & Sim, 2004; Bertsimas et al., 2004; Mansini et al., 2014). Since then, the applications in stochastic programming are numerous to various optimization tasks with uncertainties such as optimal design and operation, optimal production planning as well as optimal control of industrial processes under uncertainty (Abdelaziz, 2012; Castro, 2009; Philpott & de Matos, 2012; Pichler & Tomasgard, 2016; Shapiro et al., 2013; Uhan, 2015). Robustification provides a viable alternative to problems where optimizing the expected value of the objective is very risky, since there is the possibility of infeasible solutions or large variability caused by the uncertain elements in the problem. In addition to common robust solutions, many problems in risk management can be solved using the Conditional Value at Risk (CVaR) as well as with stochastic optimization (Dumskis & Sakalauskas, 2015; Guigues & Romisch, 2012; Mulvey & Erkan, 2006). In this paper, we consider a mathematical program under uncertainty and ask if we can solve the problem in terms of a suitable likelihood/posterior, which means to analyze the effects of uncertainty under Bayesian techniques. Uncertainty in the data is modeled using sampling models for the constraints. The value of the objective function is suitable for quantifying the effect of uncertainty, acting as a prior. Monte Carlo methods are used to explore the posterior. The novel feature of our Bayesian technique is that the unknown parameter for which we desire to make statistical inference for, is the solution vector itself. As a by-product, robust solutions are designed easily without solving mathematical programming problem. We illustrate our approach in a problem whose solution and its properties are known. Finally, we apply the proposed techniques to an empirical portfolio selection problem, using EuroStoxx 50’s securities. The paper makes some important contributions to the relevant literature. First, the novelty we propose is the solution vector for the unknown parameter, which incorporates uncertainty. Relative to simple Monte Carlo procedures for solving the stochastic mathematical programming problem (Sakalauskas, 2002), Bayesian Markov Chain Monte Carlo (MCMC) is quite different in that it provides draws directly from the likelihood/posterior of the solution vector and does not require solving the program. This process is a tremendous advantage as large and/or complicated problems can be solved at a fraction of time. Moreover, robust solutions can be found without additional cost from the results of MCMC. Post-processing using CVaR is also quite possible, which opens new ways of dealing with uncertainty and choosing a solution based on the available MCMC draws. The remainder of the paper is organized as follows. Section 2 presents the Bayesian modelling in mathematical programming involving linear, quadratic, and mixed-integer nonlinear programs as well as the data. Section 3 presents an illustrative example. Section 4 discusses the case of uncertainty in all the data and Sect. 5 illustrates an application to optimal portfolio. Finally, Sect. 6 concludes and proposes future research perspective.",
62.0,1.0,Computational Economics,15 June 2022,https://link.springer.com/article/10.1007/s10614-022-10281-3,Forecasting Forex Trend Indicators with Fuzzy Rough Sets,June 2023,J. C. Garza Sepúlveda,F. Lopez-Irarragorri,S. E. Schaeffer,Unknown,Unknown,Unknown,Unknown,,
62.0,1.0,Computational Economics,17 June 2022,https://link.springer.com/article/10.1007/s10614-022-10272-4,Optimal Limit Order Book Trading Strategies with Stochastic Volatility in the Underlying Asset,June 2023,Burcu  Aydoğan,Ömür Uğur,Ümit Aksoy,Female,,Male,Mix,,
62.0,1.0,Computational Economics,21 June 2022,https://link.springer.com/article/10.1007/s10614-022-10257-3,Quasi-Monte Carlo-Based Conditional Malliavin Method for Continuous-Time Asian Option Greeks,June 2023,Chao Yu,Xiaoqun Wang,,,Unknown,Unknown,Mix,,
62.0,1.0,Computational Economics,30 June 2022,https://link.springer.com/article/10.1007/s10614-022-10284-0,Portfolio Optimization Via Online Gradient Descent and Risk Control,June 2023,J. D. M. Yamim,C. C. H. Borges,R. F. Neto,Unknown,Unknown,Unknown,Unknown,,
62.0,1.0,Computational Economics,06 July 2022,https://link.springer.com/article/10.1007/s10614-022-10278-y,Spatial Interactions and the Spread of COVID-19: A Network Perspective,June 2023,Cui Zhang,Dandan Zhang,,,Unknown,Unknown,Mix,,
62.0,1.0,Computational Economics,13 July 2022,https://link.springer.com/article/10.1007/s10614-022-10289-9,Penalized Averaging of Quantile Forecasts from GARCH Models with Many Exogenous Predictors,June 2023,Jan G. De Gooijer,,,Male,Unknown,Unknown,Male,"Combining forecasts obtained by a hybrid approach has long been known to improve forecast accuracy. Because each model whose forecasts are to be combined may consider different predictors and make different assumptions about the underlying data generating process (DGP) and distributions, averaging the individual forecasts broadens the information embedded and may offset individual model biases as well; see, e.g., Zhang (2003) for additional reasons. In a one-dimensional (univariate) setting the hybridization of forecasts obtained from time series models/methods has been well researched; see, e.g., Aijaz and Agarwal (2020), Aydin and Isci Güneri (2015), Pai and Lin (2005), and Valenzuela et al. (2008) among others. Recently, however, the enhanced availability of large databases with many time series variables as potential predictors has stimulated interest in high-dimensional forecasting. Excellent reviews of the state-of-the-art in economics and finance are given by Fan et al. (2011) and Lee (2011). Furthermore, the introductory section of the paper by Uematsu and Tanaka (2019) provides an extensive survey of the current literature on high-dimensional forecasting and variable selection. Along this direction, conditional quantile averaging procedures in conjunction with dimension reduction methods have been considered. Examples include quantile forecasting the S&P 500 equity risk premium (see, e.g., Konzen and Ziegelmann, 2016; Lima and Meng,2017; Meligkotsidou et al.,2014; De Gooijer and Zerom, 2020) inflation forecasting (Garcia et al., 2017; De Gooijer and Zerom, 2019), quantile forecasting of macroeconomic time series (Manzan, 2015; Jiang et al., 2018), and realized volatility forecasting (Meligkotsidou et al., 2019). Most of these studies are limited by the assumption that the high-dimensional data set comes from a linear DGP, and forecasts are obtained from finite-dimensional parametric models. One exception is the study by De Gooijer and Zerom (2020). Using a large data set of predictors, involving both macroeconomic predictors and technical indicators, these authors showed that combining quantile forecasts from parametric and semiparametric methods (called hybrid quantile averaging) can be useful in practice. Semiparametric models are infinite-dimensional. As a result, the quantile forecasts are less prone to model misspecification as may happen with forecasts obtained from parametric models. In terms of quantile forecast performance the hybrid method works well in identifying relevant predictors, and more importantly results in improved combined one-step ahead forecasts over alternative (no hybridization) conditional quantile methods. While the empirical findings by De Gooijer and Zerom (2020) are interesting, they are sample-specific which makes it difficult to generalize the results to novel situations. In addition, the focus on one-step ahead out-of-sample prediction is somewhat restrictive. Multi-step ahead out-of-sample quantile forecasting results can provide more insight in the relative performance of the hybrid quantile averaging method over a longer time period. Unfortunately, a general theoretical comparison of quantile forecasts obtained from hybrid and non-hybrid methods is not feasible due to complicated interactions of nonlinear parameter estimation methods, sparse modelling, and correlated forecasts. Indeed, further insights regarding the performance of the hybrid quantile averaging method can only be obtained via a Monte Carlo simulation study. In the first half of this paper, we provide such a study. In the second half of the paper, we evaluate the out-of-sample multi-step ahead forecasting performance of the hybrid conditional quantile method and five alternative, non-hybrid, forecasting methods via an empirical application. More specifically, we report out-of-sample conditional quantile forecasts for the risk premium of the monthly S&P 500 index using a large data set of macroeconomic predictors. A simple equal-weighted combination of parametric and semiparametric conditional quantile forecasts is adopted as a benchmark. Our main finding is that that hybridization can be an effective way to improve quantile forecasts as compared to non-hybrid methods. The rest of this paper unfolds as follows. First, for ease of reference, Sect. 2 summarizes the main features of the semiparametric and hybrid conditional quantile averaging methods. Section 3 describes six quantile forecasting methods used in the Monte Carlo experiment. Section 4 introduces the large data set of exogenous predictors via semiparametric and parametric GARCH model specifications. This section also discusses the evaluation of quantile forecasts. Section 5 contains the simulation results, while Sect. 6 presents the empirical results. Finally, Sect. 7 contains some concluding remarks.",
62.0,1.0,Computational Economics,23 April 2021,https://link.springer.com/article/10.1007/s10614-021-10119-4,Reinforcement Learning in Economics and Finance,June 2023,Arthur Charpentier,Romuald Élie,Carl Remlinger,Male,Male,Male,Male,"Reinforcement learning is related to the study of how agents, animals, autonomous robots use experience to adapt their behavior in order to maximize some rewards. It differs from other types of learning (such as unsupervized or supervised) since the learning follows from feedback and experience (and not from some fixed training sample of data). Thorndike (1911) or Skinner (1938) used reinforcement learning in the context of behavioral psychology, ethology and biology. For instance, Thorndike (1911) studied learning behavior in cats, with some popular experiences, using some ‘puzzle box’ that can be opened (from the inside) via various mechanisms (with latches and strings) to obtain some food that was outside the box. Edward Thorndike observed that cats usually began experimenting – by pressing levers, pulling cords, pawing, etc. – to escape, and over time, cats will learn how particular actions, repeated in a given order, could lead to the outcome (here some food). To be more specific, it was necessary for cats to explore alternative actions in order to escape the puzzle box. Over time, cats did explore less, and start to exploit experience, and repeat successful actions to escape faster. And the cat needed enough time to explore all techniques, since some could possibly lead more quickly – or with less effort – to the escape. Thorndike (1911) proved that there was a balance between exploration and exploitation. This issue could remind us of the simulated annealing in optimization, where a classical optimization routine is pursued, and we allow to move randomly to another point (which would be the exploration part) and start over (the exploitation part). Such a procedure reinforces the chances of converging towards a global optimum, instead of converging to a more local one. Another issue was that a multi-action sequence was necessary to escape, and therefore, when the cat was able to escape at the first time it was difficult to assign which action actually caused the escape. An action taken at the beginning (such as pulling a string) might have an impact some time later, after other actions are performed. This is usually called a credit assignment problem, as in Minsky (1961). Skinner (1938) refined the puzzle box experiment, and introduced the concept of operant conditioning (see Jenkins 1979 or Garcia 1981 for an overview). The idea was to modify a part, such as a lever, such that at some points in time pressing the lever will provide a positive reward (such as food) or a negative one (i.e. a punishment, such as electric shocks). The goal of those experiments was to understand how past voluntary actions modify future ones. Those experiments were performed on rats, and no longer cats. Tolman (1948) used similar experiments (including also mazes) to prove that the classical approach, based on chaining of stimulus-responses, was maybe not the good one to model animal (and men) behaviors. A pure stimulus-responses learning could not be used by rats to escape a maze, when experimenters start to block roads with obstacles. He introduced the idea of cognitive maps of the maze that allow for more flexibility. All those techniques could be related to the ones used in reinforcement learning. Reinforcement learning is about understanding how agents might learn to make optimal decisions through repeated experience, as discussed in Sutton and Barto (1981). More formally, agents (animals, humans or machines) strive to maximize some long-term reward, that is the cumulated discounted sum of future rewards, as in classical economic models. Even if animals can be seen as have a short-term horizon, they do understand that a punishment followed by a large reward can be better than two small rewards, as explained in Rescorla (1979), that introduced the concept of second-order conditioning. A technical assumption, that could be seen as relevant in many human and animal behaviors, is that the dynamics satisfies some Markov property, and in this article we will focus only on Markov decision processes. Reinforcement learning is about solving the credit assignment problem by matching actions, states of the world and rewards. As we will see in the next section, formally, at time t, the agent at state of the world \(s_{t}\in {\mathcal {S}}\) makes an action \(a_t\in {\mathcal {A}}\), obtains a reward \(r_t\in {\mathcal {R}}\) and the state of the world becomes \(s_{t+1}\in {\mathcal {S}}\). A policy is a mapping from \({\mathcal {S}}\) to \({\mathcal {A}}\), and the goal is to learn from past data (past actions, past rewards) how to find an optimal policy. A popular application of reinforcement learning algorithms is in games, such as playing chess or Go, as discussed in Silver et al. (2018), or Igami (2017) which provides economic interpretation of several algorithms used on games (Deep Blue for chess or AlphaGo for Go) based on structural estimation and machine (reinforcement) learning. More simply, Russell and Norvig (2009) introduced a grid world to explain heuristics about reinforcement learning, see Fig. 1. Positions on the \(4\times 3\) grid are the states \({\mathcal {S}}\), and actions \({\mathcal {A}}\) are movements allowed. The optimal policy \(\pi :{\mathcal {S}}\rightarrow {\mathcal {A}}\) is here computed using sequential machine learning techniques that we will describe in this article. Sequential decision making problem on a \(4\times 3\) grid (\({\mathcal {S}}\) states), from Russell and Norvig (2009). The agent starts at the state (A,1), and moves around the environment, trying to reach terminal state (D,3) to get a +1 reward - and to avoid terminal state (D,2) where a -1 reward (punishment) is given. Possible actions (\({\mathcal {A}}\)) are given on the top-right figure. On the bottom, two policies are given with \(\pi :{\mathcal {S}}\rightarrow {\mathcal {A}}\) on the left, and \(\pi :{\mathcal {S}}\rightarrow A\subset {\mathcal {A}}\) on the right. In the later case, there can be random selection of actions in some states, for instance \(\pi ({(\textsf {A,1})})\in \lbrace \text {up},\text {right}\rbrace \) Supervised Machine Learning techniques is a static problem: given a dataset \({\mathcal {D}}_n=\{(y_i,{x}_i)\}\), the goal is to learn a mapping \({\widehat{m}}_n\) between x and y. In decision theory \({\widehat{m}}_n\) typically takes values in a binary space, which could be to accept or reject a mortgage in credit risk models, or to invest or not in some specific asset. \({\widehat{m}}_n\) can also take values in the real line, and denote an amount of money to save, a quantity to purchase or a price to ask. Online learning is based on the assumption that \((y_i,{x}_i)\) arrive in a sequential order, and the focus is on the evolution of \({\widehat{m}}_n\) as n growth, updating the training dataset from \({\mathcal {D}}_{n-1}\) to \({\mathcal {D}}_n\). Reinforcement learning incorporates the idea that at time \(n-1\), a choice was made, that will influence \((y_n,{x}_n)\), and the standard i.i.d. assumption of the dataset is no longer valid. Reinforcement learning is related to sequential decision making and control. Consider an online shop, where the retailer tries to maximize profit by sequentially suggesting products to consumers. Consumers are characterized by some features, such as their age, or their gender, as well as information about what’s in their shopping cart. The consumer and the shop will have sequential interactions. Each round, the consumer can either add a product to the shopping cart, or not buy a product and continue shopping, or finally stop shopping and check out. Those transitions are characterized by transition probabilities, function of past states and actions. Such transition probability function is unknown and must be learned by the shop. Should the retailer display the most profitable products, exploiting information he obtained previously, or explore actions, that could be less profitable, but might provide relevant information ? The induced problems are related to the fact that acting has consequences, possibly delayed. It is about learning to sacrifice small immediate rewards in order to gain larger long-term ones. If standard Machine Learning is about learning from given data, reinforcement learning is about active experimentation. Actions can be seen as an intervention, so there are strong connections between reinforcement learning and causality modeling. Reinforcement learning allows us to infer consequences of interventions (or actions) used in the past. Pearl (2019) asked the simple economic question ‘what will happen if we double the price’ (of an item we try to sell)? ‘Such questions cannot be answered from sales data alone, because they involve a change in customers behaviour, in reaction to the new pricing’. Reinforcement learning is related to such problem: inferring the impact of interventions. And the fact that intervention will impact the environment, mentioned by Pearl (2019), is precisely what reinforcement learning is about. So this theory, central in decision science will appear naturally in sequential experimentation, optimization, decision theory, game theory, auction design, etc. As we will see in the article (and as already mentioned in the previous section), models in sequential decision making as long history in economics, even if rarely mentioned in the computational science literature. Most of the articles published in economic journal mentioned that such problems were computationally difficult to solve. Nevertheless, we will try to show that recent advances are extremely promising, and it is now to possible to model more and more complex economic problems. In Sect. 2, we will explain connections between reinforcement learning and various related topics. We will start with machine learning principles, defining standard tools that will be extended later one (with the loss function, the risk of an estimator and regret minimization), in Sect. 2.1. In Sect. 2.2, we introduce dynamical problems with online learning, where we exploit past information sequentially. In Sect. 2.3, we present briefly the multi-armed bandit problem, where choices are made, at each period of time, and those have consequences on the information we obtain. And finally, in Sect. 2.4 we start formalizing reinforcement learning models, and give a general framework. In those sections, we mainly explain the connections between various learning terms used in the literature. Then, we present various problems tackled in the literature, in Sect. 3. We will start with some general mathematical properties, giving various interpretations of the optimization problem, in Sect. 3.1. Finally, we will conclude, in Sect. 3.4, with a presentation of a classical related problem, called inverse reinforcement learning, where we try to use observed decisions in order to infer various quantities, such as the reward or the policy function. Finally, three sections are presenting applications of reinforcement learning. In Sect. 4.1, we discuss applications in economic modeling, starting with the classical consumption and income dynamics, which is a classical optimal control problem in economics. We then discuss bounded rationality and strong connections with reinforcement learning. Then we will see, starting from Jovanovic (1982), that reinforcement learning can be used to model single firm dynamics. And finally, we present connections with adaptative design for experiments, inspired by Weber (1992) (and multi-armed bandits). In Sect. 4.2, we discuss applications of reinforcement learning in operation research, such as the traveling salesman, where the standard dilemma exploration/exploitation can be used to converge faster to (near) optimal solutions. Then we discuss stochastic games and equilibrium, as well as mean-field games, and auctions and real-time bidding. Finally, we will extend the single firm approach of the previous section to the case of oligopoly and dynamic games. Finally, in Sect. 4.3, we detail applications in finance. We start with risk management, valuation and hedging of financial derivatives problems on then focus on portfolio allocation issues. At last, we present a very natural framework for such algorithms: market impact and market making.",24
62.0,1.0,Computational Economics,10 October 2021,https://link.springer.com/article/10.1007/s10614-021-10210-w,Streaming Approach to Quadratic Covariation Estimation Using Financial Ultra-High-Frequency Data,June 2023,Vladimír Holý,Petra Tomanová,,Male,Female,Unknown,Mix,,

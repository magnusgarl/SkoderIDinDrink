Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050001,Historic Turning Points in Real Estate,January 2008,Robert J Shiller,,,Male,Unknown,Unknown,Male,"At the risk of repeating the obvious, but for the purpose of making sure the reader is online with some important facts, let us first reflect on those ubiquitous terms “supply” and “demand” that determine, by their intersection, prices in any market. The price has to clear the market continually. If there is an imbalance between supply and demand at any time, the price will have to change immediately. It would seem that demand for housing services should be relatively inelastic in the short run, especially with regard to the number of units (rather than their size). Most families want just one house. The decision to own two or more houses, or the decision to break up the family to spread out over more houses, is not made very often — most commonly only at important life turning points or job changes. It is difficult for builders to transform two small housing units into one larger unit, or one large unit into two small housing units, without great costs. Hence, even small changes in the number of housing units might be expected to cause major short-run changes in home prices. However, home prices do seem to show enormous momentum, and sudden changes in the market seem rare. In a speculative market, a sudden change in some component of supply or demand may produce little price change if people think that the change is temporary, and so another component, a speculative component, offsets the sudden change. But the speculative component is inherently psychological, potentially unstable, and subject to contagion and herd behavior. People may change their mind about whether a change in price is only temporary or is the beginning of a new trend. They are especially likely to change their mind because we have professional marketers whose job is to get some kind of social response moving, and, when they do find some advertising pitch that resonates with investors, they will run it for all it is worth. The supply of housing is dictated by the decisions of builders, who face markets for construction labor, materials, and land prices. According to the simple “Tobin's Q” model of investment, whenever home prices are high relative to construction costs, construction will proceed at a relatively high rate, until the gap between home prices and construction costs is closed off by new construction. If construction could be done instantaneously, it would not matter whether prices are rising or falling, builders would look only at the current price and build whenever price is high relative to construction costs. Since there are lags on the order of a year between decision to build and completion of a housing unit, builders will tend to pull back in a period of declining prices even if prices are high relative to construction costs, but will continue to build at a high rate if housing prices are still expected to be high by the time the construction can be completed. Analysis of past booms seems to indicate that investors in both the stock market and the housing market seem often not to understand the supply response to price increases. These are normal intelligent people, and so why would they repeatedly make the same mistake again and again? There seems to be what I will call a uniqueness bias, a tendency for investors to overestimate how unique an investment they favor is, failing to take account of the inevitable supply response to high prices. The uniqueness bias is reflected in quite a number of anomalies of human judgment that psychologists have documented, including the “representativeness heuristic,” “overconfidence,” “wishful-thinking bias,” “spotlight effect,” and “self-esteem bias.” The uniqueness bias is related to failure to imagine how many possible competitors there are, a tendency to think highly of oneself and one's associates, and an association of investments with one's sense of personal identity with an identified business model. The uniqueness bias has its effect in the stock market by encouraging people to think that a company's market position is unique, and thus underestimating how quickly new competition will move in to close off any initial advantage. Gordon Philips and Gerard Hoberg, in their 2007 study of booms in individual stocks, found that those in competitive industries, not concentrated industries, show significant downturns following high valuations. For competitive industries, stock returns are low following high industry valuation and investment. They concluded that firms and the investors in these firms face a signal extraction problem in booms, not knowing whether other firms have the same apparent opportunities, not recognizing potential competition. They are thus vulnerable to “new era” booms, overinvesting while neglecting to consider that many others are, or soon, will be making essentially the same investments. The uniqueness bias has its effect in the housing market when people imagine that the city they live in is unusually attractive, and increasingly so. They fail to understand that new such cities can be constructed in what are today cornfields or forests. In their 1990 paper, “The Baby Boom, The Baby Bust and the Housing Market,” N. Gregory Mankiw and David Weil argued that the housing market would soon crash as the baby boomers retired, neglecting to consider how supply would adjust to any such change in demand. In their 2004 paper “Superstar Cities,” Gyourko et al. argued for extrapolating some long-standing trends in major US cities, claiming that these superstars will only grow in status, assuming implicitly that there can be no new supply of the services those cities provide. We have seen many examples of such thinking in the history of economic thought. I wish to turn to some of these now, with special attention to the behavior of the markets around what later proved to be major turning points.",53
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050011,Socialism vs Social Democracy as Income-Equalizing Institutions,January 2008,John E Roemer,,,Male,Unknown,Unknown,Male,"Disagreements between socialists and social democrats have festered since the beginning of the 20th century. Originally, the central conflict concerned the path to socialism: socialists argued that revolutionary struggle was the only way, while social democrats argued for “boring from within” by participating in elections and traveling the parliamentary road. It is by no means clear with whom Marx and Engels would have sided on this issue: Engels famously remarked that when workers received the franchise, socialism would come quickly through the ballot box. An engaging discussion of the social democratic electoral strategy in the first half of the 20th century is Przeworski and Sprague [1986]. For an exhaustive history of these debates, see Sassoon [1996]. There is, however, a second distinction between socialism and social democracy that seems more pertinent today, when democracy has become the pervasive institutional desideratum, at least in the advanced countries, and that is with respect to the dimension of income distribution. Socialism, as defined by Marx, was an economic system in which capitalist exploitation had been eliminated. This means that the distribution of society's output to its producers was in proportion to the value of labor they expended in its production (on which more below). One institutional proposal whereby such a distribution could be achieved, perhaps, is nationalization of the capital stock, followed by the distribution of the entire product to workers, in the proportions just described, in lieu of using the capitalist system of distribution, in which the product is distributed in part to workers in proportion to the value of their labor and in part to the owners of capital in proportion to the value of their contributed capital. But this institutional proposal should not be viewed as the definition of socialism: it is merely a tactic of implementation, which the centrally planned economies tried — at least, so the justification goes. The definition of socialism is an allocation in which capitalist exploitation has been eliminated. (See, e.g., Roemer [1982] for formal definitions of exploitation.) This is summed up in Marx's phrase that socialism's allocative rule is “from each according to his ability, to each according to his work,” while the more advanced stage of communism was defined as one in which distribution would be not according to work but need. Those whose work is more valuable receive more under socialism than others do. Some may question my claim that the distributional rule of socialism is “distribution according to the value of labor performed” rather than “distribution according to labor time worked,” where time might be altered by intensity in the sense of physical exertion. The difference, here, is that the “value of labor” is labor time multiplied by an appropriate efficiency or skill factor and “labor time” is hours of work, multiplied, perhaps, by a measure of intensity but not skill. I believe the only sensible definition is the value-of-labor one. For exploitation is the appropriation of “surplus value,” and that must be the value of the product minus the value of the labor embodied in it. Clearly, the value of the product will include the value of skilled labor applied in its production, and so the elimination of exploitation must entail that the contribution of skilled labor goes to the skilled laborer. Those who seek clarification in the Marxian texts find ambivalence: in Critique of the Gotha Program, Marx (1875) writes that “labor, to serve as a measure, must be defined by its duration or intensity…” and later in the same paragraph writes that the equal right of labor is “…an unequal right for unequal labor…[which] tacitly recognizes unequal individual endowment, and thus productive capacity, as a natural privilege. It is therefore a right of inequality.” Clearly, the second quotation values labor by its skill. Social democrats, however, were not primarily concerned with the elimination of capitalist exploitation, but rather with achieving a more equal distribution of income than was associated with laissez-faire capitalism. The model that was implemented in the Nordic countries, with great success, used taxation rather than nationalization. Firms remained, in Scandinavia, almost entirely privately owned, and their ownership was quite concentrated, but income and consumption taxation succeeded in redistributing income substantially. (This is not the sole technique they used to achieve relative income equality: there is also the “solidaristic wage” policy, which reduced wage differentials considerably compared to what transpired in other advanced capitalist countries.) Perhaps because the classical Marxist model assumed that workers were homogeneous in skill (as proposed in Capital, Volume 1), socialists have tended to associate the elimination of exploitation with the achievement of income equality. But this is a false association, because in reality — certainly today, if not when Marx wrote — the distribution of skills is extremely heterogeneous. If, per socialism, the product were to be distributed in proportion to the value of labor expended, there would still be considerable income inequality. It is even conceivable that the socialist allocation (which we will define in a precise way below) would sustain more income inequality than would a social democratic regime that redistributes income through taxation, but makes no attempt to eliminate exploitation in the Marxian sense. In this article, I will define precisely, for a simple model, what the socialist allocation is. I will then calibrate the model to the American economy, and ask: What degree of income taxation and redistribution would achieve the same Gini coefficient of income as the socialist allocation would? It turns out that, given the skill distribution in the United States today, the socialist allocation would produce slightly more income inequality than exists in the present American after-tax distribution of income. Socialists must therefore resolve for themselves the following dilemma: if it is primarily income inequality with which they are concerned, then there seems little point in advocating socialism as a goal. If, however, they continue to advocate the elimination of capitalist exploitation, then reasons beyond the achievement of income equality must be provided. I will offer several proposals in the conclusion for what those reasons could be, although they may not be convincing, to myself or others. The alternative move, which I advocate, is to go beyond socialism, as classically defined, as the desideratum. There is a similarity in spirit between this paper and recent work by Saez [2005], who shows that those at the top of the current wealth distribution in the United States are now individuals whose income is earned as opposed to unearned (in the sense of being labor income rather than capital income); this contrasts with a century ago, when those at the top received their income primarily from capital. If the wealthiest people in capitalist society are simply those who receive high returns to their labor, then the Marxist critique of capitalism is weakened. Those who are disturbed by income inequality, in this case, should be concerned with reducing inequality in the distribution of skills, through education, the solidaristic wage, and the redistribution of income, rather than with the elimination of exploitation.",11
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050017,The Effect of Childhood Sexual Victimization on Women's Income,January 2008,John Robst,Stacy Smith,,Male,,Unknown,Mix,,
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050019,Is Local Government Spending Converging?,January 2008,Mark Skidmore,Steven Deller,,Male,Male,Unknown,Male,"Traditional models of public finance including the median voter [Bowen 1943; Black 1948], the Tiebout [1956] and Peterson [1981] view of competition among local governments, and the complementary theories of public choice [Bish and Ostrom 1979; March and Olsen 1989; McCabe and Vinzant 1999] are built on the assumption that government fiscal policies are a function of the preferences of economic agents.Footnote 1 Assuming that elected officials are responsive, these theoretical views of the public sector indicate that government fiscal policy will adjust to the changing preferences or circumstances of economic agents. Therefore, changes in government fiscal policies depend on agents' changing demands for government services. The goods and services provided by local governments are seldom for immediate consumption and can be interpreted as inputs to productive activity in the private sector. This includes not only the obvious infrastructure spending like roads, bridges, and water treatment facilities but also activities that facilitate the accumulation of human and social capital. Such activities include education, health care provision, environmental protection, safety, and protection of property rights. In fact, most government activity can probably be interpreted as some kind of investment. Even investments in “quality of life” attributes such as parks, recreational, and cultural services are playing an increasingly important role in the functioning of local economies [Dissart and Deller 2000; Deller et al. 2001]. In a sense, government spending can be seen as an endogenous element in a regional growth process. There is significant empirical work demonstrating that as income increases the demand for public services will also increase, translating in practice into a natural tendency to increase government spending. From a demand perspective, the question hinges on the income elasticity of demand, but from a supply perspective we argue that government spending is much like private capital and exhibits diminishing marginal returns. Local governments that have a high level of government spending therefore have limited incentives to expand spending while those with relatively small government sectors will want to increase public spending in greater proportion. We hypothesize that this will lead to convergence of government spending across localities. In the next section of this study, we review the literature on convergence in government spending and provide an outline of the analytical framework we use to explain why we expect faster spending growth in localities with lower initial levels of government spending. In the subsequent section, we document a few basic stylized facts and proceed to provide more rigorous empirical analyses, which demonstrate convergence in local government spending even after we control for a large number of complicating factors. We summarize our findings and discuss their implications in the last section.",6
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050002,Estimating Wal-Mart's Impacts in Maryland: A Test of Identification Strategies and Endogeneity Tests,January 2008,Michael J Hicks,,,Male,Unknown,Unknown,Male,"Wal-Mart's ubiquity has spawned considerable analysis from a variety of research traditions. Most of these studies assess the local economic consequences of Wal-Mart's entrance on employment, wages, and retail and wholesale structure. However, only a handful of these employ methods that effectively attempt to evaluate the incremental effects of a Wal-Mart on these variables. This leaves many important questions unanswered. A central question to researchers is the problem of endogeneity within Wal-Mart's entrance decision. Simply, estimates of Wal-Mart's impact must be disentangled from growth in regions that were occurring or would have occurred anyway. Failure to do so potentially biases the estimates. To date, three approaches have been employed to account for endogenous entrance decisions in an estimation of the impacts on labor markets and retail structure. In this paper I review these methods, adding a fourth identification strategy based upon comments regarding Wal-Mart's entrance decisions made by a company executive. I also test endogeneity of Wal-Mart's entrance decision on several variables, which may be employed to choose location, timing, and date. I extend this analytical framework to the impact Wal-Mart may have on overall employment and wages. I begin by reviewing the most recent studies that attempt to evaluate endogeneity and treat its presence econometrically. I then add to the identification debate by introducing another identifying equation derived from evidence of Wal-Mart's entrance practices. My comparisons of these results are performed through an examination of retail labor markets (wages and employment) and overall labor markets. I conclude with recommendations for further analysis and policy considerations.",22
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050003,An Economic Analysis of Libel Law,January 2008,Manoj Dalvi,James F Refalo,,Male,Male,Unknown,Male,"This paper examines the welfare effects of different libel law standards as applied to the publication of news stories about public figures. Because of public (social) externalities, not all benefits or costs stemming from publication accrue to, or are borne by, the newspaper. As a result of these distortions, the socially optimal solution is unlikely to obtain. The paper models the application of libel law by assuming that the newspaper's decision to publish is determined by the expected liability (costs) arising from publication of false stories, where its ability to mitigate some of the costs depends on the applicable liability standard. We show that compared with strict liability, the current standard governing libel law — termed “negligence” — leads to greater publication costs for stories that are likely to be true and potentially increased publication of stories that are likely to be false. This result is due to additional liability protection provided by negligence, enabling a newspaper to “insure” against liability. We also show that an implicit agency problem exists between the newspaper and society under both standards, and determine conditions for which the social optimum can be consistently attained under strict liability, when using conventional policy tools. We demonstrate that the negligence standard cannot be adjusted in a similar manner. Finally, we provide other applications for this modeling approach. Prior to 1964, the legal rule governing libel in the United States was a “strict liability” standard, under which a newspaper would be liable for all damages caused to someone's reputation by any story that was not provably true.Footnote 1 In its decision of New York Times v. Sullivan,Footnote 2 the Supreme Court ruled that “a public official cannot recover damages for a defamatory falsehood unless he proves that the statement was made with “actual malice,” — that is “with knowledge that it was false or with reckless disregard of whether it was false or not.”Footnote 3 It thereby reduced the range of stories for which a newspaper could be found liable. In 1967, the Supreme Court extended this protection to stories about public figures.Footnote 4 Thus, a negligence-style standard now governs libel suits brought by public officials or public figures.Footnote 5
 The motivation for this paper is to analyze implications for public welfare of these different liability standards in the presence of public externalities. Libel is an example of a “single activity accident” [Brown 1973; Diamond 1974], such that the actions of a newspaper damage an individual.Footnote 6 For single activity accidents, a strict liability standard in the absence of other externalities is generally efficient [Shavell 1980; 1987]. Such models examine cases in which the only externality from an activity is the harm done to a third party.Footnote 7 However, publishing news about public issues is deemed to entail a positive externality.Footnote 8 The concern with a strict liability standard for libel is a potential chilling of publication and concomitant reduction of this externality.Footnote 9 If publishers internalized all benefits from their activities and the only distortion was the negative externality from libel, this would not be of any concern. Since conventional tort models typically assume the party will internalize all benefits and costs other than third-party damages, it is important to consider some deviation from those assumptions in order to assess whether the New York Times v. Sullivan standard is appropriate. The model in this paper will allow for the possibility that, absent any libel, the social value of publishing differs from the private incentive.Footnote 10 We do this by assuming that the newspaper pursues policies that maximize expected value, and that the costs and benefits from publication differ from those of society. Such an assumption implies that a strict liability standard, by itself, does not result in the social optimum. However, we show that in theory we can adjust the strict liability standard using policy tools so that the problem solved by the publisher is proportional to that needed to maximize public welfare. In contrast, the negligence standard will fail to attain the social optimum even with the use of policy tools. We extend the existing liability literature and study the distortions caused by liability rules in a two-stage decision model where the newspaper has the option to obtain further information regarding the truthfulness of a story, and we allow abandonment of the activity.Footnote 11 The former permits a revision of the expected value of a story; the latter allows a story to be pursued that would otherwise not be sufficiently plausible to publish under a strict liability standard. Permitting investigation also allows us to examine the qualitative differences between the two liability standards, namely that under negligence, investigation permits a publisher to inure itself against liability. Using the two-stage model we are able to define regions governing the actions of the publisher based on the ex ante (or perceived) probability that a story is true. The remainder of the paper is organized as follows. The next section discusses the details of the model and derives the probability boundaries that are central to the model and govern the publishing decisions of the newspaper. The subsequent section derives the welfare equation and examines the deviation from the social optimum created by the agency problem. This section also develops the conditions under which policy tools can be used to correct for agency, and discusses policy implications. The penultimate section provides applicable extensions. The final section presents the conclusions.",4
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050009,The Quality–Quantity Trade-off,January 2008,Bryan C McCannon,,,Male,Unknown,Unknown,Male,"The purpose of this note is to extend the standard vertical-product-differentiation model to analyze the trade-off, often present, between the quality of a good and the quantity produced, and to study the implications of such a relationship. The standard model developed by Gabszewicz and Thisse [1979; 1980] and Shaked and Sutton [1982; 1983] implicitly assumes that quality and quantity are independent choices. That is, at any set level of quality, a firm is free to produce as much as it desires. Their models are set up as extensive-form games where the choice of quality is linked to quantity only, through the equilibrium refinement of subgame perfection, by the fact that for each possible choice of quality the firms will set their prices and quantities to maximize their profits. This untested assumption can be rationalized. For example, the blueprints of a factory can be used to make a second factory identical to the first. Thus, quantity can be doubled with no change in the product being produced. While this may often be a reasonable assumption, in some markets it does not hold true. Consider a market for a good with a specialized or rare input. The use of the input cannot be expanded without an effect on the ability to maintain a certain level of quality. Consider, as an example, cigars. Cigars are a luxury good that do much more than just provide nicotine. Aficionados critique cigars on various dimensions where quality is discussed and enjoyed. For example, a popular magazine on cigars provides reviews by numerous professional reviewers. Reviewers, not observing price, brand, or origins of the tobacco, critique cigars based on esthetics, construction, flavor, and strength. A cigar's quality is determined by the environment in which the tobacco is grown and by the skill of the laborers. Furthermore, it is very costly to increase production. In fact, production can only be increased at the expense of lower quality. Making more cigars requires that leaves that would have otherwise been rejected (or used in lower-priced cigars) be used. Using large quantities of nitrogen fertilizer has the effect of growing more and larger tobacco leaves, but these leaves tend to be bitter and bad-tasting.Footnote 1 Also, rolling more cigars may require using less-skilled workers or machines, either of which would reduce the quality of the final product. Thus, there is a distinct link between the quantity and quality of the cigars produced. As another example, consider an instructor teaching a course. As the number of students enrolled increases, the demands on the instructor increase. The ability of the instructor to meet with students outside of class requires either significantly more time invested or a reduction in the time allotted per capita. Projects and examinations must be either eliminated or altered, or the instructor must incur a considerable cost. In both examples, there is an inverse relationship between quality and quantity; as one increases the cost to providing the other increases and, consequently, the amount is reduced. It is this feature of markets for such goods that is considered here. In a similar setup, Sheshinski [1976] allows for a trade-off between quantity and quality. He considers a monopolist determining price, quantity, and quality to understand the distortions caused and the usefulness of regulation. He does not illustrate the effect of the quality–quantity trade-off and does not consider competition, which is done here. In this note, I lay out a model of quality selection in an imperfectly competitive market, taking into account the trade-off between quality and quantity. I show that the stronger this relationship is, the more sales are shifted from the high-quality to the low-quality producer. Furthermore, a stronger relationship between quality and quantity results in the price of all goods being greater. These results extend and revise those of Gabszewicz and Thisse [1979; 1980] and Shaked and Sutton [1982; 1983] for goods that exhibit this trade-off.",7
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050004,Introduction to the Symposium on Ethics,January 2008,Sandra J Peart,David M Levy,,Female,Male,Unknown,Mix,,
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050007,Inducing Greater Transparency: Towards the Establishment of Ethical Rules for Econometrics,January 2008,David M Levy,Sandra J Peart,,Male,Female,Unknown,Mix,,
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050006,Why the Con Hasn't Been Taken Out of Econometrics,January 2008,Martin Zelder,,,Male,Unknown,Unknown,Male,"Most people, including those who are not sure how to calculate a mean, mistrust statistical analysis. Even those among us presumably able to calculate means correctly have their doubts: “Econometricians have found their Philosophers' Stone; it is called regression analysis and is used for transforming data into ‘significant’ results! Deception is easily practised from false recipes intended to simulate useful findings” [Hendry 1980]. Although these concerns have evolved into systematic prescriptive analysis (e.g., Leamer [1978]), an irony remains: despite undoubtedly wide agreement among economists that many econometric estimates are suspect, few among us openly acknowledge our own shortcomings. Moreover, as Levy and Peart [2003] point out, economics/econometrics (unlike, e.g., dental hygiene) lacks even a basic formalized code of ethics to implement appropriate principles to guide estimation. Perhaps one reason for our lack of clear principles that should and do govern econometric estimation is the limited body of economic analysis on the subject. Indeed, the vast majority of writing on econometric practice is didactic/prescriptive, and as such is extraordinarily useful.Footnote 1 Economic analysis of econometric practice, is, however, much rarer, with exceptions found in papers by De Long and Lang [1992], Feigenbaum and Levy [1993a, 1993b; 1996], Fölster [1995], Engers and Gans [1998], and Freedman [2000], although none of these presents an explicit model of choice involving econometricians, journals, and the economics profession. It is, therefore, the task of this paper to attempt to explore such a model, formalizing the insights from the analytical literature noted above while infusing them with the spirit of the economic analysis of fraud and crime [including Becker 1968; Akerlof 1970; Darby and Karni 1973; Matsumura and Tucker 1992]. The orientation of this model is game-theoretic, with relevant choices being made in sequence by three players: an econometrician, a journal, and the rest of the economics profession. The next section defines “cons” in econometrics. The subsequent sections present the structure of the “con” game, examine the equilibria of the game, and assess the normative properties of these equilibria, respectively. The final section offers concluding thoughts.",3
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050005,A Note on Norms in Experimental Economics,January 2008,Daniel Houser,,,Male,Unknown,Unknown,Male,"Many prominent recent contributions to economic theory [Laibson 1997; Camerer 2003; Charness and Dufwenberg 2006] are motivated by the findings of economics experiments, and are then advanced by further experimentation [Houser and Kurzban 2002; Xiao and Houser 2005]. This iterative process indicates a sense of trust both in experiments and in the experimenters who conduct them. This trust hinges on two key norms that guide the design, analysis, and reporting of economic experiments in both the lab and the field. These norms help ensure adherence to the key ethical standard of “transparency” [Levy and Peart this issue]. Norms here refer to informal rules guiding the decisions of scholars who use designed investigations to inform economic hypotheses. The first key norm for experimental economists is simplicity: experimentalists adopt the simplest design that can address their research hypothesis. An experimental design is “simple” if it provides (a) ease of replicability and (b) straightforward interpretations of outcomes. The importance of the former will be discussed further below. The latter point (b) is not meant to suggest that sophisticated econometric or statistical analysis is or should be avoided in experimental economics. Rather, it means that the analysis procedures should be appropriate and defensible given the design, and that the design should be chosen in view of such considerations. The second norm is that all published data and procedures be made publicly available. In particular, the published results must be supported by a discussion that would allow the procedures to be replicated. The procedural discussion includes not just a description of the formal experimetrics procedures [Houser 2008], but also the procedures by which the data were generated. Usually, this entails publishing at least the experiment's instructions, either in the journal or on a website that the published article references.",4
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050012,Post Walrasian Macroeconomics: Beyond the Dynamic Stochastic General Equilibrium Model,January 2008,Georgios Chortareas,,,Male,Unknown,Unknown,Male,,
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050013,Introduction to Post-Keynesian Economics,January 2008,Giuseppe Fontana,,,Male,Unknown,Unknown,Male,,
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050014,Strategies of Commitment and Other Essays.,January 2008,David George,,,Male,Unknown,Unknown,Male,,
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050015,The Next Great Globalization: How Disadvantaged Nations Can Harness Their Financial Systems to Get Rich,January 2008,Eva Marikova Leeds,,,Female,Unknown,Unknown,Female,,1
34,1,Eastern Economic Journal,17 December 2007,https://link.springer.com/article/10.1057/palgrave.eej.9050016,The Bourgeois Virtues: Ethics for an Age of Commerce,January 2008,Tom Tolin,,,Male,Unknown,Unknown,Male,,
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050024,"Tort Reform, Defensive Medicine, and the Diffusion of Diagnostic Technologies",March 2008,Alfredo G Esposto,,,Male,Unknown,Unknown,Male,"Between 1966 and 1993 health care spending in the United States grew at an average annual rate of 12 percent. There was a reprieve during the period 1993–1997, when spending slowed to a rate of 5 percent. However, since 1997 the rate of growth of health care spending has been moving upward once again. Fundamental drivers of medical costs have been and will continue to be an aging population and the expanding technological capability of medicine. The latter has created a dilemma for modern health care in the United States. New technology can improve the quality of health care, but it also produces high health care costs.Footnote 1 Reforms in the health care insurance system have been partially successful in controlling increasing costs due to technological improvements, although the recent increase in health care costs indicates that these may have been one-time savings. The institutional changes did signal, however, the realization by payers that the method of payment can influence the incentives for the development and diffusion of medical technology [Weisbrod 1991; Newhouse 1992, 1993; Cutler and Sheiner 1997; Baker and Spetz 1999]. When health insurance was principally fee-for-service, the insurance system sent the message to the medical technology sector to develop new technologies that enhance the quality of care, regardless of the effect on costs. Under the now dominant prospective-payment mechanism used by managed-care organizations and public programs like Medicare, the message has changed to one that says: develop new technologies that reduce costs, provided that quality does not suffer “too much.” However, recent evidence indicates that this message may not be as effective as first thought. There is another important factor that can at least partially explain the growth of medical technology and identify the reforms that may be needed to keep medical costs in check. I refer to the impact of medical malpractice liability. Under the common law rule of negligence, an individual is liable for the harm his actions inflict on another if he did not take a level of precaution that one would expect from the typical “reasonable” person in a similar situation [US v. Carroll Towing Co., 159 F.2d 169 (2d Cir. 1947)].Footnote 2 In medical malpractice cases, the expected level of care is usually defined by the normal or standard practices of the profession. The problem is the legal rule, as several studies point out, pushes physicians to recommend and administer more medical care than the efficient level of precaution. Economists have labeled this excess care “defensive medicine.” The practice of defensive medicine should also, at least indirectly, influence the demand for medical technology. Furthermore, the demand effect should impact the diffusion of medical technology, as opposed to the rate of innovation. If we enter the mind of a physician, we see strong economic and non-economic pressures to avoid a malpractice claim. But malpractice claims do not come about because she did not use a non-existing technology. Health care professionals are never held liable because they have not created new diagnostic or therapeutic technology. Claims arise because they did not use an existing technology and therefore never detected the presence of an illness or properly treat it. To avoid such claims, our physician will use any and all technology available whenever there is even the smallest probability of a positive expected net-benefit from its use. Interestingly, in spite of the obvious connection, there are no studies on the impact of medical malpractice liability on the diffusion of medical technology. Studies of the effect of medical malpractice liability law on the treatment choices of physicians or on the overall level of expenditures do exist [Danzon 1985; Weiler et al. 1993; Kinney 1995; Kessler and McClellan 1996; Baicker and Chandra 2005]. But none of these examines the effect of medical malpractice liability on the use and diffusion of medical technology. This gap implies that we have only a partial understanding of the factors behind the use and diffusion of medical technology, which can lead to a misallocation of precious medical resources away from technologies and procedures that provide benefits significantly worth the costs and toward those where the principle benefit is a reduction in a hospital's or physician's potential legal liability. It also implies that we are missing an important alternative test of the economic theory of defensive medicine and of the effectiveness of tort law reform. The latter has become a major public policy issue. As physician groups continue to complain that medical malpractice litigation has pushed their malpractice insurance premiums to levels that have forced many of them to leave their practice or move to a different state, demands for tort law reforms can be heard almost daily. Thus this article's aim is two-fold. First, it attempts to discover whether the quantity of diagnostic technology is affected by the practice of defensive medicine, and in the process provide an alternative test of the existence of defensive medicine. Second, it additionally attempts to discover whether frequently proposed tort law reforms have any impact on the level of defensive medicine, by comparing the differences in the quantity of diagnostic technology in states with tort reform to those without.",3
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050008,"Joint Determination of Regulations by the Regulator and the Regulated: Commercial Bank Reserve Requirements, 1875–1979",March 2008,Jac C Heckelman,John H Wood,,Male,Male,Unknown,Male,"As noted many times since Stigler's [1971] landmark publication on capture theory, regulations are determined during the course of interactions between regulators and the regulated. Regulations impose costs and carry benefits for both sides of the transaction, which have incentives to attempt to influence them. Commercial banking is a potentially fruitful area of study of the determination of many of the details of regulation because banks have the ability to choose between regulators. The structure of bank regulation provides measurable supplies and demands for regulations. Banks may select a national charter and be regulated by the Comptroller of the Currency or a state charter under the regulation of the state banking authority. Charter flips are easy and frequent [Whalen 2002]. Only very small banks that do not meet the minimum capital requirement of national banks are unable to switch, and mergers provide exceptions even in this case. State banks date from the birth of the republic whereas national charters have been issued since the National Bank Acts of the Civil War. Beginning in 1914, Federal Reserve membership was conferred/imposed on national banks but was made optional for state banks. The costs of national charters have exceeded those of state charters, especially in their higher cash reserve requirements, but so have their benefits, especially for the larger (reserve city) banks, who could serve as reserve depositories for smaller (country member and sometimes state non-member) banks, and also because they made more use of the discount window and other Fed services. These costs and benefits are determined by the interactions of the regulators and the regulated. A bank presumably chooses the regulator that contributes the most to its risk/return goals. Regulators also have self-interested goals, including membership size and its corresponding revenue [Toma 1999], reinforced for the Comptroller and the Fed by their members' support of the national debt (bond purchases and high cash reserve requirements). In addition, Federal Reserve officials have indicated that the System's effectiveness is directly related to the proportion of the banking system that is under its regulation.Footnote 1
 Studies of national and state memberships suggest that, at least for some periods, their proportion has been sensitive to relative costs, especially the interest costs of differential reserve requirements [Mayne 1967; White 1983].Footnote 2 On the other hand, those costs have responded to charter flips as the authorities have tried to acquire and retain members [Starleaf 1975]. This is appropriately a joint-estimation problem, which is addressed below. Our goal here is to examine the relations between regulators and their membership sizes from (soon after) the end of the Civil War until identical reserve requirements were imposed on all banks regardless of regulator in 1980. Banks do not stop at choosing their least-cost regulator. They also try to influence those costs. This is analogous to Tiebout's [1956] “Pure Theory of Local Expenditures,” by which political entities, like private clubs, tend to attract distinct, fairly homogeneous groups, who in turn try to influence the rules. This theory of club behavior has since been applied to various organizational structures including international alliances, interest groups, political coalitions, law firms, and religions [Anderson et al. 2001]. We extend this literature by studying competitive regulations between state and national charters for commercial bank membership. Rival regulators are directly analogous to competing clubs supplied on the private markets. State banks consider the benefits and costs of joining the national system and being subject to its regulations, compared with the benefits and costs of remaining outside the system and being subject to state regulations. As the relative benefits and costs change, state banks can vote with their feet as predicted from club theory by changing the status of their national membership. In this study, we investigate how state banks sorted themselves under changing membership cost conditions, as measured by reserve ratios and interest rates. Capture theory predicts that membership costs would be endogenous to membership rates. We investigate these relationships in a system of simultaneous equations and find both effects to be influenced by legislation creating the Federal Reserve in 1913, and a subsequent increase in its regulatory power through New Deal legislation in 1935. The paper is organized as follows: the history of regulatory costs and memberships in the first section describes the events that supply the data for the estimates of the simultaneous-equations model developed in the second section. The last section summarizes the results and discusses their implications for further work.",2
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050010,Transfer College Quality and Student Performance,March 2008,Angela K Dills,Rey Hernández-Julián,,Female,Unknown,Unknown,Female,"College is growing more expensive; inflation-adjusted tuition and fees increased more than 80 percent between 1976 and 2004 and about 44 percent between 1990 and 2004.Footnote 1 Responding to this trend, many students choose to save money by completing some part of their college credits at lower quality institutions before completing their degree program at higher quality institutions. Nationally, about 50 percent of undergraduates attend more than one institution of higher education [Adelman 1999; NCES 2003a, Table 21-1]. Not only are transfer students a large part of the college student population, they are becoming increasingly common. The number of undergraduates attending more than one institution of higher education increased from 35.6 percent of students in the 1970s to 51.8 percent in the early 1990s [Adelman 1999]. We consider how the quality of the school from which a student transfers affects student performance in the receiving school. We measure college quality using the percent of students admitted, the student faculty ratio, listed tuition, the percentage of professors with Ph.D.s, and, when available, average SAT scores. In particular, we examine how students who transfer introductory-level course credits perform in the subsequent intermediate-level course at a public, 4-year Research I university, Clemson University. We find that higher quality schools are better at generating human capital. Students taking introductory courses at higher quality institutions earn higher grades in their intermediate course than students from lower quality institutions. This difference is small, but statistically significant. Any analysis estimating the benefits of higher quality institutions must account for selection bias arising from unobservably better students choosing to attend better universities [Behrman et al. 1996]. We include a rich set of students’ academic traits. The estimates are robust to the inclusion of additional controls for student academic attributes suggesting that selection bias is not a significant issue. Measurement error in college quality may attenuate estimates, and so we follow Black and Smith [2006] and use two-stage least squares (2SLS) to reduce this bias. Many of the existing studies use post-graduation wages as a criterion to estimate the return to college quality.Footnote 2 These estimates combine the returns to human capital with the return to signaling. The return to signaling arises from information conveyed by the student being admitted to the school and completing the degree. Weiss [1995] argues that the signaling value of education comprises a large fraction of the return to schooling. Using grades avoids any return to signaling and focuses on the human capital return.Footnote 3 Grades as an outcome variable present an additional advantage: in contrast to studies by Monk-Turner [1994] and much of Dale and Krueger [2002], which focus on college graduates, this study observes students during their college careers and includes the ones who may never graduate. Focusing on student grades in one institution allows the measure of performance to be more comparable than comparing grades across many institutions. We find that higher quality institutions provide a small increase in student grades. The higher quality schools in our sample charge only slightly higher tuition. Given estimated labor market return to grades from Datcher-Loury and Garman [1995] of close to 10 percent, the increased grades more than compensate for the increased tuition at higher quality schools.",6
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050026,Exchange Rate Fluctuations and the Macro-Economy: Channels of Interaction in Developing and Developed Countries,March 2008,Magda Kandil,,,Female,Unknown,Unknown,Female,"Recent episodes of currency crises have focused attention on the importance of exchange rate fluctuations and the appropriate exchange rate policy. The 1990s were the years of currency turmoil, characterized by the near breakdown of the European Exchange Rate Mechanism in 1992–93, the Latin American tequila crisis following Mexico's peso devaluation in 1994–95, and the severe crises that swept through Asia in 1997–98. Exchange rate fluctuations are likely to determine economic performance. Demand and supply channels determine the interaction between the macro-economy and fluctuations in the exchange rate. A depreciation (or devaluation) of the domestic currency may stimulate economic activity through the initial increase in the price of foreign goods relative to home goods. By increasing the international competitiveness of domestic industries, exchange rate depreciation diverts spending from foreign goods to domestic goods. As illustrated in Guitian [1976], and Dornbusch [1987], the success of currency depreciation in promoting trade balance largely depends on switching demand in the proper direction and amount, as well as on the capacity of the home economy to meet the additional demand by supplying more goods.Footnote 1
 There is a theoretical possibility, however, that currency depreciation will be contractionary. Currency depreciation gives with one hand, by lowering export prices, while taking away with the other hand, by raising import prices. If imports exceed exports, the net result is a reduction in real income within the country.Footnote 2
 Supply-side channels further complicate the effects of currency depreciation on economic performance. Bruno [1979] and van Wijnbergen [1989] postulate that in a typical semi-industrialized country where inputs for manufacturing are largely imported and cannot be easily produced domestically, firms’ input cost will increase following a devaluation. As a result, the negative impact from the higher cost of imported inputs may dominate the production stimulus from lower relative prices for domestically traded goods. To summarize, the effects of currency depreciation are ambiguous and dependent on the relative effects of supply and demand channels.Footnote 3
 Previous research Kandil [2006] has illustrated a more dominant role of the supply-side channel relative to the demand channel in determining the effects of exchange rate shocks on output and price in developing countries. Accordingly, currency depreciation decreases the output supply, decreasing real growth and increasing price inflation. This is consistent with the high dependency of developing countries on imported inputs in the production process. In contrast, the demand side channel appears more dominant in determining the effects of currency fluctuations on competitiveness and, in turn, output growth and price inflation in industrial countries.Footnote 4 Accordingly, depreciation of the exchange rate increases aggregate demand, increasing real growth and price inflation. To shed additional light on the transmission of exchange rate shocks, this paper analyzes channels of interaction between exchange rate fluctuations and the macro-economy in a sample of developed and developing countries. While the contractionary and inflationary effects of devaluation have been subject to extensive research, there has been no attempt to study the transmission channels underlying the adjustments of specific demand-side components to currency fluctuations. Specifically, the paper looks at the effects of unanticipated fluctuations in the exchange rate on components of aggregate demand.Footnote 5 The evidence illustrates patterns that differentiate, in general, between developing and industrial countries. Based on the general patterns that emerge from the analysis of time-series data within countries, the paper summarizes differences that characterize developing and industrial countries. The remainder of the paper is organized as follows. The second section provides a theoretical background for empirical investigation. The third section presents empirical models and the fourth section IV presents empirical results. The last section concludes.",3
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050029,Working and Educated Women: Culprits of a European Kinder-Crisis?,March 2008,Elizabeth A DiCioccio,Phanindra V Wunnava,,Female,Unknown,Unknown,Female,"In recent decades, a growing number of developed countries have experienced dwindling fertility rates. Decreasing fertility rates in such countries can potentially be attributed to the evolution of greater opportunities for women in industrialized nations. It is conceivable that the responsibilities associated with childbearing have become increasingly difficult to manage as the opportunities in higher education and careers have grown for women. As the development of opportunities for women is among the greatest indicator of social progress for developed countries in recent years, such opportunities should continue to be strongly promoted. However, if this social progress is significantly contributing to lower fertility rates, countries with shrinking populations may also find these trends worrisome. Hence, the aim of the empirical testing in this study is to determine whether female labor force participation is a principal culprit of lower fertility rates in European Union (EU) countries. Previous studies that have tested this relationship have used data that extended back at least into the 1970s [Becker and Lewis 1973; Willis 1973; Engelhardt et al. 2001; Del Boca 2003]. When using this time frame, it is more likely that workforce participation will negatively influence fertility decisions, as in this early time period, government and firm fertility programs were less common. However, two recent longitudinal studiesFootnote 1 based on a sample of developed countries by Ahn and Mira [2002] covering 1970–1995, and by Adsera [2004] covering 1960–1997 have already detected a changing relationship between fertility and female labor force participation rates. Specifically, both these studies found that since the late 1980s there exists a positive relationship between these two measures, as more targeted programs have since been developed to mitigate the effects of childbearing. Hence, one of the main objectives of this study is to investigate whether these positive trends could still be detected by extending the sample years to include the current millennium and also identify other significant variables that may impact European fertility. Accordingly, such an investigation will provide critical information to the respective governments, enabling them to design appropriate policies to alleviate lower fertility among European women.",2
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050031,Multifactor Asset Pricing Model and Stock Market in Transition: New Empirical Tests,March 2008,Miroslav Mateev,Atanas Videv,,Male,Male,Unknown,Male,"This paper studies empirically the role of different economy-wide factors or macroeconomic variables in explaining excess stock returns in emerging markets. The empirical linkage between macroeconomic variables and capital markets has been widely investigated in the economic literature. For instance, Fama [1981] finds strong evidence of a negative relationship between inflation and stock returns. In addition, Schwert [1989] finds evidence that stock volatility is related to the level of economic activity and that stock market volatility increases during recessions. Other macroeconomic variables that have been found to influence stock returns are industrial production, dividend yields and interest rates; variables typically used in the estimation of the business cycle. Chen et al. [1986] use a set of five relevant macroeconomic indicators to paint a broad picture of the macro-economy. Jagannathan and Wang [1996] include the “human capital” factor in their cross-sectional regression model and assume that the return on human capital is an exact linear function of the growth rate in per capita labor income. Fama and French [1996] present an alternative approach to specifying macroeconomic factors using firm characteristics that seem, on empirical grounds, to represent exposure to systematic risk.Footnote 1
 Another strand of research papers offers insights into the predictive power of traditional macroeconomic variables. Campbell [1987] finds that Treasury bill rates and several measures of the term spread can explain a substantial fraction of the variation in the following month's excess stock returns. Unfortunately, over longer horizons spanning the length of a typical business cycle, stock returns have typically been found to be only weakly forecastable. Lettau and Ludvigson [2001] study the role of transitory deviations from the common trend in consumption, asset wealth, and labor income for predicting stock return fluctuations. They find that these “trend deviations” are a strong univariate predictor of both real stock returns and excess returns over a Treasury bill rate, and can account for a substantial fraction of the variation in future returns. Groenewold and Fraser [2000] estimate a beta forecasting model using a set of economy-wide factors or macroeconomic variables, which may affect systematic risk as measured by betas. The variables chosen are broadly similar to those used in other studies of the macro-factor arbitrage-pricing models [Chen et al. [1986] for the US; Clare and Thomas [1994] for the UK; Martikainen [1991] for Finland; Groenewold and Fraser [1997] for Australia]. Their choice of macro-variables is based on the hypothesis that at the aggregate level risk is influenced by three classes of factors: real domestic activity, nominal domestic factors, and foreign factors. The empirical studies cited above concentrate mostly on well-developed markets, perhaps because data are readily available. Emerging markets, in contrast to developed markets, are characterized by volatile but substantial returns that can easily exceed 100 percent per annum. These returns, while substantial, are subject to increased risk and volatility, and are significantly reduced by the increased illiquidity of trading stocks in emerging markets relative to more developed markets. The usual approach to address the liquidity concerns for emerging markets is to use proxies. Amihud et al. [1997] and Berkman and Eleswarapu [1998] use trading volume as a liquidity proxy for the price impact to explain return differentials in their studies on emerging equity markets. Dahlquist and Robertsson [2001] use turnover as a liquidity proxy in tests of the association between foreign ownership and the market liquidity of a firm's shares, and Rouwenhurst [1999] uses turnover in examining emerging market return premiums. Lesmond [2002] applies a limited dependent variable (LDV) model of security returns to provide liquidity estimates for 31 emerging markets.Footnote 2 Bekaert et al. [2002] use two measures of liquidity (or proxies for transaction costs): average market turnover and average incidence of zero daily returns in their study of 19 emerging equity markets. Evidence suggests a strong relationship between excess returns and liquidity measures. Amihud [2002] finds evidence that over time, the expected market liquidity of stocks has a positive effect on the ex ante excess return. As yet, however, there is little empirical evidence to determine whether the traditional macroeconomic variables play a significant role in explaining stock return variations in emerging markets. We believe that we have developed an important research vehicle to advance a better understanding of the economic transition. As capital markets in transition are in the early stages of development and are characterized by high price volatility, we expect a (weak) linkage between macroeconomic variables and stock market returns. We use Bulgaria as a good example of a transition economy with strong growth potential and stable macroeconomic performance. The implemented structural reforms were key factors to invigorate the supply side, including greater labor market flexibility, and advancing privatization and improving the overall business climate in the country. EU membership from January 1, 2007 is expected to be a strong incentive for further economic development through a steady increase of foreign direct investment, improved institutional environment, and better public services. This, in turn, will result in improved capital market performance and a more predictable relationship between macroeconomy and stock returns. We use a multifactor framework of key macroeconomic variables that might proxy for relevant systematic factors in order to determine whether the variation in equity returns is related to the level of economic activities. We begin by studying the relationship between the excess returns (the returns over short-term T-bill rates) on individual stocks and a set of macroeconomic variables. The results show that the relationship is weak and the macroeconomic factors virtually play no role in explaining the stock return fluctuations. The explanatory power of the model is enhanced when individual stocks are combined into portfolios. We apply the two-pass regression procedure following the Chen et al. [1986, 383–403] approach. First, we group the sample stocks into portfolios and use 3 years of monthly return data to estimate the factor betas of these portfolios. Then using the first-pass estimates of betas as independent variables, we estimate the second-pass regression. We run this second-pass regression for every month of the sample period (5 years), re-estimating the first-pass factor betas each month. Using time-varying betas (instead of stationary betas where estimates are downward biased because of the infrequent trading in thin markets such as the Bulgarian equity market) enhances the explanatory power of the second-pass regression. We find a number of relevant risk factors that play a significant role in explaining the variations in stock market returns. The rest of the paper is organized as follows. The next section presents the data set and describes the theoretical framework of our analysis. The subsequent section details the econometric model we estimate and presents some statistical tests and then the further section presents the results of the two-pass regression procedure and the main findings in this article. Some concluding remarks are offered in the final section.",5
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050032,Monetary Policy and Labor Market Transitions,March 2008,James R Fain,Timothy O Bisping,,Male,Male,Unknown,Male,"Studying the impact of monetary policy on economic aggregates is not a novel concept. For instance, it has been shown that monetary policy is likely to impact the unemployment rate. Yet, little is known of precisely how changes in monetary policy are translated into changes in the unemployment rate. Here, we provide evidence that shifts in monetary policy cause changes in labor market transition probabilities. To accomplish this, we use vector autoregressive (VAR) techniques similar to those used in the literature regarding the relationship between monetary policy and the unemployment rate. These changes in transition probabilities in turn cause a change in both the current unemployment rate and the equilibrium unemployment rate. We thus identify the mechanism by which monetary policy impacts the unemployment rate. The outline of this paper is as follows: we review the literature on the relationship between monetary policy and unemployment and then review the literature on using Markov chains to model labor force transitions. From there we develop our hypotheses and estimate the relationship between monetary policy and labor market transition probabilities and use this information to investigate the impact of monetary policy on the equilibrium unemployment rate. Several authors who use a variety of methods to discern the impact of monetary policy have provided evidence regarding its effect on unemployment. Here we focus on those authors whose method most closely resembles that of our own. Both Thorbecke [2001] and Zavodny and Zha [2000] use a standard VAR model to study the impact of monetary policy. Thorbecke considers the effect of a shock in the federal funds rate on the unemployment rate of racial groups in order to reveal possible differences in the impact of such policy. His model is fashioned after Christiano et al. [1994], and includes the following variables: the industrial production index, inflation, the log of the commodity research board spot price index, unemployment disaggregated by race, the federal funds rate, nonborrowed reserves, and total reserves. Through the use of VAR techniques, and the resulting impulse response functions (IRFs) generated by a one standard deviation shock (54 basis points) in the federal funds rate, he finds the maximum response to be 0.09 percentage points for white unemployment, 0.17 percentage points for Hispanics, and 0.14 for blacks. Thorbecke also finds that the maximum response typically occurs between 1 and 2 years after the shock. Zavodny and Zha [2000] perform a similar study and find evidence regarding the impact of monetary policy that, in terms of timing and magnitude, is reasonably consistent with that of Thorbecke. They also consider differences by race, and they include an analysis of relative differences, as opposed to absolute differences. A Markov chain describes the transitions between a finite number of possible states. The state vector evolves over time according to the equation S

t+1=PS

t
, where P is an n × n matrix called a transition matrix. Each month the BLS works to classify the members of the Civilian Noninstitutional Population into one of three states: employed (E), unemployed (U), or not in the labor force (NLF). People frequently transition from one state to another. The labor force status of the population evolves over time according to the following Markov chain:   Each P

ij
 element of the above matrix represents the probability (per month) of transitioning from state i into state j. For example, Pue is the probability of leaving unemployment for employment. Each column in the transition matrix must sum to one, and so there are only six independent transition probabilities. In all that follows, we work with the off-diagonal terms and compute the diagonal terms by invoking the restriction that the columns must sum to one. For our purpose it is convenient to normalize the state vector so that it sums to one. Doing this allows us to use some well-known properties of regular Markov chains because we can ignore the fact that the Civilian Noninstitutional Population grows over time. For some labor force subjects, this would be inappropriate; however, this normalization does not affect the unemployment rate, which is the subject of our work. Consequently, nothing is lost and much is gained by normalizing the state vector. Consider the regular Markov chain defined by equation (1). If the state vectors are normalized and the transition matrix P is constant over time, this system will eventually converge to an equilibrium vector S
* that satisfies the equation S
*=PS
*. When the system is at equilibrium there are substantial gross flows between states in each period, but the flows into and out of each state are exactly equal so the net flow between states is zero and the state vector is unchanged from one period to the next. The elements of S
* are completely determined by the parameter values in the transition matrix. The equilibrium-normalized values of employment, unemployment, and NLF are, respectively, given by   where D=(Pne+Pnu) (Peu+Pue)+Pun(Peu+Pne)+Pen(Pnu+Pue+Pun). The equilibrium unemployment rate is given by (Pen Pnu+Peu(Pne+Pnu))/(Pen Pnu+(Pne+Pnu) (Peu+Pue)+Pne Pun. These values correspond to the equilibrium values reported in Marston [1976] and Ehrenberg [1980]. Note that these terms use no information other than the six off-diagonal elements of P. We demonstrate below that monetary policy alters these six transition probabilities. Since these probabilities completely determine the equilibrium state of the labor force, changing them causes the equilibrium vector to change. This is the mechanism by which monetary policy produces changes in the unemployment rate. 
Marston [1976] and Ehrenberg [1980] provide the basic analysis of Markov chains and transition probabilities applied to labor force transitions. DeBoer and Seeborg [1989] and Howe [1990] examine differences in transition probabilities for men and women. Moser [1986] finds trend, cyclic, and seasonal effects in the transition probabilities, while Blanchard and Diamond [1990] look for trend, cyclic, and seasonal effects in the gross flows. Shimer [2005] uses data from US from 1948 to 2004 to determine the nature of transition probabilities as related to the business cycle and finds that the probability of finding a job is procylical, but that job separation is virtually acyclical. Further, Trigari [2004] develops a general equilibrium model to further explain labor market responses to monetary policy shocks. Although Blanchard and Diamond do not use Markov chains, this work is similar to theirs in many ways. These authors estimate a VAR model with three variables: employment, unemployment, and job vacancies. Using IRFs, they examine how these variables respond to a shock in aggregate economic activity. They also estimate the relationship between their three variables and the gross flows between the labor force states. They can then estimate the relationship between the shock to aggregate economic activity and the gross flows, using the IRFs as the link between the two. Below we use IRFs as the link between a Markov chain model of labor force transitions and a VAR model that investigates the impact of monetary policy. While the precise impact of monetary policy on labor force transition rates is currently unknown, economic theory gives some guidance regarding what we might expect. Recall that the six transition probabilities considered here are Peu, Pue, Pnu, Pun, Pne, and Pen. For several of these, the impact of monetary policy would seem obvious. We would expect, for instance, that an increase in the federal funds rate would increase the probability of job loss, thus raising Peu. Similarly, such a policy makes it more difficult for those unemployed to find work, which lowers Pue. The transition rate Pnu is closely related to the commonly cited “added worker effect.” As the state of the economy deteriorates, some household members who were previously not in the labor force now find it necessary to enter, although finding employment may be difficult. Thus, we anticipate that an increase in the federal funds rate would raise this transition probability. On the other hand, the discouraged worker effect causes us to believe that an increase in the federal funds rate will increase Pun. That is, as it becomes more difficult to find employment, some unemployed individuals will become discouraged and drop out of the labor force. The effects on the Pne and Pen transition rates, as related to monetary policy, are slightly less clear. A substantial number of people transitioning from employment to NLF are retiring. This activity is likely not affected by changes in monetary policy, and so the impact of monetary policy on Pen may be negligible. If there is an impact, an increase in the federal funds rate likely will lower Pen because individuals will have a lower propensity to leave their job voluntarily and exit the labor force as jobs become harder to find. Exactly how Pne will respond to a positive shock in the federal funds rate is also unclear. While new people are likely to seek employment in response to a positive shock (similar to the added worker effect), the probability of successfully securing employment falls. The outcome therefore depends on the interaction of these two effects. We consider the net effect to be an empirical issue.",
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050034,Exploring a US Immigrant–Intra-Industry Trade Link,March 2008,Roger White,,,Male,Unknown,Unknown,Male,"A link between immigrants and trade flows has been documented for several nations; however, most researchers have used ad hoc approaches without departure from standard trade theory, implicitly assuming that trade arises due to differences in resource endowments. Immigrants are hypothesized to increase trade as connections to home country business and social networks decrease transaction costs. Additionally, immigrants may increase trade if they arrive possessing preferences for certain products and find neither the desired products, nor acceptable substitutes, available. Intra-industry trade (IIT) involves both final consumer goods and intermediate products and, as such, may increase through either a preference effect or a network effect. We examine this possibility and, in doing so, extend the IIT literature by considering immigration as a potential determinant. Simultaneously, the immigrant-trade literature is extended via investigation of a link between immigration and IIT. As most IIT takes place between developed nations that have similar industrial structures and international migration frequently involves a lesser-developed home country and a developed host country, the “north–north” direction of IIT and the “south–north” direction of international migration may suggest that immigrants add little, if at all, to IIT. Many immigrants, however, maintain close ties to their home country. An NPR/Kaiser/Kennedy School [2004] poll finds that 41 percent of immigrants return to their home country at least every year or two; 37 percent regularly send money to their home country; and 30 percent want to move back to their home country someday. The maintenance of these ties underlies documented immigrant–trade links and, when considered in conjunction with potential information asymmetries or demand that is not sated, provides a rationale for immigrants to possibly influence IIT. 
Gould [1994] first reports an immigrant–trade link for the US, positing that immigrants are more likely to add information related to consumer goods than to producer goods. Subsequent research has reported links between immigration and aggregate trade flows for several nations. For example, Wagner et al. [2002], Head and Ries [1998], and Helliwell [1997] each examine the Canadian immigrant–trade link. Blanes [2003], Piperakis et al. [2003], and Bryant et al. [2004] examine links for Spain, Greece, and New Zealand, respectively. A positive influence of immigrants on US state exports has also been established. Bandyopadhyay et al. [2006] provides a review of such studies. In all cases, aggregate trade flows have been used to identify immigrant–trade links. 
Girma and Yu [2002], classifying trading partners based on current or past “commonwealth” status, find that immigrants from “non-commonwealth” countries are responsible for the UK immigrant–trade link. It is thought that such immigrants increase trade as their host countries are relatively dissimilar to the UK (increasing imports via the preference effect) and as a result of business and social connections (i.e. network effects) to their home countries. Examining Danish data, White [2006] reports that immigrant–trade links are of greater magnitude for trade with higher income countries and for trade in differentiated products. A similar result is reported for the US; however, immigrants from low-income countries drive the link [White 2007]. Thus, variation in immigrant–trade links also appears to exist across host countries, potentially due to cultural and/or institutional dissimilarity between host nations and immigrants’ home countries. We augment the model developed in Hummels and Levinsohn [1995] such that immigrant stocks are considered as a determinant of IIT. This follows directly from Blanes [2005], who, examining the influence of immigrants on aggregate Spanish IIT, reports that immigrants from OECD member nations increase both IIT in non-manufacturing and manufacturing goods while immigrants from non-OECD member nations increase only manufacturing goods IIT. Blanes and Martin-Montaner [2006], also using data for Spain, report variation across immigrant types in the immigrant–marginal IIT relationship. While limitations of the US data preclude analysis of immigrant characteristics, we are able to consider the effects that immigrants may have on the aggregate level of IIT and on both vertical (VIIT) and horizontal intra-industry trade (HIIT). We adopt the baseline model used in Blanes [2005]; however, as White [2007] finds that home country development level is an important determinant of US immigrant–trade links, we examine finer variation in immigrant–IIT links by including a vector of lesser-developed country (LDC) dummy variables [World Bank 2003] that are interacted with immigrant stock variables.Footnote 1
 The analysis indicates that immigration increases the level of IIT and that, considering potential variation in immigrant–IIT links across home country income classifications, immigrants from lower income countries appear to influence IIT, VIIT, and HIIT all to a greater degree than do immigrants from higher income countries. Examining VIIT and HIIT separately, HIIT is consistently more sensitive than is VIIT to changes in immigrant stocks. This finding is consistent across all home country income classifications. VIIT is characterized as trade in products at different stages of production, while HIIT implies trade in goods at similar stages of production. The greater proportional effect of immigrants on HIIT suggests that both preference effects and network effects may be acting to influence IIT. The paper proceeds as follows: The next section presents the theoretical intuition and estimation equation. The subsequent section discusses the data and variable construction, while the fourth section presents the findings. The final section concludes.",8
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050020,"AIDS and the Ecology of Poverty, by Eileen Stillwaggon",March 2008,Nicoli Nattrass,,,Unknown,Unknown,Unknown,Unknown,,
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050021,"The Samaritan's Dilemma: The Political Economy of Development Aid, by Clark C. Gibson, Krister Andersson, Elinor Ostrom, and Sujai Shivakumar",March 2008,Cecilia Ann Winters,,,Female,Unknown,Unknown,Female,,
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050022,"The Economics of U.S. Health Care Policy: The Role of Market Forces, by Frank W. Musgrave and M.E. Sharpe",March 2008,Michele J Siegel,,,Female,Unknown,Unknown,Female,,
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050018,"Shaking the Invisible Hand: Complexity, Endogenous Money and Exogenous Interest Rates, by Basil John Moore",March 2008,Matías Vernengo,,,Male,Unknown,Unknown,Male,,
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050023,"Economics for Humans, by Julie A. Nelson",March 2008,Elissa Braunstein,,,Female,Unknown,Unknown,Female,,
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050025,"The Reluctant Economist: Perspectives on Economics, Economic History, and Demography, by Richard Easterlin",March 2008,Ann Davis,,,Female,Unknown,Unknown,Female,,
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050030,"Global Migration and the World Economy: Two Centuries of Policy and Performance, by Timothy J. Hatton and Jeffrey G. Williamson",March 2008,Lisa Mohanty,,,Female,Unknown,Unknown,Female,,
34,2,Eastern Economic Journal,10 April 2008,https://link.springer.com/article/10.1057/eej.2008.2,List of Reviewers 2007,March 2008,,,,Unknown,Unknown,Unknown,Unknown,,
34,3,Eastern Economic Journal,09 June 2008,https://link.springer.com/article/10.1057/eej.2008.13,The Relationship Between Health and Schooling,May 2008,Michael Grossman,,,Male,Unknown,Unknown,Male,"Many studies suggest that years of formal schooling completed is the most important correlate of good health. This finding emerges whether health levels are measured by mortality rates, morbidity rates, self-evaluation of health, or psychological indicators of health and whether the units of observation are individuals or groups. There is much less consensus as to whether this correlation reflects causality from more schooling to better health. The relationship may be traced in part to reverse causality since a longer life expectancy increases the payoffs to investments in schooling and since healthier students may attend school for longer periods of time. The relationship may also reflect “omitted third variables” that cause health and schooling to vary in the same direction. The past three and a half decades have witnessed the development of a large theoretical and empirical literature focusing on the issue just raised. I deal with that literature and what can be learned from it in this paper. Much of my paper deals with the empirical literature on the relationship between an individual's own health and own schooling or between child health and parents' schooling. To motivate this discussion, I examine time-series data on completed schooling, infant mortality, and age-adjusted mortality in the United States from the early 1900s to the present in the next section. I then outline conceptual frameworks that generate causal relationships from health to schooling and from schooling to health in the subsequent and further sections, respectively. In the penultimate section, I call attention to the role of “third variables.” These are variables that may cause health and schooling to vary in the same direction and are difficult to measure. In the three sections just mentioned, I address relevant empirical evidence. My emphasis is on studies that try to establish causality and to some extent on the difficulty of this undertaking. I cannot deal with the many contributions made to this literature in the past three and a half decades in the space allotted to me in this paper. Therefore, my strategy is to select several older papers that point to or question causal effects and several very recent ones that try to establish causality with refined econometric techniques. The reader is referred to Grossman [2006] for a more comprehensive review of the literature.",49
34,3,Eastern Economic Journal,09 June 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050037,Tinbergen Rules the Taylor Rule,May 2008,Thomas R Michl,,,Male,Unknown,Unknown,Male,"A central bank reaction function implies that the monetary authorities recognize that the inflation process depends on a Phillips curve-like relationshipFootnote 4 and that the rate of capacity utilization depends on the interest rate, through an IS-curve. We will model the inflation process (with p representing the inflation rate) as a function of the rate of capacity utilization, u,   For convenience, we define the normal or desired rate of utilization as unity, and suppress time subscripts where they can easily be inferred. Note that normal utilization does not represent full utilization in an engineering sense. Firms are assumed to build capacity slightly ahead of demand, for example so as to accommodate fluctuations in orders without losing customers. The assumption is that they will respond to high demand partly by stepping up production and partly by raising prices faster in the next period. Inflation thus has an inertial element, perhaps because of expectations-formation or some other slow process. Most textbook presentations of the Phillips curve (often unwittingly) make the implicit assumption that full employment in the labor market and normal capacity utilization (full employment of capital) correspond. In other words, they presume that sufficient capital has accumulated to make full employment possible; the well-behaved neoclassical production function is one device for achieving this legerdemain. By contrast, this paper is preoccupied with getting right the relationship between these two measures of slack. Money wages are assumed to respond to prices one-for-one so that their ratio, the real wage, remains constant.Footnote 5 Thus, the distribution of income is parametric. We will use π to represent the profit share. For simplicity, we assume, without loss of much generality, that workers live hand-to-mouth and consume their real wage, w=(1−π)x, where x is labor productivity. We will assume that changes in utilization of capacity leave output-per-worker unchanged and they are fully reflected in the output-capital ratio; growth theorists call a technical change Solow-neutral when it conforms to this pattern. Empirical evidence suggests this is not too far from the truth, although labor hoarding and other effects may cause violations in practice. The monetary authority operates according to a fixed Taylor-type rule, or a central bank reaction function. Its policy instrument is the real interest rate, R. This is not precisely the rule Taylor [1999] had in mind; he works with the nominal interest rate, for example, which places the well-known restriction that the inflation-coefficient needed for stability must exceed unity. And his rule targets the output gap (derived from the full employment level of output), rather than employment itself, although that does not seem a very substantive difference. We will take liberty on occasion and loosely refer to the reaction function as the “Taylor Rule.”Footnote 6 Because of decision and implementation lags, we assume that the central bank responds to current conditions by setting the prevailing rate for the next period. One good justification for this lag is that central banks control only the short-term interest rate on interbank lending while the long-term rates that govern investment spending change much more slowly as expectations are digested by financial markets. We write the central bank reaction function in the most general form used in this paper as:   where R
n
 is the inflation-neutral rate of interest, e is the employment rate, and bars identify target values. To obtain an IS curve, we make use of an investment equation that is the workhorse of neo-Kaleckian modeling. Investment is responsive to the degree of utilization, on the grounds that high utilization signals that demand is expanding faster than capacity. This equation can also be interpreted as an error-correction response function for investment, sensitive to deviations from the normal rate of utilization.Footnote 7 We will include the interest rate, on the grounds that investment that cannot be financed through internally generated funds (profits) will be sensitive to credit conditions. One could also appeal to standard neoclassical theory to justify the role of the interest rate, making our investment equation reasonably catholic. Normalizing by the capital stock, we have  For simplicity, we will assume that a constant proportion, s, of profits is saved. Thus, saving normalized by the capital stock is   where ρ is the output-capital ratio (at normal utilization), sometimes referred to as capital productivity. The short run is assumed to be long enough to permit changes in utilization that eliminate any excess demand in the product market. Equating planned investment and saving, we obtain the IS curve   where c=sπρ−d2 represents the marginal excess saving generated by an increase in utilization. Stability of the short-run adjustment mechanism (i.e., the multiplier) requires that c>0, and we will assume that this condition prevails. We also assume that the monetary authority knows the structure of the IS curve, and can determine that the inflation-neutral rate of interest (giving u=1) is  While this assumption is not without precedent in the inflation-targeting literature, as Carlin and Soskice [2006] make clear, it might also be possible to replace it with some kind of error-correction routine that would further condition the central bank reaction function, as in Setterfield [2004]. In simulation exercises below, we will simply assume that the central bank learns of any change in the neutral rate with a delay. Having determined the utilization rate in any short-run period through the IS equation, the rate of capital accumulation, g, can be obtained from either the investment or the saving equation above. Note that this model operates along standard Keynes–Kalecki lines in the short run. The principle of effective demand reigns: investment determines saving through changes in utilization. An autonomous increase in investment (an upward shift in the intercept term of the investment equation) has a multiplier effect on utilization in the short run. An increase in the propensity to save has a deflating effect on utilization, sometimes called the paradox of thrift. A decrease in the real wage, or equivalently an increase in the profit share, also has a deflating effect on utilization, sometimes called the paradox of costs. (Firms experience a decline in their real labor costs, yet they wind up reducing output.) It is apparent that the paradox of costs is really a variant of the paradox of thrift; it occurs because a redistribution toward the high-saving category of income (profit) raises the social saving rate.Footnote 8 In the long run, as we will see, the model gravitates (or, to be more precise, can gravitate) toward the growth rate specified by the saving function, which of course is (a version of) Harrod's warranted rate of growth. With utilization at its normal level, the warranted rate is:  For ease of exposition, let us assume that the capital-employed labor ratio (usually denoted by k) equals unity; one unit of capital employs one unit of labor (we can always choose units so that this is true). We can define the employment rate, e, as the ratio of employed workers to the labor force, L. Finally, define the ratio of capital to the labor force (not to employment, which is why we use a Greek letter) as κ. Now we can see that the employment ratio depends on the utilization rate and the capital-labor force ratio, or  For future reference, note that the unemployment rate will be 1−e. We work mostly with the employment rate. We will also ignore the fact that there is in principle an upper bound on the employment rate. A value greater than unity might reflect overtime or moonlighting. We know that u(R) evolves with the interest rate. Thus, to gain some appreciation of the dynamics of employment, we can concentrate on the capital-labor force ratio. Assuming that the rate of growth of the labor force is n, this ratio will grow by a factor (1+g)/(1+n) each period. Substituting from the IS equation and the investment function we can obtain the difference equation   where A=(1+d0+d0d2/c)/(1+n) and B=(d1+d1d2/c)/(1+n). We will model the labor force growth rate first as a variable, in which case this equation will be unnecessary. But when we model the labor force in the traditional way (with a natural rate of growth), this equation will play a central role. The three equations (1), (2) and (3), define a system of autonomous non-linear difference equations in the general form  This system will have different dynamic properties depending on the closure with respect to the labor market.",2
34,3,Eastern Economic Journal,09 June 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050036,Anticipated vs Realized Benefits: Can Event Studies be used to Predict the Impact of New Regulations,May 2008,Kara M Reynolds,,,Female,Unknown,Unknown,Female,"Economists are often asked to evaluate the impact of a new set of regulations on particular industries well before there are enough data to empirically estimate the effects. One method that economists have used to tackle this challenge in the past is the event study, which assumes that in an efficient market security prices fully reflect all available information and adjust immediately to new information.Footnote 1 Therefore, the degree to which a new policy will impact a given firm should be reflected in the change in the firm's security price at the time the new policy was first anticipated. Although researchers using event study methodology typically acknowledge the difficulties they face in estimating an event study, few consider the degree to which a well-specified event model can estimate the impact of an event on a firm. For example, many researchers note that it can be difficult to pinpoint the exact date at which the market first anticipates a new policy; thus, the effect of the new policy may already be embedded in the security price prior to the “event date” chosen by the researcher. Fewer researchers question the degree to which markets can correctly anticipate the impact of a new policy on a firm. As Lamdin [2001] notes, “the standard approach of event studies is to focus on contemporaneous market reaction to news. Whether the …market response…was warranted is rarely questioned.” I study in this research whether event studies can provide useful information on the effect of a particular policy change on a firm, given both the difficulty in estimating event studies and the fact that investors typically have extremely limited ability to anticipate the true impact of the policy. In other words, while a well-specified event study may be able to reveal how investors think a policy will impact a firm, these expectations may be poor predictors of the true impact. A change in US antidumping law enacted in 2000 known as the Byrd Amendment provides a unique opportunity to study the degree to which markets are able to correctly anticipate the financial rewards from new policies. US antidumping law allows firms to petition the US government to impose tariffs on products from specific foreign countries because allegedly unfairly low-priced products from these countries are causing material injury to domestic firms. The tariffs imposed due to a successful antidumping petition will typically result in a decrease in imports, higher US prices, and/or an increase in the output of US firms, thus increasing the profits of the US firms within the petitioning industry. Prior to the Byrd Amendment, the tariff revenue collected due to successful antidumping petitions was deposited in the US treasury. The Byrd Amendment, however, requires the US Customs Service to distribute these antidumping duties to firms that approved of the original petition associated with the duties, providing an additional, more direct monetary benefit to the firms that support successful antidumping petitions.Footnote 2 As I argue below, passage of the Byrd Amendment came as a complete surprise to most firms and analysts, and provided new information on the future revenue stream of beneficiary firms not previously incorporated into security prices; based on these characteristics, event study methodology should be able to estimate the degree to which investors expected firms to benefit from the new law. Because the US Customs Service is required to report each year the amount of money distributed under the Byrd Amendment to individual firms, this is one of the few laws in which the exact monetary benefits realized by each firm due to the law is public information. Thus, the law provides the perfect opportunity to study the extent to which markets can accurately estimate the effect of new regulations. The results illustrate a number of problems with event study methodology that researchers undertaking such analysis should be aware of. First, empirical estimates of the abnormal returns associated with a regulatory change, or the estimates of how market participants believe a change in regulation will impact a particular firm, are extremely sensitive to both the estimation method used by the researcher and the date and length of the “event window” chosen by the researcher. Second, even if the researcher happens to choose the perfect specification for their event study, it is unlikely that the market will be able to predict the true impact of the regulatory change on the value of the firm with any degree of accuracy. In the case of the Byrd Amendment, investors had virtually all of the information needed to make a fairly accurate prediction regarding the future revenue stream that would accrue to the firms in this sample under the new law. Nevertheless, event study estimates of the abnormal returns accruing to each firm suggest that the market significantly underestimated the value of the law for most beneficiary firms.",9
34,3,Eastern Economic Journal,09 June 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050033,Gender Preference and Equilibrium in the Imperfectly Competitive Market for Physician Services,May 2008,Jessica Wolpaw Reyes,,,Female,Unknown,Unknown,Female,"In recent decades, the number of female physicians has increased dramatically: while women represented only 8 percent of physicians in 1970, by the year 2000 they represented 24 percent.Footnote 1 Patient demand for female doctors has also risen over this period: not only do women make 40 percent more visits to doctors each year than men, but more women express a preference for women doctors.Footnote 2 These increases in demand for female physicians appear to have outpaced increases in supply, particularly in certain medical specialties. This raises the interesting question of how the imperfectly competitive market for physicians' services achieves equilibrium in response to this excess demand. To investigate this question, I focus on the specialty of obstetrics and gynecology (ob-gyn) where the excess demand for female physicians is especially pronounced. On the demand side, not only are all ob-gyn patients women, but more and more women prefer to see a female physician when discussing more personal matters, such as those related to sexuality or childbearing (studies indicate that more than 50 percent of women prefer a female physician [Thorne 1994; Chandler et al. 2000]). On the supply side, only a small portion of ob-gyns are female (22 percent in the time period considered in this paper). Thus, while women have been better represented in ob-gyn than in other medical specialties, the female share was less than 25 percent and was quite small relative to the demand. The low supply and high demand combine to produce a large excess demand for female ob-gyns. In addition, while supply increases between the 1980s and the 1990s were sizeable, they do not appear to have been sufficient to satisfy the demand. First of all, change is slow: even though the flow into ob-gyn has been increasingly female (by 1995, 58 percent of ob-gyn residents were women), this annual flow replaces only 2 percent of the stock.Footnote 3 In addition, it seems likely that, in the face of the new non-negligible supply, a formerly latent demand for female ob-gyns became overt, outstripping the still-small supply. The demand for female ob-gyns may also have grown as more women had the opportunity to experience this previously rare good and developed a preference for it. Thus, it might take some time for supply to increase enough to catch up to the newly expressed demand. If the pricing of doctors' visits were set in an open, competitive market and if the supply of doctors were elastic in the medium or long run, then the excess demand for female doctors would have two primary effects. In the short run, the demand shock would raise the price of visits to female doctors. In the medium or long run, the supply of female doctors would increase, eliminating any price gaps. However, healthcare markets are far from competitive [Gaynor 1994; Dranove and Satterthwaite 1999]. In the short run, both doctors' fees and the price to patients of a doctor's visit are determined by a number of non-competitive factors such as physician contracts and insurance arrangements. Visits are often rationed by mechanisms other than money price, such as waiting times for appointments.Footnote 4 In the long run, the supply of doctors in total and in each specialty is relatively inelastic, due to a 6- to 8-year production time for new physicians, a fixed number of residency positions, and low annual turnover. Even large adjustments in the flow will be slow to affect the stock significantly.Footnote 5 In sum, it is doubtful that this market will reach an equilibrium easily or quickly. I investigate how the imperfectly competitive market for ob-gyn services clears in the short run in the face of the excess demand for female ob-gyns. I use detailed data on young physicians in the late 1980s and the early 1990s, a time period when managed care grew tremendously and women began to enter ob-gyn in larger numbers. I find that both money and non-money prices adjust: female ob-gyns charge higher fees and also have longer waiting times. Furthermore, these effects are mediated by institutional structure: in contract settings in which money prices are rigid (i.e. managed care), waiting times are more likely to adjust, and in settings in which money prices are more flexible, the reverse occurs. In addition, female ob-gyns do close the gender gap in weekly income significantly more than female physicians in other specialties, but not completely.",5
34,3,Eastern Economic Journal,09 June 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050035,Ranking of Institutions in Economic Research: a Threshold Citation Approach,May 2008,Kam C Chan,Kartono Liano,,Male,Unknown,Unknown,Male,"In the ranking of economics departments, the three most common approaches are the publication-based approach, the citation-based approach, and the derived-product approach. The publication-based approach relies on the quantity of research as the ranking criterion. By counting the number of publications in a specific set of economic journals for a specific time period, Graves et al. [1982], Hirsch et al. [1984], Tschirhart [1989], Conroy and Dusansky [1995], Scott and Mitias [1996], Dusansky and Vernon [1998], and Coupé [2003] rank economics departments based on the publication record of their faculty members. To differentiate the quality among different journals, these studies also provide alternative rankings by limiting the scope to a few selected top journals and/or using a weighting scheme (e.g., the use of SSCI impact factors or journal survey scores as weights). For instance, Coupé [2003] uses articles published in all economic journals that are available from the EconLit and the Web of Science between 1990 and 2000 to rank the economics departments worldwide. Coupé uses four different weighting methodologiesFootnote 1 to derive the departmental rankings. There are two major assumptions in this publication-based approach. First, the quantity of research is assumed to also represent the quality of research, which may or may not be the case. In other words, this approach does not distinguish influential research works that have been frequently cited from those that are less frequently cited even though the two research works might have been published in the same publication outlet. Second, only a pre-determined set of economic journals are included and thus discounting and sometimes ignoring influential research works in other publication outlets such as book chapters, conference proceedings, monographs, or journals in other disciplines. Owing to these limitations, the publication-based approach, while popular, does not accurately rank economics departments. The citation-based approach explicitly considers the quality of research as the ranking criterion. Davis and Papanek [1984] and Blair et al. [1986] rank economics departments based on the number of citations credited to their faculty members. The citation-based approach completely relies on the citation data from the SSCI. While the concept is novel, the use of SSCI has several limitations.Footnote 2 First, the SSCI allocates full credit to the institutions of all authors, even for coauthored research works. This is in contrast with the common practice of adjusting for coauthor research credit by 1/N and thus inflates the ranking of economics departments that have more than average coauthored research works. To illustrate this point, the trend of authorship in Appendix A shows that before 1980, 72.7 percent of the frequently cited research works used in this study is single-authored and the percentage has dropped steadily to only 35.4 percent since 2000. Without the 1/N adjustment, this declining trend of single-authored research works over time makes the use of SSCI in citation-based ranking inaccurate. Second, due to the comprehensive coverage of economics and social science journals in the SSCI, the citation data from the SSCI does not reflect the quality of a citation. Obviously, two research works that have the same number of citations in the SSCI may be very different in the contribution to economic research if one research work is mostly cited by articles published in highly ranked journals while the other research work is mostly cited by articles published in lower-ranked journals. Third, the SSCI citation data does not distinguish self-citations by the authors and “negative” citations. These limitations undermine the accuracy of the citation-based approach in the ranking of economics departments. The derived-product approach has several strands. The ranking criterion includes using the publication record of Ph.D. graduates (e.g., Laband [1986] and Collins et al. [2000]), “expert” opinion surveys (e.g., Brooker and Shinoda [1976] and Thursby [2000]), and representation in editorial board memberships (e.g., Gibbons and Fish [1991]). While these ranking methods are logical, they also have limitations. For schools without Ph.D. programs, the publication record of Ph.D. graduates does not apply to non-Ph.D.-granting institutions and hence, there is no way of gauging their performance. Opinion surveys suffer possible sample bias and possible questionnaire design problems. Most of the time, the “experts” have different perceptions about quality (Lowe and Locke [2005]). Editorial membership representation, while also widely used, penalizes economics departments whose faculty members may be too busy with their own teaching, research, services, or other consulting activities to serve on the editorial board of journals. These drawbacks introduce biases in the use of derived-product approach to rank economics departments. In summary, while no approach is perfect, the threshold citation approach that incorporates the quantity of research and the quality of research can offer a viable alternative to mitigate the limitations of the quantity-oriented publication-based approach and the quality-oriented citation-based approach.",6
34,3,Eastern Economic Journal,09 June 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050038,Do Spa Visits Improve Health: Evidence From German Micro Data,May 2008,Jonathan Klick,Thomas Stratmann,,Male,Male,Unknown,Male,"Spa therapy dates at least to classical times when Hippocrates proposed that all disease was the result of imbalances of bodily fluids. Spas remained popular until the fall of the Roman Empire, dropping into disfavor during the Dark and Middle Ages. Although they did not regain their universal popularity, spa therapy was increasingly prescribed under medical direction during the Renaissance [Porter 1990]. While British spas tended to emphasize pleasure and leisure in the 20th century, spas that focused on serious medical treatments proliferated in continental Europe. Acknowledgment of the medical benefits of spa therapy by many rheumatologists and dermatologists coupled with coverage by many government health systems placed spa therapy in the mainstream of medical treatments [van Tubergen and van der Linden 2002]. Despite the long history of spa therapy, there is relatively little in the way of empirical evidence that overcomes selection effects and evaluates the causal benefits of spa treatment. If the benefits of spa treatments are little more than folklore or ephemeral placebo effects, subsidizing spa therapy through public or private insurance plans is socially wasteful. However, if spa therapy does produce significant medical benefits, governments and insurers might consider expanding coverage to counteract the rising cost of conventional healthcare, while providing patients with effective, non-invasive alternatives to the existing standard of care. In this study we examine whether spa treatments reduce workday loss and hospital stays, and whether the benefits of these treatments are sufficiently large to warrant subsidization of these treatments. We propose a panel data design to isolate the health effects of spa treatments with respect to missed workdays and hospital visits. Our use of a large-scale micro data set allows us to examine thousands of individuals, and the longitudinal nature of the data allows us to control for self-selection biases, isolating any causal effect of spa treatments on health outcomes. By exploiting the panel structure of our data, we can effectively control for selection effects via individual fixed effects. Further, by focusing on revealed measures of healthfulness, workplace absenteeism, and hospital visits, we mitigate the subjective nature of pain measures. Using data from the German Socio-Economic Panel Study (GSOEP), we find strong evidence of self-selection in the decision to undertake spa therapy. That is, if we ignore the longitudinal nature of the data and simply examine cross-sectional correlations, we find that individuals who visit a spa in a given year exhibit significantly more absenteeism and have higher hospitalization rates during the following year. However, if we control for self-selection by including individual-level fixed effects, and examine the within-individual variation in absenteeism and hospitalization, our estimates suggest that spa therapy significantly improves health. These health, labor market, and medical expenditure benefits from spa therapy suggest that public and private insurers might benefit from expanding coverage of spa treatments.",11
34,3,Eastern Economic Journal,09 June 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050039,To the Slimmer Go the Spoils: Heterogeneous Responses to Bodyweight Incentives in Olympic Weightlifting Tournaments,May 2008,Andrew W Nutting,,,Male,Unknown,Unknown,Male,"Certain sports divide competitors into weight classifications. Athletes in such sports must weigh no more than a predetermined maximum amount prior to competition to participate in a given weight-class. Competitors who fail to “make weight” are subject to disqualification. Research in the sports medicine and physiology fields has examined how athletes in weight-class sports optimize performance by reducing bodyweights to qualify for lower weight-classes [e.g., Fogelholm 1994; Wenos and Amato 1998; Wroble and Moxley 1998; Opplinger et al. 2003]. The goal of such weight-loss is presumably to meet the maximum weight of the next weight-class; in sports like wrestling or boxing, where competitor bodyweights have no statutory impact on who wins or loses matches, athletes who qualify for a lower weight-class have little reason to lose additional weight.Footnote 1 But an interesting rule difference may cause bodyweight strategies to differ in the weight-class sport of weightlifting. Bodyweight is a critical tiebreaker in weightlifting: when all lifts have been completed and two or more weightlifters have the same total score, the higher ordinal rank is awarded to the athlete with the lower official bodyweight. Bodyweight's role as a not-infrequent weightlifting tiebreaker permits an interesting test of tournament theory, which deals with payoff structures tied to ordinal performance rankings [Lazear and Rosen 1981]. The central concept of tournament theory, which has been studied in sports such as golf [Ehrenberg and Bognanno 1990a, 1990b], running [Maloney and McCormick 2000; Lynch and Zax 2000], and basketball [Taylor and Trogdon 2002], is that competitors' efforts will increase if the expected marginal benefits of increasing effort — the combination of expected improvement in ordinal rank and the expected increase in compensation from any such improvement — outweigh the marginal costs of increasing effort. In weightlifting, since ordinal improvements at high ranks yield higher marginal benefits than improvements at low ranks, tournament theory suggests that high-ability lifters have Ceteris paribus an incentive to achieve lower official bodyweights than other lifters. Estimations from 58 Olympic weightlifting competitions show that high-ability lifters in lighter and middle weight-classes enter tournaments with significantly lower bodyweights than their competitors. There is no significant evidence that high-ability lifters in heavier weight-classes have lower bodyweights than their competitors, though. This cross-class heterogeneity in bodyweight reduction seems attributable to higher expected benefits (as opposed to lower costs) of bodyweight reduction in lighter weight-classes. Specifically, in lighter and middle weight-classes a given reduction in log bodyweight yields a higher probability of becoming lighter than a competitor and therefore a better chance of winning a tiebreaker. There is also some evidence that medal-determining ties are more frequent in lighter weight-classes. The paper is organized as follows: The next section describes weightlifting tournaments. The subsequent section discusses the data. The penultimate section discusses empirical methods and results, and the final section concludes.",3
34,3,Eastern Economic Journal,09 June 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050040,Atheoretical and Theory-Based Approaches to the Natural Equilibrium Real Interest Rate,May 2008,Philip Arestis,Georgios E Chortareas,,Male,Male,Unknown,Male,"The equilibrium or natural real interest rate corresponds to the concept that there exists some unobservable real interest rate, which, in the absence of frictions, equilibrates demand and supply conditions in the economy. The natural real interest rate is potentially an important concept for monetary policy makers. The observable market real interest rate and the natural real rate may deviate and it is this gap that can be used to evaluate the stance of monetary policy. To the extent that the observable market interest rate deviates from the unobservable equilibrium interest rate, adjustments such as price increases or decreases occur and the central bank can achieve a neutral monetary policy stance by adjusting its interest rate accordingly. The usefulness of the estimates about the natural real interest rate, however, can be lessened because of the uncertainty that surrounds them. For example, the former Fed Vice Chairman R. W. Ferguson Jr. [2005] recently observed that “while the concept of the equilibrium real interest rate is a useful aid in thinking about setting monetary policy, it is not measured and observed with such precision as to provide a practical guide to the appropriate stance of policy.” It is, thus, of paramount importance that a reliable estimate of the long-run equilibrium real rate of interest is available to the policy maker. The idea that there exists an interest rate reflecting equilibrium at the aggregate macroeconomic level is not new. Wicksell [1936] refers to “a certain rate of interest on loans which is neutral in respect to commodity prices and tends neither to raise nor to lower them” [p. 4]. This interest rate can be defined in many different ways depending on the model that one specifies (e.g., the return to consumption equals the return to investment, or the marginal product of capital equals the interest rate, or saving equals investment). In most general terms, it can be said that it reflects equilibrium of demand and supply conditions of the aggregate economy. The natural real rate of interest is the one that would obtain under full wage and price flexibility taking a number of real factors as given. As Woodford puts it “…the Wicksellian natural rate of interest…. may be defined as the equilibrium real rate of return in the case of fully flexible prices” [2003, p. 248]. To the extent that optimal monetary policy focuses on replicating the flexible price equilibrium the actual interest rate should track the natural real interest rate. Recalling Woodford's [2000] interpretation of the natural real interest rate as being consistent with period-by-period price stability, it becomes apparent that any meaningful approximation to it has to be time varying. Some clarifications are necessary when discussing the “natural” or “neutral” real interest rate. First, the equilibrium real interest rate is different from the steady-state real interest rate that is expected to prevail in the long run and is time invariant by definition. Second, although the equilibrium real interest rate concept refers to the medium to long run, it is an interest rate with short maturity. That is, the “natural” equilibrium real inertest rate is a short-term rate. This is one important property that renders it suitable for the construction of interest rate gaps, which can be used to evaluate the monetary policy stance at cyclical frequencies. Third, although most authors,Footnote 1 including Wicksell [1936] himself, use the concepts of “natural” and “neutral” real interest rate interchangeably some further exploration may be warranted as to whether any given “neutral” real interest rate necessarily corresponds to the “natural” real interest rate. The “neutral” real interest rate variation over the business cycle which reflects inflation stabilization may not be fully warranted by the deep structure of the economy. Various approaches have been proposed in the literature for approximating the equilibrium real interest rate. One could roughly distinguish between “atheoretical” and “theory-based” measures. Although some approaches fall clearly under those two categories, there exist some approaches that combine elements of the two. There are a number of ways in which the natural real interest rate can be estimated and two broad classes of measures have been considered. The first set of measures based on “atheoretical” approaches, ranges from taking a simple average of real interest rates over time (possibly correcting for the effects of specific shocks), to using more refined statistical filtering of time series data. The second set of measures is based on explicit theoretical models. We are particularly focused on approaches that derive the equilibrium real interest rate using Dynamic Stochastic General Equilibrium (DSGE) models. In this context, the equilibrium real interest rate is the interest rate that would prevail when prices are fully flexible, with the output gap is zero and inflation is on target. That is, under fully flexible prices all markets clear and the real interest rate is at its equilibrium level. In principle, any DSGE model solved under the assumption of fully flexible prices can give rise to a real interest rate that can be interpreted as the equilibrium real interest rate. We demonstrate the applicability of such approaches by providing an empirical measure of the equilibrium real interest rate in the US using a DSGE model. One of the advantages of this approach over the non-structural/statistical approaches mentioned above is that the emerging equilibrium real interest rate is based on theory. The equilibrium real interest rate concept that we use is different from the steady-state real interest rate. Unlike the latter, the former responds to preference and technology shocks and is therefore time varying. Neiss and Nelson [2001; 2003] develop a DSGE model to analyze the equilibrium real interest rate (and real interest rate gap) for the UK. This model has a number of appealing features that enrich its structure as compared to many standard DSGE models. In particular, it incorporates a number of more realistic features, such as habit formation in consumption and capital adjustment costs. It is for these reasons that in this paper we calibrate such a model but for the case of the US. Calibrating such a model can help to obtain impulse responses. However, in order to obtain an empirical measure of the real interest rate we employ a methodology that allows producing an operational measure of real interest rate (as a function of technology and preference/IS shocks). Specifically, we calibrate a DSGE model for the US, we identify the shocks using the definitions of the theoretical model and VAR analysis, and we use a reduced form equation of the model to obtain time-varying measure of the equilibrium real interest rate. The rest of the paper is structured as follows: the next section discusses alternative definitions of the concept of the natural equilibrium real rate of interest. The subsequent section reviews a number of empirical frameworks for obtaining measures of the natural equilibrium real rate of interest; these measures are not based on an explicitly theoretical framework. The fourth section focuses on natural equilibrium real rate of interest measures based on explicitly theoretical frameworks. The penultimate section provides some estimates for the US natural equilibrium real rate of interest based on a DSGE model. The final section discusses the policy implications of these alternative measures for monetary policy in practice, summarizes the argument and concludes.",11
34,3,Eastern Economic Journal,09 June 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050027,"Targeting in Social Programs: Avoiding Bad Bets, Removing Bad Apples, by Peter H. Schuck and Richard Zeckhauser",May 2008,Amy Diduch,,,Female,Unknown,Unknown,Female,,
34,3,Eastern Economic Journal,09 June 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050028,"The Evolutionary Foundations of Economics, by Kurt Dopfer",May 2008,William T Ganley,,,Male,Unknown,Unknown,Male,,
34,3,Eastern Economic Journal,09 June 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050048,"African Americans in the US Economy, by Cecilia A. Conrad, John Whitehead, Patrick Mason and James Stewart",May 2008,Robin L Bartlett,Lariece M Brown,,,Unknown,Unknown,Mix,,
34,3,Eastern Economic Journal,09 June 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050047,"Institutions, Globalisation and Empowerment, by Kartik C. Roy and Jörn Sideras",May 2008,Esteban Pérez Caldentey,,,Male,Unknown,Unknown,Male,,
34,3,Eastern Economic Journal,09 June 2008,https://link.springer.com/article/10.1057/palgrave.eej.9050049,"Reintroducing Macroeconomics: A Critical Approach, by Steven M. Cohn and M.E. Sharpe",May 2008,K Brad Stamm,,,Unknown,Unknown,Unknown,Unknown,,
34,3,Eastern Economic Journal,09 June 2008,https://link.springer.com/article/10.1057/eej.2008.8,"Stochastic Models of Decision Making in Arranged Marriages, by Amitrajeet A Batabyal",May 2008,Bryan C McCannon,,,Male,Unknown,Unknown,Male,,
34,3,Eastern Economic Journal,09 June 2008,https://link.springer.com/article/10.1057/eej.2008.4,"The Economics of Consumer Credit, by Giuseppe Bertola, Richard Disney, and Charles Grant",May 2008,Jennifer M Shand,,,Female,Unknown,Unknown,Female,,
34,4,Eastern Economic Journal,02 November 2008,https://link.springer.com/article/10.1057/eej.2008.35,Symposium Introduction,October 2008,Jason M Barr,Troy Tassier,Leanne Ussher,Male,Male,Female,Mix,,
34,4,Eastern Economic Journal,02 November 2008,https://link.springer.com/article/10.1057/eej.2008.37,The Rise of Computationally Enabled Economics: Introduction to the Special Issue of the Eastern Economic Journal on Agent-Based Modeling,October 2008,Rob Axtell,,,Male,Unknown,Unknown,Male,"The first digital computers were primarily used by scientists and engineers to solve mathematical equations numerically, that is, to approximate analytical solutions, most commonly for difficult-to-solve differential equations. The economics profession was also an early adopter of digital computing, and many of the first uses of computation by economists involved numerical solution of economic equations that were hard or impossible to solve analytically. In this, the computer served as a powerful calculation engine while the problems being solved were conceptually similar to those that had come before, that is, constrained optimization. Much of this work had a heavy normative flavor, as companies had great financial incentive to learn how to better manage their inventories, optimize assembly lines, forecast demand for their products, and so on. Indeed, Herbert Simon, one of the early pioneers in this area, was fond of recalling that his work on dynamic programming in the 1950s taught companies how to better manage their production processes. It was very clear to that generation of economists that firm behavior was far from optimal, a fact that, Simon lamented, later generations seemed to have forgotten. In his Machine Dreams: How Economics Became a Cyborg Science, Philip Mirowski (2001) masterfully charts the early adoption of digital computer technology by economists and closely affiliated researchers (in game theory and operations research primarily, including many refugees from physics). Indeed, as operations researchers learned to solve wide classes of optimization problems computationally, through so-called mathematical programming methods (e.g., linear programming, integer programming, dynamic programming), economists were early users of such techniques for practical and policy problems. Monte Carlo methods were systematized early-on in the digital era, primarily for their use in evaluating difficult to solve integrals. This led to the development of reasonably sophisticated pseudo-random number generation algorithms and the rise of modern simulation techniques. Already in the 1950s, the so-called microsimulation was being used by economists and policy makers as a way to better forecast and understand alternative economic policies (Orcutt et al. 1961). This use of computation was somewhat different from the mathematical programming that had come before. While still involving optimization, such calculations were done at the household level, and heterogeneity and dynamics were prominent features of this approach, while largely absent in the conventional approaches. In the 1960s and 1970s, large-scale macroeconomic models and their estimation using aggregate data — essentially computational econometrics — became an important consumer of digital clock cycles in economics. These models retained the focus on solving mathematical equations that had previously been present in most uses of computing by economists. Indeed, while the realm of computing was expanding rapidly at this time — to database technology, sophisticated programming languages, even artificial intelligence (AI) — economists as a whole made little use of such advances. As the microcomputer era dawned in the 1980s, and a host of “soft computing” technologies came to the fore in computer science — expert systems, objects and object-oriented programming (OOP), computer graphics, evolutionary computing, distributed AI (DAI) — the mainstream economics profession, with its focus on systems of analytically soluble equations and static equilibria, did not systematically utilize most of these advances. Computational economics, to the extent that it existed, retained its focus on numerical solutions to otherwise neoclassical models. Despite the rise of experimental and behavioral economics in this era, and with an obvious natural affinity between the psychological and cognitive science origins of such approaches and AI as it then existed, there was precious little interaction between people from these different research camps. With the rise of so-called multi-agent systems within computer science in the 1990s, along with its fraternal twin, agent-based modeling and simulation (ABMS), poised as they are at the intersection of DAI, OOP, and game theory, we see, in the application of these techniques and technologies to economic phenomena, the first break in the stranglehold that “equation-based” approaches have had on economics. For in the agent-oriented computational approach it is typical to eschew all manner of equations at the social and aggregate levels, although permitting individuals to use equations in their own decision-making if this is reasonable in the context under study. In place of analytical and equation-based aggregation, agent computing builds up aggregate statistics from the direct, agent–agent interactions that take place in the models, more akin to the manner in which such phenomena arise in actual economies. In lieu of the representative agent, ABMS approaches utilize populations of heterogeneous agents. Instead of equilibrium specifications, agent-based computational models spin forward in time to stationary or otherwise “regular” states (e.g., basins of attraction), which may or may not involve agent-level equilibrium. That is, agent modelers are typically agnostic about the attainment of equilibrium in their models — if static equilibrium obtains, that is fine, but it is not stipulated a priori. ABMS, and its emerging instantiation within economics, the inelegant “agent-based computational economics,”Footnote 1 can be viewed as a very general class of modeling techniques that generalize numerical economics, mathematical programming techniques, and microsimulation approaches. In the rare case that agent-based models can be perfectly aggregated we end up with equation-based models that may require numerical solution. When individual agents engage in explicit optimization and a social planning agent of some kind can be viewed as coordinating or otherwise aggregating these individual decisions then we have agent models behaving like mathematical programming models. When agent models are populated by non-autonomous agents, whose behavior is pre-specified and neither adaptive nor purposive, they become microsimulations. But models in which agents are systematically heterogeneous, boundedly rational, and directly interactive with one another over networks away from equilibrium yield agent models that do not generally correspond to the usual neoclassical “sweet spot” (Axtell 2007). Indeed, so general is the agent approach to economics, and so prodigious has computing technology become, that it is tempting to call this new approach computationally enabled economics. The previous uses of digital computing in economics were, of course, impossible without computational facilities, but they did not systematically alter the practice of economics. However, the enormous capabilities and wide-ranging functionality of agents seem to make a new kind of economic science feasible, one which utilizes all aspects of modern computing hardware (multi-core CPUs, large amounts of memory, enormous storage capacity, and giant graphical displays with huge color depth and massively parallel GPUs) while leveraging modern computing software (objects, multi-threading, grid computing). A somewhat different way to say this is to point out that other uses of computing in economics fail to fully leverage all the facilities present on modern machines:
 Applied economists, running large numbers of regressions on large data sets, may utilize significant CPU and storage resources, but do not typically use even a fraction of available memory — models consisting of a few dozen or even hundreds of equations are considered “large,” but it would take millions of equations or more to fill-up the memory typically installed on today's notebook computers. Large-scale microsimulations, of the type run by government agencies and covering some significant sub-population within the United States (e.g., retirees), may require lots of storage and be capable of consuming both CPU and memory resources, but usually make no use of graphical displays. Theorists using the symbolic capabilities of software systems such as Mathematica, Maple, or Macsyma normally make use of processor speed and perhaps display technology, but do not utilize any significant fraction of available memory or hard disk storage. Numerical analysts within economics may heat up their CPUs but often make little or no use of the other subsystems of their computers. These non-agent uses of computation by economists are thus less about enabling qualitatively new kinds of economic inquiry, and more about elaborating orthodox analyses and deepening conventional results, taking small steps away from the social norms of the neoclassical synthesis. Agent computing in economics has the potential to take much bigger steps, into domains that we today do not even have vocabulary to properly describe: non-equilibrium finance, heterogeneous agent macroeconomics, multi-agent firms, and market ecologies.",2
34,4,Eastern Economic Journal,02 November 2008,https://link.springer.com/article/10.1057/eej.2008.29,Referral Hiring and Gender Segregation in the Workplace,October 2008,Troy Tassier,,,Male,Unknown,Unknown,Male,"The effect of gender segregation on inequality has received a large amount of attention. Researchers argue that gender segregation both within and across firms and across occupations may be one of the major causes of income inequality for women.Footnote 1 Several causes for gender segregation have been put forth by other researchers such as discrimination [Becker 1971; Bergmann 1974], differences in training, productivity in specific jobs, or tastes for specific types of work [Mincer and Polachek 1974], and referral hiring through segregated social networks [Marx and Leicht 1992; Mouw 1999]. In this paper, I concentrate on the least discussed of these listed items: referral hiring through segregated social networks. I consider referral hiring to be any means by which an employee learns of a job through a social contact (which may include familial contacts), or a firm learns of a potential applicant through an employee who is a social contact of the potential applicant. Thus, I consider referral hiring to be any means of transferring job information that occurs at least in part through social contacts or social networks. The literature on referral hiring is vast. Researchers robustly find that around one-half of all jobs are filled through referral hiring [Granovetter 1995; Bewley 1999] with some occupations and firms having much higher rates. Since social networks are segregated by ethnicity and gender [Marsden 1988] information about jobs is likely to be segregated across social groups as well. Thus, when firms rely on social channels to find workers, their pool of applicants can be biased by the current ethnic or gender composition of the firm. Similarly, when workers use social channels to find employers, their pool of potential employers can be biased toward the firms and industries of their social contacts. This bias of individuals referring like-individuals is labeled as referral homophily [McPherson et al. 2001]. Examining the ability of a model of referral hiring to produce workplace segregation and comparing the segregation produced by the model to observed data has not been done in other research. In this paper, I investigate the potential of referral hiring to create inter-firm gender segregation and compare the segregation produced to staffing data at US colleges and universities. I use an agent-based model to grow a distribution of firms who employ both women and men. As a firm grows, it develops an endogenous bias toward hiring employees of each gender group. The bias may be interpreted as referral hiring.Footnote 2 I vary the strength of the potential bias to see if the model can produce a distribution of women and men workers across firms that resemble observed data. I find that reasonable levels of referral hiring can generate levels of segregation comparable to observed levels of segregation in staffing at US colleges and universities. However, the model is not able to perfectly replicate the distribution observed in the data. Before I begin, I want to make clear that I am not proposing that referral hiring is the only cause of gender segregation in the workplace or even that referral hiring is the primary cause. In addition, I am not claiming that referral hiring is necessarily the cause of the gender segregation in the data used in this paper. The question that I am asking is not “did referral hiring generate this data?” Instead I am asking “how much gender segregation could be produced by observed levels of referral hiring?” In other words, I am not trying to specifically identify the effect of referral hiring in the data. That exercise is unlikely to be fruitful since many of the footprints of referral hiring are similar to the footprints of productivity differences, discrimination, and other causes of workplace segregation. In future research I plan to examine whether one can identify differences in the worker distributions produced by these various causes of segregation. But for now, the goal is simply to see if a very simple model of referral hiring can produce worker distributions with levels of segregation similar to those observed in data. There are surprisingly few studies of inter-firm gender segregation. This is mostly due to data limitations. Most of the studies that have been done tie inter-firm gender segregation to the male–female wage gap [McNulty 1967; Buckley 1971; Blau 1977; Groshen 1991; Carrington and Troske 1994]. Overall the studies find that there are substantial levels of inter-firm gender segregation and that women tend to be sorted into low-paying firms. In addition, although it is not the focus of this paper, inter-firm racial segregation has also been found to be prevalent. For instance, Becker [1980] finds “most of the segregation of black and white workers… is segregation by place of work [85 percent] rather than stratification into different occupational categories [15 percent].” As mentioned above, discrimination and taste for different jobs are most often cited as the main cause of the observed levels of inter-firm segregation. Here I discuss another potential explanation: referral hiring. Numerous studies have found that a large percentage of jobs are found by using social contacts. In summary, researchers find that between one-third and two-thirds of workers find their jobs through friends, relatives, and other social contacts.Footnote 3 It has been suggested that referral hiring occurs for several reasons. First, referral hiring may result in more certainty in the quality of a new hire. Second, in some cases referral hiring may be cheaper than more formal hiring practices. Third, employees may gain a preview of the work environment from the person who refers them and thus may be a better match for the firm. Because referrals play such a prominent role in the attainment of employment it has been argued that referral hiring may be a large cause of income inequality. This may be especially true of groups that have “recently” entered traditional labor markets such as women or recent immigrants, or groups that have traditionally faced open discrimination, such as African Americans. This line of research follows the idea that “it's not what you know, but who you know” [Montgomery 1991]. These arguments look at both the structure of one's social network and the quality (in terms of providing access to jobs) of one's social network. While most of the work on the consequences of referral hiring has concentrated on income inequality there may be a second effect: workplace segregation. Just as one can think of a group of friends as a network, one can also think of firms or jobs as a network. If an individual knows about an open job because of the job he holds, those two jobs are connected. Thus, firms and related positions in an industry or perhaps in geographic proximity form a job network. Individuals who are near to each other in a social network may tend to work in jobs that are close to each other in a job network if referrals play a prominent role in the hiring process; social contacts may work in the same firm or industry or in jobs related in some other way. Thus, workplace segregation may occur as a result of referral hiring since social networks are segregated. Social network segregation is common. It is not surprising to most that researchers find that social networks tend to be most homogeneous in race and ethnicity [Marsden 1988]. But social scientists also find segregation of social contacts to occur along lines of gender, income, education, religion, and other demographic variables as well [O'Reagan and Quigley 1993]. For the most part, there is little gender segregation in terms of geography. In other words, most geographic regions have roughly equal numbers of men and women living in them. But there is significant gender segregation in friendships, social circles, and confiding relationships even when there is an approximately even mix of men and women in a geographic area. Part of the reason that gender relationships are more homogenous than race and ethnicity is family. For instance, Marsden [1987] finds that confiding relationships are 68 percent less heterogenous in terms of gender than the general population. But once he controls for family contacts, he finds less gender homophily among family (85 percent less heterogenous than the general population) but significantly more gender homophily among non-family (53 percent less heterogenous than the general population). In another well-known study Huckfeldt and Sprague [1995] studied the discussion of politics in social networks. In terms of gender, 84 percent of men in their study reported only discussing politics with other men. Work relationships also have been found to contain high levels of gender homophily [McPherson et al. 2001] especially for men and, as one might expect, the amount of homogeneity is even stronger for men who work in environments in which they are a majority. Combining the idea that work environments tend to be composed of homophilous work relationships and the observation that many job referrals come from past work relationships [Granovetter 1995] it may be expected that there would be gender homophily in job referrals. Studies documenting the gender of a job referral and referee are rare. There are only two studies of which I am aware but both replicate the expectation of gender homophily in job referrals. Fernandez and Sosa [2005] find a significant bias to like gendered referrals for both women and men in data from a call center at a large bank. In their study roughly one-third of the referral applicants and employees are men. In the paper, the authors tally the referrals based on the gender of the referrer. If the referrer was male, 44 percent of the referral applications were men but, if the referrer was female only, 25 percent of the referral applications were from men. Thus, men and women display significant levels of homophily in their referrals. (Men refer a higher than average percentage of other men and women refer a lower percentage of men than would be expected from looking at the percentage of men employees.) In another study, using data from the NLSY, Berger [1995] also finds significant homophily in job referrals. She finds 84 percent of men using a contact to find a job used another man and 64 percent of women using a contact to find a job used another woman. There is some indirect evidence that referral hiring and referral homophily may cause workplace segregation. Marx and Leicht [1992] find that referral hiring reduces female and minority representation in job types that most frequently hire by referral. Mouw [1999] finds that firms with small numbers of minorities who hire through referral are less likely to hire a minority worker compared to similar firms who hire through newspaper advertisements. While both offer evidence suggesting referral hiring could cause segregation, neither explicitly investigates a model of referral hiring and segregation or attempts to investigate the amount of segregation that could be created by observed levels of referral hiring. In the following sections of this paper, I describe a model of referral hiring to be used to investigate the extent to which referral hiring can create interfirm gender segregation. I am able to show that the model produces levels of gender segregation similar to the levels observed in the data. However, the model cannot fully explain all of the observed segregation. But this should be expected since referral hiring is only one component of the process that creates gender segregation. A more complete model may include items such as gender differences in ability at specific jobs or in taste for jobs, and discrimination by firms.",2
34,4,Eastern Economic Journal,02 November 2008,https://link.springer.com/article/10.1057/eej.2008.27,Adaptive Microfoundations for Emergent Macroeconomics,October 2008,Edoardo Gaffeo,Domenico Delli Gatti,Mauro Gallegati,Male,Male,Male,Male,"It is not so easy … to continue telling macroeconomic stories that rely on a theory of general equilibrium dynamics abandoned over a decade ago by most serious investigators. The tâtonnement cannot be considered a harmless “as if” assumption if it is logically inconsistent with other assumptions in the models we use. [Weintraub 1977, p. 14] Within just 5 years after Roy Weintraub's forceful warning against the inadequacy of the Walrasian auctioneer to logically produce any interesting real-time macroeconomic issues, the appearance of two complementary papers contributed to the establishment of the current dominant scientific standard in macroeconomics. The two papers were written by Robert Lucas [1980] and Fynn Kydland and Edward Prescott [1982]. Both of them (1) acknowledged as a relevant research question the analysis of macroeconomic fluctuations; (2) proposed a view of aggregate dynamics in terms of the general equilibrium response of rational maximizing individuals to random changes in production possibilities and resource constraints; (3) made use of the contingent-claim interpretation of a Walrasian general equilibrium, as originally proposed by Arrow [1964] and Debreu [1959]; and (4) identified (the former) and implemented (the second) a brand new strategy to assess the predictive success of theoretical statements. While the theoretical underpinnings of the resulting Real Business Cycle (RBC) modelFootnote 1 were sensibly refined during the following three decades of active research — in that important non-Walrasian features like imperfect competition and incomplete intertemporal markets have been introduced — the core of the solution concept invariably rests (explicitly or not) on a fictitious centralized auctioneer that costlessly collects and disseminates information, adjusts prices and quantities, and organizes and executes trades. The list of logical problems affecting microfoundations rooted in the Arrow–Debreu tradition is rather long and widely known. Just to cite some of them, it must be noticed that: (i) the conventional general equilibrium theory has difficulties in finding a role for monetary exchange; (ii) the equilibrium is neither unique nor locally stable under general conditions; (iii) the introduction of a representative agent (RA) is done without paying any attention to composition and aggregation fallacies; and (iv) any tâtonnement process occurs in a meta-time, and implies that the formation of prices precedes the process of exchange, instead of being the result of it. The interested reader can consult Kirman [1989], Hartley [1997], and Hildebrand [1994] for wide-ranging discussions. In spite of all its weaknesses, the practice of modeling an economy worth several trillion dollars by means of a rational optimizing Robinson Crusoe in continuous equilibrium has become the scientific standard of modern macroeconomics, also known as Dynamic Stochastic General Equilibrium (DSGE) theory. Certainly, Weintraub's general idea that “… rich, flexible, and rigorous general equilibrium models help to provide a vision of the microfoundations of macroeconomics” [Weintraub 1977, p. 2] nowadays circulates almost undisputed among the profession. Sadly, his admonition to carefully work out the conceptual basis of the type of general equilibrium theory to be employed has, however, went almost unheard. The scientific practice of dominating macroeconomics today can be condensed by the sharp metaphor put forth by Axel Leijonhufvud [1998], according to whom economists doing research within the DSGE approach tell stories using the same expositive technique of Beckett and Pirandello: “… the economist of today expects to see a solitary representative agent, under the mathematical spotlight on a bare and denuded stage, asking agonizing questions of himself: ‘What does it all mean?’ […] or ‘I know I have optimized, but — is that all there is?’” [Leijonhufvud 1998, p. 199]. In fact, it should be noted that modern mainstream macroeconomics is in striking contrast with the institutionally oriented, behaviorally adaptive, dynamically rich stories told by economists belonging to the British Classical Tradition [Leijonhufvud 1999]. Although giants of economic thought like Alfred Marshall sometimes used RAs as a modeling tool, those agents were employed only as a means of thinking through what sorts of variables belong in aggregate relationships. No attempts were made to derive aggregate functions from the solution of a dynamic optimization problem posed to a representative consumer/worker. Marshall's stories about the ordinary business of life were filled in with asymmetric information, incomplete contracting, exchange-supporting and coordination-enhancing institutions, and process-regarding preferences. The complexity resulting from the interactions of all these constituents clashes with the reductionist approach of modern macroeconomics, as well as with the mathematical tools employed by modern macroeconomists to substantiate it. As regards this latter point, advocates of DSGE typically use fixed-point theorems to solve choice-theoretic problems consistent with the tenets of Subjective Expected Utility theory. It must be noted, however, that equilibrium solutions can be derived only if one assumes that: (i) each agent has full knowledge of the problem; (ii) he is perfectly able to compute the solution; and (iii) there is common knowledge the all agents are operating under requirements (i) and (ii).Footnote 2 No surprise, therefore, that the classical way of writing economic tales does not “… appeal to modern tastes. Too many characters on stage: the consumer, the worker, the banker, the entrepreneur, the stock market speculator, and more. And who has patience with the antics that result from the rigid idiosyncrasies and informational asymmetries of this cast? It smack of Commedia dell'Arte” [Leijonhufvud 1998, p. 199]. In this paper we shall argue that a method to construct and analyze interesting macroeconomic issues with microfoundations based on heterogeneous, adaptive, decentralized processes of individual decision-making — a là Commedia dell'Arte — is not only feasible, but also that the models one obtains in this way can rival the explanatory power of DSGE models. Since we endorse the view according to which “… a general equilibrium model is simply some specification of states of the world and a set of rules that structure the manner in which those states change over time” [Weintraub 1977, p. 2], our theory is firmly rooted in the tradition of general equilibrium analysis. However, we depart from its Walrasian interpretation in that we explicitly model an evolving network of fully decentralized trades among adaptive agents. The explanations we propose, instead of being derived deductively from the exogenous imposition of a fictitious centralized auctioneer, are generated from a systematic analysis of general interactions by means of an agent-based computational laboratory. The structure of the paper is as follows. The next section contains some methodological ruminations about how the agent-based computational approach can be employed to provide sound microfoundations to macroeconomic theory. The subsequent section presents the key features of a prototypical agent-based computational laboratory suited to address macroeconomic issues, while its ability to replicate some well-known stylized facts of the business cycle is assessed in the further section. The last section is a summary and conclusion.",66
34,4,Eastern Economic Journal,02 November 2008,https://link.springer.com/article/10.1057/eej.2008.30,The Spread of Free-Riding Behavior in a Social Network,October 2008,Dunia López-Pintado,,,Female,Unknown,Unknown,Female,"The outcomes of many socioeconomic phenomena crucially depend on the properties of the social networks in which agents are embedded. Some paradigmatic examples are the diffusion of a new technology [López-Pintado 2006; 2008; Jackson and Yariv 2007], the spread of information on job opportunities [Calvó-Armengol and Jackson 2004], the uprising of political revolts [Chwe 2000], or even the diffusion of an infectious disease [Pastor-Satorrás and Vespignani 2001]. The reason why social networks have an impact on these phenomena is that typically agents’ actions depend on the actions of their social contacts. A common ingredient in most of the examples mentioned above is that a certain behavior spreads across the social network due to some “contagion process” (namely, the more agents choose a certain action, the more it becomes appealing for another neighbor in the social network to do so as well). In this paper, however, we examine the polar framework, that is, situations where agents want to anti-coordinate with their neighbors.Footnote 1 A context where this generally applies is the case of a local public good that is non-excludable along social contacts (e.g., friends benefiting from the research into a product of other friends, firms learning from related firms the benefits of a new technology, a farmer's experience with a new crop benefiting other farmers, and so on). We focus on the basic idea that an agent can develop a certain skill valuable to all other agents, (e.g., learning how to use a new computer program) provided he exerts some effort, but can also free-ride on his (already skilled) neighbors (or co-workers) instead. This paper analyzes a social-network model in which the two following assumptions hold. First, agents make a binary decision, that is whether or not to exert effort. Second, incentives are such that whenever an agent has someone in his neighborhood exerting effort, he free-rides and decides not to do so. The first part of the paper presents a static model where agents choose their actions simultaneously. We characterize the set of Nash equilibria of this simple game. To do so, we follow the arguments exposed by Bramoullé and Kranton [2007] in their (more general) model.Footnote 2 We obtain (as they do) that the state where the set of agents exerting effort forms a maximal independent set is a Nash equilibrium, and vice versa, a Nash equilibrium is characterized by a maximal independent set of agents exerting effort (Proposition 1). In other words, in equilibrium, agents exerting effort are never connected with each other and all other agents are connected with at least one agent exerting effort. Although in our framework this result is straightforward, it serves as a natural starting point. Moreover, in Proposition 2 we show that if we consider a simple dynamics model where each period an agent, selected uniformly at random from the population, updates his strategy and chooses a myopic best response, this process converges to a Nash equilibrium of the static model. The limitation of this approach is that there is multiplicity of equilibria, and, in addition, these equilibria cannot be easily characterized in terms of relevant properties such as the fraction of free-riders. Therefore, one cannot get strong results on the comparison of different network structures. The second (and more innovative) part of this paper studies a mean-field version of the myopic best response dynamics and considers the asymptotic properties of the equilibria in (general) random networks when the network size becomes large.Footnote 3 Our analysis of this dynamic model yields the following main insights. First, the dynamics converges to a unique, globally stable fraction of free-riders (Proposition 3). This uniqueness result allows for the comparative statics of the equilibrium outcome with respect to changes in the degree distribution of the underlying network, where the degree distribution is the distribution of the number of neighbors of each agent in the population.Footnote 4 This approach raises a whole new set of research questions. For instance, how do the properties (such as the mean and variance) of the degree distribution affect the pattern of free-riding behavior? Are more dense networks better or worse for the diffusion of free-riding behavior? Also, how does the variance of the degree distribution affect this phenomenon? Our results can be summarized as follows. The higher the degree of agents in a homogeneous network, the higher the proportion of agents free-riding and the proportion of links pointing to a free-rider (Proposition 4). Under additional conditions on the degree distribution, the proportion of links pointing to a free-rider increases under a first order stochastic dominance (FOSD) shift of the degree distribution (Proposition 5) and decreases under a mean preserving spread (MPS) of the degree distribution (Proposition 6). These results suggest that there tends to be more free-riding in denser or more equal networks.Footnote 5
 This paper complements the analysis of two relevant papers on free-riding in networks, namely Bramoullé and Kranton [2007] and Galeotti et al. [2008]. Bramoullé and Kranton [2007] were the first to consider a public goods network game. They perform a classic static game theoretic analysis for a given fixed network assuming complete information. They find multiplicity of equilibria and are unable to characterize them in terms of desirable properties such as the fraction of free-riders. As a consequence, they do not obtain strong results on the comparison of different network structures. Galeotti et al. [2008], on the other hand, perform a general analysis of a network game where the public good game studied here is considered as a particular case. They study a model of incomplete information on a network structure that resolves the problem of equilibrium multiplicity. They find that there always exists a symmetric equilibrium with clear monotonic properties. Galeotti et al. [2008] are also able to obtain some results on comparative statics of networks. A problem of their incomplete information model is that it is only valid in a one-shot game. If, for instance, we think of a scenario where the game is repeated over time, their approach only makes sense when the network is so volatile that agents are unable to learn its structure.Footnote 6 The current paper presents an alternative approach to the problem. We consider a myopic best response dynamic model of the complete information framework, and study the asymptotic properties of the equilibria in general random networks, when the network size becomes large. To do so, we rely on an approximation of the model, namely a mean-field approximation that makes this alternative tractable. This enables us to address the issue of how free-riding behavior depends on the global properties of networks. Thus, the analysis gives clear predictions for differences in free-riding among different societies, depending on the global network structure of these societies, opening ways to test these predictions empirically.Footnote 7
 The paper is organized as follows. In the next section, we present the benchmark model and characterize the Nash equilibria of the static game. We also describe a myopic best response dynamics and show that it converges to a Nash equilibria of the static game. In the third section, we introduce a mean-field version of the dynamics that simplifies the analysis and leads to the characterization of the globally stable state. This in turn allows us to relate the levels of free-riding behavior in the population with the structure of the social network in which agents are embedded. In the fourth section, we provide simulations of the myopic best response dynamics on random networks with scale-free and exponential degree distributions to shed some light on the validity of the mean-field approximations. Finally, in the fifth section, we summarize and discuss the results.",12
34,4,Eastern Economic Journal,02 November 2008,https://link.springer.com/article/10.1057/eej.2008.26,Segregation and Strategic Neighborhood Interaction,October 2008,Jason M Barr,Troy Tassier,,Male,Male,Unknown,Male,"Racial and ethnic segregation in the United States continues to be common, despite survey results that show people to be increasingly opposed to the idea of racial segregation [Sethi and Somanathan 2004]. Schelling [1969; 1971] has provided one of the most compelling accounts of why segregation is still so widespread. His model, seen as a forerunner of the current agent-based modeling paradigm in economics, shows how even a relatively small preference for neighbors of one's own “type” can lead to neighborhood tipping and high levels of segregation. While agents, individually, may prefer living in majority-type neighborhoods for cultural or language-based reasons, segregation is considered a bad outcome because of the external effects that it can have on the society as a whole. For instance, it is commonly argued that racial segregation, especially when mixed with income inequality, may lead to unequal education and employment opportunities, the persistence of income and wealth inequality, and poverty traps. As Cutler and Glaeser [1997] demonstrate, blacks living in urban ghettos have reduced social and economic outcomes, such as lower high school graduation rates and labor market earnings. In general, severe segregation is associated with lower social well-being. Clearly people choose neighborhoods for many reasons beyond the racial and ethnic composition. In general, people weigh type-based preferences along with other location-based characteristics, such as the quality of the public schools, and the types of nearby stores. In addition, the utility derived from living in a specific neighborhood or community can be determined, in part, by the degree to which residents have positive interactions with their neighbors. In this paper, we consider an extension of the Schelling model by also having agents play a repeated Prisoner's Dilemma (PD) game with their neighbors. In short, agents will determine their location choice by the outcome of both the Schelling and PD games. Our interest here is in expanding the Schelling model to include other features beyond type-based preferences that can determine the residents' quality of life. People's utility derived from their residential choice is determined, in part, by the actions of their neighbors. In many cases, the actions of residents and their neighbors are endogenous. One example is that of property maintenance. As Robert Frost has written, “good fences make good neighbors”: if one neighbor does not contribute to the maintenance of a common fence, it will reduce the incentive of the other to maintain it as well.Footnote 1 Another example is parental involvement in the public schools. The more parents are involved, the more it will confer a positive benefit upon everyone in the school: increased student performance, a better sense of community, etc. The success of these neighborhood outcomes depends on agents' willingness to play a cooperative strategy in a neighborhood game. Social psychologists have documented a relationship between residents' sense of community, “neighboring,” and personal well-being. Sense of community — a psychological perception of how well neighbors get along — has been found to be associated with a greater sense of personal well-being [Farrell et al. 2004]. Neighboring is the exchange of goods and services among neighbors, such as the giving of information about good plumbers, the lending of power tools, or the provision of aid in an emergency. The willingness of neighbors to engage in these trades can directly influence residents, as the standard gains-from-trade models show, but can also improve the sense of community and, therefore, well-being [Farrell et al. 2004]. In addition, demographers have documented a negative relationship between people's sense of community (including the amount of neighborhood turnover) and their desire to move [Lee et al. 1994; Clark and Ledwith 2005]. Thus, one can envision an endogenous relationship between neighboring and mobility. Recent research in economics and sociology has investigated the effect of social capital and trust on agent behavior [Glaeser et al. 2000]. At the country level, greater degrees of trust among citizens have been found to increase economic growth and to decrease corruption. As well, research findings suggest that dense social networks can sustain trust; while interactions between different racial groups are often characterized by lower degrees of trust [Glaeser et al. 2000].Footnote 2 Marschall and Stolle [2004] found, in a sample of neighborhoods in the Detroit area, that there is a strong relationship between race and feelings of trust (holding income constant). They found, for example, that “neighborhood racial heterogeneity and neighborhood sociability significantly increase blacks' propensity to trust others” [p. 146].Footnote 3 Thus, social interactions among agents of different races or ethnicities can foster trust. It is within this context that we introduce the PD game into the Schelling model. Schelling's original model was designed to show how, even with strong preferences for integration, segregation was the only stable equilibrium. Our aim is to demonstrate that by introducing a model where cooperation (and therefore trust) among agents can develop endogenously, integrated neighborhoods can be an obtainable and stable equilibrium. Our goal then is to introduce a social dilemma game into the type-based interactions of a Schelling model in order to view how the potential maintenance of cooperation may impact the resulting levels of segregation. Of course, there are many possible social dilemmas that could be introduced. We choose to introduce a PD game due to its wide study in the social sciences and because it is a clear example of a social dilemma game. As many models have shown (discussed below), cooperation can be a sustainable outcome in a repeated PD framework under certain conditions. We view the emergence of cooperation here as the development of neighborhood trust and, also by extension, as the gains that are available to neighbors when they engage in neighboring. For example, one can imagine doing a favor (at one's personal cost) for one's neighbor and implicitly expecting that your neighbor would do a similar favor for you (at their cost). Thus, if one were to engage in providing favors that are not reciprocated then one has sacrificed the cost of doing these favors to the benefit of their neighbor and received the traditional “sucker's payoff” in a PD game while their neighbor has received the “cheater's payoff.” Or in an even simpler example, one may simply smile or wave at a neighbor as a sign of neighborly goodwill. A reciprocal greeting brings about better overall neighborhood relations but a lack of a reciprocal greeting may lead one to feel taken advantage of and potentially offer rewards of superiority to the shirker. One could imagine modeling a game for each of the potential interactions that occurs among neighbors. But we want to keep the model as simple as possible and also use a game that is well understood. Therefore, we collapse these potential interactions into a simple PD game and allow the reader to broadly interpret the game as representing many neighborhood interactions, such as the loaning of power tools, the provision of aid, and/or just being friendly.Footnote 4
 The reason the PD can be important within the Schelling game is that cooperation can potentially offset the loss of utility that neighbors receive when they live with neighbors of a different type. Our aim is to investigate under what conditions this can hold, and to what degree we can view segregation and cooperation as substitutes. That is to say, to what extent does the emergence of trust among neighbors offset or remove negative utility from living with different-type neighbors? To simplify matters, we model an equal proportion of agent types, as well as an equal initial proportion of agents who are “cooperators” and “defectors.” Certainly the interaction of agents can be more complex when one group is a minority and the other is a majority. Sociological research has found that black and whites in the US have different attitudes toward both integration and toward trust of neighbors [Marschall and Stolle 2004]. We leave this complicating variation of the model for future work. Here, we will show that low levels of segregation can be supported in our model if high levels of cooperation can also be supported as an outcome of the PD game. On the other hand, our model may generate even higher levels of segregation than are produced in the Schelling model when all agents defecting is the outcome of the PD game. Thus, our model leads to the conclusion that increasing social interactions can be helpful in reducing segregation if the process yields cooperation. But, social interactions should be limited if the interactions lead to non-cooperative outcomes. To the best of our knowledge no other paper has explored the effect that neighborhood cooperation can have in affecting the instability of integration in the Schelling model. The paper proceeds as follows. The next section discusses the related literature. Then the third section discusses the Schelling model with the inclusion of a utility function for agents. Next, in the fourth section we introduce the repeated PD game and the probability rules agents use in choosing whether to cooperate or not. Then, in the fifth section, we provide the model and results of the combined Schelling and PD game. Finally, the sixth section provides some concluding remarks.",6
34,4,Eastern Economic Journal,02 November 2008,https://link.springer.com/article/10.1057/eej.2008.32,The Impact of Imitation on Long Memory in an Order-Driven Market,October 2008,Blake LeBaron,Ryuichi Yamamoto,,,Male,Unknown,Mix,,
34,4,Eastern Economic Journal,02 November 2008,https://link.springer.com/article/10.1057/eej.2008.34,A Speculative Futures Market with Zero-Intelligence,October 2008,Leanne J Ussher,,,Female,Unknown,Unknown,Female,"The continuous double auction (CDA) was demonstrated in experimental markets by Smith [1962] to promote a speedy convergence, of price and quantity allocations, to the competitive equilibrium. This auction mechanism, which limits bilateral exchange to only the highest bid or the lowest ask, quoted up to that point, was even more dramatically acclaimed as efficient and Pareto optimizing in an agent-based simulation where profit optimizing agents were replaced with so called zero-intelligence (ZI) agents [Gode and Sunder 1993, 1997]. This paper will take a similar approach, formulating a ZI speculative model of a futures market with margin trading, transaction costs, short selling, and real-time settlement (RTS). This market, while much more volatile than traditional ZI models, also has Pareto optimizing tendencies due to the double auction (DA) mechanism and the imposition of scarcity via margin accounts held at the futures exchange, and not from rational competition between agents. This paper will elucidate the potential use of ZI agent-based models to simulate the impact that financial market architecture has on price dynamics. 
Gode and Sunder [1993] (hereafter GS), and then again more robustly Gode et al. [2004] (hereafter GSS), simulated financial market trading in an agent-based model, but replaced profit optimizing strategies and rational competition with random but constrained trading.Footnote 1 Despite a market populated with ZI traders, GS and GSS were still able to attain results where more than 90 percent of the gains from trade were quickly exploited and prices converged to the proximity of the theoretical equilibrium price. They concluded that intelligent architecture, such as the DA mechanism, and the constraint of scarcity, were alone enough to force trading prices and allocations to reach the competitive equilibrium.Footnote 2 This result was surprising within the economics profession, where the maximization of utility is thought to be a necessary assumption for the invisible hand to work, and hence the ZI model became widely cited.Footnote 3
 This paper will attempt to replicate these results in an institutionally rich environment which is liquidity constrained. It will also attempt to resolve the criticisms of the GS model by offering an alternative ZI model where ZI speculators (rather than buyers or sellers) trade in a futures market, and the imposition of a budget constraint and the marking-to-market of each trader's balance sheet in real time allows traders to lose equity when prices move against them. Market structure and trading rules cannot effectively be studied with assumptions of perfect rationality in analytical equilibrium models. Alternative methods include the use of intelligent traders in mechanism design using game theoretic techniques, or empirically via experimental markets or statistics. Mirowski [2007] has recently advocated the use of ZI models to study market structure. Such a platform appears to be a particularly useful alternative.
 Overall, the ZI traders are a tool to isolate and understand the effect of market rules on market outcomes. Understanding the effects of market rules and other social institutions is crucial because rules are observable and controllable, while individual strategies are inherently private and not directly controllable. Theories based on the effect of market rules are therefore easier to test. The ZI model provides a benchmark of the “structural” effect of market rules. The traditional strategic model in which traders respond fully to changes in market rules [and price outcomes] is another benchmark. The two benchmarks bracket the range in which human behavior lies [Gode et al. 2004, p. 2]. A significant empirical step forward in the ZI literature came when Farmer et al. [2005] was able to simulate, for a basket of stocks on the London Stock Exchange, 96 percent of the variance of the bid-ask spread, and 76 percent of the variance of the price diffusion rate (variance of price over time). The price dynamics in their ZI model were driven by the CDA and the gaps in the limit order book.Footnote 4 This empirical success by Farmer et al. makes the case for simulating institutionally rich ZI models that are applicable to specific financial markets.Footnote 5
 The contribution of this paper is its novel reformulation of the standard ZI spot market DA model ala GS and GSS into a speculative DA futures market with open outcry and retrading.Footnote 6 Each agent trades on margin, obeying exchange margin requirements and settlement rules. Chowdhry and Nanda [1998, p. 181] in an analytical model showed that “the rigidity of margin requirements is precisely what leads to price instability” via multiple equilibria. In a study on settlement frequencies, Farmer et al. [2004] found that a higher settlement frequency can have a positive impact on price volatility. Our model tries to combine both these insights. Since a margin requirement is only relevant with a settlement period, this model combines margin requirements and RTS. The marking-to-market of margin accounts in real time promotes the interaction of agents producing feedbacks into the volatility of returns. Interaction and feedback is at the heart of most agent-based models, but in this case it is through wealth and quantity effects rather than expectations and information. The risk neutrality of the ZI speculators and RTS characterizes a study of an illiquid market setting. Changes in prices can produce forced liquidations and simulations that show clustered volatility in returns. Preliminary findings suggest that leverage increases the likelihood of multiple equilibria that can lead to sustained price volatility. Leverage also multiplies wealth transfers, which also has an impact on price dynamics. The ZI speculative model simulates returns that are characteristic of real markets, not possible in the simpler GS and GSS models that converged to a single Pareto optimal price and trading stopped. In this model, despite retrading, prices can always move away from the Walrasian equilibrium creating new price dynamics. This model is able to produce endogenous liquidity shocks, volatility clustering, and fat tails in returns. And yet despite the presence of realistic return moments during the trading process, the price series remains anchored by the Walrasian equilibrium at any one point in time analogous to the original GS and GSS models. In the next section, there is a basic introduction to the GS and GSS model vs the paper's speculative ZI model of a futures market. This section is followed by the specific details of the model and an analytical derivation of a ZI speculator's demand function. The paper then presents results from ZI speculator simulations across three different levels of leverage and concludes.",6
34,4,Eastern Economic Journal,02 November 2008,https://link.springer.com/article/10.1057/eej.2008.31,"The Future of Agent-Based Research in Economics: A Panel Discussion, Eastern Economic Association Annual Meetings, Boston, March 7, 2008",October 2008,Jason M Barr,Troy Tassier,Shyam Sunder,Male,Male,Male,Male,"
LeBaron: Agent-based economics, and more generally agent-based social sciences, have been around in various forms for over 30 years. The advent of higher speed computing and new tools for the computational learning fields led to a major increase in activity in the early 1990s through today. Research activity continues to increase at the current time, but the field still remains somewhat of a “niche field” inside economics. Certain conferences and certain regions (such as Europe) are well populated with agent-based activity. However, at mainstream conferences inside the US one would have a hard time in finding agent-based researchers. Why is this so, and what might be fruitful directions for the field to go in? One key problem that is often cited is that agent-based modelers have still not come up with a “killer app.” This would be an economic model that is relatively simple, and understood by mostly all economists, but for which agent-based approaches give surprising, and hopefully empirically valid, results. Why hasn't this happened yet? I think there are several reasons. First, agent-based modelers have tended toward economic realism by building fairly detailed and complex platforms to work with. In their recent book John Miller and Scott Page [2007] stress a methodology of building simplified computer experiments that produce sharp controllable results. There are two problems that have led to models which are often more complicated than needed, and these are related to the use of computers in economics. Researchers often feel that once you are using a computer you shouldn't skimp on model details. You might as well go all the way if you've given up on analytics a long time ago. However, simplicity is still useful in being able to interpret your computational results. A related issue is a feeling that if you make a lot of simplifications to get to a highly stylized model, then you ought to be able to get some analytic results. It is often depressing, but I find the usefulness of analytics disappears very quickly (even for representative agent rational expectations models). I think a world of more stripped down, but still computational, models will lead to more thinking along general principals, and cross model comparisons. An interesting corollary to this has been the relative difficulty in getting institutions to appear from the “bottom up” in an agent-based framework. In the early days of computational modeling this was kind of the basic dream. A world would start out as a general soup of interacting purposeful agents, and from this we would see the emergence of recognizable socioeconomic entities such as governments and markets. This has proved much more difficult than originally thought, but it remains an interesting and important goal for agent-based models. Related to this is the general understanding of institutions and their importance in guiding economic decision making. A key benchmark has been the so-called zero intelligence model [Gode and Sunder 1993], where researchers concentrate on the institution alone, and agents are assumed to be as simple as possible (often random subject to constraints). There is also much hesitation in economics about computer modeling in general. Some of this is extreme caution about new things, but some of this may have validity. Analytic theories have the advantage that anyone can read them, and push them around, and modify them as they see fit. This is not really true with computational models. This has been a known barrier for some time in the agent-based modeling (ABM) world. Questions about what software to use, how to distribute and test code, have all dominated discussions of agent-based researchers for quite some time. I actually think some new tools are starting to become available which may help in this area. In particular, the computer language Netlogo can be very useful for building small-scale models that researchers want to put out and have others play with. As a language it has many good features. It runs on most platforms. It is relatively easy to get it up and running, and is also pretty simple to program. Finally, it has excellent graphics and animation, which can be tied to very nice user interfaces. (User interfaces seemed like a waste of time to me 10 years ago, but now I'm not so sure. They may be an important part of selling agent-based models to the world.) Unfortunately, I don't think any computational platform is perfect, but I'm still hopeful more and better tools will continue to appear. Getting more people than a handful of very motivated Ph.D. students to use your code is a very important thing for this field. Finally, I'm not sure whether we are all that good at the construction of heterogeneous agent models. It is not something that our skill sets are all that well developed to do. Well-crafted economic models form excellent thought experiments that you can't get out of your head. Theorists do not get at these models by random chance. Our abilities at constructing models of heterogeneous interacting agents are still primitive, and it will take time for us as model designers to make progress in this area. The most often discussed topic in ABM is empirical validation. General acceptance of these models will need some ability to align with and/or explain features of real-world economic data. It is clear that this is an important part of the ABM agenda, but the field should probably not become too obsessed with this. Agent-based models share many empirical validation features with other economic models, and some discussions of validation often overlap with problems that could be given for all realms of empirical science. However, there are some special characteristics that are a little tricky. First, agent-based models do have many degrees of freedom. It is not just parameters, but the choice of entire learning algorithms is up to the researcher's discretion. Also, most of these models have been released from strict optimizing behavior. There are several ways to handle the degrees of freedom problem. One is to boost the number of facts that you fit. This simply pushes the bar higher. Agent-based models have the advantage of generating both micro and macro time series. One can then line up with cross sectional and time series results. You can also generate “perfect” data sets of panels, which could then be compared to some rougher imperfect panels from the real world. Another interesting area is to use human experiments as a kind of calibration area to get parameters for use in the agent-based models. Fit learning algorithms to experimental data, and then take these algorithms in to a larger scale computer model. Agent-based simulations often have many interesting time series features. Non-linearities and chaotic dynamics make them very interesting to study, but also make them difficult to estimate and work with. Also, they may exhibit path dependence and non-ergodicities, which again cause serious difficulties for estimation. Finally, many simulations look at interesting ways in which agents interact with each other either in space or through social networks. Both of these bring new empirical challenges, since they are not part of the standard econometric toolkit we are used to. Our interpretation of empirical exercises is probably most effective for realms where there are other more traditional competing models available (such as finance). A kind of empirical race between the different approaches can be very useful. However, there is no well-defined way to declare a winner. Finance is an interesting case of this where fitting empirical features requires fairly complicated preferences for a representative agent, but agent-based models can fit many features with much simpler preferences inside a multi-agent learning dynamic. There also may be features (such as trading volume) where standard models have nothing to say, but the agent-based model provides useful information. Finally, it is important to keep data fitting issues in perspective. We often learn more from wrong models than from carefully tuned ones. If a biological modeler were trying to simulate evolution, should all the models that didn't generate the appropriate distributions of life forms on Earth be thrown out, or is something learned from the failures about the evolutionary process. Some models that are far from the data can still be important as counter-factuals, and interesting thought experiments. How will these models be used for policy recommendations? They probably will be used at two extremes. First, as parts of large-scale computer simulation systems. Examples of this are already in use in some governments and government agencies. These are large and are very complex. One example would be traffic simulations, which can be programmed at a very realistic level of detail. However, I think policy will also be influenced by simple stylized models of small to intermediate size. I think agent-based models will be much more likely to play an advisory role rather than as large macro models estimated and run on the entire economy. Also, they may have much to say about institutional design. An example might be the current credit crises. Agent-based simulations could make predictions about systemic risk in credit markets, but to do this well they might need a lot of confidential information on cross holdings. Without this confidential information they may be useless in predicting a system crash. However, they might be able to predict the stability of various trade networks, and which types of cross holding networks form under different policy regimes. Related to this, their big strength might be to advise during periods of stress, when various markets are not well approximated by standard equilibrium relationships. 
Chen: The theme of this panel is the future of agent-based research in economics. One main issue that concerns most of us is: can agent-based computational economics become a part of the mainstream in the future? Maybe we can amuse ourselves a little by constructing an agent-based model to do the forecast, hence the title of my discussion, “An Agent-Based Model of Agent-Based Economics.” As a simple start, we may consider the familiar two-type model, such as the fundamentalist-chartist model in finance. This model can be formulated into a jump Markov process, and the solution to the resultant master equation shall be the answer to our concern. The only thing we need to know about the jump Markov process is the transition rates. Roughly speaking, we have to know how likely it will be for a non-agent-based economist to convert to agent-based research, and how likely it will be for a cadet to initiate his career using agent-based models. A number of determinants have already been mentioned in the list of questions submitted to the panel, such as job opportunities and research publicity. Using these performance-based criteria (fitness functions) is very standard in agent-based economic modeling, so they should be readily included into the transition rate function. However, there are also utility-based criteria. Familiar psychological impacts, such as herding, may also be taken into account. Other than that, I want to single out another three equally important utility-based criteria, which received relatively less attention during the past discussion of the transition rate function. The first one is beauty. One attribute of beauty is whether we can conceptually harness what we are modeling. Of course, a parsimonious model is easy to harness. Another attribute of beauty is whether we can expect the unexpected, that is, novelty and surprises. The success of Thomas Schelling's [1978] agent-based segregation model is that it has both of these attributes of beauty and thus has become a classic in the ABM literature. It is unfortunately true that many current agent-based models are “notorious” because of their large number of parameters. Nonetheless, complex models are not necessarily “ugly,” if it has a modular design [Simon 1965]. Modulization is a powerful “decoration” for complexity. It allows the users or the prospective followers of the model to take incremental procedures to re-display or re-examine the models so that the transparency and comprehensibility of the model can be enhanced. Gode and Sunder's [1993] zero intelligence agent serves as a good illustration of such a design. The next criterion is mobility. Mobility has two parts. The first part is the mobility of our mind about a given model or issue. Each model or issue has its boundary, partially defined by its assumptions, parameters, etc. Many times we have the desire to know what will happen beyond the boundary. For example, what would be the lessons if the CARA type of risk preference is replaced by the CRRA type? Mobility measures how easy we can move to these different scenarios. One advantage of the agent-based model is its readiness to simulate many very different “what-if” scenarios, as well as to conduct robustness checks or sensitivity analysis. This highly mobile environment is very beneficial for us to conduct thought experiments. The second part is the mobility across different disciplines. We now have evidence that ABM is not just a language uniquely owned by economists, but is also a language widely shared by other social scientists. The driving force of computational social sciences is, in fact, ABM and simulation. In addition, ABM's emphasis on agents or software agents has also increased its exposure to behavioral economics and experimental economics. The latter's state of art is no longer a lab with only human subjects, but a lab comprising both human agents and software agents. New theory and tools applied to agent engineering further bring in ideas from cognitive sciences, neural sciences, and artificial intelligence. Maybe in the future we can have another panel on “The Future of Economics in the Agent-Based Interdisciplinary Era.”
 The last criterion is freedom, that is, the capability to maximize the enjoyment of doing research. Agent-based tools free us from the usually stringent analytical constraints so that we can address either the same questions using much more relaxed assumptions or new questions, which can hardly be reached under the conventional constraints. According to my personal observation, many economists who invest in agent-based research do so largely because of this consideration. I can certainly add more determinants, such as the technology of the ABM, to the transition rate function, but I am afraid that would cause the resultant master equation or the Fokker-Planck equation difficult to solve. What is the future of agent-based research in economics? Maybe we want to keep the curiosity instead of making a hasty prediction. After all, the study of the complex systems from John Conway's Game of Life to Stephen Wolfram's cellular automata are all filled with interesting unpredictable patterns. Can agent-based research become a dominant approach in economics someday? Want to bet? 
Sunder: In these remarks, I shall assume that ABM has economics as its end objective and end result. Economics is a social science which concerns the behavior and properties of communities or institutions populated by real live human beings. How does ABM help advance economics? What are these advances and what could they be? I believe ABM has contributed, and can continue to contribute, to economics in spite of the fact that the ABM label itself emphasizes neither the human nature of the agents, nor the communitarian nature of economic phenomena. To the contrary, I shall argue that ABM's contribution to economics arises precisely because of these differences. But it is important that my perspective on ABM is one of the contributions to economics through use of this technology and not on this fascinating technology itself. I understand that for many scholars, the ABM discipline is of deep interest in itself for many reasons. Allow me to use a parallel to clarify my point. Like ABM, statistics is a deep discipline with a long history and extensive literature of its own. Application and use of statistical reasoning and modeling to economic questions has contributed greatly to accomplishments of economics; and these contributions have been widely recognized. In addition, attempts to use statistics for addressing substantive economic question has led, over the past century, to the evolution of a new cross-discipline of econometrics which has developed a tradition and extensive literature of its own. Today, statistics, econometrics, and economics coexist with parallel, partially overlapping yet distinct identities. While there are plenty of scholarly contributions which could go either to economics or to econometrics journals (and the same is true of econometrics and statistics journals), the same is not true for economics and statistics. For economics, statistics is and will remain an instrument of research, no matter how valuable its applications to economics become. The same is also true of economics and mathematics with mathematical economics being the bridging sub-discipline between the two. I am not qualified to assess the contributions of econometrics and mathematical economics to statistics and mathematics, respectively. However, it is clear that the importance of these bridging sub-disciplines to economics arose from their contributions to substantive problems of economics. Perhaps it is not inappropriate to think of a parallel relationship among economics, agent-based economics, and agent design. The last of the three draws from, perhaps even lies substantially in, the domain of computer science and artificial intelligence, and draws on their knowledge base and technologies. Agent-based economics could be thought of, like econometrics, as a bridging sub-discipline between ABM and economics. As a specialized branch of ABM, agent-based economics focuses on agent models developed specially to address the problems of economics. When we use ABM to address problems of general interest in economics, they are contributions to economics itself. The future of ABM in economics will depend on our ability and willingness to address substantive problems in economics. Statistics and its economics specific branch econometrics have found an important place in economics, not just because they developed better estimators and discovered their properties (that is development of the method itself) but because they were better able to estimate, for example, the effect of education on productivity of labor. I think it is reasonable to say that the general body of economists has a similar attitude to ABM and other methods. The future place of this method depends on contributions of the ABM technology to address substantive problems of economics as a social science. The more successful we are in this endeavor, greater will be the acceptance of the method in economics. In making this assertion, I have said nothing new; because this applies to all disciplines. So, what is the general area of economics to which ABM can make substantive contributions? Physical sciences deal with discovering the universal laws of nature that apply across time and space and concern the behavior of inanimate objects or symbols [see Sunder 2006]. At the opposite end from the Science Hill on the Yale campus lie the humanities departments of literature, religion, philosophy, art and music, etc. Literature looks not for universal laws that govern the behavior of humans but eternal truths about our nature. Even though each human being is unique, endowed with free will to do as we wish, yet the eternal truths of love, hate, courage, greed, jealousy and fear appear repeatedly throughout human history and literature. Social sciences try to create a space for themselves between the sciences and the humanities. Since the object of study in social sciences is our own sentient selves, we remain uncertain about the ground under our feet. We are not quite sure of exactly what we humans are. On the one hand, we wish to have the honor of being a science and accordingly we seek universal laws that might explain and predict what we do. This pursuit leads us to model ourselves as a stone rolling down the hill under external force of gravity or a leaf blown about by wind. For the stone, the leaf, as well as the homo economicus, universal laws applied to fixed characteristics of the objects of study help us understand what happens to them. On the other hand, we are reluctant to believe that we are like a rock or a leaf, and let go the belief that we have free will to choose what we eat, and where we go. Is our behavior simply driven for external forces and our own predefined properties? Unfortunately, free will so essential to our sense of self, and universality of laws we seek in order to become a science, do not mix well. That is where the “social” part of social science comes to help. It is possible that even as individual free will may preclude the possibility of predicting individual behavior, aggregate level outcomes of larger groups of individuals may be subject to fixed and discoverable laws. It is this possibility that holds a rich promise of substantive and substantial contributions of ABM technologies applied to the problems of economics as a social science. As our friends in psychology examine individual behavior, ABM has already yielded some interesting results in identifying systematic properties of market institutions populated by simple agents. So my hope and expectation is that ABM can and will flourish as a method of making contributions to problems and economics by serving as a bridge between individual behavior (the domain of psychology, and captured by ABM technology) on the one hand and aggregate outcomes (which is the primary topic of interest in economics) on the other. In summary, I am optimistic about the role of ABM in economics as a method of addressing substantive problems of economics — especially understanding the properties of social and economics institutions populated by agents of various kinds — something we can design, manipulate, and examine using agent based models (ABMs).",7
34,4,Eastern Economic Journal,02 November 2008,https://link.springer.com/article/10.1057/eej.2008.33,Eastern Economic Journal Annual Index,October 2008,,,,Unknown,Unknown,Unknown,Unknown,,
35,1,Eastern Economic Journal,09 January 2009,https://link.springer.com/article/10.1057/eej.2008.38,35th Anniversary Issue of the Eastern Economic Journal,January 2009,Joyce P Jacobsen,Gilbert L Skillman,,Female,Male,Unknown,Mix,,
35,1,Eastern Economic Journal,09 January 2009,https://link.springer.com/article/10.1057/eej.2008.49,The Economy and the Economics Profession: Both Need Work,January 2009,Barbara R Bergmann,,,Female,Unknown,Unknown,Female,"On the occasion of the 35th anniversary of the Eastern Economic Association (EEA) and its Journal do we want to be upbeat, congratulatory, prideful? Or do we want to take the opportunity to confess our weaknesses and tell over our sins? Both, I would argue. Certainly, we want to register our pleasure at the survival of our organization, and express thanks to our colleagues who over these years have kept it going, who have brought us together at the annual meetings, who have labored on our Journal. We want to celebrate the fact that the EEJ has been far more welcoming to unorthodox approaches than most. But at such times it is also useful to step back and think about whether we are pursuing what ought to be a science in the best scientific manner. I tried to do that in my inaugural address as EEA's first president, and here I go again. My children tell me I am a bit of a nag, inclined to stress the negative. Thirty-five years of scolding fellow economists about the same thing suggests they are right. As I write this, anxiety about the US economy is very high. The year of the EEA's founding was also a year of economic crisis. Then as now, the Middle East was in turmoil, and energy prices were soaring, presaging price increases throughout the economy. Then as now, the unemployment rate was headed upward. Then there was no set of policies at hand to fight at once the two maladies that constitute “stagflation.” Now the initial proposal to resolve the financial meltdown was abandoned in a few weeks, suggesting clueless improvisation. Back then, the factionalism in the profession, and the failure of economists to come up with a set of policies that would effectively deal with the economic crisis was obvious to all. An editorial cartoon published that year by Pat Oliphant showed a group of chimps in a cage. A sign on the cage said “Economic Advisers — Do Not Feed — Ever.” Recently, as a new economic crisis has loomed, Oliphant published another cartoon dissing economists. In this latest cartoon, the economists are not in a cage; they are shown at a meeting of the Fed's Open Market Committee. Now only one of the economists is portrayed as a chimp; the rest are merely idiots. Unfortunately, not much progress has been made since 1974 in the situation in the Middle East. Nor, truth to tell, has there been much progress since then in the ability of economists to understand the economy or manage the crises that periodically arise. The poor state of economic science is what produces and allows for the well-known lack of agreement among professional economists on policy with regard to such issues as unemployment, budget deficits, taxes, inflation, international trade, the promotion of growth, and the government regulation of business. What is even more disturbing than the lack of agreement is the fact that political ideology quite obviously determines which side of any controversy about economic theory or economic policy any particular economist is likely to take. A very few biologists endorse intelligent design. But biologists do not divide on the big questions in their discipline according to whether they are Democrats or Republicans. Shamefully, our profession does. In my presidential address in 1974 I attempted to convey that our failures to advance were mainly based on two deficiencies in our methodology: a problem with micro and a problem with macro. For economics to advance — indeed for economics to become worthy to be called a science — new methodologies have to come to the fore. A more empirically based microeconomics has to be hitched up rigorously to an improved method of macroeconomic modeling. We would then have a better chance of advancing toward a more realistic picture of how the economy operates and toward a better basis for diagnosing and treating its ills. In the past 35 years some new and different modes of empirical work have become more common, but I would argue that they do not take us as far as we need to go, and that basic problems with the discipline remain.",6
35,1,Eastern Economic Journal,09 January 2009,https://link.springer.com/article/10.1057/eej.2008.48,In Praise of Modern Economics,January 2009,David Colander,,,Male,Unknown,Unknown,Male,"As a fellow critic of the economics profession, I find much to agree with in Barbara Bergmann's reflective review essay on the economics profession [Bergmann 2008]. Both the economy and the economics profession need lots of work. But in a spirit of general irascibleness — a spirit that I suspect Barbara is one of the few who can fully appreciate — I will take a somewhat different position. Specifically, I will argue that Bergmann is far too hard on the profession in her description of what the profession has accomplished in the 35 years since the founding of the Eastern Economics Association. Thirty-five years ago economics had serious problems: there was a limiting orthodoxy; students were cynical and treated economics like a game, and there was a notable lack of intellectual excitement within the field. Today, modern economics is a vibrant and advancing field that is intellectually alive and open to discovery. The orthodoxy has disappeared and graduate students are excited about what they are doing. Modern economics deserves praise, not nagging.",2
35,1,Eastern Economic Journal,09 January 2009,https://link.springer.com/article/10.1057/eej.2008.43,Smart Taxes: An Open Invitation to Join the Pigou Club,January 2009,N Gregory Mankiw,,,Unknown,Unknown,Unknown,Unknown,,
35,1,Eastern Economic Journal,09 January 2009,https://link.springer.com/article/10.1057/palgrave.eej.9050041,"Gender Pay Gap, Productivity Gap and Discrimination in Canadian Clothing Manufacturing in 1870",January 2009,Catherine L McDevitt,James R Irwin,Kris Inwood,Female,Male,,Mix,,
35,1,Eastern Economic Journal,09 January 2009,https://link.springer.com/article/10.1057/palgrave.eej.9050042,What Determines Student Evaluation Scores? A Random Effects Analysis of Undergraduate Economics Classes,January 2009,Michael A McPherson,R Todd Jewell,Myungsup Kim,Male,Unknown,Unknown,Male,"Student evaluation of teaching (SET) at the college and university level and its determinants has been an area of active research for more than a half-century.Footnote 1 The large and growing literature in this area points to the importance of the role that SET scores have come to play in academic departments. For example, colleges and universities routinely use SET scores to assess the quality of an instructor's teaching for purposes of promotion and tenure. Furthermore, SET scores are often an important component in deliberations for merit or excellence raise allocations. While some strands of the literature in this area debate whether or not SETs should be of such central importance, the fact remains that these scores have been and continue to be used extensively. Understanding the determinants of SET scores may be of considerable interest and utility to instructors and to administrators. Despite the breadth of the literature, much of the research has been unconvincing due to either data difficulties or statistical shortcomings. This paper takes advantage of an unusually large panel of data from 24 consecutive semesters comprising economics courses taught at a large public university. While McPherson [2006] analyzes a smaller portion of these data, his use of a fixed effects methodology precludes an examination of characteristics of instructors that are time-invariant. Instead, we use a random effects model estimated with feasible generalized least squares (FGLS). This enables an examination of instructor-specific, time-invariant characteristics such as gender and race. In addition, our method permits a proper accounting of unobservable effects specific to individual instructors. In the earlier literature there are only a small number of examples of efforts to tackle this important issue [Mason et al. 1995; Tronetti 2001; Isely and Singh 2005; McPherson 2006; McPherson and Jewell 2007]. A final area of interest involves the manner in which faculty members are ranked according to SET scores. Based on our estimation, we suggest at least two ways in which rankings could be usefully adjusted to account for extrinsic factors that might otherwise pollute the rankings. For example, if instructors can increase their evaluation scores by causing students to expect higher grades, departments may want to adjust rankings to eliminate the incentive to engage in such behavior. Similarly, if teaching intermediate-level theory classes means an instructor will receive lower evaluation scores than a colleague assigned to teach upper-level electives, then certain instructors may find themselves at a disadvantage when merit raise or tenure decisions are made. We show that such adjustments can lead to statistically significant changes in departmental rankings based on SET scores.",62
35,1,Eastern Economic Journal,09 January 2009,https://link.springer.com/article/10.1057/palgrave.eej.9050043,How Competitive is the US Manufacturing Sector?,January 2009,Fatma Abdel-Raouf,,,Female,Unknown,Unknown,Female,"In 1982, William Shepherd examined the market structure of the US economy, including the manufacturing sector, where he found that in 1980, 69 percent of the manufacturing sector operated in effective competition (loose oligopoly, perfect competition, and monopolistic competition), 27.08 percent operated in tight oligopoly, 3.92 percent operated in dominant firm markets, and none operated in monopoly. Moreover, Shepherd compares the market structure of the US economy over different years: 1939, 1958, and 1980. Shepherd finds that the US manufacturing sector became more competitive over the aforementioned periods of time, with most of the changes happening between 1958 and 1980. In particular, Shepherd finds that the monopoly's share of the US manufacturing sector decreased from 0.75 percent in 1939 to 0.35 percent in 1958 and to zero in 1980. The tight oligopoly's share increased slightly from 36.4 percent in 1939 to 37.46 percent in 1958 then decreased to 27.08 percent in 1980. The dominant firm's share witnessed the largest decrease over the same period from 11.35 percent in 1939 to 6.29 percent in 1958 and to 3.92 percent in 1980. Over the ensuing decades, the manufacturing sector has witnessed some changes: Its share of GDP, employment, and employees’ compensation decreased from 19.66, 20.75, and 26.37 percent, respectively, in 1980 to 15.41t, 14.43, and 16.76 percent, respectively, in 1997.Footnote 1 In addition, the 1980s and 1990s witnessed two movements, the large merger movement of the 1980sFootnote 2 and the increase in imports. The former increased concentration, while the latter decreased it.Footnote 3 Furthermore, there were large cutbacks in antitrust activities and increases in deregulation during the 1980s, especially during the Reagan administration. The antitrust activities increased slightly during the 1990s but were still considered weak. As such, an update of Shepherd's work is needed, and that is the intention of this paper. Doing so enables us to see whether the US manufacturing sector has become more or less competitiveFootnote 4 than it was in 1980. Note, however, that this research uses 1997 data, which is organized and classified by the North American Industry Classification System (NAICS). This system was first introduced in 1997. Earlier data (1992 and prior years) are organized and classified by the Standard Industry Classification (SIC) system. In assessing the structure of the US manufacturing sector, I am going to use the 1997 four-firm concentration ratio. Knowing its shortcomings as a measure of market structure, correction methods will be used to remedy these shortcomings. Four corrections are used to address overaggregation, underaggregation, market locality, and international trade. The modified concentration ratio — in conjunction with other criteria — will be used to analyze the structure of the US manufacturing sector. In addition, a comparison will be made between the published and corrected concentration ratio to see how closely the former substitutes for the latter. Comparing the result of this research to that of Shepherd's in 1982, I find that the US manufacturing sector has become slightly more competitive than it was in 1980. In particular, I find that 58 percent of the manufacturing sector operate in competitive markets, 17.5 percent operate in loose oligopoly, and 24.5 percent operate in tight oligopoly markets. Moreover, confirming Shepherd's results, none of the manufacturing sector industries in 1997 operates in a monopoly market. Furthermore, agreeing with Shepherd's findings in 1982, international trade plays an important role in keeping the US markets competitive. International trade increases the percentage of the manufacturing sector operating in competitive markets by 31.6 percent, while decreasing the percentage of the manufacturing sector operating in loose and tight oligopoly markets by 35.2 percent and 15.3 percent, respectively. The paper proceeds as follows: The next section shows the importance of concentration and market structure. Then the further section introduces the reader to the new industry classification system —NAICS. The subsequent section discusses the concentration ratio, its shortcomings, and the methodology used to correct for these shortcomings. The penultimate section presents the results. The last section concludes the research.",1
35,1,Eastern Economic Journal,09 January 2009,https://link.springer.com/article/10.1057/palgrave.eej.9050044,The Impact of 9/11 on the Persistence of Financial Return Volatility of Marine Firms,January 2009,Anthony C Homan,,,Male,Unknown,Unknown,Male,"The attacks of September 11, 2001 had both political and economic effects. After 9/11, investors may have perceived that the physical assets of the transport system were not only targets but were a means to carry out terrorist attacks. Ships, goods, cargo, and facilities can all serve as weapons of destruction of terrorism. Additionally, the response to a significant terror attack has wider impacts on trade and transportation. For example, the US government response to 9/11 included shutting down the traffic system, which caused huge delays and disruptions to users of the port system [Bichou 2004]. Because of potential adverse impacts to future business operations, investor's perceptions of future profit and dividend streams would be less than before. In efficient market pricing theory, prices are a function of those streams and the market translates new information and perceptions on the threat of terror attacks into prices. Similarly, the market translates this information into changes to the assets' financial risk profile based on its underlying relationship with the market. In other words, if 9/11 exerted a relatively negative impact on the financial risk of marine operators, then these firms would face a higher relative financial risk than the market as a whole. If the market viewed these events as having a long-term impact on the operations of directly impacted firms such as marine operators (and airlines), then there could be long-term adverse market effects to these firms. Conversely, if the market viewed the terrorist attacks as a one-time fluke event, there would not be any expected long-term impacts on financial risk. Preliminary results are that there have been long-term effects. Drakos [2004] found that there was an increase in systematic risk (beta) on a set of airline stocks following the terror attacks of September 11, 2001. His results had adverse implications for portfolio diversification and the cost (and ability) of airlines in raising capital. Drakos also found that idiosyncratic risk (return volatility) increased significantly as well; this result implies increased market risk for those firms. Both were real economic costs and were ancillary costs resulting from 9/11. Similar to Drakos, Homan [2006] also found that 9/11 resulted in a structural increase in systematic and idiosyncratic risk for a sample of marine operator firms listed on Nasdaq and the NYSE. Both of these papers investigated first and second moment effects on the return probability distribution. This paper studies the effects of 9/11 on kurtosis and the persistence of idiosyncratic risk (volatility) on the same set of marine operator stocks studied by Homan. The analysis focuses on investigating structural changes in return volatility (idiosyncratic risk) following 9/11 and structural changes affecting the persistence of that return volatility. The paper builds on Homan and differs from it in that it investigates the persistence of second moment effects and investigates fourth moment effects. The paper is organized as follows: The next section provides a description of the data sources used and the firms in the sample. The third section provides a general background on the finance theory underpinning the discussion of total financial risk, and the section that follows discusses the impact of 9/11 on changes in return volatility and kurtosis. The fifth section discusses the impact of 9/11 on the persistence of return volatility using a generalized autoregressive conditional heteroskedasticity (GARCH) approach. The sixth section provides a sensitivity analysis. The section investigates results using higher order GARCH models, looks at results after controlling for significant firm-specific events, and investigates the results using asymmetric models. The section also compares the results to two samples of firms potentially affected by 9/11: one that is expected to be more affected (airlines) and one that would be somewhat less (more diversified marine firms). The final section concludes the paper.",6
35,1,Eastern Economic Journal,09 January 2009,https://link.springer.com/article/10.1057/palgrave.eej.9050045,Is More Always Better? Empirical Evidence on Optimal Portfolio Size,January 2009,Alla A Melkumian,Arsen V Melkumian,,Female,Male,Unknown,Mix,,
35,1,Eastern Economic Journal,09 January 2009,https://link.springer.com/article/10.1057/palgrave.eej.9050046,"The Internet's Impact on Competition, Free Riding and The Future of Sales Service in Retail Automobile Markets",January 2009,Ellen Sewell,Charles Bodkin,,Female,Male,Unknown,Mix,,
35,1,Eastern Economic Journal,09 January 2009,https://link.springer.com/article/10.1057/eej.2008.1,NJ and PA Once Again: What Happened to Employment When the PA–NJ Minimum Wage Differential Disappeared?,January 2009,Saul D Hoffman,Diane M Trace,,Male,Female,Unknown,Mix,,
35,1,Eastern Economic Journal,09 January 2009,https://link.springer.com/article/10.1057/eej.2008.6,"Handbook on the Economics of Happiness, by Luigino Bruni and Pier Luigi Porta",January 2009,Amitava Krishna Dutt,,,Male,Unknown,Unknown,Male,,
35,1,Eastern Economic Journal,09 January 2009,https://link.springer.com/article/10.1057/eej.2008.7,"The Institutional Economics of Corruption and Reform: Theory, Evidence, and Policy",January 2009,Udaya R Wagle,,,Female,Unknown,Unknown,Female,,
35,1,Eastern Economic Journal,09 January 2009,https://link.springer.com/article/10.1057/eej.2008.5,"The Origin of Wealth: Evolution, Complexity, and the Radical Remaking of Economics",January 2009,Richard V Adkisson,,,Male,Unknown,Unknown,Male,,
35,1,Eastern Economic Journal,09 January 2009,https://link.springer.com/article/10.1057/eej.2008.36,Financial Statement for the Eastern Economic Association for Fiscal Years 2006–2007 and 2007–2008,January 2009,,,,Unknown,Unknown,Unknown,Unknown,,
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2008.20,Kantian Ethics and the Prisoners' Dilemma,April 2009,Mark D White,,,Male,Unknown,Unknown,Male,"The prisoners' dilemma game stands as the prototypical example of conflict between individual and collective rationality: through mutual pursuit of self-interest, all players end up worse off than if they had behaved otherwise.Footnote 1 It has been suggested that ethical behavior, as opposed to self-interested decision-making, would help solve prisoners' dilemma problems: most prominently, Amartya Sen, describing the classic bank robber interpretation of the game, has written that “it is indeed easy to see that it will be difficult to find a moral argument in favor of confession by the prisoners” [1974, p. 77], implying that any ethical system (other than ethical egoism) would demand cooperative behavior in prisoners' dilemma situations, including Cournot oligopolistic competition, private contributions to finance public goods, and arms races, to mention just a few. As an example of such an ethical system, many scholars have focused on the moral philosophy of Immanuel Kant, the 18th century German philosopher best known for his emphasis on duty and autonomy. They argue that application of Kant's ethics, as represented by his famous categorical imperative, would require that the players in such games cooperate rather than deviate, and therefore reach the Pareto-superior outcome. In this note, I argue that Kant's moral theory, while commonly thought to be very demanding, does not in fact guarantee a solution to the prisoners' dilemma. This is due to the fact that Kant ruled out many actions (and omissions) as immoral, but did not specify precise positive moral requirements of the sort that would correct the behavior that creates the suboptimal outcome. I will demonstrate this in terms of two versions of Kant's categorical imperative and with reference to his distinction between perfect and imperfect duties.",6
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2008.3,Mexican Migration to the US: A Comparison of Income and Network Effects,April 2009,R Todd Jewell,David J Molina,,Unknown,Male,Unknown,Male,"In analyzing the decision to migrate, researchers have investigated the effects of both market and non-market factors. The introduction of market factors to models of migration began in earnest with the work of Todaro [1969] and Harris and Todaro [1970], where the concept of the opportunity cost of migration is introduced. The opportunity cost of migration is a function of relative wages, since migration means giving up income in the home location for income in the destination location and not migrating means giving up income in the destination location. More recently, other market factors such as relative income [Stark and Taylor 1989; 1991] have been introduced to the migration model. If individuals migrate in order to improve their income relative to others, then the migration decision may be impacted by the distribution of income in the home location. Non-market factors have increasingly been used to explain the decision to migrate. The most commonly used non-market factors are migration networks. Migration networks are connections established between the home location and the destination location that facilitate migration. Mexico–US migration is not only an issue of interest to social scientists, it is also of profound interest to policy makers in both countries. In January 2004, US President Bush announced details of a temporary worker program for Mexican immigrants, including an expansion of opportunities for legal, temporary work in the US and an increase in employment-based green cards [Loven 2004]. Former Mexican President Fox has also proposed an increase in visas for Mexicans from the current 75,000 per year to 250,000 per year with the expectation that by 2015 the number required yearly would decrease due to improved economic performance in Mexico [Leiken 2002].Footnote 1 Proposals such as these normally assume that market factors have strong impacts on Mexican migration to the US and that market factors will unambiguously affect the migration decision. There are two major problems with these assumptions. First, the impact of market factors is only part of the story, since non-market factors must also be included in any analysis of the effect of policy on migration decisions. Second, market factors may have ambiguous effects on migration. Although this is ultimately an empirical issue, theory suggests that an improvement in the Mexican economy may result in either an increase or a decrease in migration to the US. However, there is no reason to expect a priori that the effect will definitively be negative. The focus of this paper is to analyze the decision to migrate from Mexico to the US, concentrating on the impact of market and non-market factors. In particular, we analyze the first migration decisions of male, illegal migrants (i.e., those without US entrance visas) who reside in communities that have been traditionally a source of migrants from Mexico to the US. Our results suggest that absolute and relative wages, migration network connections, individual demographic characteristics, and community characteristics have significant impacts on the migration decision.",5
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2008.9,Foreign Sourcing of Intermediate Inputs: Impacts on Unskilled Labor in US Manufacturing Industries,April 2009,John K Mullen,James Panning,,Male,Male,Unknown,Male,"The consequences of international outsourcing of production activities remain a contentious issue. The oft-disparate views of researchers, the media, and politicians hold important implications both for public policy and the level of support for the globalization process within industrialized nations. Yet there is surprisingly little empirical evidence on the impacts of international production fragmentation on labor market outcomes for the US. Moreover, the empirical efforts that focus exclusively on the foreign sourcing of intermediate inputs by US firms are based generally on data from the 1970s and 1980s. Clearly, additional evidence concerning likely impacts on the labor force is warranted, especially if the view of international outsourcing as a more recent phenomenon is correct. Such research also should clarify the relative roles played by technology investment vs outsourcing in the changing skill composition of the workforce. The present study augments our understanding of these issues as they pertain to employment outcomes within a sample of six-digit (NAICS) US manufacturing industries from 1997 to 2002. We begin by reviewing the pertinent literature in the next section. A discussion of data and measurement issues follows, along with an examination of the empirical strategy. The penultimate section presents and examines the statistical results; a final section summarizes and offers implications for future research and policy directions.",3
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2008.10,Reducing Inequities Among Worker-Owned Cooperatives: A Proposal,April 2009,Robin Hahnel,,,,Unknown,Unknown,Mix,,
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2008.11,Geography and Labor Market Performance,April 2009,Horst Feldmann,,,Male,Unknown,Unknown,Male,"While the role of geographical conditions in economic development had been neglected for a long time, it has been closely analyzed over the past decade. One of the first to put geography back on the agenda was Jared Diamond [1998]. In his history of the causes of development and underdevelopment around the world, he argues that Europe's economic rise over the past centuries was largely due to certain geographical advantages it held over other continents. Specifically, according to Diamond [1998], favorable geographical conditions implied that Europe had the largest number of wild plant species that lent themselves to domesticated high-yielding crops; allowed Europeans the widespread use of draft animals, increasing productivity; enabled Europeans to develop some resistance to germs like smallpox and measles (with devastating effects on ingenious populations in the Americas when Europeans arrived). In his more recent book, Diamond [2006] extends his analysis by arguing that geography has often been the main catalyst in the collapse of civilizations as overgrazing, overhunting, and overfarming led to vicious circles of deforestation, erosion, and starvation. Sachs and coauthors also hold that geographical conditions have a major impact on economic development [e.g., Gallup et al. 1999]. For example, they argue that tropical climates adversely affect human health and agricultural productivity. Another example is a long distance to the coast, which increases transport costs for international trade. In a series of papers, Sachs and coauthors have shown that both levels and growth rates of GDP per capita are lower in countries that are characterized by adverse geographical conditions [e.g., Bloom and Sachs 1998; Gallup et al. 1999; Sachs 2001; 2003]. By contrast, Acemoglu et al. [2001; 2002], Easterly and Levine [2003], and Rodrik et al. [2004] argue that geographical conditions do not have a direct impact on GDP per capita. According to their studies, institutions rather than geographical conditions are the main causes of economic development. In their regressions, the coefficients on geographical variables mostly are statistically insignificant once the effect of institutions is controlled for. These authors do not deny that geographical conditions may have played a part in economic development. Rather, they hold that geography played an indirect role at most — in particular, by influencing what types of institutions Europeans established in their colonies: in areas of adverse geographical conditions they set up extractive institutions that hampered subsequent economic development. In any case, geographical conditions, such as soil fertility, endowment of natural resources, climate, access to oceans and elevation, are major characteristics of any country. One may thus hypothesize that they are likely to affect not only economic development but also labor market performance. For example, adverse geographical conditions, like tropical climates or a long distance to the coast, may reduce overall economic activity, leading to comparatively low employment and high unemployment rates. Conversely, adverse geographical conditions may induce more people to work in order to compensate for these disadvantages, leading to higher employment and lower unemployment rates. Yet another possibility is that labor market performance is mainly determined by other factors, such as labor market regulations and business cycle fluctuations, so that geographical characteristics have no, or hardly any, effect at all. Whereas the impact of geographical conditions on economic development, in general, and GDP per capita, in particular, has been closely analyzed in recent years, their impact on labor market performance has not yet been analyzed at all. This paper intends to shed some light onto this hitherto unexplored field of research. Its contribution is mainly empirical. We use 9 different variables to measure major geographical characteristics (for definitions and sources of all variables, see Appendix A). Our study covers 76 countries from all over the world (for a list of countries, see Appendix B). The next section develops hypotheses on the labor market effects of each of the geographical characteristics and explains the data we use to measure them. The subsequent section describes the dependent and control variables as well as the estimation method. The fourth section presents our main results. The penultimate section reports results from various robustness checks and briefly discusses the results for the control variables. The final section concludes.",3
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2008.12,Subsistence Savings Strategies of Male- and Female-Headed Households: Evidence from Mexico,April 2009,Julia Paxton,,,Female,Unknown,Unknown,Female,"A growing literature on the financial habits of low-income male and female-headed households in developing countries contributes to our understanding of the risks, vulnerability, and tools used by distinct household types.Footnote 1 Most of this literature has focused on credit use and consumption patterns, leaving a substantial gap in the literature on savings behavior. In rural areas of developing countries where formal finance is nearly non-existent, informal savings is one of the main tools that households have to confront shocks and smooth family consumption, yet savings patterns have not been well documented, particularly with respect to gender. How do the volume and types of savings instruments vary in different types of households? What are the key determinants of savings in female-headed households and how do they differ from savings patterns in male-headed households? One of the obstacles to gaining insights into savings behavior in developing countries has been data quality. Much of the most important work on savings in recent years has concentrated on theory and intertemporal issues that do not require detailed savings data, partly due to the inadequacy of household survey data [Deaton 1999]. This study benefits from the use of a large and detailed survey that is capable of providing the richness of detail on savings behavior that is lacking in aggregate country-level statistics. This paper draws upon detailed cross-sectional data collected from 2,029Footnote 2 rural Mexican households to reveal that while male- and female-headed households have a similar volume of total assets, the determinants and the portfolio composition of savings are quite distinct. Male-headed households hold larger volumes of informal and formal financial assets and quasi-liquid assets related to farming and business. In contrast, female-headed households have a greater concentration of short-term liquid family consumption assets such as grain and small animals. Female-headed households invest more in their homes as a long-term savings strategy. Important determinants of liquid and/or total savings include income, credit access, education, and reliance on agriculture. Savings levels in female-headed households are more significantly impacted by the number of dependents and the availability of remittances. The second section provides an overview of key findings from gender-based studies in developing countries. A description of the data and an overview of household portfolio allocation are given in third and fourth sections. The fifth section presents the model specification for measuring the determinants of savings. Then sixth and seventh sections provide the empirical results of the empirical models for liquid savings and total savings, followed by the central conclusions and findings in the eighth section.",9
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2008.14,Crime and Remittance Transfers,April 2009,Carlos Vargas-Silva,,,Male,Unknown,Unknown,Male,"People move across regions of a country or from one country to another to improve their working conditions, find better opportunities, or to escape violence, among other reasons. In the past, migration often meant the migrant was cut off from, or had limited communication with, family and friends left behind. However, technological advances in telecommunications have greatly increased migrants’ ability to communicate with family members and friends in their home community. These advances in technology have also simplified domestic and international money transfers, thus allowing migrants to send money back home to support family and friends. By simply paying a small fee to a money transfer agency (e.g., Western Union), migrants can make funds available in their home community within minutes. A recent study by the International Fund for Agricultural Development and the Inter-American Development Bank estimates that in 2006 alone migrant workers sent home more than $300 billion US dollars [International Fund for Agricultural Development 2007]. Previous studies have used an extensive list of variables related to the migrant, the household, and the home country to explain the motivations of migrants to remit money home. However, there is still no consensus about migrants’ motivations to remit. In one of the first studies on the subject, Lucas and Stark [1985], using information on a drought that occurred in Botswana, tested the insurance motivation to remit. The insurance motivation suggests that by leaving the household and moving to another region or country, the migrant will be subjected to risks that are uncorrelated to those that the household faces; hence, the migrant and the household are able to diversify their risks [Amuedo-Dorantes and Pozo 2006b; Choi and Yang 2007]. In their study, Lucas and Stark posited that if a coinsurance agreement was in place between the migrant and the household, households with a higher risk of losing crops or cattle, given the seriousness of the drought, would receive more remittances (i.e., to gain access to supplies of water). Although their results support the insurance motivation for remitting, they noted that, given the lack of enforceability of the insurance arrangement, the migrants’ behavior may still imply that they acted in an altruistic manner.Footnote 1
 
Agarwal and Horowitz [2002], in a study of Guyana, used the impact of the number of migrants in each household on remittances as an indicator of altruism vs insurance. They argued that if the number of migrants from a single family increases and remittances sent by each migrant do not decrease, then individual migrants must be insuring themselves with the household. In other words, each migrant needs to “pay in” regardless of the total number of migrants. However, if the number of migrants increases and the number of remittances sent by each migrant decreases, then remittances are likely made for altruistic reasons; that is, as more migrants begin to remit, the household's demand for support from individual migrants decreases. Agarwal and Horowitz's results suggest that altruism is the main motivation for remittance transfers. 
Cox et al. [1998] used data from the Peruvian Living Standards Survey to test the altruistic motivation of transfers among households against an exchange motivation (i.e., households transfer money because they expect reciprocation). Unlike the altruistic motivation, which implies a negative relation between transfers and the recipient's pre-transfer income, under the exchange motivation, there may be a positive relation between the transfer amount and the recipient pre-transfer income. Using variables such as social security payments, Cox et al. found evidence favoring the exchange motivation to remit. With this article, we add to this growing body of literature by using the 2003 Quality of Life Survey of Colombia to examine the main motivations behind the decision to remit and the amount remitted, placing special attention on the impact of crime on transfers. We posit that in areas where violent crimes are commonplace, variables related to criminality and violence can be used to study the motivations to remit. Crime can have a profound impact on household income and assets, thereby encouraging altruistic transfers; however, crime can also discourage the flow of self-interested transfers by negatively affecting returns. By examining the response of transfers after a household member has been a victim of a crime, we provide insights on why migrants transfer money and, thereby, increase our understanding of the motives that drive remittances and migration. To the best of our knowledge, our paper is the first study to focus on crime as a means to study the motivation for transfers. Colombia is a country with a legacy of violence that continues today. Violence has forced approximately 4 percent of the population to leave their homes and become refugees [Arboleda and Correa 2002]. In 2004 alone, over 250,000 Colombians were forced to become refugees [UNICEF 2006]. Most of the refugees come from rural areas, but the violence, in conjunction with an economic downturn at the end of the 1990s, also induced a large exodus of highly educated citizens.Footnote 2 In total, between 2000 and 2005, about 1 million Colombians migrated to the United States, Spain, and Costa Rica [Inter-American Development Bank 2006]. A consequence of this surge in internal and overseas migration has been a significant increase in the flow of domestic and international remittance transfers into the country as Colombian migrants send money to family and friends left behind in their home communities. In 2002, remittances to Colombia amounted to 2,092 million US dollars (US$) but rose more than twofold to US$4,200 million by 2006.Footnote 3 A recent survey on licensed money transmitters conducted by the New York State Banking Department found that in one New York City neighborhood alone (Jackson Heights in Queens), Colombians sent back home over US$20 million between June 2004 and June 2005 [Nikolov 2006]. International remittances in Colombia are also substantial when compared with other Latin American countries. In 2006, Colombia ranked third among Latin American countries in total remittances received, just behind Brazil and Mexico. Moreover, remittances to Colombia accounted for 3.3 percent of the country's gross domestic product — a higher share than both Brazil (0.3 percent) and Mexico (2.9 percent) [International Fund for Agricultural Development 2007]. In addition, the declining in the cost of transferring money and the increasing stock of Colombians abroad promises a continuing flow of remittances to Colombia in the years ahead.Footnote 4
 In addition to studying the impact of crime on money transfers, this article also contributes to the overall discussion of remittances in Colombia, a topic that remains relatively unexplored in the literature. The Colombian experience represents a unique and interesting case for studying the relation between remittances and crime for several reasons: (a) Colombia has a large number of internal and overseas migrants, (b) it receives a substantial flow of remittances (ranking third in Latin America), (c) it is one of the most violent countries in Latin America, and (d) it has a large drug trafficking industry. Because one of the main priorities of the US government in regulating remittances is to disrupt illegitimate transfers and facilitate legitimate transfers, information on the determinants of remittance transfers can assist government authorities to ensure that remittance channels are not abused by criminals and yet remain open between hard-working migrants and their families in Colombia.Footnote 5
",15
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2008.18,Mathematical Miscalculations and Monopoly Pricing Strategies,April 2009,Bryan C McCannon,,,Male,Unknown,Unknown,Male,"Economic models are dominated by the implicit assumption that the price of a good takes the form of a cost for one unit of the good.Footnote 1 Such a price, which I refer to as a single-unit price, is linear in that multiple units may be purchased at the constant per unit price. Non-linear prices, on the other hand, allow a firm to vary the per unit price with the quantity purchased to discriminate between heterogeneous consumers. There exists a third form to the announcement of a price. Often, multiple units of a good are quoted a cost. For example, a good may have a quoted price “two for eight dollars” where one unit can be purchased for four dollars. I refer to such an announcement as a multi-unit price. Multi-unit prices are linear and thus one might be inclined to think that this is equivalent to a single-unit price, both allow any number of units to be purchased at a constant per unit price. The question I pose is why do multi-unit prices exist? I show in a model with a monopolist selling units of a good to a heterogeneous population of consumers that there exist environments where each of the three pricing strategies arise. The innovation introduced to generate this result is the following. When confronted with a multi-unit price a consumer, who does not want to buy the recommended quantity, must complete a mathematical division problem to determine the price she must pay for fewer units. This allows for the possibility of a miscalculation. The possibility of such an error has two potential consequences. She may buy a unit when it is in her best interest not to do so. Alternatively, she may not purchase a unit when she should have. These two consequences provide a reason for the monopolist to prefer each of the two linear pricing strategies to the other. Consider the following example. Suppose there are one hundred consumers of either type A or type B. There are 70 A consumers who value one unit at $4.10 and two units at $8.10 (so the second unit has a value of $4). The 30 B consumers value one unit at $2 and two at $3. The multi-unit price of “two for $8” encourages A consumers to buy two units. B consumers, though, do not prefer to make a purchase, but with a probability of 0.1 they make a miscalculation and buy a unit. With this pricing strategy a profit of $572 is generated. The monopolist cannot use a single-unit price to extract any more of A consumers’ surplus. A higher per unit price results in fewer sales.Footnote 2 The monopolist could lower its per unit price to $2 and use a single-unit pricing strategy to sell a unit to all B consumers, which results in a profit of $340. Finally, it could select a non-linear price that charges $8.10 for two units and a high price for one unit (any price above $4 discourages A consumers from buying a unit). This, though, generates only $567 in profit. Thus, in this example multi-unit pricing is best. Alternatively, if there were 30 A consumers and 70 B consumers the monopolist would prefer all the B consumers to purchase two units of the good. The non-linear price schedule of “two for $3 and one for $2.50” (any price for one greater than $2 discourages single-unit purchases) generates a profit of $300, which is more than the $268 generated from the multi-unit price. The example highlights the main result of the model. If one type of consumer has preferences such that the marginal utility of both the first and second unit of the good exceeds the marginal utility the other type of consumers place on the first (which I refer to as dominant preferences), then multi-unit pricing has an advantage. The monopolist selects a per unit price at the level those with the dominant preferences receive for the second unit. This amount is greater than the other type is willing to pay for one. Some of these consumers, though, mistakenly purchase a unit. The additional sales generated by a multi-unit price make such a pricing strategy optimal. It is required that there is a sufficient amount of each type of consumer. If the population consists primarily of one type of consumer it is best for the monopolist to select a pricing strategy that extracts as much surplus as possible from multiple purchases made by this type. Additionally, if the probability of a miscalculation is too small the additional profit generated from mistaken consumers does not compensate for the surplus that could be extracted from a non-linear price schedule. Finally, the dominant preferences allow for miscalculations to increase the monopolist's profit by selling the former two units and collecting mistaken purchases from the latter. Without a consumer with such preferences a non-linear price schedule effectively separates the consumers and extracts more surplus. The paper is organized as follows. The standard model is presented in the next section while the subsequent section introduces the extension incorporating mathematical miscalculations. Further sections study miscalculations when the monopolist selects linear prices and consider non-linear pricing. Empirical data of multi-unit prices in use is summarized in the penultimate section and the final section concludes. The proofs of the results are given in the Appendix.",
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2008.21,"The Local Economic Impact of Wal-Mart, by Michael J. Hicks",April 2009,Lester Hadsell,,,Male,Unknown,Unknown,Male,,
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2008.22,"Obesity, Business and Public Policy, by Zoltan J. Acs and Alan Lyles",April 2009,Inas Rashad,,,Female,Unknown,Unknown,Female,,
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2008.23,"Dennis Robertson: Essays on his Life and Work, by Gordon Fletcher",April 2009,John Smithin,,,Male,Unknown,Unknown,Male,,
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2008.24,"The Illusions of Entrepreneurship: The Costly Myths That Entrepreneurs, Investors, and Policy Makers Live By, by Scott A. Shane",April 2009,Sanjay Paul,,,Male,Unknown,Unknown,Male,,1
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2008.39,"The Decline of Latin American Economies: Growth, Institutions, and Crises, by Sebastian
Edwards, Gerardo Esquivel and Graciela Marquez",April 2009,Matthew Q McPherson,,,Male,Unknown,Unknown,Male,,
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2008.40,"What Makes a Terrorist: Economics and the Roots of Terrorism, by Alan B. Krueger",April 2009,Siddhartha Mitra,,,Unknown,Unknown,Unknown,Unknown,,
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2008.41,"Imperfect Knowledge Economics: Exchange Rates and Risk, by Roman Frydman and Michael D. Goldberg",April 2009,Joseph M Santos,,,Male,Unknown,Unknown,Male,,
35,2,Eastern Economic Journal,31 March 2009,https://link.springer.com/article/10.1057/eej.2009.1,List of Reviewers 2008,April 2009,,,,Unknown,Unknown,Unknown,Unknown,,
35,3,Eastern Economic Journal,26 June 2009,https://link.springer.com/article/10.1057/eej.2009.24,The Current Economic Crisis and Lessons for Economic Theory,June 2009,Joseph E Stiglitz,,,Male,Unknown,Unknown,Male,"Turning first to the crisis. We have at long last emerged from the paralysis, from the period of denial, from the notion that recovery was around the corner. For years, it has been clear that America's growth was not sustainable. It was based on a real estate bubble, which sustained a consumption boom. America was living beyond its means. Alan Greenspan may have been right that you could not be sure that there was a bubble until after it broke, but policy-makers are supposed to make decisions based on the analysis of risk. The likelihood that there was a bubble was increasingly clear; and the more housing prices grew, the greater the likelihood that the eventual crash would be disastrous. How could prices continue to grow, especially for housing for lower and middle-income individuals, as incomes stagnated? One doesn't have to have a Ph.D. to know that you can't spend more than 100 percent of your income on housing. Over inflated housing prices allowed Americans to take out hundreds of billions of dollars in mortgage equity withdrawals, in 1 year alone an estimated US$900 billion. It has also long been clear what was required: a stimulus package, a program to deal with the housing market, and a program to deal with the financial sector. For months, nothing was done on the first two, and what was done on the third was totally ineffective. It is such a relief that finally something is being done that I hesitate to raise a note of criticism. But regrettably, I do not believe we are doing enough, or the right thing, in any of these areas. Let me be clear: what we are doing is much, much better than doing nothing, or doing what we were doing. It will make a difference, but it is likely not enough. A year from now the economy will remain weak, the housing problems will still be with us, and our banks will still be ailing.",55
35,3,Eastern Economic Journal,26 June 2009,https://link.springer.com/article/10.1057/eej.2008.19,Wal-Mart and the US Economy,June 2009,Robert Jantzen,Donn Pescatrice,Andrew Braunstein,Male,Male,Male,Male,"The Wal-Mart corporation has come under media scrutiny for a myriad of reasons in recent years. It continues to be the focus of countless news stories, case studies, and organizational analyses — some lauding its business acumen, others decrying its negative social, environmental, and economic impacts. The company's expansion plans are increasingly subject to referendums as local governments question whether a Wal-Mart presence produces net economic and employment gains. In recent years, residents have turned back Wal-Mart penetration plans in the states of California, New York, Illinois, and Vermont, among others. Why has the Wal-Mart corporation been the subject of such intense media scrutiny while often being cast as the villain by many labor, environmental, and local civic groups? The answer is rather obvious — it is too big to ignore. Consider the following salient economic statistics reflecting Wal-Mart operations. Wal-Mart was the world's largest company from 1992 to 2005, only recently dethroned when a spike in oil prices catapulted Exxon Mobil to the top. Wal-Mart is by far the largest retailer in the world, and operates about 3,900 stores in the US and another 1,600 internationally. It is the number one retailer in the US with 2006 annual sales of $270 billion, generating 2.0 percent of the US gross domestic product [Wal-Mart website]. Wal-Mart's total sales are larger than the combined sales of the next five big retailers — Home Depot, Kroger, Target, Costco, and Sears/Kmart [Basker 2007]. Wal-Mart is a ubiquitous retailer with 92 percent of Americans reporting that they live close to a Wal-Mart, and 42 percent claiming to shop there every week [Pew Research Center 2005]. The company serves approximately 19 million customers a day and nine cents out of every US retail dollar is spent in one of its stores. The company sells 30 percent of all US staple goods (personal care, pet food, cleaning items, etc.), is the largest US grocer (21 percent of total grocery sales), the biggest toy seller (19 percent of all toy sales), and the third-largest pharmacy (16 percent). The major consumer products companies Kellogg, Kraft, and General Mills each derive about 15 percent of their total sales from Wal-Mart stores. Wal-Mart is the largest employer in the US — 1.3 million employees locally out of a total of about 1.8 million employees worldwide. It is the largest employer in 21 states, hiring 3.3 percent of all workers in its home state of Arkansas, and is also the largest employer in Mexico [Bianco and Zellner 2003; Useem 2003]. Because the Wal-Mart corporation is now a dominant US firm, its operations may signal trends in aggregate economic conditions in general. That dominant US businesses could have significant effects on the overall economy has long been established. However, that economic history has not examined the link between such businesses and specific macro economy measures, a void this study attempts to fill by examining Wal-Mart.",7
35,3,Eastern Economic Journal,26 June 2009,https://link.springer.com/article/10.1057/eej.2008.15,"The NAIRU, Demand and Technology",June 2009,Servaas Storm,C W M Naastepad,,Unknown,Unknown,Unknown,Unknown,,
35,3,Eastern Economic Journal,26 June 2009,https://link.springer.com/article/10.1057/eej.2008.16,On Cyclicality in the Current and Financial Accounts: Evidence from Nine Industrial Countries,June 2009,Jens R Clausen,Magda Kandil,,Male,Female,Unknown,Mix,,
35,3,Eastern Economic Journal,26 June 2009,https://link.springer.com/article/10.1057/eej.2008.17,Differential Impacts of Economic Volatility and Governance on Manufacturing and Non-Manufacturing Foreign Direct Investments: The Case of US Multinationals in Africa,June 2009,Adugna Lemi,Sisay Asefa,,Unknown,Unknown,Unknown,Unknown,,
35,3,Eastern Economic Journal,26 June 2009,https://link.springer.com/article/10.1057/eej.2008.28,Institutions and Human Progress: An Analysis of International Pooled Data,June 2009,Nathan J Ashby,,,Male,Unknown,Unknown,Male,"The amount of literature on the effect of economic institutions — in particular, economic freedom — on growth is significant. The evidence seems to strongly favor sound property rights and its positive effect on economic growth. However, there has been criticism of the use of growth in GDP per capita as a measurement of overall well-being or quality of life [Sen 1999; Bauer 2000]. Such criticism has been rejoined by various studies that have demonstrated that economic freedom positively influences other measurements of life satisfaction such as human poverty measures, literacy, overall education, life expectancy, and mortality rates [Esposto and Zaleski 1999; Norton 2003]. The Human Development Index (HDI) has been used by the United Nations and is a weighted index measuring income, education, and health of 162 countries [UNDP 1999; 2001]. This analysis contributes to these studies in two ways. First, an alternative measurement of human progress from the Fraser Institute will be used, the Index of Human Progress (IHP), which includes additional indicators of well-being including the attainment of technological goods [Emes and Hahn 2001; McMahon 2002]. Specifically, the IHP consists of measurements of GDP per capita, education, technology, and health. It also is a better measurement of how countries progress through time. Regressions will be run on the index as a whole and on its individual components. The Economic Freedom of the World Index (EFW) is made up of five subcomponents measuring the size of government, property rights policy, monetary policy, trade policy, and regulation [Gwartney and Lawson 2006]. The second contribution of this paper will be to test the impact of these components on the IHP and its components to analyze the specific effect that each of these components has on human progress. Sensitivity analysis will be used to determine which components have a more robust relationship. Decomposing the index has been attempted in other studies dealing with economic growth [Ayal and Karras 1998; Heckelman and Stroup 2000; Carlsson and Lundstrom 2002], but this is the first attempt to determine the effect that they have on the measurement of human progress suggested herein. The format for the remainder of this paper will be as follows. The following section discusses the importance of quality institutions on development and provides arguments dealing with property rights and other free-market policies. Next, the various data gathering sources are described in minor detail and the measurements that will be used in this study will be discussed. Section 4 explains and interprets the theoretical model and the regressions for this study. The statistical and economic significance of these results will be analyzed. Finally, the paper will terminate with concluding remarks and important implications of the study.",
35,3,Eastern Economic Journal,26 June 2009,https://link.springer.com/article/10.1057/eej.2008.42,Categorically Unequal,June 2009,LaTanya N Brown,,,Unknown,Unknown,Unknown,Unknown,,
35,3,Eastern Economic Journal,26 June 2009,https://link.springer.com/article/10.1057/eej.2008.51,From Higher Aims to Hired Hands: The Social Transformation of American Business Schools and the Unfulfilled Promise of Management as a Profession,June 2009,Richard Marens,,,Male,Unknown,Unknown,Male,,
35,3,Eastern Economic Journal,26 June 2009,https://link.springer.com/article/10.1057/eej.2008.56,Teaching Pluralism in Economics,June 2009,Robert F Garnett,,,Male,Unknown,Unknown,Male,,
35,4,Eastern Economic Journal,23 October 2009,https://link.springer.com/article/10.1057/eej.2008.46,How Do Students at Median Graduate Economic Programs Differ from Students at Top-ranked Programs?,October 2009,David Colander,Tiziana Dominguez,KimMarie McGoldrick,Male,Female,Unknown,Mix,,
35,4,Eastern Economic Journal,23 October 2009,https://link.springer.com/article/10.1057/eej.2008.25,"A Study of High School Economic Literacy in Orange County, California",October 2009,Chiara Gratton-Lavoie,Andrew Gill,,Female,Male,Unknown,Mix,,
35,4,Eastern Economic Journal,23 October 2009,https://link.springer.com/article/10.1057/eej.2008.44,"Public Spending, Market Imperfections, and Unemployment",October 2009,Donatella Gatti,,,Female,Unknown,Unknown,Female,"This paper aims to explore the links between public spending and product market structure, and their joint impact on equilibrium employment. The issue has been overlooked in the recent literature, with the exception of few contributions related to the process of unification in Europe [Andersen and Sørensen 1995; Gatti and van Wijnbergen 2002]. Several mechanisms have been highlighted in the literature to help understand how fiscal policy impacts the level of economic activity. New growth models, real business cycle models as well as New Keynesian models allow us to capture the consequences of fiscal spending in a dynamic context [see, e.g., Burnside et al. 2004]. Matsuyama [1995] proposes a set of static models with imperfect competition and limit pricing behavior where fiscal policy affects the equilibrium production level. However, the question of unemployment is left aside by Matsuyama [1995]. More generally, the role that market imperfections play in enabling fiscal spending to affect unemployment has been less thoroughly explored within static equilibrium models. Building on the seminal work by Layard et al. [1991], a rich literature has developed tackling the issue of structural unemployment within static “wage-setting/price-setting” (WS/PS) models [see, e.g., Nickell 1997; Siebert 1997; Blanchard and Giavazzi 2003]. Public spending can be integrated in such framework, after including nominal rigidities and out-of-equilibrium adjustments. In this paper, I study the effects of public spending within a static equilibrium framework. The paper presents a generalization of a standard WS/PS model capturing the impact of public spending on employment, in the presence of imperfect competition in labor and product markets. The model is built on previous works that have analyzed the effects of product market imperfections on job flows and aggregate employment, within a dynamic efficiency wage framework [Amable and Gatti 2002; 2004; 2006]. The structure of the paper is the following. The general framework of the model is presented in the next section. The section “Equilibrium employment” presents the results concerning equilibrium unemployment. The last section briefly concludes.",1
35,4,Eastern Economic Journal,23 October 2009,https://link.springer.com/article/10.1057/eej.2008.45,A Decomposition of Factors Influencing Horizontal and Vertical FDI: A Separate Analysis,October 2009,Kazuhiko Yokota,Akinori Tomohara,,Male,Male,Unknown,Male,"Questioning which factors characterize the structure of multinational enterprises (MNEs) has been one of trade economists’ concerns ever since Mundell [1957] related trade in goods with trade in factors and developed the trade model incorporating capital flows. Seminal works in the 1980s considered MNEs location choice by relating trade and foreign direct investment (FDI), providing a framework for explaining MNE’s different activities [Helpman 1984; Markusen 1984; see Markusen 2002 for the survey]. The MNEs structure is related to FDI motives. Horizontal MNEs are concerned with market access, while vertical MNEs focus on comparative advantage. A horizontal MNE has headquarters in its home country while assembling final products in both the home and a host country. A horizontal MNE can avoid trade costs (such as tariffs and transportation costs) by locating an affiliate in a host country. FDI to establish a horizontal MNE primarily serves the local market. Alternatively, a vertical MNE splits its production process into more than two locations. Keeping headquarters in the home country, a vertical MNE assembles final goods only in a host country. Vertical MNEs locate their affiliates in host countries’ offering cheap factor inputs. FDI to establish a vertical MNE primarily serves non-local markets. Trade theory provides different FDI implications depending on the MNE’s structure. The horizontal MNE model shows that similarity in size and relative factor endowments between a home and a host country are important factors in determining FDI. The vertical MNE model emphasizes the importance of the two countries’ relative factor endowments, since MNEs choose locations based on input costs. Different factor prices are the reason for establishing the vertical MNE.Footnote 1 Recently, the knowledge capital model was developed to encompass both factor price and market access elements [Markusen et al. 1996; Markusen 1997]. Recent empirical and/or simulation models try to distinguish the horizontal MNE model from the vertical MNE model by examining the theoretical implications [Carr et al. 2001; Markusen and Maskus 2002; Blonigen et al. 2003; see Feenstra 2004; Navaretti and Venables 2004 for a survey].Footnote 2 The consensus is that horizontal MNEs emerge when countries are of similar size and share similar relative factor endowments. Vertical MNEs emerge when countries differ in relative factor endowments. While the literature has focused on the different roles of horizontal and vertical MNEs, little attention has been paid to differences within the same MNE type. This paper examines whether MNEs within the same category may have different characteristics (or are motivated by different FDI reasons) depending on industry or FDI destination (e.g., either developed countries (DC) or less developed countries (LDC)). The analysis requires classifying companies as being either horizontal or vertical MNEs. We propose that the sales–imports ratio of an affiliate operating in a foreign host country be used an index to distinguish between horizontal and vertical MNEs. After stratifying the sample using the index, we examine whether there are any structural differences within the same MNE type. Our analysis uses data from US multinational affiliates operating in foreign countries. We construct the dataset using sources such as the Bureau of Economic Analysis (BEA), the International Labor Organization, the IMF's International Financial Statistics, Penn World Tables, and the World Bank. Our analysis uncovers interesting FDI patterns from the data and obtains a more nuanced typology of the US multinational affiliate activities. The analysis indicates that, in addition to the commonly used explanatory variables in the literature (i.e., world income, differences in relative market sizes, and skilled labor abundance), industry type and FDI destinations (and the interaction of the two) distinguish MNE structures even within the same FDI type. The results show that both horizontal and vertical FDI behave similarly regarding FDI destinations in food and chemical industries. In the electric machinery industry, vertical FDI is dominant in less developed host countries, but the importance of horizontal FDI increases in developed host countries. Perhaps this arises because horizontal MNEs produce high quality electric machinery for the markets in DC, while vertical MNEs seek cost advantages in LDC to produce cheap electric machinery for either the host markets, the home market or another export market. The remainder of the paper is organized as follows. The next section reviews the data used in the analysis. The subsequent section describes a framework for studying different characteristics within the same MNE type. Results of the analysis are presented in the further section. Finally, the last section concludes our paper.",6
35,4,Eastern Economic Journal,23 October 2009,https://link.springer.com/article/10.1057/eej.2009.26,Introduction to the Symposium on Mega Events,October 2009,Victor A Matheson,,,Male,Unknown,Unknown,Male,,1
35,4,Eastern Economic Journal,23 October 2009,https://link.springer.com/article/10.1057/eej.2009.31,Public Sector Support for Special Events,October 2009,Larry Dwyer,Peter Forsyth,,Male,Male,Unknown,Male,"There are sometimes good economic and non-economic reasons why a government may provide support for a special event. Special events increase the opportunities for new expenditure within a host region by attracting visitors to the region. They have the capacity to stimulate business activity, creating income and jobs in the short term, and generate increased visitation and related investment in the longer term. Sponsorship by governments of special events, even when they are run at a financial loss, is often justified by the claim that the events produce economic benefits for the region, and country, in which they are hosted. It is recognized that there may be other perceived benefits from events, such as enhancing the image of a city or region, facilitating business networking, and civic pride. Events can also result in associated social and cultural benefits to a destination, providing forums for continuing education and training, facilitating technology transfer, etc. On the other hand, events are recognized to generate adverse environmental impacts such as various forms of pollution and adverse social impacts such as disruption to local business and community backlash [Dwyer et al. 2000]. These aspects are very difficult to test or evaluate. Granted this, however, much of the public justification of events funding seems to center on their expected positive economic impacts, with the social and environmental effects treated separately. There are two standard approaches to assessing events, the first being economic impact analysis, which estimates the impact of the event on variables such as Gross State Product (GSP) and employment. The other, cost benefit analysis (CBA), provides estimates of the wider effects of the event, both positive and negative, and attempts to put dollar values on these in order to estimate the overall result. The usual approach to event evaluation has been for researchers and consultants to estimate the economic impacts of an event, and then, alongside these, consider some of the possible wider effects of events that are not captured in the economic modeling. Effects of the latter type can be estimated using a formal CBA. This has resulted in a less than satisfactory approach to event evaluation, as the economic impact analysis and the CBA can give conflicting results. The economic impacts of special events are usually specified in terms of the multiplier effects on output and employment that are generated by expenditure on the event and by visitors. The multipliers are typically based on I-O modeling. In recent years, a number of articles have been published that have been critical of various aspects of this approach. Thus, critics such as Porter [1999]; Matheson [2002]; Matheson and Baade [2003]; Blake [2005] and Crompton [2006] argue that the economic impacts of events are often exaggerated. These critics have highlighted some inappropriate practices in event assessment that inflate the estimated economic impacts. These practices include exaggerating visitor numbers and expenditure, failure to deduct residents’ expenditure prior to modeling, abuse of multipliers, and inclusion of time switchers and casuals, as well as the tendency to ignore the various costs associated with special events (e.g. opportunity costs, costs borne by the local community, and displacement costs). These criticisms indicate that considerable caution must be employed before the results of any economic impact assessment of an event may be accepted. The issues that these critics raise certainly need to be better understood by the research community, but importantly they need to be better understood by industry stakeholders, as it is their hired consultants who often flaunt “best practice” in pursuit of their objective of providing the client with large numbers and optimistic economic impact assessments of events. In addition to the above types of “in-house” criticisms of event assessment, a number of articles and reports have appeared in recent years that have argued that the entire approach to event assessment needs a re-examination. One element of this approach emphasizes that the economic assessment models used for estimating the economic impacts of major events should reflect contemporary developments in economic analysis, particularly regarding the use of computable general equilibrium (CGE) modeling. More specifically, this has involved an attack on the uncritical use of I-O modeling, hitherto the standard technique of economic impact assessment of major events [Dwyer et al. 2005]. A second element in the new approach argues that event assessment that focuses only on economic impacts is too narrow in scope to provide sufficient information to policy makers and government funding agencies, and that, where practical, a cost benefit approach should be employed to embrace the emerging importance of social and environmental impacts in addition to economic impacts [Jago and Dwyer 2006]. CBA is the “ideal” approach to event assessment, as it is the expected net benefits from any use of government funds that should guide resource allocation. Unfortunately, CBA has enormous data requirements that make it rather impractical to apply for all events. Because of this, a review of the worldwide literature on event assessment reveals that, despite acknowledgement of the wider social and environmental effects typically associated with events, only a handful of studies have employed a detailed CBA as an evaluation tool. This article has three main aims. The first is to highlight the limitations of the standard I-O technique in event economic impact assessment. As I-O analysis has inherent biases that overstate the impacts on output and jobs, it fails to provide information on industries adversely affected by the increased tourism demand. As a result, there is likely to be a misallocation of events funding and excessive overall spending in promoting events. In particular, I-O models will be contrasted with CGE models that provide potentially much more accurate assessments of an event's economic impacts. The second aim is to argue that a focus solely on the economic impacts of events will provide insufficient guidance to policy makers as to whether or not the event warrants support by way of public funding. In particular, it will be argued that it is the expected net benefits of an event that is important in allocating scarce resources to its support. Thus, a more detailed assessment of a major event requires CBA. CBA is the most comprehensive of the economic appraisal techniques, and comprises a systematic process for identifying and assessing all (both direct and indirect) costs and benefits of an event, including, in principle though not always in practice, social and environmental costs and benefits. The third aim of the article is to address an issue about which there is much confusion on the part of tourism researchers and industry stakeholders. This relates to how the outcomes of economic impact assessment and CBA can be reconciled. Researchers tend to treat economic impact analysis and CBA as distinct techniques of assessment, with the potential to provide conflicting recommendations. Accordingly, this article will explore the relationship between the preferred economic impact assessment tool of CGE modeling, and CBA, highlighting the type of data that is needed for informed policy making. To set a context for the discussion, reference will be made to the findings from research recently undertaken to estimate both the economic impacts and the net benefits of the Formula One Grand Prix held in Melbourne, the capital city of the state of Victoria, Australia.",32
35,4,Eastern Economic Journal,23 October 2009,https://link.springer.com/article/10.1057/eej.2009.30,"Economic impacts of the FIFA Soccer World Cups in France 1998, Germany 2006, and outlook for South Africa 2010",October 2009,Swantje Allmers,Wolfgang Maennig,,Female,Male,Unknown,Mix,,
35,4,Eastern Economic Journal,23 October 2009,https://link.springer.com/article/10.1057/eej.2009.25,Rejecting “Conventional” Wisdom: Estimating the Economic Impact of National Political Conventions,October 2009,Robert A Baade,Robert Baumann,Victor A Matheson,Male,Male,Male,Male,"Convention tourism is big business in the United States. According to the Convention Industry Council, in 2004 the meetings, conventions, exhibitions, and incentive travel industry generated over $122.3 billion in direct spending and 1.7 million jobs. These figures are “more than the pharmaceutical and medicine manufacturing industry and only slightly less than the nursing and residential care facilities industry” [CIC, 2005]. In hopes of gaining a piece of this lucrative business, cities compete vigorously to host meetings and conventions, and billions of dollars of taxpayer money has been directed toward the construction of ever larger and more elaborate convention centers in cities all across the country. Perhaps the most sought-after jewels of the convention industry nationwide are the quadrennial National Democratic and Republican Conventions at which each party's presidential candidate is nominated. City and party officials suggest that these events generate significant economic windfalls for host cities and also serve to focus national and even international attention on the host city. For example, city officials of New York City and Boston claimed net economic impacts of $255 million and $156 million, respectively, for the 2004 Republican and Democratic National Conventions. These economic impact numbers figured prominently in press releases promoting the 2008 Republican Convention in St. Paul/Minneapolis. The rosy economic impact numbers touted by convention promoters (both for political conventions as well as other prominent events) are also used to justify hefty public subsidies for the construction and operation of municipal convention centers. Over the past decade, tens of billions of dollars, including significant public funds, have been spent on new or refurbished convention centers in cities, for example, the Boston Convention Center ($800 million), D.C.'s Washington Convention Center ($850 million), and Omaha's Qwest Center ($291 million) [Malanga 2004]. The question of whether this has been money well spent is one of major public interest. Economists tend to be skeptical of the large economic impact numbers touted by convention facilities and event organizers. Our examination of 18 national political conventions from 1972 to 2004 fails to support the promoters' optimistic economic projections and finds that these events have a statistically insignificant impact on local economies.",5
35,4,Eastern Economic Journal,23 October 2009,https://link.springer.com/article/10.1057/eej.2009.29,The Impact of College Football Games on Local Sales Tax Revenue: Evidence from Four Cities in Texas,October 2009,Dennis Coates,Craig A Depken II,,Male,Male,Unknown,Male,"A football game day in a college town can be frenetic, especially if the school plays football in Division I, the top echelon of college sports. Thousands of visitors from around the state and region flock to the host city, spending money in bars and restaurants, hotels and motels, and even inside the stadium. All of this commercial activity carries with it sales tax revenues to the state and to the community. Of course, the large crowds lead to more traffic in town, greater congestion in the streets and eating and drinking establishments, and an increased need for police and emergency services relative to non-game days. One important policy question is whether the event generates net additional economic activity, and associated sales tax revenue, which would help offset any additional costs borne by the host city thereby increasing the net benefit of hosting the event.Footnote 1 In this paper, we analyze how monthly sales tax revenue in four small- to medium-sized cities in Texas (Austin, College Station, Lubbock, and Waco) is influenced by college football games being held in their jurisdictions. Each of the four cities investigated is home to a university that plays football in the Football Bowl Subdivision (FBS) of NCAA Division I (formerly known as Division I-A): the institutions are the University of Texas (UT) at Austin (Austin), Texas Tech University (Lubbock), Baylor University (Waco), and Texas A&M University (College Station). Using these data, we assess the impact of FBS football games on the sales tax revenues of the host jurisdiction and assess whether conference games, games against teams from within the state, and games against specific rivals have different impacts on local sales tax revenues than other games. The importance of games against rivals has a policy dimension that does not feature prominently in the evaluation of professional franchises and games. State politicians have expressed interest that public colleges and universities in their state schedule football games against one another. During the 1990s, legislators in Texas took positions on the rumored departure of UT at Austin and Texas A&M University from the now-defunct Southwest Conference. After the 1987 “Death Penalty” imposed on Southern Methodist University by the NCAA and the 1991 departure of the University of Arkansas to the Southeastern Conference, there was an increased pressure on the eight Texas teams that comprised the Southwest conference.Footnote 2 An Associated Press article dated August 17, 1990, reported, “Texas House Speaker Gib Lewis says he strongly opposes the University of Texas or Texas A&M University leaving the Southwest Conference. He vowed ‘to do everything in my power to prevent it from happening’ — even slashing appropriations to the two universities. For fiscal 1991, the Legislature appropriated $233 million to UT and $183 million to Texas A&M — none of it for athletics.” The article also indicated “[a] common concern is that the smaller schools left in the conference would lose too much income from television revenues and ticket sales without UT and Texas A&M on their schedules.” The issue did not go away, and in February of 1994, speculation that UT, Texas A&M, Baylor, and Texas Tech were considering offers to enter the Big Eight Conference “prompted several Texas legislators to hold hearings on the SWC's future and to pledge legislative retaliation if any teams flee the conference.” While no legislative retaliation occurred when these four teams did eventually leave the Southwest Conference, these news stories emphasize how the scheduling of college football games and the organization of college conferences can be an important policy issue for some state legislators.Footnote 3 While there has been little work done specifically on estimating the effects of college sports on local economies, there is a larger literature measuring the effects of holding sporting events of various types.Footnote 4 The general consensus in the academic literature that analyzes these issues ex post is that there is not a large return in terms of permanent jobs or income (see, for example, Porter 1999; Baade and Matheson 2001, 2004a, 2004b; Coates and Humphreys 2002; Matheson and Baade 2005; Coates 2006). However, the literature focusing on the amount of tax revenue generated while the event is taking place, which reflects the immediate net impact of the event on local spending, is less developed. Baade et al. [2008] examine how sports-related strikes and lockouts impact a host city's share of state taxable sales in Florida. They test for any negative impact of events not being held. They find no statistically significant evidence that work stoppages, opening of a new stadium, or the arrival of a new team influence taxable sales. Coates [2006] estimates the impacts on local sales tax revenues in Houston, Texas, from hosting the 2004 NFL Super Bowl and the 2004 Major League Baseball (MLB) All-Star game. He finds that hosting the Super Bowl may have generated an increase in sales tax revenues collected in Houston but the MLB All-Star Game likely did not. Coates and Depken [2007] evaluate the impact of a wide array of professional and amateur, regular season, playoff, and championship sporting events and a national political convention on sales taxes in 26 Texas municipalities. While they include college football games in their analysis, they do not control for which teams were playing in any game. This paper extends their analysis by using more refined game-level data to assess the impact of games against “rivals,” against teams within the conference, and against teams from within Texas.Footnote 5",18
35,4,Eastern Economic Journal,23 October 2009,https://link.springer.com/article/10.1057/eej.2008.57,"Banking on Global Markets, Deutsche Bank and the US — 1870 to the Present",October 2009,Mark S LeClair,,,Male,Unknown,Unknown,Male,,
35,4,Eastern Economic Journal,23 October 2009,https://link.springer.com/article/10.1057/eej.2008.58,"Future Directions for Heterodox Economics, edited by John T. Harvey and Robert F. Garnett",October 2009,Pierre Lacour,,,Male,Unknown,Unknown,Male,,
35,4,Eastern Economic Journal,23 October 2009,https://link.springer.com/article/10.1057/eej.2008.59,"Queer Economics: A Reader, edited by Joyce Jacobsen and Adam Zeller",October 2009,Lisa Giddings,,,Female,Unknown,Unknown,Female,,
35,4,Eastern Economic Journal,23 October 2009,https://link.springer.com/article/10.1057/eej.2008.60,"Frontiers in Ecological Economic Theory and Application, edited by Jon D. Erickson and John M. Gowdy",October 2009,Michael J Radzicki,,,Male,Unknown,Unknown,Male,,1
36,1,Eastern Economic Journal,22 December 2009,https://link.springer.com/article/10.1057/eej.2009.46,Blogometrics,January 2010,Franklin G Mixon Jr.,Kamal P Upadhyaya,,Male,Male,Unknown,Male,"As the epigraph above suggests, there is a relatively long history of self-examination by academic economists that attempts, at its base, to rank the scholarly achievements of individuals, groups (departments), and economics journals. One of the scholars in this area of inquiry, Tom Coupé, uses the apt phrase “the economics of economics” to describe these efforts [Coupé 2004]. Over the past few decades, researchers have attempted to rank economists and economics departments using a variety of metrics related to economists' traditional forms of academic research [e.g., Gibbons and Fish 1991; Scott and Mitias 1996; Mixon and Upadhyaya 2001; Coupé 2003; Coupé and Walsh 2003; Roessler 2004; Çokgezen 2006; Lo et al. 2008]. The arguments surrounding these metrics often involve whether to count publications and/or page numbers or, instead, to consider only citations to past work. In each case, whether or not all of a researcher's output, or just the subset of it that is published in so-called elite academic journals, is to be considered when computing the metric is also a point of contention. The recent study by Lo et al. [2008] gets back to the basics by offering an alternative to using economists' traditional forms of academic research (i.e., basic and applied) to rank departments. In their study, both the number of and citations to economists' pedagogical publications are used to rank economics departments worldwide. They find that, while a number of institutions that often place highly based on the impact of the traditional forms of research published by their respective scholars also rank highly using pedagogical publications only (e.g., Vanderbilt University, Duke University), a number of institutions that do not customarily appear in ranking studies based only on traditional forms of research do rank highly when using a different base, such as pedagogical research (e.g., Denison University, University of Missouri - Rolla). The Lo et al. [2008] study provides an impetus for using other alternative department ranking mechanisms in economics and other fields. One possible alternative emerged only in recent history, with the advent of the Internet. That is, the recent proliferation of economics blogs and economics bloggers offers a new avenue for evaluating the members of an academic institution's economics faculty.Footnote 1 The present study opens this new avenue by gathering information on a wide array of economics bloggers and blogs. In the next section, we present a ranking of economics bloggers that is based on citations to their academic research. In the subsequent sections we employ the ranking of economics bloggers to devise a ranking of the economics blogs themselves, and later to rank academic (economics) departments that are affiliated with the economics blogs.",3
36,1,Eastern Economic Journal,22 December 2009,https://link.springer.com/article/10.1057/eej.2008.47,Time to Pick a Fight? Interest Group Decision Making to Enter the Hydropower Regulatory Process,January 2010,Lea Kosnik,,,Female,Unknown,Unknown,Female,"This study investigates the involvement decision of interest groups in a federal US regulatory process. Interest group activity is common in many regulatory processes in the United States, including at the Environmental Protection Agency (EPA) [Ando 1999], the Food and Drug Administration (FDA) [Carpenter 2002], and the Federal Energy Regulatory Commission (FERC) [Kosnik 2008]. Previous research has shown that interest groups do have significant effects on ultimate regulatory outcomes [Cropper et al. 1992; Sigman 2001; Carpenter 2002; Kosnik 2006],Footnote 1 and legislation in the United States (and many other countries) also now guarantees them a seat at the table.Footnote 2 The question this paper specifically asks is, how do these influential interest groups choose which regulatory battles over others to be involved in, in the first place? Given the limited resources but broad objectives of most interest groups, what factors tend to influence their choice of policy battles in particular contexts? This is a rather difficult question to answer, primarily because the state space of total interest group activity is hard to define. In order to effectively model and comment on which battles interest groups have chosen to fight, a researcher needs to know all the possible battles that existed for them to have chosen from in the first place. That such a range of information is difficult, if not impossible, to acquire is a large reason why previous research papers investigating the black box of interest group battle choice have been rare. We overcome this difficulty by drawing on a unique data set regarding the hydroelectric dam relicensing process in the United States. Every year in the United States many private hydroelectric dams come up for relicensing before FERC. When they do, interest groups from across the country typically submit comments, protests, and motions of intervention in an effort to affect the final outcome of the dam relicensing process. It is possible, given a range of relicensing applications and a set of potential interest group actors, to empirically estimate the decision-making behavior of each interest group as they decide whether or not to be involved in any particular dam relicensing procedure. Not all interest groups intervene in every relicensing application, but many interest groups intervene in quite a number of them. What factors affect which applications they choose to get involved in, and why? How does this involvement ultimately affect the hydroelectric industry? It is true that not all interest groups involved in hydroelectric relicensing are solely focused on hydroelectric relicensing regulation. Many may in fact be making battle decisions from an even wider choice set that includes other environmental issues and other regulatory procedures. This paper does not deny that a particular interest group's objectives may be broad, but it does rest on the assumption that useful analysis can still be made by focusing on certain aspects of choice behavior parsimoniously. Interest groups, for example, often have dedicated staff assigned to particular policy contexts — such as hydropower relicensing — where most of the context-related decisions are made. It is within this purview that we choose to investigate battle choice decisions, and we believe that this is a useful approach. Human beings, for example, buy many things when they go to the grocery store, but economists have a history of productively analyzing the decision-making choice of how much beef to buy, without simultaneously modeling the entire range of products available throughout the grocery store. Our question is modest, therefore, in asking how interest groups choose particular policy battles over others, given a specific context. While limiting the generalizability of the results, this still presents a first step toward empirically analyzing a question that has largely been ignored in the literature. In our chosen context, our model includes variables on both the site characteristics of a particular battle (i.e. the size of the dam, the age of the dam, and the number of endangered species in the basin), and on the strategic characteristics of a particular battle (i.e. the strength of the enemy and the number of other allied interest groups involved). An intriguing result of the analysis is that the dam site characteristics are only sporadically significant in affecting battle choice. There is evidence that what is more consistently significant in affecting battle choice across the interest groups under study is strategic considerations — who else is getting involved in a particular fight and, even more importantly, who the dam owner is that an interest group will be battling. This implies that interest groups, when surveying an array of battle choices, may consider strategy before they consider the site specifics of any particular battle. This is a striking result. Thirty years ago regulatory capture theory [Stigler 1971; Peltzman 1976] changed the way we thought about government bureaucrats and government agency decision making, from an automatic assumption of social benevolence to one of individualistic, local utility-maximizing considerations. The theory of interest group decision-making could benefit from a similar reappraisal. A common assumption seems to be that interest groups choose battles according to regulatory “importance”; importance being vaguely defined by some social welfare criteria. But is this the case? Or do groups like American Rivers take donations every year and then choose to fight the most politically convenient battles, rather than the most environmentally demanding ones? In order to win more battles over time, do they have to? This paper adds to the regulation literature on interest group effects. It should also be useful for a number of reasons to policy makers, interest groups, and general constituents. The hydroelectric dam relicensing process, like most regulatory processes, is not a natural, stable, or necessarily efficient process. It was created, which means that it can also be changed and improved. Effective reform, however, can only occur once we understand the basic forces behind how regulatory processes operate. If, for example, a goal of regulatory bureaucracies is to increase involvement of interest groups and the public in decision-making procedures that affect the competitive structure of industries (perhaps in order to add legitimacy to the process), this paper aids in understanding when interest groups choose to get involved and what are their motivations. The insights gleaned from this study will help in crafting any new rules or procedures geared toward increasing outside participation in governmental decrees that affect the hydroelectric power industry. And when participation rules do change (as, for example, with the implementation of the new Integrated Relicensing Process in the hydroelectric dam relicensing process), this work will aid in predicting the likely effects of these changes on the regulatory process.",4
36,1,Eastern Economic Journal,22 December 2009,https://link.springer.com/article/10.1057/eej.2008.50,Gender Differences in Predispositions towards Economics,January 2010,Cynthia Bansak,Martha Starr,,Female,Female,Unknown,Female,"Women are underrepresented in economics, as in certain other scientific and technical fields. As shown in Figure 1, 31 percent of all bachelor's degrees in economics were awarded to women in 2005/2006. Finance has a share in the same range as economics; only engineering, computer science, and physics have lower shares. In contrast, many other fields that were previously predominately male — including math, biology, chemistry, and business — now award near half or more of their degrees to women.Footnote 1 Women's share of total bachelors degrees awarded, by major field of study, 2005/2006. Source: National Center for Education Statistics [2008, p. 395]. A number of studies have investigated reasons for women's underrepresentation in economics programs. Factors that may exert an influence include: pedagogical methods that favor male students, such as lectures and multiple choice tests [Jensen and Owen 2001]; a paucity of female role models [Robb and Robb 1999]; differences in math aptitudes [Ballard and Johnson 2004; 2005]; and presentation of subject matter in a way that overrepresents men and male concerns [Ferber 1995]. While studies indicate that indeed such factors help to account for women's lower propensity to study economics, they do not explain it away: even when women are well-prepared mathematically and confident academically, they are still less interested in exploring economics as a possible field of study [Dynan and Rouse 1997; Jensen and Owen 2000].Footnote 2 An important issue identified in previous research concerns gender differences in predispositions towards economics: upon entering the college class, women are less likely to be interested in economics, less likely to be considering it as a major, and more likely to expect it to be difficult [Dynan and Rouse 1997; Jensen and Owen 2001]. This paper investigates the factors that underlie gender differences in predispositions towards studying economics and presents empirical evidence from a survey of students at the outset of their first college economics class. The next section of the paper outlines conceptually the factors that may contribute to male/female differences in attitudes towards economics. We draw on the theoretical model of Breen and García-Peñalosa [2002], which identifies a number of reasons why women may not enter a traditionally male-dominated field, even after men's and women's preferences towards work/family balance have become similar; these include lower intrinsic interest in studying the subject, lower intrinsic interest in the careers to which it would lead, concerns about being poorly rewarded in those careers due to productivity or discrimination, continued preference for jobs that permit relatively good work/family balance, and/or pay differentials that are not large enough to offset the expected “cons” associated with the field. Their work also highlights how learning problems may contribute to slow changes in gender balance in fields that have been gender differentiated in the past. The paper then goes on to present findings from a survey conducted in 2005/2006 at San Diego State University (SDSU), where one of us taught at the time. The questionnaire was administered to 762 students in the first introductory economics course and asked questions about their interests, abilities, and job plans; their reasons for taking economics; their expectations of the class; the types of jobs to which they expect an economics degree to lead; and what they expect economics jobs to be like along a number of dimensions. An important finding of our work is that, despite the fact that academic economists generally think of the discipline as being centrally concerned with social welfare, both male and female students tend to see it as a financially oriented business field — which is not out of line with employment patterns of people with bachelors degrees in the subject. Women's more negative predispositions towards economics have much to do with this view of the discipline, especially widespread expectations that jobs in economics prioritize math skills and are primarily oriented to making money — a combination that seems to be a turnoff for fields like finance and engineering as well.",24
36,1,Eastern Economic Journal,22 December 2009,https://link.springer.com/article/10.1057/eej.2008.52,Price Impacts of Small-Firm Entry in US Manufacturing,January 2010,Robert M Feinberg,,,Male,Unknown,Unknown,Male,"Although there is a vast economic literature on determinants and effects of entry [for a few, Dunne et al. 1988; Bresnahan and Reiss 1991; Geroski 1995], little of this work has examined differential impacts by type of entry. Geroski [1995] did suggest that the limited survival of small entrants likely made incumbent responses to this type of entry more limited. Acs and Audretsch [1989] did a careful analysis of determinants of small-scale entry, but did not consider the competitive impacts of this (or other) types of entry. More recent work has also been more focused on determinants of small-firm survival than on market responses to small entrepreneurial entry. The latter is more important in judging the societal benefits of entry, and is the focus of this project. Although entry effects on other dimensions of firm performance — profits, output, productivity — are of interest to economists, the most basic predictions of microeconomic theory deal with how entry influences market prices. This theory is straightforward in static models, less so in dynamic and strategic models of incumbent behavior. In any static model — whether perfect competition, dominant firm price leadership (just monopoly with a competitive fringe), or standard Cournot — any increase in supply will drive down price; furthermore, the measure of entry which should matter is clearly net entry (entry minus exit), as it is the level of supply and its change, which determines (along with demand) the price. However, in strategic/dynamic models, the effect of entry is less clear, and the appropriate measure is also somewhat ambiguous. For example, in the static entry-limit pricing literature it is the threat of entry that determines incumbent pricing (and there is little discussion of what happens if entry actually occurs). In dynamic versions of this model, an equilibrium rate of entry is consistent with a price path by the incumbent. Gross entry in these models may proxy entry threat (and barriers) better than net entry. Davis et al. [2004] present a model in which actual entry may have no or perverse effects of incumbent pricing whereas potential entry (threat) will constrain that price. In Feinberg and Shaanan [1997], entry at the four-digit SIC level for 44 industries was disaggregated into three domestic types and two types of foreign (import) entry, for the period 1972–1982, and price effects of these different types were the focus of econometric analysis. Although pro-competitive impacts on domestic producer prices are found, these are limited to new entrepreneurial entry and (what might be viewed as the foreign analogy) gains in non-OECD imports. One limitation of that work was the small sample, representing roughly 10 percent of the manufacturing industries — this was necessitated by the use of four-digit industries as the unit of observation. Another limitation was the identification of entry by a firm not previously engaged in manufacturing as entry by “new entrepreneurs,” when in fact these ventures could have been quite large and controlled by either major retail/service sector players or well funded by consortia of investors. This study updates and expands on the Feinberg–Shaanan work, using annual data for the decade of the 1990s, and virtually all three-digit SIC industries. Instead of distinguishing between types of entry, entry and expansion by firms in different employment size categories are examined and econometric analysis seeks to find differential impacts on producer prices. A second innovation of this study is to examine the competitive impacts of both net and gross entry. Finally, this study is the first to link an exploration of entry with a body of work on exchange rate impacts on domestic prices [e.g., Feinberg 1989a], with the expectation that a domestic entry effect will be more likely to be accurately observed if other determinants (both foreign and domestic) are better controlled for. After discussing some of the relevant prior literature, I discuss the theoretical motivation and econometric specification to be employed. The data and variables used are then described; the focus is on entry and expansion by firms in different employment size categories and econometric analysis seeks to find differential impacts on producer prices. The results presented support a pro-competitive role of small-firm entry, as industries experiencing greater rates of small-firm entry show smaller price increases, after controlling for cost, demand, and exchange rate pressures.",
36,1,Eastern Economic Journal,22 December 2009,https://link.springer.com/article/10.1057/eej.2008.55,Are Foreign and Public Capital Productive in the Mexican Case? A Panel Unit Root and Panel Cointegration Analysis,January 2010,Miguel D Ramirez,,,Male,Unknown,Unknown,Male,"The onset and aftermath of the debt crisis of the early 1980s forced Mexico's cash-strapped government to abandon its long-standing import substitution industrialization (ISI) strategy (1946–1981) of economic growth and development. The dismantling of state-led ISI brought forth the privatization of the country's massive state-owned sector, the deregulation of its labor and financial sectors, and following the country's accession to the General Agreement on Tariffs and Trade in 1986, its economy was transformed from a heavily protected and highly regulated one to, arguably, one of the most open and unregulated economies of the region. This market-led, outward-oriented process, although begun by the De la Madrid administration (1982–1988), was effectively locked in, both economically and institutionally, by the Salinas de Gortari administration (1988–1994) with the passage and phased implementation of the NAFTA beginning in January of 1994. The country's transition from a closed to an open economy has been anything but easy. It has often been marred by a series of economic crises and financial setbacks, most dramatically in 1994–1995 with the onset and aftermath of the so-called “peso crisis,” and most recently, with the severe economic slowdown the economy has experienced following the relatively mild US recession in 2001. These economic crises and financial setbacks have led the Mexican government to the involuntary adoption and implementation of several IMF-sponsored adjustment programs which have resulted in sharp cuts in real government spending across the board, the severe contraction and increase in the cost of credit, and last but not least, repeated devaluations of the domestic currency in real terms [see Ibarra 1996; Lustig 2001; Gonzalez 2002]. Not surprisingly, the economic and investment performance of the Mexican economy under these stabilization and adjustment programs has been uneven at best and below the high expectations of its neoliberal advocates, particularly when compared to its performance under state-led ISI.Footnote 1 One possible factor in explaining Mexico's poor growth and investment performance is the sharp fall in public investment on economic and social infrastructure (roads, bridges, ports, sewerage systems, and education and health services) demanded by the stringent fiscal deficit targets of the various stabilization programs. For example, overall public (infrastructure) investment spending as a proportion of GDP (RG) fell precipitously from 11.2 percent in 1981 to barely 2.2 percent in 2002; the dramatic fall in government investment is further revealed by the fact that average public infrastructure investment spending as a proportion of GDP for the 1990s stood at just 3.6 percent, which is less than half as much as the level recorded during both the 1980s and 1970s.Footnote 2 Turning to the sectorial composition of public (infrastructure) investment, Figure 1 shows that the share of public (infrastructure) investment channeled to industry (RGI) fell sharply from 8.8 percent of GDP in 1981 to only 1.3 percent of GDP in 2001. Similarly, state investment directed toward the service sector (RGS) dropped from over 2 percent of GDP in 1981 to levels slightly below those recorded in the industrial sector (approximately 1 percent of GDP in 2001). Finally, the share public investment channeled to the primary sector (RGP) rose to 0.53 percent of GDP in 1980, but following the abandonment of the state-led ISI strategy, it declined to barely 0.1 percent of GDP in 2001. Public investment as a share of GDP by sector. On a more positive note, Mexico did experience a sharp increase in inward foreign direct investment (FDI) flows that can, in part, be traced to the country's implementation of macroeconomic stabilization measures and market-oriented structural reform programs, as well as the general surge in FDI inflows to developing countries associated with the rapid globalization of production during the decade of the 1980s and 1990s [Chakraborty and Basu 2006]. FDI inflows to the country jumped from just $2.5 billion in 1989 to $17.4 billion in 2002 — almost a sevenfold increase [ECLAC 2004]. In relative terms, FDI inflows as a percentage of GDP rose from about 1 percent in 1989 to 3.6 percent in 2001 [see ECLAC 2004]. On a sectorial basis, 60 percent of FDI inflows were channeled to the industrial (maquiladora or assembly-line) sector throughout the 1990s, with the service (banking, finance, and commerce) sector receiving 37 percent, and the primary (agriculture and mining) sector absorbing 3 percent [see UNCTAD 2005, Figure II.16]. Foreign investors have been attracted to Mexico as a result of privatization and debt conversion programs, the liberalization of the tradeable sector associated with the passage of NAFTA, and the wholesale removal of restrictive FDI legislation concerning the repatriation of profits, prior authorization of investments, and sectorial restrictions such as local content and export requirements. [Bloomstrom and Wolff 1994; Agosin 1995; De Mello 1997; Carrada-Bravo 1998; Ramirez 2000; Apergis et al. 2006]. Critics of FDI, however, contend that these inflows, rather than contributing to the financing of Mexico's private capital formation (and economic growth) are, in fact, a net drain on the country's resources because they generate substantial reverse flows in the form of remittances of profits and dividends to the parent companies [see Correa and Vidal 2006]. For example, ECLAC [2004] reports that profit and dividend remittances more than doubled between 1990 and 2001 (from $2.3 to $5.1 billion) and that, relative to the inflows of FDI, Mexico's remittances of profits and dividends averaged 46.1 percent (see Table A-5, p. 40). In this connection, Ramirez [2006] reports figures for Mexican FDI inflows during the 1990–2000 period, adjusted for the remittance of profits and dividends (as a percentage of GDP), which show that the net contribution of FDI inflows to the financing of fixed private capital formation is far less than that advertised by the unadjusted FDI flows (see Table 1, p. 804). Finally, it is likely that the net contribution of FDI (from a financing standpoint) would be further reduced if the widespread practice of intra-firm transfer pricing to avoid taxes on the repatriation of profits is also taken into account. In recent years, a number of investigators have undertaken (univariate) empirical studies that attempt to assess the economic impact of changes in public and foreign capital investment in developing countries such as Mexico. The renewed interest in this topic stems from the fact that investments in economic and social infrastructure, whether originating in the public and/or foreign sector, often generate substantial positive spillover benefits for the private sector by reducing the direct (and indirect) costs of producing, transporting, and delivering goods and services to consumers [see Barth and Cordes 1980; Ram 1986; Aschauer 1989; Khan and Reinhart 1990; Green and Villanueva 1991; Cardoso 1993; Devarajan and Heng-fu 1994; Lin, 1994; Moguillansky 1996; Nazmi and Ramirez 1997; Albala-Bertrand and Mamatzakis 2001; Ramirez 2002; Hermes and Lensink 2004]. However, the major problem with several of these studies is that the data set available to test the complementarity hypothesis is often in annual terms and for a limited time period. Thus, even when cointegration tests are performed and error correction models are generated, the reliability of the estimates is questionable because unit root tests have low power when the number of observations is less than 50, as is often the case with univariate (annual) time-series studies.Footnote 3 In light of the above, this paper estimates a pooled model that attempts to determine whether public and foreign capital in three major sectors of the Mexican economy have a positive and significant effect on Mexican output (and labor productivity) over the 1960–2001 period. The information contained in the time series data is thus enhanced by the cross-sectional (sectorial) data which makes it possible to reliably test whether increases in government and foreign investment spending enhance overall output and labor productivity in Mexico. The focus on Mexico is particularly relevant because it is one of the few countries in Latin America and the Caribbean that has reliable and disaggregated time-series data on public, private, and foreign investment spending on a sectorial basis going as far back as the 1950s. More importantly, perhaps, it also allows policymakers to determine where the effects of public and foreign investment spending, if any, are most significant. And since Mexico is a country faced with severe constraints in generating public revenues, any additional information that improves the allocation of scarce resources should prove highly useful to the country's policymakers. The paper is organized as follows. Next section provides an economic rationale for including the public and foreign capital stocks as arguments in a modified neoclassical production function, and discusses the empirical methodology to be employed in subsequent sections. The following section pools data for the primary (agricultural and mining), industrial (manufacturing and heavy industry), and service (banking and retail services) sectors and estimates a “stacked” production function (and labor productivity function) over the 1960–2001 period. This section also applies recently developed panel unit root tests to the relevant variables to determine if they are stationary and a panel (and group) cointegration test developed by Pedroni [1999a] is used to determine whether there is a stable long-term relationship among the relevant panel regressors of the modified pooled production (labor productivity) function. In addition, it proceeds to estimate the pooled production (productivity) function via a “group-mean” panel fully modified ordinary least squares (FMOLS) estimator developed by Pedroni [1999b; 2001] which not only generates consistent estimates of the parameters in relatively small samples, but also controls for potential endogeneity of the regressors and serial correlation. This study thus represents an important contribution to the extant literature on the complementarity hypothesis because it addresses the important question of spurious correlation among the variables in the pooled (stacked) model. The last section summarizes the paper's major findings and offers some policy prescriptions.",10
36,1,Eastern Economic Journal,22 December 2009,https://link.springer.com/article/10.1057/eej.2008.61,Texas Treasury Notes and the Mexican-American War: Market Responses to Diplomatic and Battlefield Events,January 2010,Gary M Pecquet,Clifford F Thies,,Male,Male,Unknown,Male,"In May 1846, following border clashes, the United States declared war on Mexico. The military forces of the United States subsequently scored a series of victories over those of Mexico. Following the end of major hostilities, the Treaty of Guadalupe Hidalgo — signed by representatives of the two countries in February 1848, and subsequently ratified by the US Senate in March and by the Mexican government in May — brought the war to a conclusion. The treaty gained for the United States all or part of the present day states of Arizona, California, Colorado, Nevada, New Mexico, and Utah, as well as Mexico's recognition that Texas had previously gained its independence. As thus summarized, it might appear that the Mexican-American War was a short and decisive war. However, upon examination, the war is found to have been problematic. This study uses newly constructed time series of the market values of Texas Treasury Notes to reveal investor sentiment regarding the war. The Texas Treasury Notes, along with other securities, had been issued by the Republic of Texas during its short period of independence. They were repudiated by that government in 1842, but were revived upon the annexation of Texas by the United States in 1845 [Pecquet and Thies 2006; 2007]. During the Mexican-American War, these notes traded both in New Orleans and Philadelphia, and their values were quite responsive to news related to the war. The Texas Treasury Notes were a “play” on the war because a diplomatic agreement that either avoided a war or kept the war short, by securing to Texas its border with Mexico, could have been expected to attract immigrants to the state, who would have increased the demand for and the value of the vast public lands of Texas. As provided by the terms under which Texas was admitted to the Union, the value of the Red Backs was determined by the value of the public lands of Texas. According to the annexation agreement, the State of Texas would pledge its public lands for their Red Backs [Pecquet and Thies 2006, p. 251]. Upon annexation the Red Backs traded for much less than their face value reflecting the glut of Texas land scrip and claims that could be converted into the same that existed at the time. This glut was expected to persist for some time, although some may have hoped for a relatively quick absorption of the glut of claims on the public land of Texas and a quick rise in the value of these claims, as happened with US land scrip during the 1790s [Sylla et al. 2005]. Once the Mexican-American War started, the Red Backs were also play on the war because the US government started to award federal land scrip to soldiers [Oberly 1990], making Texas land scrip less valuable. If indeed the Red Backs were a play on the war, other securities that were plays on the war would have moved at the same time as did the Red Backs in response to the arrival of information of the war. US Treasury debt and an index of high-grade state debt are examined, and are found not at all responsive to the arrival of information of the war. The Mexican-American War was important to the value of the Red Backs merely because of reasons such as securing the border of Texas and the issue of federal land scrip to soldiers, and not because the war posed a fiscal threat, not to speak of an existential threat to the United States. During the Mexican-American War, the newspapers of the United States proved to be very capable at gathering and disseminating information. This included ferreting out secret information, and quickly transmitting information throughout the country. In most cases, it is clear exactly when news of military and diplomatic events reached New Orleans and Philadelphia. In other cases, the arrival of news can be approximated by the historical date and place of an event and the speed of communication. Events originating in Washington, DC typically reached the Philadelphia market within 2 days, and the New Orleans market within 2 weeks.Footnote 1 Conversely, the news of events originating in Mexico typically reached the New Orleans market within 2 weeks, and the Philadelphia market within 3 weeks.Footnote 2 To be sure, information related to events did not always arrive in discrete packages, as when no response to a cease-fire proposal eventually communicated a negative response. Event study methodology has been applied to the study of battlefield and diplomatic events during wars, being first applied to the US Civil War [Willard et al. 1996; see also Weidenmier 2002]. The methodology has, recently, been applied to other wars. Frey and Waldenstrom [2004], for example, examine market data from Zurich and Stockholm to identify breaks in national bond price indices that correspond to major events of World War II such as the outbreak of the war, the German invasion of the lowlands in 1940, the battle of Stalingrad, and the Potsdam conference. In the majority of these studies, the data are examined for their most probable “break-points.” Although useful for examining market reaction to what Oosterlinck [2006] describes as “extreme events,” critical to the outcomes of existential wars such as the US Civil War and World War II, this method may be less useful for examining market reaction to events of middling significance, as might characterize battlefield and diplomatic events associated with wars of choice such as the Mexican-American War. In the case of events of middling significance, possible break-points might be better identified a priori, through a detailed study of the history of the time, and then put to the classical statistical test. In this paper, both the a priori and empirical methods are applied to a pair of financial markets, Philadelphia and New Orleans, during a war that preceded the wiring of continent for the telegraph. At the time, the New Orleans market was one of competing brokers who acted as market-makers, quoting the prices of non-current bank notes, foreign and domestic exchange, and certain other securities in terms of local currency. Insofar as the Red Backs were concerned, Philadelphia was a hybrid market with both broker- and exchange-based transactions. At the time, the Philadelphia Stock Exchange was characterized by crowd trading, with the Red Backs and other obligations of the Republic of Texas treated as speculative grade or “fancy” securities. The prices of Texas Treasury Notes in New Orleans are end-of-week broker quotations from the New Orleans Price Current. In Philadelphia, prices are mainly end-of-week broker quotations from Bicknel's Reporter. In both markets, prices in the referenced newspapers were compared to those in other papers in the same market, with substitutions when price information was non-available or stale.Footnote 3 Two other time series are examined: the price of US Treasury 6 percent stock (or, in today's parlance, bonds) of 1862 (i.e., redeemable at par beginning in 1862), and the average price of the 6 percent stocks of Kentucky, New York, and Ohio, in the New York market.Footnote 4 These are the last-observed sales prices or broker quotations if there were no sales during the prior week, from New York Journal of Commerce. These time series are displayed in Figures 1 and 2. Market value of Texas treasury notes per $100 par value in New Orleans and in Philadelphia, November 1, 1845–December 30, 1848. Market value of US 6s of 1862 and state 6s per $100 par value in New York, November 1, 1845–December 30, 1848. 
Burdekin [2006] has examined the course of the value of the Red Backs during the Mexican-American War period as one of several periods from the revival of the Red Backs to their ultimate redemption, using the empirical method and sales data from the Philadelphia Public Ledger. He identifies two break-points in each period. One of the two break-points he identifies for the Mexican-American War period corresponds to the outbreak of the war, which we similarly identify as the most probable break-point using the empirical method in both the Philadelphia and New Orleans markets. The second break-point that Burdekin identifies is a week in November 1847 that he claims reflects the end of major hostilities. But, news of the last several battles of the war reached Philadelphia during August and September of that year and during October, the market had already turned its attention to the initially disappointing diplomatic effort to settle the war.Footnote 5 The second-most probable break-point we identify in Philadelphia is the signing of the peace treaty ending the war, in February 1848. In New Orleans, where the market apparently became inefficient toward the end of the period, the second-most probable break-point is a week following news of the Oregon Treaty with Great Britain. The rest of this paper is organized as follows: The next several sections present a terse history of the war interspersed with market commentary. Following this, an econometric model is developed to precisely identify the market impact of the more important military and diplomatic events of the war. Following this, the findings of the paper are summarized and discussed.",2
36,1,Eastern Economic Journal,22 December 2009,https://link.springer.com/article/10.1057/eej.2008.53,"Oligopolistic Competition, Firm Heterogeneity, and the Impact of International Trade",January 2010,Haiwen Zhou,,,Unknown,Unknown,Unknown,Unknown,,
36,1,Eastern Economic Journal,22 December 2009,https://link.springer.com/article/10.1057/eej.2008.54,A Reevaluation of the Effect of Human Capital Accumulation on Economic Growth Using Natural Disasters as an Instrument,January 2010,Hideki Toya,Mark Skidmore,Raymond Robertson,Male,Male,Male,Male,"Theoretic models of economic growth suggest that human capital accumulation is a significant determinant of rising per capita income. Microeconomic evidence of the positive relationship between schooling and wages supports this prediction. Estimates using macroeconomic data demonstrate that the initial stock of human capital is an important determinant of economic growth, but empirical estimates of the effects of changes in human capital (human capital accumulation) poorly match theoretic predictions [Benhabib and Spiegel 1994; Barro and Sala-i-Martin 1995]. Most studies approach this poor match as a measurement error problem [Mankiw et al. 1992; Temple 1999a; Krueger and Lindahl 2001], including correcting for imperfect measures of quality [Hanushek and Kimko 2000; Wossman 2003]. Other studies have focused on the effects of outliers [Temple 1999b] or the use of incorrect specifications [Englander and Gurney 1994; Gemmel 1996; Bassanini and Scarpetta 2002; Engelbrecht 2003]. These studies suggest that restricting the sample to OECD countries can generate a generally positive effect of changes in schooling or school enrollments on growth that is similar in magnitude to those found in microeconomic estimates based on survey data, but tell us little about the experience of countries outside the OECD, which are often considered to be the developing countries. Our contribution to the literature is based on the possibility that the poor match between theory and empirical work results not from how we measure human capital, but from the potentially endogenous relationship between changes in human capital and economic growth. Using data from developed and developing countries, we present evidence suggesting that human capital is, in fact, endogenously determined and therefore empirical analysis requires an instrumental variable approach. We are not the first to introduce instrumental variables to this literature. Pritchett [2001] and Krueger and Lindahl [2001] apply an instrumental variables technique to estimate the effect of changes in average years of schooling on growth, using Nehru et al. [1995] and Kyriacou's [1991] schooling data as instruments, respectively.Footnote 1 Their purpose in using the instrumental variables method is to overcome the measurement error issue, and not necessarily to address endogeneity per se. Indeed, the Nehru, Swanson, and Dubey and Kyriacou schooling variables are not appropriate instruments if one is trying to address endogeneity. A valid instrument in this context is one that determines changes in schooling but is not a direct determinant of economic growth; alternative measures of schooling are arguably just as important determinants of growth as is the Barro and Lee [1996] measure of schooling. Furthermore, researchers such as Glewwe and Jacoby [2004] have shown that demand for education is positively correlated with increases in household income and wealth, thus emphasizing the two-way relationship between economic growth and human capital accumulation. It appears that a key reason that researchers have not yet addressed the endogeneity issue is because of the lack of valid instruments. To our knowledge, no studies exist in which the endogeneity of human capital accumulation is tested, and if found to be present, the instrumental variables method is used to estimate the effects of changes in human capital on growth. Skidmore and Toya [2002] demonstrate that climatic natural disasters affect growth through human capital accumulation. This result suggests that a climatic disaster variable may be an appropriate instrument. In the aftermath of the 2004, Southeast Asian Tsunami that killed more than 280,000 and affected millions, it is not unreasonable for economists to consider how the threat of natural disasters might affect human and physical capital decisions. Skidmore and Toya [2002] suggest that due to relatively recent advances in forecasting, climatic disasters (as opposed to geologic disasters such as the tsunami) are primarily a threat to immobile physical capital but not mobile forms of capital such as human capital. The relative increase in exposure to risk of physical capital provides an incentive for economic agents to invest relatively more heavily in human capital. The correlation between exogenous natural disasters and endogenous investment decisions over time suggest that disaster propensity is a valid instrument for factors that affect growth. In this article we use measures of the propensity for natural disasters to test for the endogeneity of changes in average years of schooling over the 1960–1990 period. We find evidence of endogeneity, and therefore employ instrumental variables techniques to estimate the effects of changes in human capital on economic growth. The instrumental variables estimation procedure yields a coefficient on human capital accumulation that is larger in magnitude than found in our OLS estimates, in most previous studies that use data from a wide range of countries, and is closer to theoretic predictions. Bils and Klenow [2000] attempt to determine the causal relationship between schooling and economic growth. They point out that a common belief is that “reverse causality” or simultaneity is likely to lead to an over-estimate of the effect of human capital accumulation on growth because anticipated increases in future economic growth could cause schooling to increase. In fact, however, the direction of bias introduced by simultaneity is indeterminate. The fact that empirical macroeconomic estimates of the contributions of human capital accumulation to growth are considerably smaller than those suggested by the microeconomic evidence of the returns to schooling suggests that perhaps the macroeconomic estimates suffer from downward rather than upward bias. This bias could be the result of a combination of factors: mismeasurement, simultaneity, and omitted variable bias. An effective instrumental variable analysis can address all of these issues. The remainder of the paper is organized as follows. The next section ‘Theoretical and empirical underpinnings’ provides a review of the relevant theoretical considerations and outlines our empirical strategy. In the section ‘Natural disasters as an instrument’, we present an overview of natural disasters and describe the intuition behind their effectiveness as instruments. In the section ‘Empirical analysis’, we carefully evaluate the validity of natural disasters as an instrument and present the empirical analysis, and the section ‘Conclusions’ concludes.",17
36,1,Eastern Economic Journal,22 December 2009,https://link.springer.com/article/10.1057/eej.2009.12,"Marx and the Meaning of Marxism: Introduction and Analyses, by Stanley Bober",January 2010,Ronald G Bodkin,,,Male,Unknown,Unknown,Male,,
36,1,Eastern Economic Journal,22 December 2009,https://link.springer.com/article/10.1057/eej.2009.13,"Human Rights and Structural Adjustment, edited by M. Rodwan Abouharb and David Cingranelli.",January 2010,Steve Onyeiwu,,,Male,Unknown,Unknown,Male,,
36,1,Eastern Economic Journal,22 December 2009,https://link.springer.com/article/10.1057/eej.2009.19,"Small Loans, Big Dreams: How the Nobel Prize Winner Muhammad Yunus and Microfinance are Changing the World, edited by Alex Counts",January 2010,Yilma Gebremariam,,,Unknown,Unknown,Unknown,Unknown,,
36,1,Eastern Economic Journal,22 December 2009,https://link.springer.com/article/10.1057/eej.2009.14,"Financialization and the US Economy, edited by Özgür Orhangazi.",January 2010,Matthew Fung,,,Male,Unknown,Unknown,Male,,
36,1,Eastern Economic Journal,22 December 2009,https://link.springer.com/article/10.1057/eej.2009.16,"The Drunkard's Walk, edited by Leonard Mlodinow.",January 2010,Eugene McManus,,,Male,Unknown,Unknown,Male,,
36,2,Eastern Economic Journal,19 March 2010,https://link.springer.com/article/10.1057/eej.2009.6,"Grades, Course Evaluations, and Academic Incentives",March 2010,David A Love,Matthew J Kotchen,,Male,Male,Unknown,Male,"At the end of an academic term, students fill out course evaluations and faculty assign grades. The proximity of these events makes it natural to wonder whether they are linked. In particular, can professors improve evaluations by grading more easily? And if so, what are the implications for students, faculty, and academic institutions? We address these questions in the context of a simple theoretical model and demonstrate that the link between grades and evaluations can lead to grade inflation, diminished student effort, and a disconnect between institutional expectations and faculty incentives. We assume that students value the educational quality of their classes, in addition to time spent on other activities and future earnings. Professors, in turn, are assumed to value the education of students but also understand that tenure and promotion decisions are based on research output and course evaluations. We show that if evaluations reflect a broader measure of overall student welfare, which includes educational quality, leisure, and future income, lenient grading offers a low-cost means of boosting course evaluations without sacrificing time for research. Beyond the basic setup of the model, our main contribution is to demonstrate how the interaction between students and professors can lead to a divergence between an institution's academic goals and the actual behaviors that its incentives induce. Academic institutions and individual departments create incentives for faculty by choosing how much emphasis to place on teaching and research in promotion and tenure decisions. Faculty, in turn, set incentives for students through designing the structure of a course and establishing grading standards. If instructors use grades to influence evaluations, the incentives become linked, creating the possibility for perverse responses to institutional policies. We identify a range of new and seemingly counterintuitive insights about how an institution's promotion criteria may affect student and faculty behavior. First, we find that as institutions seek to improve teaching quality by placing more weight on course evaluations, professors respond by relaxing grading standards and possibly even teaching effort. Second, students react to the changes in grading policy and teaching effort by adjusting the time they spend studying. Interestingly, the adjustment of study effort to grade inflation need not be negative. In fact, under reasonable assumptions, students may actually increase their effort in response to grade inflation, a response that follows from the potential for more stringent grading to discourage students who have little to gain from higher grades. Third, increased emphasis on research also contributes to grade inflation, as it induces professors to allocate time away from teaching and to offset the resulting decline in course evaluations by raising grades. These findings suggest that grade inflation can arise from an institution's efforts to improve teaching quality, research productivity, or both. This is not to suggest, however, that grade inflation is a necessary concomitant of an institution's efforts to improve quality. Grade targets, as we show, can effectively restrain grade inflation by aligning professorial incentives with institutional objectives. Our final set of results relate to the implications of the model for hiring, promotion, and tenure. We show that professor heterogeneity provides one channel through which institutions can adjust research and teaching quality without exacerbating grade inflation. The model demonstrates the importance of recruiting faculty whose preferences for research and teaching are closely aligned with institutional goals. The alignment of professorial preferences with institutional goals also plays an important role in determining a professor's behavior after tenure. While the model demonstrates the possibility for enhanced or diminished teaching and research, a clear prediction is that we should expect more stringent grading.",30
36,2,Eastern Economic Journal,19 March 2010,https://link.springer.com/article/10.1057/eej.2008.62,Functional Impairment and the Choice of College Major,March 2010,John Robst,Jennifer VanGilder,,Male,Female,Unknown,Mix,,
36,2,Eastern Economic Journal,19 March 2010,https://link.springer.com/article/10.1057/eej.2009.2,European Current Account Sustainability: New Evidence Based On Unit Roots and Fractional Integration,March 2010,Juncal Cunado,Luis A Gil-Alana,Fernando Pérez de Gracia,Unknown,Male,Male,Male,"The current account balance is the focal point for measurement of economic performance in any open economy. The reason for this is twofold. First, the current account balance is closely related to the other components of national investment and saving — the fiscal balance and private savings. Thus, it has important implications for overall growth. Second, the current account balance will often have implications for the exchange rate and competitiveness. The persistence of large current account deficits in most open market economies raises the issue of whether these deficits are sustainable, that is, whether the country's intertemporal budget constraint is violated. Short-run or temporary current account deficits are not ‘bad’ as they reflect reallocation of capital to the country where capital is more productive. However, long-run or persistent deficits can have serious effects as they might impose excessive burden on future generations, as the accumulation of large external debt would imply increasing interest payments and lower standard of living. Authors such as Trehan and Walsh [1991], Gundlach and Sinn [1992], Otto [1992], Wickens and Uctum [1993], Liu and Tanner [1996], Wu [2000], Wu et al. [2001], Taylor [2002], Christopoulos and León-Ledesma [2004], Lau et al. [2006] and Cunado et al. [2008] among many others have examined the statistical properties of the current account. One approach to examine the stationarity of current accounts is to look at the possibility of cointegration between exports and imports, testing also the restriction that the cointegrating vector is (1,  −1), as in Husted [1992], Fountas and Wu [1999] and Arize [2002]. This approach focuses on the long-run relationship between exports and imports and imposes that both individual variables are integrated of order 1. Although some studies, such as Husted [1992] or Arize [2002], show that there is a long-run relationship between imports and exports for the US case, implying that trade deficits are sustainable, other studies, such as Fountas and Wu [1999] show that the hypothesis of no long-run relationship cannot be rejected, concluding that the US trade deficits are not sustainable. In fact, a common empirical finding in the literature across countries is the non-stationary nature of current accounts, at least when using traditional unit root tests. In this paper, we re-examine the external sustainability issue by looking at the orders of integration of exports, imports and current accounts. However, instead of using standard I(0)/I(1) approaches [i.e., Dickey and Fuller 1979; Phillips and Perron 1988; Kwiatkowski et al. 1992], we use first a recent method for testing unit roots proposed by Ng and Perron [2001]. A problem with the above approaches is that all them are based on autoregressive (AR) alternatives and do not take into account possible fractional orders of integration. Note that this is a very limited specification and Diebold and Rudebusch [1991], Hassler and Wolters [1994], Lee and Schmidt [1996] and others showed that most unit root tests embedded in AR models have very low power if the alternatives are of a fractional-form. Therefore, in this paper we also employ a version of the tests of Robinson [1994], where the unit root is embedded in a fractional structure of form: where x
t
 is the observable data (once the deterministic terms have been removed), d can be any real number, and u
t
 is I(0) defined as a covariance stationary process with spectral density that is positive and finite. The fractional differencing parameter d plays a crucial role from both theoretical and empirical viewpoints. Thus, if d<0.5, x
t
 is covariance stationary and mean-reverting, with the effect of the shocks dying away in the long run. If d∈(0.5,  1), x
t
 is no longer covariance stationary but is still mean reverting, while d⩾1 implies non-stationarity and non-mean-reversion. Thus, the fractional differencing parameter d plays a crucial role for our understanding of the economic variables such as exports, imports and current account balance. In particular, a variable having a unit root supports the view that any shock will have a permanent effect, so a policy action will be required to bring the variable back to its original long-term behavior. On the other hand, if d is smaller than 1, fluctuations will be transitory and therefore, there exists less need for the policy action, since the series will in any case return to its trend sometime in the future. Thus, in the context of current accounts, sustainability is satisfied as long as d is strictly smaller than 1, though the convergence towards equilibrium will take longer time as higher is the value of d. There are several distinguishing features of Robinson's [1994] tests compared with other procedures. In particular, they have a standard (normal) null limit distribution and they are the most efficient ones when directed against the appropriate alternatives. The functional form of the test statistic can be easily found in any of the numerous empirical applications of these tests, [Gil-Alana and Robinson 1997; Gil-Alana 2000a; 2001]. Additionally, we also performed Sowell's [1992] procedure of estimating d by maximum likelihood in the time domain, and though not reported, and with some differences in the estimates of the d's, the results were practically the same in terms of the mean reverting (d<1) or no mean-reverting (d⩾1) properties of the series.",9
36,2,Eastern Economic Journal,19 March 2010,https://link.springer.com/article/10.1057/eej.2009.3,When Are Supply And Demand Determined Recursively Rather Than Simultaneously?,March 2010,Kathryn Graddy,Peter Kennedy,,Female,Male,Unknown,Mix,,
36,2,Eastern Economic Journal,19 March 2010,https://link.springer.com/article/10.1057/eej.2009.4,The “New Consensus” and the Post-Keynesian Approach to the Analysis of Liquidity Traps,March 2010,Alfonso Palacio-Vera,,,Male,Unknown,Unknown,Male,"The most salient feature of the institutional framework that characterizes most, if not all, present-day OECD economies is that the central bank (hereafter CB) fine-tunes the economy through conventional monetary policy actions with a view to achieving an inflation target in the medium-term. We define the former as the regular actions that characterize the day-to-day setting of short-term nominal rates by the CB. However, the latter are subject to a zero lower bound (hereafter ZB). This constraint arises because, in a money-using economy, individuals will not be willing to hold any financial asset other than money when the nominal yield of the former is equal or less than zero. Its existence may cause a “liquidity trap” (hereafter LT), that is, a situation where for a given set of inflation expectations the CB is unable to push real interest rates down far enough so as to keep inflation constant in the absence of supply shocks. The existence of the ZB has been well-known for a long time. Yet, it has only been of recent that mainstream economists have focused their attention on it. Research on the causes and policies to deal with a LT has been led by proponents of the “New Consensus” in macroeconomics (hereafter NC).Footnote 1 Two events may explain, according to us, this phenomenon. First, it has become much clearer, especially since the early 1990s, that CBs fine-tune the economy through changes in interest rates rather than through changes in the monetary base. Second, there is the protracted period of sluggish growth and deflation experienced by the Japanese economy since the 1990s and its attribution by some prominent scholars to the occurrence of a LT [Krugman 1998; Svensson 2005]. The main contribution of this study is to identify a number of key differences in the diagnosis of the occurrence of a LT in the NC and the Post-Keynesian (hereafter PK) approach. According to us, the crucial difference is that, in the former, a LT is viewed as a fairly rare and transitory situation which may only emerge in the wake of unusually large adverse shocks that depress the “neutral” or “natural” interest rate. By contrast, in the PK approach, an economy may also exhibit a “structural” or long-lasting LT. This will be the case if a combination of high precautionary saving, low investment spending by firms, and stringent bank credit conditions for firms and households alike stemming from a high degree of liquidity preference (hereafter LP) makes the sum of the steady-growth “neutral” interest rate and the expected inflation rate fall short of the term/risk premium on long-term interest rates. The structure of the paper is as follows. Section 2 presents a formal definition of a LT and identifies the factors that may engender it within a general framework. Section 3 reviews briefly the NC approach taking the influential contribution in Woodford [2003] as a benchmark. Section 4 expounds the PK approach to the explanation of the LT vis-à-vis the NC approach. Section 5 expounds a simple PK macroeconomic model for a closed economy without a government sector that illustrates the PK position with regard to the occurrence of a “structural” LT. Section 6 summarizes and concludes.",4
36,2,Eastern Economic Journal,19 March 2010,https://link.springer.com/article/10.1057/eej.2009.5,The Economist as Dean: An Investigation of the Academic Training of Business School Deans,March 2010,M Scott Niederjohn,Sarah B Cosgrove,,Unknown,Female,Unknown,Female,"The motivation for this paper originates from the authors’ observation that a prevailing number of deans of US business schools were trained as economists. Upon further examination, data reveal that nearly 18 percent of the deans at the American Assembly of Collegiate Schools of Business [AACSB]-accredited business schools hold Ph.D.s in economics. This represents the largest percentage of deans from any academic discipline. The next closest areas are marketing and organizational behavior/human resources, at around 12 percent of the deans. A review of the relevant literature pertaining to the characteristics, responsibilities, and challenges of academic administrators reveals a number of insights about the nature of the deanship. For example, deans tend to have experience serving in lower-level administrative positions before becoming deans. Further, the literature review explores the wide-ranging duties that most deans are expected to perform. Previous research also illuminates the numerous challenges that deans face. The finding that so many business school deans come from the ranks of economists seems contrary to what one might expect. Given the need to manage a large organization as dean, management and human resource Ph.D.s would seem to be good fits to dominate the deanship. Alternatively, the fundraising duties now required of deans might suggest that marketing Ph.D.s would be well suited for the position. The fiscal challenges deans face could lead finance or accounting professors to prosper as candidates for the deanship. Siegfried [1997] examines why economists may be particularly well suited to be academic administrators. He highlights many of the principles of economics that are valuable tools in the position of chief academic officer. Finally, this paper examines both supply- and demand-side theories for an explanation of why so many economists become deans of highly ranked business schools. We explore the characteristics desired in a typical dean search on the demand side, and the role that salary might play on the supply side. Lastly, interviews with a subset of the business school deans were conducted with particular attention focused on the academic discipline most valuable to a dean. Why so many “dismal” business deans? While there is likely no way to answer this question definitively, this study should be of interest to all those who teach and study economics. A solid understanding of scarcity, tradeoffs, marginal analysis, costs, and benefits seems to be important to the dean of a highly ranked business schools.",1
36,2,Eastern Economic Journal,19 March 2010,https://link.springer.com/article/10.1057/eej.2009.7,Property Taxation and Density of Land Development: A Simple Model with Numerical Simulations,March 2010,Richard W England,Mohan Ravichandran,,Male,Male,Unknown,Male,"For the past decade or more, economists have been exploring the complex phenomenon known as metropolitan sprawl [Nechyba and Walsh 2004]. Although sprawl has a variety of root causes, consequences and even definitions, one of its important dimensions is the relationship between property taxation and the land development choices of private developers.Footnote 1 In this paper, we focus on how property tax rates might affect building footprint and height decisions of developers, thereby influencing the rate of metropolitan expansion. Several authors have already explored the theoretical relationships between property taxation, on the one hand, and the timing and density of land development, on the other hand. Brueckner [1986] is an early exploration of the density question. Anderson [1986] offers a parallel treatment of the timing issue. Capozza and Li [1994] find that a higher property tax rate could accelerate land conversion and reduce capital intensity of developed parcels. England and Mohr [2006], on the other hand, predict that a higher property tax rate would delay development of those rural parcels enrolled in use-value assessment programs. Brueckner and Kim [2003] conclude that the property tax encourages urban sprawl when the substitution between housing and other goods is low and that a revenue-neutral switch to a land tax would favor more compact metropolitan development. Arnott and Petrova [2006] predict that a higher property tax rate lowers density of development projects, especially for higher elasticities of substitution between capital and land. In this paper, we try to advance the theoretical analysis of property taxation and density of land development by addressing the following set of issues in a static model of the decision to develop a parcel. One is that the rents accruing to the owner of a developed parcel derive not just from its location within the metropolitan region and the physical capital constructed on the site but also from the amenities provided by the undeveloped portion of the parcel itself. Second, structures on developed parcels are three-dimensional and hence both the footprint of a building and its height need to be modeled explicitly as development decisions. Third, the ease of substituting physical capital for undeveloped land in the production of “parcel services” needs to be modeled explicitly. Fourth, the effect that building height has on construction cost per square foot should be acknowledged.Footnote 2 A final consideration is that analysis of the land-use impacts of property taxation needs to recognize that the conventional property tax is actually two taxes bundled together at the same ad valorem rate, one on building value and the other on land value. In the following section, we introduce these considerations into a model of the decision to develop a vacant parcel of land within a metropolitan region. (For earlier treatments of this dual tax rate issue, see Brueckner [1986] and Anderson [1999].) An important implication of our model is that it is the portion of the property tax levied on building values that affects density of development projects. If the portion of the property tax levied on land values is fully capitalized, then it simply reduces the land price paid by developers and does not affect density.Footnote 3",6
36,2,Eastern Economic Journal,19 March 2010,https://link.springer.com/article/10.1057/eej.2009.8,Who Pays for Drug Quality?,March 2010,Jie Chen,John A Rizzo,,,Male,Unknown,Mix,,
36,2,Eastern Economic Journal,19 March 2010,https://link.springer.com/article/10.1057/eej.2009.9,The Second Paycheck to Keep Up with the Joneses: Relative Income Concerns and Labor Market Decisions of Married Women,March 2010,Yongjin Park,,,Unknown,Unknown,Unknown,Unknown,,
36,2,Eastern Economic Journal,19 March 2010,https://link.springer.com/article/10.1057/eej.2009.15,"The Street Porter and the Philosopher: Conversations on Analytical Egalitarianism, edited by Sandra J. Peart and David M. Levy",March 2010,Karl Widerquist,,,Male,Unknown,Unknown,Male,,
36,2,Eastern Economic Journal,19 March 2010,https://link.springer.com/article/10.1057/eej.2009.17,"International Political Economy: An Intellectual History, by Benjamin J. Cohen",March 2010,Mina Baliamoune-Lutz,,,Female,Unknown,Unknown,Female,,
36,2,Eastern Economic Journal,19 March 2010,https://link.springer.com/article/10.1057/eej.2009.18,"A Future of Good Jobs? America's Challenge in the Global Economy, edited by Timothy J. Bartik and Susan N. Houseman",March 2010,Amy B Schmidt,,,Female,Unknown,Unknown,Female,,
36,2,Eastern Economic Journal,19 March 2010,https://link.springer.com/article/10.1057/eej.2009.20,"How Do We Spend Our Time? Evidence from the American Time Use Survey, edited by Jean Kimmel",March 2010,Charlene M Kalenkoski,,,Female,Unknown,Unknown,Female,,
36,2,Eastern Economic Journal,19 March 2010,https://link.springer.com/article/10.1057/eej.2010.13,List of Reviewers 2009,March 2010,,,,Unknown,Unknown,Unknown,Unknown,,
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2010.22,Spreading the Wealth Around: Reflections Inspired by Joe the Plumber,June 2010,N Gregory Mankiw,,,Unknown,Unknown,Unknown,Unknown,,
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2009.11,Identifying the Effect of a Welfare-To-Work Program Using Program Capacity Constraints: A New York City Quasi-Experiment,June 2010,John Ifcher,,,Male,Unknown,Unknown,Male,"Over the past two decades, US welfare programs have been transformed. A central objective of the reforms has been to help recipients move from welfare to work. In response, a variety of “welfare-to-work” programs have been implemented. They typically include one or more of the following components: unpaid work experience, classroom instruction, on-the-job training, financial incentives, and job search assistance. A substantial literature has developed to identify the resulting changes in welfare use, employment, well-being, and family structure [recent reviews include Blank 2002; Grogger and Karoly 2005; Moffitt 2003]. The results largely support the following two conclusions. First, in the short run at least, welfare-to-work programs had the intended effect: reducing welfare use and increasing employment. Second, the most effective welfare-to-work programs are those that include mandatory work requirements with an emphasis on job placement [Grogger and Karoly 2005]. These findings are largely derived from well-designed random-assignment experiments. Although such experiments are certainly an excellent method for identifying the effect of a welfare-to-work program, they are not impervious to criticism. The following five shortcomings have been identified and are summarized in Grogger and Karoly [2005]. First, as experimental programs are often implemented by above-average managers, it is unclear whether successful pilot programs can be expanded without losses in effectiveness. Second, experiments miss some general equilibrium effects. For example, whereas full-scale implementation of a welfare-to-work program might crowd-out other job seekers or even subsequent program participants, a pilot program might not. Third, the “message” of the pilot program may cross over from the program group to the control group. Fourth, experiments that only include current welfare recipients do not capture entry effects. That is, a welfare-to-work program may change the attractiveness of receiving welfare and therefore affect not only the exit rate, but also the entry rate. Finally, random-assignment experiments can be costly and hard to implement. The first four shortcomings are noteworthy, as each could cause experimental estimates to be biased. For example, the first presumably introduces a positive bias and the third a negative bias. Thus, available estimates, which are largely based on experimental evidence, are potentially biased, with the direction and magnitude of the overall bias unknown. Moreover, no new random-assignment experiments have been conducted since the Personal Responsibility and Work Opportunity Reconciliation Act.Footnote 1 A few non-experimental and quasi-experimental studies have explored the effect of welfare-to-work programs. Two of these studies have challenged the claim that the most effective welfare-to-work programs are those that emphasize rapid job placement [Hotz et al. 2006; Dyke et al. 2006]. These authors find that more intensive training, which emphasizes human capital development, generates larger positive effects. These effects, however, take longer to emerge than do the effects of programs that emphasize job placement.Footnote 2 The paucity of non-experimental and quasi-experimental studies is largely due to two factors. First, there has been a shortage of clever strategies (in the absence of random-assignment) to identify the effects of welfare-to-work programs using administrative data. Second, welfare-to-work programs vary along numerous dimensions, making it difficult to parameterize the programs in a manner that is useful for identification. Thus, data from nationally representative surveys have not been successfully used to estimate the effect of welfare-to-work programs.Footnote 3 In this paper, I demonstrate a quasi-experimental identification strategy to estimate the effect of a welfare-to-work program. To do so, I take advantage of a quirk in the program's administration. Specifically, when the program was initiated, all eligible General Assistance (GA) recipients could not participate concurrently due to capacity constraints.Footnote 4 Rather, GA recipients were selected for the program in “waves.” The wave enrollment process creates the opportunity to identify the effect of the program by comparing “selectees,” recipients who were selected on a given date, to “non-selectees,” recipients who were eligible but not selected on that date. The results indicate that the program increased the likelihood that a recipient started a job by 15 percentage points, and increased the likelihood that a recipient exited welfare by 10 percentage points. This quasi-experiment is similar to a random-assignment experiment in that it enables one to estimate the effect of a welfare-to-work program. It also ameliorates three of the five previously discussed shortcomings of random-assignment experiments. First, the program was not a small-scale, pilot program implemented by above-average managers. Rather, it was the principal welfare-to-work program for all welfare recipients in New York City (NYC). Second, this analysis should capture general equilibrium effects in the labor market given (1) the size of program (over 10,000 GA recipients were enrolled during the program's first year) and (2) the number of welfare recipients who started a job (over 100,000 recipients of Family Assistance (FA) and GA combined reported finding a job during the program's first 2 years). Finally, this program was not a costly, or difficult to implement, random-assignment experiment. The next section of this paper presents an overview of welfare reform in NYC. The third section describes the quasi-experimental identification strategy. The fourth section discusses the empirical implementation. The fifth section presents the results. The sixth section describes a robustness check that was performed. The final section discusses the findings and presents a brief fiscal Cost–Benefit Analysis (CBA).",1
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2009.10,Of Altruists and Thieves,June 2010,Brian T Kench,Neil B Niman,,Male,Male,Unknown,Male,"In the standard dictator game (DG), a subject must decide how much, if any, of an endowment to give to another subject.Footnote 1 If the endowment is nominal, say $5, the average decision is to give approximately 20 percent to the other subject with over 50 percent of subjects passing a positive amount of money [Camerer 2003, pp. 57–58]. Henrich et al. [2004] interpret these results as emphatically falsifying the textbook representation of homo economicus because hundreds of experiments have suggested that people care not only about their own material payoffs by also about such things as fairness, equity, and reciprocity.Footnote 2 Levitt and List [2007, p. 154] qualify the claim made by Henrich et al. [2004] by suggesting that human decisions are influenced not just by simple monetary calculations but also by the presence of moral and ethical considerations, the nature and extent to which one's actions are scrutinized by others, the particular context and process by which a decision is embedded, the self-selection of the individuals making the decisions, and the stakes of the game. Levitt and List [2007] offer a model of utility maximization that depends on wealth and an individual's desire to “do the right thing” or make the “moral” choice.Footnote 3 The weight an individual places on moral desires is likely to increase when a subject is being watched, when the decision process is emphasized, and/or when the stakes of the game decrease [Levitt and List 2007, p. 157].Footnote 4 For example, in a small stakes DG, the model in Levitt and List [2007] predicts greater transfers from the dictator to the other subject despite the wealth-maximizing Nash equilibrium predicting that the dictator should not transfer any money to the other subject. List [2007] designed a DG to observe how the weight a subject places on their “moral desires” is altered by changes in DG treatments. First, List [2007] observed in a baseline treatment that after both the dictator and the other subject are endowed with $5 and then the dictator is given the opportunity to allocate another $5, dictators give an average of 26.6 percent ($1.33 of $5). Second, when dictators have the opportunity to give to or take from the other subject, they take an average of 50 percent ($2.48 out of $5). However, when subjects had earned their endowment, by engaging in a 30-min task that included sorting and handling mail solicitations for a charitable fundraiser, they reduced their taking to an average of 20 percent ($1 out of $5). List [2007] concludes that differences in observed behavior between the give to or take from earnings and non-earnings DG treatments are evidence of an exogenous increase in the moral cost of taking since taking earned money is likely met with greater social disdain than taking the experimenter's money [List 2007, p. 490]. Thus, the first set of objectives for the present study are to (1) replicate List's [2007] baseline treatment with subjects earning their endowment, (2) determine whether a decrease in the stakes of the game matter in observed dictator decisions, and (3) conduct, for the first time, a DG in which we endow the dictator and the other subject with $5 and then offer the dictator the opportunity to take (but not give) up to $5 from another subject.Footnote 5 Rather than having subjects sort mail solicitations for a charitable fundraiser for their endowment, we had all of our dictator subjects participate in a 15–25-min online questionnaire. The other subject did not complete the full questionnaire, rather they answered a few demographic questions. All of our dictator subjects were told that by answering the questions that they had “earned” $5. Upon completion of the questionnaire, subjects were placed into one of four online DG treatments: List-Give, Give-Only, Take-Only, or Give-Take. The List-Give treatment is similar to List's baseline DG, except that all of our subjects earned their endowment. In it, the dictator and the other subject are each endowed with $5 and the dictator has the opportunity to allocate another $5 (Figure 1, panel a, bold downward sloping budget line segment). Here, we find that dictators gave an average of 38.4 percent ($1.92 of $5) of the additional $5 to the other subject. We found that our dictators, who earned their endowment, gave 11.8 percent more than the dictators in List's [2007] non-earnings treatment. Moreover, 58 percent of our dictators selected an equal distribution of money, compared to 25 percent of List's [2007] dictators in the baseline treatment. These results are consistent with the model in Levitt and List [2007]. DG budget lines. The next three treatments are different from the List-Give treatment in that the total stakes of the game equal $10 rather than $15. In the Give-Only treatment, the dictator and the other subject are endowed with $5 and the dictator has the opportunity to give away her earnings (Figure 1, panel b, bold budget line segment). Dictators give an average of 13.6 percent ($0.68 of $5) to the other subject in this treatment. Interestingly, as the model in Levitt and List [2007] predicts, the percent of dictators that choose an equal distribution increased from 58 to 76 percent as the stakes changed from $15 to $10. In the Take-Only treatment, the dictator and the other subject are endowed with $5 and the dictator has the opportunity to take the other subject's earnings (Figure 1, panel b, dashed budget line segment). In this treatment, our dictators take an average of 49.8 percent ($2.49 of $5) of the other subject's endowment. 42.3 percent of our dictators choose an equal distribution, which is a lower percentage relative to the List-Give and Give-Only treatments. Finally, in the Give-Take treatment, the dictator and the other subject are endowed with $5 and the dictator has the opportunity to give away her earnings or take the other subject's earnings (Figure 1, panel b, entire budget line). Dictators take an average of 36.8 percent ($1.84 of $5) from the other subject. Three observations result from the Give-Take treatment. First, when the budget constraint is expanded in the Give-Take, relative to the Take-Only treatment, dictators take less. This occurs even though the stakes of the game remain unchanged, dictators are watched in a similar way, the subject pool is unchanged, and the decision process is the same as in other treatments. Second, 44.4 percent of our dictators choose an equal distribution, which is similar to the Take-Only treatment. Third, dictators in our online Give-Take treatment take more than dictators in List's [2007] give to or take from earnings treatment (perhaps because they are not watched in the same way as in the laboratory), but less than dictators in his give or take no-earnings treatment (perhaps because moral costs have increased because the endowment is earned or because the stakes of the game decreased). Our second set of objectives for this paper are to evaluate whether or not (1) ex post perceptions of the experiment and (2) framing correlate with observed dictator decision-making behavior. In the spirit of Frohlich et al. [2001], we conducted an ex post survey of our dictators and found that our experimental design may create doubts in our dictators’ minds regarding (1) the truthfulness of experiment, (2) whether money is actually being transferred or taken from another subject, and (3) the existence of another subject. In addition, many subjects view the experiment as a game in which they were a player trying to win. However, our ex post survey results are similar to Frohlich et al. [2001]. This result engenders confidence that ex post subject perceptions in our online experiment are similar to the ex post subject perceptions in a more controlled laboratory setting. Lastly, contra Brañas-Garza [2007] and Charness and Gneezy [2008], our effort to alter social distance by framing some of our dictators’ instructions, in a single blind experimental setting, did not induce statistically different decision-making. Nor was gender a statistically significant variable in our study. The next section details the experimental design and hypotheses to be tested in this study. Following that, the third section reports the experimental results. The last section offers a discussion of our findings.",1
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2010.26,Introduction to the Symposium on Forensic Economics,June 2010,David Schap,,,Male,Unknown,Unknown,Male,,2
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2010.27,Forensic Economics: An Overview,June 2010,David Schap,,,Male,Unknown,Unknown,Male,"Conceived of as economics applied to legal matters, forensic economics is a broad field indeed. Some applications that could fall under such a rubric, for example antitrust and public utility regulated rate setting, for tradition's sake continue to be classified under other fields within economics. These omissions only slightly reduce the array of issues examined or confronted by practicing forensic economists (FEs). Personal injury and wrongful death are the two types of cases that occupy the most attention of FEs and as such they are given special, detailed attention later in this essay. Here is a list of other types of cases in which FEs participate with some regularity (and parenthetically, some of the specific issues that get addressed):
 Business valuation and lost profits (appraisals cannot be deemed to be “speculative”) Marital dissolution (equitable distribution of joint wealth; the earning power of an advanced degree; future income from a family business) Wrongful termination (duration of unemployment) Failure of fiduciary duty (safety, prudence) Structured settlement appraisal (appropriate discount rate) Pension fund valuation Stock-option valuation Workplace discrimination (prima facie statistical evidence) The list is not exhaustive, and even an exhaustive list would be short-lived as new specialized applications arise from time to time. Unusual cases often require that an FE think creatively as no template may exist to guide them. One such case occurred in the Commonwealth of Massachusetts under statutory law that called for a surviving spouse of a government employee killed in the line of duty (fire fighter, police officer) to receive a sum of money annually for a fixed number of years, after which time a judicial hearing would determine if the surviving spouse were “self-sufficient.” If deemed “self-sufficient,” the annuity would cease entirely and permanently; if not, the annuity was to continue unabated (at least until such time as a subsequent hearing determined that self-sufficiency had been achieved). The meaning of “self-sufficient” was not defined in the statute nor had it previously been interpreted in the courts. Counsel for the insurer issuing the annuity in the particular case retained my services to guide the court as to the meaning of “self-sufficient.” How would an economist determine the meaning of “self-sufficient” in a way that would satisfy the court? After some reflection it occurred to me that the wrong question had been posed. An economist, even an expert one, lacked legitimate authority to settle the question of self-sufficiency. Recalling the holiday film classic, “Miracle on 34th Street,” in which the one “true” Santa Claus was identified by delivery of mail from the United States Post Office Department (now Postal Service), an official branch of government and thus authoritative, I resolved to provide official (and thus authoritative) government statistics to the court that would assist the court itself in settling the matter. At one end of the relevant financial spectrum one finds statistics concerning the official poverty level, below which a person cannot be meaningfully termed “self-sufficient” by the government's own standards. At the other end one finds a benchmark by which the government itself requires an individual to give proportionately more back to the system, namely the income level that triggers a move from the 15 percent marginal tax bracket to the next higher (at that time 28 percent) tax bracket, presumptive evidence by government standards that self-sufficiency has been achieved. The anecdote illustrates that forensic economics is more than mere number crunching, but it also points up the very limited role of an FE in any court proceeding. One accomplished FE, Gerald Martin, has quipped on numerous occasions that FEs are mere spear-carriers at the opera. The remainder of this essay concerns cases of personal injury and wrongful death exclusively. A recently published survey of members of the National Association of Forensic Economics (NAFE) reports that roughly two-thirds of the cases in which survey respondents were engaged were of the personal injury or wrongful death type; and a majority of survey respondents indicated that at least 80 percent of their earnings as FEs derived from these two types of cases alone [Brookshire et al. 2009, p. 31]. In what follows, the key elements in personal injury type cases are described (with certain aspects of wrongful death type cases addressed parenthetically). The review is far from nuanced: a quick read of the material will not make an expert. Those yearning for more detail in any area should consult Martin [2009] to start. Classic articles in forensic economics may be found in Kaufman et al. [2006] and all previously published articles from the Journal of Forensic Economics are available to those who become members of NAFE at its website (www.nafe.net).",3
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2009.52,Cohort Effects in Age-Earnings Profiles for Women: Implications for Forensic Analysis,June 2010,Matthew J Cushing,David I Rosenbaum,,Male,Male,Unknown,Male,"Forensic economists are frequently asked to value the economic loss when someone is injured or killed in an accident. In a personal injury case, a large component of that loss can be lost earning capacity. In a wrongful death case, it is generally lost contribution to the family. Typically, predicting economic losses in either case involves estimating future earnings. For a younger person, however, there may be little or no employment record on which to base an estimate of lost earning capacity or economic contribution. Consequently, a forensic economist may have to rely upon broader Bureau of Labor Statistics data to estimate an age-earnings profile that maps expected earnings over a person's lifetime. The traditional method for developing an age-earnings profile is to use a cross-section from one year of average or median earnings for individuals in a variety of age groups. For example, 2007 Current Population Survey (CPS) data might be used to look at earnings of women aged 25 to 29, 30 to 34 and so on in 2007. The earnings from each age group would be graphed against age to create an age-earnings profile. Projected earnings would be adjusted for expected wage inflation and real earnings growth and appropriately discounted to arrive at the present value of the economic loss. This cross-sectional method captures age effects: older workers tend to have higher earnings. Using cross-sectional data to estimate lifecycle earnings will miss significant cohort effects if younger generations will have fundamentally different labor market experiences than older generations. These cohort effects may be especially pronounced for women. Recent cohorts of women entering the job market tend to be better educated and have greater access to higher paying occupations than the cohorts that entered the job market before them. More extensive education and greater access mean they are likely to experience higher average salaries than the cohorts of women preceding them. One consequence will be that the traditional practice of projecting lifecycle earnings based on the current cross-sectional age-earnings profile, which ignores these potentially important cohort education and access effects, will underestimate future earnings for young women. The relative earnings of women who are 55 in 2007 may be a very poor guide to the earnings that women currently in their twenties will have when they eventually turn 55. In this article we attempt to quantify these underestimates. We focus on relatively straightforward empirical methods that are suitable for use in a forensic setting. We examine these cohort effects from three different perspectives. We first examine the impact that educational attainment has had on women's earnings. The traditional approach, in effect, assumes that when the cohort of women just entering the labor market eventually turns 50, the education levels of that cohort will be the same as the education levels of women who are currently age 50. More likely, the level of education that will be attained by these young women will be considerably higher than those of past cohorts. We account for this by predicting eventual educational attainment and earnings by education for the cohort of women just entering the labor market and using those predictions to form a cohort-adjusted age-earnings profile. Our results show that the projected age-earnings profile is well above the profile traditionally developed using cross-sectional estimates based on age alone. For women aged 45 to 64, cohort-adjusted earnings are from 8 to 12 percent above the traditional cross-sectional estimates. We next examine the impact that greater access to the job market has had on women's earnings. The traditional approach, in effect, assumes that when the cohort of women just entering the labor market eventually turns 50, the occupation mix of that cohort will be the same as the occupation mix of workers who are currently age 50. However, the projected occupation mix of these young women differs from those of past cohorts. There has been a trend towards women entering higher paid occupations. Adjusting for projected occupational mix leads to a 5 to 10 percent upward shift in the age-earnings profile. In the first two subsections we focused on the impacts that education and then access have on earnings. These two are undoubtedly inter-related. There are also other factors that will affect younger women's potential for lifetime earnings. In the third empirical subsection, we examine overall cohort effects using time series analysis. To do this, we rely on data showing the gap between male and female earnings. Historical data show that for any age group, the gap has been closing over time as women of any particular age in more recent cohorts are earning at rates closer to men than women at that same age in less recent cohorts. Our analysis forecasts wage gaps and shows that for women entering the job market in 2007, expected gaps are smaller when cohort effects are accounted for. Using traditional cross-sectional analyses, women aged 25 to 29 in 2007 would expect to earn 77 percent of male earnings by the time they reach the age 50–54. In contrast, once cohort effects are accounted for, the same women would earn almost 85 percent of their male counterparts’ earnings. Using the traditional cross-sectional approach underestimates women's earnings by more than 10 percent. The next section discusses recent literature related to age-earnings profiles. It points to several factors that have allowed women's labor market experiences to evolve. In the subsequent section, we empirically examine the implications of this evolution. There are three subsections looking at the implications of education, age, and overall cohort effects. The analysis is followed by a conclusion in the last section.",
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2010.25,The Effect of the Loss of a Parent on the Future Earnings of a Minor Child,June 2010,John Kane,Lawrence M Spizman,Rick R Gaskins,Male,Male,Male,Male,"Forensic economists are often called upon to estimate the loss of services, guidance, and income to minor children resulting from the personal injury or death of a parent. A methodology and data sources exist for valuing these losses.Footnote 1 Receiving much less attention in the forensic literature is the effect of the loss of a parent on the future earnings of a minor child. No doubt a child's future loss of earnings due to the absence of a parent traditionally has not been an element of damages in cases involving the wrongful death of a parent and therefore there has not been research published in the forensic literature on this topic.Footnote 2 Should it be one of the elements of damage? If a parent's death has a large negative effect on a child's future earnings, it could be argued that good public policy would require such a loss to be added to the list of damages that can be claimed in such cases. A forensic economist may also be interested in the effect of the absence of a parent on educational attainment when projecting lost earnings capacity in the case of wrongful injury to a child. Spizman-Kane [1992] used an ordered probit model to estimate the probability of a child attaining different educational levels based on family background characteristics and subsequently to estimate lifetime earnings of the child. Kane-Spizman [2001] extended their earlier study and showed that educational attainment and lifetime earnings were predicted to be higher when both biological parents were present in the household when the child was 14 years old. However, Kane-Spizman did not distinguish between the reason for the absence of a parent (i.e., death or other causes such as divorce or separation). This paper examines how the absence of a biological parent affects the future earnings of a minor child. In ‘Studies of the effect of the loss of a parent on a child's future earnings’ section, we review literature in economics and psychology that directly or indirectly bears on the impact a parent's loss has on a child's future educational attainment and earnings. In ‘Assessing the effect of losing a parent via the impact on educational attainment’ section, we examine one approach to quantifying the effect of the loss of a parent on a child's future earnings by expanding the model that has been proposed by Kane and Spizman [2001] for estimating the educational attainment of a minor child, based on the child's personal and family characteristics and the socio-economic circumstances of the child's parents. The data used to estimate this model are discussed in ‘Data’ section. Empirical results are presented in ‘Educational attainment probabilities’ section. Following this quantitative exercise, in ‘Problems of interpretation’ section we discuss some problems and issues that arise with the approach described in ‘Assessing the effect of losing a parent via the impact on educational attainment’ section. ‘The effect of absence of a parent on a child's lifetime earnings’ section provides estimates of lifetime earnings for different family demographics. ‘Conclusion’ section presents our main conclusions. Our results suggest that the absence of a biological parent for reasons other than death reduces a child's lifetime earnings between 3 and 12 percent. The loss of a parent to death, however, has an effect that appears to vary by the gender of the child. Lifetime educational attainment is adversely affected if a parent of the same gender as the child dies, and the effect on lifetime earnings is between 2 and 4 percent, which is always smaller than if the absence is for reasons other than death. Lifetime educational attainment and earnings appear not to be adversely affected by the death of the father of a female child or the death of the mother of a male child.",
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2010.24,The Pecuniary Value of Commuting Time,June 2010,Edward Foster,,,Male,Unknown,Unknown,Male,"Forensic economists (FEs) are generally limited by judicial rulings to offering evidence on pecuniary damages to a plaintiff caused by some claimed wrongful act of a defendant: damages that can be measured directly in money or for which monetary equivalents are readily estimated; in legal proceedings these pecuniary damages are sometimes called economic damages. FEs are rarely allowed to translate loss of utility into financial terms by using such welfare measures as the compensating or equivalent variation in income. For example, an FE may testify to financial loss because of a physical injury that requires a plaintiff to move to a lower paying job, but not to the loss in utility from the unpleasant working conditions that the new job imposes. A lawyer may argue that the deterioration in working conditions is part of his client's damages, but those damages are “non-pecuniary” or “non-economic,” outside the scope for economic testimony, whether or not the economist agrees. This is not to say that only the direct financial consequences of an injury are considered to be pecuniary; “pecuniary” damages have been stretched to include the value of services for which no monetary payment is made. For example, if an injury or death means that a father is no longer capable of providing household services for his family, in most US states the FE can estimate financial measures for that loss, generally based on the market value of similar services, or on the opportunity cost of the person's time. Injury can occasionally lead to significant changes in time spent commuting. This paper argues that if permitted to testify on the subject, the FE should use well-established results from transportation economics to measure the value of travel time, rather than the measures that they typically use in valuing household services [Expectancy Data 2009].",
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2010.29,The Practice of Forensic Economics: An Introduction,June 2010,Frank D Tinari,,,Male,Unknown,Unknown,Male,"Over the course of the past four decades, economists increasingly have been called upon to offer their expert opinion regarding the valuation of economic losses related to specific events such as auto accidents, employment termination, breach of contract to name a few among numerous potential claims.Footnote 1 This work has developed into the applied field that has come to be known as forensic economics. Its practice by economists invokes a multi-layered set of skills including (1) competency in economics, (2) effective oral and written communication skills, (3) a reasonable level of familiarity with relevant law, (4) acumen in operating one's practice as a small business, (5) the ability to withstand harsh criticism under oath, and (6) skillful management of interpersonal professional relationships. The purpose of this essay is to provide an overview of the practice of forensic economics, touching upon some of the key cited skills. My goal is that readers will come away with a succinct yet full understanding of the practice of forensic economics.",4
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2010.28,Legal Requirements in the Practice of Forensic Economics in the Northeast States,June 2010,David Schap,,,Male,Unknown,Unknown,Male,"Statutory law and case law affect many aspects of economic damages awarded in cases of personal injury. Two salient aspects are considered here, namely the rules governing the task of discounting to present value and the rules regarding the treatment of taxes. This study explores the legal requirements of state jurisdictions exclusively for the nine northeast states (CT, ME, MA, NH, NJ, NY, PA, RI, VT). The results presented constitute a subset of the results being developed in an ongoing study covering all 50 states in the US. In a personal injury case, a court award of money damages typically takes the form of a lump-sum payment designed to be equivalent in value to the stream of future losses induced by the injury. Discounting future streams to present value thus becomes of primary importance to the practice of forensic economics. To be competently accomplished, loss estimates calculated by forensic economists (FEs) must conform to the dictates of state law concerning the treatment of taxes, which vary widely by jurisdiction among the nine northeast states. The focus herein is on cases involving personal injury. The Journal of Forensic Economics (JFE) has adhered to a focus on personal injury and wrongful death type cases in published studies that concern controlling law and customary forensic economics practice in the states as part of an ongoing state series (with about 20 individual states featured already in published articles). The JFE state series was in fact one source relied on heavily in the survey of state law. The methodology employed called for amassing all studies in the state series published before 2009 to which were joined certain of those papers in the state series nearing completion. About three quarters of these studies were reviewed in detail and the information contained in them with respect to the key areas of law was recorded. That first logical step produced (1) results for the particular states involved as well as (2) a set of key words (e.g., economic damages, present value, discount, discounting, interest, etc.) that could be subsequently applied in a Lexis-Nexis search of case and statutory law and in state website searches of current statutory law for the remaining states. After completing the keyword search of the law in the remaining states and recording the specific results of those searches, the articles and papers concerning the remainder (one quarter) of the states in the JFE state series were examined to assure that the keyword search as applied to the remainder of the states had been in fact sufficient to uncover the relevant statutory and case law in those jurisdictions. The search technique proved up to the task inasmuch as only one discrepancy was found, and it concerned an undetected ordinary FE practice in one state, but not one at odds with the surveyed law. Tabled findings that would be of interest to practicing FEs have been prepared concerning the detailed findings for each of the northeast states [Schap 2010], where greater documentation of the particular points made in this essay may be found. In this short introduction to the laws affecting the practice of forensic economics the findings are summarized for a general audience of economists to demonstrate that there is significant variation in legal requirements across the states, even when only the northeast states are considered. FEs can only competently perform the functions required of them if their techniques are in synch with the legal dictates of the jurisdiction(s) in which they practice.",
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2009.21,The Global Diffusion of Markets and Democracy,June 2010,Nicole Bissessar,,,Female,Unknown,Unknown,Female,,
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2009.22,Who Really Made Your Car? Restructuring and Geographic Change in the Auto Industry,June 2010,Fatma Abdel-Raouf,,,Female,Unknown,Unknown,Female,,
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2009.23,Falling Behind: Explaining the Development Gap Between Latin America and the United States,June 2010,Miguel D Ramirez,,,Male,Unknown,Unknown,Male,,
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2009.35,The Subprime Solution,June 2010,Oscar T Brookins,,,Male,Unknown,Unknown,Male,,
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2009.33,The Failed Welfare Revolution: America's Struggle over Guaranteed Income Policy,June 2010,Richard K Caputo,,,Male,Unknown,Unknown,Male,,1
36,3,Eastern Economic Journal,30 June 2010,https://link.springer.com/article/10.1057/eej.2009.36,Economic Development in India and China: New Perspectives on Progress and Change,June 2010,Richard Grabowski,,,Male,Unknown,Unknown,Male,,
36,4,Eastern Economic Journal,22 September 2010,https://link.springer.com/article/10.1057/eej.2010.20,"Lineages of Crisis Economics from the 1930s: Keynes, Hayek, and Schumpeter",September 2010,Duncan K Foley,,,Male,Unknown,Unknown,Male,"Keynes' adult life spanned a period of wrenching crisis for the world capitalist system centered in Europe.Footnote 1 The themes of Keynes' major work were shaped by this distinctive period in capitalist development. Before the economic and financial crisis of 2008, it appeared that the inter-war turmoil was an unusual break in the pattern of capitalist economic development, but from our current perspective it looks, as it did to people at the time, as if these problems are inherent in capitalism and will recur. To contemporary observers in the 1930s, laissez-faire policy seemed to be unable to cope with the problems of advanced industrial capitalism. Without the anchor of gold, speculation in currencies produced pressures for inflation or deflation in national economies which destabilized them politically and led to chronic unemployment. The forces leading to classical equilibrium appeared to be weak or inoperative for much of this period. Many people during this crisis argued that socialism along the communist model of the Soviet Union was the only workable alternative. Keynes was a strong critic of central-planning socialism, and aimed rather at reforming capitalism to make it function better through a great expansion of the economic role of national governments and central banks. Keynes argues that Say's Law is irrelevant to economies with a highly developed financial system. When there are numerous and varied financial instruments, the sale of one commodity may be separated by a long and variable period from the purchase of another. If the lag between sale and purchase lengthens, there may be insufficient monetary demand to buy back all the commodities produced and offered for sale on the market. In this case some firms and households are “liquidity constrained,” rationing their purchases of commodities because they simply do not have the financial resources to buy. Under these circumstances, the decision to spend money has a positive externality for the economic system as a whole. The spender has the private advantage of purchasing the commodity she wants, but she also increases the money balances of another agent, which permits that agent to make a desired purchase that was previously impossible because of financial constraints. Because individual spenders do not take into account the external impact of their decisions, the volume of spending may be too small to employ all the resources of the economy, and there is a case for government intervention to subsidize spending (or spend itself) to make up the difference. In the 1940s, when Keynes' ideas were coming to dominate economic theory in the United States and Britain, this problem was addressed by a compromise, enunciated by Paul Samuelson, among others, the “neoclassical synthesis.” The neoclassical synthesis held, in agreement with Keynes, that free markets cannot guarantee the full employment of productive resources (or at least not very fast), so that governments and central banks have to adjust fiscal and monetary policy to ensure full, or close to, full employment. Once full employment demand has been achieved, however, the basic force of the laissez-faire analysis comes back into play, and markets should be largely free to allocate resources without further government intervention. Ingenious as it was, the idea of the neoclassical synthesis proved to be unstable ideologically. In the 1970s monetarist and rational expectations theory in economics insisted on the need to return to the full Classical and neoclassical orthodoxy, including the assumption of Say's Law, and, to a considerable extent, they captured the high ground in economic theory (if not in economic policy). Many of our current dilemmas stem from the evolution of the neoclassical synthesis into a dogmatic belief that an inflation-targeting monetary policy could be counted on to eliminate the problem of aggregate demand management. In the marginalist conception of equilibrium in the labor market, firms hire workers up to the point where the wage equals the value of the goods one more worker can produce (the marginal product of labor). Workers in turn supply labor up to the point where the marginal utility of the goods and services the wage can buy is equal to the negative marginal utility of working an extra hour (the marginal disutility of labor). Equilibrium in the labor market is defined by the equality of the marginal product and the marginal disutility of labor. Marginalists can conceive of the labor market being in disequilibrium. But they believe that at a disequilibrium with excess supply of labor there are forces tending to lower the real wage. Keynes argues that the only way workers could respond to the excess supply of labor would be by cutting the money wage, since actual wage bargains are made in terms of money, not real goods and services. Keynes agrees that there might be a sharp fall in money wages in the presence of involuntary unemployment, though he does not think this is a good thing for the economy by any means. (In the early 1930s when unemployment was very high, money wages in the US did drop rapidly.) He points out, however, that cuts in money wages cannot bring about a fall in the real wage, because money wages are such a large part of the costs of production. As money wages fall in the economy, all producers find their costs lowered, and competition will force them to lower the money prices of goods and services in proportion. This keeps the real wage constant, and leaves the economy with involuntary unemployment. Keynes argues persuasively that a downward spiral of money wages and money prices is the last thing an economy suffering from substantial unemployment needs. The deflation of money prices and wages increases the real interest rate and burden of servicing existing debts, and thus may discourage businesses from undertaking new investment, thereby making the liquidity constraints in the economy even more severe. Keynes' stated position on unemployment and money wages makes clear how far his thinking was from the versions of what Joan Robinson called “Bastard Keynesianism” that interpret it as neoclassical economics under the assumption of “sticky” money wages. Neoclassical economics argues that the evaluation and allocation of risk is the function of freely operating asset markets. The paradigmatic case of market allocation of risks for neoclassical economics is insurance. Financial theory tends to treat all risk as being of a statistically predictable character, and sees the continuing development of financial markets as the best way for the economy to cope with risk. Keynes emphasized the difference, also noted by other economists such as Frank Knight, between calculable and therefore insurable risks, and unresolvable uncertainties about which we can form no coherent statistical opinion. He argues that while financial assets and markets can allocate insurable risks, the more important economic risks are unresolvable uncertainties which financial markets may in fact make worse. The problem is that macroeconomic uncertainty is largely generated within the economic system, unlike the risk of individual death or fire. For example, the risk that an economy will plunge into recession does not arise from uncertainty about external factors like weather, but from uncertainty about the interaction of capitalist expectations. If everyone comes to believe that a recession is imminent, they will reduce their investment expenditure and production levels, thereby reducing incomes and making the expectations come true in a self-fulfilling way. Since the recession is not the manifestation of calculable risks, but of essentially incalculable dynamic interactions of human beings, asset markets cannot allocate or hedge this type of risk. Furthermore, Keynes sees considerable danger in entrusting the allocation of investment entirely to financial markets. When risks are incalculable there is no rational basis on which to value assets, and market valuations can swing wildly with fashion, herd instincts, and panic, destabilizing investment and the real economy in the process. Keynes argues that in this situation the financial markets are like a type of beauty contest run by British newspapers in which the aim is not to choose which entrant is most attractive, but which one will get the most votes from the public. To guard against the instability of financial markets Keynes recommends a “somewhat comprehensive socialization of investment,” assigning to the political process the role of economic balance wheel in relation to financial markets. The heart of the capitalist system, in Keynes' vision, is the willingness of wealth-holders to speculate on the profitability of the future by making long-term investments. Keynes worried that this speculation depends on a fragile and unstable psychology of investors, who are prone to a kind of manic-depression syndrome, oscillating between extreme optimism about the future, which leads to high investment and a self-fulfilling boom in aggregate demand and employment, and extreme pessimism, leading to low investment, and a self-fulfilling depression of aggregate demand and employment. Keynesians refer to this psychological element in the formation of long-term expectations as the “animal spirits” of capitalists. Keynes thought that the solution to the inherent instability of long-term expectation was for the government to adopt fiscal and monetary policies which would stabilize aggregate demand, and to take on a much larger share of the total investment of the economy. This would reduce the anxiety of investors about the possibility of catastrophic depressions. In the years since the Second World War, advanced capitalist economies have indeed worked on both these strategies. Government spending and taxation now represent one-third to one-half of GDP in most advanced capitalist countries. As a result the effects of liquidity constraints are sharply reduced, since a fall of output and incomes in recession throws government budgets into deficit, and maintains spending streams. At the same time there has been an explosive growth of financial markets and in the spectrum of available financial instruments, which presumably strengthens the ability of wealth-holders to hedge risks and form a more consistent view of the future path of the economy. But the economic future is not predetermined nor completely predictable, so that it is unlikely that futures markets can completely eliminate the instabilities of expectation Keynes identified. Governments may be no better than markets at predicting the future, but the collective action of society can stabilize some of the key boundary conditions in which capitalist investment takes place, and thus strengthen the animal spirits on which the system rests. In retrospect it is clearer that Keynes' thought centered on what recent economics calls “social coordination” problems, when external effects of privately motivated decisions lead to powerful but often unwelcome social outcomes. Keynes did not come up with a memorable phrase to encapsulate this general idea, but it is clear enough in his discussion of key topics. The “beauty contest” metaphor for financial markets is one example. The issue here is that the asset-holding and pricing decisions of individual wealth-holders depend in an important way on their assessment of the average decisions of all other wealth-holders. This type of interdependence leads to an externality and a social coordination problem, which in turn can imply multiple equilibria and instability. A second important example of a social coordination problem in Keynes' thought is the consumption function and multiplier analysis, which, as I described above, arises when liquidity constraints in the economy as a whole mean that the decision of each agent to spend has an externality in improving the liquidity of other agents. A third is apparent in Keynes' speculation that workers may be more interested in relative money wages than in real wages. This side of Keynes' thought is worth more attention than it has received theoretically, certainly at least as much attention as the dubious hypothesis of sticky wages and prices. From the perspective of post-2008 crisis capitalism, it is also clear that Keynes was a precursor of what we now call “behavioral economics,” not surprisingly, since the crucible of behavioral economics is the wide range of anomalous behavior we observe in financial markets. The same wealth-holders whose attempts to guess each other's behavior creates a social coordination problem in financial markets are deviating significantly from the rational ideal of homo economicus. Keynes had puzzled over these types of behavior in his own speculative encounters with financial markets. They were empirical facts in his experience, not abstract hypotheses. The presence of these elements of social coordination failure and behavioral anomalies in Keynes' thinking is presumably one reason why his work has been “rediscovered” by such one-time supporters of market solutions as Richard Posner in the aftermath of the current crisis.",3
36,4,Eastern Economic Journal,22 September 2010,https://link.springer.com/article/10.1057/eej.2009.27,Environmental Tobacco Smoke and Children's Health,September 2010,James M Williamson,,,Male,Unknown,Unknown,Male,"The research examining environmental tobacco smoke (ETS) exposure and its associated health effects on children is truly vast. Since the mid-1970s, the issue of ETS exposure has spawned hundreds of studies on the topic. The studies range from small samples involving less than 100 children to nationally representative epidemiological studies with thousands of subjects. Investigators have established a strong link between exposure to ETS and poor health, and the research for the most part has focused on the effects on the respiratory system, primarily morbidity, or continuous lung function [Klonoff-Cohen et al. 1995; California Environmental Protection Agency 1997; Adair-Bischoff and Sauve 1998; Aligne et al. 2003; Chapman et al. 2003]. As a result of the extensive body of literature, it is widely recognized that ETS exposure in early life contributes significantly to childhood morbidity. Today, the exposure of children to ETS is a particularly important public health issue because exposure is generally preventable, and although laws exist to regulate ETS in workplaces and public areas, the major avenues of exposure for children, private dwellings, and vehicles are unregulated (as of the date of this writing the legislators of California, Arkansas, New Jersey, and Louisiana have introduced laws that prohibit smoking in cars when children are present). As evidence consider that recent efforts to reduce ETS in public spaces have had a significant effect on adults, but children have not benefited to the extent that adults have. Researchers report that the level of a biomarker for ETS exposure, serum cotinine,Footnote 1 has declined by 17 percentage points more in non-smoking adults than in children over a 10-year period in the United States [Stephenson 2003]. Consequently, although adults continue to receive relatively higher average levels of exposure to ETS, children's exposure has failed to decline as rapidly. Despite widespread agreement in the literature that ETS contributes to lung function deficits, an important gap in the research has yet to be addressed. The gap is the non-random nature of ETS exposure that is due to heterogeneity between smoking and non-smoking parents and the potential that children may be exposed based on their present respiratory health, or selectively exposed. An association between lung function deficit and ETS is well established by a large medical literature; however, a majority of past ETS research was conducted using observational data — data that are non-experimental by design, and therefore pose a particular challenge to human exposure research. Unlike exposure experiments that can be controlled in laboratories involving animals, for example, the exposure in observational studies cannot usually be treated as random.Footnote 2 Exposure to a particular toxin may be correlated with other factors associated with the exposed person. An example is a child who has an existing respiratory illness. If a parent takes precautions against exposing their child to ETS because of the awareness that exposure exacerbates the child's symptoms, selection will be introduced into the lung production function. Consequently, underlying or unobserved factors that might influence exposure may also influence the outcome of interest, leading to bias in LS estimates. In this research, I will also estimate the impact of ETS exposure on the lung function of children, but I will address the issue posed by observational data. Because exposure is not random, I will explore the endogenous nature of exposure. In the face of this issue, I use an instrumental variables (IVs) approach to estimate the effect of ETS on lung function. I argue that an important policy instrument, cigarette taxes, can be used as an instrument for exposure because such taxes are plausibly exogenous to children's lung function and past research has shown smokers to be sensitive to prices. I will test the sensitivity of exposure to the instrument and the hypothesis that two sources of bias, unobserved heterogeneity and selection, lead to bias in LS estimates. To handle the intensive data needs, I exploit detailed individual health data from the Third National Health and Nutrition Examination Survey (NHANES III) to measure the effect of ETS exposure on the continuous lung function of adolescent children. I then combine NHANES III data with state-level cigarette tax data and use within-state variation in cigarette taxes over time to predict ETS exposure in children. My article makes several important innovations to the literature. To my knowledge, this is the first article to exploit variation in cigarette taxes to estimate ETS exposure levels in children. Second, unlike much of the previous ETS research, I use a highly accurate biomarker of exposure, serum cotinine, and therefore forgoing the need for self- or proxy-reported ETS exposure. Third, by exploiting a plausibly exogenous source of variation in ETS exposure, state cigarette excise taxes, I account for endogenous exposure and produce consistent estimates of the effect of exposure on children's lung function. Further, the use of state-level data allows me to account for time trends and heterogeneity across states. The results of my study contribute important new information to the literature in human health exposure. I provide the first estimates of ETS exposure's elasticity with respect to taxes. Policy makers weighing the option to raise cigarette taxes may face stigma, and competing exposure abatement options, such as tobacco control programs, are costly. With accurate elasticity figures and evidence of the effect of exposure on lung health, policy makers may be able to make better-informed decisions regarding the trade-offs involved in these policy decisions.",2
36,4,Eastern Economic Journal,22 September 2010,https://link.springer.com/article/10.1057/eej.2009.39,What Can We Learn from Education Production Studies?,September 2010,Ellis Anthon Eff,Christopher C Klein,,Male,Male,Unknown,Male,"Failure to measure all outputs of the educational process and to take account of the endogeneity of school outputs and expenditures seriously undermines the integrity of efficiency measures based on estimated cost or production frontiers, or related reduced forms. These problems have been noted in the literature [Hanushek 1979, 1986], yet few studies examine the effect on efficiency measures of variations in the number of outputs observed. Meanwhile, the flow of research utilizing these techniques continues [Ruggiero and Vitaliano 1999; Hoxby 2000; Chakraborty et al. 2001; Grosskopf et al. 2001; Abbott and Doucouliagos 2003; Bonesronning 2003; Dolton et al. 2003; Ouellette and Vierstraete 2005; Ruggiero 2007; Waldo 2007].Footnote 1 This article examines the theoretical and empirical effects of including or excluding outputs on the measured efficiency of public school districts. These concerns are different from similar issues raised elsewhere. Pritchett and Filmer [1999] consider production distortions that may arise through teachers’ influence on input usage, ignoring output choices, while Wenger [2000] argues for examining multiple educational outputs, but does not derive the full implications of failure to do so. Ruggiero [2003] and Bifulco and Bretschneider [2003] investigate the econometric effect of measuring one observed output with error; here multiple outputs are considered, some observed while others are not. The Data Envelopment Analysis (DEA) literature contains a few studies examining the effect of omitted inputs on efficiency scores. Using Monte Carlo experiments, Galagedera and Silvapulle [2003, p. 659] find that efficiency scores are more seriously distorted when omitting relevant inputs than when including irrelevant inputs. Other Monte Carlo studies show that omitting an input vector that is highly collinear with retained inputs has less effect than omitting an orthogonal input vector [Pedraja-Chaparro et al. 1999; Saen et al. 2005]. The DEA literature has not examined the effect of omitted outputs. The purpose of this article is to test the implications of a model of public educational funding and production that explicitly accounts for failure to measure all educational outputs. A Becker–Peltzman–Stigler model [Stigler 1971; Peltzman 1976; Becker 1983] of the political choice of two educational outputs subject to a budget constraint is analyzed. A single socioeconomic status index is introduced to shift the demand for education, to increase willingness to pay in the form of the schools budget, and to reduce the cost of educational attainment. To examine the limiting case, efficient production is assumed and only one output is observed, although the results generalize to the case in which only a subset of multiple outputs are observed. The model generates observations consistent with the empirical literature in which variations in students’ household socioeconomic characteristics appear to cause variations in educational productivity across institutions. It suggests that efficiency measures may behave inconsistently as variations in the number of observed outputs occur, as different school districts choose different mixtures of outputs. The implications of the model are tested by examining public school data for 95 Tennessee county school districts in 1999–2000. These data contain a large number of output measures, including test scores, attendance and graduation rates, and other measures of school performance. Out of 20 initial output measures, six output variables are constructed by consolidating highly collinear measures. Six additional output variables for art and music instruction, vocational training, special education, health and guidance services, transportation and racial/socioeconomic integration are constructed. Per pupil operating expenditure is the sole input. DEA efficiency scores and efficiency rankings are calculated for these county school districts as the number of outputs increases. The results confirm that DEA efficiency scores and rankings change in unpredictable ways as the number of observed outputs increases. Thus, the relative efficiency of educational institutions in general cannot be inferred from such studies, unless one is confident that all outputs are measured. Moreover, due to a previously unrecognized artifact of the DEA process, simply adding outputs to a traditional DEA model leads to efficiency measures highly correlated with the inverse of per pupil expenditure. Rather than giving high-spending districts credit for the outputs they produce, adding new outputs is likely to exaggerate their inefficiency. Attempting to correct for variations in output demand in a second-stage tobit regression, however, without appropriate measures of the local forces shaping that demand, is likely to be incomplete or misleading. The next section presents the theoretical model, followed by a section describing the data. Then the DEA efficiency scores and rankings are presented and analyzed. A conclusion suggests policy implications and directions for future research.",1
36,4,Eastern Economic Journal,22 September 2010,https://link.springer.com/article/10.1057/eej.2009.38,Hate Fuel: On the Relationship Between Local Government Policy and Hate Group Activity,September 2010,Sean E Mulholland,,,Male,Unknown,Unknown,Male,"The Southern Poverty Law Center (SPLC), an authority on hate group activity, declared that the number of active hate group chapters in the United States increased from 474 in 1997 to 888 in 2007.Footnote 1 Some types of hate groups have experienced a decline in the number of active chapters, while others have experienced an increase. Figure 1 depicts the slow and steady increase in the total number of active Ku Klux Klan (KKK), neo-Nazi, and Racist Skinhead chapters and Christian Identity Churches across the United States. Although previous studies have focused on economic conditions, demographics, and history as reasons for hate group activity, few have addressed the possible link between government provided services and the choice to form or join a hate group.Footnote 2 Number of Ku Klux Klan, neo-Nazi, Skinhead, and Christian identity chapters in the US: 1997–2007. Hate groups share many characteristics with social clubs. Members must sacrifice private consumption to be eligible for goods and services produced by the club. While it is clear that those seeking to join such groups often do so for ideological reasons, hate group membership provides services that may substitute for various government services. Therefore, increases in the quality or quantity of government-provided services may lower the marginal benefit of participating in an active hate group. However, government supplied goods and services may also serve to sustain and encourage hate group activity by reducing the negative wage effect of signaling membership. This paper seeks to determine if changes in local government policy are associated with changes in the presence of hate group activity. Using county-level panel data from the United States for 2002 and 2007 and controlling for unobserved county-level time-invariant heterogeneity, I show that active hate groups are more likely to be present when the percent of households below the poverty line increases. Attempting to reduce the impact of poverty through welfare payments does not, however, appear to reduce the likelihood that a hate group is present. In fact, it appears to increases it. The following section discusses past research on the general dynamics of group formation and on hate groups and hate crime. The next section discusses possible reasons why individuals join hate groups. The subsequent section presents general facts about the number of hate groups across the counties of the United States. The section after that discusses the fixed effects logistic estimation methodology in more detail. The estimation results are introduced and discussed in the penultimate section. Conclusions and possible extensions are presented in the final section.",6
36,4,Eastern Economic Journal,22 September 2010,https://link.springer.com/article/10.1057/eej.2009.28,The Influence of a Change in Immigration Law on US Marriage Rates,September 2010,Claudia Smith Kelly,,,Female,Unknown,Unknown,Female,"Gary Becker's marriage model suggests that individuals choose to marry when the utility associated with being married exceeds the utility when single [Becker 1973; 1974]. Empirical evidence on this theory confirms that individuals respond to economic incentives when considering the decisions of whether to marry and when to marry [Alm et al. 1999; Brien et al. 2004]. Most of the literature in this area, including Alm and Whittington [1995], Dickert-Conlin and Houser [2002], and Sjoquist and Walker [1995] focused on the income tax system or welfare programs, and use yearly time-series variation. An exception is Brien et al. [2004]. The authors examined an age restriction for remarriage in the US social security system and found that individuals respond to economic incentives for marriage. Like changes in the income tax system, welfare programs, and social security rules changes in immigration laws also affect marriage patterns. Jasso and Rosenzweig [1990] documented the correlation between immigration law changes and the number of immigrant spouses in the United States, with emphasis on the imposition of numerical limits on the availability of immigrant visas. In this article, I focus on whether an immigration law change affected the marriage behavior of undocumented immigrants residing in the United States. Unlike previous studies where the duration of the legislation changes examined is at least 1 year [Alm and Whittington 1995; Sjoquist and Walker 1995; and Dickert-Conlin and Houser 2002], I examine a change in legislation (a change in an immigration law), which was in effect for a shorter period of time (4.4 months) at the beginning of the year. Therefore, marriages in this shorter period of time were less likely to be correlated with trends and other policy changes throughout the year, such as any change in the income tax system or welfare programs that create a marriage tax or subsidy. Previous research has also examined the relationship between immigration law changes and marriages [Jasso and Rosenzweig 1990], but has not narrowed the population to undocumented immigrants residing in the United States. In sum, the analysis is new in that it focuses specifically on undocumented immigrants and examines a legislation change that was in effect for a short period of time. On December 21, 2000, the Legal Immigration and Family Equity Act of 2000 (henceforth referred to as the LIFE Act) was put into effect. This law provided a small window of opportunity, from December 21, 2000 to April 30, 2001, for certain qualified undocumented immigrants to complete the process of becoming legal immigrants without having to leave the United States. Before this law change, undocumented immigrants who were ineligible for completing the process of becoming legal residents in the United States had to go to an American consulate in their home countries to complete the process. Returning to their home countries is costly because most undocumented immigrants have acquired unlawful presence in the United States, and if they leave at anytime, they are barred from re-entering the United States for anywhere from 3 to 10 years. This has dissuaded many undocumented immigrants from obtaining legal status. They accept the risk of remaining in the United States illegally rather than possibly being separated from their family, friends, and employment for years. Therefore, for many qualified undocumented immigrants, the LIFE Act reduced the cost of obtaining legal status by not requiring them to return to their home countries to later encounter the entry restrictions. Marriage to US citizens or legal permanent residents is one way undocumented immigrants can qualify for the immigration benefit provided by the LIFE Act. Therefore, the LIFE Act should influence marital patterns. The identification in this empirical analysis comes from comparing marriage patterns before, during, and after the immigration law change using monthly marriage rates and county monthly marriage counts. The source of variation in marriage counts arose from the differences in the cost of obtaining legal status, which changed when the LIFE Act was put into effect. Using variations in US monthly marriage rates, the results indicate that while the LIFE Act was in effect, the national monthly marriage rate for this period was unaffected. The size of immigrant populations across both states and counties differ. Thus, in addition to using time-series variation, the analysis uses variations in the size of immigrant populations across counties. I use this additional variation to analyze the marriage propensity of residents in small and large immigrant population counties before, during, and after the immigration law change. Data from three of the top six “immigrant-receiving” states are examined. Using monthly county marriage data for New York, Florida, and New Jersey from 1999 to 2005, and employing a difference-in-differences methodology, the results indicate that when the LIFE Act was in effect the relative percent increase in marriages was approximately 59 percent. The results also indicate that in a period of 6 months before and after the LIFE Act was in effect, the number of marriages decreased by approximately 10 and 21 percent, respectively. Thus, the results suggest that the immigration law change not only affected the timing of marriage for some undocumented immigrants but produced relatively large temporal effects. Apart from theoretical interests, this article will contribute to the ongoing debate concerning undocumented immigrants and their future documented status. The article contributes to the debate by informing policymakers of the immediate effects of their immigration policies. The shifting of the timing of their marriages to within the window of opportunity and the approximately 34 percent increase in applications to become permanent residents from within the United States in 2001 suggest that the immigration policy has the ability to increase the number of documented immigrants.Footnote 1 Also, since the results suggest that the LIFE Act had positive effects on marriage counts, and to the extent that some of the increase in marriages are between undocumented immigrants and citizens or legal residents that would not have married otherwise, the immigration policy has the ability to increase the number of documented immigrants as well as change the family structure. Organization of the article is as follows. Section 2 describes relevant institutional details about changing residence status and the immigration law under consideration. Section 3 reviews the relevant existing literature. Section 4 provides evidence from national monthly marriage rates and county monthly marriage counts that individuals respond to economic incentives when considering the decisions of whether to marry and when to marry. Section 5 is the conclusion.",7
36,4,Eastern Economic Journal,22 September 2010,https://link.springer.com/article/10.1057/eej.2009.40,Childhood Behavioral Problems and Dropping Out of School,September 2010,John Robst,Charlie Weinberg,,Male,,Unknown,Mix,,
36,4,Eastern Economic Journal,22 September 2010,https://link.springer.com/article/10.1057/eej.2009.34,Solutions for the World's Biggest Problems: Costs and Benefits,September 2010,Daphne T Greenwood,,,Female,Unknown,Unknown,Female,,
36,4,Eastern Economic Journal,22 September 2010,https://link.springer.com/article/10.1057/eej.2009.42,The Monetary Policy of the Federal Reserve: A History,September 2010,Louis-Philippe Rochon,,,Unknown,Unknown,Unknown,Unknown,,
36,4,Eastern Economic Journal,22 September 2010,https://link.springer.com/article/10.1057/eej.2009.32,The Industrious Revolution: Consumer Behavior and the Household Economy — 1650 to the Present,September 2010,Zdravka Todorova,,,Female,Unknown,Unknown,Female,,
36,4,Eastern Economic Journal,22 September 2010,https://link.springer.com/article/10.1057/eej.2009.37,Beyond the World Bank Agenda: An Institutional Approach to Development,September 2010,John Volpe,,,Male,Unknown,Unknown,Male,,
36,4,Eastern Economic Journal,22 September 2010,https://link.springer.com/article/10.1057/eej.2009.45,The Fate of Young Democracies,September 2010,Clodagh Harrington,,,Female,Unknown,Unknown,Female,,
36,4,Eastern Economic Journal,22 September 2010,https://link.springer.com/article/10.1057/eej.2009.43,Corporations and Citizenship,September 2010,Gwendolyn Yvonne Alexis,,,Female,Unknown,Unknown,Female,,
37,1,Eastern Economic Journal,28 December 2010,https://link.springer.com/article/10.1057/eej.2010.65,Introduction to the Symposium on Agent-based Computational Economics,January 2011,Jason Barr,Troy Tassier,Leanne Ussher,Male,Male,Female,Mix,,
37,1,Eastern Economic Journal,28 December 2010,https://link.springer.com/article/10.1057/eej.2010.33,Heterogeneous Interacting Agent Models for Understanding Monetary Economies,January 2011,Joseph E Stiglitz,Mauro Gallegati,,Male,Male,Unknown,Male,"The current economic crisis is one of several factors contributing to questioning the robustness and the limitations of the mainstream approach. Formulated as a DSGE (dynamic stochastic general equilibrium) model, it is based upon the process of intertemporal maximization of utility in the market clearing context of a standard competitive equilibrium theory. There have been numerous critiques of that theory, for example, related to the assumptions of rational expectations and perfect competition. Our analysis focuses on another feature that makes the model's usefulness in macro-economic analysis particularly limited. Built upon the representative agent (RA) framework,Footnote 1 it rules out most of the key macro-economic interactions by assumptionFootnote 2: since most of what is relevant in economics concerns interaction and coordination of heterogeneous agents, the RA framework undermines macroeconomic analysis. The model begins with assumptions that trivially lead to conclusions that there can be no unemployment or liquidity crises; how can such a model provide much insight into the economy's current predicament of high unemployment, or how it got there? This approach also has (by construction) nothing to say about the network aspects of lending and inter-bank linkages that have become apparent during the current crisis. Economic theory based on the RA model has, in short, nothing to say about financial crises, bankruptcies, domino's effects, systemic risk and any pathology in general. Any claim it might make about the efficiency of the market is suspect: it is a result of the extreme assumptions underlying the model.Footnote 3 In an RA model, there are no lenders, no borrowers, and therefore no credit markets. Small departures from the perfect information hypothesis have been shown to undermine most of the key propositions of the standard competitive model. (Stiglitz 1992, 2001, if prices convey information about the quality there cannot be an equilibrium price as determined by a standard demand-supply analysis, since demand curves themselves depend on the probability distribution of the supply; Grossman and Stiglitz 1976, p. 98; 1980). In short, the RA framework of the DSGE models adopts the most extreme form of conceptual reductionism in which macroeconomics and the financial network are reduced to the behavior of an individual agent.Footnote 4 The RA in economics tantamount to saying that “macroeconomics equals microeconomics.”",45
37,1,Eastern Economic Journal,28 December 2010,https://link.springer.com/article/10.1057/eej.2010.34,Agent-based Modeling and Institutional Design,January 2011,Leigh Tesfatsion,,,,Unknown,Unknown,Mix,,
37,1,Eastern Economic Journal,28 December 2010,https://link.springer.com/article/10.1057/eej.2010.60,Learning in Agent-based Models,January 2011,Alan Kirman,,,Male,Unknown,Unknown,Male,"In basic economic theory agents are supposed to optimize, given their constraints. This, in situations where they are anonymous, isolated and have no influence over the constraints with which they are faced. Any intelligent student, coming to economics for the first time, finds this unrealistic and several intensive courses, particularly in micro-economics, are necessary to persuade him otherwise. What is it that is difficult to reconcile with economic reality? The usual answer is that the calculations needed to take an optimal decision are too complicated. But this is not the real problem. An individual consumer, for example, has only to know his preferences over every possible choice with which he is faced and his budget, which is determined by the prevailing prices. Then he simply chooses his most preferred option. Thus, he has to make no calculations but simply to choose. Of course, as soon as he is faced with uncertainty the situation is a little more complicated but here again, he chooses in such a way that his choices are consistent with his subjective probabilities. In the case of the firm, specific calculations have to be made but there is nothing intrinsically difficult in that. Nevertheless, the assumptions of complete preferences, of complete information, and of coherent probabilities are very strong. What is worse, is the implicit assumption that choices are made for the whole horizon faced by the individual, and that his underlying preferences are not affected by his experience, nor does he have to learn anything about his environment or the consequences of the choices that he makes. All of this has led to alternative bases for explaining or justifying the choices that individuals and firms make. One way out is to justify optimization as the result of a learning process. This involves an intellectual pirouette but a typical example of how the assumption of optimization may be justified is that of Lucas who said the following:
 In general we view, or model, an individual as a collection of decision rules (rules that dictate the action to be taken in given situations) and a set of preferences used to evaluate the outcomes arising from particular situation-action combinations. These decision rules are continuously under review and revision: new decisions are tried and tested against experience, and rules that produce desirable outcomes supplant those that do not. I use the term “adaptive” to refer to this trial-and-error process through which our modes of behavior are determined. [Lucas 1988] In other words, individuals simply learn to use rules which turn out to work well. However, Lucas goes on to say,
 Technically, I think of economics as studying decision rules that are steady states of some adaptive process, decision rules that are found to work over a range of situations and hence are no longer revised appreciably as more experience accumulates. While this might sound like a reasonable justification for optimizing behavior, it is worth reflecting on the nature of the sort of learning process he envisages. Lucas’ basic argument relies on an analogy with biology, which is that the evolution of the environment is very much slower than the speed at which agents or organisms adjust to that evolution. As a result, he suggests, we can safely ignore the impact of the agents’ behavior on the environment. Yet, a characteristic of economic environments is that they are made up of agents all of whom may be trying to learn about the environment and hence about what the other agents are doing. This rapidly becomes very complicated and it is not at all clear that the behavior of individuals will “co-evolve” to something corresponding to the theoretical static equilibrium solution. One solution to this, even for situations with many individuals, is to acknowledge the effects of the adjustments or learning of the individuals on each other. This means using a game-theoretic approach and to attribute game-theoretic strategic reasoning to the participants in a market and to try to show that agents learn to coordinate on an equilibrium (see e.g. Fudenberg and Levine [1998] for a full account). In particular when there are many equilibria of a game, for example, one can find out if individuals, by learning, coordinate on a specific equilibrium. Thus, in this view, learning is just an equilibrium selection device. But it is important to note that, even here where learning is assimilated to an adjustment mechanism, one has to specify precisely how individuals learn. This, of course, brings us back to a standard dilemma in economics, should we find learning rules which are analytically tractable, or should we explore rules which are intuitively plausible. While a consensus has developed in economic theory, rightly or wrongly, as to the definition of “rationality” and the associated axioms, no such consensus has developed about the appropriate definition of “learning.” Thus one is free to choose which is the appropriate learning model, but any theoretical results will be tied to that definition. Such results will be open to the criticism that they are ad hoc. In fact, the standard axioms of rationality are also ad hoc but have become accepted. Given this, one can look in a new light at the idea of using a different approach to modeling choice and learning what to choose. The approach in question is that of agent-based modeling (ABM). To quote Borrill and Testfatsion [2010], “Roughly, ABM is the computational modeling of systems as collections of autonomous interacting entities.” The important word here is “computational.” The idea is to specify the nature of the agents involved, the rules by which they behave and by which they interact and then to simulate the model in order to observe the outcomes. In fact, the agent-based approach to learning takes Lucas at his word, and models individuals as using simple rules and choosing those which work best, and then sees what the result of the interaction between them will be. Rather than starting out with an equilibrium notion and assuming that the individuals converge to “as if optimizing” and thus to an equilibrium, the idea is to find out whether they, in fact, do so. Indeed, in this way, one can examine what happens over time as agents learn to modify their behavior together, whether they are conscious or not of what the other participants in the market or economy are doing. Typically, it will only be in certain limited cases that one can simplify a model to the extent of being able to undertake a formal analysis of the phenomenon in question. Yet, these very simple cases can provide the basis for a fuller analysis. Thus, what one can do is to build a more general model in which agents follow the same rules as those in the analytical model, and then extend and expand it and, by simulating, see whether the results in the simplest cases hold up in the more general ones. ABM can thus provide a bridge between very simple stylized theoretical models and more general ones. Furthermore, it is often the case that apparently small modifications to the original theoretical model can lead to marked changes in outcomes and this can become apparent when simulating the associated agent-based model. Thus, agent-based models can help reinforce theoretical results but can also reveal their limitations. A somewhat more radical approach is to take ABM as a methodology in its own right and not to regard it as a way of checking the applicability to more general cases of limited theoretical results [see Epstein 2006; and Borrill and Testfatsion 2010]. In this paper I will give an example that adopts the approach of analyzing theoretically a situation in which agents learn about their environment, in which one can obtain some simple theoretical results. I will then go on to show how agent-based models help us to enrich and modify it. In this case agents on a market for a perishable good learn what prices to pay and to charge and with whom to trade. The two questions that are posed are: will agents learn to form stable buyer-seller relationships and will there be price dispersion and discrimination?",13
37,1,Eastern Economic Journal,28 December 2010,https://link.springer.com/article/10.1057/eej.2010.30,Agent-based Modeling and Computational Experiments in Industrial Organization: Growing Firms and Industries in silico,January 2011,Myong-Hun Chang,,,Unknown,Unknown,Unknown,Unknown,,
37,1,Eastern Economic Journal,28 December 2010,https://link.springer.com/article/10.1057/eej.2010.53,Active and Passive Learning in Agent-based Financial Markets,January 2011,Blake LeBaron,,,,Unknown,Unknown,Mix,,
37,1,Eastern Economic Journal,28 December 2010,https://link.springer.com/article/10.1057/eej.2010.43,Agent-based Models for Economic Policy Design,January 2011,Herbert Dawid,Michael Neugart,,Male,Male,Unknown,Male,,49
37,1,Eastern Economic Journal,28 December 2010,https://link.springer.com/article/10.1057/eej.2010.55,"Entry, Exit, and the Endogenous Market Structure in Technologically Turbulent Industries",January 2011,Myong-Hun Chang,,,Unknown,Unknown,Unknown,Unknown,,
37,1,Eastern Economic Journal,28 December 2010,https://link.springer.com/article/10.1057/eej.2010.64,Open- and Closed-Loop Supply Chain Dynamics: Specification and Exploration of an Agent-based Model,January 2011,Christopher S Ruebeck,Jeffrey O Pfaffmann,,Male,Male,Unknown,Male,"Environmental concerns are important drivers of innovation today, and such issues are pervasively linked to the supply chain of every good we produce and consume [Thomas and Griffin 1996]. The supply chain itself has impacts that are difficult to model [Mabert and Venkataramanan 1998]. We study an important component of delivering goods from producer to retailer: the shipping pallet. Shipping pallets are used at every stage of production, including distributing material within the firm. For simplicity, we focus on the final step of shipment to the retailer, a significant component of total pallet use [Freedonia 2008]. This final step is particularly important because the pallet is moving between economic actors. While a pallet stays within a firm, the firm's incentives to manage its private resources align well with the optimal social outcome, but as the pallet moves between firms, natural externalities may cause the firm's optimal choice to diverge from social welfare. This investigation is part of a larger effort to include the effects of private incentives — market interactions — in environmental policy considerations, particularly when performing a life cycle analysis (LCA) on a system of interest. We are not the first to use agent-based modeling in a supply chain context [see, e.g., Swaminathan et al. 1998; Choi et al. 2001; Gosling 2003; Hensher and Puckett 2005; Chaturvedi et al. 2006; Gosling et al. 2006]. Our innovation is to link this analysis with policy making. To date the LCA policy tool does too little to recognize the interaction between policy and markets [Fischhoff and Small 2000] and also does not address durable goods issues well [Cooper 2005]. Our approach to melding policy interests and markets is through agent-based modeling of the market itself. This will allow the policy maker to observe and interact with the model as it demonstrates the consequences of changed market incentives. In this paper we describe our model's structure and the results from validating at the level of a “stylized fact.” In particular, we describe valid parameter ranges as those that reproduce the observed market behavior of repair firms’ tendency to cluster near pallet users. This validation method is necessary when data on participant behavior is difficult to attain, and recognizes that any market model must be an abstraction of the true system's complex details. We can thus incorporate both market parameters that we have observed and abstractions of the market structure that keep the model tractable. Our model thus abstracts significantly from reality, maintaining an appropriate level of complexity for an initial investigation into a model of this type along with its method of validation. Validation methods for agent-based modeling are in their infancy, and models currently developed often use agents of very few types [Franke 2009; Lux 2009]. We discuss the additional complexity that can be modeled with later efforts in a section below on “the big picture” and in our concluding section. We next describe the market and its relationship to our model's structure, followed by our methodology and the experiments we have used to investigate the model's functionality, and we finish with a description of future work.",3
37,1,Eastern Economic Journal,28 December 2010,https://link.springer.com/article/10.1057/eej.2010.57,Reinforcement Learning in Experimental Asset Markets,January 2011,Shu-Heng Chen,Yi-Lin Hsieh,,Unknown,,Unknown,Mix,,
37,1,Eastern Economic Journal,28 December 2010,https://link.springer.com/article/10.1057/eej.2010.31,Income Distribution in a Stock-Flow Consistent Model with Education and Technological Change,January 2011,Stephen Kinsella,Matthias Greiff,Edward J Nell,Male,Male,Male,Male,"[W]hat markets do … is to generate the pressures that increase productivity … these pressures bring about innovations, organizational innovations as well as new technologies, which markets then diffuse throughout the system by the force of competition. Market adjustment — the price system — mobilizes the profits to underwrite the investment in these innovations, making the diffusion possible. This means that markets pick winners and losers, which is, indeed, a rough sort of allocation [Nell 1998]. In this paper, we claim the market system generates inequality endogenously, producing the “rough sort of allocation” described in the quote above. In markets characterized by decentralized traders interacting locally with other traders, and globally as members of a “sector,” rewards accrued by the successful benefit the rewarded, and losses hurt the losers. This modeling structure allows us to replicate observed empirical regularities of income distribution, educational levels, and firm sizes, and allows us to contribute to several literatures at once.Footnote 1 We model the simple insight of the market as an allocative mechanism between winners and losers using a four sector macrodynamic model comprised of firms, households, banks, and a government. Gains and losses are induced by market trading of heterogeneous agents for goods, better jobs, and risky assets, and innovative products. Initially, each agent in our model is given randomly allocated resources and abilities (households) or productivities (firms), which define a reservation wage for an individual household, and a basic cost structure for a firm. The corresponding gains from trade in goods and services (there is both labor and rental income generated in this model) go to those who “won” at the end of each trading sequence, creating an “allocative” market. Winners win, and losers lose, although a loss in one period does not necessarily “kill” the losers, thanks to a government transfer to the newly unemployed or disenfranchised in the form of a dole. The system carries on as an allocative mechanism — not of resources, but of relative wealth, and the result is a complex systemFootnote 2 exhibiting many of the regularities found in the empirical literature, as wealth, in the form of claims, is transferred from agent to agent in a bidding process which we can see and track in this agent-based model. Those households that do not find jobs in the labor market do not die: a “dole” is provided for them by a government, allowing them to survive until the next period. This creates the “tail” of the distribution we see in empirical studies of income distribution [Sinha 2006; Yakovenko 2009]. We make a twofold contribution in this paper. First, our model contains no representative agent, no utility function, no production function, and no rational expectations. We assume individual behavior is unpredictable — that individuals follow simple rules. We are not interested in discerning the outcome of these rules at the individual level. We begin from the premise of indeterminacy of individual choice at the micro level, which we instantiate as random selection from a given distribution. The action of any two agents are almost independent of each other, that is, there are a large number of degrees of freedom at the micro level. We do not look for a deterministic equilibrium but for a statistical equilibrium, defined as unregulated micro fluctuations with stable macro regularities [Foley 1994, 1999]. Second, our model generates observed empirical regularities in a parsimonious and easily extensible way, including income distribution, firm size distribution, business cycles, and other fat tailed distributions. We are keen to distribute the model, created in Mathematica, to allow others to extend the model. Economics has studied “fat tailed” or power-law distributions which characterize firm size and income distributions since the 19th century [Pareto 1896, 1965; Champernowne 1998; Gabaix et al. 2003; Yakovenko 2009]. The existence of these power laws has also been demonstrated in the econophysics, industrial organization, and finance literatures in recent work.Footnote 3 Models of interaction between heterogeneous agents which generate these dynamics are also beginning to become widespread.Footnote 4 Our model contributes to the literature on agent-based computational economics by combining the insights of transformational growth [Nell 1998], with modern simulation methods and modeling. The rest of this paper is laid out as follows. We describe the model in detail in the following section. We describe and discuss the results of our simulation study in the subsequent section. We conclude with a blueprint for further work on this model in the final section.",40
37,1,Eastern Economic Journal,28 December 2010,https://link.springer.com/article/10.1057/eej.2010.63,Linking Entity Resolution and Risk,January 2011,Germán Creamer,,,Male,Unknown,Unknown,Male,"This paper reviews (1) the application of a learning algorithm to generate a matching accuracy score that quantifies the status of entity resolution between consumer records of a major financial company and an external database, and (2) the relationship between the matching accuracy score and several risk segments. The excessive exposure of financial institutions to risky customers, especially the Subprime segment, accelerated the credit crisis of 2008. The magnitude of the crisis shows that many institutions may have underestimated their level of risk exposure. As a result of this crisis, enterprise-wide risk management systems are becoming more popular, especially for their promise to discover hidden risks. One of the risks that data providers and consultants are recently paying more attention to is the risk that these institutions do not correctly match customers with their records. This is part of a problem called entity resolution, the way records of similar or diverse databases referring to the same entity are identified and integrated [Wang and Madnick 1989; Talburt et al. 2005]. This concept is also recognized as record linkage [New-combe et al. 1959; Winkler 2006], object identification [Tejada et al. 2001], or customer recognition in the corporate world. Entity resolution can be solved using the features or attributes of its records (attribute-based), so that when the similarities of two records are above a certain level, they are considered coreferentFootnote 1 [Fellegi and Sunter 1969; Cohen et al. 2003]. For example, the evaluation of the similarity between two references is based on their edit distances. The most typical metrics used are vector-space cosine similarity [Baeza-Yates and Ribeiro-Neto 1999] and edit distance [Gusfield 1997]. Fellegi and Sunter [1969], extending the work of Newcombe et al. [1959], pioneered the use of probabilistic classification for entity resolution. This approach has influenced the most recent literature on entity resolution that combines several metrics using unsupervised learning [Monge and Elkan 1996; Navarro 2001; Chaudhuri et al. 2003; Cohen et al. 2003], unsupervised relational clustering [Bhattacharya and Getoor 2007], active learning [Tejada et al. 2001; Sarawagi and Bhamidipaty 2002], and to a lesser degree supervised learning [Ristad and Yianilos 1998; Cohen and Richman 2002; Bilenko and Mooney 2003; Singla and Domingos 2006]. The major difficulty with the latter approach is having a good sample of labeled examples for training and testing. The effect of inadequate customer recognition on credit risk might be very important in the Subprime segment, the riskiest segment of the market. A financial institution may calculate the amount of capital required to support its current credit risk exposure without taking into consideration the quality of its data. This institution may have several customers that are not appropriately recognized, such as accounts matched to a specific customer while they are owned by a riskier customer or customers. This may create many accounts with small variations of their names that borrow from all these accounts simultaneously, and the credit risk exposure of this financial institution will be much higher than what it initially anticipated and its capital might not be enough to cover its potential losses. This paper's approach to entity resolution is to combine several attributes of the matching accuracy of customer records in a major financial institution with a high-quality master database of an external vendor, using a supervised learning method, such as boosting. As a final result, an overall matching accuracy score is obtained for each customer using the most current account information and a learning algorithm. As the matching accuracy score is an indicator of the level of customer recognition, it is correlated with a risk score (FICO) to evaluate the relationship between customer recognition and risk segments. This paper has two main objectives: evaluating a methodology to generate a matching accuracy score to quantify the status of entity resolution between consumer records of a major financial company and an external database, and studying the relationship between matching accuracy and risk segments. We expect that well-identified customers are part of the Near Prime, Prime, and Superprime segments, while the rest are part of the Subprime segment. The rest of the paper is organized as follows: the “Learning methods” section presents the methods; the section “Data” introduces the data and features used in this research; the section “Experimental design” explains our experiments in detail; the section after that presents the results; the penultimate section discusses the results, and the last section presents the conclusions.",
37,2,Eastern Economic Journal,17 March 2011,https://link.springer.com/article/10.1057/eej.2010.14,Is the Permanent Income Hypothesis Really Well-Suited for Forecasting?,March 2011,Rangan Gupta,Emmanuel Ziramba,,Unknown,Male,Unknown,Male,"This paper first tests the restrictions implied by Hall's [1978] version of the permanent income hypothesis (PIH) obtained from a bivariate system of labor income and savings, using quarterly data over the period of 1947:01–2008:03 for the US economy, and then uses the model to forecast changes in labor income over the period of 1991:01–2008:03, using 1947:01–1990:04 as the in-sample. The forecast performance of the PIH model is compared using the Root Mean Squared Error (RMSE) for one- to eight-quarters-ahead forecasts generated from the univariate and bivariate versions of both classical and Bayesian variants of the Vector Autoregressive (VAR) model. In addition to this, given that the period of 1991:01–2008:03 was characterized by high levels of variation in the changes in labor income, as indicated in Figure 1, we also use a Gibbs-sampled version of the Bayesian Vector Autoregressive (BVAR) model characterized by heteroscedastic disturbances to capture the non-constant variance in the data over the out-of-sample horizon. Changes in labor income. The motivation of our study emanates from the work of Ireland [1995]. In this paper, the author first tested for the restrictions implied by Hall's [1978] version of PIH, as derived by Campbell [1987], over the period of 1959:01–1994:03 and then used the PIH model to forecast changes in labor income over the period of 1971:01–1994:03. Ireland [1995] failed to accept the restrictions, but showed that the PIH model outperformed the AR and VAR models — a result that we were also able to corroborate during our robustness checks (see footnote 16 for further details). Hence, Ireland [1995] indicated that the PIH must be judged on its ability to forecast better than alternative econometric models. In this backdrop, the objective of this paper is simply to revisit the forecasting ability of the PIH model, relative to standard alternative econometric models, over an out-of-sample horizon characterized by high degree of volatility in the variable of interest, namely, the changes in the aggregate labor income, based on longer data set. To the best of our knowledge, this is the first attempt to compare the ability of a PIH model in forecasting changes in labor income, relative to alternative Bayesian models, and in particular, Gibbs-sampled BVARs that account for non-constant variance in the variable of interest.Footnote 1 The PIH has its origins in Fisher's [1907] theory of interest rate, followed by the generalized version of Friedman [1957]. In the next section, we outline Hall's [1978] version of the PIH, since as in Ireland [1995], we use Campbell's [1987] econometric model for forecasting change in labor income. Note that Campbell [1987] showed how Hall's [1978] version of the PIH can be derived based on appropriate restrictions on an unrestricted VAR model. The section “Testing the PIH” presents the empirical estimates and the test for the restrictions implied by the PIH model, following the works of Campbell [1987] and Ireland [1995]. The “Alternative Models” section outlines the basics of the alternative models used for the forecasting exercise, while the section “Evaluation of Forecast Accuracy” presents the forecasting results. Finally, the last section concludes.",2
37,2,Eastern Economic Journal,17 March 2011,https://link.springer.com/article/10.1057/eej.2010.21,"Wage Differentials, Occupational Segregation, and Gendered Creativity Perceptions in the Chinese Science and Technology Sector: Beijing and Wuhan",March 2011,Gale Summerfield,Xiao-yuan Dong,Jie Hu,,Unknown,,Mix,,
37,2,Eastern Economic Journal,17 March 2011,https://link.springer.com/article/10.1057/eej.2010.32,Why Did the Private Business Equity Share Fall in Canada?,March 2011,Jie Zhou,,,,Unknown,Unknown,Mix,,
37,2,Eastern Economic Journal,17 March 2011,https://link.springer.com/article/10.1057/eej.2010.35,Cyclical Inflationary and Contractionary Biases in Latin America and the Caribbean: Evidence and Implications,March 2011,Magda Kandil,,,Female,Unknown,Unknown,Female,"Constraints on the supply side of the economy determine the allocation of aggregate demand shocks between output growth and price inflation. The more binding capacity constraints the higher is the degree of price flexibility with respect to shocks impinging on the economic system. Earlier research has focused on the speed of price adjustment in the face of demand shifts to measure price flexibility. Traditionally, the literature on price flexibility has focused on determinantsFootnote 1 and implications of price flexibility.Footnote 2 A higher degree of price flexibility is the result of a steeper supply curve, reflecting supply-side constraints that limit the effects of aggregate demand shifts on the real economy. Capacity constraints and/or institutional rigidities are important determinants of price flexibility and the output adjustment to demand shifts. Capacity constraints necessitate faster adjustment of price to iron out excess demand while increasing inflationary pressures. Alternatively, institutional rigidity may determine the frequency of adjusting wages and/or prices to demand shocks and, hence, affects the slope of the short-run supply curve. Nominal flexibility may be a function of institutional determinants or an endogenous response to aggregate uncertainty impinging on the economic system. Higher uncertainty increases the opportunity cost of fixing wages and/or prices, increasing price flexibility and limiting output adjustments to demand shocks. Capacity constraints and/or stochastic uncertainty are likely, therefore, to induce higher price flexibility. The slope of the supply curve may vary, however, with respect to positive and negative demand shocks. Recent research has attracted attention to possible asymmetry in the effects of aggregate demand shocks on economic activity.Footnote 3 Higher price flexibility during a boom would imply a kinked-slope supply curve, reflecting a steeper curve in the face of expansionary demand shocks. Subsequently, output contraction during recessions exceeds expansion during a boom. The shape of the supply curve differentiates macroeconomic performance in the face of demand variability. Higher variability increases the probability of realizing expansionary and contractionary demand shocks. A steeper supply curve during a boom increases price inflation relative to deflation and output contraction, relative to expansion. Subsequently, demand variability produces a bias towards higher inflation and lower output growth. Furthermore, as policy makers attempt to smooth the outcome of demand variability on economic performance, asymmetric constraints on the supply side may impose a serious challenge to their efforts. This paper builds on the earlier theoretical foundations to shed some light on structural rigidities governing the relation between demand variability and economic performance and trace the nature of cyclical fluctuations on the macro-economy. The data under investigation are for a sample of 32 developing countries in Latin America and the Caribbean, which have not been thoroughly analyzed for this purpose and offer a good match to enrich our understanding of the issues under consideration. Countries in Latin America and the Caribbean are small open economies that have been exposed to a variety of exogenous shocks. More importantly, countries in the sample are highly dependent on export receipts as a major source of income. Subsequently, government spending, which remains the engine of growth in many economies, has often fluctuated procyclically with export receipts, exacerbating the effects of expansionary and contractionary demand shocks. As policy makers grabble with the appropriate policy response, constraints on the supply side could exacerbate the adverse effects of procyclical policies. For example, higher export receipts, coupled with an increase in government spending, could increase inflationary pressures if capacity constraints limit output expansion with respect to demand expansion. Likewise, if wages and prices are rigid to adjust downward, the reduction in fiscal spending during a downturn could reinforce the contractionary effect on output contraction. As policy makers reevaluate the appropriateness of the fiscal stance, a thorough evaluation of constraints on the supply side are worthy of their attention to better gauge the policy response to external shocks and avoid exacerbating the adverse effects on the economy. Despite differences in size, openness, exchange rate arrangements and the direction of demand policies, many countries in the region have moved to liberalize their trade and capital accounts over the past two decades, increasing their exposure to external shocks. Moreover, governments across the region have taken a leading role in driving the growth process. Fluctuations in fiscal revenues with external shocks have exacerbated the severity of external fluctuations in these economies, warranting a careful evaluation of the macroeconomic implications. The evidence will evaluate the effects of positive and negative shocks to aggregate demand on the macroeconomy and draw the necessary implications for the appropriateness of procyclical policies in the face of external shocks. Table A1 contrasts average indicators across countries based on the exchange rate system. The high variability of the various indicators is consistent with the objective of this paper to evaluate the implications of demand-side variability in the face of supply-side constraints. The implications will define the paper's contribution in revealing capacity constraints and institutional rigidity that underlie asymmetry in Latin America and the Caribbean and draw policy implications. Inability to reflect expansionary demand shocks in output growth identifies the extent of capacity constraints in some countries that increase inflationary pressures during booms. Similarly, inability to reflect contractionary demand shocks in price deflation identifies the extent of institutional constraints that underlie downward nominal rigidity and reinforce the severity of output contraction during a downturn. On a regional front, the paper addresses an important and long-standing topic in the Latin American and Caribbean neo-structuralist (macroeconomic) literature that seeks to identify institutional, sectorial, and trade-related rigidities that underlie the poor macroeconomic performance and low growth in the countries of the region. Factors that underlie this rigidity have long been referenced in previous research [see, e.g., Prebisch 1986; Espinosa and Noyala 1997; Meller 2003; Sunkel 2004; Kay 2005; Lustig 2009; and Tsikata et al. 2009]. Across all of these studies, authors have directly or indirectly addressed structural rigidities, for example, low agricultural productivity, lack of growth in key areas, deterioration in terms of trade, and contraction of income and wealth. The implications of such rigidities have been identified on the size of the internal market, premature industrialization, and wide-spread indexation. The previous research has not identified, however, the composite effects of these factors on macroeconomic performance in Latin America and the Caribbean. The objective of the paper is to fill in the gap. The analysis seeks to identify the effects of factors identified in previous research on the shape of the supply curve in the countries under investigation. Having identified the structural constraints, the analysis will evaluate the implications of aggregate demand fluctuations on macroeconomic performance under the existing supply-side constraints. The evidence indicates that the majority of countries are characterized by a kinked-supply curve; that is, one that is flat when output is below potential and steep when it is above. During demand expansions, inflation accelerates while the real output response is moderate. On the other hand, during demand contraction, a flatter supply curve implies a bigger drop in real output growth with only a small deceleration in price inflation. The results point to two important policy implications: (i) the need to address structural rigidities that create the kink in the supply curve, and (ii) the danger of procyclical policies that accentuate demand shocks and exacerbate the associate upward bias on inflation and downward bias on real growth. Structural rigidity refers to capacity constraints that limit output expansion with respect to demand increase. Examples are constraints on capital, technology, and infrastructure. Likewise, institutional constraints underlie failure to absorb demand contraction in wage and price adjustments. Examples include labor contracts and menu costs that limit the frequency of wage and price adjustments, particularly in high inflationary environments and in the face of high variability of aggregate demand. The remainder of the paper is organized as follows. The next section provides a theoretical background for the kinked-slope of the supply curve. The subsequent section presents the empirical models. The penultimate section describes the time-series results. The cross-country analysis in the final section evaluates variation in price flexibility and the implications of the difference.",
37,2,Eastern Economic Journal,17 March 2011,https://link.springer.com/article/10.1057/eej.2010.44,Tax Design in the OECD: A Test of the Hines-Summers Hypothesis,March 2011,Davide Furceri,Georgios Karras,,Male,Male,Unknown,Male,"The OECD countries have been raising tax revenues in remarkably different ways. For example, the United States and Japan raise almost half of total revenue with income taxes and less than one-fifth by expenditure taxes (taxes on goods and services). On the other hand, Mexico and Korea rely on expenditure taxes for half of their revenue, and income taxes for around one-quarter. What can account for such marked differences? In a recent contribution, Hines and Summers [2009] argue that differences in tax design can be attributed to differences in country size and trade openness. Their argument is simple, but very intuitive. Because of globalization, governments find themselves operating in an environment of increasing mobility of economic activity and factors of production. As a result, countries which are small and open have tax bases that are more mobile than countries that are larger or less open. Therefore, small open countries have an incentive to rely less on income taxes and more on expenditure taxes, compared to larger and less open economies. Indeed, the smaller the size and the greater the openness of the economy, the more it will rely on expenditure taxes and the less on income taxes. We call this the Hines-Summers Hypothesis. The implications of the Hines-Summers Hypothesis are clearly important. First, it is well known that income taxes and expenditure taxes have very different properties both in terms of economic efficiency and distributional equity. In particular, income taxes are generally more distortionary, but expenditure taxes are usually less progressive. The Hines-Summers Hypothesis is also important because of its policy implications in a world of increasing mobility of economic factors. The literature on globalization and tax design has generally focused on tax competition as an outcome of increasing factor mobility [OECD 2008].Footnote 1 In the fact of tax competition, national government will reduce their autonomy both in terms of rising taxes and in the provision of public goods [Zodrow et al. 1986; Wilson 1995]. Moreover, tax competition is likely to increase the convergence and harmonization of the tax systems among countries,Footnote 2 in order to reduce negative spillover effects that government decision of one country can have on other countries.Footnote 3
 More recently, according to the Hiners-Summers Hypothesis, another effect of globalization on tax design is expected to occur through changes in the share of income and expenditure taxes as a result of an increasing degree of openness. The purpose of the present paper is to test the validity of the Hines-Summers Hypothesis for the OECD countries. In particular, we will try to assess whether the fraction of revenue raised by income taxes is indeed increasing with country size and decreasing with trade openness, while the fraction raised by expenditure taxes is decreasing with country size and increasing with trade openness. To this purpose, we use annual data for a set of 30 OECD countries for most of which we have data from 1965 to 2007. Our results are broadly consistent with the Hines-Summers Hypothesis. Thus, the evidence suggests that higher economic size is associated with increased reliance on income taxes and reduced reliance on consumption taxes, while the opposite holds true for greater trade or financial openness. Moreover, our estimates are quantitatively substantial, statistically significant, and robust. The rest of the paper is organized as follows. The next section describes the empirical methodology and the data we use to assess the effects of country size and trade openness on tax design. The third section presents and discusses the empirical results, and the fourth section concludes.",
37,2,Eastern Economic Journal,17 March 2011,https://link.springer.com/article/10.1057/eej.2010.45,The Role of Non-standard Work Status in Parental Caregiving for Young Children,March 2011,Rachel Connelly,Jean Kimmel,,Female,Male,Unknown,Mix,,
37,2,Eastern Economic Journal,17 March 2011,https://link.springer.com/article/10.1057/eej.2010.46,"A Note on Trade Unions, Unemployment Insurance, and Endogenous Growth",March 2011,Luciano Fanti,Luca Gori,,Male,Male,Unknown,Male,"One of the most important economic problems in Europe today is the apparently permanent high rate of unemployment experienced especially in countries with unionized labor markets. Theoretical studies using the standard overlapping generations (OLG) model to examine the effects of unionization on economic growth have generally found negative [Daveri and Tabellini 2000] or at best neutral effects [Corneo and Marquardt 2000]. An exception is Irmen and Wigger [2002], who compared the growth performance of an economy with unionized labor and unemployment with that of a competitive equilibrium economy with full employment in a two-period OLG context where a Romer-type externality represents the engine of endogenous growth.Footnote 1 In particular, they viewed a trade union as an institution that can (i) transfer resources from the old (dissavers) to the young (savers), and (ii) influence the factor income distribution in the whole economy, thus affecting aggregate savings. As only the young save in the OLG context, an increase in wages may enhance savings, which may in turn raise economic growth despite a corresponding reduction in the employment rate. Irmen and Wigger [2002] showed in fact that a rise in the relative importance the union puts on raising wages rather than maintaining employment (and thus a higher unemployment), spurs economic growth if the sum of the elasticity of substitution between capital and efficient labor and the output elasticity of efficient labor is smaller than unity, because saving increases in that case. This means that a union-growth-enhancing mechanism is triggered if and only if the degree of substitutability between the production inputs and the reduction in output due to the reduced employment rate are both low enough. Different from Irmen and Wigger [2002], in this paper we consider a double Cobb–Douglas economy and we introduce the fiscal sector (represented by an unemployment benefit system financed at a balanced budget by the government) in a context where the labor market is unionized and, hence, involuntary unemployment exists. The objective of this paper, therefore, can roughly be captured by the following question: can unionized-wage economies with unemployment grow faster than competitive-wage economies with full employment when both the utility and production functions are of the Cobb–Douglas type? The answer is yes, provided that individuals belonging to the working-age cohort are entitled to (even infinitesimal) benefit payments for the time of unemployment. In particular, similar to Corneo and Marquardt [2000] and Daveri and Tabellini [2000], we build a simple OLG model of endogenous growth that highlights both a wage setting union's formulation and a public provided unemployment benefit system. In our model, in line with Corneo and Marquardt [2000] and different from Daveri and Tabellini [2000], (i) the union's wage-setting program is assumed to be of the “right-to-manage” type (where the union cares about wages and employment and then seeks to set the wage above the competitive level), rather than the “monopoly wage-setting union”; and (ii) the productivity parameter in the production function is assumed to be proportional to the average per worker (rather than per capita) stock of capital in the economy.Footnote 2 As to this point, we note that in a context with unemployment, assuming the production externality to be defined in either per worker or per capita terms may lead to different final outcomes as regards economic growth and the unemployment dynamics.Footnote 3 We think, however, that assuming the labor productivity index in the production function to be influenced by the learning-by-doing of the employed rather than that of all the young (employed and unemployed) individuals in the economy is still in the spirit of the seminal Romer [1986], in which learning-by-doing is precisely the engine of growth.Footnote 4 A fortiori, thinking about the existence of a positive production externality also determined by the stock of capital per unemployed is at odds with the common belief that the unemployment time may actually reduce the skills of individuals, thus favoring — loosely speaking — a process of “unlearning-by-not-doing” in a Romer-type model. Moreover, we assume that the unemployment benefit expenditure is financed with a proportional tax levied only upon the consumption of the younger (working-age) generation rather by earmarking proportional wage taxes paid by either the employed people (as assumed by Daveri and Tabellini) or both firms and employees (as assumed by Corneo and Marquardt). Finally, different from Corneo and Marquardt, we abstract from the analysis of public pensions and focus exclusively on the effects of unionization on economic growth. As regards the results of such an investigation, while Daveri and Tabellini claimed that unionization is growth-reducing, Corneo and Marquardt argued that it is growth-neutral. In contrast with both contributions, in this paper we show that unionization always promotes economic growth when the union's preference weight on wages relative to employment is relatively low. Our finding, therefore, is in line with Irmen and Wigger [2002], but, given the difference between the underlying mechanisms behind the positive effect of unionization in the two papers, it deserves some comments: as the growth-promoting effect of unionized labor shown by Irmen and Wigger holds to the extent that the degree of complementarity between capital and labor is relatively strong, then in the Cobb–Douglas case the condition required by a unionized-wage economy with unemployment to grow faster than a competitive-wage economy with full employment can never be satisfied.Footnote 5 In other words, the union-growth-enhancing effect described by Irmen and Wigger [2002] requires that the technology is relatively favorable to labor income when facing with employment drops caused by unionization.Footnote 6 This is because in an OLG context capital accumulation occurs only through labor income, which is, in the case of such a technology, relatively favored by employment reductions. Different from Irmen and Wigger [2002], we find that a union-growth-enhancing effect can exist even if the technology of production is of the Cobb–Douglas type. In particular, the mechanism behind our result can be briefly summarized as follows: a rise in the union's relative wage intensity increases both the wage earned by the young and the unemployment rate (i.e., it reduces the employment rate). A higher wage rate (unemployment rate) affects in a positive (negative) way both savings and capital accumulation. The positive saving effect dominates the negative unemployment effect as long as the government provides an (even infinitesimal) unemployment benefit to the young-aged (assumed to be financed by consumption taxes). In this case, therefore, an economy with unionized labor and unemployment grows always faster than an economy with competitive labor and full employment. In contrast, if the government does not provide an unemployment benefit, then both the positive saving effect and negative unemployment effect of a rise in the union's relative wage intensity compensate each other exactly, and hence unionization is growth neutral in spite of a reduced employment rate. The novelty of this result is that in a double Cobb–Douglas context, the rate of per capita income growth in a unionized-wage economy with unemployment when (i) the labor productivity parameter in the production function depends on the per worker stock of capital, and (ii) unemployment benefits are financed by a consumption tax, may be always higher than the rate of per capita income growth in a competitive-wage economy with full employment, no matter the size of the unemployment benefit. The remainder of the paper is organized as follows. In section “The Model”, we develop the model and the main results are analyzed and discussed. The last section concludes.",3
37,2,Eastern Economic Journal,17 March 2011,https://link.springer.com/article/10.1057/eej.2010.47,Emigrant Effects on Trade: Re-examining the Immigrant-trade Link from the Home Country Perspective,March 2011,Bedassa Tadesse,Roger White,,Unknown,Male,Unknown,Male,"The objective of this study is to re-examine the effects of immigrants on trade from the perspectives of their home countries. Beginning with the seminal work of Gould [1994], a large literature examining the effects of immigrants on trade flows has emerged.Footnote 1 An empirical regularity established by these studies is that immigrants indeed affect trade flows between their host and home countries through direct and indirect channels. White [2007a] and Dunlevy and Hutchinson [1999], for example, indicate that through their preferences for goods from their home countries immigrants directly increase their host countries’ imports from their respective home countries, particularly if acceptable substitutes are unavailable in host country markets. Likewise, Tadesse and White [2008] and Rauch and Trindade [2002], respectively, indicate that by bridging cultural differences and by matching exporters with importers, and/or informally enforcing contracts, immigrants facilitate the initiation and completion of trade deals. Bryant et al. [2004], Girma and Yu [2002] and Gould [1994] similarly conclude that because of their knowledge of the languages spoken in both their home and host countries and their understanding of business practices in both countries, immigrants remedy communication gaps and/or reduce search costs; thus, directly contributing to increased trade flows between their host and home countries. Immigrants may also indirectly affect their host country's imports if their consumption of home country-produced goods influences the preferences of native-born residents or those of immigrants from other countries who reside in the host country such that they too consume home country goods. Through remittances and direct investment flows to their home countries, they may also enable home country residents to consume and/or produce at higher levels than would otherwise be possible; thus, indirectly affecting the home country's trade with the host country or other countries [Gupta et al., 2007; Murat and Pistoresi, 2009b]. Finally, immigrants may also influence trade, generally, since migration has a positive impact on global income levels which, in turn, increases the aggregate demand for tradable goods and services [Lewer and Van den Berg, 2009]. These observations on the direct and indirect impacts of immigrants on trade emerge from two types of studies: (a) those that examine the link from the perspectives’ of the home countries, and (b) those that examine the link from the perspectives’ of the host countries. A significant proportion of the available studies, however, are of the latter type. Typically employing immigration data for a single host country, these studies estimate the impact of the size of immigrant populations on bilateral trade flows between the host country and home countries of the various immigrant groups. On the other hand, only a small number of studies [Ehrlich and Bacarreza, 2006; Murat and Pistoresi, 2009a] undertake similar exercise from the standpoint of a home country — that is, how the size of a home country's emigrant flows to multiple host countries impacts the volume of trade between the home and hosts. Although the findings from host country-oriented studies indicate that immigrants have significant positive effects on their host countries’ trade with their home countries, the magnitudes of the observed effects of immigrants vary greatly across the host and home country cohorts examined. It is also questionable as to whether the positive effect of immigrants on their host country's trade is consistent across home countries. While one country's immigrant population is another's emigrant population (just as one country's imports are another country's exports), that a larger immigrant population within a host country generally encourages imports from the immigrants’ home countries does not imply the existence of an equivalent effect in terms of greater emigrant flows from a particular home country encouraging exports from the home country to all host countries. There are several reasons why this is the case. First, as the source countries for an immigrant population in a given host may differ, the destinations of an emigrant population from a given home could vary. Second, contrary to the extrapolations often made about the effect of emigrants on their home countries exports from the findings based on host country-oriented studies, immigrants may reduce their home country's exports to a given host if immigration from the home country increases the production of the home country's exportable goods in the host countries [Bryant et al., 2004]. Third, by increasing the relative prices of the home country's non-tradable goods and, in turn, reducing the production and exports of the home country's tradable goods, increased remittances from immigrants to a given home country may have a “Dutch-disease” effect on the country's exports to the world [World Bank, 2006].Footnote 2 Fourth, the extent to which immigrants (emigrants) affect trade flows may vary according to the anthropogenic make-up of the immigrant (emigrant) population. Head and Ries [1998] and White and Tadesse [2010], for example, report significant differences in the extents to which refugee and non-refugee immigrants affect their home countries’ trade with Canada and the USA, respectively. Likewise, Epstein and Gang [2006] indicate that immigrants who are aware of developments that influence trade or who have persistent cultural/ethnic ties to their home countries play greater roles as trade facilitators, and the tendency of immigrants to maintain regular contacts with individuals in their home countries and the persistence of such ties may also differ across emigrant populations from different home countries. Finally, Rauch and Trindade [2002] find that immigrants of Chinese origin exert pro-trade influences on their host countries’ trade with their home countries due to their strong ethnic (emigrant-to-emigrant) networks that give them the power to enforce sanctions in the face of default on agreements. However, in the presence of well-functioning legal and institutional frameworks, the trade-facilitating role of emigrants’ ethnic networks may not be as strong. In short, there is potential for heterogeneity in migrant-trade links to exist and such heterogeneity is less likely to be accounted for when the immigrant-trade relationship is examined solely from the perspectives of the host countries. As a result, we expect the extent to which emigrants from different countries influence their respective home countries’ trade to differ across home countries and across host countries. Given that many prior studies have examined the relationship from the host country perspective, our interest is in re-examining the effect of immigrants on trade from the perspectives’ of their home countries. Our study makes important contributions to the literature. We determine whether the effects of emigrants on their respective home countries’ trade coincide with the assessments often extrapolated from host country-oriented studies and, more generally, provide information that contributes to a better understanding of the avenues through which emigrants/immigrants influence trade flows between their home and host countries. To this end, using data that represent the stock of emigrants from 131 home countries that reside in 110 host countries (a number that is significantly larger than any previous study), we examine the immigrant-trade link from the perspectives of both the home country and the host country and provide comprehensive estimates of emigrants’ effects on trade for each home country in our study. Complementing the pro-trade effects of immigrants reported in previous studies, our results indicate that a 1 percent rise in the stock of immigrants increases the typical host country's exports to and imports from the typical home country by 0.15 and 0.17 percent, respectively. Estimating home country-specific emigrant-trade effects, however, reveals positive influences of emigrants on their home country's trade in 100 (for imports) and in 96 (for exports) of the 131 home countries included in our study. The magnitudes of the estimated effects also vary significantly across the individual home countries considered. Our results suggest that while projections of the pro-trade effects of emigrants from previous studies that have examined the link from the host country perspective are valid, the observed variation in the effects of emigrants on trade across their home countries suggests a need for further examination of what determines the ability of emigrants to influence their home country's trade. The paper proceeds as follows. We next present the empirical specification, discuss the data and the explanatory variables included in the analysis and indicate our a priori expectations of the signs of respective coefficients. We then discuss the results obtained from our analysis and conclude.",11
37,2,Eastern Economic Journal,17 March 2011,https://link.springer.com/article/10.1057/eej.2010.12,"Inequality, Consumer Credit, and the Saving Puzzle",March 2011,Zdravka Todorova,,,Female,Unknown,Unknown,Female,,2
37,2,Eastern Economic Journal,17 March 2011,https://link.springer.com/article/10.1057/eej.2011.1,List of Reviewers 2010,March 2011,,,,Unknown,Unknown,Unknown,Unknown,,
37,3,Eastern Economic Journal,23 June 2011,https://link.springer.com/article/10.1057/eej.2011.8,The Profession and the Crisis,May 2011,Paul Krugman,,,Male,Unknown,Unknown,Male,"What should economists have known about the impending crisis, and when should they have known it? Ask any one economist that question, and by and large the answer is that they should have known what he or she knew, and can be excused for not knowing more. Me too! Clearly, it's not fair to demand that economists have known that Lehman would go bust on September 15, 2008; in fact, I think most people would agree that it's unrealistic to have expected economists to get either the year of the crisis or the firms that fell first right. But should they have seen a crisis building several years before it happened? Should they have had at least a rough idea of how bad it would be? Well, from my point of view — which, because I’m like everyone else, is that what I saw and no more is what everyone should have seen — it still seems bizarre how many economists failed to see that we were experiencing a monstrous housing bubble. As Robert Shiller has documented — and, crucially, was documenting in real time circa 2004–5–6 [Shiller 2005] — the rise in real housing prices after 2002 or so took them into completely unprecedented territory. It was the clearest market mispricing I’ve seen in my professional life, even more obviously out of line than the dot-com bubble, which at least had the excuse that it involved novel technologies with unknown potential; houses have been with us for 7,000 years or so, and we should have a reasonable idea of what they’re worth. So why were so relatively few economists willing to call the bubble? I suspect that efficient market theory, in a loose sense — the belief that markets couldn’t possibly be getting things that wrong — played a major role. And in that sense there was a structural flaw in the profession. What about what would happen when the bubble burst? I personally failed to realize how big the “knock-on” effects would be; and according to the self-justifying principle, I’m tempted to say that nobody could reasonably have been expected to get that right. But actually, we should have seen that coming too — maybe not in full detail, but even a casual walk through historical crises should have indicated that a housing bust was likely to bring large financial and balance-sheet problems in its wake. I kick myself every once in a while for failing to think that part through. In particular, those of us who had worked on the Asian financial crisis of the 1990s had placed large weight on balance-sheet effects [Krugman 1999]. Why didn’t I think of applying the same logic to the coming bust in US home prices? Beyond that, surely experts in banking and finance should have been aware of rising leverage, of the growing reliance on unregulated shadow banking, and so on. It's quite remarkable how few warnings we had that the system might be dangerously fragile. By all means, let's give credit to people like Rajan [2005] who saw some of it; but the very fact that such people were given a hard time for their analysis is telling about the profession. Still, as Yogi Berra said, it's tough to make predictions, especially about the future. There are so many things going on in the world, many of them off any modeler's radar, that the profession's failure to see this crisis coming is not, in my mind, anything close to its biggest sin.",56
37,3,Eastern Economic Journal,23 June 2011,https://link.springer.com/article/10.1057/eej.2011.6,Fairness and Tax Policy: A Response to Mankiw's Proposed “Just Deserts”,May 2011,Jonathan Weinstein,,,Male,Unknown,Unknown,Male,,1
37,3,Eastern Economic Journal,24 January 2011,https://link.springer.com/article/10.1057/eej.2009.41,"Financial Aid, Student Background, and the Choice of First-year College Major",May 2011,Mark Stater,,,Male,Unknown,Unknown,Male,"The majors that students choose in college have important consequences for both the students themselves and the larger society. Research has shown that these choices have an impact on a student's career options and earning power that lasts long after the college experience is over [Sacerdote 2001]. The knowledge and training obtained in the pursuit of a college degree also comprises a vital element of the human capital of the work force, which in turn affects the economy's productivity and standard of living. Currently, some experts anticipate slow labor supply growth relative to labor demand in vital areas that require extensive post-secondary training, such as science and health care [Medical News Today 2006]. Whether the higher education system will be able to produce enough scientifically trained graduates to increase labor supply in these sectors at a pace commensurate with labor demand is a matter that depends, in part, on the majors students choose in response to the costs and benefits of alternative fields of study. A large higher education literature has examined the determinants of college majors. Research has identified the important roles played by student socio-economic and demographic characteristics such as race and gender; pre-college academic credentials; institutional characteristics; and post-college labor market returns. What is less well understood, however, is the effect of financial factors students face in college. Financial variables such as tuition and financial aid can, in theory, affect major choices by altering the relative streams of lifetime benefits associated with alternative college degrees. As higher education continues to rapidly become more expensive to the average student [McPherson and Schapiro 1998; Ehrenberg 2002], tightening financial constraints may dissuade students from choosing technically demanding majors that can be difficult to complete and require additional years of schooling beyond college. This effect may be exacerbated in light of the mediocre achievement of US pre-college students in math and science [National Center for Education Statistics 2008]. Therefore, it is important to understand whether educational policy-makers have any curricular or financial policy leverage over the decisions that students make about their college majors. This paper examines data on a sample of first-year college students at three large public universities to examine the effects of tuition and financial aid on the choice of first-year college major, as well as how the effects of tuition and aid vary with a student's background in various high school subjects. A theoretical model of utility-maximizing major choice predicts that when the net costs of college increase, majors with higher expected wages become more attractive relative to majors offering lower expected wages. Consistent with this prediction, the data suggest that higher net costs of attendance (tuition minus total financial aid) are associated with a higher probability of choosing professional majors and lower probabilities of choosing humanities and science majors. Furthermore, the effect of tuition on the probability of choosing a major is generally larger for those with more high school credits in similar subjects and smaller for those with more credits in dissimilar subjects. This is consistent with a positive relationship between high school preparation in a subject and the expected returns to a related major. Overall, financial variables and prior subject backgrounds appear to influence college major choices in ways that reflect a student's academic comparative advantage. In particular, deficiencies in math and science achievement, together with the rapidly escalating tuitions that have been a hallmark of the higher education marketplace for the last two decades, may combine to reduce the probability that students choose science- and health-related fields in their first years of college. This does not guarantee that there will be fewer science, engineering, and health professionals in the future because the impact on final degrees depends on how rising costs affect major choices for a given student over time as well as across students in different sectors of higher education. However, the results do suggest that, if costs continue to increase, any given large public university student is less likely to declare an intention to acquire the training needed for these careers upon entering college. More generally, in that it may be socially valuable for students to experiment with subjects where their backgrounds are limited in order to find the best match, recent trends in tuition and financial aid may impede the efficient sorting of students across majors. The remainder of this paper is organized into the following sections: Literature, Theory, Data and Descriptive Statistics, Empirical Model and Results, and Conclusion.",5
37,3,Eastern Economic Journal,24 January 2011,https://link.springer.com/article/10.1057/eej.2009.48,Inequalities within Couples in Europe: Market Incomes and the Role of Taxes and Benefits,May 2011,Francesco Figari,Herwig Immervoll,Holly Sutherland,Male,Male,Female,Mix,,
37,3,Eastern Economic Journal,28 February 2011,https://link.springer.com/article/10.1057/eej.2009.49,Will You Covenant Marry Me? A Preliminary Look at a New Type of Marriage,May 2011,Amanda J Felkey,,,Female,Unknown,Unknown,Female,"In an effort to combat growing divorce rates and create stronger, happier marriages, some states have begun to offer a stricter legal marriage contract called a covenant. Covenant marriages make more costly the divorce process — making them a seemingly apt remedy to growing divorce rates. But are they? And even if covenant marriages prove successful in strengthening the bond between husband and wife, ultimately lowering the divorce rate, are couples in them happier? Are the costs or benefits beyond those experienced by the couple using the covenant? Questions surrounding the possible trend toward stricter marriage contracts abound. But before anyone can attempt to answer them, we need to understand who is marrying with these contracts and why. For instance, do younger couples marry with covenants or is their use driven by religion? Are potential mates using this new marriage as a way of signaling or screening their partner or is it rather a commitment device? The answers to these questions certainly affect the potential consequences for divorce rates. One might think that if younger couples are using this stricter marriage as a commitment device, the result may not be fewer divorces, just more costly ones. On the other hand, if couples are screening for some unobservable trait, like commitment, and covenants give them more information before marriage, they may be useful in their goal of stifling divorce and increasing happiness. This paper serves as a preliminary exploration of the covenant marriage option and is the first systematic economic analysis of this new marriage. As the fundamental first step to understanding how covenant marriages will effect couples, this paper aims to (1) describe what a covenant marriage is and outline the debate surrounding this new type of marriage; (2) consider why couples might use this stricter and potentially more costly union; and (3) explore who is marrying with a covenant by comparing different populations of couples and empirically estimating what individual and couple characteristics affect the decision to use a covenant. This paper addresses each of these issues in turn. Section two of this paper outlines what a covenant marriage is and comments on the debate surrounding this type of union. The third section considers why couples might choose the covenant marriage option. It offers competing theoretical explanations for why individuals may prefer the more binding contract. Section four uses marriage certificate data from Louisiana and Arkansas to determine empirically who is using the covenant option. It describes and compares subpopulations based on time as well as marriage and ceremony type, and utilizes a logit analysis to determine what couple and individual characteristics affect the decision to engage in a covenant marriage. This section sheds some light on why couples are choosing covenants. Section five concludes and outlines remaining questions.",2
37,3,Eastern Economic Journal,31 January 2011,https://link.springer.com/article/10.1057/eej.2009.47,Can the Built Environment Reduce Obesity? The Impact of Residential Sprawl and Neighborhood Parks on Obesity and Physical Activity,May 2011,Deliana Kostova,,,Female,Unknown,Unknown,Female,"In an effort to explain the obesity epidemic, it is frequently noted that obesity prevalence has broad geographic variation, with obesity being less common in areas with denser residential development and more widespread in areas characterized by residential sprawl. This observation is the basis for a body of research suggesting that the obesity rate is tied to the built environment. The built environment is a term used to describe urban form and can be defined as the man-made surroundings that provide the setting for human activity, such as neighborhood design, street connectivity, sidewalks, etc. The predominant pattern of the built environment in the US promotes dependence on cars and is commonly blamed for contributing to the spread of obesity. For that reason, modifying the built environment is increasingly being considered as a potential solution to physical inactivity and obesity. The proposed changes aim at encouraging routine physical movement and range from constructing sidewalks to incorporating greenspace to building mixed-use developments. Major institutions such as the World Health Organization and the Centers for Disease Control and Prevention (CDC) have endorsed change in urban planning patterns for this purpose. Built environment advocates take the view that activity-friendly city planning may have a curbing effect on obesity, similar to the effect that the “healthy city movement” had on tuberculosis in the 19th and 20th centuries.Footnote 1
 The built environment and its effect on health and obesity have so far been discussed mainly in terms of residential sprawl. Sprawl has been shown to be strongly associated with increased car use and higher obesity levels [Ewing et al. 2003; Frank et al. 2004; Lopez 2004]. Although it has long been established that sprawling residential areas are more likely to be populated by heavier individuals, the main issue with using sprawl control as a public health tool is the uncertain causal link between sprawl and obesity. There are reasons to suspect that the observed obesity–sprawl association may be due to the self-selection of lower-weight people into more activity-friendly environments or vice versa. Alternatively, people's behavioral preference for higher comfort and lower physical activity may be driving up both obesity and sprawl, producing a false impression of a causal link between the two. In either case, resources spent on building active living environments may miss their target population as those at the highest risk for obesity prefer to avoid active-living situations. The primary goal of this paper is to study the effect of the built environment on obesity while looking for evidence of causation. The contribution of this research to the existing literature on obesity and urban form is threefold. First, in addition to estimating the effect of residential sprawl on obesity, I also examine an additional popular characteristic of the built environment, proximity to neighborhood parks. Second, I estimate the impact of these two built environment features on the prevalence of obesity instead of measuring their effect on the Body Mass Index (BMI). In other words, this research asks the question, “Can building more parks or reducing sprawl reduce the number of obese people?” as opposed to, “How many pounds could people lose on average?” While modeling general BMI levels may have an advantage from a public health perspective, it is also relevant from a policy perspective to assess if, and potentially by how much, the actual rate of obesity in the population could be affected by urban form changes. The third contribution is in the analytic approach of examining the link between obesity and the built environment. This approach takes two forms. The first one is indirect, and consists of modeling the effect of the built environment on physical activity. The second approach directly models the effect of the built environment on the prevalence of obesity with the help of instrumental variables (IV). Since physical activity is the main mechanism through which the built environment is expected to affect obesity, it makes sense to evaluate the relationship between physical activity and urban form in order to validate any obesity results we might eventually produce. For example, a finding that physical activity is unaffected by the built environment would cast doubt on the existence of a causal link between the built environment and obesity. Previously, an assessment of the link between physical activity and sprawl has been done by Ewing et al. [2003]. Using similar personal and sprawl data as the ones used in this analysis, Ewing et al. [2003] find that higher levels of physical activity as well as time spent walking are both negatively associated with sprawl but they also find that this association is no longer present when the definition of physical activity is expanded to include minimal levels of activity. Given the importance of physical activity as a mechanism to reduce obesity, a re-evaluation of the built environment as a predictor of physical activity is an important contribution to the obesity-urban form literature. Similar to Ewing et al. [2003], I find that there is no statistically significant link between the built environment and participation in physical activity when physical activity is defined to include not only medically recommended amounts of exercise but also any level of activity. This result holds in both ordinary least squares (OLS) and two-stage least squares (2SLS) specifications, reducing the plausibility of a causal connection between the built environment and obesity even before such a connection is directly examined. Following the analysis of physical activity as a product of the built environment, this paper focuses on the direct relationship between the built environment and obesity status. A major econometric concern here is that where people live is largely a matter of choice, which makes standard single equation estimation of urban form characteristics inappropriate.Footnote 2 I use two-stage specifications in order to identify the effect of the built environment in the presence of unobservable individual characteristics. While the success of a two-stage model depends on the validity of its excluded instruments, it also has several advantages over the methodologies applied so far in the literature. Previous studies of the relationship between BMI and sprawl either ignore the endogeneity of urban form [Ewing et al. 2003; Giles-Corti et al. 2003; Saelens et al. 2003; Frank et al. 2004; Lopez 2004], or address it on much smaller samples with panel data methods [Ewing et al. 2006; Plantinga and Bernell 2007; Eid et al. 2008]. Regardless of the methods employed, existing research finds that when self-selection and unobserved individual characteristics are ignored, residential sprawl is positively associated with BMI, but when self-selection is addressed, this association is no longer significant. In the most recent study on the subject, Eid et al. [2008] use fixed effects to remove the interference of unobserved characteristics and self-selection by looking at a sub-sample of people who move from a sprawling area to a denser area (and vice versa) over time. Although this approach is a valid way to look for evidence of causation, it can be criticized for the assumption that the residential move is always unrelated to changes in the person's unobserved predisposition to obesity. For instance, a person may receive a doctor's warning that motivates him to avoid/reduce obesity and consequently decide to move to an activity-friendly area. This would change both his propensity for obesity and his living location. Restricting the sample to individuals who move could also be problematic because this sub-sample may not be fully random, and is likely to exclude individuals most prone to obesity. For example, a person who has a naturally lower energy level may prefer to drive more, which makes him prone to obesity, and may also dislike the hassle of moving, which makes him more likely to stay in the same house. A two-stage instrumental-variables approach as the one used in this paper avoids the need to restrict the analysis to individuals who move and to a possibly non-random sample of the population. The findings of this research are as follows. When the predicted outcome is the occurrence of physical activity, even an ordinary single equation framework which ignores unobserved individual heterogeneity produces no significant association between physical activity and the built environment. This conclusion can be used to question the validity of any subsequent results that show a significant relationship between the built environment and obesity. In line with previous studies, I find that the relationship between the built environment variables and obesity is highly significant when unobservables are ignored, with a positive association between sprawl and obesity and a negative association between park access and obesity. However, as we may have come to expect from the physical activity results, all of these associations disappear once unobserved individual characteristics are accounted for. Although this is the first paper to employ instrumental variables in this context, the conclusions agree with the rest of the literature in which different methodologies are used to control for unobservables. In summary, the evidence from the two analytic approaches used in this research (the physical activity models and the two-stage obesity model) both point to lack of causation between obesity and the built environment features under consideration.",18
37,3,Eastern Economic Journal,31 January 2011,https://link.springer.com/article/10.1057/eej.2009.51,"Monetary Policy and the Credit Channel, Broad and Narrow",May 2011,Torben W Hendricks,Bernd Kempa,,Male,Male,Unknown,Male,"Large fluctuations in aggregate economic activity sometimes arise from what appear to be relatively small monetary impulses. A large and growing literature views the credit channel as the decisive conduit through which monetary policy affects the real economy. In this literature, the financial system is seen as both an accelerator of monetary impulses and as an independent source of non-monetary effects on the business cycle. The basic premise of the credit channel theory is the recognition of imperfections in credit markets due to asymmetric information and imperfect contract enforceability. Two variants of the credit channel can be distinguished: a narrow bank lending channel and a broad credit channel [Bean et al. 2002]. The bank lending channel holds that monetary policy influences the supply of intermediated credit through its impact on banks’ loanable funds [Bernanke and Blinder 1988]. According to this view, rising interest rates associated with monetary tightening may aggravate problems of asymmetric information in the form of both adverse selection and moral hazard effects. Adverse selection effects arise because low-risk investment projects with associated low returns are crowded out of the credit market, whereas moral hazard occurs as higher loan interest rates induce borrowers to turn to more risky investment projects. Rather than raising interest rates, banks may thus prefer to curtail credit instead. As some borrowers, mainly households as well as small and medium-sized firms, are dependent on bank credit and are unable to borrow in the securities markets, credit rationing acts as an amplifier of monetary policy impulses on the real economy. In contrast to the bank lending channel, the broad credit channel starts from the assumption that informational imperfections are encountered in all credit market segments with no special role for the banking sector [Bernanke 1993; Carlstrom and Fuerst 2001]. The amplification effect of monetary policy then works through interest rate spreads rather than credit rationing. According to this view, informational asymmetries give rise to an external finance premium that drives a wedge between the cost of internal and imperfectly collateralized external finance. The external finance premium that a borrower must pay should depend inversely on the strength of the borrower's financial position, measured in terms of factors such as net worth, liquidity, and current and future expected cash flows [Bernanke and Gertler 1989]. By inducing a decline in aggregate real activity and inflation, monetary tightening deteriorates borrowers’ cash flow and increases their real debt burden, thereby raising their external finance premia. This effect should be strongest for small firms with weak balance sheets [Gertler and Gilchrist 1993; Oliner and Rudebusch 1996a], but even large firms with relatively easy access to capital markets may face non-trivial external finance premia [Levin et al. 2004]. Moreover, these effects need not be confined to firms and capital spending but may operate through household spending decisions as well. Households face an external finance premium which is lower the stronger their financial position is, with home equity a significant part of household's net worth [Aoki et al. 2004; Iacoviello 2005; Almeida et al. 2006]. Whereas the general existence of a credit channel is rather uncontroversial, its effectiveness depends on a number of different factors. In particular, the leverage of the credit channel appears to vary throughout and across business cycles [Gertler and Gilchrist 1994], may be strongly influenced by whether monetary policy is tight or easy [Oliner and Rudebusch 1996b], and may deteriorate on a secular path as financial innovation increases the substitutability of intermediated and non-intermediated credit [Bernanke and Gertler 1995]. In this paper we attempt to identify the importance of both variants of the credit channel from US output, interest rate, and bank lending data. We are concerned with identifying macroeconomic factors influencing the strength of the credit channel in the monetary policy transmission process. To this end we investigate how the credit channel depends on the business cycle, the stance of monetary policy, and possibly other latent factors. In order to account for such latent factors we employ a Markov-switching model in which the parameters of the data generating process of the observed time series depend on an unobservable state variable, which we associate most broadly with developments in financial markets. The remainder of the paper is structured as follows: the next section contains technical details on our estimation strategy, the section after that reports on the estimation results and a concluding section summarizes our findings.",2
37,3,Eastern Economic Journal,28 March 2011,https://link.springer.com/article/10.1057/eej.2010.58,Ethnic and Religious Diversity and Income Inequality,May 2011,Oguzhan C Dincer,Michael J Hotard,,Unknown,Male,Unknown,Male,"As Putnam [2007] argues, one of the most important challenges facing modern societies is the increase in ethnic diversity. He goes on to warn about the short- and medium-term problems for social cohesion which enhanced diversity can bring. According to conflict theory, ethnic and religious diversity generate conflicts which in turn lead to poor quality of institutions and poorly designed economic policies. Several empirical studies using cross-country data have found persuasive evidence supporting conflict theory [see Alesina et al. 2003; Alesina and La Ferrara 2005; Easterly and Levine 1997; Montalvo and Reynal-Querol 2005a, 2005b; Dincer 2008]. Perhaps because income inequality and ethnic and religious diversity are considered as alternative sources of conflict, the possible relationship among them has largely been ignored by the literature. In this study, we investigate whether such a relationship exists. Diversity affects income inequality in a variety of ways. It hampers policies to redistribute income. Viewing each other as direct competitors for scarce economic resources, individuals who belong to one ethnic group are less willing to support redistribution helping other ethnic groups [Alesina and Glaeser 2004; Bobo and Kluegel 1993; and Bobo and Hutchings 1996]. A perfect example of how diversity affects redistribution is given by Alesina et al. [1999]. After becoming a more diverse county, Prince George's County in Maryland next to Washington, DC, voters passed a law called TRIM in the late 1970s. TRIM ceiled the property tax rate, main source of revenue for schools. For many, TRIM is one of the reasons for poor quality of public schools in Prince George's County. In Montgomery County, which is next to Prince George's County, on the other hand, voters repeatedly rejected tax laws affecting funding for public schools. Montgomery County is known for the quality of the public schools. It has large white majority and less ethnically diverse than Prince George's County [Alesina et al. 1999, p. 1244]. Using survey data from the US, Luttmer [2001] finds increasing support for welfare programs among individuals as the share of the recipient individuals who belong to the same ethnic group increases. His findings suggest that one additional black recipient reduces support for welfare by non-black respondents but has little effect on black respondents. Conversely, an additional non-black recipient reduces black support for welfare but have little effect on non-black support. Again using survey data, Okten and Osili [2004] investigate the determinants of monetary and non-monetary contributions in Indonesia to community organizations, another important redistribution channel, and find similar results. Households are less likely to contribute to community organizations if they belong to a non-majority group. This supports Alesina and La Ferrara's [2000] hypothesis that the members of the non-majority ethnic group derive positive utility from interacting with the members of the same ethnic group and negative utility from interacting with the members of the majority ethnic group. The purpose of this study is to explore the relationship between ethnic and religious diversity and income inequality in search of confirmation for the causalities suggested above. Measurement of diversity is, of course, vital to our analysis. There are two competing indices frequently used in empirical studies: the fractionalization index (FI) and the polarization index (PI).Footnote 1 FI, which gives us the probability that two randomly selected individuals in a country belong to two different ethnic or religious groups, is calculated as follows:   where n

ij
 is the population share of group j in country i. FI

i
 increases with the number of groups and reaches a maximum if every individual in a country belongs to a different ethnic or religious group. PI, on the other hand, measures the distance of any distribution of ethnic or religious groups from the situation that could lead to maximum conflict. It is calculated as   and reaches a maximum when there are two ethnic or religious groups of equal size in a country.Footnote 2 In our study, we use both the polarization and the fractionalization indices. Figure 1, taken from Montalvo and Reynal-Querol [2005a], shows FI and PI as functions of the number of groups (here assumed of equal size). Figures 2 and 3 show the relationships we find between ethnic and religious polarization and fractionalization indices for the countries that we use in our analysis. As the figures show, for low levels of fractionalization, PI and FI are positively and highly correlated. For medium levels of fractionalization, the correlation is zero, and for high levels of fractionalization it is negative. Polarization and fractionalization as a function of the number of equal size groups. Ethnic fractionalization vs polarization.Source: Montalvo and Reynal-Querol [2005a, 2005b]. Religious fractionalization vs polarization. Source: Montalvo and Reynal-Querol [2005a, 2005b]. Regarding income inequality, its measurement is at least as important as the measurement of diversity. We use the most commonly used measure of income inequality in the literature: the Gini coefficient. Controlling for variables such as gross domestic product, education, and inflation, we find a linear and positive relationship between ethnic and religious diversity and income inequality when we use the polarization index as our measure of diversity. According to our estimations, going from an ethnic (religious) polarization index of 0 (only one group in the society), to an ethnic (religious) polarization index of 1 (two groups of equal size in the society), would increase the Gini coefficient by almost 6 (respectively, 3) percentage points. This is quite significant given the average Gini coefficient of 0.44. When we use the fractionalization index as our measure of diversity, we find inverse U-shaped relationships between both ethnic and religious diversity and income inequality. In other words, we find inequality maximizing levels of fractionalization. According to our estimations, the Gini coefficient would be maximized when the ethnic (religious) fractionalization index is equal to 0.44 (respectively, 0.34). Our results confirm Dincer and Lambert's [2008] similar findings for US states. The study is organized as follows. In the next section, we present our data on income inequality, ethnic and religious diversity, and on the control variables we use in our analysis. In the subsequent section, we present and discuss our empirical model and estimation results regarding relationships between ethnic and religious diversity and income inequality. In the concluding section, we consider the implications of our empirical findings.",10
37,3,Eastern Economic Journal,23 June 2011,https://link.springer.com/article/10.1057/eej.2009.44,"Multidimensional Poverty Measurement: Concepts and Applications, by Udaya Wagle",May 2011,Sylvain Weber,,,Male,Unknown,Unknown,Male,,
37,3,Eastern Economic Journal,23 June 2011,https://link.springer.com/article/10.1057/eej.2010.1,"Morals and Markets: An Evolutionary Account of the Modern World, by Daniel Friedman",May 2011,David Levy,,,Male,Unknown,Unknown,Male,,
37,3,Eastern Economic Journal,23 June 2011,https://link.springer.com/article/10.1057/eej.2011.4,Financial Statement Fiscal Years 2009–2010 and Previous Fiscal Year,May 2011,,,,Unknown,Unknown,Unknown,Unknown,,
37,4,Eastern Economic Journal,31 January 2011,https://link.springer.com/article/10.1057/eej.2009.50,Effect of State Health Insurance Mandates on Employer-provided Health Insurance,October 2011,David N van der Goes,Justin Wang,Katharine C Wolchik,Male,Male,Female,Mix,,
37,4,Eastern Economic Journal,07 February 2011,https://link.springer.com/article/10.1057/eej.2010.2,Shipwrecks on the Great Lakes and the Lake Carriers Association,October 2011,Christopher S Decker,William Corcoran,David T Flynn,Male,Male,Male,Male,"This paper addresses the role played by the Lake Carriers Association (LCA) in the reduction of accidental shipping losses on the Great Lakes between 1900 and 1939. In doing so, this study sheds light on the potential social benefits that industry trade associations may generate. It also offers a comparison of private sector successes in achieving desirable outcomes through such associations with public sector successes. Industry trade associations have been in existence for centuries, but became increasingly widespread in the early part of the twentieth century [Laurent 1992]. Usually membership required the payment of dues and regular meetings were established for the purposes of information exchange and coordination of market activities. Much of the existing scholarship on trade associations highlights the market coordination efforts of these organizations. Roberts [1926], for instance, articulated that the major benefit of any trade association is to create an environment whereby members profit by the affiliation, perhaps through coordinated supply constraints and price-fixing agreements. Indeed, in a study of price-fixing cases in the United States, Hay and Kelley [1974] reported that trade associations were involved in seven out of eight cases involving at least 15 firms. Frass and Greer [1977] found that 36 percent of all price fixing cases involved trade associations. Similarly, Posner [1970] studied all anti-trust cases in the United States and found 44 percent involved trade associations. As for Great Lakes shipping, the LCA, the largest lake shippers association in that region of the United States, was the most important institutional device for communication and market coordination among shippers [Laurent 2002].Footnote 1 As a result, it is generally accepted that trade associations can and do result in higher prices and supply restrictions that generate welfare losses in the markets in which they operate. Research by several authors indicates the activities of trade associations go beyond price and production coordination and these other activities may be welfare enhancing. Bradley [1965] describes nine different activities engaged in by trade associations. Among the most salient are standardization of products and services and establishing criteria for product quality, sponsoring research designed to improve product or service quality, and educational efforts such as the sponsoring of workshops, short courses, and clinics, for such activities as employee training and safety. Schaede [2000] documents successful efforts by a Japanese trade association representing door shutter manufacturers to introduce safety and minimum quality standards that were eventually adopted industry-wide. Evidence is mixed, however, as to the efficacy of trade association efforts to bring about socially desirable outcomes. King and Lenox [2000] investigated the Chemical Manufacturers Association's (CMA) Responsible Care program and found little evidence that firm participation in Responsible Care improved their environmental performance more than non-members. Such a result suggests sanctions levied by the CMA on participating members for failing to achieve program goals were not efficacious enough. The King and Lenox [2000] study hints that a combination of public and private efforts to influence firm behavior may be necessary, but prompts the question: Which element, public or private, offers the greatest opportunity for success? The literature on the effectiveness of public institutions vs private enterprise on economic activity and the provision of goods and services is substantial. Yet there is still much debate regarding the preference of one method over another, as well as the relative merits of each. For instance, Coase [1974] challenged the notion that lighthouses must be provided by the government. Lighthouses are a common example of a non-excludable and non-rival good that would thus be undersupplied by the private sector. Coase argued that many lighthouses were constructed and maintained via private interests. Van Zandt [1993] responded to Coase by arguing that, in the case of lighthouses, often there was a hybrid-type of system, where both public institutions and private interests were involved in lighthouse construction and operation. As to the relative efficacy of each system, there is also much debate. In an investigation of nineteenth century arctic explorations, Karpoff [2001] finds while publicly funded expeditions were better funded, they tended to meet with less success and greater disaster than privately funded exploration efforts. In contrast, Craft [1999] argues the provision of reliable weather information and forecasts for shipping on the Great Lakes in the nineteenth century was most efficiently provided by the US Weather Bureau, rather than by a group of private weather organizations. Among other reasons, this was due to the network externalities associated with the substantial startup costs of constructing and maintaining a sufficiently large system of weather stations. Moreover, in an econometric investigation of Great Lakes shipping in the 1870s and 1880s, Craft [1998] measured the effect of US Army Signal Service and Canadian Meteorological Service storm warning stations on shipping losses. His analysis indicates publicly funded weather stations, which collected and disseminated information to shippers on the Great Lakes, substantially reduced shipping losses, generating social welfare enhancing results.Footnote 2 While Craft [1998] did not consider private-sector efforts to reduce losses, the results clearly indicate that government investments in promoting economic growth can be quite effective. A theme running through much of the previous discussion is information dissemination. It is quite clear that publicly funded and managed weather stations provide information, such as weather patterns, that is valuable to shippers. Yet, as alluded to above, trade associations also provide an efficient means of information sharing that is also quite valuable to shippers. The association, in effect, can be thought of as a means of making valuable information more readily available, and easily implemented.Footnote 3 Indeed, each shipper's experiences would likely be unique as would the solutions to their problems. Thus, knowledge would be distributed among different firms within the association that would include new techniques, the use of new products, and information of unique weather or topography (i.e., weather anomalies, shifting sandbars, currents, handling certain cargos, etc.). Because much of this would be unknown if not shared, there are large externalities (spillovers) from the sharing of such information. Indeed, the type of detailed information flow facilitated by a trade association likely improves the long-term efficiency of day-to-day operations, including safety promotion, which government is unlikely to supply. In fact, as we detail below, the LCA periodically published information booklets specifically addressing guidelines to promote on-board safety operation to prevent loss of life and cargo. Many of these guidelines are too detailed for the standard legislative process and standard governmental regulation, and unlikely to be communicated via weather station information. In what follows, we reconsider Craft's [1998] model to address the role the LCA played in reducing accidental shipping losses on the Great Lakes between 1900 and 1939. Our results confirm existing research that weather information supplied through National Weather Bureau stations, a public sector effort, generally resulted in smaller accidental shipping losses. However, we also find that increases in the LCA membership reduced losses as well. This result is consistent across different measures and types of shipping losses. Indeed, our results support the idea that private sector-developed institutions, such as trade associations, can and do generate positive outcomes. Fewer shipping losses mean fewer goods, capital, and lives lost. This paper is organized as follows. In the next section, we briefly review the link between trade associations and self-regulatory safety efforts with specific attention paid to such efforts by the LCA. In the third section, we present our basic model and data. In fourth section, we discuss some salient econometric issues and in the following section we present our results. The last section concludes.",
37,4,Eastern Economic Journal,07 February 2011,https://link.springer.com/article/10.1057/eej.2010.3,Using Admission Tests to Predict Success in College — Evidence from the University of Puerto Rico,October 2011,James F Ragan,Dong Li,Horacio Matos-Díaz,Male,,Male,Mix,,
37,4,Eastern Economic Journal,28 February 2011,https://link.springer.com/article/10.1057/eej.2010.4,Inside Debt and the Stability of Inflation,October 2011,Thomas I Palley,,,Male,Unknown,Unknown,Male,"In a seminal paper published some 30 years ago, James Tobin [1975] presented a neo-Keynesian model of deflation and depression. The current paper shows how Tobin's model also provides a neo-Keynesian analysis of inflation and it can be used to understand conditions under which a demand-pull inflation process can be unstable. Inside debt effects, related to debts that private sector agents owe each other, are a feature that has become of increasing policy concern to monetary authorities in many countries — including the US, the UK, and Australia. That is because household borrowing has financed consumer spending and property booms. However, whereas inside debt effects have been extensively examined in connection with deflationary environments, they have been largely overlooked in connection with inflationary environments. The main innovations of the paper are (i) the examination of the impact of inside debt on the dynamics of inflation; and (ii) expansion of Tobin's dynamic model to incorporate borrowing, debt repayment, and endogenous money. This allows the debt stock to adjust in response to disequilibrium and introduces a distinction between price level (Fisher) debt effects and debt stock adjustment effects. The former are destabilizing, while the latter are stabilizing. The Tobin model uses a demand-pull approach to inflation. Such an approach has not been popular with Post Keynesians, who have preferred a conflict/cost-push approach. “Either — or” thinking is misplaced. Both types of inflation can be relevant depending on circumstance. In the 1970s and into the 1980s, when labor was stronger, a conflict/cost-push frame made sense. In the current period, in which it is widely acknowledged that labor is weak, inflation is better explained in terms of demand-pull. This is particularly true for the United States. In his original paper on deflation, Tobin emphasized the importance of the Tobin [1965]–Mundell [1963] effect whereby expected deflation encourages agents to hold money balances, thereby increasing money demand and real interest rates. This mechanism is also important for analysis of the inflation process. Stability again depends upon the strength of the Tobin–Mundell effect. Inflation discourages agents from holding money, which reduces money demand, lowers real interest rates, and raises aggregate demand (AD), thereby adding to inflationary pressures. The stronger the Tobin–Mundell effect, the more likely that an inflationary process will be unstable. The paper makes several analytical contributions. First, it shows how Tobin's model provides a simple framework for exploring the dynamics of inflation as well as the dynamics of deflation. Second, the model provides some Keynesian insights into the factors affecting the stability of interest rate targeting rules. Third, the paper introduces the notion of consumption and investment spending acceleration effects whereby inflation induces agents to accelerate purchases of consumption and investment goods. These acceleration effects can be viewed as the Keynesian analog of increases in the velocity of money that have been emphasized in monetarist analyses of hyperinflation [Cagan 1956]. Fourth, the paper includes inside debt in the analysis of inflation. Whereas much attention has been paid to the effect of inside debt in deflations [Fisher 1933; Tobin 1980; Caskey and Fazzari 1987; Palley 1999, 2008a, 2008b], it has been ignored in analyses of inflation [Friedman 1968; Lucas 1973; Sargent 1986]. Inside debt increases the likelihood that inflation is unstable and if unchecked could develop into high inflation. Fifth, the paper incorporates the effects of changing debt stocks brought about by borrowing during expansions and loan repayments during contractions. These effects can increase the likelihood of instability. The expenditure flow effects of new borrowing and repayments are destabilizing, but the debt stock adjustment effect of borrowing and repayment is stabilizing. Incorporating these flow — stock financial dynamics links with the Post Keynesian theory of endogenous money and brings out the macroeconomic significance of endogenous money. Finally, the paper closes with some thoughts on the significance of inside debt for the conduct of monetary policy. The model helps explain the course of US monetary policy over the past decade. Fear of deflationary debt effects helps explain why the Federal Reserve so aggressively lowered nominal interest rates in the recession of 2001 and again in the current recession that began in (2007). Inflationary debt effects help explain why the Fed raised interest rates over the period 2004–2007, and they also explain why the Fed has reason to be concerned with inflation when the economy eventually exits the current recession.",3
37,4,Eastern Economic Journal,21 February 2011,https://link.springer.com/article/10.1057/eej.2010.5,Inward Foreign Direct Investment in the US: An Empirical Analysis of their Impact on State Economies,October 2011,Kostas Axarloglou,William L Casey Jr.,Hsiang-Ling Han,Male,Male,Unknown,Male,"The recent growth in foreign direct investment (FDI) globally has led to a proliferation of scholarly efforts to study the various dimensions of this phenomenon. Two major streams of research have emerged in the literature. The first analyzes the economic factors/conditions in the host countries that attract FDI flows [Blonigen 2005], whereas the second stream studies the impact of FDI inflows in the economy of the host countries. The purpose of our paper is to add to this latter dimension of the literature. Typically, studies assess the impact of FDI inflows on the host countries’ economic growth, employment and wages. However, results are rather mixed and in some cases even contradictory [Lall 1995; Poon and Thompson 1998; de Mello 1999; Elahee and Pagan 1999; Fung et al. 1999]. At the same time, Feenstra and Hanson [1997] in a study of FDI flows in Mexico concluded that the labor market effects of FDI can be both positive and negative, whereas Zhao [1998] found that some of the favorable effects of FDI on aggregate unemployment in both industrialized countries and in developing countries are offset by the tendency of FDI to reduce union employment. On the other hand, several researchers, study the differences in the structure and operations of foreign and domestic companies in the host countries. Thus, Howenstine and Zeile [1994] find that foreign affiliates in the US are larger, more capital intensive and pay higher wages than domestic companies. Globerman, Ries and Vertinsky [1994] find qualitatively similar results for foreign establishments operating in Canada. Doms and Jensen [1996] support these findings even when they control for industry and location characteristics, the plant age and the plant size. Extending this literature, researchers study possible spillover effects between the higher wages paid by foreign plants and the wages paid by domestic plants. Aitken, Harrison and Lipsey [1996] find that, in the US, a higher level of foreign ownership in an industry and location is associated with higher wages in domestically owned plants. Also, Feenstra and Hanson [1997] reveal that FDI in Mexico accounts for more than half of the increase in skilled labor share that occurred in the country in the late 1980s. Moreover, Lipsey [2004] finds that US affiliates in the host countries tend to employ relatively more skilled workers, pay higher wages to both skilled and unskilled employees and have higher labor productivity. In a similar tone, Onaran and Stockhammer [2008], employing FDI inflows data in various Central and East European countries for 2000–2004 find that FDI inflows boost wages only in capital and skill intensive industries. However, very little has been done in evaluating the effects of the operation of foreign plants on the local economies of the US states receiving the FDI flows. Figlio and Blonigen [2000] focus on South Carolina, and, by using county level data, find that manufacturing employment by foreign plants has a strong positive impact on county and industry-specific wages. Also, the entry of an average-sized foreign company increases real wages for all workers in the specific county and industry by much more than the entry of a similar domestic company. Our study extends this literature in several dimensions. First, we focus on the effects of FDI inflows on industry and state-specific wages and employment (labor market effects) in the 21 US states that were the top recipients of FDI inflows between 1974 and 1994. Second, our data are also broken down to all 20 2-digit Standard Industrial Classification (SIC) industries in manufacturing operating in each of the states in the sample. Third, our data allow for a direct evaluation of the impact FDI has on some crucial aspects of economic activity in the local economies in the US states. Finally, our results reveal significant heterogeneity of the labor market effects of FDI inflows across the US states that is industry-specific and thus lead to important policy recommendations. The paper is organized as follows. FDI and the labor market section provides a theoretical foundation for testing empirically the impact of FDI on local employment and wages. Data section describes the data in use, whereas sections Empirical results and Lessons from the empirical results present our empirical analysis and discuss the empirical findings in detail. In Policy implications section, we discuss the policy implications of our findings. Finally, we conclude in Conclusions section.",3
37,4,Eastern Economic Journal,07 February 2011,https://link.springer.com/article/10.1057/eej.2010.6,The Immigrants' Odds of Slipping into Poverty: Double Jeopardy?,October 2011,Jongsung Kim,Edinaldo Tebaldi,,Unknown,Unknown,Unknown,Unknown,,
37,4,Eastern Economic Journal,07 February 2011,https://link.springer.com/article/10.1057/eej.2010.7,Measuring Fishing Capacity When Some Outputs Are Undesirable,October 2011,Rolf Färe,James E Kirkley,John B Walden,Male,Male,Male,Male,"There is a long and rich history of research on economic aspects of undesirable outputs such as pollution. Pigou [1932] recognized pollution as a negative externality, which imposed costs on society, and suggested direct taxes as a way to reduce the externality. Coase [1960] argued that with no transaction costs and readily available information, polluters and those who were affected by the pollution could come to a mutually beneficial agreement on use of a resource, and hence there would be no need for Government intervention in the form of taxes, as proposed by Pigou. This led to further inquiry on how a firm could internalize the cost of reducing undesirable outputs to desired levels (e.g. surcharges). Bubbis [1963] demonstrated that surcharges substantially reduced waste discharges from industry. Kneese and Bower [1968] concluded that surcharges encourage firms to make changes resulting in reductions in the volume of effluent, and surcharges may actually lower production costs over time. Ethridge [1973] developed an economic theory of the firm that specifically incorporated aspects of reducing undesirable outputs. Ayers and Kneese [1969] considered the appropriate level of pricing undesirable outputs within a general equilibrium framework. The research of the 1960s and 1970s, however, did not explicitly attempt to assess technical efficiency, economic efficiency, or capacity adjusted for undesirable outputs. Although Ethridge [1973] provided a basic framework for including undesirable outputs in the theory of the firm, Pittman [1983] and Färe et al. [1989a] offered more formal quantitative methods for estimating economic and technical efficiency, respectively, in the presence of undesirable outputs. Both of these approaches offered a framework for assessing productivity and efficiency when some outputs are undesirable and cannot be freely or costlessly disposed. Our focus is on the measurement of fishing capacity when there are undesirable outputs, usually referred to as bycatch or discards (here we will refer to both as discards), although we note that these models can be applied to most industries where pollution may be part of the production process. Both the measurement of fishing capacity and the reduction of discards have received worldwide attention for the last 15 years. Alverson et al. [1994] provided a comprehensive assessment of the levels of discards and options for reducing them in fisheries. They found that mortality from unintended catch was significant worldwide, and concluded that one way to reduce that mortality would be to reduce fishing capacity, and the overall level of fishing effort. Since discards generally do not survive, their loss represents a cost to society in two ways. First, they are not available to the discarding vessel, or another vessel, for harvest at a future point in time. Second, they represent lost reproductive potential for the species in question. Therefore, if vessels can decrease their discards while maintaining, or increasing, their level of catch, it would represent a societal gain because the fish would be available at a later point in time for harvest, and could also have a chance to reproduce and sustain the population over time. During the period that there was concern raised over discards, the Food and Agriculture Organization of the United Nations, and member nations facilitated a wide range of research to estimate capacity in commercial fisheries [Pascoe et al. 2004]. However, there was no effort to address the assessment of capacity while accounting for discards in an integrated and comprehensive manner.Footnote 1 Failing to account for discards may inflate capacity estimates because the cost of discarding is not explicitly considered. Additionally, results that account for discards will yield insight about the vessels ability to reduce discards while simultaneously increasing desirable outputs. Färe et al. [2006] offer an empirical analysis of technical efficiency in a fishery adjusted for undesirable outputs. Scott et al. [2007] provide an example of estimating capacity in a fishery with undesirable outputs, with the analysis restricted to observations obtained from experimental trips. In this study, we extend the analysis of Färe et al. [2006] in four ways. First, we estimate capacity rather than technical efficiency while explicitly incorporating discards in our estimates. Second, we use trip-level data over a 3-year time period, rather than tow-by-tow data. Third, we invoke two measures, a directional and hyperbolic distance function, and then utilize different assumptions about the directional vector used in the directional distance function approach, which was not done in the Färe et al. [2006] analysis. We find considerable similarities between the two approaches that allow desirable outputs to expand and undesirable outputs to contract. Finally, we analyze the capacity bias from not accounting for bycatch, and find a large difference in capacity estimates. Our results also differ from those of Färe et al. [2006] in that we find it is difficult for fishing vessels to both contract undesirable outputs, and increase desirable outputs. Our findings suggest that when producers are forced to reduce undesirable outputs, the production of desirable outputs can only be increased by an average of 11.5 percent per trip, rather than 28 percent as shown in the previous study. The next section describes the methods used to estimate capacity in the presence of undesirable outputs; the section after that describes the Georges Bank fishing vessels used in this study, and describes the data; the subsequent section compares the capacity estimates using each of the approaches shown in the second section; the final section concludes.",8
37,4,Eastern Economic Journal,15 September 2011,https://link.springer.com/article/10.1057/eej.2010.8,"Theoretical Foundations of Law and Economics, by Mark D. White",October 2011,James R Wible,,,Male,Unknown,Unknown,Male,,
37,4,Eastern Economic Journal,15 September 2011,https://link.springer.com/article/10.1057/eej.2010.9,"Money, Enterprise and Income Distribution: Towards a Macroeconomic Theory of Capitalism, by John Smithin",October 2011,Ajit Zacharias,,,,Unknown,Unknown,Mix,,
38,1,Eastern Economic Journal,21 March 2011,https://link.springer.com/article/10.1057/eej.2010.48,Tournament Chasing NASCAR Style: Driver Incentives in Stock Car Racing's Playoff Season,January 2012,J Brian O'Roark,William C Wood,Benjamin Demblowski,Unknown,Male,Male,Male,"When driver Matt Kenseth claimed stock car racing's championship after winning only one of 36 races in 2003, racing and television network executives felt that action had to be taken. Kenseth's competent style, finishing in the top 10 a total of 25 times, gave him the championship on points but caused fan interest and network ratings to lag. Drivers went through the motions of the last race knowing that Kenseth had already locked up the championship. In fact, in the 5 years preceding Kenseth's 2003 championship, the champion had been mathematically certain before the final race four times. The response from the National Association for Stock Car Auto Racing (NASCAR) was the Chase for the Championship. The season's first 26 races would be used to determine the top 10 drivers in points. These drivers would then become the chasers, running the final 10 races to determine the overall championship.Footnote 1
 When the Chase begins, drivers are segmented into two strata. Those who have qualified for the Chase are grouped together, and those drivers who do not make the Chase are merged into a second group. The incentive effects for these two groups are quite different even as the groups compete in the same events. For the Chase drivers, winning races is nice, but point accumulation still determines the overall championship, and therefore not finishing a race due to a crash or mechanical failure can greatly diminish a driver's chances of becoming the champion. On the contrary, while all drivers in a race win some prize money, the non-Chase drivers’ point accumulations become nearly meaningless. There is a 1-million dollar bonus for the driver who finishes first among the non-Chase drivers, but in the early years of the Chase even this was usually settled well in advance of the final race. For the vast majority of non-Chase drivers, winning is more important because this is the surest way to increase their financial standing; however, non-Chase drivers are also trying to demonstrate to their current team or future employers that they can win, or at least perform on a high level worthy of a contract for the next year. Thus, the Chase drivers should be more concerned about consistency, whereas the non-Chase drivers should be more willing to take risks. In the parlance of tournament theory we have a mixed tournament. A caveat, however, is in order. Compared to a more conventional tournament, whether in sports or CEO pay, the risks inherent in motor sports are substantially different. There are significant externalities, up to and including death. Therefore, the tendency toward increased effort in events with larger prize differentials, as predicted by tournament theory, may be absent. In addition, as opposed to golf or tennis, getting along with competitors on the field of play — by this we mean not causing competitors to wreck to gain an advantage — differentiates driver behavior from other sports tournaments. In golf and tennis, the objective is to win. In so doing there are few, if any, opportunities to physically harm opponents. In NASCAR, hurting opponents is a very real possibility. The sport essentially amounts to an infinitely repeated game in which reputation matters significantly. The rules tell the drivers: If you do not play nice, not only will you hear it from your fellow drivers, but the officials can prevent you from entering a race.Footnote 2
 The purpose of this paper is to examine the incentive effects imposed on drivers during the first 3 years of NASCAR's Chase for the Championship playoff structure. We utilize a dataset of individual driver outcomes in each race during the years 2004 through 2006. We test the model in a number of ways to establish its robustness and reach a consistent conclusion that the Chase has created an environment that generates additional wrecks.",3
38,1,Eastern Economic Journal,21 March 2011,https://link.springer.com/article/10.1057/eej.2010.49,Financial and Product Market Integration under Increasing Returns to Scale,January 2012,Lei Wen,Haiwen Zhou,,,Unknown,Unknown,Mix,,
38,1,Eastern Economic Journal,21 March 2011,https://link.springer.com/article/10.1057/eej.2010.50,"Trade, Environment, and Welfare in a Model of Monopolistic Competition",January 2012,Monica Das,Sandwip K Das,,Female,Unknown,Unknown,Female,"Recent studies on the trade-environment nexus focus on two issues. The first is the concern that trade liberalization is likely to increase pollution emissions. The second is related to the debate concerning the effect of environmental regulations on international competitiveness.Footnote 1 Both issues are closely related to the widely debated “pollution haven” hypothesis.Footnote 2 Comparative advantage is the basis for trade in models that assume competitive or oligopolistic market structure, and environmental policy may affect a country's comparative advantage. However, there is plenty of evidence that intra-industry trade in differentiated goods between the developed and the developing countries is quite substantial. This kind of trade is not based on the concept of comparative advantage but is a result of imperfect competition and economies of scale.Footnote 3 Most of the empirical studies on this question apply an index developed by Grubel and Lloyd [1975] or some variation of this index to measure intra-industry trade. Hellvin [1994] shows that intra-industry trade between China and OECD countries increased by about 66 percent between 1980 and 1992. Further studies by Hu and Ma [1999] establish certain interesting patterns in trade between China and its major trading partners: their estimates of country-specific and industry-specific vertical as well as horizontal intra-industry trade show that the pattern is similar to that of the developed countries. One may see similar results in Okubo [2007], who demonstrates that the volume of intra-industry trade in Korea, China, and Taiwan, Thailand and Philippines is fairly high and comparable to that of the UK, France, Germany, USA or Canada. Trigo [2002] finds a substantial amount of intra-industry trade between the European Union and the Central and Eastern Europe Countries as well as Mediterranean Partner Countries. While the element of product differentiation in North–South trade is clearly established by Ballance et al. [1992], Lovely and Richardson [1998] estimate North–South intra-industry trade in differentiated and skill-intensive intermediate goods (horizontal trade) as well as in intermediates for finished manufactures (vertical trade) and show that increased US trade with newly industrializing countries is associated with increased reward to skill and reduced reward to pure labor, consistent with heightened wage inequality and distributional conflict. Finally, Clark and Stanley [1999] estimate the Grubel–Lloyd index of intra-industry trade between developing countries and the US and show that it rises with size and trade orientation of the developing countries. Though a large number of ‘South’ countries, particularly the newly industrializing countries, produce and export differentiated goods, there are also resource-rich ‘South’ countries whose export basket is dominated by raw materials. The OPEC members and many other countries are mineral exporters and major importers of differentiated goods. There are two modeling approaches to intra-industry trade. The first is the love-of-variety approach by Krugman [1979, 1980], Dixit and Norman [1980], Spence [1976] and Dixit and Stiglitz [1977]. Lawrence and Spiller [1983] model both inter- and intra-industry trade within the same framework. The second is the specific preferences approach by Lancaster [1979] and Helpman [1981]. Some attempts have been made to extend the love-of-variety approach to discuss issues related to trade and the environment. Rauscher [1997] discusses the effect of environmental policy on outputs and number of varieties, while Haupt [2000, 2006] deals with consumption externalities and shows that stricter regulations, shift resources from non-environmental R&D to environmental R&D. The love-of-variety approach treats the number of varieties as an endogenous variable determining welfare, along with the quantity of every variety purchased by the consumers. If this approach is adopted, a stricter environmental regulation may affect wage rates, prices and number of varieties produced in a country. It is possible that some varieties are shifted to countries with weaker regulations, which would be one version of the pollution haven hypothesis. But what is the net effect of a stricter regulation on welfare? It seems likely that a country with stricter regulation gains because more varieties are imported, local varieties are exported at better terms of trade, and pollution decreases. Under certain conditions the country with a weaker regulation may also gain if the variety effect is sufficiently strong and the citizens are willing to accept an increase in pollution. The literature does not adequately deal with these welfare issues. The paper addresses these issues and shows that from North's point of view there are certain benefits of an environmental policy that may outweigh the costs. An optimally chosen pollution tax directly hurts but indirectly benefits South. Indirect welfare effect of a tariff is similar to that of a pollution tax. However, a positive optimum tariff may not exist in which case North's optimum policy would be free trade.",3
38,1,Eastern Economic Journal,21 March 2011,https://link.springer.com/article/10.1057/eej.2010.51,Real Estate Continuing Education: Rent Seeking or Improvement in Service Quality?,January 2012,Benjamin Powell,Evgeny Vorotnikov,,Male,Male,Unknown,Male,"Real estate agents are subject to occupational licensure laws in all 50 US states.Footnote 1 Over the past 30 years every state except New Jersey has amended its licensing laws to require real estate brokers and sales people to complete some form of continuing education requirements prior to renewing their licenses. Massachusetts was the most recent state to adopt a continuing education requirement, and its new law took effect in 1999. Continuing education's stated goal is to improve the quality of service to consumers. But it also may have the effect of limiting competition and raising wages by decreasing the number of active real estate agents. Milton Friedman's observations regarding occupational licensure nearly 50 years ago still ring true today:
 the pressure on the legislature to license an occupation rarely comes from the members of the public who have been mulcted or in other ways abused by members of the occupation. On the contrary, the pressure invariably comes from members of the occupation itself [1962, p. 140].Footnote 2
 Realtors have long maintained that continuing education would improve customer service. In 1978, the National Association of Realtors’ (NAR) Committee on Education and License Law, developed guidelines for the state real estate commissions. The guidelines claimed that the goal of continuing education was “to provide licensees with opportunities for obtaining necessary information which will enable them to conduct real estate negotiations and transactions in a legal and professional manner in order to better protect client and public interest” [National Association of Realtors 1981b, p. 3]. The NAR claimed that continuing education,
 insures that licensees are exposed to recent developments in the real estate industry; provides an awareness of new provisions or rules for the regulation of the business and the licensee and, in so doing, affords a protection the public might not otherwise have; leads to a greater recognition by the public that members of the industry are to be considered professionals; and causes licensees, formerly licensed under modest educational requirements, to maintain higher standards of proficiency or face loss of their licenses [Real Estate Today 1980, p. 14]. The NAR's advocacy led 49 states to adopt a continuing education requirement.Footnote 3 The NAR is the third-largest donor to political campaigns with the biggest Political Action Committee and “lobbies members of Congress and the administration on virtually every issue facing business” [Center for Responsive Politics 2009]. So perhaps it should be unsurprising that their strong advocacy predicated on improving customer service led to widespread adoption of continuing education requirements. In Massachusetts the main push for a continuing education requirement began in 1990 when the NAR's state-level affiliate, the Massachusetts Association of Realtors (MAR) began lobbying for it. The requirement was signed into law in 1996 by Governor William Weld, who had received a contribution of $5,000 from NAR during the 1996 election cycle [Center for Responsive Politics 2009]. According to The Boston Herald, “The Massachusetts Association of Realtors spent six years pressing the Legislature to adopt a continuing education bill, and MAR President Laura Shifrin was pleased to prevail. This law will raise the level of professionalism within the real estate industry and help ensure quality representation for consumers relative to home buying and selling, ‘stated Shifrin’” [Matte 1996]. There are good reasons to question whether continuing education requirements will do anything to enhance “professionalism” or “ensure quality representation.” Courses do not require exams at their completion. As a result real estate agents can just sit through a lecture passing time while typing on their Blackberrys or doing some other activity.Footnote 4 However, this is not to say that continuing education requirements have no effect. Many people who obtain a real estate license eventually no longer pursue a real estate career full time. Some work on a regular, part-time basis. Others maintain their license by sending in nominal renewal fees every couple of years but rarely use their license. These people might occasionally list a home for a friend or simply act as their own real estate agent when purchasing or selling their own home. These agents might go a year or more without making a single commission but then do one deal and earn a few thousand dollars. When Massachusetts’ continuing education requirement took effect in 1999, all real estate agents were required to complete 12 h of Board-approved continuing education training in the 24 months prior to each renewal in order to maintain the active practicing status of their real estate licenses.Footnote 5 Those who do not complete the education but wish to renew their license are placed on “inactive status” where they cannot represent themselves or other clients but can only earn referral fees by referring the business to an agent with an active license. The cost, in terms of both time and money, of 12 h of continuing education is unlikely to cause many full-time realtors to exit the industry or to deter others from entering. However, for those agents who only occasionally make a sale, the barrier may encourage them to go into inactive status. If this is the case, by pushing part time and infrequent agents into inactive status, full-time real estate agents, who are the vast majority of members of the National and Massachusetts Associations of Realtors, could enhance their own income.Footnote 6
 We examine data from Massachusetts to investigate whether continuing education has improved the quality of service as the MAR claimed it would, or, if the effect has been to decrease the number of active agents and enhance the income of those who remain. If a decrease in the number of active real estate agents is accompanied by an increase in their income with no corresponding improvement in quality, then the efforts of the Realtor Associations should be interpreted as rent seeking activity intended to capture the regulations for their own benefit rather than a public spirited attempt to improve industry quality of service as they claimed. The following section reviews some of the previous occupational licensure literature with a focus on real estate licensing and continuing education requirements. The subsequent section outlines our data. The penultimate section contains our main contribution which presents our empirical methodology and discusses our results. The final section concludes.",4
38,1,Eastern Economic Journal,14 March 2011,https://link.springer.com/article/10.1057/eej.2010.52,Ethnic Social Networks and Self-Employment of Immigrant Men in the US,January 2012,Maude Toussaint-Comeau,,,Female,Unknown,Unknown,Female,"Self-employment has traditionally been and continues to be an important vehicle by which some immigrant groups have increased their wealth and socioeconomic mobility [Olson et al. 1996]. Immigrant small businesses have to a large extent fueled the growth experienced by the (ethnic/minority) small business sector in the US. For example, according to the Ewing Marion Kauffman Foundation, immigrants outpaced native-born Americans in new business start-ups: Immigrants had an entrepreneurial index activity rate that increased from 0.37 percent in 2006 to 0.46 percent in 2007, while that of the native born remained constant at 0.27 percent over the same period.Footnote 1 In 2008, the index for immigrants increased by 0.53 percent, widening further the gap in entrepreneurial disposition. Self-employment is an important vehicle by which some immigrant groups have been able to increase their income and wealth and speed up their economic assimilation, contributing toward lowering the earnings gap with natives [e.g., Light 1972; Bonacich and Modell 1980; Fairlie and Meyer 1996; Olson et al. 1996; Lofstrom 2002]. The growth of the immigrant business sector underscores the importance of research on the topic. A notable feature of immigrant self-employment is the high level of concentration of the businesses in distinct industries and locations in the US. These immigrant geographic and economic concentrations are referred to as ethnic economies and ethnic enclave. More precisely, “ethnic economy,” a term first introduced by Bonacich and Modell [1980], refers to a concentration of immigrants based on members’ shared involvement in a business sector or niche, which may or may not be within an ethnic enclave. It is characterized by a situation where an immigrant or ethnic minority group produces distinct goods in which they have market power, and where the members of the group have ownership stake and hire co-ethnic employees. An “ethnic enclave,” a term coined by Wilson and Portes [1980], is a type of ethnic economy, except that it is defined by a geographic clustering of these activities.Footnote 2
 These communities and economic niches are interesting places to study the dynamics of ethnic social networks, that is, contacts or institutions that facilitate, potentially, the immigrants’ incorporation in the economy. These immigrant communities tend to be relatively cohesive social units — often with a common language, culture, and religion. Especially for immigrants who are relative newcomers to these communities, who may be in need of getting information on job opportunities or business opportunities (as well as on housing, schooling for their children and other needs), these institutions are particularly useful [Munshi 2003]. Ethnic social networks could encompass the overall social fabric and economic structure of an immigrant community. As explained by Aldrich and Waldinger [1990, p. 127], “ethnic social structures consist of the networks of kinship and friendship around which ethnic communities are arranged, and the interlacing of these networks with positions in the economy (jobs), in space (housing), and in society (institutions).” Within these communities, ethnic institutions may encourage business formation in several ways. They may channel information flows to co-ethnic members about business and funding opportunities.Footnote 3 Especially for immigrants with less transferable labor market skills, ethnic networks reinforced through ethnic enclaves may serve as a “pulled” factor for self-employment, an alternative that helps mitigate some of the costs associated with full integration in the labor market [Aldrich et al. 1985; Chiswick and Miller 2002]. Economists are increasingly incorporating network or information spillover effects into their theoretical models to explain various human behaviors, ranging from welfare participation, fertility, crime, and education [see, e.g., Case and Katz 1991; Glaeser et al. 1996; Borjas 1995; Bertrand, Luttmer, and Mullainathan 2000; Munshi 2003; Topa 2001]. Surprisingly, much less has been written on ethnic networks and self-employment of immigrants. Some of the notable exceptions addressing the question of ethnicity and self-employment have focused solely on the geographic concentration, as a measure of ethnic enclaves [e.g., Borjas 1986; Chiswick and Miller 2002; Lofstrom 2002]. In this paper, we contribute to the spate of economic research on ethnic interactions and individuals’ choices, and expand upon the notion of ethnic enclaves in determining the self-employment decision process. We conduct an empirical analysis to assess the effect of ethnic networks, which we measure by considering both within ethnic immigrant geographic enclaves effect and the self-employment intensity of groups, as a proxy for potential information flow regarding self-employment. Our empirical setup addresses these specific questions/hypotheses: Are individuals more likely to be self-employed if they live in a neighborhood in which others from their same ethnic group are segregated? Taking the average self-employment rate of a group as an indicator of the “potential information flow,” do individuals in ethnic groups that have a higher average self-employment rate exhibit greater self-employment sensitivity to the number of other people from their group who live in their neighborhood? We test how the sensitivity of the network proxy varies across ethnic groups. We also test for the alternative hypothesis of local area fixed effects. Our findings are consistent with the prediction of a network effect model. We find that ethnic networks have a positive and significant effect on the probability of being self-employed, more so for individuals in groups who have higher self-employment intensity. This is consistent with a pulled factor framework. We investigate further potential sources of influence of ethnic networks and found that as a source of strength for self-employment, networks are more important for immigrants who have been in the US for a longer period of time, but who are less educated and not fluent in English (a finding consistent with a push factor framework). Alternative measures of ethnic networks are also considered. Namely, we use the interaction between language group geographic concentration and self-employment intensity, consistent with previous research that has shown that the use of mother tongue is a determinant of ethnic identity [Alba 1990]. This additional conceptualization of ethnic networks allows us to tease out other interesting intricacies of networks. For example, we are also able to address the possibility that English fluency could affect the influence of enclaves on immigrant entrepreneurs. We found that immigrants who speak English perform better with respect to language network effects than those who do not [Mora and Davila 2005]. Overall, network relationships based on country-of-origin identification works slightly more strongly than language commonality-based networks. To gain even further insights into how ethnic networks operate, we use a measure of networks, which is the interaction between country origin geographic density and industry self-employment intensity. The network effect is stronger for individuals in groups that have distinctive industry niches. The remainder of the paper is organized as follows. In the next section, we conduct a brief review of the sociology and economic literature, including a brief presentation of theoretical background. The subsequent section discusses our measure of ethnic networks and our interpretation of this measure in this paper. The fourth section presents the methodological framework and the data. The penultimate section presents the data, the empirical results, and various robustness tests. The final section summarizes the paper and discusses the policy implications of the findings.",5
38,1,Eastern Economic Journal,21 March 2011,https://link.springer.com/article/10.1057/eej.2010.54,The Success and Failure of Counseling Agency Debt Repayment Plans,January 2012,Daniel T Brown,Charles R Link,Michael E Staten,Male,Male,Male,Male,"Millions of American consumers seek advice and assistance from a credit counseling organization each year. Upwards of one-third of these consumers enroll in voluntary repayment plans, called Debt Management Plans (DMPs), as an alternative to bankruptcy. Counseling agencies broker these unique plans by getting creditors to voluntarily exercise mutual forbearance in the form of concessions on finance charges and repayment terms, a halt to late fees and collection calls, and a re-aging of accounts to “current” status while the consumer is on the repayment plan. But, the majority of DMPs terminate prior to completion. Creditors worry that some borrowers opportunistically enroll (with the tacit approval of counseling agencies that act as screeners) just to get a temporarily lower interest rate with no intention of sticking with a plan to its conclusion. The moral hazard risk has contributed to creditor reluctance to make deep concessions on plans. But, a DMP can be a far less costly debt relief option for many consumers than either bankruptcy or debt settlement.Footnote 1 Creditor reluctance to make concessions leaves many consumers unable to qualify for a beneficial DMP, as will be explained below. This paper investigates whether success on a DMP, as measured by the amount of original debt repaid, can be predicted at the time of counseling based on observable client and debt attributes. One objective of building such a model is to identify the impact of creditor concessions on DMP participation and completion. In addition, a predictive model accessible to both counselors and creditors could be used to move the industry away from the typical one-size-fits-all DMP and toward a system in which creditors are more willing to make deeper concessions for those consumers with a demonstrated greater need. A predictive model could also help to boost DMP completion rates by giving agencies a tool to make operational adjustments to allocate more resources to clients who are likely to need extra assistance as they work through their repayment plans. We are not aware of any prior econometric studies of the determinants of DMP payment experience. With more than 1.5 million households projected to file for bankruptcy in 2010, at the same time that credit card chargeoffs for the largest issuers have soared about 10 percent of outstanding balances, there is clear need for creditors and counseling agencies to reduce the barriers to making DMP products available to a wider segment of consumers in order to prevent bankruptcies and lower losses.Footnote 2 This paper examines the factors that determine the repayment of debt through a DMP using data on over 17,000 consumers who were counseled and recommended for a DMP by a large non-profit credit counseling agency during 2003. The objective is to identify attributes observable at the time of counseling that predict which consumers will be more likely to do well on DMPs, among the pool of clients for whom a counselor recommended a DMP at the end of the initial counseling session. The paper is organized as follows. We first provide background on the DMP and the resulting partnership between consumers, counselors, and creditors. Then a brief literature review regarding the potential determinants of DMP success is given. The methodology underlying the research is then discussed. Next, the data used in the empirical models are described. Regression model estimates of DMP repayment are subsequently analyzed. We then acknowledge and explore sample selection issues and provide additional insight into the effectiveness of the counseling agency's screening process. Finally, we offer concluding thoughts.",4
38,1,Eastern Economic Journal,21 March 2011,https://link.springer.com/article/10.1057/eej.2010.56,The Relationship between Medicare Supplemental Insurance and Health-care Spending: Selection Across Multiple Dimensions,January 2012,David M Zimmer,,,Male,Unknown,Unknown,Male,"Medicare, the primary health insurance program for US seniors, imposes cost containment mechanisms designed to curb over-consumption of health-care services. The most important of these mechanisms is a cost sharing arrangement that requires seniors to pay some expenses out-of-pocket. However, cost sharing imposes potentially significant financial risks on some seniors. To protect against those risks, most Medicare beneficiaries have some form of supplemental insurance coverage. Supplemental coverage might enhance overall welfare by reducing, or eliminating, financial risks [Finkelstein and McKnight 2008], but supplemental coverage also short circuits Medicare's primary cost containment mechanism, potentially contributing to overuse of health-care services and escalating health-care spending [Atherly 2002]. A better understanding of the relationship between supplemental insurance and Medicare spending is crucial to fiscal policy. In 2008, Medicare passed an unwelcome milestone when the program, for the first time, spent more than it collected in taxes. Without changes in policy, the program is expected to become insolvent by 2019.Footnote 1 If supplemental insurance, by impairing Medicare's cost-containment mechanisms, has contributed to escalating spending and attendant prospects of insolvency, then policymakers may wish to consider corrective actions. One option would be to impose regulations that encourage supplemental policies to reduce first-dollar coverage in favor of increased catastrophic coverage. The classic economic model of insurance posits that individuals with higher expected claims buy more insurance [Rothschild and Stiglitz 1976]. In Medicare supplemental insurance markets, however, the direction of these so-called “selection effects” is less clear. Existing studies produce results that range from findings of adverse selection in which high-risk seniors choose coverage [Wolfe and Goddeeris 1991; Cartwright et al. 1992; Long 1994; Fox et al. 1995; Ettner 1997; Finkelstein 2004] to findings of favorable selection in which low-risk seniors choose coverage [Del Bene and Vaughan 1992; Short and Vistnes 1992; Vistnes and Banthin 1997/1998; Cutler et al. 2008] to findings of zero selection in which expected claims are not related to insurance demand [McCall et al. 1991]. A potential reason for inconsistent results is that selection may occur across multiple dimensions [De Meza and Webb 2001]. For example, selection may be favorable with respect to health, in that healthy seniors are more likely to enroll in supplemental coverage, but selection may be adverse with respect to risk attributes, in that more risk averse seniors tend to choose supplemental plans, and, conditional on health status, more risk averse seniors might be heavier consumers of health care. Failure to account for selection across multiple dimensions may render findings difficult to interpret. This paper contributes to the supplemental insurance literature in several respects. First, estimates from a large, nationally representative household survey of seniors provide an up-to-date assessment of the relationship between supplemental insurance and health-care spending. Second, the paper investigates selection into supplemental insurance across multiple dimensions, including risk, health, insurance availability, and socioeconomic traits. Thus, rather than treating selection as a one-dimensional problem, this paper is able to separate heterogeneous selection across different observed traits. Most importantly, the empirical models include detailed information on health status and attitudes toward risk, as adverse or favorable selection, to the extent that it exists, likely derives from these characteristics. Third, the empirical implementation considers employer-provided supplemental coverage separately from official “Medigap” policies, as the two may differ substantially [Atherly 2002]. Fourth, separate models are estimated for prescription drug spending, as previous evidence suggests that selection into plans that offer drug coverage may differ from selection into other plans. Finally, this paper attempts to address the larger public policy issue concerning whether supplemental Medicare policies influence overall Medicare outlays by the government. This paper shares important similarities with a recent paper by Fang et al. [2008]. Like their paper, this paper seeks to identify selection effects across different directions, and for the most part, this paper corroborates their findings. However, this paper also presents some important differences with regard to magnitudes of selection effects. As discussed in greater detail below, some of these discrepancies likely stem from differences in data sources between the two papers.",
38,1,Eastern Economic Journal,12 December 2011,https://link.springer.com/article/10.1057/eej.2010.10,"Violence and Social Orders: A Conceptual Framework for Interpreting Recorded Human History, by Douglas C. North, John Joseph Wallis and Barry R. Weingast",January 2012,Solomon W Polachek,,,Male,Unknown,Unknown,Male,,
38,1,Eastern Economic Journal,12 December 2011,https://link.springer.com/article/10.1057/eej.2010.11,"Adam Smith, by Gavin Kennedy",January 2012,Tom Birch,,,Male,Unknown,Unknown,Male,,
38,1,Eastern Economic Journal,12 December 2011,https://link.springer.com/article/10.1057/eej.2010.15,"Economic Development and Transition: Thought, Strategy, and Viability, by Justin Yifu Lin",January 2012,Hasan A Faruq,,,Male,Unknown,Unknown,Male,,
38,2,Eastern Economic Journal,21 March 2011,https://link.springer.com/article/10.1057/eej.2010.59,Political Ideology and Economic Freedom Across Canadian Provinces,February 2012,Christian Bjørnskov,Niklas Potrafke,,Male,Male,Unknown,Male,"Disentangling the complex relationship between political ideology and economic policies is a fundamental issue in political economy. Comparing ideological positions across countries, however, entails substantial difficulties. Empirical findings on ideology-induced economic policy-making — for example, government spending, taxation, debt accumulation, social policies, and policy interventions in specific markets — are mixed [Hibbs 1977; Cusack 1997; Tavares 2004; Schneider 2010; Imbeau et al. 2001; Potrafke forthcoming, a]. As a solution to the methodological problems of cross-country ideology comparison, a series of studies have explored ideological differences across the US states [e.g., Berry et al. 1998; Reed 2006]. But it remains as an open question whether the US findings generalize to other institutional settings and political traditions. We therefore explore the potential ideological differences across Canadian provincial governments and parliaments, taking specific measurement problems of political ideology into account. Previous studies exploring economic policies in the Canadian provinces suggest that, for example, leftwing governments pursued more expansionary fiscal policies by increasing public expenditures while public revenues appear to be less driven by government ideology. Exploring ideology-induced effects in fiscal policy is meaningful because the Canadian constitution and federal politics leave provincial governments with significant room to maneuver.Footnote 1 In a similar vein, labor market regulations and legislation are also primarily the responsibility of provincial governments instead of being governed from the federal level. Yet, the influence of government ideology on labor market regulation and economic liberalization in general has been largely ignored in the political economy literature. Against the background of the sustained interest in the role of political ideology in Canadian economic policy, this is a surprising omission, not least since the few cross-country studies exploring government intervention and liberalization tend to find evidence of ideology-induced effects. The ideological structure of provincial parliaments and the variation across different factions and parties plays a significant role in Canadian politics. Contrary to US politics the Canadian political system is most often characterized as closely fitting a “brokerage model.” Only considering the ideology of the party in government, as previous studies have done, does not provide a comprehensive portray of political ideology and economic policies in the Canadian provinces. Neither does a static characterization of party positions, as political parties tend to shift positions on a left-to-right scale over time. Political economy studies on partisan politics in the Canadian provinces may hence benefit from addressing parliamentary cohesion and party changes. In this paper, we consequently examine how government and parliament ideology has influenced economic freedom across the Canadian provinces, taking the challenge of measuring ideology into consideration. We analyze the data set of economic freedom indicators compiled by the Fraser Institute in 10 Canadian provinces over the 1981–2005 period and introduce two different indices of political ideology: government and parliament ideology. We allow for parties to drift on the left-to-right scale. The results suggest that government ideology influenced labor market reforms: market-oriented governments promoted liberalization of the labor market but parliamentary ideology did not influence economic liberalization at all. At a more disaggregated level, we find that the ideological effects on the labor market are due to more leftwing governments increasing public employment. These findings (1) identify differences between leftist and rightwing governments concerning the role of government in the economy and (2) indicate that ideological polarization concerns governments but less so parliamentary fractions in the Canadian provinces. The rest of the paper is organized as follows: The next section discusses the related literature on partisan politics in Canadian provinces and formulates the hypotheses to be tested. The subsequent section presents our indices on political ideology in the Canadian provinces. The fourth section presents the data on economic freedom and specifies the empirical model. The penultimate section reports and discusses the estimation results, and investigates their robustness. The last section concludes.",44
38,2,Eastern Economic Journal,14 March 2011,https://link.springer.com/article/10.1057/eej.2010.66,"Risk Aversion, Business Volatility and Exchange Rate Regimes in Small Open Economies",February 2012,Cristian Pardo,,,Male,Unknown,Unknown,Male,"The differences in output fluctuations in emerging markets relative to developed economies have been a topic widely studied by economists. For instance, Prasad et al. (n.d.), among many others, provide evidence supporting the fact that output volatility is on average much higher than in industrial economies.Footnote 1 At the same time, economists have developed models to help explain this fact. In many cases, these models start from stylized facts typically observed in emerging markets. For instance, Calvo and Reinhart [2000] and Chang and Velasco [2000] focus on the role of “dollarized liabilities” as a vehicle towards financial instability in emerging markets. Namely, real exchange rate depreciations negatively impact firms’ and banks’ balance sheets by increasing the value of outstanding debt but not the value of revenues denominated in local currency. The fact that emerging markets tend to present a relatively larger privately held rather than corporate sector is another stylized fact for emerging economies. Market capitalization as a fraction of GDP, for instance, is about 100 percent or more in high income countries, while only 35 percent or less in low and middle income countries [The World Bank 2010]. The fact that privately owned firms tend to rely on debt instead of securities to finance their investments (in contrast to the corporate sector) may play an additional role in creating frictions in emerging economies. In this line of thought, Bernanke et al. [1999] study the role of information asymmetries in the borrower–lender relationship in creating a risk premium that is endogenous to banks’ and firms’ balance sheets. This external finance premium is able to cause stronger output responses to shocks. In addition, Cespedes et al. [2004] consider the above premium and add the role of exchange rate policy. Given the relative importance of the private entrepreneurial sector in emerging economies and its potential to create frictions in the economy, it may be worth examining some stylized facts about the private entrepreneurial sector. For instance, private entrepreneurial activity is often volatile in emerging economies, as well. An illustrative example is the Chilean case. While it boasts one of the most robust financial systems in its region, its economy still reacted strongly to the effects of the Asian crisis. A reason often cited for this response was that the entrepreneurial sector moved quickly from an early-to-mid 1990s boom euphoria to a deep depression in the following months.Footnote 2 Another stylized fact deals with the size and ownership of private companies. Moskowitz and Vissing-Jørgensen [2000], who use a large sample of US private firms’ equity, show that these companies are typically small and owned by few or just a single entrepreneur. At the same time, private entrepreneurs usually invest at least 50 percent of their assets in a single company. The objective of this paper is to combine a stylized fact commonly cited in the literature (liability dollarization), with facts somewhat overlooked that are related to the private entrepreneurial sector, in order to provide an additional explanation for higher output volatility observed in emerging markets. For that purpose, a next step is to motivate the role of the private entrepreneurial sector in creating stronger output responses to shocks in a small open economy. Although by nature people may be generally risk averse, many models assume for simplicity risk neutrality on the part of active agents. However, one might question whether this simplification compromises the model's worth if agents are in fact risk averse. Gale and Hellwig [1985], for instance, point out that “risk neutrality is not an unreasonable assumption to make in the case of investors since it can be justified as a consequence of risk-pooling.” That is, by pooling and investing large amounts of funds and thus taking advantage of economies of scale, lenders tend to successfully maintain diversified portfolios. Therefore, though still risk averse, given the fact that lenders can significantly reduce the exposure to risk through their portfolios, it is feasible to assume risk neutrality. Consequently, most papers model lenders’ behavior as risk neutral agents that maximize expected returns.Footnote 3 It may be less desirable to assume risk neutrality on the part of private entrepreneurs. In the same paper, Gale and Hellwig [1985] emphasize the fact that the risk neutrality assumption “makes less sense in the case of entrepreneurs and indeed is merely a ‘simplifying’ assumption which ought to be relaxed if possible.” In addition, as shown by Moskowitz and Vissing-Jørgensen [2000], the high concentration of ownership of privately held companies and their importance in the owners’ portfolios leave private entrepreneurs highly vulnerable to project-specific, uninsurable risks. Therefore, risk neutrality seems to be a much stronger assumption in modeling the investment decisions of private entrepreneurs. That is, the fact that they are less likely to have access to complete risk-pooling for their idiosyncratic risks causes them to internalize more the cost of volatility. Entrepreneurs themselves often consider the notion that entrepreneurs are risk takers one of the major misconceptions about their sector. Although there are some that do present such behavior, several studies and interviews suggest that most of them, the successful ones in particular, are actually risk averse. They invest considerable time and effort to try to limit risk, pursue ideas that have good prospects and discard those that do not. As Don Hofstrand, an entrepreneur and co-director of Ag Marketing Resource Center at the University of Iowa, puts it, “evidence suggests that [entrepreneurs] are risk-averse just like you and me ... They ... carefully [assess] the risk/reward relationship of their actions. Risk is assumed only when the opportunity for reward is sufficiently large enough to warrant the risk” [Hofstrand 2006]. Sanjeev Bikhchandani, a renowned Indian entrepreneur, also points out the falsehood of conventional wisdom that entrepreneurs are risk takers. He affirms that “[every] entrepreneur I have known is risk averse. I will go so far as to say that most entrepreneurs are really cautious people — they think a hundred times before making any significant investment ... They manage risk, they try and mitigate it and then of the various alternative courses of action available they go for that one that carries the least risk” [Bikhchandani 2009]. Besides qualitative evidence, data also suggest that entrepreneurs’ risk preferences are no different from those of wage earners. Wu and Knott [2005], for instance, propose a bi-dimensional feature of risk: uncertainty about market demand and uncertainty about own entrepreneurial ability. Using data from the FDIC research database, the authors find that entrepreneurs show risk aversion with respect to what it is out of their direct control (demand) but show evidence of overconfidence with respect to what they have some control (entrepreneurial ability). Nevertheless, their discussion further indicates that these risk profiles are common to both entrepreneurs and wage earners. The authors conclude that differences in human capital and wealth are more likely to account for differences between these two groups. Bernanke et al. [1999] study the impact of information asymmetries between risk-neutral lenders and entrepreneurs in a closed-economy context. The entrepreneur finances capital goods using both his own net worth and borrowed funds. Their model assumes that the entrepreneur has private information on the true ex-post profitability of the project. Agency problems arise from the fact that there is a positive probability that the project will fail, in which case the lender will not recover all remaining revenues after default, due to some bankruptcy costs. As a result, lenders optimally charge the entrepreneur an external finance premium. This endogenous premium is decreasing in the level of net worth of entrepreneurs, since a lower reliance on borrowed funds reduces the agency costs that the lender internalizes. Their paper shows that the external finance premium becomes a mechanism that magnifies and propagates the impact of real shocks over time. Pardo [2010] builds on Bernanke et al. [1999] to show that introducing risk aversion among private entrepreneurs modifies the optimal contract in two ways. First, entrepreneurs demand insurance in order to ensure a positive minimum consumption. Second, the overall risk premium incorporates the private equity premium, or the positive risk premium that risk-averse entrepreneurs demand due to the stochastic nature of their private investment returns. When the private equity premium is present, it becomes a mechanism that further magnifies the aggregate effects of real shocks. The mechanism works as follows: any real shock that reduces entrepreneurial profits and net worth, at the same time reduces entrepreneurs’ minimum guaranteed level of consumption. Consequently, their effective risk aversion and the private equity premium also rise. In response, entrepreneurs contract their supply of capital to final goods firms, producing a magnified impact of shocks on investment and production. The first part of this paper studies the effect of asymmetric information and entrepreneurial risk aversion in a small open economy framework. I initially examine the role of the real exchange rate in affecting the private equity premium and the small open economy's equilibrium and dynamics. Intuitively, a real depreciation may benefit the economy by shifting demand towards domestic goods. At the same time, though, it may also raise the cost of producing capital by increasing both the value of imports (provided that capital production uses imported goods) and the value of debt repayment (provided that there is “liability dollarization”). In this context, shocks not only affect entrepreneurs’ net worth directly, but also indirectly through the increase in the value of debt that the corresponding real exchange rate adjustment produces. As wealth falls, the private equity premium rises, therefore amplifying the effects of shocks over time.Footnote 4 In the second part, as in Cespedes et al. [2004], I execute an exchange rate regime comparison allowing for risk-averse entrepreneurs. Their model and others have found that incorporating the external finance premium does not overturn the standard Mundell-Fleming result that a floating exchange rate is superior to a pegged regime in terms of absorbing real shocks.Footnote 5 These studies point out that, in general equilibrium, the favorable impact of depreciations on exports more than offsets the adverse balance-sheet effect in the long run. In this paper I examine whether allowing for risk-averse entrepreneurs can affect the above conclusions. In particular, I show that when the private equity premium is incorporated into the analysis, so that real depreciations affect entrepreneur's net worth and effective risk aversion, shocks produce significantly higher increases in volatility under fixed exchange rates than under free floating rates. Put differently, not only does the Mundell-Fleming result remain intact, but adding entrepreneurial risk aversion increases the relative output response of shocks in the fixed-rate scenario. That is, ignoring the impact of entrepreneurial risk aversion in economies where this sector is relatively important is likely to understate the costs of adopting a fixed exchange rate policy. The outline of this paper is as follows. The next section examines the partial equilibrium interaction between a domestic risk-averse entrepreneur and a foreign risk-neutral lender. The subsequent section embeds the partial equilibrium results into a dynamic open economy general equilibrium model, and analyzes its main aggregate features. Next, calibration of the model and simulation exercises are presented. That section also executes an exchange rate regime comparison analysis by contrasting the response to shocks under both regimes. The last section provides some concluding remarks.",1
38,2,Eastern Economic Journal,14 March 2011,https://link.springer.com/article/10.1057/eej.2010.67,Firms as Bundles of Discrete Resources – Towards an Explanation of the Exponential Distribution of Firm Growth Rates,February 2012,Alex Coad,Max Planck,,Male,Male,Unknown,Male,"It has long been known that the distribution of firm growth rates is fat-tailed. In an early contribution, Ashton [1926] considers the growth patterns of British textile firms and observes that: “In their growth they obey no one law. A few apparently undergo a steady expansion … With others, increase in size takes place by a sudden leap …” [Ashton 1926, pp. 572–573]. Little dedicates a section of his 1962 empirical study to the distribution of growth rates, and also finds that the distribution is fat-tailed. However, he concludes the section without proposing any theoretical explanation: “I do not know what plausible hypothesis explains the highly leptokurtic nature of the distributions” [Little 1962, p. 408]. Recent empirical research into industrial dynamics has discovered that the distribution of firm growth rates closely follows the Laplace distribution, also known as the symmetric exponential distribution. Using the Compustat database of the US manufacturing firms, Stanley et al. [1996] and Amaral et al. [1997] observe a “tent-shaped” distribution characterized by a straight line on logarithmic plots that corresponds to the Laplace density. The Laplace distribution is also found to be a rather useful heuristic when considering growth rates of firms in the worldwide pharmaceutical industry [Bottazzi et al. 2001]. Giulio Bottazzi and co-authors extend these findings by considering the Laplace density in the wider context of the family of Subbotin distributions. They find that, for the Compustat database, the Laplace is indeed a suitable distribution for modeling firm growth rates, at both aggregate and disaggregated levels of analysis [Bottazzi and Secchi 2003a]. The Laplacian nature of the distribution of growth rates also holds for other databases such as Italian manufacturing [Bottazzi et al. 2007].Footnote 1 In addition, the Laplace distribution appears to hold across a variety of firm growth indicators such as sales growth, employment growth or value added growth [Bottazzi et al. 2007]. The growth rates of French manufacturing firms have also been studied, and roughly speaking a similar shape was observed, although it must be said that the empirical density was noticeably fatter-tailed than the Laplace [Bottazzi et al. forthcoming].Footnote 2 In Figure 1, we use the Compustat database to show the heavy-tailed distribution of annual employment growth rates for large US firms. The empirical distribution of employment growth rates elaborated by the author using the Compustat dataset for large US firms. Note the log scale on the y-axis. Employment growth is calculated in the usual way of taking log-differences of total employment in successive periods. Annual employment growth rates are calculated for the periods 1980–1981, 1990–1991, and 2000–2001, obtaining 5,256, 5,931, and 7,948 observations respectively. For each period, firms are sorted into 100 bins. In this paper, we argue that it would be fruitful to conceive firms as being composed of discrete, interrelated resources that are subject to local interactions, and susceptible to containing some degree of organizational slack. We sketch out two similar theoretical models that rely on these characteristics of firms to explain a number of “stylized facts” of firm growth. In “A discussion of previous models”, we review and discuss previous models of industry growth. “Theoretical foundations” contains a discussion whereby we identify some features common to firms — that is firms can be seen as composed of lumpy, indivisible resources that are subject to non-linear interactions. In “A simplified model”, we present a simple model of employment growth in a hierarchical organization. “Extending the model” contains an extended model that is computationally rather complex, the properties of which we explore using simulation analysis. “conclusion” concludes.",18
38,2,Eastern Economic Journal,15 August 2011,https://link.springer.com/article/10.1057/eej.2011.3,The Effect of Ticket Resale Laws on Consumption and Production in Performing Arts Markets,February 2012,Melissa Boyle,Lesley Chiou,,Female,,Unknown,Mix,,
38,2,Eastern Economic Journal,06 June 2011,https://link.springer.com/article/10.1057/eej.2011.5,What is Health? A Multiple Correspondence Health Index,February 2012,Jennifer L Kohn,,,Female,Unknown,Unknown,Female,"With increasing computing power and availability of panel data sets researchers have more options to use dynamic models and control for unobservable heterogeneity. Both persistence and heterogeneity have been proven to be significant factors in the evolution of health. Contoyannis et al. [2004], hereafter CJR, find strong persistence in health and that unobservable heterogeneity accounts for over 30 percent of the error variance for both men and women. The persistence in health can arise from various theoretical sources including partial adjustment to optimal health demand [Wagstaff 1993] and health as an element in health production [Kohn 2009] or statistically from distributed health shocks. Unobservable heterogeneity results from the myriad of factors that contribute to health but are not typically found in the data or measured with substantial error including genetics, diet, and exercise. Thus, static and cross-section models of health risk substantial omitted variable bias. In addition, health is an independent factor in models with persistence and unobservable heterogeneity including life-cycle models of wages and models to forecast medical care demand. The most widely available proxy for health in large social surveys is “self-assessed health” (SAH), which generally asks respondents to rate their health on an ordinal scale typically from 1=excellent to 5=very poor. However, there are both theoretical and econometric problems associated with using this ordered discrete variable. Critically, these problems are exacerbated in dynamic models that attempt to control for unobservable heterogeneity. First, SAH is subject to various sources and degrees of measurement error [Currie and Madrian 1999, review this evidence; see also Hernandez-Quevedo et al. 2005; CJR and references therein]. SAH is also subject to reference bias [Groot 2000], cultural and other differences in cut points between categories [Jürges 2007], and may be endogenous with dependent variables such as labor force participation [Bound 1991]. Second, extant dynamic panel data estimators for limited dependent variables cannot accommodate fixed effects due to the incidental parameters problem [Lancaster 2000; see Greene 2004, for a review and simulations].Footnote 1 The random effects specifications require a restrictive assumption that the effect is uncorrelated with the covariates. This key assumption is difficult to justify with respect to health particularly given the myriad sources of error in the SAH measure just noted. Finally, inferences on the marginal effects from discrete models are substantially more difficult to compute and interpret than those from linear models. These difficulties in inference associated with SAH as both a dependent and independent variable are poignantly illustrated by the dynamic panel ordered probit model in CJR. The first problem here is that there is no consensus as to the best way to compute average partial effects (APE) for arbitrarily scaled discrete regressors such as the lags of SAH in the presence of unobservable heterogeneity [Wooldridge 2005].Footnote 2 Second, it is difficult to concisely display the results. As a result, CJR only report the APEs for the probability of reporting excellent health; not for the probability of reporting poor health, which reasonably may be of interest. Finally, the interpretation of the APE, like the interpretation of the coefficients on all discrete variables, is with respect to the omitted category. Thus, when CJR find an APE of 0.234 on excellent health for men, this does not directly speak to the persistence of excellent health, but to the persistence of excellent health over that of good health, which is their omitted category. A continuous measure of health would solve these econometric problems. A continuous health index would allow the use of extant fixed effects dynamic panel data estimators such as Arellano and Bond [1991] and Blundell and Bond [1998] (available as a STATA user written code “xtabond2” by Roodman [2009]). The marginal effects for these linear models are easily presented and interpreted. In addition, the continuous health index exhibits considerably more variation in both the cross section and time series than the discrete SAH. Such variability is essential to support identification in differenced models that control for unobservable heterogeneity. Moreover, when health is the independent variable of primary interest (e.g. the effect of health on dependent variables such as medical care use, wages, or relationship status) a single index measure is easier to interpret than multiple discrete categories. Finally, whether health is the variable of interest or a control, using a single continuous health index rather than multiple discrete health indicators is a more parsimonious specification that preserves degrees of freedom in smaller samples, avoids problems of multicollinearity, and supports clear inferences without respect to omitted categories. The purpose of this paper is to illustrate such an index identifying instruments using a multiple correspondence analysis (MCA). This index was first suggested by Greenacre [2002] using the Spanish National Health Survey. However, Greenacre used a two-dimensional model (simple CA rather than MCA) merely to derive weights for SAH for different age categories. This method of adjusting the 1–5 scores of SAH to reflect the nonlinearity among the categories is used in the HALex index [Erickson 1998]. The index presented here extends these applications to a general multi-dimensional health index that maximizes the use of broadly available data with minimal modeling assumptions. While there has been substantial work on an alphabet soup of health-related quality of life indices (some of the more common of these are EQ-5D, HUI2, HUI3, SF-36v2, SF-6D, QWB-SA, HALex) these indices are not available consistently in the large panel data surveys needed for dynamic analysis of broad policy-relevant questions. For example, the British Household Panel Survey (BHPS) is one of the longest surveys with comprehensive health, wealth, and medical care data, but the SF-36 questions were only asked in waves 9 and 14, and the CASP-19 questions were only asked in wave 11. More problematic, some of the algorithms used for these indices are proprietary and thereby not transparent to the researcher (e.g. the HUI2/3). Similarly, other methods to proxy for health require extensive modeling assumptions that risk model misspecification. For example, Multiple Indicators Multiple Causes requires separate equations for each health proxy [see Van der Gaag and Wolfe 1991; Van de Ven and van der Gaag 1982; Kuklys 2005 for a more recent example associated with the rise of the “capabilities” approach to social index modeling following Sen 1976, 1985]. Similarly, the latent variable method [Bound 1991; Krishnakumar and Nagar 2008] uses a complex system of equations to orthogonalize various health measures with reduced form OLS estimates on the discrete SAH variable. By contrast, MCA is a transparent statistical procedure that computes weights to maximize the correlation in categorical data. While this method is used in clinical studies [Costa et al. 2008] and to develop socioeconomic status indices [Cortinovis et al. 1993], I have not found similar applications of MCA applied in health economics for the purpose of creating a health index. The paper proceeds as follows: the next section describes the MCA procedure. The section after that describes the data, discusses certain modeling choices, and presents the index weights. The subsequent section compares the index with SAH first in terms of descriptive statistics and then in models using health as a dependent and independent variable. The last section concludes with a summary and directions for future research. Appendix A provides details on the BHPS variables used in the health index and STATA code to create the index, and Appendix B provides summary statistics and component weights for the health index.",24
38,2,Eastern Economic Journal,25 July 2011,https://link.springer.com/article/10.1057/eej.2011.9,The Relative Performance of Head Start,February 2012,Cory Koedel,Teerachat Techapaisarnjaroenkit,,,Unknown,Unknown,Mix,,
38,2,Eastern Economic Journal,15 February 2012,https://link.springer.com/article/10.1057/eej.2010.16,"Animal Spirits: How Human Psychology Drives the Economy, and Why it Matters for Global Capitalism, by George A. Akerlof and Robert J. Shiller",February 2012,Martin Rapetti,,,Male,Unknown,Unknown,Male,,1
38,2,Eastern Economic Journal,15 February 2012,https://link.springer.com/article/10.1057/eej.2010.17,"Intangible Capital: Its Contribution to Economic Growth, Well-Being and Rationality, by John F. Tomer",February 2012,Carol A Robbins,,,,Unknown,Unknown,Mix,,
38,2,Eastern Economic Journal,15 February 2012,https://link.springer.com/article/10.1057/eej.2012.2,List of Reviewers 2011,February 2012,,,,Unknown,Unknown,Unknown,Unknown,,
38,3,Eastern Economic Journal,11 June 2012,https://link.springer.com/article/10.1057/eej.2012.10,Dilemmas of Economic Growth,May 2012,Duncan K Foley,,,Male,Unknown,Unknown,Male,"In the Fall of 2008, I gave a talk to a workshop made up of a group of well-informed and concerned environmentalists, agronomists, geophysicists, and other scientists, philosophers, and writers.Footnote 1 The talk summarized [Rezai et al. 2011], and centered on the slide reproduced as Figure 1. The optimal equilibrium path, OPT, is plotted in light gray, the BAU equilibrium in black, and the constrained equilibrium, COPT, in dark gray. The carbon price on the optimal path is about US$200/t, and the damage on the optimal path is less than 1 percent of potential output. The carbon price is the social marginal value of foregoing the emissions from 1t of carbon. For OPT and COPT, the carbon price is effective in economic decisions, but not in BAU (and is plotted as a dashed line). For OPT, the mitigation percentage is the proportion of world product devoted to mitigation. For COPT and BAU, the mitigation percentage is the investment called for by the imputed carbon price (and is plotted as a dashed line), while the actual mitigation is zero. In the overall context of the paper and talk, this slide was intended to convey two important points about the economics of global warming. First, following [Foley 2009], the simulations reported in the slide exemplify the theoretical point that from a welfare economics point of view global warming is an externality, the correction of which (whether through direct controls, carbon taxes, or a cap-and-trade system) allows a Pareto-improvement across generations. This point of view is in sharp contrast to the presentation in many economics discussions (e.g., Nordhaus and Boyer 2000; Nordhaus 2008] that frame the global warming policy problem in terms of a trade-off between the interests of current and future generations. I thought this perspective might be welcome to the workshop audience in substituting “gains in efficiency and welfare” for “sacrifices of economic growth and standard of living” in the political debate over measures to mitigate global warming. Second, the slide simulations, based on plausible but far-from-watertight assumptions about the costs and benefits of greenhouse gas mitigation and standard rational-expectations equilibrium assumptions, present a scenario in which relatively modest diversion of world output (on the order of 1 percent) to investment in reducing greenhouse gas emissions yield enormous benefits, measured in terms of long-term per-capita world income. On an optimal path in which the global warming externality is internalized, world per-capital income continues a steady rise from current levels of about US$6,000 per year to a long-run steady-state level of US$20,000 per year, while on a “business-as-usual” equilibrium path where there is no effective price for greenhouse gas emissions, a climate catastrophe about 100 years from the present induces a sharp fall in world per-capita income, which falls to a steady-state level below current levels. The optimal mitigation investment succeeds in halting and reversing greenhouse gas accumulations in the atmosphere and returning them to industrial levels, whereas the business as usual path allows accumulation of greenhouse gases to rise until climate damage itself stabilizes economic output. Somewhat to my surprise, the workshop audience did not seem very interested either in the welfare economics policy message of the simulations or in their relatively optimistic scenario of greenhouse gas mitigation. The strong consensus of the workshop audience was that the projection on the optimal path of a steady-state world per-capita income level of US$20,000 per year with an assumed steady-state population of 8.5 billion was grossly, indeed, to this audience, ludicrously unrealistic. In their view, even if the consequences of global warming could be mitigated effectively, a rise in world product from current levels of approximately US$60 trillion a year to the US$250 trillion a year envisioned in the simulation would put sufficiently catastrophic stresses on world resources and ecologies as to guarantee a complete collapse of human civilized life. I was somewhat astonished myself to learn that a broad consensus in that workshop thought a more “realistic” scenario for the world economy would involve a reduction of world population during the next century from its current level of approximately 6.5 billion to 1 billion, a reduction the workshop consensus considered inevitable, but hoped could be accomplished without violence or the compromise of human rights and dignity. This prospect appeared to me just as unlikely as the output levels in our simulation appeared to the rest of the workshop. My co-authors and I had regarded our specification of output and productivity growth as reaching an asymptote as rather conservative compared with many economic growth models that unapologetically imply much higher levels of world production than our scenario. This experience suggested to me that there is an urgent need for dialog between economists and other disciplines to narrow the gap between the futures the two groups envision as feasible and desirable.",5
38,3,Eastern Economic Journal,23 January 2012,https://link.springer.com/article/10.1057/eej.2011.7,"Student Evaluation of Teaching, Formulation of Grade Expectations, and Instructor Choice: Explorations with Random-Effects Ordered Probability Models",May 2012,Horacio Matos-Díaz,,,Male,Unknown,Unknown,Male,"Student evaluation of teaching (SET) has become widespread in colleges and universities. At the University of Puerto Rico at Bayamón (UPR-Bayamón) faculty members are evaluated by their peers in the personnel committee, by the department head, and by their students. All those ratings are used by the UPR-Bayamón for tenure and promotion purposes. According to Anderson and Siegfried [1997], the SET rating expected by a professor varies directly with the utility derived by the students. To the extent that such utility depends only on the knowledge gained by students, the SET would serve its purpose. Yet, other factors like student, professor, and course characteristics could influence the generation of utility, possibly transforming the SET into a popularity contest of sorts. If such is the case, the process becomes biased and unreliable. There is much statistical evidence that tends to confirm that all those variables explain part of the variability observed in SET ratings.Footnote 1 Practically since its experimental implementation, the academic community challenged the validity and adequacy of SET ratings. According to Sailor et al. [1997], the origin of the debate goes back to 1928. Since then, the positive correlation between SET ratings and the grades received (or expected) by students has been widely documented. Although there is general consensus about this statistical result, the controversy about its interpretation has not been solved [Simpson and Siguaw 2000]. Some researchers argue that this positive correlation is evidence in favor of the objectivity and reliability of the SET because if students learn more in their courses they will rate their professors highly [Cohen 1986; Marsh 1987; Moore 2006]. Other researchers interpret this result as evidence of bias [Hamilton 1980; Wachtel 1998]. In the economic literature the discussion concerning SET occurs predominantly in the context of the grade inflation phenomenon [Johnson 2003]. According to the leniency hypothesis, faculty members can buy higher SET ratings if they are willing to reduce the price (effort) that students pay in order to get higher grades by relaxing their grading standards.Footnote 2 McKenzie and Staaf [1974], McKenzie [1975], and Becker [1979] have developed economic models that rationalize this conjecture and its relationship to grade inflation. Based on the guidelines of those models, several papers study the statistical relationship among SET ratings, course expected grade (EG), and grade inflation.Footnote 3 This paper explores the merits of a new research agenda which has been largely ignored in the economics of education literature. The paper attempts to identify the determinants of two different processes carried out by students: the EG formulation (process 1) and the probability of taking more courses with the professor being evaluated (process 2). To this end, it will be assumed that students perform both processes independently. First, based on the information available in their academic environment, as well as on the information and signals sent by their professors, students complete process 1. Second, depending on the optimism of their relative EG (REG), they undergo process 2. The remainder of the paper is organized as follows. The next section discusses the purpose of the paper and the hypotheses to be tested. The subsequent section discusses the nature of the data, variable definitions, and models to be estimated. The results are discussed in the penultimate section. The final section provides some conclusions.",8
38,3,Eastern Economic Journal,15 August 2011,https://link.springer.com/article/10.1057/eej.2011.15,A Benefit from the Division of Labor that Adam Smith Missed,May 2012,Wenli Cheng,,,Unknown,Unknown,Unknown,Unknown,,
38,3,Eastern Economic Journal,15 August 2011,https://link.springer.com/article/10.1057/eej.2011.16,Sectoral Growth Effects of Cross-Border Mergers and Acquisitions,May 2012,Nadia Doytch,Merih Uctum,,Female,Male,Unknown,Mix,,
38,3,Eastern Economic Journal,15 August 2011,https://link.springer.com/article/10.1057/eej.2011.17,"Women, Men, and Job Satisfaction",May 2012,Cheryl J Carleton,Suzanne Heller Clain,,Female,Female,Unknown,Female,"Job satisfaction has been a subject of interest to labor economists for a good number of years. One strand of research has focused on the significant impact of job satisfaction on important aspects of worker performance, such as absenteeism [Vroom 1964; Mangione and Quinn 1975; Clegg 1983] and turnover [Flanagan et al. 1974; Freeman 1978; Akerlof et al. 1988; Clark 2001]. Another strand of research has in turn investigated the determinants of job satisfaction itself. Early studies among the latter [Borjas 1979; Bartel 1981; Hamermesh 2001] investigated the determinants of job satisfaction for men only. More recent studies have included women in the analysis [Clark 1997; Sloane and Williams 2000; Sousa-Poza and Sousa-Poza 2000; Donohue and Heywood 2004; Bender et al. 2005; Kristensen and Johansson 2008]. These have generally found indications in European Union, British and US data that job satisfaction is greater for women than for men. The finding has been seen as somewhat paradoxical, given the long-standing gender earnings gap that favors men. In their efforts to explain the source of the paradox, these researchers have considered many possibilities. A common approach to the issue has been to search for some previously overlooked factor that, once included in the analysis, causes the gender difference to disappear. Notable among the explanations is one by Clark [1997], suggesting that women's higher job satisfaction is caused by women's improved position in the labor force relative to their expectations. Clark came to this conclusion after first supposing the gender difference in expectations to be smaller for younger workers and for better-educated workers. In his empirical work, he found no gender differential in job satisfaction among the youngest workers (16–19) or the most educated workers in his sample, but differentials for other ages and levels of education. On the basis of this work, Clark was led to predict the gender difference in job satisfaction to be temporary, lasting only until expectations adjust so that there is no gender difference in rewards relative to expectations. Donohue and Heywood [2004] similarly found that no gender gap in job satisfaction existed for a young (23–31) cohort of US workers, surveyed in 1988. Taking this to be evidence of a cohort effect, Donohue and Heywood hypothesized that the gender differences with respect to job satisfaction in the population as a whole might vanish as the young cohort ages. However, Donohue and Heywood noted that their findings were also consistent with a vintage effect, whereby the gender gap in job satisfaction remains a fact of life, regularly observable once workers age out of the young cohort. The present study re-examines the job satisfaction of men and women, using recent US data, with a primary focus on estimating job satisfaction separately by gender and marital status. Previous researchers tended to investigate gender differences in job satisfaction by pooling men and women together, regardless of marital status, and estimating a model with a dummy variable for gender. Such a specification could obscure the source of gender differences, if in fact tastes (i.e. slope coefficients) vary by gender. A secondary focus of the analysis is sample selectivity, an issue that Clark [1997] considered but discarded in his search for sources of gender differences in job satisfaction. Given research in the area of labor supply, it is reasonable to think that women (particularly married women) may have greater discretion than men (and probably unmarried women) in the decision to work for pay. Consequently, the decision of married women to work in the market may itself indicate a taste for market work and an expectation of a high level of job satisfaction. However, the decision of men to work in the market may reflect social conditioning and traditional gender roles more so than anything in particular about a taste for market work or an expectation of a high level of job satisfaction. It may be that men (and unmarried women) who are dissatisfied with work nevertheless tend to continue to work at their unsatisfying jobs, while married women who are unhappy with market work tend to quit unsatisfying jobs in order to pursue something more satisfying.Footnote 1 If so, then these behaviors affect the underlying nature of the observed sample of job holders used to explore job satisfaction. As such, the differences in labor supply behaviors could be responsible for the appearance of greater job satisfaction among married working women, compared to working men (or unmarried working women). The observed samples of job holders with data reporting associated job satisfaction may be predestined to show gender differences in job satisfaction, based on the underlying gender differences in the nature of the decision to work for pay. In this paper, we first apply conventional specifications to all workers and then to subsamples separated by gender or marital status. Initial results duplicate the findings of previous researchers. However, results of estimation applied to various subsamples (males, females, marrieds, and unmarrieds) suggest a more complicated picture. The gender difference in job satisfaction is found to be less significant among unmarried workers than among married workers. Also, while job satisfaction of males is not significantly different by marital status, the job satisfaction of married females is significantly higher than the job satisfaction of unmarried females. In an effort to investigate the possible role of variation in taste, we apply the analysis to subsamples separated by gender and marital status (married men, married women, unmarried men, and unmarried women). Applying the analysis to such subsamples diverges from conventional specifications of job satisfaction, in that it allows for gender effects beyond intercept differences. We also expand the analysis to include techniques designed to correct for sample selection. In doing so, we find that sample selectivity related to the decision to work appears to have a role in explaining the job satisfaction of married women but not to have a role in explaining the job satisfaction of married men, unmarried men, and unmarried women. Given that there are differences in sample selectivity, personal/job characteristics (X's) and the impacts of personal/job characteristics on job satisfaction (β̂′s or tastes) across subsamples, a relevant question concerns the extent to which each of these contributes to conventional findings of gender differences in job satisfaction. We address this question by re-estimating job satisfaction by marital status (marrieds, unmarrieds), this time using a full set of interaction terms of gender and X's to capture gender differences. This analysis reaffirms that sample selectivity is a relevant factor for married but not unmarried workers, but suggests no significant difference in its role for married women and married men. It does pinpoint other specific personal/job characteristics that have β̂′s that differ significantly by gender. In doing so, it also suggests that conventional findings can be fully attributed to gender differences in the β̂′s of married workers; with these specifications there is no significant remaining gender difference in the intercept of the job satisfaction equation of married workers. However, a significant gender difference in the intercept of the job satisfaction equation of unmarried workers is found; it contributes an additional upward boost in the job satisfaction of male unmarried workers. This finding suggests that more work is needed to understand gender differences in the job satisfaction of unmarried workers. A brief discussion of the theoretical model and the methodology behind the analysis follows in the next section. The third and fourth sections describe the data and the analytical findings, respectively.",21
38,3,Eastern Economic Journal,10 October 2011,https://link.springer.com/article/10.1057/eej.2011.18,Bribery and Endogenous Monitoring Effort: An Experimental Study,May 2012,Aaron Lowen,Andrew Samuel,,Male,Male,Unknown,Male,"Bribery and corruption among public officials reduces the effectiveness of a wide range of public policies and regulations. For example, a recent investigation found that safety inspectors from the New York Department of Education accepted bribes from school bus companies in exchange for declaring unsafe school buses were safe for transporting students [Greenhouse 2008]. Similarly, Rubin [2006] finds that corrupt border guards play a key role in facilitating the smuggling of opium out of Afghanistan. Papers by Polinsky and Shavell [2001] and Mookherjee and Png [1995] formally study bribery among regulators and clearly articulate theoretical structures that substantiate this argument. Specifically, they observe that the presence of corrupt enforcement agents reduces deterrence and thereby lowers compliance because bribe payments are lower than the official sanctions. Thus, Bardhan [2006] rightly asserts that the presence of bribe-taking officials makes it difficult to implement welfare-enhancing policies and enforce regulations.Footnote 1 Although there is a significant body of theoretical micro-level research on corruption, there is relatively little empirical research on the incentives of bribe-taking officials. This is likely because bribery is rarely observed in a way that permits data collection or controlled analysis of the institutions in which it occurs. Recently, however, Svensson [2003], Hunt and Laszlo [2005], and Olken and Barron [2007], have used surveys and field experiments to collect data from bribe-payers (such as individuals and firms). Although this line of research has found interesting data on bribe-payers, such methods are not likely to yield data on the decision-making of corrupt bureaucrats. Specifically, corrupt bureaucrats are likely to both face strict sanctions for their behavior and lose income from policies that root out corruption. Hence, they are unlikely to be willing to honestly report their corrupt activities (other complications are discussed below). Thus, despite the importance of understanding the incentives for bribery, empirical research at the level of individual bureaucrats or law-enforcement officials is scarce. Given the challenges in conducting this type of research, experiments provide an alternative route to investigating the behavioral foundations of corruption. The last several years have seen a growth in experimental research on corruption (see surveys by Dusek et al. [2004] and Abbink [2006]). In this paper we present the findings from an experimental game of bribery where an inspector exerts costly endogenous effort to collect evidence against an agent (referred to as the “owner,” such as a restaurant owner who might be inspected for health violations) who is guilty of violating some regulation. Within this inspector–owner setting bribery may occur in one of two stages. First, a bribe may occur after the inspector has exerted effort and found evidence against the owner (an ex-post bribe). Alternatively, a bribe may be paid before the inspector has exerted effort to find evidence against the owner (a preemptive bribe).Footnote 2 Both preemptive and ex-post bribery are observed in the real world. For example, when smugglers approach border crossings, customs inspectors frequently demand bribes — even before they inspect people's belongings [Simpson and Bojovic 2005]. Such a preemptive bribe differs substantively from an ex-post bribe, which would be paid after the inspector finds evidence of illicit or improperly transported merchandise. Other examples of preemptive and ex-post bribery are described in Lubin [2003] and Rubin [2006]. Our experiment implements the model of bribery first developed in Mookherjee and Png [1995] and extended in Samuel [2009]. These papers study bureaucratic incentives under preemptive and ex-post bribery, and the results of Mookherjee and Png have been used to design anti-corruption policies [Bardhan 2006]. Their results have not been tested empirically, however, because an empirical analysis of these theoretical models requires observation of effort levels, which is unlikely to exist in field data. To our knowledge, our paper provides the first implementation of Mookherjee and Png's seminal model. In our experiment the level of monitoring is endogenous, and the effort level is almost continuous.Footnote 3 Endogenous effort is essential to understanding bribery because the inspector accepts a preemptive bribe in order to avoid exerting (costly) effort to inspect. Our paper incorporates preemptive and ex-post bribery and examines their relationship to the inspector's effort choice. More broadly, we examine whether inspectors exert the level of effort and demand bribes that are consistent with the theoretical predictions of these models. To our knowledge, no economic experiments on bribery examine these issues. Although not obvious, ex-post bribery is a variant of the ultimatum game. In our setting, the inspector demands an ex-post bribe after any preemptive bribe demands have been rejected, inspection effort has been exerted, and the inspector has hard evidence that the owner is guilty.Footnote 4 Once these are sunk, the inspector is the offerer and the owner is the respondent in a context-specific version of the ultimatum game. Thus, the game theoretic predictions are quite straightforward [Becker and Stigler 1974; Mookherjee and Png 1995; Polinsky and Shavell 2001]. Experimental results on the ultimatum game, however, lead us to expect results that differ from the Nash predictions in a number of ways. First, Phillips et al. [1991] found that between 5 and 50 percent of participants did not treat past events as sunk (depending on treatment). If these findings carry over to our context, then the size of the ex-post bribe may indeed depend on the (sunk) cost of effort. Second, papers such as Camerer and Thaler [1995] find norms of fairness and reciprocity affect player behavior in the ultimatum game. Guth and van Damme [1998] and Schmitt [2004] extend these results to show that offerers may anticipate fairness norms in responders, and thus offer a substantial amount due to strategic considerations. In our setting this is embodied by behavior that follows strategic considerations for, and the sharing norm of, reciprocity.Footnote 5 A strong counter to these findings comes from Cherry et al. [2002] earned income effect. They find that when a dictator has earned the money to be split, rather than using unearned experimenter (house) money, experimental results in the dictator game are much closer to the Nash predictions. It is unclear whether participants see the evidence gathered by inspector's costly effort as earned, and thus subject to Cherry et al. [2002] earned income effect. There is a small but growing literature on experiments in bribery; Dusek et al. [2004] and Abbink [2006] provide excellent surveys of the experimental literature on bribery and corruption. Generally, experiments on corruption take the form of ultimatum games, trust games, or gift-giving games. They then add features such as the right to refuse a bribe demand, hold-up (opportunism and reneging), different types of reciprocity, externalities levied against other players, and chances of getting caught and punished (either exogenous or endogenous). An influential implementation of bribery is the “moonlighting game” of Abbink et al. [2000], which we generalize for our discussion. Their game has two players: the first (such as a firm owner) chooses to give money or take money from the second (such as a government bureaucrat). Any amount taken is a direct transfer (dollar-for-dollar), while any amount given is multiplied (say, by 3, representing an increase in value). Giving money to the second player can be thought of as an attempted bribe, and the multiplication represents the creation of joint surplus, possibly taken from society. The second player then chooses to give to, or take from, the first player. Although amounts given to the first player are a direct transfer, every dollar taken from the first player (which can be thought of as a punishment) is costly to the second player (depending on the rules, the value taken by the second player may be destroyed instead of expropriated). For this transaction to increase total surplus, the owner must be able to trust that the bureaucrat will carry through on his promise.Footnote 6 This form, therefore, extends the investment game of Berg et al. [1995], which only permits non-negative transfers from the first to the second players, and only non-negative transfers from the second to the first. The Subgame Perfect Nash Equilibrium (SPNE) prediction of the moonlighting game is that the second player does not take any money from the first whereas the first takes the maximum amount. For implementations of the game where only non-negative transfers are allowed, the second player does not return any money to the first, and so the first transfers zero to the second. These predictions are violated in two ways. First, even in one-shot games, first players often give money to the second, and second players frequently return some of the multiplied money (called positive strong reciprocity). Furthermore, second players who have had money taken away frequently choose to penalize the first at their own expense (called negative weak reciprocity).Footnote 7 Thus, reciprocity appears to prevent hold-up, even in one-shot games. In reality, the hold-up problem is also avoided when the firm can punish an inspector for not keeping his end of the deal. For example, Boycko et al. [1995] and Lubin [2003] point out that Russian officials who renege on collusive contracts face violence and harassment. An important question is whether the results from experiments can provide meaningful insights into real-world corruption beyond those available from survey and field experiments, especially in light of the previously discussed studies that collect real data on bribe-payers [Svensson 2003; Hunt and Laszlo 2005; Olken and Barron 2007]. A substantial problem with collecting field data from bureaucrats is selection bias; corrupt institutions attract corrupt individuals. This bias makes it difficult to distinguish the effects of selection from the incentives for corruption. Further, within the context of our research, field data on bribes is rarely sufficiently detailed distinguish between preemptive and ex-post bribes. Experiments thus offer an opportunity to gain insight into human behavior when good real-world data is unavailable, unreliable, or unsafe.Footnote 8 Our research finds evidence that inspector behavior deviates from the Nash equilibrium predictions in two ways. First, inspectors demand bribes that are lower than the Nash equilibrium prediction that the inspectors extract the maximum surplus. Second, we find that inspectors exert less than the payoff-maximizing level of effort. These two findings have significant implications for the welfare implications and the costs of corruption, which we discuss in the conclusion. The next section sets up the theoretical model, the subsequent section presents the experiment and treatments, the section after that analyzes the data and the hypotheses, and the final section concludes.",4
38,3,Eastern Economic Journal,10 October 2011,https://link.springer.com/article/10.1057/eej.2011.20,"School, Department, and Instructor Determinants of Assessment Methods in Undergraduate Economics Courses",May 2012,Georg Schaur,Michael Watts,William E Becker,Male,Male,Male,Male,"Choosing how to assess student learning and course performance is a key part of teaching any subject at any level. There are many goals instructors would like to achieve with evaluation — including formative evaluation in providing information and feedback to students and the instructor, and summative evaluation of student understanding of concepts and course material at appropriate levels or “depth” of conceptual understanding — using methods and instruments that are equitable in not favoring some students over others for reasons unrelated to student learning and academic performance. Assessment methods must also be justified in terms of efficiency criteria: How much class, student, and instructor time do they require, and what is given up if more time is spent for assessment? Considerations of self-interest also face both students and instructors. Students care about grades and about leisure time [Becker 1982] — both of which increased for US college students over the past half century [Babcock and Marks, 2011] — so they tend to favor assessment methods that take less of their time to complete, entail more frequent but shorter assessments to avoid higher risk associated with longer and more heavily weighted assessment instruments (such as comprehensive final exams or long term papers), and classes in which the expected average class grade is higher. Instructors have incentives to use assessment methods that require less time to prepare and grade, and know that student course and instructor evaluations are affected by the grades they assign and how much time students have to devote to course assessment and other assignments. Given this wide range of issues associated with assessment, few studies short of book-length treatments even try to address all of the issues. That is true in this paper, where we use survey data from instructors to investigate factors related to the instructors’ choice of particular assessment methods in four different types of undergraduate economics courses (principles, intermediate theory, econometrics and statistics, and other upper-division field courses). We do not have survey data from students or data on their performance in the courses the faculty surveys are based on that would provide information or perspectives from that side of the classroom. Nevertheless, the instructor-provided data allow us to offer the first regression (OLS and probit model) estimates of how school, departmental, and instructor characteristics affect the choices different economics instructors make about which assessment methods to use. Our results are generally confirmatory, which is to say in line with expected behaviors given the incentives facing instructors at different kinds of schools and at different points of their careers. However, there is value in providing the empirical confirmation and estimated values that suggest how large a role different factors play, particularly in light of the tradeoffs instructors face in deciding how much assessment to do and how to do it. The absence of earlier studies offering estimates of these effects is less surprising given that there are also very few studies that investigate the determinants of instructors’ choice of which pedagogical methods to use in teaching undergraduate economics courses [but see Siegfried and Kennedy 1995 and Harter et al. 1999]. Fundamentally, it is difficult to find or develop data sets covering large numbers of courses, schools, and instructors, to make those estimations possible. We have already reported [Schaur et al. 2008] descriptive statistics and trends from three national surveys concerning the use of 10 different assessment methods in the undergraduate courses. These surveys were conducted in 1995 [Becker and Watts 1996], 2000 [Becker and Watts 2001], and 2005 [Watts and Becker 2008]. Most items on the surveys asked instructors to identify teaching and assessment methods used in courses they had recently taught. The last section of the survey collected background information on respondents’ schools, their departments’ policies and practices related to teaching assignments and incentives, and personal information on rank, gender, years of teaching experience, native language, and so on. There has been relatively little previous work on assessment practices in undergraduate economics, with most of the work focusing on general issues from a theoretical, common sense, or improving pedagogy perspective, sometimes drawn from the general education literature on assessment. For example, Walstad [2001] discusses how to do assessment more effectively, and other papers review using specific kinds of assessment methods such as multiple choice questions [Walstad 2006a, 2006b], essay questions [Buckles and Siegfried 2006], and various kinds of writing assignments [Hansen 1998; Frank 2006; Watts 2006]. Harvie and Philip [2006] discuss the oral exams commonly used in the United Kingdom and across Europe. A few studies have addressed the relationship between student performance on multiple choice and essay or short answer questions, using scores for students who took both kinds of questions in the same course or in the same exam [Walstad and Becker 1994; Becker and Johnston 1999; Chan and Kennedy 2002]. Others investigate student gender and performance on multiple choice and essay questions [Lumsden and Scott 1987; Williams et al. 1992; Hirschfeld et al. 1995; Walstad and Robson 1997]. Siegfried et al. [1996] found that assessment in introductory economics courses relied heavily on multiple choice questions, supporting concurrent and later findings from the larger and more extensive surveys conducted by Becker and Watts, described above. Saunders [2001] surveyed instructors of introductory finance courses, and reported on similarities and differences in both teaching and assessment practices compared with practices in economics courses. Grades in finance courses were determined almost entirely by in-class exams, but a majority of instructors also used group work and some form of writing assignments.",5
38,3,Eastern Economic Journal,03 October 2011,https://link.springer.com/article/10.1057/eej.2011.21,The Impact of the Teaching High School Economics Workshop for Teachers on Student Achievement,May 2012,John R Swinton,Benjamin Scafidi,Howard C Woodard,Male,Male,Male,Male,"Education policy analysts and professional educators have long called for more and better professional learning opportunities for in-service teachers [NCTAF 1996]. This issue appears to be an acute problem in high school economics. As early as 1977, researchers called for more content training for high school economics teachers [Mackey et al. 1977]. More recently, Walstad [2001] found that the typical teacher who is teaching economics has completed no more than one college course in economics. Mackey et al. suggested that teachers at least minor in economics to be qualified to teach at the high school level. However, few high school economics teachers do so. State Councils on Economic Education in 45 states and the 275 university-based Centers for Economic Education provide a variety of in-service workshops for teachers that offer both economic content and lesson materials. The various workshops address topics that range across all grades and virtually all economic subject matter. Some of the more popular offerings include the Stock Market Game workshops (where teachers learn how to administer the Stock Market Game in their classroom and pick up strategies as to how to incorporate economic lessons into the game as their students play), Virtual Economics (where teachers receive the Council for Economic Education (CEE) CD-ROM that includes both an explanation of economic concepts and access to all of the Council's teaching material), and Teaching High School Economics (where teachers get lesson plans that correlate with state economics standards). In this study, we examine the impact of the Teaching High School Economics workshop (HSEW) for teachers as measured by their students’ scores on Georgia's state-mandated and high stakes end-of-course test (EOCT) in economics. One of the motivations behind examining individual workshops such as the HSEW is to help state Councils and university-based Centers for Economic Education determine which workshops have the most influence on teachers’ ability to instruct their students. An understanding of the relative impact of different offerings will help such organizations better allocate their scarce resources. In this study, we provide estimates from empirical models of student test scores, where performance on Georgia's standardized EOCT in economics is a function of student characteristics and teacher in-service training from this specific workshop. Our intent is to isolate the impact of the HSEW while accounting for other factors in student performance. We employ a variety of empirical approaches including specifications with teacher fixed effects, comparing teachers who had the HSEW with teachers who have never taken any economics workshop, and comparing student performance on the economics EOCT before and after their teacher took the HSEW. Furthermore, the high-stakes testing environment in Georgia is uniquely suited for studying the impact of in-service workshops. Our estimates suggest that the HSEW leads to an increase of 0.02–0.06 SD in student performance on Georgia's economics EOCT. Further, this increase is above the average increase that earlier research has found to come from other workshops offered by Georgia Council on Economic Education (GCEE). As the HSEW represents only 2 days of training, these gains in student achievement come at a very low cost when compared with other treatments typically used in k-12 education. In the following section, we review past studies concerning the effectiveness of in-service teacher training. In the next section, we offer some background information about the in-service workshops offered through GCEE and other state Councils, and why the Georgia testing environment is uniquely suited for addressing this issue. In the subsequent section, we describe the data and present the base model in the section after that. We discuss our results from several specifications in the penultimate section and provide concluding remarks in the final section.",5
38,3,Eastern Economic Journal,11 June 2012,https://link.springer.com/article/10.1057/eej.2010.18,"Capitalists, Workers and Fiscal Policy: A Classical Model of Growth and Distribution",May 2012,John McCombie,,,Male,Unknown,Unknown,Male,,
38,3,Eastern Economic Journal,11 June 2012,https://link.springer.com/article/10.1057/eej.2012.11,Financial Statement Fiscal Years 2010–2011 and Previous Fiscal Year,May 2012,,,,Unknown,Unknown,Unknown,Unknown,,
38,4,Eastern Economic Journal,19 December 2011,https://link.springer.com/article/10.1057/eej.2011.26,The Servants of Obama's Machinery: F.A. Hayek's The Road to Serfdom Revisited?,September 2012,Edward McPhail,Andrew Farrant,,Male,Male,Unknown,Male,"According to Boettke, Hayek supposedly demonstrates “what would structurally emerge from the failure of socialist planning to achieve its desired results” [1995, p. 9]. Hayek, however, inadequately explained why command planning per se would ultimately be adopted. The key steps in Hayek's reasoning can be found in Chapter 5 of The Road to Serfdom (Democracy and Planning). In many ways — and as was immediately and readily noted by Hayek's most perceptive critics [e.g., Durbin 1945, p. 359] — Hayek's argument is primarily driven by assumptions that necessarily lead to Hayek's conclusions.Footnote 5 In particular, Hayek assumes that socialists are wholly infatuated with the idea of command planning per se [Hayek 1994, p. 37]: Accordingly, Hayek's “socialists” seemingly view planning as a lexicographically desirable end per se rather than a particular policy machinery by which to attain a set of high ideals. Importantly, Boettke points to a supposed similarity between Hayek's thesis and Arrow's Impossibility Theorem: As is well known, Arrow notes the possibility of intransitive preferences (majority voting will be plagued by vote cycling). As is abundantly clear in Hayek's Chapter 5, however, vote cycling plays no role whatsoever in driving Hayek's thesis. Instead, Hayek's analysis of the “organizational logic” [Boettke 1995, p. 11] supposedly inherent to planning places immense weight on Hayek's rather moot supposition that socialists value planning for its own sake: As Hayek explains, the idea of planning per se (planning parsed to imply a mere slogan) is majority preferred to the supposedly no-planning status quo [Hayek 1994, p. 69]: In any pair-wise majority vote, however, the no-plan status quo trounces any particular draft plan (e.g., plan X). As Hayek makes clear, the no-plan status quo similarly enjoys majority support in any pair-wise vote against draft plan Y or draft plan Z [Hayek 1994, p. 71]. Accordingly, and as Hayek candidly and transparently puts it, no particular draft “plan may appear preferable to a majority to no plan at all” [p. 71]: And thus, instead of Arrow-type intransitivity and pervasive vote-cycling, we have a readily unambiguous majority vote for the no-plan status quo relative to any particular draft plan, and a similarly apparent legislative impasse. As Hayek explains, “Parliaments” are increasingly “regarded as ineffective ‘talking shops’ ” [p. 69]. Accordingly, the “inability of democratic assemblies to produce a plan” is deftly solved by the supposedly pervasive demand “that … some single individual should be given powers to act on their own responsibility” [p. 75]. Ultimately, the legislative body appoints an “economic dictator” [p. 75].Footnote 6 Unsurprisingly, Hayek's logic in Chapter 5 plays a central role in driving Hayek's argument in Chapter 10 of The Road to Serfdom (Why the Worst Get on Top), with Hayek once again heavily stacking the deck in his favor: As Hayek explains, prior to the “suppression of democratic institutions and the creation of a totalitarian regime … it is the general demand for quick and determined government action that is the dominating element in the situation … dissatisfaction with … the cumbersome course of democratic procedure … makes action for action's sake the goal … . It is here that the new type of party, organized on military lines, comes in” [Hayek 1994, p. 150, emphasis added].",5
38,4,Eastern Economic Journal,03 October 2011,https://link.springer.com/article/10.1057/eej.2011.19,The Servants of Obama's Machinery: F.A. Hayek's The Road to Serfdom Revisited? — A Reply,September 2012,Peter J Boettke,Nicholas A Snow,,Male,Male,Unknown,Male,,3
38,4,Eastern Economic Journal,21 November 2011,https://link.springer.com/article/10.1057/eej.2011.22,Special Interest Groups and Economic Growth in the United States,September 2012,Oguzhan Dincer,,,Unknown,Unknown,Unknown,Unknown,,
38,4,Eastern Economic Journal,17 October 2011,https://link.springer.com/article/10.1057/eej.2011.23,An Inquiry into the Pay Structure of the New York Yankees: 1919–1941,September 2012,Kenneth H Brown,Paul E Gabriel,David G Surdam,Male,Male,Male,Male,"Because of the reserve clause that essentially bound players to one team, except at an owner's sufferance, Major League Baseball owners possessed monopsony bargaining power over players until the mid-1970s. With regard to monopsony power, Simon Rottenberg [1956, p. 3] noted that baseball's reserve clause, “can have the effect of depressing salaries.” Given their weakened bargaining position, there is a strong presumption that Major League Baseball players were not paid their marginal revenue product. If players were not paid their marginal revenue product, was there any systematic mechanism used by owners to establish compensation, or did they set salaries capriciously? It is possible that in spite of monopsony potential, owners may have relied on performance data, via a myriad of statistical measures, to establish pay levels for their players. To address the issues of pay, productivity, and the potential for monopsonistic exploitation, we have at our disposal a rich and unique data source: individual player contract information for the New York Yankees for the inter-war years (1919–1941). The empirical analysis presented below uses this data to assess the link between player compensation and performance, based on a human capital earnings model. We employ fixed-effects regression analysis instead of traditional Ordinary Least Squares (OLS) regression because of the panel nature of the data. Unlike prior studies of the Major League Baseball labor market, we have precise earnings profiles on individual reserve-clause players for an entire team across two decades. More recent studies have used a mixture of players; some were still bound under the reserve clause, while others were free agents. We can therefore answer several questions germane to labor economics: How did the Yankees determine salaries? If individual salaries were connected with productivity, what measures did the team employ? Were the players exploited in the sense of being paid less than their marginal revenue product?",2
38,4,Eastern Economic Journal,12 December 2011,https://link.springer.com/article/10.1057/eej.2011.24,A Theory on Predatory Advertising After a Demand-Reducing Shock,September 2012,Yutian Chen,Wei Tan,,Unknown,,Unknown,Mix,,
38,4,Eastern Economic Journal,05 December 2011,https://link.springer.com/article/10.1057/eej.2011.25,Labor Markets and National Culture: Salary Determination in Japanese Baseball,September 2012,Michael A Leeds,Sumi Sakata,Peter von Allmen,Male,Female,Male,Mix,,
38,4,Eastern Economic Journal,20 February 2012,https://link.springer.com/article/10.1057/eej.2011.32,"Fluctuations, Uncertainty and Income Inequality in Developing Countries",September 2012,Fadi Fawaz,Masha Rahnamamoghadam,Victor Valcarcel,Male,Female,Male,Mix,,
38,4,Eastern Economic Journal,23 January 2012,https://link.springer.com/article/10.1057/eej.2011.33,Sleep and Student Achievement,September 2012,Eric R Eide,Mark H Showalter,,Male,Male,Unknown,Male,"The topic of sleep has received relatively little attention in economics despite the fact that nearly a third of a person's life is spent in slumber. Research done outside of economics suggests that quantity and quality of sleep can have major impacts on the development of human and health capital. For example, sleep disorders have been found to affect absenteeism, productivity, and injury rates among adults [Hillman et al. 2006]. In this paper, we focus on the relation between the amount of sleep adolescents receive and their performance on standardized tests. We allow for a nonlinear relation between sleep and test scores, and thus we are able to estimate the “optimal” hours of sleep that maximize student test score performance. There has been substantial work done outside of economics on how academic performance is affected by sleep, especially for children and teens. Wolfson and Carskadon [2003] is an excellent summary of the medical research in this area. One group of papers they review examines sleep-wake patterns and grades. They note that a cause and effect relationship has not been established, but that some patterns do emerge when considering these studies collectively. They find that self-reported sleep quantity, delayed and/or erratic sleep schedules, late weekend rise times, and daytime sleepiness are associated with poor school performance for children and adolescents. Another group of papers reviewed by Wolfson and Carskadon compare academic performance for early vs later starting schools, and for students who are “larks” (morning-type phase preference) vs “owls” (evening-type phase preference). These studies suggest that self-reported “owls,” delayed sleep schedules, and early school start times may be associated with daytime sleepiness, dozing in class, attention difficulties, and poorer academic performance. Wolfson and Carskadon urge caution in interpreting these findings because of differences in the details of these studies, particularly in the measurement of sleep and academic performance and in the omitted factors. Taras and Potts-Datema [2005] also review several studies on the relation between sleep and student performance. Many of the studies focus on how sleep disorders and habits among adolescents are correlated with educational outcomes. For example, studies find that sleep-related obstructive breathing is associated with reduced attention, memory, intelligence, and increased problematic behavior. The studies also show improvement in academic performance among children with disordered breathing problems after corrective measures are taken. While few studies in economics examine the relation between sleep and student performance, two recent studies take on this issue, both from the perspective of school start times. Carrell et al. [2011] identify the causal effect of school start time on academic achievement by using two policy changes in the daily schedule at the US Air Force Academy along with the randomized placement of freshman students to courses and instructors. Results show that starting the school day 50 minutes later has a significant positive effect on student achievement, which is roughly equivalent to raising teacher quality by 1 standard deviation. Hinrichs [2011] studies whether moving school start times later in the morning improves academic performance. He first focuses on the Twin Cities metropolitan area, where Minneapolis and several suburban districts have made large policy changes, but St. Paul and other suburban districts have maintained early schedules. He uses individual-level ACT test score data on all individuals from public high schools in this region who took the ACT between 1993 and 2002 to estimate the effects of school starting times on ACT scores. He then employs school-level data on starting times and test scores on statewide standardized tests from Kansas and Virginia in order to estimate the effects of school start times on achievement for a broader sample of students. The results do not suggest an effect of school starting times on achievement. The causes of sleeplessness among children and teenagers can come from a variety of sources, both behavioral and physiological, which are described in Mitru et al. [2002]. Parental involvement in setting sleep and wake times can be important for establishing sleep patterns among younger children. Older children and teens often have more control over their sleep schedule, and are more likely to engage in unsupervised night time activities than younger children. Older students may stay up later doing homework and wake up earlier for school. Many teenagers work part-time after school, which may push back bedtime as they stay up later in order to complete homework, socialize with friends and family, or relax. Some children suffer from sleep-disordered breathing such as sleep apnea and respiratory disturbances such as asthma, which can also cause sleep problems. How sleep affects the development of human capital for elementary and secondary students is a complex and fascinating topic. This research question fits into a broader emerging area of research in economics, which examines the connection between health and education outcomes. The early theoretical work in this area was put forth by Grossman [1972], with more recent contributions to theory provided by Becker [2007]. The empirical research estimating the relation between education and health is rapidly growing. Eide and Showalter [2011] provide a review of recent research examining how education may affect health outcomes, and vice versa. In this paper, we explore the relationship between sleep and student performance for youth ages 10 through 19, with a focus on estimating the optimal hours of sleep that maximize test score performance. We rely on the well-developed theoretical link between health, human capital, and productivity and assume that sleep affects health. We use standardized math and reading test scores as our measure of human capital. Our regression approach uses a quadratic term in sleep, interacted with age, which then allows us to determine age-adjusted optimal sleep hours. This contrasts sharply with the bulk of previous work, which assumes a simple linear relationship between sleep and test scores. We also use nationally representative data — the Child Development Supplement (CDS) of the Panel Study of Income Dynamics (PSID) — which contrast with previous work that typically uses information from individual schools and districts, and is often based on small sample sizes. The CDS/PSID data also includes a rich set of socioeconomic variables for the household and the child that allows us to control for confounding variables; these variables have often been unavailable in previous research on sleep. Much of the work examining the relation between sleep and student performance is based on data from controlled environments, for example when a child's sleep is regulated or restricted in some way in order to examine how performance changes under different sleep conditions [Sadeh et al. 2003]. The few papers that use regression analysis use outcomes such as grades, tardiness, or problem behavior. Our regression approach of modeling standardized test scores as a nonlinear function of sleep based on a large sample adds new evidence to the body of knowledge on the relation between academic performance and sleep. Key to our paper is the idea of optimal sleep. The way in which we think about this idea is based on how much sleep a student needs to maximize test score performance. The medical literature has conceptualized optimal sleep in a different way [Carskadon et al. 1980; Sadeh et al. 2003]. The seminal research characterizing optimal sleep from a medical perspective comes mainly from a longitudinal study completed at the Stanford University sleep camp during the 1970s [Carskadon et al. 1980]. The participants enrolled at 10–12 years of age and their sleep was monitored every year for 5–6 years. Researchers found that regardless of age, when adolescents were allowed to control the amount of sleep they obtained between 10 p.m. and 8 a.m., all children slept about 9.25 hours. Hence, 9.25 is the generally accepted hours of sleep that is considered optimal for adolescents, and which is cited in sleep policy recommendations [National Sleep Foundation 2000]. Our results show a statistically significant and relatively large effect of sleep on test scores, with the quadratic results suggesting negative effects on test scores beyond the optimal level. Results are similar across the four academic tests administered in the CDS. Optimal hours decline with age and tend to be lower than the 9.25 hours, which is suggested by medical research and which is promoted in sleep policy guidelines.",39
38,4,Eastern Economic Journal,06 February 2012,https://link.springer.com/article/10.1057/eej.2011.34,The Empirical Validation of an Agent-based Model,September 2012,Pasquale Cirillo,Mauro Gallegati,,Male,Male,Unknown,Male,"The aim of this paper is to understand to what extent the bottom-up agent-based macroeconomic (BAM) model described in Gaffeo et al. [2008] Footnote 1 is able to reproduce real world phenomena, by applying tools and techniques of empirical external validation [Fagiolo et al. 2008]. Agent-based (AB) models [Axelrod 1997; Axtell 2000] have been developed to study the behavior of several heterogeneous interacting agents. They are based on microfoundations, according to a bottom-up approach: starting from simple behavioral rules at the individual level, some aggregate statistical regularities appear and they cannot be inferred from the behavior of single agents in isolation. This emergent behavior often feeds back on individuals, respectively, affecting their actions. In other words, micro and macroeconomic behaviors co-evolve in an adaptive way. From this point of view, the pattern of the aggregate is not the result of a simple summation and averaging of individual preferences and choices, but it is the product of self-organization “from the bottom-up.” A self-organized macroeconomy is furthermore susceptible of abrupt phase transitions when a scenario of criticality occurs. In the literature, the number of specific AB models addressing macroeconomic issues is still relatively small. Recent contributions include the model of Gaffeo et al. [2008], here analyzed, which is on the wake of other AB models such as Delli Gatti et al. [2005], Gaffeo et al. [2007], Russo et al. [2007] and Delli Gatti et al. [2008], but also the works of Dawid et al. [2008], Haber [2008] and Delli Gatti et al. [2010]. Moreover we cannot forget the European EURACE project, for which we refer to Cincotti et al. [2010] and the references there cited. An interesting survey about AB macroeconomics can be found in Chen [2003; 2005]. Finally we suggest to visit the constantly updated websiteFootnote 2 of Leigh Tesfatsion on AB macroeconomics. Most AB models present simulations carried out using ad hoc parameters’ values and the same initial set up for all the agents belonging to the same classes. Paraphrasing Nicholas Kaldor, they start from some hypothesis that could account for some stylized facts (say, the tendency of modern economies to fluctuate around a growing path), without necessarily adhere to historical accuracy [Kaldor 1965]. In recent years, however, thanks to the increasing popularity of this kind of models and the desire to use them for policymaking, researchers have started considering the issue of their validation [Axtell et al. 1996], to assess whether their results may be considered correct, where correct means “able to reproduce actual phenomena with a sufficiently good level of precision.” As Sargent [1998] says: “This concern is addressed through model verification and validation. Model validation is usually defined to mean substantiation that a computerized model within its domain of applicability possesses a satisfactory range of accuracy consistent with the intended application of the model.” Using an initial set up of actual Italian data, we currently aim at verifying whether the BAM model, simulated over a period for which actual data are fully available, is an acceptable representation of the actual system at micro, meso and macroeconomic scales. In a nutshell, we intend to perform an ex-post validation of a microsimulated version of the model. We shall be in a position to conclude that the result of this validation exercise is positive if the model outcome represents a good approximation of the actual data generating process. While the validation process should not be regarded in any case as a formal prerequisite for the development of an AB model and for a discussion of its properties and results [Carley, 1996], a careful assessment of the adequacy and accuracy of the model in matching real world data is fundamental to foster the use of AB models for policy reasons, and should help us in resisting “the temptation to use any data input that will enable one to continue playing what is perhaps the ultimate game of solitaire” [Lovelock 2000]. As we show in the paper, the BAM model of Gaffeo et al. [2008] is able to reproduce the dynamics of actual data with a good degree of precision along several dimensions. In particular, it nicely replicates some interesting cross-sectional results about size distributions and other remarkable pieces of empirical evidence. Naturally some defects are present, especially in the labor market, and we discuss them too. The paper is organized as follows: the following section describes the validation procedure we use to assess the goodness of the BAM model; the next section summarizes the basic features of the model of Gaffeo et al. [2008]; the subsequent section describes the actual data we use for validating; the penultimate section is devoted to the results of the validation exercise and their robustness; and the last section concludes.",18
38,4,Eastern Economic Journal,30 August 2012,https://link.springer.com/article/10.1057/eej.2010.19,"Stuck in the Middle: Is Fiscal Policy Failing the Middle Class?, by Antonio Estache and Danny Leipziger",September 2012,Sasha Breger Bush,,,,Unknown,Unknown,Mix,,
38,4,Eastern Economic Journal,30 August 2012,https://link.springer.com/article/10.1057/eej.2010.23,"Economics Versus Human Rights, by Manuel Branco",September 2012,Nancy Ruth Fox,,,Female,Unknown,Unknown,Female,,
38,4,Eastern Economic Journal,30 August 2012,https://link.springer.com/article/10.1057/eej.2010.36,"The Return of Depression Economics and the Crisis of 2008, by Paul Krugman",September 2012,Nazneen Ahmad,M Imtiaz Mazumder,,Unknown,Unknown,Unknown,Unknown,,
38,4,Eastern Economic Journal,30 August 2012,https://link.springer.com/article/10.1057/eej.2010.37,"Welfare Reform and Its Long-Term Consequences for America's Poor, by James P. Ziliak",September 2012,James A Buss,,,Male,Unknown,Unknown,Male,,
38,4,Eastern Economic Journal,30 August 2012,https://link.springer.com/article/10.1057/eej.2010.38,"The Hesitant Hand: Taming Self-Interest in the History of Economic Ideas, by Steven Medema",September 2012,David Colander,,,Male,Unknown,Unknown,Male,,
38,4,Eastern Economic Journal,30 August 2012,https://link.springer.com/article/10.1057/eej.2010.39,"Reforming the World Bank: Twenty Years of Trial — and Error, by David A Phillips",September 2012,Amyaz A Moledina,,,Unknown,Unknown,Unknown,Unknown,,
38,4,Eastern Economic Journal,30 August 2012,https://link.springer.com/article/10.1057/eej.2010.40,"The Provocative Joan Robinson: The Making of a Cambridge Economist, by Nahid Aslanbeigui and Guy Oakes",September 2012,Claudio Sardoni,,,Male,Unknown,Unknown,Male,,
39,1,Eastern Economic Journal,06 February 2012,https://link.springer.com/article/10.1057/eej.2011.35,Cross-Sector Spillover Effects of Trade Liberalization,January 2013,Aleksandr Vashchilko,,,Male,Unknown,Unknown,Male,"This study looks at the spillover effects of trade liberalization in one sector on firms in the other sector. The spillover effects are analyzed in the framework that takes into account the stylized facts about trade. To account for stylized facts about international trade, the traditional approach with perfect competition market structure in two sectors was modified in line with recent literature. One stylized fact suggests substantial intra-industry trade among industrialized countries, which has grown over time [Balassa 1966 and Helpman 1999]. Krugman [1979] addressed intra-industry trade in a one-sector model with monopolistic competition market structure by modifying Chamberlin's [1962] approach. Further, in the setup of a two-country two-sector model with sector-specific factors of production, Krugman [1981] demonstrated the positive relationship between the extent of intra-industry trade and the similarity in factor endowments across countries. In addition, the traditional approach does not account for the considerable heterogeneity of firms with respect to productivity and self-selection of firms into exporting. Clerides et al. [1998], among others, provided substantial evidence for this stylized fact. To address the mentioned stylized facts, Melitz [2003] introduced heterogeneous firms on the top of Krugman's [1979] monopolistic competition market structure in a one-sector model with many countries. Melitz found that with trade liberalization the average productivity of firms increases, since less productive firms leave the market (self-selection effect). Although Melitz's [2003] model takes into account both stylized facts, this model is a one-sector model. The drawback is the impossibility of studying the effect of trade liberalization (reduction in the trade costs) in one sector on the average productivity of firms and the number of firms in the other sector. Later, these effects are referred to as the spillover effects of trade liberalization. The contribution of this paper is to add a sector to Melitz's [2003] model in order to study the spillover effects of trade liberalization in one sector on the average productivity of firms and on the number of firms in the other sector. The number of countries is limited to two. While keeping the monopolistic competition market structure with heterogeneous firms in one sector, we assume the perfect competition market structure in the other sector. This allows to concentrate on the effects of the reduction in one sector's trade costs on the firms in the other sector and abstract from the influence on the firms in the same sector. In addition, it is assumed that the factors of production are sector specific. The spillover effects of trade liberalization were addressed by Vashchilko [2008] in the setup of specific factors model with monopolistic competition and heterogeneous firms in both sectors. The obtained results are similar to the results in this study. At the same time, the mechanism of spillover effects is less clear because of model complexity. The spillover effects of the reduction in variable trade cost in the sector with perfect competition on the firms in the differentiated commodity sector of a particular country are sector dependent. The average productivity of firms increases and the number of active firms decreases in a comparative advantage sector in response to trade liberalization in the other sector. Conversely, the average productivity of firms decreases and the number of active firms increases in the comparative disadvantage sector in response to trade liberalization in the other sector. This finding allows for new predictions about the changes in the average productivity of firms in the differentiated commodity sector of a particular country, when trade costs in both sectors have been reduced. The following mechanism explains the effects of trade liberalization in the sector with perfect competition market structure on the average productivity of firms in a differentiated product sector. With the decrease in trade cost in a perfectly competitive, import-competing sector of a country, the wage rate in this country's sector decreases relative to the wage rate in the perfectly competitive sector of the other country. As a result, the income in the country as a whole decreases relative to the wage rate in the perfectly competitive sector of the other country. The reduction in a country's income leads to the decrease in the demand for differentiated commodity. The country's firms selling differentiated commodity in a domestic market start to make smaller profits. The least productive firms will be leaving the market. And the average productivity of firms in the differentiated product sector with comparative advantage increases. On the contrary, with the decrease in trade cost in a perfectly competitive, export-competing sector of a country, the wage rate in this country's sector increases relative to the wage rate in the perfectly competitive sector of the other country. As a result, the income in the county as a whole increases relative to the wage rate in the perfectly competitive sector of the other country. The increase in a country's income leads to the increase in the demand for differentiated commodity. The country's firms selling differentiated commodity in a domestic market start to make higher profits. As a result, the firms with even smaller productivities start selling in a domestic market. The average productivity of firms in a differentiated product sector with comparative disadvantage decreases. This study relates to the research by Bernard et al. [2007]. Although Bernard et al. [2007] consider the transition from autarky to costly trade in a two-country two-sector model with differentiated product in both sectors and two mobile factors of production, the consequences of trade liberalization in one sector on the average productivity of firms in the other sector are not analyzed. One of the results in Bernard et al. [2007] is that, in the transition from autarky to costly trade, zero-profit productivity cutoff increases more in the comparative advantage sector than in the comparative disadvantage sector. It should be mentioned that this result was derived under the assumption that technology differs across sectors with respect to factor intensity, but the variable trade cost, fixed cost of exporting, and fixed cost of production are the same across sectors. While it is not clear whether this result in Bernard et al. [2007] would hold in case the mentioned costs are different across sectors, Proposition 2 in this paper provides the mechanism for the result in Bernard et al. [2007] without imposing any constrains on the cost parameters across sectors. The main point is that in the comparative advantage sector the direct effect of trade liberalization is reinforced by the spillover effect, while in the comparative disadvantage sector the direct effect is offset by the spillover effect. The reinforcement of direct effect by spillover effect in the comparative advantage sector implies a larger increase in zero-profit productivity cutoff in the comparative advantage sector when moving from autarky to costly trade in the case of Bernard et al. [2007]. Apart from the spillover effects of trade liberalization analyzed in this paper, other types of spillover effects of trade liberalization were explored in the literature. Ben-David and Loewy [1998] looked at the knowledge spillover effects associated with trade liberalization. In their setup, the country's per capita output positively depends on the aggregate stock of knowledge this country possesses. Further, through trade with other countries, the country's stock of knowledge increases as it gets access to the knowledge of other countries. The increase in knowledge is larger, the larger the trade between countries. In this setup, trade liberalization between any two countries positively affects the growth rates of all trading countries. Bernard et al. [2006] outlined the channel through which trade liberalization affects a firm's productivity. They extended Melitz's [2003] framework by allowing the production and export of the range of products by each firm. In their framework, with trade liberalization, a firm stops the production of products in which it is less productive. This leads to the increase in a firm's productivity. Further, Helpman et al. [2010] explore the effect of trade liberalization on unemployment and wage inequality. The remainder of this paper includes: The next section develops the model; the subsequent section explores the properties of the costly trade equilibrium; the penultimate section provides results and describes the underlying mechanisms behind stated results; and the final section provides conclusions.",1
39,1,Eastern Economic Journal,06 February 2012,https://link.springer.com/article/10.1057/eej.2012.1,Capital Intensity and US County Population Growth During the Late 19th Century,January 2013,Burton A Abrams,Jing Li,James G Mulligan,Male,,Male,Mix,,
39,1,Eastern Economic Journal,05 March 2012,https://link.springer.com/article/10.1057/eej.2012.3,Estimating Willingness to Pay for River Amenities and Safety Measures Associated with Shale Gas Extraction,January 2013,Paula Bernstein,Thomas C Kinnaman,Mengqi Wu,Female,Male,Unknown,Mix,,
39,1,Eastern Economic Journal,05 March 2012,https://link.springer.com/article/10.1057/eej.2012.4,Differences Do Not Matter: Exploring the Wage Gap for Same-Sex Behaving Men,January 2013,Michael E Martell,,,Male,Unknown,Unknown,Male,"An emerging literature suggests that same-sex behaving men earn significantly less than heterosexual men. However, this literature does not document the source of these earnings differentials. Documenting the source of these differentials is important, as there is a contemporary policy debate over the need for employment protection for gay men and lesbians. I build on the existing literature and contradict the findings of Carpenter [2005] by showing that wage differentials for same-sex behaving men are significant and unrelated to differences in characteristics of same-sex behaving and heterosexual men. I estimate that same-sex behaving men earn 12–20 percent less than different-sex behaving men. I build on the findings in the existing literature by decomposing earnings differentials and showing that none of the earnings differential experienced by same-sex behaving men is related to differences in worker characteristics between same-sex behaving and different-sex behaving men. This result builds on the only other wage decomposition technique addressing the entire same-sex behaving community in the United States [Berg and Lien 2002] by using improved data and a refined empirical specification. In arriving at these results, I am the first to be able to compare self-identified sexual orientation with the sexual behavior proxies utilized in the bulk of this emerging literature. My work refines existing estimates in an emerging literature that document the size, but not the source, of wage differentials for same-sex behaving workers. To explore the source of the differentials, I develop a signaling model of wage determination to show how concealing and revealing sexual orientation can affect wage differentials for gay men. I show that there is an equilibrium in which wage differentials may reflect compensating differentials that gay men accept in exchange for the ability to reveal their sexual orientation in tolerant workplaces. The model brings about two important conclusions that do not follow from traditional models of discrimination [e.g., Becker 1957]. First, increased competition will not necessarily decrease wage differentials for gay men, as they occur in equilibrium. Second, my work highlights the fact that, if wage differentials are to be eliminated, public policy mechanisms must increase the number of tolerant workplaces and not simply punish discriminating firms. To measure the size and source of wage differentials, I use General Social Survey (GSS) data for 1994–2008.Footnote 1 I classify respondents as same-sex behaving using questions about sexual history. I am the first to compare sexual history with self-identified sexual orientation to show that sexual history is a good proxy for self-identified sexual orientation. Classifying respondents as same-sex behaving using sexual history allows my results to be more generalizable than many of the results in this emerging literature, which only apply to same-sex behaving men in cohabiting relationships. My classification method also allows me to provide robustness checks that much of the existing literature cannot. I augment the GSS data with income data from the Current Population Survey (CPS) and control for state Employment Non-discrimination Acts (ENDAs) that make sexual orientation discrimination illegal to obtain the most precise estimates of wage differentials currently possible. My data allow me to demonstrate more convincingly than others that wage differentials cannot be explained by differences in worker characteristics. I find that if same-sex behaving workers had identical characteristics to different-sex behaving workers, same-sex behaving men would experience wage differentials that are 33 percent larger than that which is observed in wage function estimates.",25
39,1,Eastern Economic Journal,09 April 2012,https://link.springer.com/article/10.1057/eej.2012.5,"Wealth, Human Capital and the Transition to Self-Employment",January 2013,Berna Demiralp,Johanna L Francis,,Female,Female,Unknown,Female,"Entrepreneurship has been widely recognized as an important contributor to economic growth and job creation. Therefore, how the economy fosters entrepreneurs has been a topic of interest to both researchers and policymakers. A substantial portion of the research on this topic has focused on whether potential entrepreneurs are constrained in obtaining capital to start businesses. In this paper, we build on the existing research by investigating whether potential entrepreneurs differ in the degree of capital constraints they face. In particular, we ask the following question: Do more educated and more experienced people face higher capital constraints when they consider becoming entrepreneurs? The answer to this question has important implications for both policymakers and researchers. From a policy perspective, identifying the characteristics of those potential entrepreneurs who are most likely to face credit constraints would help policymakers to better channel funds intended to ease these constraints. Shifting the debate from whether potential entrepreneurs are credit constrained as a group, to whom within the group of potential entrepreneurs are credit constrained is an important step in formulating public policy fostering entrepreneurship. Investigating the heterogeneity of aspiring entrepreneurs with respect to their degree of liquidity constraints can also help us better understand and perhaps reconcile the wide range of empirical results on the existence of liquidity constraints for potential entrepreneurs. While the question of whether potential entrepreneurs are credit constrained in starting businesses has been studied extensively, a consensus on the empirical results has failed to form.Footnote 1 In this paper, we focus on how the degree of liquidity constraints may vary across entrepreneurs with different levels of human capital. Human capital is an important dimension of heterogeneity among entrepreneurs, and its effect on the degree of liquidity constraints can be complicated by the numerous ways in which it can affect entrepreneurial choice. Human capital can impact both wage earnings and entrepreneurial earning, so returns to human capital in the wage sector and entrepreneurial work have to be considered in assessing how human capital affects entrepreneurial choice. Furthermore, human capital can affect entrepreneurial earnings in two ways: it can affect entrepreneurial earnings directly by increasing the productivity of the entrepreneur and also indirectly by relaxing or tightening liquidity constraints. Since human capital can influence many aspects of the entrepreneurial decision, its net effect on entrepreneurial choice and liquidity constraints faced by aspiring entrepreneurs critically depends on the specifications of the theoretical models. For example, in the Evans-Jovanovic model [1989] human capital only enters the wage equation and not the entrepreneurial earnings equation. As a result of its positive effect on wage, human capital has a negative effect on the probability of becoming an entrepreneur and no effect on liquidity constraints. Astbro and Bernhardt [2005] extend the Evans-Jovanovic model by endogenizing the savings decision and making self-employment earnings dependent on human capital. Their model implies that the effect of human capital on liquidity constraints may be positive or negative depending on the relative sizes of returns to human capital in wage and entrepreneurial work. On the other hand, Parker and van Praag [2006] extend Bernhardt's [2000] model of credit rationing to include human capital. They show that theoretically the effect of human capital on liquidity constraints is ambiguous if the entrepreneur's production function is non-separable in human and physical capital. If it is separable, greater human capital relaxes credit constraints of potential entrepreneurs. Our approach in this paper is one of empirical investigation. We use data from the National Longitudinal Survey of Youth (NLSY) to examine whether the degree of liquidity constraints faced by aspiring entrepreneurs varies with human capital. In that respect, our paper complements a significant body of research aimed at the role of liquidity constraints in entrepreneurial choice.Footnote 2 By focusing on the heterogeneity of entrepreneurs with respect to human capital, we investigate whether the findings of previous studies might mask differences among entrepreneurs. The results of our empirical work can also be used in developing theoretical models that illustrate the link between human capital, liquidity constraints, and entrepreneurial entry. We follow a widely used approach to testing for liquidity constraints by looking for a positive relationship between wealth and the likelihood of becoming an entrepreneur. The intuition behind this test is that if individuals are restricted in their ability to finance their businesses by the need to provide collateral for loans, then people with low wealth are likely to face capital constraints in opening their businesses. The key in such an analysis is to find appropriate instrumental variables (IVs) for wealth, since wealth might be endogenous to unobservable factors also determining entrepreneurial entry. We use housing price changes, initially proposed by Hurst and Lusardi [2004], as an instrument for wealth in our analysis. In our pooled sample, we find that wealth has a statistically insignificant effect on the probability of becoming an entrepreneur, suggesting evidence for lack of liquidity constraints. However, we do observe differences in this relationship across people with different levels of human capital. Among individuals with less than a high school degree and those with low work experience, wealth is negatively related to the likelihood to become an entrepreneur. This negative relationship cannot be explained by the credit rationing theory of Evans and Jovanovic. We offer a potential explanation for our finding. This paper consists of five sections. In the first section, we discuss the theoretical framework and the econometric model underlying, our analysis, and in the next section we describe the data used. The following section presents our empirical findings, and the subsequent section provides a discussion of our results. The last section concludes with a summary of our findings and suggestions for future work.",4
39,1,Eastern Economic Journal,02 April 2012,https://link.springer.com/article/10.1057/eej.2012.6,"Linkage Effects, Oligopolistic Competition, and Core-periphery",January 2013,Haiwen Zhou,,,Unknown,Unknown,Unknown,Unknown,,
39,1,Eastern Economic Journal,06 August 2012,https://link.springer.com/article/10.1057/eej.2012.22,Using a Difference-in-Differences Approach to Estimate the Effects of Teacher Merit Pay on Student Performance,January 2013,Mark Gius,,,Male,Unknown,Unknown,Male,"The vast majority of public school teachers in the United States are compensated according to a single salary schedule [Podgursky 2008]. With a single salary schedule, teacher pay is based solely upon two factors: years of experience and level of education. Under a single salary schedule, the quality of a teacher's instruction has no bearing on their pay. Instead, the more seniority and education a teacher has, the greater is their salary. It is commonly argued that such a system does not promote teacher development or student performance. It is believed that a single salary schedule protects ineffective, older teachers who, by virtue of their seniority, are among the highest paid teachers. Furthermore, under a single salary schedule, many effective and dynamic teachers are paid less than poorly performing teachers, thus demoralizing the better teachers and possibly driving them from the profession. Hence, it is believed that a merit pay system for teachers that rewards individual performance should increase student performance and do much to improve public education in the US. Many teachers, however, oppose merit pay primarily because they believe that it would undermine one of the most important aspects of teaching: collaboration. Teachers would not be willing to collaborate under such a system because it would detract from their individual performance and potentially reduce their compensation. If there is a fixed pool of funds for merit pay, then helping a fellow teacher improve their teaching may result in less merit pay for everybody else; hence, collaboration would suffer if merit pay were implemented. Murname and Cohen [1986] provide supporting evidence for the teachers’ argument against merit pay and suggest that this type of compensation system is not a viable option in education. They claim that many factors affect student achievement and to hold teachers directly accountable for the academic success of their students is unfair and neglects the roles that others [other teachers, parents, school administrators, and the students themselves] have in achieving scholastic success. In addition, as noted earlier, collaboration is very important in teaching [Murname and Cohen 1986]. Teachers should be encouraged to collaborate. However, under most merit pay plans, the role of collaboration or joint production is ignored. A successful and fair merit pay system must find a way to accurately separate individual effort from team effort and reward each accordingly. It is difficult, however, to separate out individual performance from team achievement in joint production efforts, such as teaching. Unlike the production of some goods or services, the output of the educational process is ambiguous and difficult to measure. It is difficult to parse out exactly what impact a teacher has on a student vs the effects of other influences. Regarding prior research on this topic, an excellent review of the empirical literature and a descriptive analysis of several merit pay plans that were used in various schools and districts in the United States are presented in Podgursky and Springer [2007]. As noted in this review, most of the empirical studies done on the effectiveness of teacher performance pay have used only regional data or data from foreign schools [Ladd 1999; Eberts, Hollenbeck and Stone, 2002; Lavy 2002, 2009; Goodman and Turner 2009; Glazerman and Seifullah 2010; Glewwe et al. 2010; Fryer 2011]. The use of these types of data may limit the applicability of their results. Prior research in this area has yielded mixed results. Merit pay had positive effects on student performance in India [Muralidharan and Sundararaman 2011], Israel [Lavy 2002, 2009], and Dallas [Ladd 1999]. However, in Chicago [Glazerman and Seifullah 2010], Kenya [Glewwe et al. 2010], New York [Goodman and Turner 2009], Texas [Lincove 2012], and Michigan [Eberts et al. 2002], the effects of merit pay on student performance were found to be insignificant. Most of this prior research looked at only small subsets of schools, schools in only one state, or schools in foreign countries. In several of these studies, the authors admitted that their samples were non-random. Only one prior study looked at the effects of teacher merit pay using data from across the United States [Figlio and Kenny 2007]. Their results suggest that those schools that have merit pay for teachers exhibit greater student achievement, where achievement is measured by 12th grade test scores for math, reading, science, and history. However, even Figlio and Kenny [2007] admitted that they cannot determine if this relationship is causal or if better performing schools are just more likely to implement teacher merit pay programs. The present study expands on this body of research by using district-level, restricted-access data from the US Department of Education in order to determine if merit pay has any statistically-significant effects on student achievement.",2
39,1,Eastern Economic Journal,22 October 2012,https://link.springer.com/article/10.1057/eej.2012.30,Are Women More Generous than Men? Evidence from Alumni Donations,January 2013,Tomas Dvorak,Shayna R Toubman,,Male,Female,Unknown,Mix,,
39,1,Eastern Economic Journal,03 December 2012,https://link.springer.com/article/10.1057/eej.2010.41,"Handbook of Research on Complexity, by J. Barkley Rosser, Jr. and Edward Elgar",January 2013,Troy Tassier,,,Male,Unknown,Unknown,Male,,
39,1,Eastern Economic Journal,03 December 2012,https://link.springer.com/article/10.1057/eej.2010.42,"Keynes: The Return of the Master, by Robert Skidelsky",January 2013,L Randall Wray,,,Unknown,Unknown,Unknown,Unknown,,
39,1,Eastern Economic Journal,03 December 2012,https://link.springer.com/article/10.1057/eej.2011.10,"Educating Economists: The Teagle Discussion on Re-evaluating the Undergraduate Economics Major, by David Colander and KimMarie McGoldrick",January 2013,Deborah M Figart,,,Female,Unknown,Unknown,Female,,
39,1,Eastern Economic Journal,03 December 2012,https://link.springer.com/article/10.1057/eej.2011.11,"Expectations, Employment and Prices, by Roger E.A. Farmer How the Economy Works: Confidence, Crashes and Self-Fulfilling Prophecies, by Roger E.A. Farmer",January 2013,William D Craighead,,,Male,Unknown,Unknown,Male,,
39,1,Eastern Economic Journal,03 December 2012,https://link.springer.com/article/10.1057/eej.2011.12,"Insufficient Funds: Savings, Assets, Credit, and Banking among Low-Income Households, by Rebecca M. Blank and Michael S. Barr",January 2013,Robert Scott,,,Male,Unknown,Unknown,Male,,2
39,2,Eastern Economic Journal,07 May 2012,https://link.springer.com/article/10.1057/eej.2012.7,Does Money Matter in Pennsylvania? School District Spending and Student Proficiency Since No Child Left Behind,March 2013,Sean Flaherty,,,Male,Unknown,Unknown,Male,"After several decades of empirical study, much contention remains among policymakers, practitioners, and scholars about the relationship between public school spending and student achievement. The scholarly debate about what the evidence shows is represented both broadly and succinctly in the 1996 Brookings volume, Does Money Matter?, particularly in essays by Burtless, Hanushek, and Hedges and Greenwald.Footnote 1 Writing a decade later on the “weak effects” of school funding, W. Norton Grubb [2006] pronounced the debate a “stalemate,” noting that “money might make a difference under certain conditions, but it remains unclear what these conditions are.” Most if not all would agree that the circumstances within which schooling expenditures are made will play an important role in determining the extent to which such spending can and does affect student outcomes. If incentives to maximize success in the measured outcomes — particularly standardized test scores — are not everywhere consistent and compelling for both students and educators, then enhanced resource availability may not necessarily produce the desired results. Indeed, Eric Hanushek [1996], in judging the totality of evidence as having shown that resources and student performance are “at best weakly linked,” offers an explanation based on incentive failure: “school personnel — teachers, principals, superintendents, librarians, and other staff — have little at stake in student outcomes.” If this is an overly broad and coarse indictment, it nevertheless points to a reasonable concern about the role of incompletely or malformed incentives in vitiating the relationship between “money” and student performance and outcomes. But such a concern is arguably less compelling today than it may have been in decades past. No Child Left Behind (NCLB), the 2001 reauthorization of the federal government's Elementary and Secondary Education Act, has done nothing if not shaken up incentive dynamics in America's public schools. With its “report cards” on “achievement gaps” and “adequate yearly progress,” post-NCLB rhetoric and policy implementation has focused the attention of administrators, teachers, students, parents, and politicians on the standardized test scores that are the hallmark of a new “accountability framework” for public education. Whether for better or for worse, the concerns and energies of educators in America's public schools have been effectively channeled toward preparing students to demonstrate “proficiency” on state-sponsored standardized tests. It is within this context of a realigned and seemingly more coherent incentive structure in America's public schools that I seek in the research this article reports to determine whether changes in per pupil spending across Pennsylvania school districts can be linked to improvements in the percentage of students scoring “proficient or better” on the Pennsylvania System of School Assessment (PSSA) tests. This research takes a “value added” approach but is conducted at the district-level of aggregation, comparing the percentage of a district's 8th grade students who scored proficient or better to the similarly defined success of that district's 5th grade students 3 years earlier, and likewise with respect to a district's 11th and 8th grade students over the same period of time. Using a fixed-effects estimation framework to account for differences in socio-economic conditions and potentially important environmental factors across Pennsylvania's 500 school districts,Footnote 2 I find systematic and statistically significant evidence that students in districts with more rapid growth in regular education expenditures show larger improvements in their aggregate “passing rates” on the state's standardized math and reading tests, particularly so among the 5th–8th grade cohorts, and particularly so again in the case minority students and/or those of economically disadvantaged (ED) status.Footnote 3",
39,2,Eastern Economic Journal,04 June 2012,https://link.springer.com/article/10.1057/eej.2012.9,Playing Well with Others: The Role of Social Capital in Traffic Accident Prevention,March 2013,Matthew G Nagler,,,Male,Unknown,Unknown,Male,"Social capital has received increased attention in recent years from academic researchers across the social sciences, as well as journalists and policy makers. A considerable volume of work suggests that interpersonal connections, trust, and civic engagement have positive economic impacts [e.g., Helliwell and Putnam 1995; Narayan and Pritchett 1999; Knack 2001; Zak and Knack 2001; Grootaert et al. 2002]. Research also indicates that social capital, measured variously, has beneficial effects on health and well-being.Footnote 1 Most of the empirical evidence has centered on statistical associations between social capital and other variables. Definitive causation has been hard to establish [Helliwell 2001], and the putative causal mechanisms involved in social capital's economic and health effects have often been discussed loosely with little hard evidence [Sobel 2002]. As an exception to this, a few recent studies have shown that social connections create economic benefits specifically by acting as collateral to secure transactions and reduce moral hazard [Karlan et al. 2009; Feigenberg et al. 2011; Jackson and Schneider 2011]. These studies have demonstrated that people are motivated to make good on commitments to repay loans and protect property — in effect, fulfilling implicit contracts — so as not to jeopardize their relationships with people they know or, alternatively, risk their standing within close-knit groups (for example, ethnic communities). The studies provide convincing evidence of economic benefits that follow directly from social cohesion in the context of close relationships. But important questions remain unanswered. Does social connectedness provide demonstrable benefits (that is, with demonstrable causality) beyond the scope of close relationships? Is it possible to identify behavioral pathways by which social capital propagates to beneficial outcomes in the general context? This paper contributes evidence on the role of social capital in creating economic benefit in a broader relational context. Specifically, I provide evidence that social capital within a geographically defined community leads to welfare-enhancing pro-social behavior among people who do not know each other nor share any obvious group connection — people who meet on the roads. Nagler [forthcoming] previously showed that higher measures of social capital are causally associated with a number of improved highway safety outcomes. The study made use of aggregate measures of interpersonal trust and investment in communal ties to explain variations in the level of traffic fatalities and three other measures of highway safety across a panel of US states over the years 1997–2006. The present paper investigates the mechanism underlying this effect by examining which types of traffic incidents are most strongly influenced by social capital. I distinguish traffic incidents on two dimensions, intended to indicate whether interpersonal interactions are more or less likely to play a role: by number of vehicles involved (multi-vehicle vs single vehicle) and by location (junction-related vs non-junction-related). As motivation for the first distinction, consider Figures 1, 2 and 3. Figure 1 plots the rate of fatalities in multi-vehicle and single-vehicle crashes by US state against survey respondents’ average level of agreement with the statement, “Most people are honest.” Figure 2 plots the same two incident rate measures against the share of people by state who said they believe “most people can be trusted.” Figure 3 plots the two measures against voter turnout by state, a measure of the civic engagement of the populace. A consistent pattern can be observed in the plots. While a strong negative relationship exists between the rate of fatalities in multi-vehicle accidents and each measure of social capital, the relationship between single-vehicle fatalities and social capital appears far more diffuse and less clearly negative. It would seem, observationally, that the role of social capital in explaining single-vehicle fatalities, in which inter-driver interactions are not necessarily a factor, is much less clear its role in explaining multi-vehicle fatalities, where inter-driver interactions are a crucial factor. The hypothesis that social capital promotes safety on the road by fostering pro-social interaction between drivers may be tested by econometrically evaluating whether social capital indeed results in a significantly greater accident-prevention and life-saving effect in interactive incident situations relative to non-interactive situations. Fatalities in multi-vehicle and single-vehicle crashes — relationship to believing “most people are honest.” Notes: Each dot represents a state. Plots show averaged data over the 1997–2006 sample period for 46 states. Two outliers with unusually high single-vehicle fatality rates were removed. For data sources, see text. Fatalities in multi-vehicle and single-vehicle crashes — relationship to trust. Notes: Each dot represents a state. Plots show averaged data over the 1997–2006 sample period for 46 states. Two outliers with unusually high single-vehicle fatality rates were removed. Source for trust data: the Bowling Alone website of Robert D. Putnam — see text. Fatalities in multi-vehicle and single-vehicle crashes — relationship to voter turnout. Notes: Each dot represents a state. Plots show averaged data over the 1997–2006 sample period for 45 states. Two outliers with unusually high single-vehicle fatality rates, and one outlier with high turnout, were removed. Source for the voter turnout data: United States Elections Project, http://elections.gmu.edu/voter_turnout.htm. To prove causation flowing from social capital to different categories of traffic safety outcomes, I use an identification strategy introduced by Nagler [forthcoming]. The main difficulty with estimating the effect of social capital on highway safety at an aggregate level is that unobserved characteristics of the population may present sources of selection bias. For example, if less conscientious individuals who both eschew civic engagement and drive more recklessly tend to sort disproportionately across states, one might observe higher rates of traffic incidents in states with lower levels of social capital, even in the absence of a causal relationship. I address the identification problem by exploiting variation in winter snow depth across states as an exogenous source of variation to social capital formation. Snow depth offers a relevant instrument because a snowy climate impacts the long-term movement patterns of individuals. These in turn are relevant to the extent to which individuals form strong ties with each other. For this instrument to be valid, it must also be orthogonal to unobservable determinants of highway risks. As snow accumulation likely contributes directly to the incidence of crashes during the winter, I restrict the dependent variable to safety-related incidents occurring during the summer, thereby satisfying the exclusion criteria.Footnote 2 Conceptually, while variation in snow depth plausibly explains variations in social capital from state to state, it does not directly influence the rate of crashes in the summer (non-snow) months of the year. Thus I am able to examine how relative differences in social capital — for which variation comes about by exogenous snow depth variation — influences non-snow-related rates of crashes and fatalities. Whereas Nagler [forthcoming] employs this identification strategy to independent two-stage instrumental variables estimation of different traffic incident types (for example, crashes and fatalities, each estimated independently), the present paper uses it to estimate simultaneous equations systems of complementary traffic incident types (that is, interaction-related vs non-interaction-related) via three-stage least squares (3SLS). I find that instrumented social capital has a larger relative effect, measured in terms of both traffic fatality mitigation and reduction in the number of fatal crashes, in situations involving more than one vehicle and in junction-related situations. The results are robust to the use of different measures of social capital and variations in time- and fixed-effect specifications. The results strongly suggest that social capital positively impacts highway safety through an inter-driver behavioral mechanism, consisting of some form of pro-social behavior or, colloquially, “playing well with others.” Importantly, because this effect occurs on the roads, where pairings of motorists occur to some degree randomly, it provides evidence of a beneficial effect of social capital in a generalized context, that is, outside of a close-knit group setting. To better substantiate this claim, I present the results of a separate set of regressions demonstrating social capital's effects on multi-vehicle and junction-related traffic incidents occurring on principal arterial roads vs non-principal-arterial (that is, minor arterial, connector, and local) roads. I do not find social capital's effect on the incident types in question to be greater in the non-principal-arterial context. This supports the notion that social capital's pro-social behavioral effects are not confined to communities of people that know each other, occurring as they do in the most impersonal and randomly paired driving contexts. The next section discusses the relationship of social capital to highway safety and advances a pro-social behavior hypothesis of social capital's effects. Following this is a section that details my empirical strategy, and then a section that describes the data used in the study. The paper then proceeds to present the empirical results and examine robustness to different measures of social capital. After this, I take up the question of whether the measured effects of social capital are generalized effects as opposed to being specific to close relationships. A final summary section concludes the paper.",7
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2013.2,Relative Wage Changes and Fertility in the US,March 2013,Aliaksandr Amialchuk,,,Male,Unknown,Unknown,Male,"Over the past three decades, fertility has declined sharply in many developed and developing countries.Footnote 1 Most European countries, especially those in the south, have seen their total fertility rates drop well below the replacement rate of 2.1 children per woman [Adsera 2004]. In the US, total fertility declined sharply during the 1960s and the early 1970s, but it has leveled off since the mid-1970s. However, the steady total fertility rate masks large compositional changes: the number of births to unmarried women has increased [Gray et al. 2006]; a higher proportion of births is given to the rapidly growing Hispanic population; and there has been a delay of fertility, manifested in decreasing birthrates among younger women and increasing birthrates among older women [Martin 2000; Mathews and Hamilton 2002; Mullin and Wang 2002]. The decline in fertility and the corresponding aging of the population have become a major policy focus for many countries, where the shrinking workforce may strain pension and social security programs. Fertility policies have been implemented that use the assumed relationship between economic variables and fertility.Footnote 2 In order to forecast fertility and to justify public policies that will modify it, a basic understanding of the determinants of fertility is required. In developed countries in the last three decades, fertility has been widely viewed as a voluntary choice variable, mainly because of the very low cost of limiting births after the introduction of modern birth control technologies. Many factors shaped the recent trends in fertility, among which economic conditions have been viewed as particularly important. The classic models of Becker [1960] and Mincer [1963] first explored the connection between women's wages, household income, and fertility. In these models, children are durable goods in the utility function of parents. While husband's earnings were traditionally associated with higher demand for children, women's wages were associated with both household income and the time cost of children, thereby having offsetting income and substitution effects on the demand for children. Much of the theoretical literature that has tried to reconcile the frequently observed and paradoxical negative association between standards of living and fertility and the assumption that children are normal goods, has associated the value of parental (usually women's) time with the opportunity cost of having children. For example, Mincer [1963] argued that the rise in women's participation in the labor force and the decline in fertility were both caused by factors that increase women's wages relative to other prices in the economy. Willis [1973] argued that increasing a woman's level of education improved her productivity in the market, thereby increasing the relative price of home production and decreasing fertility. Butz and Ward [1979] hypothesized that the negative association between women's wages and fertility increased with the proportion of employed women.Footnote 3 Alternatively, Becker and Lewis [1973] and Willis [1973] assumed that parents view the human capital of their children (child quality) and the number of children (child quantity) as substitutes by incorporating an interaction between child quality and child quantity into the parental budget constraint. Assuming that the income elasticity of child quality is greater than that for quantity helped to explain why parents have fewer children (of higher quality) as income rises, assuming that children are a normal good. While theoretical predictions regarding the signs and the magnitudes of the effects of men's and women's earnings on fertility vary, the important question in the empirical analysis of fertility is how to obtain exogenous variation in earnings in order to identify theoretical income and substitution effects. Estimating the causal effect of men's and women's earningsFootnote 4 on fertility is complicated by the endogeneity of earnings. Third factors that affect earnings potential, such as health or preference for family, could also affect family formation and fertility decisions. At the level of a family, fertility may be closely associated with other lifetime choices of parents, such as the amount of time allocated to work, the investment in the human capital of children, and saving to smooth lifetime earnings. At the aggregate level, earnings can be influenced by labor supply, migration, and selective characteristics related to fertility. Estimation techniques that do not account for these factors lead to biased estimates. Previous studies have used various strategies to address the endogeneity of earnings in the fertility regression.Footnote 5 Some studies have used aggregate variation in earnings to estimate earnings’ effect on aggregate birthrates and instrumented wages and incomes with their lagged values [e.g., Butz and Ward 1979; Hyatt and Milne 1991; Jackson 1995]. However, the use of lagged values of endogenous earnings as instruments is usually problematic [Hotz et al. 1997]. Other studies have used changes in world prices and technology (and their interactions with measures of natural resource endowments or immobile capital) as a source of econometric identification, since they are plausibly correlated with wages and plausibly uncorrelated with other variables, such as preference for children. For example, Schultz [1985] instrumented aggregate earnings with the changes in agricultural prices induced by international trade and changes in production technology in pre-industrial Sweden.Footnote 6 A similar approach was taken to study the effect of income on fertility rates in the Appalachian coal-mining counties [Black et al. 1996; Black et al. 2009] by utilizing the variation in the world prices for energy in conjunction with the county coal endowment. Recent studies have also used dynamic macro-style theoretical models to analyze the nexus between fertility, marriage, and labor force participation (for a review, see Jones et al. 2008). Such models have been used frequently to analyze the impact on fertility of various policy changes, such as parental leave policies [Erosa et al. 2005], child support and welfare reform [Aiyagari et al. 2000; Greenwood et al. 2003], social security [Zhao 2008], and tax reform [Whittington et al. 1990; Zhang et al. 1994; Dickert-Conlin and Chandra 1999]. However, the focus of this body of literature has been the response to changes in the costs of having children, rather than the response to changes in earnings. Yet another body of literature has focused on the effect of aggregate economic uncertainty on fertility using cross-country labor market differences [Adsera 2005], German reunification [Bhaumik and Nugent 2005; Kreyenfeld 2005], and Russian transition [Kohler and Kohler 2002]. This paper uses the changes in the US wage structure that have prevailed since the late 1970s for different ages, education groups, and regions to identify the effect of men's and women's earnings on fertility. Starting in the mid-1970s and accelerating in the 1980s, the US wage structure experienced substantial changes that were due to energy price shocks, increased international competition, and technological changes [Levy and Murnane 1992; Klein et al. 2003; Autor et al. 2003]. It has been argued that part of these wage changes are due to a secular rise in demand for more skilled workers and women [Katz and Murphy 1992; Autor, Katz and Kearney 2005]. The college wage premium increased dramatically, and the wages of less-educated men fell. In addition, each region can be treated as a separate labor market if labor is partially immobile across regions. The US regions are characterized by significant differences in the levels and trends of wages [Karoly and Klerman 1994; Topel 1994]. Several studies have utilized changes in the wage structure to identify income and substitution effects in labor supply models. For example, Pencavel [1998], Devereux [2004], and Blau and Kahn [2005] used between-group variation in wage and employment trends to estimate wives’ own wage elasticity, as well as the cross-elasticity with respect to husbands’ wages. Blundell et al. [1998] used British data and relative wage and tax variations across education and cohort groups to estimate labor supply elasticities. To my knowledge, no other paper has looked at the impact of these changes in relative wages on fertility. On the other hand, the use of aggregate-level shocks to identify the parameters of a dynamic fertility model follows in the tradition of Schultz [1985] and Black et al. [1996; 2009]. The use of this innovative identification scheme allows updated estimates of the effect of men's and women's earnings on fertility to be obtained for a general population while imposing only mild requirements on the data. This paper utilizes the 1981–2009 surveys of March Current Population Survey (CPS) supplements from the Integrated Public Use Microdata Series (IPUMS) to construct age-specific birthrates and the 1979–2007 CPS Outgoing Rotation Groups (ORG) extracts to construct yearly earnings trends by region and education. The use of individual-level series of cross-sectional data allows cross-sectional and time-series variation to be used. It also allows determinants of marital and non-marital fertility to be investigated using actual, rather than imputed, explanatory variables separately for married and single women, which favorably contrasts with other studies of aggregate fertility rates.Footnote 7 Out-of-wedlock fertility has become an important and increasing phenomenon, with births to unmarried women increasing from 10% of total births in 1970 to more than 30% by 2000 [Greenwood and Guner 2005]. In order to identify exogenous variation in earnings, I use group-level regressions of age-specific fertility rates (ASFR). Since the data on fertility rates are aggregated across many dimensions (years, age groups, education categories, regions), the estimation accounts for the sampling measurement error by applying Devereux's [2007] approximately unbiased errors-in-variables estimator (UEVE). The remainder of this paper is organized as follows: the next section describes the data used to construct fertility and earnings series, the subsequent section presents the estimation methodology, the penultimate section describes the results, and the final section puts the results in perspective and offers a conclusion.",1
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2013.7,"Single Mothers’ Time Preference, Smoking, and Enriching Childcare: Evidence from Time Diaries",March 2013,Sabrina Wulff Pabilonia,Younghwan Song,,Female,Unknown,Unknown,Female,"Previous research has shown that time preference affects individuals’ market time allocation and own human capital investments [Mincer 1974; Becker 1975]. Recent research by Song [2011] has shown that time preference, as measured by smoking behavior, may also affect an individual's activities on a daily basis. For example, women who smoke, and thus have a higher rate of time preference, spend more time on activities that provide immediate gratification, such as watching TV, while those who have never smoked spend more time on activities that incur current costs for the sake of future benefits, such as exercising and educational activities. This paper extends this line of research by examining how time preference, as measured by smoking, affects single mothers’ time investments in their children who are under the age of 13. We pay particular attention to distinguishing effects on differential qualities of maternal childcare. We also examine whether children of mothers with a high rate of time preference score lower on several early cognitive tests. This research is important because there is evidence that early childhood cognitive skills predict future cognitive skills, which are rewarded in the labor market [Ensminger and Slusarcick 1992; Katz and Murphy 1992]. Parents make time and monetary investments in their children's health and education [Becker 1981]. Leibowitz [1974] found a positive relationship between parental time with children and child outcomes later in life. Agee and Crocker [2002] showed that a higher discount rate on the investments that parents make in their children leads to lower cognitive skills in their children. Using data from the Tobacco Use supplements to the Current Population Survey (TUS-CPS), the American Time Use Survey (ATUS), and the Panel Study of Income Dynamics (PSID)-Child Development Supplement (CDS), this paper finds that, even after controlling for parental differences in income, employment, and education, single mothers with a higher rate of time preference spend significantly less time with their children overall and slightly less time engaged with their young children in educational activities, such as reading and homework, and much less time sharing meals with their children. Their children also have lower scores on reading comprehension tests.",4
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2011.13,"Local Economic Development in the 21st Century: Quality of Life and Sustainability, by Daphne T. Greenwood and Richard P.F. Holt",March 2013,Natalia V Smirnova,,,Female,Unknown,Unknown,Female,,
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2011.27,"Macroeconomic Theory and Macroeconomic Pedagogy, by Guiseppe Fontana and Mark Setterfield",March 2013,Nevin Cavusoglu,,,Female,Unknown,Unknown,Female,,
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2011.30,"Strategic Competition, Dynamics and the Role of the State: A New Perspective, by Jamee K. Moudud",March 2013,Tazewell V Hurst III,,,Unknown,Unknown,Unknown,Unknown,,
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2011.31,"The Thief of Time: Philosophical Essays on Procrastination, by Chrisoula Andreou and Mark White",March 2013,Jonathan B Wight,,,Male,Unknown,Unknown,Male,,
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2012.25,"Beauty Pays: Why Attractive People are More Successful, by Daniel Hamermesh",March 2013,Jennifer Tennant,,,Female,Unknown,Unknown,Female,,
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2012.26,"Game Theory: A Nontechnical Introduction to the Analysis of Strategy, by Roger A. McCain",March 2013,Myong-Hun Chang,,,Unknown,Unknown,Unknown,Unknown,,
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2012.27,"Economic Aspects of Obesity, by Michael Grossman and Naci Mocan",March 2013,Irina B Grafova,,,Female,Unknown,Unknown,Female,,
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2012.28,"Path Dependency and Macroeconomics, by Philip Arestis and Malcolm Sawyer",March 2013,Matías Vernengo,,,Male,Unknown,Unknown,Male,,
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2012.29,"Money, Investment and Consumption: Keynes's Macroeconomics Rethought, by Omar Hamouda",March 2013,Pavlina R Tcherneva,,,Female,Unknown,Unknown,Female,,
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2013.3,"School Accountability, Autonomy, and Choice Around the World, by Ludger Woessmann, Elke Luedemann, Gariela Schuetz and Martin R. West",March 2013,Lisa M Dickson,,,Female,Unknown,Unknown,Female,,
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2013.4,"Health Care for Us All: Getting More for Our Investment, by Earl L. Grinols and James W. Henderson",March 2013,Frank W Musgrave,,,Male,Unknown,Unknown,Male,,
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2013.5,"The Fall of the House of Credit: What Went Wrong in Banking and What Can be Done to Repair the Damage?, by Alistair Milne",March 2013,William C Perkins,,,Male,Unknown,Unknown,Male,,
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2013.6,"Housing Market Challenges in Europe and the United States, by Philip Arestis, Peter Mooslechner and Karin Wagner",March 2013,Yongsheng Wang,,,Unknown,Unknown,Unknown,Unknown,,
39,2,Eastern Economic Journal,12 March 2013,https://link.springer.com/article/10.1057/eej.2013.8,List of Reviewers 2012,March 2013,,,,Unknown,Unknown,Unknown,Unknown,,
39,3,Eastern Economic Journal,18 June 2012,https://link.springer.com/article/10.1057/eej.2012.8,"Increasing Returns, Institutions, and Capital Flows",June 2013,Thomas Jack Snyder,,,Male,Unknown,Unknown,Male,"Poor, capital-scarce countries should attract more capital flows than rich, capital-abundant countries under the assumption of diminishing marginal productivity of capital. However, the data clearly show that capital predominantly flows to the rich countries. This phenomenon, referred to as the Lucas Paradox [Lucas 1990], has generated many theoretical explanations. Some studies have tried to explain this phenomenon by pointing to differences across countries that could affect the productivity of capital, such as differences in institutions and human capital, while other studies have focused on other factors that may restrict international capital flows, such as asymmetric information, sovereign risk, and transaction costs. While numerous theoretical explanations exist, there are relatively few empirical studies of international capital flows and the macroeconomic fundamentals suggested by the theories. Of particular interest in this study will be the role of initial capital and institutions in attracting capital flows. This study will follow a similar procedure to that of Alfaro et al. [2008], but different variables will be used as a measure of institutional quality. As will be shown, institutions play a key role in attracting capital flows, but this finding does not resolve the Lucas Paradox. The result in this study is in contrast to the study by Alfaro, Kalemli-Ozcan, and Volosovych (referred to as AKV for the rest of the paper), which concluded that initial capital does not affect capital inflows. The data on institutions in the AKV study do not match the same time frame as their data on capital flows, but this study will use institutional data that correspond well to the capital flow data. Initial capital is found to be positively correlated with capital inflows, even when controlling for institutions and other factors that can influence capital inflows. In addition, and in contrast to AKV and other studies, this paper compares a sample of poor countries with a sample of rich countries, and the evidence suggests that the determinants of capital flows for poor countries may not be the same determinants of capital flows for the rich countries. To determine which variables to include as the fundamental determinants of capital flows, this paper follows AKV and the different theories for why the data do not support the standard models of economic growth. In a typical economic growth model, as in Solow [1956], if countries operate with the same technology with constant returns, with the individual marginal products of capital and labor diminishing, the countries with less capital (poor countries) per worker will get a higher marginal return to capital. Following this logic, if individuals seek to get the highest return on their capital, they would invest in the poorest countries. Obviously, the data suggest there is something wrong with these models or assumptions, since capital has predominately flowed to rich, capital-abundant countries. Economic theories suggest a few remedies to the standard model growth model, and the following paragraphs will discuss how those theories will be incorporated in this study. One possible reason why capital has not flowed to the poor countries is differences in human capital. As discussed in the work by Lucas [1990], capital may not flow to poor countries if the labor force, for a given amount of capital per capita, is less productive in the poor countries. In addition, human capital may be subject to externalities, where the productivity of one worker may influence the productivity of another worker. To account for differences in human capital, a measurement of schooling is included as an explanatory variable. Since initial Gross Domestic Product (GDP) is also included as an explanatory variable, which is highly correlated with schooling, one would think that the coefficient on initial GDP would diminish, but as will be shown, the significance of initial GDP's impact on capital flows remains strong. Poor institutions in developing countries may also deter capital flows. According to Douglass North [1994], “Institutions are the humanly devised constraints that structure human interaction. They are made up of formal constraints (for example, rules, laws, and constitutions), informal constraints (for example, norms of behavior, conventions, and self-imposed codes of conduct), and their enforcement characteristics. Together they define the incentive structure of societies and specifically economies.” As detailed later, this paper uses a comprehensive measure of institutional quality to account for different institutions across countries. Institutional quality is also highly correlated with initial GDP, which again one would expect to diminish the significance of initial GDP's impact on capital flows, but as we will see from the regression results, the significance of GDP is robust in the world sample. Since capital inflows may affect institutions, two-stage least squares will be implemented, using log European settler mortality as an instrument for institutions, as popularized by Acemoglu et al. [2000]. Along with facing different institutions, an international investor suffers from sovereign risk and asymmetric information. If investing in a foreign country, a typical investor is not only less familiar with the foreign country's environment than with his own, including its rules and regulations, but he may not have a strong recourse if the sovereign nation defaults or refuses to follow through on a contract. In a textbook model of sovereign debt, as in Obstfeld and Rogoff [1996], a poor sovereign borrower may not be able to obtain enough loans to invest efficiently if it cannot commit to repay the loans. Reputation may also play a key role, and some investors may shy away from countries who have defaulted in the past [Eaton and Gersovitz 1981; Bulow and Rogoff 1989; Reinhart and Rogoff 2004; Tomz 2007]. Theoretically, a poor country may not have an incentive to efficiently invest foreign loans in the domestic economy, but instead secretly buy risk-free securities (i.e., capital flight), default on the loans, and avoid the punishment by feigning bankruptcy, as described in Gertler and Rogoff [1990]. Most of these explanations rely on weak contract enforcement, which is incorporated in the institutional composite variables used in this empirical investigation. However, some of the theories suggest that poor countries have collateral problems, which may help explain why initial GDP is positively correlated with capital flows. Foreign investment restrictions and capital controls may also play a role in restricting capital flows, and this is included in the institutions variable. Geography and infrastructure may also help explain the lack of international capital flows to poor countries. The return on capital may be lower when accounting for the cost of transport (as discussed in Obstfeld and Rogoff [2000], who relate transport and trade costs in goods to the lack of capital flows). Since infrastructure and geographical location may play a key role in capital flows, they will be included as explanatory variables in the regression. However, as with education and institutions, infrastructure is highly correlated with initial GDP, which one would think would diminish the significance of the impact of initial GDP on capital flows, but as we will see, the coefficient is still significant in the whole sample when including all of the above explanatory variables. The above reasons may not sufficiently explain the lack of capital flows to poor countries. Including infrastructure, institutions, and education as explanatory variables may pick up some externalities associated with being developed, but it may not entirely pick up the entire source of the increasing returns. Several studies suggest learning-by-doing and industrialization as a source of increasing returns, as in Romer [1986], Krugman [1987], and Matsuyama [1992]. Controlling for education may not fully pick up human capital differences and advantages of producing in a developed market. However, it may be possible to pick up the effect of industrialization, or moving out of an agricultural economy. To control for industrialization, the percentage of GDP that comes from agriculture is included as an explanatory variable. The surprising result, as we will see, is that initial GDP still has a significant impact on capital flows in the world sample, even when controlling for industrialization and the previously mentioned variables. An alternative explanation for the persistence of increasing returns may exist. Economies of scale may inherently exist in the most productive sectors. Countries with enough capital may invest in sectors with high fixed costs (or low initial returns), which may yield a better return than other sectors that only require a small investment. For example, investing in infrastructure or high-tech manufacturing may only be profitable if a large amount of capital is used, while agriculture or low-tech manufacturing require less initial capital to be profitable. If sovereign risk and asymmetric information constrain capital flows, initial capital may play a key role in attracting foreign investment. A country with a high initial capital endowment may attract more capital flows than a country with a low initial capital, since the poor country would have to borrow too much (to be worth the risk) to invest in the productive sectors. To test for this possibility, the sample is divided into rich and poor countries. Within the poor group, there may be no increasing returns; within the rich group, there may be no increasing returns; but within the whole sample, increasing returns exist as countries move to sectors with high productivity once they acquire high-income levels. The next section shows the estimates from Ordinary Least Squares (OLS). The subsequent section discusses endogeneity, and displays the results from two-stage least squares. The results are summarized in the penultimate section, and the policy implications are discussed. The final section is an appendix that describes the data.",3
39,3,Eastern Economic Journal,04 June 2012,https://link.springer.com/article/10.1057/eej.2012.12,Sentiment toward Trading Partners and International Trade,June 2013,Edward M Feasel,Nobuyuki Kanazawa,,Male,Male,Unknown,Male,"There is an extensive literature examining the determinants of international trade. Major trade theories including the Heckshir–Ohlin model, New Trade Theory, and the Gravity Model identify the primary factors determining trade levels as comparative advantage, economies of scale, and physical distance between trade partners, respectively. More recently, several studies have found that psychological distance between trade partners such as having similar religious or linguistic roots also play an important role in determining the flow of bilateral trade. Building on this latter development, this paper investigates the role that Japanese citizens’ sentiment toward other countries play in determining the level of trade between Japan and its trading partners. Being the first paper to investigate the issue in a time-series framework, the paper allows for the dynamic interaction between sentiment and trade, and investigates whether there may be asymmetries in how each variable responds to changes in the other. By focusing on an attitudinal variable and its relationship with trade, this paper adds to the recent focus on behavioral economics, particularly the branch that allows for the possibility that emotional factors may play a role in explaining economic phenomena. The paper utilizes recent time-series developments in panel VAR (vector autoregression) analysis to investigate the relationship between sentiment and trade. Using a system GMM approach, short- and long-run restrictions are applied in a panel VAR setting for the first time to identify the structural shocks to sentiment, exports, imports, and the exchange rate in the case of Japan. Results suggest that there is strong asymmetric relationship between imports and sentiment. Positive increases to sentiment lead to increased imports from Japan's trading partners, but increases in imports lead to lower sentiment on the part of Japanese toward importing countries. Imports are also the primary determinant in explaining fluctuations in sentiment. Exports and sentiment are not strongly related, except through the effect that exports have on imports. Evidence suggests that in the case of Japan, there is strong reciprocation of increased imports from trading partners when these partners increase their purchase of Japanese exports. This increase in imports, however, leads to a decline in sentiment toward Japan's trading partners. The exchange rate primarily affects imports and exports with a lag. This paper is organized as follows: the next section of the paper discusses the recent findings in the literature on the relationship between psychological closeness and trade and explores how sentiment may be related to international economic variables of interest. The subsequent section discusses the methodology used and presents results for panel unit root and granger causality tests, which provide evidence for the short- and long-run restrictions employed. The penultimate section gives the results for the four-variable panel VAR and the final section concludes.",1
39,3,Eastern Economic Journal,16 July 2012,https://link.springer.com/article/10.1057/eej.2012.14,National School Lunch Program Participation and Child Body Weight,June 2013,Donka M Mirtcheva,Lisa M Powell,,Female,Female,Unknown,Female,"Obesity among American children and adolescents, defined as age- and gender-specific Body Mass Index (BMI) in the 95th percentile or higher, has more than tripled in the past 40 years. Between 1999–2000 and 2007–2008, the prevalence of obesity among school-aged children rose from 15.3 to 19.6 percent for ages 6–11 and from 15.5 to 18.1 percent for ages 12–19 [Ogden et al. 2002, 2010]. Among 6–19 year olds, 20.1 percent of boys and 17.3 percent of girls were classified as obese [Ogden et al. 2010]. Obese children are more likely to become obese adults [Serdula et al. 1993], who are at increased risk of a myriad of adverse health conditions, such as cardiovascular disease, certain types of cancer, diabetes, hypertension, osteoarthritis, sleep apnea, depression, and overall lower life expectancy [National Institute of Health 1998; Must and Strauss 1999], some of which appear as early as childhood and adolescence [Hannon et al. 2005; Freedman et al. 2007]. If left unchecked, the rise in childhood obesity could cause, for the first time in two centuries, children to have a shorter life expectancy than their parents [Olshansky et al. 2005]. The school cafeteria provides a natural environment where children learn eating patterns, since students eat between one and three meals a day in school, for approximately 180 days of the year [Resnicow 1993]. The US Department of Agriculture (USDA) National School Lunch Program (NSLP), established in 1946 and administered by the USDA Food and Nutrition Service (FNS), is the second largest food assistance program in the US and largest school meal program, offered in nearly all public and many private schools. In 2011, the NSLP served on average 31.8 million children each school day and cost the federal government over $10.1 billion in cash payments, in addition to expenses on entitlement goods [USDA 2012a, 2012b]. The NSLP provides school lunches for free or at a reduced price (no more than 40 cents) to children from families with income at most 130 or 185 percent of the federal poverty level, respectively, and at full price (around $2.50, still slightly subsidized) to children with family incomes above 185 percent of the poverty line [USDA 2011]. Policy-makers and educators have posited that the school lunch contributes to the higher obesity rates of school children [e.g., Haskins et al. 2006; Physicians Committee for Responsible Medicine 2007]. However, based simply on positive correlations it is unclear whether the school lunch per se increases body weight and the risk of being obese, or whether such a correlation is spurious due to unobserved factors related to both NSLP participation and higher body weight. Utilizing 1997 and 2003 data from the Child Development Supplements (CDS) to the Panel Study of Income Dynamics (PSID), this paper builds on the previous literature to examine by gender the relationship between NSLP participation and body weight of public school children. This paper contributes to the existing literature in several ways. First, whereas previous literature has examined primarily elementary and middle school students, we extend the age range of the study by including children in grades 1–12 (ages 6–18). Second, the study utilizes quantile regression (QR) methods to better understand the school lunch–body weight relationship since that relationship may depend on the child's position in the BMI percentile distribution. Finally, we also estimate individual-level fixed effects (FE) models to account for unobserved time invariant heterogeneity. Namely, since NSLP participation is a choice variable for families and participation may be correlated with unobserved factors, the FE analysis helps to account for selection of participation into the NSLP program. Further, in addition to child and family demographics and socio-economic status (SES), this study also controls for the availability of food stores, restaurants, local area food prices, and neighborhood poverty rate, as those contextual characteristics may be important factors in determining obesity and the opportunity cost of eating school lunch.Footnote 1",13
39,3,Eastern Economic Journal,02 July 2012,https://link.springer.com/article/10.1057/eej.2012.15,A Competitive Model of (Super)Stars,June 2013,Timothy Perri,,,Male,Unknown,Unknown,Male,"In his 1981 paper, Sherwin Rosen was the first to formally analyze the phenomenon of what he called superstars. Rosen assumed more talented individuals produce higher quality products. Assuming, for simplicity of discussion, individual talent and product quality are identical, superstar effects imply earnings are convex in quality, the highest quality producers earn a disproportionately large share of market earnings, and the possibility of only a few sellers in the market. Rosen argued superstar effects were the result of two phenomena: (1) imperfect substitution among products, with demand for higher quality increasing more than proportionally so small differences in talent may result in large earnings differences, and (2) technology such that one or a few sellers could profitably satisfy market demand, with higher quality producers having lower marginal cost of output. In the extreme case, we have a joint good, where an additional buyer can be serviced at little additional cost to the seller. Borghans and Groot [1998] refer to a market with such cost conditions as one with “media stars.” Rosen [1983] argued such markets almost always require mass media, and, depending on the distribution of consumer preferences, may contain only a few sellers. Television shows and recorded music are examples of media markets. However, imperfect substitution and joint consumption do not characterize all markets in which superstar effects appear. For example, Krueger [2005] identifies significant superstar effects for music concerts in the US — effects that have become even larger in recent years. He reports revenue for music concerts from 1982 to 2003. Most of the artists would fall under the heading of rock music, but other artists are included.Footnote 1 In 1982, the top 5 percent (in terms of revenue) of artists earned 62 percent of concert revenue. For 2003, the corresponding figure was 84 percent. Note, larger superstar effects for music concerts do not necessarily imply either less substitutability among products or technological changes favoring mass media. Krueger suggests these effects result from changes in pricing due to concerts and recorded music becoming weaker complements. Further, Krueger argues the time and effort for a live performance of a song should not have changed much over time. It is also unlikely the cost of performing a song depends significantly on the quality of the musicians. The technology of reaching more buyers for a live performance is much different than it is for selling additional CDs. As Rosen noted: “It is preferable to hear concerts in a hall of moderate size rather than in Yankee Stadium.”Footnote 2 Quality of live performances is significantly diluted by audience size [Rosen, 1983], and cost thus increases in market size.Footnote 3 The purpose of this paper is to demonstrate how superstar effects may occur in markets absent imperfect substitution and joint consumption. There are several differences between our model and the typical superstar model. First, imperfect substitution is not required. Superstar effects result because a few sellers have quality significantly higher than other sellers. One apparent advantage of Rosen's [1981] model is superstars may occur even when quality differences between superstars and others are small. This is because the joint good nature of production in his model allows one seller to satisfy additional buyers at little additional cost. Thus, even if a seller's quality is viewed as only slightly higher than that of another, everyone is still able to buy from the higher quality seller. However, Adler [2006] notes Rosen's model may not result in relatively high profit for supposed superstars unless there are significant quality differences between sellers. If several sellers of similar quality exist, and marginal cost declines with firm output, Adler argues firms will compete and drive price towards average cost. If the high quality seller's product is valued only slightly more than that of the low quality seller, even if marginal and average cost are negatively related to quality, a small quality difference implies the higher quality seller's price will be only slightly above average cost (since price is competed down to average cost of the lower quality seller, which is only slightly greater than average cost of the higher quality seller). Thus, one “superstar” may survive and sell most, if not all of market output, but it will not earn significant economic profit. Second, marginal cost need not decline in output and quality. We assume marginal cost increases in output, and, although superstar effects are more pronounced if marginal cost is inversely related to quality, such effects may occur even if marginal cost increases in quality.Footnote 4 Third, competition occurs in the model because there are many potential and active firms, most of which have the lowest quality level, and none of which sells a significant share of market output. In Rosen's superstar model, price depends on a seller's output. The threat of entry and the assumption sellers of similar quality are good substitutes force firms to behave competitively. However, as discussed above, with declining marginal and average cost, such competition would eliminate one of the superstar effects, the high level of profit for such sellers. Also, if stars are very scarce, potential entrants are likely to be of the lowest quality. Thus, in the Rosen model, small quality differences may imply no large earnings for “superstars,” but large quality differences suggest a lack of competition. Our model has price-taking producers, and, because marginal cost increases with output, those with a large quality advantage over other firms will produce only a small percentage of market output, another feature of a competitive market. We assume there are many potential sellers of the lowest quality called non-stars. Since some firms could have quality only slightly greater than that of the lowest level of quality, it seems a bit extreme to refer to such firms as superstars. Thus, herein all firms with quality above the lowest level will be referred to as stars. Stars can not be created, unlike non-stars who exist in relatively large numbers. For example, it is easy to put together a musical group that is comparable to many other groups, but the determination of what groups are high quality is at the whim of consumers. Since the concept of superstar effects is well established, superstar will still be used to denote the phenomena of revenue and profit increasing and convex in quality, and a few sellers earning a large percentage of market revenue and profit. Rosen [1981] used profit when considering superstar effects, but we use both revenue and profit. Profit is not used exclusively for the following reasons. In our model, low quality producers earn zero profit. Thus, stars always earn all market profit. Also, in our model, as in the special case in Rosen [1981, pp. 851–52] closest to our model, revenue and profit are identically affected by quality. Further, earnings reported for top performers in entertainment are not net of cost. The data on concert earnings from Krueger [2005] considered below involve revenue. The assumption herein is quality levels are perfect substitutes.Footnote 5 With free entry, a large number of potential producers with low quality, and full arbitrage between quality levels, a competitive market results without Rosen's assumptions of potential entry by (super)stars and sellers having similar quality levels. Becker and Murphy [2000] note competition and free entry yield a price equal to the marginal and average cost of new units, implying superstar effects can not result in such a world. However, higher quality sellers can sell at higher prices and earn positive economic profit if free entry is at the lowest quality level. Besides the case of music concerts, discussed above, other examples of growing superstar effects exist. Consider the market for best-selling books [Sorensen, 2006]. From the mid-1980s to the mid-1990s, the share of books sold in the US by the top 30 authors nearly doubled. By 1994, 70 percent of all fiction sales were accounted for by four authors: Clancy, Crichton, Grisham, and King. Another example is in the market for dentists in the US. Frank and Cook [1995] find the number of US dentists who make more than $120,000 per year (in 1989 dollars) increased by 78 percent from 1979 to 1989, while the number of dental specialists (surgeons, orthodontists, etc.) produced each year was basically unchanged, the total number of dentists declined slightly, and average real dental earnings increased only slightly. Real earnings of the highest paid dentists tripled over this period, and dentistry is clearly not a media market. Frank and Cook cite one possible explanation (offered by the editor of the Journal of Dental Education), which is the increased demand for cosmetic dentistry, a high value service. Such a change implies an increased gap between the value consumers place on high and low quality dental services, and growing superstar effects even if the dentistry market is competitive. One question we do not address is why some are viewed as higher quality than others. Becker and Murphy [2000] offer one reason for the existence of stars in what they call social markets. They argue some (followers) gain acceptance and prestige by emulating the consumption of others (leaders). Whatever the reason for the existence of stars, technological advances in recent years may have caused the perceived quality of stars in some sectors to increase. Products such as Walkman, Discman, and iPod enable consumers to listen to music virtually anywhere. If the music market is indeed “social,” the ability of followers to emulate leaders would have increased, implying an increase in consumers’ valuation of higher quality products. We simply equate higher quality with a higher willingness to pay for the product by consumers. The outline of the rest of the paper is as follows. In the next section, the competitive superstar model is developed. Numerical examples of superstar effects are considered in the subsequent section. The section after that contains a discussion of recent changes in ticket prices for music concerts. Concluding remarks are presented in the last section.",3
39,3,Eastern Economic Journal,09 July 2012,https://link.springer.com/article/10.1057/eej.2012.16,"The Role of Industry and Occupation in Recent US Unemployment Differentials by Gender, Race, and Ethnicity",June 2013,Marios Michaelides,Peter R Mueser,,Male,Male,Unknown,Male,"Over the past 50 years, there have been significant shifts in the socio-economic structure of the US workforce. Some of the most important changes have been the increase in the labor force shares of women, non-Whites, and Hispanics. In the 1950s, women accounted for only about a third of the US labor force, whereas, by the early 2000s nearly half the US labor force participants were women. The labor force share of racial and ethnic minorities has more than doubled over the past three decades and, at this pace, non-White and Hispanic workers will soon account for a third of the US workforce. At the same time, the US economy has transitioned from a system in which a large share of employment was in manufacturing and blue-collar jobs to one in which services and white-collar jobs play a dominant role. Shifts in the socio-economic, industrial, and occupational composition of the labor force have been associated with changes in unemployment differentials by gender, race, and ethnicity. Up through the 1970s, women had higher unemployment rates than men, but, by the 1980s, the rates had converged. The unemployment rates of non-Whites and Hispanics over the past few decades have been appreciably higher than the rates for the remaining populations. These groups have also faced higher unemployment durations and greater sensitivity to variation in the business cycle. In the past 20 years, however, there is evidence of convergence in the unemployment experience by race and Hispanic ethnicity. Another important change is the decline in unemployment rates for manufacturing and blue-collar workers over the past 30 years. Nevertheless, manufacturing and blue-collar jobs still have higher unemployment and business-cycle volatility than services and white-collar jobs, respectively. There is an extensive literature examining unemployment patterns for key socio-economic groups and how these are related to overall unemployment patterns. There is also substantial research examining shifts in the industrial and occupational structure of the US economy and the extent to which these shifts affect overall employment and unemployment patterns. Perhaps surprisingly, there is limited work documenting the unemployment rate patterns for women, non-Whites, and Hispanics — three workforce groups that experienced dramatic growth over time — and how these compare with the rest of the population. There is also very limited work considering whether the changing unemployment experiences of these groups in recent years are associated with the shifts in the industrial and occupational distribution of employment. Such research is essential in gaining a better understanding of overall unemployment patterns of key demographic groups and the importance of underlying economic development in explaining changes in those patterns. This paper fills these research gaps by documenting historical unemployment trends by gender, race, and Hispanic ethnicity, and providing a decomposition of unemployment differentials in terms of industry and occupation. Following our discussion of the literature, we provide a historical overview of the major changes in the socio-economic, industrial, and occupational composition of the US labor force based on an analysis of the Current Population Survey (CPS) monthly data from 1948 through 2007. This overview highlights the growing importance of female, non-White, and Hispanic workers in the US labor force, as well as the shift of the US workforce toward services and white-collar jobs. Using the same data, we present historical comparisons in the unemployment rates by gender, race, ethnicity, industry, and occupation. We then turn to CPS March Supplement data from 1988 through 2007 to document more recent trends in the unemployment rate and in Unemployment Insurance (UI) receipt and to examine the degree to which these trends are explained by underlying gender, race, and ethnicity differences in the industrial and occupational workforce distribution. These analyses reveal the important role of industry and occupation in explaining overall unemployment patterns by gender, race, and ethnicity in the modern US economy. Results differ in important ways for the three groups. An apparent full convergence in unemployment by gender is largely a result of industry and occupational differences, which camouflage substantially higher unemployment for women within industry-occupation categories. In contrast, although the race gap has declined, unemployment for non-Whites remains much higher than for Whites, even when we adjust for industry and occupation. Finally, relative Hispanic unemployment has declined, and industry and occupational differences explain much of the remaining gap.",4
39,3,Eastern Economic Journal,09 July 2012,https://link.springer.com/article/10.1057/eej.2012.18,"Civil War, Ethnicity, and the Migration of Skilled Labor",June 2013,James T Bang,Aniruddha Mitra,,Male,Unknown,Unknown,Male,"Over the last decade and a half a vast literature on the causes and consequences of brain drain or the migration of tertiary skilled labor has emerged.Footnote 1 Yet, the role of conflict as a determinant of skilled migration remains relatively unexplored. This paper investigates the impact of internal conflict or civil war in the countries of origin on the magnitude of brain drain to the OECD over the period 1975–2000. Controlling for economic and institutional characteristics of the source countries, we find that the presence of civil war increases the emigration of high skilled labor on the average. Further, the consequences of civil war on the migration of tertiary skilled labor depend critically on the type of conflict occurring in the country of origin. While ethnic civil war increases the fraction of tertiary skilled emigrants in the total emigrant pool, non-ethnic civil war does not have a robustly significant impact on the magnitude of brain drain, and its impact is significantly less than the effect of ethnic war. Further, conditional on the presence of ethnic civil war, an additional year of conflict has a significant positive impact on brain drain. However, the marginal impact of an additional year of non-ethnic civil war on brain drain is statistically insignificant. Interestingly, the salience reverses when we consider the intensity of violence: Conditional on the presence of conflict, a unit increase in the intensity of ethnic conflict has a statistically insignificant impact on brain drain, while a unit increase in the intensity of non-ethnic conflict has a significant but small negative impact. In providing a more nuanced analysis of the role of internal conflict on skilled migration, our study contributes to the literature that investigates the causes and consequences of brain drain. Further, it contributes to a recent literature that looks at the formation of diasporas and their impact on economic outcomes in the countries of origin.Footnote 2 Recent evidence has revealed that skilled diasporas may facilitate the flow of foreign direct investment to the source countries [Kugler and Rapoport 2007]; help in the transfer of technology [Lodigiani 2008]; and contribute towards the adoption of needed institutional reforms [Li and McHale 2006]. Significantly for our purpose, they may contribute significantly to the post-conflict reconstruction of a society [Fair 2007; Koser 2007]. Since the nature of diasporic intervention depends critically on the historical experiences that constitute them [Fair 2007; Natali 2007], our exploration of the role of conflict in forming immigrant communities contributes towards a better understanding of the impact of diasporas. Lastly, our study contributes to an emerging interdisciplinary literature that urges the recognition of ethnic and non-ethnic conflict as conceptually distinct phenomena.Footnote 3 There is considerable evidence that ethnic civil wars tend to be of greater duration [Kirschner 2009]Footnote 4; exhibit a greater probability of recurrence [Kreutz 2010]Footnote 5; lead to a greater intensity of violence; and bear a greater risk of escalation [Eck 2009] than non-ethnic civil wars.Footnote 6 Evidence also shows that ethnic and non-ethnic conflict may arise from fundamentally different motivations, in that the former are predominantly fueled by political grievance [Gurr 2000; Sambanis 2001; Reynal-Querol 2002], while the latter arise either due to the lack of economic opportunities [Sambanis 2001] or due to the desire for loot [Reynal-Querol 2002]. Our study contributes to this literature by documenting a differential impact of the two forms of conflict on economic outcomes; in our case, the migration of skilled labor. The paper is organized as follows: the next section presents the conceptual foundations of our analysis; the subsequent section introduces the empirical model and data; the penultimate section presents the results of our inquiry; and the last section concludes the paper by providing a brief summary of our analysis and indicating directions for further research.",8
39,3,Eastern Economic Journal,09 July 2012,https://link.springer.com/article/10.1057/eej.2012.19,International Competition and Small-Firm Exit in US Manufacturing,June 2013,Robert M Feinberg,,,Male,Unknown,Unknown,Male,"The important role of new entry in promoting employment and growth in the US economy and in providing competitive discipline to established firms has been well-studied. Less attention has been given to the staying power of new entrepreneurs, explaining the patterns of exit by small firms. Certainly small business is of tremendous importance to the US economy, with just over half of all private-sector workers employed in firms of less than 500 employeesFootnote 1; in the manufacturing sector (as of 2007), almost 99 percent of firms are categorized as small businesses, with 44.4 percent of manufacturing employees in these firms. The previous empirical literature on determinants of exit or firm survival has mostly involved cross-industry studies (with little attention given to time series variation in entry) or hazard rates of individual firms. Few researchers have considered differential determinants of rates of exit of different size categories of small firms or the increasingly important role of foreign competition on small firm survival. This study analyzes both the time-varying and cross-sectional determinants of small firm exit rates in US manufacturing over the 1989–2004 period, especially the reaction of domestic firms to the nature of foreign competition as measured by industry-specific real exchange rate movements (interacted with import penetration by industry). Exit rates for several size categories of small firms will be explained, and explanatory variables will include lagged industry data and macroeconomic variables. Annual data from 1989 to 1998 for 140 3-digit SIC manufacturing industries and from 1999 to 2004 for 86 4-digit NAICS industries were obtained from the Statistics of US Business (SUSB), available from the US Small Business Administration (in collaboration with the US Census Bureau). The study explains small firm exit rates in several employment size categories — under 10 employees, 10–19 employees, 20–99 employees, and 100–499 employees — using industry data and international and macroeconomic determinants as explanatory variables, with data sources including the Census of Manufactures, Annual Survey of Manufactures, Bureau of Labor Statistics, and National Science Foundation. Employment cost and demand proxies will be included as will variables measuring capital intensity and R&D activity.",
39,3,Eastern Economic Journal,04 June 2013,https://link.springer.com/article/10.1057/eej.2011.28,"Capital as Power: A Study of Order and Creorder, by Jonathan Nitzan and Shimshon Bichler",June 2013,Alan Freeman,,,Male,Unknown,Unknown,Male,,
39,3,Eastern Economic Journal,04 June 2013,https://link.springer.com/article/10.1057/eej.2011.29,"Money and Macrodynamics: Alfred Eichner and Post-Keynesian Economics, by Marc Lavoie, Louis-Philippe Rochon and Mario Seccareccia",June 2013,Michele I Naples,,,Female,Unknown,Unknown,Female,,
39,3,Eastern Economic Journal,04 June 2013,https://link.springer.com/article/10.1057/eej.2013.18,"Guaranteed to Fail: Fannie Mae, Freddie Mac and the Debacle of Mortgage Finance, by Viral V. Acharya, Matthew Richardson, Stijn van Nieuwerburgh, and Lawrence J. White",June 2013,Cynthia Bansak,Peter Carpenter,,Female,Male,Unknown,Mix,,
39,3,Eastern Economic Journal,04 June 2013,https://link.springer.com/article/10.1057/eej.2013.27,"Erratum: Single Mothers’ Time Preference, Smoking, and Enriching Childcare: Evidence from Time Diaries",June 2013,Sabrina Wulff Pabilonia,Younghwan Song,,Female,Unknown,Unknown,Female,,
39,4,Eastern Economic Journal,10 September 2013,https://link.springer.com/article/10.1057/eej.2013.35,The Superficial Morality of Color Blindness: Why “Equal Opportunity” May Not Be Enough?,September 2013,Glenn C Loury,,,Male,Unknown,Unknown,Male,"A history of racial/ethnic subordination resulting in significant economic inequality between identifiable social groups has left many socieites facing what I will call a “transition problem.” (Think of racial inequality in South Africa after the fall of Apartheid, or in the United States after the civil rights revolution; or, of caste disparities in India after Independence.) In the aftermath of reforms that sought to end overtly discriminatory practices against members of a disadvantaged population subgroup, these and other societies have had to figure out what kind of “transition policies” constitute a fair way to deal with the legacy of those immoral historical practices. In my view, critical assessment of the future of racial affirmative action in the United States should acknowledge this problem and address it explicitly. That is my goal in this essay. I argue in what follows that “color blindness” – by which I mean official indifference to racial/ethnic identity in the formulation of policy – is NOT an adequate basis for reckoning with the transition problem, and that some kinds of affirmative actions are ethically justified, and may be ethically required. My argument is based on the fact that – because of continued social segregation and the importance of human capital spillovers within social networks – the consequences of historical discrimination can persist into the indefinite future, absent some racially egalitarian intervention. In this discussion I will take a broad view of “racial affirmative action,” understanding it to encompass any policy whose explicit objective is to create more equal social outcomes between racial groups. Of course, doubts about the propriety of racial affirmative action are not new. Such reservations have been voiced since the 1960s. Indeed, I can recall addressing this very topic as a young economist, 32 years ago (!), at the 1980 American Economics Association (AEA) Meetings (Loury, 1981). There, though I did not then use the language of “transition problems,” I nevertheless raised what I believe remains a critical question: Can something approximating equality between racial groups be expected from purely color-blind (i.e., non-racially discriminatory) policies and practices, given a history of race-based exclusion and social hierarchy? My central point, then and now, is that in general one must answer that question in the negative. There are limits to the ability of non-discrimination policy by itself to bring about genuinely equal opportuity between social groups. My book, The Anatomy of Racial Inequality (Loury, 2002), develops this point by emphasizing the distinction between what I called there discrimination in contract and discrimination in contact: Equal Opportunity (EO) policies aim to prohibit racial “discrimination in contract” – that is, in the formal sectors of employment, credit, and housing markets, in government contracting, college admissions and the like. However, given the autonomy in the choice of social affiliations that individuals expect to enjoy in a free society, racial “ discrimination in contact” – in the formation of friendship networks, households, business partnerships, and professional ties, for instance – is not and cannot be reached by EO policies. And yet, because human development always and everywhere takes place within some social context, when network-mediated human capital spillovers are important “discrimination in contact” can cause racial disparities to persist into the indefinite future. This observation, it seemed to me in 1980 and still does now, has enormous implications for conceiving of what genuine “equality of opportunity” between racial/ethnic groups in a stratified society should entail. My principle claim is that if EO policies cannot be relied upon to eventually undo the inegalitarian consequences of a blatantly unjust history then, at least in principle, some kind of “affirmative actions” to promote that end are morally justified. I will explore a simple economic model to illustrate how racial segregation in social affiliation (i.e., ongoing discrimination in contact) bears on the legitimacy of racial affirmative action. In the aftermath of the civil rights movement, in mid-20th century, the United States faced a classic “transition problem.” The US Supreme Court’s (1954) decision in Brown v. Board of Education struck down de jure racial segregation of public schools on the grounds that “separate educational facilities are inherently unequal.” However, de facto segregation in the schools has persisted to this day. Moreover, as the data to be reviewed briefly below suggest, the “dream” – that the demise of legally enforced segregation and discrimination against Blacks, coupled with an apparent reduction in racial prejudice among Whites, would cause the significant social and economic disadvantage of African–Americans to fade – has not been fulfilled. As I have previously emphasized (see, e.g., Loury 1995, 1998), one of the principle barriers to achieving greater racial equality in the post-civil rights era in the United States is racial assortation in social networks. The real economic opportunities of any individual depend not only on his own income, but also on the incomes of those with whom he is socially affiliated. Such patterns of affiliation, in a society like the United States, are not arbitrary but depend in part on race and ethnic identity. Given a history of open, widespread, and severe racial discrimination, group differences in economic success may persist across generations without ongoing discrimination against the less affluent group because racial segregation of friendship networks, mentoring relationships, neighborhoods, workplaces, and schools leaves the less affluent group at a disadvantage in acquiring the things – contacts, information, cognitive skills, behavioral attributes – that contribute to economic success. Two philosophical approaches to assessing the legitmacy of racial affirmative action may be contrasted: (i) A procedural/rule-oriented approach: “Let us establish equal treatment without regard to race, allowing the chips to fall as they may.” This approach implicitly assumes either that convergence to something approximating equality will obtain in the absence of overt economic discrimination, or that persisting group disparity that originates in historical injustice is morally irrelvant. I maintain that neither assumption is tenable. Alternatively, there is: (ii) A substantive/group-redistributive approach: “Let’s redress racial inequality via direct group-egalitarian interventions like affirmative action policies, recognizing that this may be the only way to overcome historical inertia so as to achieve genuine equality over the long term.” I claim that when social segregation is extensive and human capital spillovers are important, only the substantive/group redistributive approach is morally adequate.",1
39,4,Eastern Economic Journal,10 September 2013,https://link.springer.com/article/10.1057/eej.2013.28,The Fed’s Research Mission and Measure of Success,September 2013,David Colander,,,Male,Unknown,Unknown,Male,"In the past, central bank research economists saw themselves as quite different from academic research economists. To become a Fed economist took time. The Fed took students trained in academia, and gave them an education in real-world macroeconomics at the Fed. The process took 5–7 years of additional training but after that period the Fed had changed the academic scientists they had recruited from graduate programs into Fed engineers and artists who understood the real-world economy and were able to relate academic research to real-world problems. As they interacted with academic economists, they enriched academic research, infusing it with real-world insights. Fed research economists carefully followed academic work, but did not see their primary research role as contributing to it. They did, of course, sometimes contribute to it, but that was done on the side for enjoyment and to keep up with the academic field. Their bread and butter research — what their promotion depended on — was hands-on applied policy research. In the past, it was possible to distinguish a Fed research economist from an academic research economist; a Fed research economist had more real-world smarts, a knowledge of institutions, and what was called a Fed sensibility. There were problems with that insulated Fed sensibility, but it was a sensibility focused on the real world. Twenty years ago, former Fed governor, Dewey Daane, and I lamented the fact that academic macroeconomists were not more like Fed economists in the introduction to the book we edited, The Art of Monetary Policy (Sharpe Publisher, 1994). We argued that teachers of monetary policy should be more like Fed economists and give less focus to teaching students about academic research. Our goal was to develop competition for the Fed sensibility, thereby improving it. We got half of our wish. Academic economists are today more like Fed economists, but the reason is that Fed economists have changed — they have become more academic. This is precisely the opposite of what we wanted. The focus on “working dog” research is decreasing, rather than increasing. The percentage of research staff at the Fed composed of artists with institutional sensibilities and educated common sense seems to be decreasing. While this is especially true at a number of regional Feds, it seems also to be taking place at the Board. There are still a number of artists there, but more and more those artists are being replaced by highly talented academic researchers who are less trained in the sense of nuance and institutional knowledge that characterized the earlier Fed sensibility. Today, a young Fed research economist is likely to be indistinguishable from an academic research macro economist. The reason for the change is a failure of management at the Fed that has allowed academic publication to determine advancement at the Fed. Previously, promotion of an economist at the Fed was based on background papers and briefings for senior Fed officials (working-dog papers) along with some minor focus on publishing in academic journals (show-dog papers). Those internal papers and briefings required deep institutional knowledge and a sense of the markets. That knowledge and sense determined how the academic models would be applied. Teaching new economists that such knowledge was important, and providing them with that knowledge, was what Fed training involved. In the 1980s, that started to change, and over the past 30 years, from what I understand, that training has been reduced, as the requirements for advancement in research departments at the Fed have changed. Today, many Fed research economists are evaluated by the same publishing criteria as academic economists. I know this not only from conversations with people at the Fed, but also from a written account of Loretta Mester, an economist at the Philadelphia Fed. In an article designed to encourage young economists to consider a career at the Fed (CSWEP Newsletter, Spring/Summer, 2006), she provides a good sense of the change in incentives that have occurred. First, she notes “Research time at the Philadelphia Fed surpasses that which a new PH.D would get at most academic departments.” She points out the research at the Fed means “the same thing as is meant in academia” — research on self-chosen topics rather than directed research. She then states that research economists at the Fed can expect to spend half to three-quarters of their time working on their own research agendas, not on the Fed’s research agenda. In addition, she points out that the time researchers spend on Fed research is secondary to their own research. Fed research is arranged far in advance so that “it does not interfere with the economist’s independent research.” She states that in the past, “one could succeed in the department by focusing more on policy work or taking on other directed research projects” but that “this is not the case today.” Today, one must do academic research to succeed. She concludes by pointing out that this independent research is strongly supported; Fed researchers are provided with superb support facilities, and money to bring in visiting scholars, and to organize conferences and workshops that relate to their research. The only real problems with a Fed job for an academic economist that she sees are: (1) the lack of tenure pressures and (2) the need to do some policy work.Footnote 1 The later is a problem because “some economists may find the non-research parts of the job policy work — to be onerous.” If her summary is correct, and from informal discussions with Fed economists it is, today academic research is the primary path to promotion for research economists at a number of the regional Feds, and a more important path than it was at the other regional Feds and the Board. Measuring research output by academic article publication metrics is a major reversal of previous situation where the Fed training turned academic economists into Fed economists.Footnote 2 It means that the distinction I made earlier between Fed economists and Fed understanding of the economy and academic economists’ understanding is disappearing.",1
39,4,Eastern Economic Journal,12 November 2012,https://link.springer.com/article/10.1057/eej.2012.32,Understanding Academic Journal Market Failure: The Case of Austrian Economics,September 2013,Scott A Beaulier,J Robert Subrick,,Male,Unknown,Unknown,Male,"Members of unorthodox schools of economic thought claim to suffer from explicit discrimination in the academic journal market. Their ideas, they argue, do not receive fair treatment from editors and referees. Thus, these unorthodox thinkers claim that major journals represent outlets for generally accepted approaches within narrow ranges and do not take chances on ideas far from the general discussion. Such risk aversion prevents novel research from gaining attention, except in non-mainstream journals or books. The institutional discrimination, in turn, affects the likelihood of tenure for members of unorthodox schools of thought because the ability to publish articles in quality journals has a significant impact on tenure decisions. A heterodox economist who only publishes in heterodox journals (because she cannot get articles published in mainstream journals) has lower prospects for academic success. In response, the heterodox economist does not pursue her true interest. Instead, she writes articles acceptable for a more mainstream audience. The process leads to a dwindling of heterodox approaches, or what McCloskey [[1995] 2000] calls a “kelly green golf shoe” production of ideas in economics, meaning that the intellectual products of heterodox economists differ little from mainstream articles. These claims raise questions about the efficiency of the academic journal market. Most importantly, does the academic journal market systematically discriminate against high-quality, unconventional ideas? Klein [2005] and Holcombe [2004] suggest that high quality ideas get underrepresented because of bias and flaws in ranking systems. Also, do correct and insightful ideas generated by heterodox schools of thought eventually replace incorrect mainstream ideas? Rosen [1997] and Yeager [1997] sparked some debate regarding the efficiency of the academic market. Rosen [1987, p. 151] recognized that the academic journal market does have “the equivalent of monopolistic elements, in the form of fashion and peer pressure” that “appear in the intellectual marketplace from time to time.” He proceeded to ask, “Why aren’t monopolies in ideas just as temporary as monopolies in goods?” He responded that academic journal markets do eliminate bad ideas over time and the process of truth-seeking continues. Some form of evolutionary process akin to Alchian's [1950] theory of competition exists in the academic market and, as a result, poor mainstream arguments disappear. Yeager [1997] responded by arguing that academic truth is not the same as a popularity contest. Editors and referees lack adequate incentive to weed out poor mainstream ideas and accept ideas from unconventional sources. In some cases, doing so would undermine the editor or referee's own research. The self-interest of editors and referees precludes an honest assessment of heterodox ideas. The acceptance of heterodox ideas occurs only when editors and referees maximize truth uninfluenced by their self-interest. The Rosen-Yeager exchange focused on the rise, decline, and re-emergence of the Austrian School of Economics. When the Austrian revival began in 1974, significant problems had arisen within mainstream economics. Economists had made little progress in the theory of imperfect information. The seminal contributions of Akerlof had appeared but both Spence and Stiglitz's major contributions had only recently been published. Furthermore, their contributions had not become part of an economist's toolkit. The theory of market socialism remained a viable model for many mainstream economists. Central planning, with its emphasis on investment, formed the basis of international efforts to improve the well-being of many people in the developing world. Perfect competition reigned supreme as the ideal for welfare economics for many, especially those economists associated with the University of Chicago.Footnote 1 President Richard Nixon, like Milton Friedman, aptly summarized the mainstream position in macroeconomics when he stated that “we’re all Keynesians now.” The role of institutions as a primary factor in explaining economic outcomes remained outside the domain of economics. As problems with the neoclassical approach became evident, Austrian economists responded with their own solutions. The Austrians offered insights into the theory of imperfect information [Thomsen 1992]. They challenged the viability of market socialism and development planning [Lavoie 1985a, 1985b; Boettke 1994], developed the theory of the market process as an alternative to the perfect competition model [Kirzner 1992], and began to analyze the impact of legal and monetary institutions [Rizzo 1980; White 1984]. Austrian economists provided potential answers to analytical shortcomings of the mainstream models as they existed in the mid-1970s. While Austrian economists were raising important challenges to the neoclassical approach, mainstream economists did not refrain from addressing the shortcomings of their models, and they offered their own answers to these issues. Joseph Stiglitz, with numerous co-authors, challenged the perfect information assumption and provided new insights into the workings of imperfect markets [see Stiglitz 1994, 2002 for overviews]. The theory of market socialism lost its appeal due to the fall of communism in Central and Eastern Europe. The development of public choice models further eroded support for market socialism [Levy 1990; Shleifer and Vishny 1998]. Top-down planning organizations, such as the World Bank, shifted from an emphasis on investment to market-oriented reforms (as characterized by the 1996 World Development Report, “From Planning to Markets” and the 2003 World Development Report, “Building Institutions for Markets”). The Washington Consensus's emphasis on privatization and deregulation explicitly recognized the importance of government failure and the need for market reforms in the developing world [Krueger 1990]. Even the perfect competition model, which had long served as the benchmark for efficiency, diminished in importance as game-theoretic and imperfect competition models incorporated features of reality. Finally, partially due to Douglass North's work [1981, 1990], the role of institutions became a major field of inquiry within economics. While Austrian economists provided potential solutions to the problems of the 1970s, their arguments did not appear in highly ranked mainstream journals.Footnote 2 To this day, Austrian research continues to appear in second- and third-tier journals. In this paper, we offer an explanation as to why Austrian economists have not succeeded in placing their articles in top journals, and we present evidence that Austrians do not engage mainstream economists in ways that lead to extended discussions. Austrian economists have self-selected into groups with interests that differ from mainstream journal editors and referees, so they focus on topics that have little interest for most of the profession. As a result, the content of the articles has caused Austrian-inclined economists to publish in lower-tier journals. Furthermore, their attempts to engage mainstream research often do not involve the formation of clearly stated hypotheses that could, in principle, be empirically assessed either econometrically or by an analytic narrative, which attempts to combine historical narratives and rational choice theory. This raises the costs for mainstream economists to engage in extended discussions with Austrians. In fact, not only does the lack of definite hypotheses raise the cost of intellectual engagement, but it sends a strong signal that attempts at such engagement are likely to be pointless. In order to illustrate our hypotheses, we examine four areas of research in mainstream economics that have emerged as important to the profession, and we look at the Austrian response to each of these advances. First, we examine competing treatments of the theory of economic development in the mainstream and Austrian literature. No longer does the Solow growth model command the respect it once held; nor does the Cass-Koopmans model of economic growth. Instead, development economics focuses primarily on the fundamental determinants of economic performance: institutions, policy, geography, integration into the world trading system, and culture.Footnote 3 Second, we examine the new trade theory and its emphasis on imperfect competition and increasing returns. Ricardian comparative advantage and Hecksher-Ohlin models no longer offer sufficient explanations for the patterns of international trade observed due to the recognition of the importance of trade between similar countries. Models of increasing returns have emerged to provide an explanation for the latest wave of globalization.Footnote 4 Third, we look at the emerging theory of political economy with its emphasis on the interactions between states and markets. Finally, we examine developments in the theory of the firm. The firm as a black box has disappeared with the emergence of a large empirical and theoretical literature examining the workings of firms. In each case, the Austrians have either not responded to these recent developments with additional insights of their own or they have begun to develop their own approach recently. We could have included other research areas such as behavioral economics and experimental economics that have received substantial attention over the past two decades but we did not because Austrians have not engaged these literatures. As a result, they have had little impact on the discussions that appear in the most prestigious journals. Of course, our suggested hypothesis does not explain every instance of Austrian ideas not appearing in mainstream journals. In some cases, Austrian-inclined economists may suffer from discrimination based on their ideological orientation and their avoidance of formal models and econometric techniques. Their adherence to market-based solutions and skepticism towards government solutions stretches credibility in some instances and may preclude discussion. Their claims appear extreme. However, our hypothesis is that their ideas that have avoided extreme statements have not appeared in mainstream journals because of their style of argument rather than perceived biases. In addition, their ideas often take on a normative dimension that signals an unwillingness to critically examine real-world findings that raise questions about their explanations. Some may have been rejected because of ideology and approach but more have been rejected because of their presentation style and lack of scientific substance (as understood by the mainstream).",5
39,4,Eastern Economic Journal,06 May 2013,https://link.springer.com/article/10.1057/eej.2012.24,Determinants of Illegal Mexican Immigration into the US Southern Border States,September 2013,Andreas Buehn,Stefan Eichler,,Male,Male,Unknown,Male,"This paper studies illegal immigration from Mexico to the United States (US) between 1985 and 2004. We contribute to the literature in two ways. First, we explicitly consider illegal immigration as an unobservable phenomenon using a Multiple Indicators Multiple Causes (MIMIC) model. This allows us to measure the level of illegal immigration with more than one indicator variable. While the literature typically uses the number of illegal Mexican immigrants apprehended by the US Border Patrol at or behind the US-Mexico border (linewatch apprehensions), we additionally employ non-linewatch apprehensions as a second indicator of illegal immigration. Although the number of linewatch apprehensions is highly correlated with the actual number of illegal Mexican immigrants entering the US, it does not represent the number of illegal Mexican immigrants successfully entering the US since most people who are apprehended at the border are sent back to Mexico. Using non-linewatch apprehensions as a second indicator of illegal immigration may help to improve estimations of illegal immigration as it captures the number of Mexican immigrants who succeeded in illegally entering the US but were later apprehended somewhere in the interior. Second, we analyze illegal immigration at the state-level and examine the determinants for entering the US through Arizona, California, and Texas. Using these state-specific determinants, we calculate the first state-specific estimates for the inflow of illegal Mexican immigrants to Arizona, California, and Texas for each month between 1985 and 2004.Footnote 1 MIMIC models are commonly applied to measure the size and development of informal economic activities, which are not reported to the authorities and whose exact size can therefore not be measured precisely. The MIMIC methodology explicitly treats the object being studied as an unobservable or latent variable that can presumably be measured using appropriate observable indicator variables. Several informal economic activities have already been studied using the MIMIC approach. For example, Dell’ Anno and Schneider [2003], Schneider [2005; 2007], and Dell’ Anno and Solomon [2008] apply MIMIC models to estimate the determinants and size of the shadow economy.Footnote 2 Farzanegan [2009] and Buehn and Eichler [2009] apply the MIMIC approach to study the determinants and development of illegal trade (smuggling) in Iran and across the US-Mexico border, respectively. In this paper we argue that illegal immigration is an integral part of the informal economy since it involves breaching the law and its size is not recorded by the authorities. For this reason we study the determinants and development of illegal immigration using the MIMIC methodology, which is particularly designed to deal with informal, unobservable economic activities. Relying on previous literature, we derive hypotheses about the determinants of illegal immigration across the US-Mexico border. For each of three of the four US states bordering Mexico — Arizona, California, and Texas — we specify an MIMIC model to test the impact of observable causes/determinants (incentives to immigrate) specific to that state on the latent phenomenon of illegal Mexican immigration — which, in turn, is indicated by linewatch and non-linewatch apprehensions recorded in that state. Using the significant coefficients of the determinants of the MIMIC model specific to each state, we can estimate the monthly inflow of illegal Mexican immigrants to each state from 1985 to 2004. Our results indicate that labor market conditions and the intensity of border enforcement in the US states determine illegal immigration from Mexico to the US. For Arizona, for example, a low rate of unemployment acts as a pull factor for illegal Mexican immigrants. For California and Texas, higher real wages are the most significant labor market determinants of illegal Mexican immigration. Labor market conditions in Mexico also determine illegal immigration into the US: immigrants are pushed to Arizona and California by low Mexican real wages and to Texas by a high Mexican unemployment rate. We find robust evidence that more intense border enforcement in the US significantly deters illegal immigration since a higher probability of being caught at the border significantly increases the costs associated with crossing. Using the MIMIC models, we estimate the inflow of illegal Mexican immigrants to each state per month. In general, the annual inflow of illegal Mexican immigrants is relatively stable over time. It ranged between 12,000 and 18,000 in Arizona, between 80,000 and 110,000 in California, and between 40,000 and 60,000 in Texas per year from 1985 to 2004. Several events led to abnormally large fluctuations in illegal Mexican immigration. The outbreak of the peso crisis in 1994/95, for example — which was associated with a decline in real wages and employment opportunities in Mexico — dramatically increased the number of Mexican immigrants who illegally crossed the border into the US in 1995 to 20,000 in Arizona, 140,000 in California, and 70,000 in Texas. Several US border enforcement operations, such as Operation Hold-the-Line in Texas, Operation Gatekeeper in California, and Operation Safeguard in Arizona, also — albeit temporarily — deterred Mexican immigrants from entering the US illegally. Re-enforcement of the Southern US Border following the terrorist attacks of September 11, 2001 resulted in a steep decline in illegal Mexican immigrants to 3,000 in Arizona, 35,000 in California, and 17,000 in Texas by the end of 2001. Since 2002, the number of illegal Mexican immigrants has recovered to normal levels but is much more volatile than in the period 1985–2000. In addition, our results indicate that the flow of illegal immigration shifted from the high-enforcement California border to the lower enforcement Arizona and Texas borders from 2001 to 2004. The paper is organized as follows. The second section reviews the literature on illegal immigration across the US-Mexico border. The third section discusses the indicators and determinants, that is the costs and benefits of illegal immigration across the US-Mexico border, and derives our hypotheses for the empirical analysis. The fourth section presents the empirical analysis, explains the results, calculates long-term indices of illegal immigration from Mexico to Arizona, California, and Texas, and relates their pattern to macroeconomic events in Mexico and US border enforcement policies. The final section concludes.",2
39,4,Eastern Economic Journal,19 November 2012,https://link.springer.com/article/10.1057/eej.2012.33,Employer-provided Health Insurance and Labor Supply of Married Women,September 2013,Merve Cebi,Chunbei Wang,,Female,Unknown,Unknown,Female,"Employment-based health insurance coverage is the most common form of health insurance in the United States: in 2010, 59 percent of non-elderly individuals and 69 percent of non-elderly workers were covered by employer-provided health insurance [Fronstin 2011]. The issue of health insurance being tied to employment in the US health care system has received much attention from both policymakers and researchers. Accordingly, a substantial body of research has been devoted to examining the relationship between health insurance and various labor market decisions, including labor force participation, hours worked, job mobility, and retirement [see the reviews by Currie and Madrian 1999; Gruber 2000; Gruber and Madrian 2004]. One important group whose labor force outcomes are likely to be affected by the availability of health insurance coverage is married women. Because employers who provide health insurance often provide it to both employees and their families, many married women receive health insurance coverage through their husbands [Madrian 2006]. This availability of alternative health insurance coverage may reduce wives’ demand for insurance in their own name and therefore affect their labor market decisions. In the past decade, a number of studies have examined the effect of husbands’ health insurance coverage and wives’ labor supply [Olson 1998; Buchmueller and Valletta 1999; Wellington and Cobb-Clark 2000; Schone and Vistnes 2001; Olson 2002; Honig and Dushi 2005; Royalty and Abraham 2006; Murasko 2008; Kapinos 2009]. These studies typically use cross-sectional data (with the exception of Murasko [2008]), estimate reduced-form labor supply equations for wives, and consistently find a negative relationship between husbands’ health insurance and wives’ labor supply. A central problem in estimating the relationship between spousal coverage and labor supply is that spousal coverage is not likely to be exogenous. There are two main reasons for this. First, unobserved personal characteristics of husbands and wives that affect labor supply — such as preferences for work — may be correlated because of the marriage selection process [Lundberg 1988]. For example, if women with strong preferences for leisure, child rearing, or home production tend to marry men who work long hours and hence provide health insurance to the family, then ignoring such unobserved heterogeneity would lead the estimates to overstate the negative effect of spousal coverage. Alternatively, if women with high ability or some other unobserved characteristics, which make them more likely to work in jobs that offer health insurance, tend to marry men with similar characteristics, then the estimates would understate the negative effect of spousal coverage. A second possible source of endogeneity is that husbands and wives may make joint job-choice decisions determined by the health insurance options available to the family [Scott et al. 1989; Black 2000]. For example, husbands may sort into larger firms or into industries that are more likely to provide health insurance coverage to their workers because their wives decide to leave the labor force or work shorter hours. In this case, the simultaneity of wives’ labor supply and the health insurance status of their husbands would lead the estimates to overstate the negative effect of spousal coverage. Acknowledging this endogeneity problem, the earliest empirical studies attempt to analyze the sign of the bias. Most of the studies find that failing to account for the endogeneity of spousal coverage leads to an overstatement of its negative effect on married women's labor supply [e.g., Olson 1998; Buchmueller and Valletta 1999], although some find an understatement of the negative effect [e.g., Wellington and Cobb-Clark 2000]. Later work suggests finding appropriate instruments for spousal coverage [Schone and Vistnes 2001; Olson 2002; Honig and Dushi 2005]. However, the close link between health insurance and labor supply makes it quite difficult to identify suitable instruments that are correlated with wives’ coverage through their husbands’ health insurance, but unrelated with wives’ labor supply decisions. This paper uses the National Longitudinal Survey of Youth (NLSY) to provide further evidence on the magnitude of the relationship between spousal coverage and labor supply. We exploit the panel nature of the NLSY and use first-differencing (FD) and fixed-effects (FE) approaches to difference out time-invariant unobserved factors that are correlated with husbands’ health insurance coverage. This arguably allows us to reduce the bias that may characterize estimates of the relationship between husbands’ health insurance and wives’ labor supply based on cross-sectional data. To our knowledge, only Murasko [2008] has previously used panel data to address this issue. Specifically, Murasko [2008] estimates first-differenced models using the 1996–2004 Medical Expenditure Panel Surveys (MEPS), and finds a smaller negative influence of spousal coverage on labor supply than does research using cross-sectional data. Our analysis builds on Murasko's [2008] work and adds to the literature in four ways. First, by using the NLSY, we are able to examine labor supply decisions over a 12-year period that covers the prime working years and years of major life transitions of households in the sample. Compared with a short panel like the MEPS (which follows respondents only for two years), such a long panel should offer greater within-household variation and yield more precise estimates. In particular, more than 48 percent of the women in the NLSY sample we use experience a change in spousal coverage during the years we observe them, whereas 10 percent of women experience such a change in the sample Murasko [2008] examines. Also, the NLSY survey instrument asks more specific labor force questions than the MEPS, which should result in less measurement error of the key labor force status variables. Overall then, a re-examination of this issue using NLSY data promises to add valuable insights. Second, we estimate the same model specification using both a panel data set (the NLSY) and a cross-sectional data set (the Current Population Survey (CPS), which has been used frequently in past studies of this question), allowing for a thorough comparison of panel and cross-sectional estimates. Murasko's [2008] conclusion that panel data estimations show smaller effects than those using cross-sectional data is based on a comparison of his findings to those of earlier research; however, the differences could also be due to use of a particular estimator or different model specifications. Providing a direct comparison of findings from the NLSY sample with the CPS sample allows us to control for these possible differences and reach a firmer conclusion about the importance of panel data in controlling for unobserved heterogeneity. Third, as a specification check, we compare FD models with FE models. A comparison of the efficiency of these two estimators is made possible because of the long panel. Finally, we estimate how married women with different characteristics differ in their responses to spousal coverage, which could provide useful policy implications for health care reform. Our estimates yield three key findings. First, they support Murasko's [2008] conclusion that cross-sectional estimates of the relationship between spousal coverage and labor supply tend to overstate the magnitude of that relationship. That is, netting out time-invariant unobserved heterogeneity using panel methods leads to a weaker (although still statistically significant) estimated relationship between spousal coverage and labor supply. Second, the estimates based on the NLSY sample suggest that married women respond to changes in spousal coverage mainly by shifting between full-time and part-time work and by changing work hours, rather than by moving in or out of the labor force. This contrasts with Murasko's [2008] finding that married women respond to changes in spousal coverage mainly by moving in or out of the labor force. Third, we find heterogeneous effects that are consistent with previous findings in the literature. In particular, the negative effects are stronger among married women who have children under age 6, are less educated, or are Hispanic.",3
39,4,Eastern Economic Journal,16 July 2012,https://link.springer.com/article/10.1057/eej.2012.20,"Invention, Innovation, and Wage Inequality in Developed Countries",September 2013,Kevin J Bowman,Sarinda Taengnoi,,Male,Unknown,Unknown,Male,"The United States (US) has experienced a sharp increase in wage and earnings inequality during recent decades. In large part, the rise has reflected higher returns to skills. The consensus today for an explanation of this trend is skill-biased technological change, which has presented itself significantly in the form of information and communication technologies (ICTs). Yet France and Germany have not experienced rising earnings inequality during the 1980s and early 1990s, as they increased their exposure to these technologies [Aghion et al. 1999]. Earnings inequality, heavily influenced by wage inequality, has been roughly constant in Germany and decreased in France. Nevertheless, DiNardo and Pischke [1997] indicate that computer usage was nearly as high in these two continental European countries toward the end of this period. The percentage of workers using computers was 37.4 percent in the US in 1989, 35.3 percent in Germany in 1991–1992, and 34 percent in France in 1991. Aghion et al. [1999] relay that the US and United Kingdom (UK) are the only advanced countries that experienced a large and simultaneous increase in three measures of inequality during the 1980s that reflect higher returns to skill: (i) the educational wage differential, (ii) the age-related wage differential (proxy for experience), and (iii) within group wage inequality (a proxy for ability while controlling for education and experience). On the basis of surveys over the 1994–1998 period, Blau and Kahn [2005] report that compared with other advanced countries, the US had a greater unexplained return to high skills after controlling for experience, wage setting institutions, and differences in the variance of skills. The non-US comparison group in the Blau and Kahn study comprised Canada, Denmark, Finland, Italy, the Netherlands, Norway, Sweden, and Switzerland. We calculated the average share of ICT in manufacturing production and the average income inequality for this non-US group from data available by country from the OECD [1997] and Deininger and Squire [1997], respectively. The non-US group had a smaller share of ICT in 1985, 3.7 percent, vs the US, 6.7 percent. Yet, the non-US group increased their ICT exposure by 1994 to 4.6 percent while the US share of ICT in manufacturing production remained roughly constant at 6.7 percent in 1985 and 6.8 percent in 1994. But it was the US that experienced an increase in its income inequality Gini coefficient from 35.6 in the early 1980s to 37.9 by the early 1990s. Income inequality started at a lower level, 32.0, in the early 1980s for the non-US group, and decreased to 30.9 by the early 1990s. Data cited above for France and Germany and the non-US group pose a challenge to the existing models of technological change that match the rising wage and earnings inequality data for the US. For example, Krusell et al. [2000] attributed the changes to technological advances that create better quality capital equipment. This equipment complements high-skilled workers, but substitutes for low-skilled workers, thereby raising the relative wage. Although they only examined US data, the model suggests that the increased exposure to ICT (typically considered high-tech capital equipment) in the non-US countries should also increase wage inequality there.Footnote 1 The model of this paper is the first to endogenously link key supply-side aspects of invention, innovation, and adaptation for two developed economies. This allows us to compare wage inequality among advanced countries that may have contributed differently to the invention and innovation of new general-purpose technologies. The model shows that an advanced country that must invent and innovate will have higher wage inequality at its steady state than an advanced country that may specialize in innovation. The innovation specialist will also experience a smaller increase in wage inequality during a period of skill-biased technological change. Invention is modeled as the discovery of new knowledge or technologies that has the potential to increase total factor productivity in the production of goods and services. Innovation is the new application of invention (given to this sector) that actually does raise the total factor productivity of its inputs. Innovation is also modeled as creating additions to adaptive knowledge (or adaptive technologies) available for later use in the less skill-intensive, adaptive sector. The modeling assumption in which innovation helps diffuse inventive ideas to less skill-specific knowledge is supported by studies by Rogers [1995] and David [1990] that show general-purpose technologies are first applied by high-skilled individuals or sectors creating social learning opportunities for subsequent use by less-skilled workers or sectors. Thus, we call this modeling assumption the social learning assumption.Footnote 2 While licensing invention from abroad, the innovation specialist will have (unlike the invention leader) all of its high-skilled workers involved in innovation, which helps diffuse skill-biased invention into knowledge accessible by low-skilled workers. Countries that specialize in innovation while licensing frontier knowledge from abroad still need comparable per capita human capital in order to access frontier knowledge. Their low-skilled workers must also have adequate skills to adapt innovative techniques. Thus, the innovation specialist of the model represents an advanced rather than a developing country. A lower steady-state relative wage of high-skilled workers results for the innovation specialist. Furthermore, comparative statics show that parameter shocks that increase the relative wage have a stronger effect in the invention leader. These results hold with or without certain costless international spillovers to the innovation specialist. By avoiding a reliance on such spillovers, we are consistent with Jorgenson and Stiroh [1999] who find that “the returns to investment in IT equipment have been successfully internalized by computer producers and users … through substitution, not because of spillovers to third parties standing on the sidelines of the computer revolution.” The innovation specialist in the case of no international spillovers must pay foreign high-skilled workers their full opportunity cost to add to the innovation specialist's domestic stock of inventive knowledge and to train domestic high-skilled workers to use this knowledge. Technological change is presented here by phases of invention, innovation, and adaptation. Generally stated, each productivity-enhancing use of IT requires a unique skill-using creation or application rather than using the technology as a complement to a pre-existing activity. Costless international spillovers may counter the spirit of the Solow Productivity Paradox. Solow [1987] stated that “you can see the computer age everywhere but in the productivity statistics.” Therefore, the model's results help to explain the higher returns to skills in the US and its sharper surge in wage inequality compared with other advanced countries if the US is considered relatively more inventive. This is a plausible assertion as many of the computer and information technologies originated in the US. Recall that the OECD data showed the US as having a higher share of ICT in manufacturing production before the gap narrowed with the non-US group. The narrowing of the gap during the 1980s is consistent with the argument that the non-US group engages in relatively more innovative use of ICTs rather than inventing the ICTs. The US is also known to have numerous high-quality research universities. These institutions produce a disproportionate amount of invention (ideas and basic research without immediate commercial application, but with that potential). America's research universities are considered to be the best in the world [see, e.g., Graham and Diamond 1997].Footnote 3 In the latest U.S. News and World Report [2011] ranking of the top 400 universities in the world, the US had the largest share at 21 percent. Twelve of the top 16 were American.Footnote 4 During the period examined and well before that, the US has been large economically relative to the individual, continental European countries (and Canada). The US was also in the position of economic leadership in the decades following World War II. Therefore, the US could not rely as easily on other countries for additions to frontier knowledge. In certain industries, any of these small, advanced countries in the real world might be frontier leaders, but we are analyzing the aggregate here. The difference between the two countries in the model is economic size, which allows the small country to specialize in innovation. All agents in the small country prefer to specialize because steady-state growth in high- and low-skilled wages is faster as an innovation specialist compared with the small country having to generate all its own invention. The formal model is presented first without the possibility of international spillovers. Section “Modeling invention, innovation and adaptation” presents the inventive, innovative, and adaptive sectors. The demand and the supply of high-skilled labor relative to low-skilled labor are then examined in sections “The relative demand for high-skilled labor” and “The relative supply of high-skilled labor,” respectively. Section “Equilibria” derives the general and steady-state equilibria and the steady-state propositions before ending with a comparison of the results to the case with international spillovers. Section “Conclusion” summarizes the paper and proposes extensions.",
39,4,Eastern Economic Journal,12 November 2012,https://link.springer.com/article/10.1057/eej.2012.31,The Evolution of Federal Reserve Transparency Under Greenspan and Bernanke,September 2013,Roger W Spencer,John H Huston,Erika G Hsie,Male,Male,Female,Mix,,
39,4,Eastern Economic Journal,28 January 2013,https://link.springer.com/article/10.1057/eej.2012.21,Could Different Retirement Benefits Result in More Effective Teachers?,September 2013,Christian E Weller,,,Male,Unknown,Unknown,Male,"Attracting more effective teachers is a continuing policy puzzle. Education experts have recently suggested that states should replace their defined benefit (DB) pensions with defined contribution (DC) cash balance (CB) plans — a hybrid between DB pensions and DC plans — in part to affect teacher effectiveness [Costrell and Podgursky 2009; Barro and Buck 2010; Hansen 2010; McGurn 2010]. And, state lawmakers have also considered changing their retirement benefits from DB pensions to DC or CB plans in the midst of the fiscal crisis after 2007. Teachers typically receive DB pensions, whereby a retiree receives monthly benefits for the rest of her life. DB pensions defer part of an employee's compensation into the future. Employees earn more benefits relative to their salary later in their careers than earlier. Deferring compensation creates incentives for employees to stay with one employer since there are rewards for staying with one employer for extended periods. DC and CB plans typically do not defer compensation, which could affect teacher effectiveness. Employees receive more compensation earlier in their careers with alternative benefits than with DB pensions. This may attract more highly qualified applicants into the teaching profession possibly raising average effectiveness. But alternative benefits could also increase turnover among more experienced teachers since there no longer is an incentive to stay with one employer for extended periods. The loss of more experienced teachers may lower effectiveness if there is a learning curve for teachers such that more experienced teachers are also more effective ones. Higher initial compensation and greater turnover with an existing learning curve hence may work against each other in determining teacher effectiveness. The exact effect of retirement benefit changes on teacher effectiveness will depend on the size of the impact of compensation, turnover, and experience on effectiveness. Most studies show no link between initial salary and teacher effectiveness [Hanushek and Rivkin 2006] and the effects that have been found tend to be relatively small [Cohn 1968; Perl 1973; Rivkin et al. 2005; Hanushek and Rivkin 2006]. Further, the estimates of the size of the turnover increase vary widely [Allen et al. 1993; Even and MacPherson 1996; Ippolito 1997; Nyce 2007]. And, some researchers have argued that it may take teachers only a year or two to reach their maximum effectiveness [Rivkin et al. 2005], while others find that it takes up to a decade before they hit their full stride [Harris and Sass 2011]. The impact of benefit changes on teacher effectiveness is a priori unclear due to this uncertainty over the size of the relevant factors that determine average effectiveness. We use a simulation model to estimate the impact of changing benefits on teacher effectiveness. We account for parameter uncertainty by using randomly selected parameter values in a Monte Carlo simulation based on the literature that links initial compensation, turnover, and experience to effectiveness. We bias our estimates of the impact of retirement benefits on teacher effectiveness in favor of benefits other than DB pension. We underemphasize, for instance, statistically insignificant estimates of the correlation between higher pay and teacher effectiveness and the administrative costs of alternative benefits. Our results thus present an easily interpretable best-case scenario of the impact of changing retirement benefits on teacher effectiveness, as is often done in policy simulations on changes in public outlays.Footnote 1 Our research is the first systematic evaluation, to our knowledge, of the effect of changing retirement benefits on teacher effectiveness. We specifically quantify the risk for teacher effectiveness from changing benefits, we calculate the impact of simultaneous benefit changes and benefit cuts to account for the fiscal constraints in many states, and we estimate the transition costs of replacing existing DB pensions with alternative benefits. The rest of the paper is organized as follows. The section “Literature review” discusses the relevant literature, the section “The model of retirement plans and teacher effectiveness” sets out the formal simulation model and presents the model parameters, the section “Simulation estimates” summarizes our estimates, and the section “Policy implications and conclusion” concludes.",
39,4,Eastern Economic Journal,10 September 2013,https://link.springer.com/article/10.1057/eej.2013.19,"Public Investment, Growth and Fiscal Constraints: Challenges for the EU New Member States.",September 2013,Anil Duman,,,Male,Unknown,Unknown,Male,,
39,4,Eastern Economic Journal,10 September 2013,https://link.springer.com/article/10.1057/eej.2013.32,Editor's Note,September 2013,Carl P Kaiser,,,Male,Unknown,Unknown,Male,,
40,1,Eastern Economic Journal,04 November 2013,https://link.springer.com/article/10.1057/eej.2013.38,Should Calculus Be a Requirement for Intermediate Macro?,January 2014,David C Colander,,,Male,Unknown,Unknown,Male,"The question about the calculus requirement raises a larger question about macro theory with which neither the Keynesian nor the Dynamic Stochastic General Equilibrium (DSGE) parts of the profession have come to grips. The problem for DSGE advocates is that any tractable formal general equilibrium model is so simple, relative to the macro problem that the economy faces, that the jump to policy discussions does meet the smell test. As Robert Solow put it, telling someone that he or she should understand macro policy problems based on a tractable DSGE model is the equivalent to someone coming to you and saying, “Assume I am Napoleon” and then wanting to start debating war strategies with you. Solow, quite reasonably, suggests that faced with such a request, it makes most sense to walk away. American graduate students have walked away; they have voted with their feet, and graduate school interest in applied micro has expanded while interest in macro (which today in almost all top graduate programs is some variant of DSGE macro) has waned, which has made it difficult, if not impossible, for liberal arts programs to hire someone who can teach a reasonable undergraduate policy-oriented macro course. The result is a serious imbalance in undergraduate economic programs; they are becoming far too heavily applied micro focused. Let me be clear. The issue is not that the DSGE mathematics are too complicated; the issue is that they are not complicated enough to deal with the complexities that arise in an economics system of heterogeneous agents, unknown models, and highly interdependent expectations. After a thoughtful, highly mathematically oriented student has worked through all that math, he or she should be left wondering: What have I actually captured about the workings of a real-world macro economy with the limited math that I am using? Unfortunately, a reasonable answer to that is: Not much that you couldn’t have captured by simply thinking deeply about the problems and not formally modeling it at all. I am not arguing that Keynesians have resolved the problem. It was the problems with their models that brought about the movement to DSGE models. The reality is that Keynesian models are at best engineering models that are then put into a comparative static framework unable to handle the difficulties presented by the complexities of the macro economy. To the degree that Keynesians present their models as theoretical models, not heuristic ad hoc engineering models, they are not conveying to students the analytical difficulties of developing a formal macroeconomic theory. They face the same problems that DSGE modelers face. The general equilibrium theoretical grounding for Keynesian-type arguments about how the macro economy works is in the analysis of non-linear dynamical systems with turbulence. To have any reasonable formal model of such turbulent systems, you need a lot (lot) more than a beginning course in the calculus. If we were going to teach a meaningful formal Keynesian theory in intermediate macro, we should require not only the calculus but differential equations and non-linear dynamical systems. I suspect such a course would not have a large enrollment. The point I am making is that what is called Keynesian theory is far from a formal theory, just as what was called Classical macro theory was far from a formal theory. The models that became enshrined in the Keynesian–Classical synthesis were a collection of simple algebraic comparative static multi-market models, which talked about interrelated goods, labor, and money markets. From a theoretical standpoint, the composite model never held water, was inconsistent with much economic reasoning, and only became understandable when accompanied by a lot of hand waving and fuzzy reasoning. The models that resulted were unrealistic and their underlying logic easily undermined. When these Keynesian, Classical, or synthesis models were taken to the data, they were inevitably tweaked and massaged with fudge factors to make them fit the data. Modern applied DSGE models are little better. The original DSGE models provided a logical core for a general equilibrium economy with a representative agent so they had a core logic to them, if you could stomach the assumptions. However, in terms of thinking about an actual economy, those core models were so simplistic and filled with such unrealistic assumptions that their relevance to the actual macro economy was close to nil, except to those who have been indoctrinated into the DSGE groupthink. When DSGE models are taken to the data, they too are massaged and tweaked with a variety of ad hoc assumptions to make them fit. Thus, for example, the Fed’s DSGE model is full of ad hoc assumptions and modifications that are a far cry from the DSGE models that purists had called for. The reality is that we do not teach scientific macro theory in intermediate macro. The reason is that the economics profession does not have an acceptable macro theory that meets any reasonable scientific empirical criteria of confirmability. We have a variety of models that we teach, but they are merely suggestive, not definitive. The reason is not lack of effort; the reason is the complexity of macro interrelationships and the lack of the data that is necessary to seriously deal empirically with that complexity that the underlying relationships present. What we present to students as a formal macro theory is better described as a collection of insights into the macro economy that some bright economists have put together, and blended into some baby math models to give it an aura of sophistication. To present it as other than that is to mislead students.",
40,1,Eastern Economic Journal,03 September 2012,https://link.springer.com/article/10.1057/eej.2012.23,Subsidization and Privatization: In a Multinational Mixed Oligopoly,January 2014,Ali Dadpay,,,Male,Unknown,Unknown,Male,"An increasing number of authors have contributed to the studies on mixed oligopoly since the model's introduction in the 1960s, [Merril and Schneider 1966]. The defining characteristic of mixed oligopoly markets has been the presence of a welfare-maximizing public firm competing with profit-maximizing private firms. The early work focuses on analysis of a domestic market, where a public firm interacts with private ones [Harris and Wiens 1980; Cremer et al. 1989, 1991; DeFraja and Delbono 1989, 1990; Fershtman 1990; Sertel 1988; Nilssen and Sorgard 2002]. More recently, authors have considered mixed oligopoly in a domestic market open to foreign competition [Fjell and Pal 1996; Pal and White 1998, 2003; Fjell and Heywood 2004]. Within both strands of work, there has been interest in the influence of government subsidization. In mixed oligopoly models, where foreign competition is present, subsidization is considered an element of strategic trade policy. The purpose of this paper is to extend the examination of subsidization beyond the models of domestic markets with foreign competition to a model of a single market that crosses national boundaries and includes competing public firms (see Dadpay and Heywood 2006). The present study is motivated by the commercial airline industry in the Persian Gulf region. This region includes countries such as Iran, United Arab Emirates (UAE), Kuwait, Saudi Arabia, Qatar, and Bahrain. Their mutual business interests and cultural ties have created a growing commercial airline industry. Several public flag carriers and private airlines connect the region's capitals and its major cities. On board these flights, citizens of different nationalities share the consumers’ surplus while governments maximize their respective social welfares and entrepreneurs maximize their profits. The governments of the region differ in their policies regarding the airline industry. Some subsidize the industry through discounted fuel and public funds for its operations, as in the case of Iran Air; and some encourage tourism and hotel industries but do not subsidize the airline industry directly, as is the case of the UAE and the Emirates. In particular, this paper is motivated by the airline markets for the routes between Iran and the UAE.Footnote 1 This paper considers a mixed oligopoly market with a foreign public firm and a domestic public firm, as well as foreign and domestic private firms with consumers from different nationalities, to extend the existing analysis. This paper is also motivated by the fact that market subsidies have been much debated by authors studying government involvement in such markets. Many [Myles 2002; Fjell and Heywood 2004] have suggested that subsidization recovers first-best pricing in a mixed oligopoly. The present study introduces the idea of a subsidy rate in a single multinational market and investigates whether such is the case in this kind of market. The findings depart from the existing literature in suggesting that the unilateral privatization in multinational mixed oligopoly markets does not necessarily involve a prisoners’ dilemma, as suggested previously. A series of previous papers consider a multi-stage game in which the government sets a subsidy in the first stage in an attempt to eliminate the inefficiency associated with market power and to maximize the social welfare function. Later stages involve quantity setting in a variety of mixed oligopoly structures. Poyago-Theotoky [2001] shows that in a mixed oligopoly closed to foreign competition, the optimal subsidy rate is identical regardless of whether the public firm acts as a Stackelberg leader, moves simultaneously with other firms, or maximizes its profit like private firms. Moreover, the optimal subsidy achieves first best with marginal cost equal to price. As the equilibrium is identical with either a profit-maximizing private firm or a welfare-maximizing public firm, privatization has no influence on welfare in the face of subsidization. Myles [2002] shows that these results hold for a wide variety of cost structures as long as some modest conditions on functional form are met. Sepahvand [2002] argues that privatization of mixed oligopoly does not improve domestic welfare even in the presence of foreign firms. He introduces adjusted marginal cost pricing based on the relationship between foreign firms’ output and domestic firms’ output, showing that regardless of the objective function of the public firm, there is an identical optimal subsidy rate, which adjusts marginal cost pricing.Footnote 2 The present study finds that the optimal subsidy rate is a function of the number of firms in the market, their cost structure, and the domestic country's share of consumers’ surplus. It shows that the optimal subsidy does not result in marginal cost pricing. As a consequence, privatization is found to yield ambiguous economic results. The direction of changes in equilibrium values under privatization differs with respect to the share of consumers’ surplus, relative number of private firms in the market, and cost structure. The study also shows that unilateral privatization enhances welfare in the country where public industry is subsidized. This departs from previous literature. Pal and White [1998] show that in a single domestic market, privatization reduces welfare. Fjell and Heywood [2004] confirm this for the case of a domestic market with a Stackelberg leader, and Pal and White [2003] argue it for the case of intra-industry trade markets. Dadpay and Heywood [2006] show that governments are trapped in a prisoners’ dilemma in a multinational mixed oligopoly market, and that neither benefits from unilateral privatization. This study argues that governments can evade this prisoners’ dilemma through subsidization and can benefit from unilateral privatization. The “Model” section presents the model, and the “Characteristics of the Equilibrium” section describes the equilibrium and simulation results. The “Privatization” section examines privatization effects in this model. The “Conclusion” section summarizes the conclusions.",2
40,1,Eastern Economic Journal,29 July 2013,https://link.springer.com/article/10.1057/eej.2012.34,Government Mandates and Atypical Work: An Investigation of Right-to-Work States,January 2014,Christopher J Surfield,,,Male,Unknown,Unknown,Male,"One of the criticisms levied against atypical work arrangements (AWAs), such as contract, consulting, and temporary work, has been that such work affords workers with less employment continuity and stability than that offered by regular, or open-ended, employment. These work forms, while disparate in their demographic, employment, and wage implications for an employee,Footnote 1 all share one common characteristic: there is no expectation of continued employment for a worker beyond that of their current assignment, client, or project. There is a difference in the degree of this employment instability across the various AWAs. At one end of the spectrum lie those workers who are employed as agency temporaries. These workers receive assignments that are often for either a short duration or a fixed term. In addition, agency temporaries are subject to dismissal or replacement requests by the client firm without the provision of cause as well as a lack of subsequent placement by the temporary help agency. At the other end lie those workers who hold contracting and consulting positions. While these workers generally hold both more favorable views towards these work forms and longer tenures in these arrangements relative to other AWAs, they still face the prospect of having to secure new clients or projects once their current assignment ends. Workers engaged in consulting/contracting work themselves assume the risk of failing to secure continued employment. Aside from the question as to why workers themselves would pick up work in an AWA given the diminished employment security thought to be associated with such work, further questions are raised as to why a firm would choose to employ workers in these capacities. Workers employed in an AWA are not likely to be as productive as their open-ended counterparts. Moreover, those jobs being filled by temporary workers tend to be either low-skilled or highly routinized tasks for which little training is required. Abraham and Taylor [1996], Houseman and Polivka [2000], and Gramm and Schnell [2001] argue that atypical workers have little incentive to pick up firm-specific training, or, even, for a firm to provide it, given the rather transient nature of their employment with a given firm or client. Given this wedge in the productivity between atypical and regular workers within a given enterprise, the concern that has arisen is that AWAs are being used for other purposes, such as wage/benefits savings, increased flexibility in the hiring/firing decision or, of more concern, as a means to avoid interventions made into the labor market by the government.Footnote 2 This paper helps to shed some additional light on the use of AWAs by firms in light of one particular governmental intervention in the labor market, which was explicitly designed to increase a worker's employment security. We contrast the prevalence of AWAs in right-to-work (RTW) states against those states that remained agency- or union-shop states. In exercising their prerogative under the federal law created to strengthen the role and bargaining power of labor unions within the context of labor-management relations (the Taft-Hartley Act), some states opted out of the legislation by adopting RTW legislation.Footnote 3 RTW states deviate from their non-RTW counterpartsFootnote 4 in that new hires are not obligated to join any existing union or pay any union dues, which are mandated by any collective bargaining agreement in effect at the time or place of hire as a condition of employment. RTW advocates argue that this opt-out has enhanced the labor market flexibility within those states that adopted it. The argument commonly advanced by advocates of this legislation is that non-RTW rules impose artificial constraints on a firm's ability to subsequently dismiss any poorly matched employees given the automatic induction of any new hires into an existing union or collective bargaining agreement.Footnote 5 Under this assumption, atypical work might be less desirable to firms located in RTW states relative to their non-RTW counterparts if atypical work is indeed being used as a response to legislatively mandated constraints on a firm's hiring/firing ability. Given the more flexible nature of those labor markets within RTW states, firms might not need to resort to using atypical work to obtain the flexibility needed to meet staffing needs or to initially screen new hires. Noted by both Hylton [1996] and Nollen [1996], one particular form of atypical work, agency temporary work, allows firms to nearly completely divest themselves of the legal responsibilities of worker benefits and obligations, while still securing their output. The legal employer of temporary workers is not the firm obtaining the output but, rather, it is the temporary help agency. The temporary help service provides the workers with their paychecks and, if applicable, benefits. It is the temporary help service, and not the client firm, which would be responsible for compliance with any governmental mandates and legal requirements. It is also the temporary help service that secures any subsequent assignments or clients for the workers requesting them and providing whatever employment continuity that such employment offers. In sum, the client firm could, in theory, use a temporary help service to “sample” potential new hires or scale their operations to an appropriate size while not adversely impacting their unemployment insurance risk rating or assuming the legal obligations associated with these workers. Although there is no third-party intermediary, independent consultants and contractors hold similar implications for a firm. With this particular form of atypical employment, workers enter into a contract for a specified time period or for a given project with a firm. The firm obtains the output of the worker without having to assume the legal responsibility that would otherwise be associated with employing the worker in an open-ended capacity. These contracted workers themselves are largely responsible for their tax obligations as well as the securing of any of desired benefits, thereby relieving the firm of these mandates.Footnote 6 Such work, therefore, affords the firm with a greater degree of flexibility in adjusting its workforce as well as a means to avoid any interventions made in the labor market by the government than would regular employment. The structure of this paper is as follows. After a parsimonious review of the literature regarding state labor market regulations and atypical work, we discuss our data and expectations regarding RTW states and AWAs. We use the recently discontinued Contingent and Alternative Employment Arrangement Supplement to the Current Population Survey (CAEAS/CPS) to derive estimates on the prevalence of atypical work at the state level. These CAEAS/CPS data are combined with a number of other data sources, such as the Quarterly Census of Employment and Wages (QCEW) and the National Annenberg Election Survey (NAES), to provide our ceteris paribus findings on the prevalence of atypical work in a state's workforce.",2
40,1,Eastern Economic Journal,17 December 2012,https://link.springer.com/article/10.1057/eej.2012.35,Price Volatility and Contract Maturity: Evidence from an Online Futures Market for Sports Tickets,January 2014,Jihui Chen,Xiaoyong Zheng,,Unknown,Unknown,Unknown,Unknown,,
40,1,Eastern Economic Journal,24 December 2012,https://link.springer.com/article/10.1057/eej.2012.36,The Trade-off between Family Size and Child Health in Rural Bangladesh,January 2014,Christina Peters,Daniel I Rees,Rey Hernández-Julián,Female,Male,Unknown,Mix,,
40,1,Eastern Economic Journal,04 February 2013,https://link.springer.com/article/10.1057/eej.2013.1,Asymmetric Association between Exposure to Obesity and Weight Gain among Adolescents,January 2014,Muzhe Yang,Rui Huang,,Unknown,Male,Unknown,Male,"The obesity rate in the United States has been rising dramatically. According to the National Health and Nutrition Examination Survey, between 1976–1980 and 2007–2008, the prevalence of obesity among adults aged 20–74 increased from 15 to 34.3 percent [Ogden and Carroll 2010a]; during the same period, the obesity rates also increased among children and adolescents, from 5 to 10.4 percent among children aged 2–5, from 6.5 to 19.6 percent among those aged 6–11, and from 5 to 18.1 percent among adolescents aged 12–19 [Ogden and Carroll 2010b]. The consequences of obesity are far-reaching. Obesity may shorten life expectancy [Flegal et al. 2005] and can cause many ailments, such as diabetes and heart disease.Footnote 1 The negative impacts of obesity or overweight on an individual's labor market outcome have been examined by many studies [Hamermesh and Biddle 1994; Averett and Korenman 1996; Biddle and Hamermesh 1998; Baum and Ford 2004; Cawley 2004; Morris 2006]. After smoking, obesity has become the second leading cause of preventable deaths [Goel 2006]. As the obesity rate increases in the United States, so do health-care expenditures.Footnote 2 Because of escalating health-care costs imposed on the entire society, obesity has gone beyond a personal matter. Public interventions aimed at controlling or reducing the obesity rate, such as taxes on junk-foods [Schroeter et al. 2008; Yaniv et al. 2009] or state-level physical education requirements for students [Cawley et al. 2007], may be justified [Philipson and Posner 2008]. Such interventions can be imperative for children and adolescents, because the long-term effects on health of childhood obesity are worse than the effects of adult obesity [Olshansky et al. 2005]. The dramatic increase in the obesity rate in recent decades and its far-reaching consequences have driven many researchers to search for explanations.Footnote 3 Several studies have taken a social perspective because people's health-related behaviors, such as smoking, can be interdependent [Harris and López-Valcárcel 2008], as are other risky behaviors [Lundborg 2006; Clark and Lohéac 2007]. One hypothesis is that if people care about their own body weight relative to others’, then imitative “keeping up with the weight of the Joneses” may exist [Blanchflower et al. 2009]. Another hypothesis is that if people have more social contacts who are obese, then their tolerance for obesity may increase [Philipson and Posner 2008], or there may be a softening of body weight norms [Burke and Heiland 2007]. Those changes could make it possible for obesity to “spread” within social groups [Christakis and Fowler 2007].Footnote 4 Burke et al. [2010] provide empirical evidence showing that body weight norms have become more forgiving in recent decades. Johnson et al. [2012] further introduce the construct of a perceptual threshold for weight-related norms to identify a transition point where body weight shifts from normal to overweight. Although this social aspect of obesity does not pinpoint the origin of obesity, it suggests a potential pathway through which obesity may “propagate” itself. Because social interactions are common among adolescents, several studies have investigated the existence of peer effects among adolescents in terms of obesity or overweight status [Cohen-Cole and Fletcher 2008a; Fowler and Christakis 2008; Renna et al. 2008; Trogdon et al. 2008; Halliday and Kwak 2009].Footnote 5 If social interactions generate a multiplier effect [Glaeser et al. 2003] for obesity, then policy interventions targeting social groups may be effective in curbing the upward trend in the prevalence of obesity: a policy intended to prevent or treat overweight or obesity will have effects not only on the targeted individuals but also on their peers in the same social group. Conceptual analysis of peer effects, also referred to as social interactions or endogenous effects, has been discussed by Manski [2000]. Identification of peer effects is hindered by a series of problems [Shalizi and Thomas 2011], such as self selection of peer groups [Evans et al. 1992], group-level unobserved heterogeneities [Cohen-Cole and Fletcher 2008a; 2008b], the simultaneity problem pointed out by Manski [1993], and the homophily in friendship retention over time discussed by Noel and Nyhan [2011]. Several econometric models with various identification assumptions have been proposed, such as linear-in-expectations models [Manski 1993; Graham and Hahn 2005], linear-in-means models [Lee 2007], discrete choice models with social interactions [Brock and Durlauf 2001, 2002, 2007; Durlauf 2001], multiple-group models [Cohen-Cole 2006], and excessive variances contrast models [Graham 2008]. For these models, one common identification strategy relies on random groupings, in which the self-selection of peer groups and the group-level unobserved heterogeneities can be controlled for across the groups formed randomly [Zimmerman 2003; Guryan et al. 2009]; other identification strategies, for example, are based on sibling fixed effects [Aaronson 1998] or individual fixed effects applied to longitudinal data [Mas and Moretti 2009]. Our study aims to provide empirical evidence consistent with the presence of peer influence in body weight among adolescents. We use data from the first two waves of the National Longitudinal Study of Adolescent Health (Add Health). Different from other studies of peer effects in obesity [Cohen-Cole and Fletcher 2008a; Trogdon et al. 2008; Halliday and Kwak 2009], we focus on a discrete, not a continuous, change in the body weight of a peer group. Specifically, we do not use the obesity rate or the average body weight of a peer group as the key variable for exposure to obesity. Instead, we measure the exposure to obesity by the number of obese friends each respondent nominates in each wave of the Add Health surveys. We use only the first two waves of Add Health because information on friend nominations is not available in the later (the third and the fourth) waves. In our empirical setting, respondents’ peer groups (i.e., their friends) are identified by self-nominations, not formed by random groupings. To identify a causal effect of peers, even with random assignment of peers, one still need to rule out two possibilities: (a) a common shock to a group that induces correlated outcomes of the group members who experienced the shock, as discussed by Guryan et al. [2009]; and (b) a mutual influence between members in a group after the group is formed. With our data, we do not have random groupings, nor do we have a valid identification strategy to rule out the two aforementioned possibilities. As a result, we focus on the association, not the causation, between respondents’ body weight and their exposure to obesity. Although we are unable to identify a causal effect of exposure to obesity on body weight, our data allow us to examine the symmetry in the association between exposure to obesity and body weight — whether weight gain (or loss) is associated with an increase (or a decrease) in the number of obese friends. In doing so, we aim to provide empirical evidence on the theory of rational emulation and deviance proposed by Clark and Oswald [1998]. Unlike other theories directly assuming that individuals wish to be similar between one another in a group, Clark and Oswald [1998] set up a model explaining how an independent and self-interest-driven decision-maker will make a utility-maximizing choice of emulation or deviance. To represent the total utility of a decision-maker, Clark and Oswald [1998] use a convex combination of a private-utility term and a comparison-utility term. They derive three utility-maximizing results, which describe conformable actions, deviant actions, and independent actions, corresponding to the comparison-utility function being concave, convex, and linear, respectively. In essence, Clark and Oswald's [1998] model finds that when the marginal utility of a self-action increases with peers’ action (or not), the self-interest-driven utility maximizer will choose to “follow the crowd” (or not). One implication of Clark and Oswald's [1998] model is that an asymmetry in peer effects will arise if the marginal utility of a self-action can either increase, decrease, or be neutral in response to changes in peers’ action. This means that two cases could coexist in our empirical setting: (a) a positive peer effect (conformity) in body weight: an individual's marginal utility of own body weight increases (or decreases) when the individual has more (or fewer) obese friends; and (b) a negative peer effect (deviance) in body weight: an individual's marginal utility of own body weight decreases (or increases) when the individual has more (or fewer) obese friends. The goal of our study is to examine the existence of an asymmetric association between self body weight and exposure to obesity. The asymmetric association can be driven by the asymmetric peer effects, which, if causal and robust, can have important policy implications. Previous studies of peer effects in body weight have tended to assume that peer effects operate symmetrically, and therefore that weight loss of peers would be subject to a virtuous social multiplier. This, however, need not be the case, as our study offers suggestive evidence that weight loss of peers could have different effects than weight gain of peers. The rest of this paper is organized as follows. The next section describes the data. The subsequent section lays out our econometric models and discusses the threats to causal inference. The penultimate section presents our empirical findings. The final section concludes.",9
40,1,Eastern Economic Journal,18 March 2013,https://link.springer.com/article/10.1057/eej.2013.16,The Effect of Health Insurance Benefit Mandates on Premiums,January 2014,James Bailey,,,Male,Unknown,Unknown,Male,"Much work in health economics has tried to determine why health costs in the United States have increased so rapidly in the last few decades. One underexplored mechanism for increasing costs is state-level health insurance regulation. In 1970, the average US state had less than one health insurance mandate. By 2011, the average US state had 37, according to Laudicina et al. [2011]. Most mandates require health insurance plans to cover a specific procedure, such as mastectomy, or a specific condition such as autism. Other mandates require insurance to cover certain types of health care providers, such as chiropractors. Finally, mandates may specify who is covered by health insurance, for instance, the policyholder's grandchildren. When a mandate is passed, more medical spending is channeled through insurers, rather than being paid directly out of pocket by consumers. This partly explains the long-term shift away from out-of-pocket spending in the US health care market, which is shown in Figure 1. As more medical spending is done using insurance, health insurance costs and premiums will rise. It is quite clear that health insurance premiums have been rising over time, but it is not so clear why exactly this has been happening. This paper will estimate how much of rising premiums can be attributed to the passage of insurance mandates. Source of funds for the US health care spending (1960–2008). The main economic argument for mandates is the classic Rothschild and Stiglitz's [1976] concern about adverse selection. Just as sicker people can push out healthier people by raising the cost of insurance generally, people with a high demand for a specific insured benefit can raise costs for others. For example, an insurer may want to cover treatments for AIDS but may worry about attracting customers who reveal that they have AIDS right after signing up at the normal premium. If AIDS coverage is mandated for all insurers then none suffers an adverse selection problem. The behavioral argument for mandates is that people may not be able to correctly evaluate an insurance policy when enrolling in it. If people systematically undervalue some kinds of coverage, a mandate could make them better off. Finally, individuals may support mandates because they believe that the costs will be born by insurers and employers, giving individuals coverage at no cost. However, Gruber [1994a] found that the cost of a maternity care mandate was almost entirely passed on to individuals likely to use the coverage in the form of lower wages. Lahey [2012] found that infertility benefit mandates did not reduce wages for the affected group, but they did reduce employment. The simple argument against mandates is that they reduce welfare by forcing people to pay for coverage that they were not willing to pay for voluntarily. The adverse selection argument suggests, however, that people may be more willing to pay for coverage if others are doing it too. Therefore, most arguments against mandates try to demonstrate that they raise insurance premiums and reduce the number of people with insurance. Most academic work has focused on the connection between mandates and the number of people without insurance. In one of the first papers on the subject, Jensen and Gabel [1992] developed a theoretical model of a firm's decision to offer health insurance, and showed that mandates make firms less likely to offer insurance. Gruber [1994b] found that five specific mandates did not have a significant effect on the percentage of people with health insurance. He attributed this finding mainly to the fact that the mandates were not binding, since most insurance plans already covered the mandated services. Sloan and Conover [1998] found that a higher total number of mandates does reduce the number of people with insurance, while Cummins [2011] found that different types of mandates may increase or decrease insurance coverage. This paper proceeds as follows: The section “Previous work” describes previous studies of the effect of mandates on premiums, the section “Data” describes the data sources used, the section “Results” presents the econometric results, the section “Discussion” discusses the robustness and implications of the results and the section “Conclusion” concludes.",14
40,1,Eastern Economic Journal,16 July 2012,https://link.springer.com/article/10.1057/eej.2012.13,Is America Coming Apart?,January 2014,Frederic L Pryor,,,Male,Unknown,Unknown,Male,"Economists have paid extensive attention to the increasing income inequality in America since the 1970s, but they have left many of the social and political causes and consequences of this trend for other disciplines. Charles Murray, a political scientist unafraid of stirring controversy, has stepped into the breach and sought to fill this gap. As shown in previous books, such as Losing Ground [Murray 1984], which documents failures in the social policy of the previous two decades to help the poor and disadvantaged, or The Bell Curve [Herrnstein and Murray 1994], which analyzes the emergence of a cognitive elite and the implications of differences in intellectual capacity on US society, Murray is not afraid to enrage his audience. The book under review here is fascinating yet depressing. It explores the empirical evidence that America is currently losing the key qualities underlying its greatness and, presumably, its economic strength.",1
40,1,Eastern Economic Journal,12 December 2013,https://link.springer.com/article/10.1057/eej.2013.20,John Kenneth Galbraith,January 2014,Richard P F Holt,,,Male,Unknown,Unknown,Male,,1
40,2,Eastern Economic Journal,04 March 2014,https://link.springer.com/article/10.1057/eej.2014.10,Some Government Skin in the Game: How to Encourage New Technology,March 2014,David C Colander,,,Male,Unknown,Unknown,Male,,
40,2,Eastern Economic Journal,04 March 2014,https://link.springer.com/article/10.1057/eej.2014.7,Social Capital and Health: A Concept whose Time has Come,March 2014,Richard M Scheffler,Lorenzo Rocco,Susan L Averett,Male,Male,Female,Mix,,
40,2,Eastern Economic Journal,03 June 2013,https://link.springer.com/article/10.1057/eej.2013.26,Does Associational Behavior Raise Social Capital? A Cross-Country Analysis of Trust,March 2014,Paul Downward,Tim Pawlowski,Simona Rasciute,Male,Male,Female,Mix,,
40,2,Eastern Economic Journal,04 November 2013,https://link.springer.com/article/10.1057/eej.2013.40,Older Migrants’ Social Capital in Host Countries: A Pan-European Comparison,March 2014,Caroline Berchet,Nicolas Sirven,,Female,Male,Unknown,Mix,,
40,2,Eastern Economic Journal,04 March 2014,https://link.springer.com/article/10.1057/eej.2014.1,Friends with Health Benefits: Does Individual-level Social Capital Improve Health?,March 2014,Susan L Averett,Laura M Argys,Jennifer C Kohn,Female,Female,Female,Female,"Social capital (hereafter SC) has increasingly been touted as an explanation for a wide range of phenomena including economic well-being, crime, education, and health [Furstenberg and Hughes 1995; Hagan et al. 1996; Teachman et al. 1997; McNeal 1999; Morgan and Sorensen 1999; Sun 1999; Veenstra 2000; Veenstra et al. 2005]. However, as Coleman [1988] noted, “If physical capital is wholly tangible … and human capital is less tangible … social capital is less tangible yet, for it exists in the relations among persons” (p. S100). Reflecting this lack of tangibility, the literature has yet to reach consensus around a single measure of SC making comparisons of the effect of SC on various outcomes difficult. In addition to this measurement problem, the empirical SC literature struggles with many of the econometric challenges faced when estimating the causal effect of human capital, such as education or health, on outcomes such as wages, including endogeneity from both reverse causality and unobservable heterogeneity. These challenges are worth addressing because SC may offer promising policy avenues to confront pressing social problems. In particular, given concerns about the effects of aging populations on health care budgets in OECD countries, SC may offer a low-cost option to improve public health. In this paper, we use longitudinal data from the British Household Panel Survey (BHPS) that includes several indicators of health and SC allowing us to contribute to the literature on the effect of SC on health in a number of ways. We use Multiple Correspondence Analysis (MCA) to create indices for both health and SC that reflect the broad, multidimensional conceptions of these intangible assets. Our continuous health index combines the most commonly used measure, self-assessed health (SAH), with other physical and mental health indicators and health-related behaviors, such as accidents and smoking. Our SC index combines neighborhood factors with the quality of individual relationships and participation in organizations. We compare our results using the MCA SC index with results from models using a summation index and a more commonly used measure of participation in organizations to assess the sensitivity of our results to the assumptions underlying these different measures. We also compare our results using the health index to those using SAH. In addition, this extended panel of data from the BHPS allows us to estimate empirical models to address the potential endogeneity of SC as a determinant of health. First we estimate individual fixed-effects models that purge estimates of any time-invariant heterogeneity that biases estimates of the impact of SC on health. However, there is strong theoretical and empirical evidence that health is not only dynamic, but a strongly persistent process [Contoyannis et al. 2004]. Thus, omitting lagged health risks omitted variable bias due to the strong persistence in health. In addition it is also possible that SC is, in turn, determined by health. We attempt to control for both reverse causality and unobservable heterogeneity in this strongly persistent dynamic process by using the Blundell and Bond [1998] dynamic panel data estimator (BB). The BB estimator is valuable in this application because finding plausibly exogenous external instruments for measures of health and SC has proved challenging in the literature. The BB technique instruments for both health and SC by using lagged values and is also able to model covariates such as labor force status, education, and income as predetermined rather than strictly exogenous. To our knowledge we are the first in this literature to model health dynamically with a long panel.Footnote 1 Our results suggest a strong association between SC and health and are remarkably robust to several different measures of SC as well as two different measures of health. The paper proceeds as follows: the next section reviews the literature defining SC and describing the theoretical link between SC and health and empirical evidence to date. Our empirical methodology is laid out after that. The subsequent section describes the BHPS data including our health and SC measures. The section after that presents our results. Robustness checks are discussed in the penultimate section and the last section concludes. An appendix includes details on our health and SC indices.",4
40,2,Eastern Economic Journal,04 March 2014,https://link.springer.com/article/10.1057/eej.2014.4,Social Capital and Smoking Behavior,March 2014,Lorenzo Rocco,Beatrice d'Hombres,,Male,Female,Unknown,Mix,,
40,2,Eastern Economic Journal,04 March 2014,https://link.springer.com/article/10.1057/eej.2014.5,Effects of Maternal Depression on Social Interactions,March 2014,Hope Corman,Kelly Noonan,Nancy E Reichman,Female,,Female,Mix,,
40,2,Eastern Economic Journal,18 March 2013,https://link.springer.com/article/10.1057/eej.2013.14,Labor Mismatch and Skill Premia,March 2014,Sherif Khalifa,,,Male,Unknown,Unknown,Male,"The 2 × 2 × 2 Heckscher–Ohlin model predicts that trade openness induces countries to export the good that intensively uses the relatively abundant factor of production, and to import the good that intensively uses the relatively scarce factor of production. Accordingly, skill-abundant developed countries are expected to export the good that intensively uses skilled workers. This contributes to an increase in the relative price of the skilled-intensive good, a rise in the relative demand for skilled workers, and consequently an increase in the skill premium. The model also predicts that skill-scarce developing countries are expected to export the good that intensively uses unskilled workers. This contributes to an increase in the relative price of the unskilled-intensive good, a rise in the relative demand for unskilled workers, and consequently a decrease in the skill premium. Theoretical predictions, however, are not supported by empirical evidence. Some developing countries experienced an increase in the skill premium after trade liberalization, while others witnessed a decrease in the skill premium after trade liberalization. This is documented by Hanson and Harrison [1995], Robbins [1996], Wood [1997], Freeman and Oostendorp [2000], and Goldberg and Pavcnik [2004]. Figure 1 shows the relationship between trade openness and the skill premium in several developing countries.Footnote 1 Figures 1(a)–1(e) show the skill premium decreasing with trade openness in some countries, while Figures 1(f)–1(j) show the skill premium increasing with trade openness in other countries. Openness and the skill premium in developing countries. Several studies attempted to address this puzzle in order to resolve the inconsistency between the theoretical predictions and the empirical evidence. The first stream of studies attributed the increase in the skill premium in the South to outsourcing and technology transfer. For instance, Feenstra and Hanson [1995] argue that outsourcing shifts a portion of the input production from the North to the South. As this portion is the most skilled-intensive in the South, outsourcing increases skill demand and the skill premium in both countries. Similarly, Zhu [2004] and Zhu and Trefler [2005] argue that if the North loses competitiveness in unskilled-intensive products, a process of technology transfer is induced. This process implies that the production of the unskilled-intensive products is relocated to the South. The relocated goods are the most skilled-intensive by Southern standards. This Southern catchup raises the relative demand for skilled workers and exacerbates wage inequality. Xu [2003] shows that, in a framework where there are non-traded goods whose range is endogenously determined by the level of trade barriers, a tariff reduction causes an expansion in the South's import range. This increases the demand for skilled workers in the North. The increase in the skilled labor cost in the North leads the South to expand its export range as well. The increase in the export ranges of both countries leads to an increase in skill demand and in the skill premium. Beaulieu et al. [2004] argue that a reduction in trade barriers within the high-tech sector can raise the demand for these products in both countries, and increase the demand for skilled labor and the skill premium. Other studies argued that trade induces skill-biased technological change. Acemoglu [2002, 2003] shows that trade causes the relative price of skilled-intensive goods to increase in the North. The price increase makes the technologies used in the production of these goods more profitable to develop. This skill-biased technical change contributes to the increase in wage inequality in the North. Since the South imitates the North technologies that are becoming more skill-biased, it experiences an increase in the skill premium as well. Thoenig and Verdier [2003] argue that when globalization triggers an increased threat of technological leapfrogging, firms respond by biasing the direction of their innovations toward skilled-intensive technologies. Thus, openness causes defensive skill-biased technical change in the North and technical upgrading in the production of the imitated goods in the South toward more skilled-intensive ones. This causes an increase in wage inequality in the North and the South. These studies provide insights into the factors generating an increase in the skill premium in developing and developed countries. However, they do not address the asymmetry of the response of the skill premium to trade openness within developing countries. This paper contributes to the literature by introducing a framework that features search frictions with heterogeneous workers and occupations. In this context, the paper develops a setup with two countries: the North and the South. In each country, the labor force is divided into skilled workers and unskilled workers. There are two types of final goods: a complex good and a simple good. The former is produced utilizing skilled workers. The latter is produced utilizing both skilled workers and unskilled workers. Labor markets in the two countries feature search frictions. There are two types of firms: those that produce complex goods and those that produce simple goods. The former post, complex vacancies, can be filled by skilled workers only. The latter post, simple vacancies, can be filled by both skilled workers and unskilled workers. This is motivated by the observation that developed and developing countries experience a high level of mismatch between the level of education of workers and the educational requirement of jobs they are occupying. This feature of the labor markets is referred to as overeducation. The introduction of the aspect of skill mismatch is justified by ample documented observations. The evidence on the incidence of overeducation across developed and developing countries is discussed in details in the next section. Wages of the skilled workers in complex occupations, the skilled workers in simple occupations, and the unskilled workers in simple occupations are determined by a sharing rule as in Gautier [2002]. The skill premium is the ratio of the weighted average wage of all skilled workers to the wage of unskilled workers. Trade liberalization increases the skill premium in the North, while the impact on the skill premium in the South depends on the level of skill abundance. Developing countries that are relatively more skill abundant experience an increase in the skill premium after trade liberalization, while others that are less skill abundant experience a decrease in wage inequality. In the context of endogenous skill acquisition, the higher the skill abundance the higher the average productivity of skilled workers compared with unskilled workers. This higher skill bias leads to a higher employment of skilled workers in simple occupations, since they are more productive compared with unskilled workers. This implies that the increase in the price of the simple good in the South, due to trade liberalization, will have a positive impact on the skill premium through the increase in the wage of the skilled workers in simple occupations. If the proportion of this labor type is high, the increase in the price of the simple good leads to an increase in their total wage. This will offset any negative impact on the skill premium caused by the decrease in the wage of the skilled workers in complex occupations or the increase in the wage of the unskilled workers in simple occupations. The opposite impact on the skill premium takes place in countries that are less skill abundant in the South. Accordingly, the response of the skill premium to trade openness in developing countries is conditional upon skill abundance. An empirical analysis is undertaken using the threshold estimation technique introduced by Hansen [1999]. The estimation results suggest the presence of a statistically significant skill abundance threshold, below which the coefficient estimate of the relationship between openness and wage inequality is significantly negative, and above which the point estimate is significantly positive. Thus, the empirical estimation provides evidence that supports the theoretical findings. The remainder of the paper is organized as follows: the section ‘Observations’ presents the evidence on the incidence of overeducation, the section ‘Model’ presents the model, the section ‘Estimation’ includes the empirical estimation, the section ‘Simulation’ includes the simulation, the section ‘Conclusion’ includes the conclusion, and the section ‘Appendix’ includes the data and derivations appendices. References, tables, and figures are included thereafter.",
40,2,Eastern Economic Journal,29 July 2013,https://link.springer.com/article/10.1057/eej.2013.31,An Investigation of Editorial Favoritism in the AER,March 2014,Philip R P Coelho,James E McClure,Peter J Reilly,Male,Male,Male,Male,"This paper contributes to an emerging literature assessing the credibility of academic research. Journal ethics and research validity are being reviewed by government and philanthropic agencies that are investigating potential biases in publications.Footnote 1 Double-blind refereeing has declined, an alleged casualty of technological change; The American Economic Review (AER) dropped it in favor of single-blind refereeing (the referee is unknown to the article’s authors) because working papers are increasingly posted on the internet. Concerns over ethical issues in publication transcend national (and discipline) borders to encompass the entire world; this has led to serious examinations of ethical issues.Footnote 2 Questioning the value of academic research is not restricted to the social sciences.Footnote 3
 Research may be compromised in a variety of ways; this paper examines one aspect of it: editorial favoritism defined as a bias in academic journals in favor of submissions on criteria other than intellectual merit. Editorial favoritism or bias in economics journals has been the subject of numerous investigations; these studies have investigated different: (1) biasesFootnote 4 ; (2) methodologiesFootnote 5 ; (3) data sets; (4) control variables, and (5) time periods. The results of these studies yield ambiguous and/or tentative conclusions (in an egregious example one publication even contained competing conclusionsFootnote 6 ). In this paper, we focus on a type of “favoritism” or bias discussed by Laband et al. [2002]; their study provides evidence that acceptance policies of the AER may favor papers that cite editorial insiders frequently.Footnote 7 We find that citations to insiders are widespread in the AER; in our data set of over 600 papers published in the AER, approximately 46 percent of these papers cited editorial insiders.Footnote 8
 Laband and colleagues are very cautious concerning the hypothesis that there is editorial favoritism in the AER; their caution is consistent with their evidence. In their study, they examine four volumes (years) of the AER (1985, 1990, 1995, and 2000); Table 1 is the same as their Table 3 and is the basis for their (tentative) belief that editorial policies are changing in favor of submissions that give numerous citations to the editors and other journal insiders.Footnote 9
 
Table 1 shows that: (1) in the 1985 volume, the rate that AER articles cited AER editorial insiders was essentially the same as the rates that the Journal of Political Economy (JPE) and Quarterly Journal of Economics (QJE) articles cited AER editorial insiders; and (2) in the 2000 volume, the articles in the AER cited editorial insiders at a rate that was four times higher than the rate at which JPE and QJE publications cited AER insiders. This along with their speculations concerning the monetary value of the citations led Laband et al. [2002, p. 327] to conclude that “rent seeking motives seem to play a relatively small role in the quality control process in economics.” These initial speculations led to a debate between Whaples [2006], and Coelho and McClure [2006] about the decline in critical commentary. Coelho and McClure [2006] ended their paper with a final speculation: “we suspect that a larger factor [in the decline of critical commentary] was editors’ interest in increasing the citations to themselves and their editorial colleagues.” (p. 6) The results in Table 1 are from a data set restricted to “articles” — a subset of all AER publications (articles plus critical commentary works such as comments replies and rejoinders). Coelho and McClure constructed a table similar to Table 1, but solely for “critical commentary” pieces that were excluded by Laband and colleagues; the critical commentary articles had virtually no citations to AER editorial insiders. Juxtaposing this with Laband and colleagues’ findings in Table 1 led Coelho and McClure to speculate that editorial favoritism might be more important than suggested by Leband and colleagues.Footnote 10
 Evidence is lacking for the assertions and speculations surrounding editorial favoritism. Toward a resolution of these issues this study provides: (1) an expanded sample; and (2) controls for variables, such as article length (number of pages), the extent of the bibliography, lead article status, and self citations. In our study, we use an original metric to measure editorial bias and attempt to overcome the limitations inherent in examining the issues.",5
40,2,Eastern Economic Journal,04 March 2014,https://link.springer.com/article/10.1057/eej.2013.21,Big Time Sports in American Universities,March 2014,Victor Matheson,,,Male,Unknown,Unknown,Male,,1
40,2,Eastern Economic Journal,04 March 2014,https://link.springer.com/article/10.1057/eej.2014.11,List of Reviewers 2013,March 2014,,,,Unknown,Unknown,Unknown,Unknown,,
40,3,Eastern Economic Journal,05 June 2014,https://link.springer.com/article/10.1057/eej.2014.25,Economists Do a Lousy Job Teaching Students about Inflation,June 2014,David Colander,,,Male,Unknown,Unknown,Male,"There are a number of problems with how we teach inflation. One is misplaced concreteness — we present inflation as a precise concept, and create models in which a 1 or 2 percent change in inflation is assumed to affect people’s behavior. But a 1 or 2 percent change in the aggregate price level is below individuals’ perception threshold. If they were not told by economists what the level of inflation was, they would not be able to distinguish between the various levels of inflation plus or minus 3 or 4 per cent. People are not interested in a general price level index; they are interested in the prices of the goods they consume and sell. A 2 percent inflation caused by a 10 percent rise in the price of rice in a developing country is something quite different than a 2 percent inflation caused by a more general rise in prices. In advanced economies, where the composition of goods is constantly expanding due to technological innovation, a general price index is even more suspect due to the inherent indexing issues that accompany innovation and technological change. One cannot unambiguously define a price index for a constantly changing, and hence incomparable, market basket of goods. Depending on how one chooses to define inflation, one can come up with significantly different “correct” measures of inflation. The bottom line: There is no one correct inflation rate in the economy, and we should be teaching students that.",
40,3,Eastern Economic Journal,23 December 2013,https://link.springer.com/article/10.1057/eej.2013.42,The Effects of a Minimum Cigarette Purchase Age of 21 on Prenatal Smoking and Infant Health,June 2014,Ji Yan,,,,Unknown,Unknown,Mix,,
40,3,Eastern Economic Journal,04 March 2013,https://link.springer.com/article/10.1057/eej.2013.9,Merger Performance and Antitrust Review: A Retrospective Analysis,June 2014,Ralph Sonenshine,Robert Feinberg,,Male,Male,Unknown,Male,"There is a considerable evidence that a large share of acquisitions made in the United States are unprofitable ex post or that they lead to reorganization and/or divestitures of many of the merged assets not long after the merger. Why does this pattern emerge? Why is it that the factors that originally motivated the merger did not yield the anticipated value? To examine why many mergers have failed, it is useful to consider the motives behind the acquisition. Some of the most often cited potential benefits to mergers include operational, marketing, and financial synergies (including traditional scale economy rationales), gains in market share (and, hopefully, market power), and R&D improvements. This article uses a multi-faceted approach to explore the determinants of merger outcomes, focusing on the key structural and valuation factors that drove the companies to merge. A sample of mergers reviewed by the US antitrust authorities for concentration concerns is used to consider whether increased market concentration levels influence stock market performance. To determine merger success and failure, we analyze the abnormal returns to the merged company, using two alternative measures: for the first, company returns are compared over a period of time to a market capital-weighted index of comparable equities; the second calculates abnormal returns relative to the average of the merged firm's rivals. Abnormal returns are calculated (for both measures) over 1, 3, and 5 years. The sample of mergers used in this study is a group that was reviewed by the government for potential antitrust violations, most of which were challenged (though eventually consummated) and all of which were horizontal in nature. In addition to assessing the influence of structural factors such as market concentration and R&D intensity, this article also examines the effect of valuation discrepancies on post-merger performance. The inherent uncertainty attached to a firm's market value allows for valuation differences between bidders and the owners and management of a target firm. It has been suggested that acquirers overbid to gain control of another firm, and, therefore, the underperformance of a merger can be influenced by the premiums paid. Acquirers may pay overly large premiums because of management's miscalculation of the value of the target's intangible assets and/or goodwill. The presence of multiple bidders can also raise the acquisition price; this is essentially the “winner's curse” discussed by many authors,Footnote 1 which claims that overvaluation of the auctioned asset is a function of the number of bidders, and the return to the winning bidder is inversely related to the uncertainty in the value of the auctioned asset [Boone and Mulherin 2008]. Finally, the merger may have been initiated as part of a pattern or wave of consolidation in a particular industry. Being part of the wave then may impact the price paid and post-merger performance. This article reviews the prior financial and economic literature on the factors that potentially explain the below average performance of the merged firm. We then present the data used and our regression techniques for assessing the impact that particular structural and valuation factors have on merger performance. The results then follow, which show significant negative average abnormal returns for each period studied with the magnitude of these negative abnormal returns generally increasing with the length of time. The factors influencing the results vary somewhat based on the time frame analyzed and the model used. However, we generally find that increasing market concentration and product overlap between firms have a negative effect on abnormal returns in the short run. In the long run, merger size (both absolute and relative) along with acquirer R&D intensity and the deal premium (DP) have a negative effect on abnormal returns, while intangible asset intensity and the presence of a merger wave have a positive effect.",
40,3,Eastern Economic Journal,04 November 2013,https://link.springer.com/article/10.1057/eej.2013.39,The Effect of New Jersey Lottery Promotions on Consumer Demand and State Profits,June 2014,Kathryn L Combs,Jocelyn Elise Crowley,John A Spry,Female,Female,Male,Mix,,
40,3,Eastern Economic Journal,25 March 2013,https://link.springer.com/article/10.1057/eej.2013.11,Centralized Wage Setting and Active Labor Market Policies in Frictional Labor Markets: The Nordic Case,June 2014,Francesco Vona,Luca Zamparelli,,Male,Male,Unknown,Male,"The “Nordic” model provides a way to look at the relationship between inequality, productivity, and employment compatible with the social democratic goal of combining egalitarian distribution of earnings, security of income, and efficiency [see Moene 2008]. The original formulation of the model is due to two Swedish trade union economists, Gosta Rehn and Rudolf Meidner, and dates back to the 1940s. Later on, Rehn and Meidner perfected it and advocated its implementation by the Swedish government throughout the 1950s and 1960s. Three main policies constituted the core of the model: restrictive fiscal policy, active labor market policies (ALMPs), and solidaristic wage policy. In the context of a small open economy during the post-World War II boom, fighting inflation was a bigger concern than stimulating aggregate demand, which was kept high by the external channel: fiscal restraint served this purpose. The two other policies together aimed at fostering structural change while guaranteeing distributive equality and high employment. Centralized bargaining was at the center of the system. By negotiating equal remuneration for identical jobs (“Equal pay for equal jobs”) regardless of the productivity of plants or firms, centralized wage bargaining was thought of as a tool capable not only of providing the equalization of earnings but also of fostering productivity growth. A more compressed distribution of wages would put pressure on low productivity plants, obliging them either to rationalize production, thus increasing productivity directly, or to shut down, thus freeing resources potentially employable by more dynamic and productive firms or sectors. Put differently, wage compression would act as a subsidy to investment in more productive plants by increasing their relative value, and thereby enhancing the scope for job relocation in high-tech activities. ALMPs complement wage solidarity as they help the transition of workers from low to high productive firms, sectors, or regions. ALMPs could be either universal (matching policies and employment subsidies) or selective (supply-side retraining, vocational education, relocation grants). It is a matter of debate whether this model has been faithfully implemented [see Erixon 2010]. Centralized wage setting, in any case, became a distinctive feature of Swedish economic policy between 1956 and 1983. Two different phases can be distinguished. Phase I began in 1956, when national unions of blue-collar workers (LO) and employers (SAF) found the first comprehensive framework agreement for private blue-collar workers; it lasted till the late 1960s. During this period, solidarity wage policy was properly applied according to the principle “equal pay for equal jobs”, and centralized agreements favored wage equalization among analogous jobs in different industries and plants [Hibbs and Locking 2000, p. 760]. In phase II, which began in the early 1970s and ended in 1983 when the last comprehensive agreement was signed, the main goal of wage solidarity shifted from facilitating structural change to achieving wage equalization per se, irrespective of the type of job. As a result, wage inequality was reduced not only across plants and industries, but also within plants and across skill grades. From the late 1970s, this extensive wage compression tended to reduce both returns to and investment in human capital, thus possibly favoring a productivity slowdown [Leamer and Lundborg 1997; Lindbeck 1997]. Empirical evidence on the relation between wage dispersion and productivity [Hibbs and Locking 2000] supports the view that a reduction in “across-plant” wage inequality positively affects labor productivity growth, while a reduction in “within-plant” wage inequality — accompanied by an equalization across skill levels — would be harmful. Such difficulties led to a progressive abandonment of centralized national bargaining, which after 1983 mostly took place at industry and firm levels. Wage inequality regained ground, but currently it still stands at levels substantially lower than in Anglo-American economies [see Pontusson 2006]. Spending on ALMPs became a key component of Swedish economic policies in 1957, and it showed a positive trend as share of both GDP and government budget till the early 1980s. In terms of international comparisons, the Swedish GDP share of ALMP expenditures was consistently among the top OECD countries throughout the 1970s and the 1980s [see Erixon 2010]. Empirical evaluations of the effect ALMPs had on unemployment in Sweden are however not conclusive [see Calmfors 1993; Calmfors and Skedinger 1995]. Agell and Lommerud [1993] and Moene and Wallerstein [1997] provided possible formalizations for the Rehn–Meidner model by analyzing the positive relation between centralized wage bargaining and structural change. Agell and Lommerud [1993] develop an endogenous growth model to show that egalitarian pay compression, combined with active labor market policies, is analogous to an industrial policy of subsidizing the most promising industries. Moene and Wallerstein [1997] compare the performances of centralized and decentralized wage bargaining in terms of productivity and employment outcomes in a vintage growth model with exogenous technical change. They show that centralized bargaining is always superior to local bargaining in terms of steady-state productivity while it is always inferior in terms of employment; the effects on investment and total output depend on the share of productivity accruing to workers, that is the degree of wage moderation. They do not consider, however, the role of active labor market policies. Our paper adopts a standard search and matching model with endogenous job destruction to investigate two issues. First, we use a simplified version of Boeri and Burda [2009] to show that at low levels of wage share, centralized wage bargaining performs better than decentralized bargaining in terms of average productivity and unemployment. This result is similar to the findings of Moene and Wallerstein [1997] and appears to describe properly the experience of Nordic countries till the 1980s. Next, we introduce ALMPs in Boeri and Burda [2009] to assess their comparative performance under decentralized and centralized wage bargaining regimes. We establish analytically that, under certain conditions, such policies are more effective in reducing unemployment under the centralized regime. Furthermore, numerical analyses allow us to demonstrate this result for more general configurations of the relevant parameters and to show that the difference in policy effectiveness is quantitatively relevant. Notice that, by adopting a framework like the one proposed by Boeri and Burda [2009], we cannot properly talk about structural change, which would require a full-fledged multi-sectoral growth model. This is the cost we pay to be able to achieve an analytical comparison of active labor market policies under the alternative wage-setting regimes. The rest of the paper is organized as follows. The next section outlines a standard search and matching model with endogenous job destruction, and compares equilibrium outcomes between centralized and decentralized wage-setting regimes. The subsequent section introduces labor market policies in the model and carries out a comparison of their effectiveness under the two bargaining systems. The final section offers some concluding remarks.",3
40,3,Eastern Economic Journal,18 March 2013,https://link.springer.com/article/10.1057/eej.2013.12,Neoclassical Growth Theory and Heterodox Growth Theory: Opportunities For (and Obstacles To) Greater Engagement,June 2014,Mark Setterfield,,,Male,Unknown,Unknown,Male,"Growth theoryFootnote 1 has a long and illustrious history in economics, and has occupied some of the discipline's great minds.Footnote 2 It is perhaps not surprising, then, that the field is characterized by a great many different specific models of growth. In contemporary economic theory, these models can be divided into two broad types: neoclassical growth theory (NGT) and heterodox growth theory (HGT).Footnote 3 The purpose of this article is to examine the structure of NGT and HGT with a view to establishing the possibilities for greater engagement between these traditions. A core assumption of the article is that in any academic community charged with the creation and dissemination of knowledge, more (rather than less) interaction between researchers (and teachers) is always to be preferred. The benefits of greater engagement between NGT and HGT — and hence the motivation for this inquiry — follow directly from this assumption. A central claim of the article is that even if we confine our attention to simple steady-state models of growth, there exists sufficient theoretical congruence between NGT and HGT to facilitate greater engagement. In particular, recent developments in both NGT and HGT suggest the interaction of trend and cycle — and more specifically, the capacity of demand to influence the course of long-run growth — as an emerging “common front” in growth theory. Although a mixture of rhetorical and sociological factors within the discipline of economics may obstruct exchange between NGT and HGT, it is argued that these factors should not be allowed to thwart profitable interaction between researchers in these different traditions. The article is organized as follows. The next section provides an overview of the core models that comprise NGT and HGT. All models are developed so as to emphasize the way they describe the basic “mechanics” of growth [Jones 2002]. The level of generality so achieved enables us to describe the core insights of the NGT and HGT traditions in terms of just five structural models.Footnote 4 The following section “Opportunities for greater engagement between NGT and HGT” then discusses the key theoretical similarities between NGT and HGT that provide opportunities for greater engagement between these traditions, before the section “Obstacles to greater engagement” identifies rhetorical and sociological barriers to such engagement. The final section offers some conclusions.",5
40,3,Eastern Economic Journal,15 April 2013,https://link.springer.com/article/10.1057/eej.2013.13,Testing the Effectiveness of Regulation and Competition on Cable Television Rates,June 2014,Mary T Kelly,John S Ying,,,Male,Unknown,Mix,,
40,3,Eastern Economic Journal,15 April 2013,https://link.springer.com/article/10.1057/eej.2013.24,The Linkage between Fertility and Labor Productivity: A European Perspective,June 2014,Mathias Laich,Andrea Schneider,,Male,Female,Unknown,Mix,,
40,3,Eastern Economic Journal,22 April 2013,https://link.springer.com/article/10.1057/eej.2013.25,Fiscal Stimulus and Credibility in Emerging Countries,June 2014,Magda Kandil,Hanan Morsy,,Female,Female,Unknown,Female,"The deep global recession following the collapse of Lehman Brothers in September 2008 has focused attention on the need for counter-cyclical macroeconomic policies. The scope for monetary policy was hampered by the credit freeze in the financial system, which was constrained by the accumulation of toxic assets awaiting a resolution to restore confidence and efficient intermediation.Footnote 1 While a heated debate has emerged on the specifics, the need for fiscal intervention to support demand proved to be larger and of longer duration than initially envisaged. Further, there is a need to consider lags governing the fiscal policy transmission to decide on the speed of fiscal withdrawal without jeopardizing the recovery efforts. The debate surrounding the effectiveness of fiscal policy has been centered on the concern about fiscal space, that is, the scope to invoke a much-needed fiscal stimulus during a downturn. Can governments afford the cost of the fiscal stimulus, hoping for higher revenues once recovery is at full speed to service the new debt and ensure sustainability? While fiscal expansion may be necessary to stimulate economic activity, not every country has the resources to finance fiscal stimulus. Some countries do not have enough fiscal space to run counter-cyclical policy during a recession with limited access to financing from international capital markets, and high concerns about policy credibility and debt sustainability.Footnote 2 The need for fiscal stimulus necessitates a careful evaluation of fiscal space and available financing. Fiscal policy in emerging market countries tends to be pro-cyclical because capital flows and commodity exports drive business cycles in these countries. Hence, fiscal spending varies pro-cyclically with government revenues increasing during a boom and decreasing during a downturn, and therefore increasing the severity of cyclical fluctuations. Therefore, when capital flows dry up and commodity prices plunge, financing an expansionary fiscal policy becomes increasingly difficult.Footnote 3 Nonetheless, in response to the global slowdown, a number of emerging markets have announced fiscal stimulus plans to revive economic conditions and assist a speedy recovery. Countries that have managed to accumulate international reserves during a boom were potentially less exposed to sudden stops and market access to finance additional spending during the crisis. Moreover, these countries were in a better position to accommodate fiscal spending by increasing domestic liquidity to afford domestic borrowing if necessary. The issue of affordability has turned attention to available international reserves. Countries with adequate international reserves would be seen as more credible and better positioned to respond with fiscal stimulus, with less concern about crowding out private activity. Reserves availability would increase the scope for accommodating monetary policy, relaxing domestic financing constraints, and reducing the risk of crowding out private activity. Furthermore, reserves adequacy would improve credit rating, reducing the risk premium on external financing. Among countries with abundant international reserves are energy-producing countries that have built cushions during the recent surge in world energy prices.Footnote 4 Other emerging countries such as China and Brazil have also accumulated record-high international reserves, benefiting from a surge in export prices, robust demand, and sustained capital inflows. Countries with a limited pool of international reserves tend to have less scope for fiscal stimulus. In their case, fiscal expansion tends to push up borrowing costs, which reduces the credibility of fiscal expansion, as it crowds out private activity and offsets the effectiveness of the stimulus. A number of developing countries have become increasingly resource constrained as they continue to struggle to safeguard international reserves in the face of a surge in the cost of imports, particularly for food and fuel, and mounting external debt service costs, which present severe pressures on their limited foreign resources. Countries that have come into the crisis with excessive fiscal deficits or public debts — or that have current account deficits that can no longer be financed — had little room for maneuver. Similarly, loss of revenues — particularly commodity-related or import-related taxes — may also constrain fiscal space. The objective of our investigation is to evaluate the scope for fiscal stimulus across two groups of countries, using the availability of international reserves as the dividing benchmark. The first group is countries that have adequate reserves, defined by international reserves that equal to or exceed 3 months of imports.Footnote 5 The second group is resource-constrained countries with international reserves below the equivalent of 3 months of imports. We seek to study the implications of these differences and other financing constraints on the credibility and effectiveness of fiscal policy in emerging countries.",1
40,3,Eastern Economic Journal,05 June 2014,https://link.springer.com/article/10.1057/eej.2013.22,Prosperity Without Growth: Economics for a Finite Planet,June 2014,Emily Northrop,,,Female,Unknown,Unknown,Female,,2
40,3,Eastern Economic Journal,05 June 2014,https://link.springer.com/article/10.1057/eej.2013.23,"Who Shall Live?: Health, Economics and Social Choice",June 2014,Lall Ramrattan,Michael Szenberg,,Unknown,Male,Unknown,Male,,
40,3,Eastern Economic Journal,05 June 2014,https://link.springer.com/article/10.1057/eej.2013.45,The Price of Civilization,June 2014,Mark L Wilson,,,Male,Unknown,Unknown,Male,,
40,3,Eastern Economic Journal,05 June 2014,https://link.springer.com/article/10.1057/eej.2013.46,Sharing the Prize: The Economics of the Civil Rights Revolution in the American South,June 2014,Marie Christine Duggan,,,Female,Unknown,Unknown,Female,,
40,3,Eastern Economic Journal,05 June 2014,https://link.springer.com/article/10.1057/eej.2013.47,Maynard’s Revenge: The Collapse of Free Market Macroeconomics,June 2014,Jesper Jespersen,,,Male,Unknown,Unknown,Male,,
40,4,Eastern Economic Journal,12 September 2014,https://link.springer.com/article/10.1057/eej.2014.39,Gross Output: A New Revolutionary Way to Confuse Students about Measuring the Economy,September 2014,David Colander,,,Male,Unknown,Unknown,Male,"Before I explain my reasoning, let me briefly discuss the history of the new gross output measure. The impetus to provide this new measure came from Mark Skousen, a maverick supply-side economist who has been pushing for such a measure for the last 25 years, at least. He made the argument for the measure in “The Structure of Production” [Skousen, 1990, 2007]. In it he argued that gross output was a much better measure of production in the economy than the standard Gross National Product (GNP) and Gross Domestic Product (GDP) measures. Even before Mark had published the book he had written me about the measure. He felt that since we were both harsh critics of the AS/AD framework, I would be supportive of his new measure. I wrote him back that I saw the measure as different, but not better; which measure was better depended on what one was trying to measure. I saw the two measures as compliments, not substitutes. That, I suspect, was the standard economist’s response. At least it should have been; despite being a maverick in my own right, I’m more standard than Mark. In any case, initially, Mark didn’t have much luck in pushing for his measure, and his “gross output” measure never really was discussed in the economic literature. But Mark is a persistent well-connected supply-side economist, and he kept at it. So I wasn’t all that surprised when last year Mark wrote me that the BEA decided to start providing the measure quarterly. (It has always been available on a yearly basis. I’m not sure what the politics of that BEA decision were, but I suspect they were interesting.)",3
40,4,Eastern Economic Journal,12 September 2014,https://link.springer.com/article/10.1057/eej.2014.40,Introduction to the Symposium on Sports Economics,September 2014,Victor Matheson,Peter von Allmen,,Male,Male,Unknown,Male,,2
40,4,Eastern Economic Journal,12 September 2014,https://link.springer.com/article/10.1057/eej.2014.42,Passing Up Uncertainty for Attendance: The NCAA Basketball Tournament Organizers Change Direction,September 2014,Stephen G Bronars,Todd A McFall,,Male,Male,Unknown,Male,"It is not uncommon for cartels to update their products in order to maintain market power. In 2002, the National Collegiate Athletic Association (NCAA), a cartel discussed famously by Koch [1973], Fleischer et al. [1992], and Branch [2011], implemented slight adjustments to the organizational structure of its crown jewel, the annual Division I men’s basketball tournament. The new organizational system, referred to as the “pod system,” exists because the NCAA lifted a self-imposed constraint it faced when determining the field for its tournament. At the time of the change, the NCAA was not coy about what it wanted to accomplish by using the pod system. The policy change allowed the organization to provide the most powerful members of the cartel with improved first-round assignments. The purpose of this paper is to analyze the trade-off between uncertainty of its tournament games and increased attendance at its tournament the NCAA made when it adopted the pod system. Our analysis of 896 first-round tournament games, played from 1985 to 2012, allows us to conclude the NCAA’s use of the pod system is consistent with its stated intent — the most powerful members of the cartel have been provided increased opportunities to play first-round games at relatively familiar tournament sites. Collectively, the adoption and use of the pod system allowed the organization to trade-off reduced uncertainty in its first-round games for increased attendance. Our analysis of the NCAA’s actions is informed by three previous strands of economic studies. First, the NCAA’s actions are consistent with the successful cartels outlined in Levenstein and Suslow [2006], which notes the importance of a well-defined hierarchy within a cartel for easing contentious cartel decision making. The trade-offs the organization faces in regulating competition within the Division I level are also consistent with the keen observations made in Neale [1964], which discusses the competing need for athletic competitions to satisfy fans’ desires to see their favorite teams be successful while competing in fair and legitimate contests. Finally, the effect of the policy switch — reduced uncertainty leading to increases in attendance — is related to previous studies of the trade-offs between uncertainty of contests and attendance at such contests, as summarized in Szymanski [2003].",
40,4,Eastern Economic Journal,12 May 2014,https://link.springer.com/article/10.1057/eej.2014.27,A Violent Response to Changing the Rules of the Game: The Case of “The Split” in Scottish Premier League Soccer,September 2014,Ryan Dansby,R Todd Jewell,,,Unknown,Unknown,Mix,,
40,4,Eastern Economic Journal,23 December 2013,https://link.springer.com/article/10.1057/eej.2013.43,Quantifying Market Inefficiencies in the Baseball Players’ Market,September 2014,Ben Baumer,Andrew Zimbalist,,Male,Male,Unknown,Male,"Lewis’s [2003] bestselling book Moneyball was published in June of 2003.Footnote 1 Stripped of its storytelling, Lewis presented two strong theses. The first was that baseball executives had been using the wrong metrics to value the productivity of players and that this tendency was reversed by general manager Billy Beane and the Oakland Athletics in the early 2000s. The second, as a consequence, was that certain skills were undervalued (market inefficiency) and intelligent executives at small market teams could overcome their competitive disadvantage by exploiting the skill undervaluation. While the Athletics performed well above what their meager resources would lead one to predict, the notion that sabermetric smarts could undo baseball’s competitive balance problems engendered significant skepticism. If Lewis was correct in his assertion of skill undervaluation then the world would know the secret shortly after his book was published and Billy Beane’s advantage would soon disappear. Central to Lewis’s narrative was the importance of a player’s walk rate. On-base percentage (OBP)Footnote 2 was more closely correlated with a team’s win percentage and revealed higher consistency from 1 year to the next for individual players than did a player’s simple batting average (BA). OBP was more closely correlated to win percentage because a walk not only put a runner on base and sometimes moved other runners up, but it also allowed an additional batter to reach the plate during an inning and wore down the arm of the opposing pitcher. BA did not capture the important skill of having a good batter’s eye and being able to work a walk. While the superior value of the OBP metric, relative to BA, is manifest upon a moment’s reflection, sabermetricians have turned their attention to more ambitious metrics, such as quantifying a fielder’s range or separating out the value of good defense from good pitching. The analytic work done in baseball front offices these days is guarded closely as proprietary with teams seeking to reap the benefits of their discoveries as long as possible before other teams catch on. As the work of sabermetricians becomes more secretive, the speed of the market adjustment process may slow. In the case of OBP, however, once the Athletics’ strategy was recognized, it was easy for other teams to emulate it. Indeed, it is possible that the appreciation of the skill of walking became overdeveloped and OBP became overvalued; that is, the return to the ability to work a walk may have outpaced the value of working a walk. Ironically, to the extent that the market overadjusts to OBP or to other skills, general managers who pursue this skill in the marketplace will find that their team can be disadvantaged by the application of sabermetric knowledge. In contrast, the laggard general manager who eschews analytics may be temporarily advantaged by his obscurantism. In any event, now, with 10 years of market response since the publication of Moneyball, it is interesting to follow how the skill of plate discipline has changed in value over time. To study this question, we began with two papers by Hakes and Sauer [2006; 2007], In a 2006 article, Hakes and Sauer employ data from 2000 to 2004 to compare the relative salary returns to changes in slugging percentage (SLG)Footnote 3 and OBP to the relative impact of SLG and OBP on win percentage. The key distinction they make is that SLG is a traditional measure of batting prowess, whereas OBP is the sabermetric variable of choice. They find that OBP is undervalued relative to SLG between 2000 and 2003, but that this undervaluation is abruptly reversed in 2004, the year after Moneyball is published. While instructive, this first article is limited by (a) only including 5 years of data and 1 year of data after the book’s publication; (b) the fact that both OBP and SLG include singles in the numerator and outs in the denominator and, hence, are correlated with each other; and (c) the likelihood that the valuation of SLG is a function both of its contribution to winning and, independently, its contribution to fan enjoyment of the games (extra base hits and home runs are more exciting to watch than walks and singles). Hakes and Sauer published a second paper in 2007 that extended their data set to 1986–2006 and introduced a refined separation of different hitting skills. Hitting skills were now delineated as Bat (batting average), Eye ((walks+hit by pitch)/plate appearances), and Power (bases per hit). The basic results corroborated those of the 2006 study. Eye was relatively undervalued until 2004, when its valuation spiked. The authors also found that the returns to Eye diminished in 2005 and 2006, indicating a possible overcorrection in 2004. By 2006, the return to Eye or plate discipline was at the same level it had been in 2003. In this paper, we seek to further refine the modeling of these relationships and to take advantage of the time passed to follow the pattern of market response through 2012.",8
40,4,Eastern Economic Journal,02 June 2014,https://link.springer.com/article/10.1057/eej.2014.28,"Gender Differences in Competition: Running Performance in 1,500 Meter Tournaments",September 2014,Jamie Emerson,Brian Hill,,,Male,Unknown,Mix,,
40,4,Eastern Economic Journal,04 March 2013,https://link.springer.com/article/10.1057/eej.2013.10,Understanding Price Movements in Point-Spread Betting Markets: Evidence from NCAA Basketball,September 2014,Brad R Humphreys,Rodney J Paul,Andrew Weinbach,Male,Male,Male,Male,"We analyze the relationship between changes in point spreads on National Collegiate Athletic Association (NCAA) Division I men's basketball games and the volume of bets placed on the home and visiting team using survival analysis. Little previous research has examined changes in point spreads over time. Survival analysis techniques are well suited to the analysis of price changes because they exploit both the direction and timing of price changes, both of which are important in betting markets where the outcome of the wager has a well-defined end point and relatively little time elapses between the opening of trading and the revelation of the outcome. Analyzing the relationship between movements in point spreads and betting volume can extend our understanding of the behavior of sports books, the suppliers of bets in sports betting markets. In addition, new insight into the relationship between changes in point spreads and bet volumes can be generalized to other related financial markets, such as equities, furthering our understanding of the functioning of these markets. We find that observed changes in point spreads set by the Mirage and Hilton sports books in Las Vegas on several thousand regular season NCAA Division I men's basketball games to be unrelated to the volume of bets placed on home and visiting team in these games. Point spreads on games with unbalanced betting on either side (home win or visitor win) do not appear to change more often than point spreads on games with balanced betting. The most commonly discussed model of sports book behavior in the literature, the “balanced book” model, assumes that sports books set point spreads in order to balance the volume of bets on either side of propositions in sports betting markets. According to the “balanced book” model, changes in point spreads should be more likely to occur in games where the early betting is unbalanced, as the sports book attempts to induce later bettors to bet on the other side of the proposition by adjusting the point spread. We present evidence that is inconsistent with his model; observed point-spread changes do not appear to be related to imbalances in betting volumes on college basketball games. Instead, sports books are more likely to change the point spread on games with relatively large opening point spreads. Games with large point spreads are played between teams with relatively unequal abilities. The outcome, in terms of the score difference, in games played between teams of unequal abilities may be more difficult to forecast than the outcome in games played between teams with relatively equal abilities. In this case, observed changes in point spreads can be interpreted as attempts to improve the forecast of the point difference in these games, and not to alter the volume of bets made on each side of the proposition. Evidence exists that unbalanced bet volume is associated with higher bookmaker profits. Humphreys [2010] showed that the larger the volume of bets placed on the home team in National Basketball Association (NBA) games, the less likely was a bet on the home team to win, controlling for the point spread on the game using an IV approach to correct for potential endogeneity of the point spread, suggesting that bookmakers earn higher profits on games with unbalanced betting volume. Humphreys [2011] showed that bookmakers earned a higher profit by accepting unbalanced betting on National Football League (NFL) games than they would have if the betting volume was balanced on each game, using financial simulations based on the actual point spreads and game outcomes from a large number of games played in the NFL. Prices in financial markets change frequently. Because of the frequent nature of price changes in financial markets, survival analysis has been used to analyze changes in stock prices and the time between trades in order to better understand the determinants of these changes. In the sports betting literature, a number of previous studies have analyzed the difference between the opening line and the closing line in betting markets for NFL games [Gandar et al. 1988; Avery and Chevalier 1999], The NBA [Gandar et al. 2000; Gandar et al. 2002b; Colquitt et al. 2007], and NCAA football [Dare et al. 2005; Durham et al. 2005; Durham and Perry 2008]. Durham and Santhanakrishnan [2010] analyzed the effect of changes in information on changes in point spreads. In general, the analysis of changes in point spreads assesses opening and closing lines as forecasts for actual game outcomes and estimate reduced form regression models explaining variation in the change in the point spread from the opening to closing of the market. In both the approaches, the empirical analysis proceeds from the assumption that observed changes in point spreads reflect attempts by sports books to balance betting volume on either side in games. The literature emphasizes the idea that either private information on the part of bettors, or bettor biases in the form of beliefs in “hot hand” effects or the simple desire to bet on a specific team to win or lose a game at any point spread leads to imbalances in betting volume on specific games. In general, the literature shows that closing lines forecast game outcomes better than opening lines, and changes in point spreads from the opening line to the closing line are not well explained by public information such as poll standings, experts’ opinions about game outcomes, or recent performance. These results lead researchers to conclude that significant private information exists in sports betting markets, or that bettors are “noise traders” who exploit short-term volatility in the market. Nearly every paper in this literature explicitly assumes that the balanced book model provides the underlying rationale for observed point-spread changes.",5
40,4,Eastern Economic Journal,15 April 2013,https://link.springer.com/article/10.1057/eej.2013.17,Crime Rates and Police Efficiency,September 2014,Gregory DeAngelo,Donald F Vitaliano,Hannes Lang,Male,Male,Male,Male,"According to one of America's leading criminologists, Wilson [2011], greater efficiency of police departments is one of the four major causes that have driven down the level of crime in the past 40 years. In the face of a growing fiscal crisis, it is more important than ever that policing be carried out as efficiently as possible. For example, Oregon's State Police had to reduce its force by 35 percent in 2003 solely due to budget cuts, which decreased officer effectiveness [DeAngelo and Hansen 2010].Footnote 1 In New York State, Governor Cuomo proposed a US$60 million budget cut to the New York State Police [Smart, WGXC Hands-on Radio Newsroom 2011], while other upstate New York cities such as Rochester unveiled a spending plan that includes laying off 51 full-time positions in the city police department — a story that is echoed throughout upstate New York cities [Voorhees and George 2011]. In this paper, we investigate the comparative efficiency in minimizing serious crimes of 50 local police departments in New York State. An output-oriented Data Envelopment Analysis (DEA) is used to investigate the potential for each police force to reduce crime while holding constant budgeted resources. The results are used to make recommendations about how to increase police efficiency and effectiveness. The DEA method of mathematical programming was first introduced by Charnes et al. [1978]. Among its advantages, this method does not need a specific parametric function and allows for zero values of inputs or outputs. Its principal shortcoming is the possibility of measurement errors that can lead to outlier observations resulting in a false efficient frontier. A careful examination of the raw data is therefore necessary when DEA is utilized. The data for our analysis is taken from the 2003 Law Enforcement Management and Administrative Statistics (LEMAS) survey [Bureau of Justice Statistics 2003], the US Census, and the Crime Index for New York State. The data is discussed in more detail below. This paper breaks new ground in several areas. First, we believe that we present the first logically coherent model of the production of police services in contrast to the somewhat ad hoc framework found in the existing literature on measuring police efficiency. This model informs both the choice of technique (data envelopment) and the metrics of inputs and outputs employed. Second, ours is the first paper to compare the efficiency of a cross-section of municipal police departments in the same state. Since these departments are all located in the same state, issues of cross-state differences in police culture and legal framework are minimized, and the best-practice standard of efficiency used is more policy-relevant. Finally, questions are raised about community policing, which enjoys widespread support but appears to impose an opportunity cost of higher crime rates. In the proceeding sections of the paper, we offer a review of the literature on police force efficiency, describe the theoretical framework, offer an output-oriented DEA model, and empirically evaluate the efficiency of 50 city police departments under investigation. The paper concludes with a description of the findings and recommendations for future research.",1
40,4,Eastern Economic Journal,09 September 2013,https://link.springer.com/article/10.1057/eej.2013.29,"Publishing Trends in Economics across Colleges and Universities, 1991–2007",September 2014,Anne E Winkler,Sharon G Levin,Wolfgang Glänzel,Female,Female,Male,Mix,,
40,4,Eastern Economic Journal,14 October 2013,https://link.springer.com/article/10.1057/eej.2013.33,Does the Hot Hand Drive the Market? Evidence from College Football Betting Markets,September 2014,Michael Sinkey,Trevon Logan,,Male,Male,Unknown,Male,"How do markets function when responding to behavioral strategies? We study the behavior of a prediction market — the American college football betting market — where a market maker, the betting house, sets the price for the entire market. This paper examines how a market maker prices behavioral strategies when there may be several different sources of bias. In a betting market, the market maker has one choice variable, the market price, with which to control an entire array of betting strategies. How does a market maker price different available strategies when she is not able to discriminate the price? Does balancing leave open opportunities for bettors to profit, and are those profits large and consistent enough to cause market inefficiency? In any betting market bettors have a number of different strategies available, but how betting houses balance these strategies in their pricing is unknown. Betting houses would presumably minimize their exposure to the most prevalent forms of bias, but this could open them to exposure to less prevalent forms that bettors could exploit, which mirrors more familiar “contrarian strategies” [Lakonishok et al. 1994; Grinblatt and Keloharju 2000]. This usually leaves us in the situation of searching for sophisticated strategies that a handful of investors could exploit for profit [Sullivan et al. 1999; White 2000; Conrad et al. 2003]. We examine the pricing of a narrow set of simple, transparent betting strategies that unsophisticated bettors could adopt (such as always betting on the home team) to ask how betting houses minimize their exposure to prevalent forms of bias. While one could argue that a small number of especially sophisticated bettors could always strategize for profit, we concentrate here on strategies that an unsophisticated bettor could use. Presumably, these are the strategies that the betting house is most sensitive to since the majority of bettors are unsophisticated. We use data from the universe of college football betting outcomes from 1985 to 2008, over 14,000 games, to test for market efficiency in the college football betting. Specifically, our direct test for market efficiency estimates whether or not bettors can consistently make profit by betting clearly identifiable, simple strategies. We find robust evidence that, for some strategies, the market is inefficient, in contrast with other work on college football, including Dare and McDonald [1996] and Fair and Oster [2007]. In particular, we find that favorites are consistently overpriced. We estimate that, for example, a bet of US$1,000 against a favored team in prominent games would yield $1,105 in expectation, a gross return of nearly 11 percent and a net return of 1 percent after accounting for the transaction cost associated with placing the bet. More importantly, we analyze how this betting market functions to understand the source of this inefficiency. The key piece of information used in a betting market is the point spread, also known as the betting line, which is the predicted margin of victory in a given game. Bettors place bets that a team will “beat the spread” (exceed the predicted margin of victory) or not. Existing analysis of market function postulates that previous performance against the line should not be predictive of future performance if the market is efficient. We find that betting lines are not independent from game to game. Betting lines have memory — we present robust evidence that they are functions of previous betting market results. In particular, we find that betting lines are systematically greater for teams who beat the betting line the previous week. We also find that the magnitude of these increases is significantly greater in this market than in other betting markets, such as professional basketball [Brown and Sauer 1993a] and professional football [Brown and Sauer 1993b]. Given this serial correlation, we next test to see whether or not teams that exceed the betting line are likely to do so in the following week. We find that teams who exceed the betting line in 1 week are no more likely to do so in the following week. Thus, bettors who believe that teams are more likely to exceed the betting line in subsequent weeks believe so erroneously, as “hot” teams are priced efficiently. We find no evidence that would suggest that a bettor could examine past performance against the spread and construct a strategy based on this data to make profit. We then ask why some strategies (like the hot hand) are priced efficiently while others are priced less accurately. As a rule, betting houses are particularly sensitive to any bias among bettors. It is commonly assumed that the profit motive of a betting house is to have an equal amount of money on either side of the bet to minimize their exposure to the risk of more bets being placed on a winning bet than on a losing bet. However, if a betting house were to actively take sides, that is, to allow a significant betting imbalance for a particular strategy (as detailed in Paul and Weinbach [2009]), it would still need to price this strategy in a way so that those using the strategy would lose their bets more frequently than they would win their bets. Our analysis suggests that betting houses are particularly sensitive to potential “hot hand” bias among bettors. If a sizable number of bettors believe in the hot hand — specifically, that betting on teams based on recent performance against the spread is profitable — the profit motive of betting houses will cause them to eliminate any possible profitability associated with this strategy by increasing the betting lines of “hot” teams. Increasing the lines of “hot teams” relative to other strategies leaves the betting houses vulnerable to other possible betting strategies. We support our conclusion with both qualitative and empirical evidence. Qualitatively, we use narrative evidence from a variety of sources to document how previous performance against the betting line is commonly used by bettors to predict current performance. We find strong evidence that links the statistically inefficient pricing found for a few simple strategies to the efficient pricing found for “hot” teams. Empirically, we show how the mispricing of games varies by whether or not they contain “hot” teams. One consequence of the adjustment for the “hot hand” is that it makes other conditional strategies, such as betting on home underdogs, profitable. Both of these pieces of evidence lend credence to our conclusion that counteracting the “hot hand” creates profitable strategies in this betting market. As such, our study expands the market efficiency literature by providing evidence suggesting that market makers intentionally leave some strategies mispriced to account for other strategies, yielding a potentially rational explanation for observed inefficiencies.",11
40,4,Eastern Economic Journal,12 September 2014,https://link.springer.com/article/10.1057/eej.2011.14,Euro Crash: Implications of Monetary Failure in Europe,September 2014,Cathyann D Tully,,,Unknown,Unknown,Unknown,Unknown,,
40,4,Eastern Economic Journal,12 September 2014,https://link.springer.com/article/10.1057/eej.2013.48,The Art and Practice of Economics Research: Lessons from Leading Minds,September 2014,Janet Spitz,,,Female,Unknown,Unknown,Female,,
40,4,Eastern Economic Journal,12 September 2014,https://link.springer.com/article/10.1057/eej.2013.49,The Global Economic Crisis: A Chronology,September 2014,Anil Bolukoglu,,,Male,Unknown,Unknown,Male,,
40,4,Eastern Economic Journal,12 September 2014,https://link.springer.com/article/10.1057/eej.2014.15,Alaska’s Permanent Fund Dividend: Examining its Suitability as a Model,September 2014,Evelyn L Forget,,,Female,Unknown,Unknown,Female,,
40,4,Eastern Economic Journal,12 September 2014,https://link.springer.com/article/10.1057/eej.2014.16,"Jane Austen, Game Theorist",September 2014,Aurelie Charles,,,Female,Unknown,Unknown,Female,,
41,1,Eastern Economic Journal,29 September 2014,https://link.springer.com/article/10.1057/eej.2014.49,Marketing Economic Ideas: The Problem with Capital,January 2015,David Colander,,,Male,Unknown,Unknown,Male,"The US idea mavens, or at least the eastern liberal establishment idea mavens, have been expecting and wanting a discussion of the growing inequality in wealth and income distribution for decades. The economics profession hasn’t provided it — the profession has a strong tendency to simply disregard the issue. Instead, mainstream economists vaguely talk about a marginal productivity theory of distribution that was long ago abandoned as inadequate as a full theory of distribution. The profession has done little to extend the theory, and show how it blends with social and institutional criteria to create the income distribution we have. Instead, the economics profession avoids the distribution issue in much of its theoretical work by assuming that costless lump sum transfers exist that can get us to the income distribution society wants. That implicit assumption allows it to talk only about allocative efficiency. Of course we don’t have costless lump sum transfers. Never have, never will. The need to focus on income distribution, and the problems with economic theory, has been much discussed in economics, but that work has usually acquired the prefix “heterodox,” which within the sociology of the economics profession causes it to be dismissed without serious consideration. What differentiates this book is that it is written by a “reformed insider.” Piketty is a well-connected, technically trained economist who received his Ph.D. at 22, and who formerly taught at MIT. That pedigree and his well-connectedness likely account for the kid gloves used by many top economists as they have reviewed and commented on this book. What do I mean by kid gloves? I mean overfavorable reviews that overlook weaknesses in the book and comments such as “economics has yet to get over its childish passion for mathematics,” which if made by anyone else, would cause them to stop reading and to pan the book. Given this failure of economics to deal with the income distribution issue, there is a natural tendency for reporters and economics observers to look for some entré into the wealth and income distribution issue. So they were primed for any book that raised the question. Piketty’s book, and the broader research program that Piketty is involved with (work with economists such as Tony Atkinson and Emmanuel Saez) does. That overall work is commendable and important; I am a strong fan of it. But this book is just a small part of that research program, and Piketty’s connection to that larger important project does not make this a great book, or justify the rave reviews the book has gotten. The reality is that the book is not a path-breaking look at the problem of income distribution and what to do about increasing inequality appropriate for lay readers. It is an academic book concerned with fine points regarding data collection, blended with some loose discussions of theory and policy. It is not a theoretical or policy contribution to the income distribution debate. Its discussion of theory is problematic and its discussion of policy is, quite frankly, pedestrian. So while a discussion of income and wealth distribution is overdue, this is a strange vehicle to use for that purpose. Thus we must look for some other explanation for the book’s success.",2
41,1,Eastern Economic Journal,10 March 2014,https://link.springer.com/article/10.1057/eej.2014.12,“Walking ATMs”: Do Crime Rates Affect Remittances of Mexican Migrants in the United States?,January 2015,Michael Coon,,,Male,Unknown,Unknown,Male,"Numerous news stories have documented that Mexican immigrants around the United States are being targeted by criminals, to the extent that immigrants have earned the moniker “walking ATMs” [Medina 2007; Haglund 2008; Abel and Amrhein 2009]. This is because of the fact that immigrants, particularly illegal immigrants, are often paid their wages in cash. As many also lack proper documentation, they are usually unable to open bank accounts and access financial services in the United States. As a result, the bulk of their purchases are made with cash, and personal savings are also held in cash. The lure of easy money for criminals is compounded by the fact that immigrants may also be hesitant to contact the police for fear of being questioned over their documentation status. Thus, immigrants may benefit from some way to safeguard their wealth while in the United States. One method of securing their wealth would be to access the US financial system. However, lack of adequate identification documents may prevent this for many immigrants. In recognition of the need for immigrants to have government-issued identification while in the United States, in March 2004 Mexican consulates in the United States began issuing the high-security consular registration card, commonly referred to as the matricula consular, to Mexican citizens in the United States, regardless of their legal immigration status [Congressional Research Service 2005]. Shortly thereafter, several banks began voluntarily accepting the matricula consular as proof of identification for the purpose of opening a bank account. Citing the need to protect immigrants from criminals, among other reasons, the New York State Assembly [2011] introduced a bill that would require all banks to accept the matricula consular as valid identification for all banking transactions. If approved, this legislation would provide Mexican immigrants with potential access to the US financial system. By providing immigrants with a safe place to deposit cash and with non-cash methods of purchasing goods and services, such as checks and/or credit cards, it is likely that there would be significant changes to migrants’ saving and consumption patterns since both would be protected from would-be thieves. This legislation may also affect remittance flows between the United States and Mexico. This paper presents evidence that remittance flows to Mexico respond to differences in the crime rates observed by immigrants in the United States. Thus, any legislation that affects immigrants’ potential losses to crime will also affect their decisions over sending remittances. This study is one of the first to examine the relationship between crime and remittances. Vargas-Silva [2009] examines the effects of crime in the remittance receiving location. This study examines the impact of crime in the remittance-sending location and thus highlights the importance of safety and security on the migrants’ remittance-sending behavior. Using household survey data from the Mexican Migration Project (MMP), I examine the role of crime in the United States in determining the propensity of Mexican migrants to send remittances and the size of the remittances sent. Furthermore, I find that the response to crime rates is not unidirectional, and depends on the type of crime. That is, crimes that are more likely to affect migrants’ savings are positively related to remittances, and crimes that are more likely to affect migrants’ consumption while in the United States are negatively related to remittances. The following section presents a brief discussion of the relevant literature and a theoretical model that describes how migrants’ remittance behavior is affected by crime in the destination location. The third section examines the question empirically, matching survey data pertaining to migrants’ experience in the United States with crime statistics for the migrants’ destination locations and period in which they migrated. This is followed by the results of the empirical tests and robustness checks. The final section concludes and offers some policy implications.",4
41,1,Eastern Economic Journal,19 August 2013,https://link.springer.com/article/10.1057/eej.2013.30,The Effect of Regulated Competition on Pharmaceutical Advertising and Promotion,January 2015,Guy David,Sara Markowitz,,Male,Female,Unknown,Mix,,
41,1,Eastern Economic Journal,09 September 2013,https://link.springer.com/article/10.1057/eej.2013.34,"Spousal Effects in Smoking Cessation: Matching, Learning, or Bargaining?",January 2015,Kerry Anne McGeary,,,,Unknown,Unknown,Mix,,
41,1,Eastern Economic Journal,21 October 2013,https://link.springer.com/article/10.1057/eej.2013.36,Non-Linearities in the Relationship between Aggregate Income and Mortality Rates,January 2015,Fidel Gonzalez,Troy Quast,,Male,Male,Unknown,Male,"How are mortality rates and income per capita related? For decades, academics have found a positive relationship between health and income. Preston [1975] first showed for a cross-section of countries that life expectancy was positively related to income per capita. Updates using more recent data from the World Bank such as Deaton [2004] and Jack and Lewis [2009] find a similar pattern. This relationship, known as the Preston curve, documents a positive association that weakens as the level of income increases. The implication is that for countries with low to medium levels of income per capita, increases in income are associated with large gains in life expectancy. However, these gains decrease at higher levels of income to the point where higher incomes are not related to significant improvements in life expectancy. Studies such as Pritchett and Summers [1993], Filmer and Pritchett [1999], Cutler et al. [2006], Deaton [2006], Soares [2007], and Cutler et al. [2008] have found a negative association between national income per capita and mortality rates. Nevertheless, as noted by some of these authors [Preston 1975; Blakely et al. 2004; Cutler et al. 2006; Deaton 2006; Soares 2007; Georgiadis et al. 2010], the full extent and the channels through which income affects health (including mortality) is still unclear. These studies along with McKeown [1976] suggest that lower mortality rates are explained by improvements in nutrition, diet, public health, medical treatments, and vaccinations rates. Nevertheless, Preston [1975] and McKeown [1976] differ on the importance of each of these elements. McKeown [1976] attributes the majority of the mortality reduction from 1700s to the present to improvements in overall living standards, particularly nutrition and diet. Colgrove [2002] provides a good overview of McKeown’s work. In contrast, Preston and others credit most of the improvement to public health services and health-promoting technologies. Although many of the McKeown’s [1976] conclusions have been refuted, questions remain about the nature of the relationship between income and mortality. For example, Snyder and Evans [2006] find that higher income leads to higher mortality rates on a subset of the elderly population in the United States. In this paper, we analyze the relationship between income per capita and overall and specific causes of mortality. Rather than use a cross-section of countries, we employ a state-level panel data analysis for Mexico for the 1993–2008 period. Our data include annual, age-adjusted overall and specific mortality rates, life expectancy at birth, and real Gross Domestic Product (GDP) per capita for each of the 32 Mexican states. Although our choice of time period is dictated by the availability of data, the size and completeness our data set is remarkable for a developing country. We estimate panel regressions with year and state fixed effects and allow for a non-linear relationship between mortality rates and real GDP per capita. Specifically, we include linear, quadratic, and cubic terms of real GDP per capita. As in Pritchett and Summers [1993], we do not include other explanatory variables because our goal is to estimate the reduced-form relationship between mortality rates and GDP per capita rather than the determinants of mortality.In contrast to Pritchett and Summers [1993], we do not attempt to estimate the causal relationship between mortality and income and consequently do not follow their instrumental variable approach. The inclusion of other variables could capture some of the relationship between mortality rates and GDP per capita and would thus undermine our main goal. Our panel approach takes advantage of the time series aspect of the data to reduce the potential bias from unobservable state and year invariant characteristics or shocks. We contribute to the existing literature in a number of dimensions. In addition to overall mortality, we analyze the relationship between income per capita and disaggregated mortality categories and subcategories. Further, while existing studies of developed countries have utilized similar approaches, this paper is one of the first to implement a fixed-effects panel estimation approach and utilize state-level data. Finally, rather than assuming linearity, we allow for a non-linear relationship between mortality and income per capita and estimate the income values at which the slope of relationship changes sign. We use Mexico for several reasons. First, the wide range of income per capita across Mexican states implies that our findings may potentially also apply to countries at similar levels of income per capita. The income per capita range for the Mexican states comprises the 15th–55th percentile of all countries. This range comprises about 70 countries or 40 percent of the countries included in the International Monetary Fund (IMF) [2010] World Outlook Database. It includes developing countries such as Albania to developed countries with lower incomes such as Cyprus and Greece. Second, Mexico has high-quality annual mortality and GDP data at the state level for a considerable period of time. The quality and quantity of these data allow us to employ the panel econometric approach described above. We do not observe a statistically significant relationship between overall mortality and income per capita, but we do find significant relationships for certain mortality categories and subcategories. Our results suggest that the lack of statistical significance at the overall level may be due to offsetting relationships across mortality categories. We find that the relationship between income per capita and mortality rates differs from what would be expected from the Preston curve for countries with similar incomes per capita. These differences could be due to the use of mortality rates rather than life expectancy, the fixed-effects panel approach, or state-level data for one country. Further, it is difficult to directly compare our results with studies such as Snyder and Evans [2006], Ferreira and Schady [2009], and Baird et al. [2011]. Many of these papers utilize mortality rates for infants or elderly adults and employ data for different countries and time periods. Given the aggregate nature of our data and lack of statistical significance, we are unable to provide a definitive explanation of the relationship between the overall mortality rate and income per capita. While the use of individual-level data could provide some insight, they are not available for Mexico. Moreover, caution is warranted when making inferences regarding individuals using results from aggregate data because of the “ecological fallacy” problem.The ecological fallacy consists in assuming that the relationships observed for a population necessarily hold for individuals. For example, while higher income can improve public sanitation or public health facilities, it can also lead to increase in risky individual behavior (such as smoking, alcohol consumption) that results in higher mortality. Nevertheless, regardless of whether the relationships we observe are present at the individual level, our results may provide important insights into the aggregate relationships. Finally, given the differences in health-care systems around the world, our results may not extend to other countries with similar levels of income per capita. Our paper is structured as follows. The next section contains background information on Mexico. The third section describes the data and econometric specification employed in our analysis. The fourth section shows and discusses the results, while the last section presents potential explanations for our results and concludes.",
41,1,Eastern Economic Journal,09 September 2013,https://link.springer.com/article/10.1057/eej.2013.37,"Older Americans, Depression, and Labor Market Outcomes",January 2015,Brandon C Koford,Attila Cseh,,Male,Male,Unknown,Male,"The literature analyzing the relationship between various mental disorders and labor market outcomes has grown in recent years. Mostly, these studies find a negative relationship between various labor market outcomes and mental health. As these studies use surveys that primarily include young individuals [Cseh 2008], prime-age individuals [Bartel and Taubman 1986; Ettner et al. 1997], or mixed age groups [Frank and Gertler 1991; Baldwin and Marcus 2007], what we have learned from these studies may not hold for those who are still active in the labor market but closer to retirement age. This study fills this gap by specifically focusing on the impact of depressive symptoms on labor market outcomes for baby boomers. It is especially important to analyze the relationship between labor market outcomes and mental health for this group for two reasons. First, even though some other age groups show a higher rate of depression, a notable recent change in the epidemiology of mental illness indicates that 45–64 year olds have the highest risk for lifetime depression. This is a change compared with findings based on surveys from the 80s and 90s when highest life-time prevalence occurred for young adults [Hasin et al. 2005). This suggests that the current baby boomers, as a cohort, had higher life-time prevalence when they were young and they are still the highest risk group for ever having depression now that they are older. Second, previous studies for this particular age group mostly focus on the relationship between employment/retirement and mental health, not earnings [Midanik et al. 1995; Wray 2003; Tefft 2007; Mandal and Roe 2008]. Our study contributes to the understanding of how depressive symptoms may be correlated with poorer labor market outcomes for an age-group whose members are expected to retire soon. The retirement of baby boomers is expected to reshape the labor market; however, lower earnings may delay their retirement decision, which will impact younger workers waiting to replace the baby boomers. If depressive symptoms are an important cause of lower earnings, medical treatment could help the mentally ill in two ways. One is through improving the quality of life (symptomless living). The second is through improving earnings potential that could contribute to the retirement decision. If, on the other hand, depressive symptoms are not causal, this second channel of help may not exist. Identifying the direction of causality has proven to be difficult because of the complicated relationship between mental health and labor market outcomes. On one hand, poor labor market conditions may lead to poor mental health. On the other hand, working may provide insurance coverage, possibly provided by the employer, which in turn can facilitate getting professional help in the time of need and thus improve mental health status. If causality flows the other direction, mental health issues may lead to poor labor market outcomes. For example, mental health can be debilitating, hindering the individual’s productivity and job performance. Another complicating factor is the relationship between labor market outcomes and personality. Specifically, research in psychology has showed that individuals with certain personality characteristics are more prone to depression than others lacking these characteristics [Clark et al. 1994; Watson and Clark 1995; Clark et al. 2003]. On the other hand, the management literature has pointed out that certain personality characteristics are also associated with labor market success [Tett et al. 1991; Barrick and Mount 1991; Goodstein and Lanyon 1999; Bono and Judge 2003]. Therefore, it could be that personality characteristics (which are often unobserved for the researcher analyzing survey data) are at least partly responsible for the negative correlation between labor market outcomes and depression. In this study, we apply a familiar econometric technique in a novel context to estimate how depressive symptoms affect labor market outcomes. In particular, we take advantage of the longitudinal nature of Health and Retirement Study (HRS) to analyze whether unobserved personal characteristics could be the underlying reason for the negative correlation between depressive symptoms and wages. We use panel data techniques to take time-invariant personal characteristics, such as personality, into account in estimating the magnitude of this impact. As most of the previous studies either fail to take into account the impact of underlying personality or inadequately control for it, this paper fills in an important gap in the literature. Early studies by Bartel and Taubman [1979, 1986] mostly assume mental health to be exogenous and estimate the impact by including a mental health measure (for psychoses and neuroses) into a simple wage equation. Not surprisingly, they find very strong (between 12–47 percent) reduction in wages for people with these mental disorders. These authors also rely on utilization-based measures of mental distress that (aside from the potential endogeneity) could further bias the results as only a subgroup of people with mental illness selects to seek medical help. Frank and Gertler [1991] compare population-based measures of mental illness generated from survey responses with utilization-based measures and find that utilization-based measures may not be subject to a large bias after all; however, their precision is substantially lower than that of population-based measures. In a more recent study, Baldwin and Marcus [2007] use the Medical Expenditure Panel Survey to analyze whether the labor market outcomes associated with mental illness could be due to discrimination. Using an Oaxaca-type decomposition, they find that most of the decrease in predicted wages can be explained by differences in other explanatory variables besides mental illness. In another recent study, Jofre-Bonet et al. [2005] use the Community Tracking Survey to analyze the impact of poor mental health and smoking. In their most basic specification, they find that poor mental health reduces wages for both males and females, and by interacting poor mental health and smoking they also find that smoking intensifies this negative effect. Using one wave and three waves of the HRS, respectively, Tian et al. [2005] and Emptage et al. [2005] show the importance of comorbidity. They find that depression that co-occurs with mild, moderate, or severe pain is associated with lower household income and worse employment prospects than depression alone. While these studies investigate causes of wage reductions that potentially occur with depressive symptoms, they do not control for personality traits, nor do they analyze the bias due to reverse causality. Recognizing the potential endogeneity of the mental health variable, Ettner et al. [1997] and Marcotte et al. [2000] use the National Comorbidity Survey and the instrumental variable method to identify the effects on earnings. The study by Ettner, Frank, and Kessler uses psychiatric disorders of the parents and the respondents’ own past disorders as instruments in the first stage. The results suggest that any diagnosed disorder reduces employment by 14 percent and annual income by 28 percent for females. For males, a diagnosed disorder reduces employment by 12 percent and wages by a statistically insignificant 9.5 percent. The study by Marcotte, Wilcox-Gök, and Redmon also uses the parent’s history of affective disorders as the instrument and finds that the three disorders analyzed (affective disorders, major depression, and dysthymia) do not significantly impact wages of males; however, they reduce females’ earnings by about 32 percent. One disadvantage of using these instruments is that they could be also correlated with personality characteristics that have been found important in work performance, thus weakening the effectiveness of the instrument. Another line of the literature takes advantage of longitudinal survey designs. Luo et al. [2010] use multiple waves of the National Epidemiological Survey on Alcohol and Related Conditions to analyze the effect of a more focused disorder group, Major Depressive Disorders (MDDs), on labor force status. Using information before and after Wave I and before Wave II, they categorize changes in MDD episodes into the following categories: incidence, recent remission, persistent remission, relapse, and persistent depression. They find that for males incidence, relapse, and persistent depression all increase the probability of being out of the labor force. Recent remission, incidence, and persistent depression increase the probability of being unemployed, and given being employed recent remission, incidence, and relapse increase the probability of working part-time only. For females, only persistent depression is found to significantly increase the odds of being unemployed, and incidence significantly increases the odds of working part-time given being employed. The argument made by Luo and colleagues. implies that reverse causality is averted because their “empirical strategy relies on capturing transitions in depression status before measuring labor market outcomes” [144]. The authors also recognize the potential bias caused by unobserved personality traits, but they find that adding indicators for certain personality characteristics (such as lifetime antisocial, avoidant, dependent, obsessive-compulsive, paranoid, schizoid, histrionic, borderline, schizotypal, and narcissistic personality disorders) do not substantively change the main results. Our study is closest in approach to Cseh [2008], who also uses a longitudinal data set, the National Longitudinal Survey of Youth 1979, to estimate the impact of depressive symptoms on wages for a group of younger adults and finds some evidence that in case of young workers, the lack of control for personal characteristics may negatively bias the estimates of mental health on wages. Our contribution to the literature is to specifically focus on analyzing the impact of depressive symptoms on labor market outcomes for those right before retirement, a group for whom this question has not been answered. We find that the ordinary least squares (OLS) estimates, though strong and significantly negative for earnings, may be severely biased by not controlling for personality characteristics. Once personality is taken into account, point estimates of depressive symptoms are considerably smaller. The remainder of the paper proceeds as follows: In the next section, we describe the model and the applied methods. We then introduce the data we use for our analysis followed by the results. In the last section, we provide some concluding remarks.",4
41,1,Eastern Economic Journal,09 December 2013,https://link.springer.com/article/10.1057/eej.2013.41,Testing for Ethnicity Discrimination among NHL Referees: A Duration Model Approach,January 2015,Kevin Mongeon,Neil Longley,,Male,Male,Unknown,Male,"Economists studying labor market discrimination have often used the professional sport industry as an empirical focus. Historically, this interest was largely rooted in the fact that the sport industry provides a unique data-generating process of multiple identical, or at least similar, experiments that provide a data set not generally found in other industries. This has allowed researchers to control for a variety of confounding factors that could potentially contaminate the estimates and hypothesis tests. The body of literature in the area is now vast, with much of the research focusing on African Americans, and examining the extent to which they may suffer from discriminatory treatment in sports like baseball, basketball, and football.Footnote 1 Traditionally, the literature has viewed discrimination against a player as emanating from three possible sources — from the player’s employer (owner, managers, coaches), from his coworkers (teammates), or from consumers (fans). In turn, it has been presumed that if discrimination were to exist, it would manifest itself in one of three possible forms: through salary differences, through entry barriers, and/or through positional segregation. More recently, however, a new trend has emerged in the literature that considers an additional potential source of discrimination, that being the referees/umpires that adjudicate the on-field play in a sport. Price and Wolfers [2010] note that “referees and players are involved in repeated interactions in a high-pressure setting, with referees making split-second decisions that might allow implicit racial biases to become evident” (p. 1859). In their analysis, they find evidence of racial discrimination in the foul calling of National Basketball Association referees at both the game-team and game-player levels. At the game-team level, they find that the foul differential of the two teams competing in the contest increases with the racial dissimilarity of the two teams, relative to the racial makeup of the refereeing crew. At the game-player level, they examine player foul rates to find that the marginal increase in the fraction of opposite-race referees on a player’s foul rate per 48 min is approximately 0.197. In baseball, Parsons et al. [2011] study Major League Baseball to find that pitches are 0.59 percentage points more likely to be called strikes when the umpire and pitcher are of the same race, although the effect only exists in ballparks without computerized systems monitoring umpires’ calls, and at poorly attended games.Footnote 2 Our paper extends this literature on referee discrimination to the sport of hockey, where it is ethnicity, rather than race, which is the issue. The National Hockey League (NHL) has the most ethnically diverse labor market of the four major professional team sports in North America. While at one time almost 100 percent of the players in the league were Canadian, the last four decades have seen the league become increasingly internationalized, as significant numbers of European and American players have entered the league. In the current era, Canadians comprise only about 50 percent of NHL players, with approximately 30 percent coming from Europe, and the remaining 20 percent from the United States. However, there has also always been heterogeneity within the Canadian group itself — while the majority of Canadian players have been Anglophones, a significant minority have been of French descent, with almost all of these players being from the province of Quebec. These Francophone players are distinctly different from the Anglophone majority, not only in terms of first-language, but also along social, political, and cultural lines. With French Canadians being the NHL’s original minority group, most of the discrimination research in hockey has focused on that group, with both entry and salary discrimination being examined. Early research [see, e.g., Lavoie et al. 1987; Jones and Walsh 1988; Lavoie and Grenier 1992; Walsh 1992] tended to use methodologies that aggregated all French Canadians in the league into a single group, regardless of where they played. Longley [1995], however, argued that such an approach ignores the potentially important effects of team location — he found that French Canadians playing for teams in English Canada appeared to suffer from significant salary discrimination, but found no evidence that French Canadians playing for either US- or Quebec-based teams suffered from salary discrimination. He hypothesized that these findings may be a manifestation of the historic social and political tensions between Anglophones and Francophones in Canada. Subsequent research [see, e.g., Jones et al. 1999; Lavoie 2003, 2012; Longley 2000, 2003; Curme and Daugherty 2004] has recognized the potential importance of team location, with the results being mixed across these papers as to the presence of discrimination. This issue of potential discrimination against French Canadian NHL players has implications beyond just hockey, and must be viewed within the broader social and political context of French–English relations throughout Canada’s history. While it is not the purpose of this paper to examine this history, the issues can be concisely summarized as revolving around the protection of minority rights, with Francophones attempting to protect their culture and language from domination by the English-speaking majority in the country. Politically, this dissent has manifested itself during the past 40 years with the election of several provincial governments in Francophone-dominated Quebec that have openly advocated that province’s separation from the rest of Canada.Footnote 3 Recent anecdotal evidence that social tensions still exist was demonstrated in a January, 2012 protest by French Canadians that called for the dismissal of newly appointed Montreal Canadiens’ coach Randy Cunneyworth, a unilingual Anglophone (http://www.cbc.ca/news/canada/montreal/montreal-en-francais-protesters-chant-at-habs-game-1.1296502). Within this context, this paper tests whether or not professional hockey referees exhibit ethnic discrimination in their penalty calling decisions. The hockey penalty data used in this paper is generated in a similar manner to the data generated in basketball, with referees making decisions to either call an infraction or to allow the play to continue by not calling a foul, with the latter not being documented by data and, for analysis purposes, being unobservable.Footnote 4 This underlying data-generating process is important because if endogenous factors exist that contribute to the supply of infractions produced by players, then data aggregation beyond these factors can act as a source of contamination, thus contributing to potentially inconsistent estimates and hypothesis tests. In this regard, a critical assertion underlying our approach is that there can be an endogenous relationship between score margin, penalties called, and the probability of winning. For example, when a team is called for a penalty in hockey, the opponent is awarded a man-advantage for 2 min (called a power-play), which increases the probability of an opponent’s goal, and correspondingly increases the probability of the opponent winning. This can confound the analysis because score margin can alter the cost of an opponent’s goal, and therefore the cost of a team penalty, creating an environment in which the team that is leading (trailing) in a game has less (greater) incentive to commit infractions. To illustrate the point, Columns 2 and 3 of Table 1 present the 2008–2010 in-game score-margin-specific unconditional probabilities of a home team win, and the changes in the probability of a home team win given a visiting team goal, respectively. For example, when leading by two goals or trailing by two goals, the probability of a home team win is 0.909 and 0.136, respectively. From each of these score margin states, a subsequent visiting team goal results in a change in the score margin to either the home team leading by one goal or trailing by three goals, respectively, changing the probability of the home team winning to 0.724 and 0.043, respectively. Thus, the change in the probability of a home team win, given a visiting team goal, when leading or trailing by two goals is −0.185 (=0.724–0.909) and −0.092 (=0.043–0.136), respectively, which is a difference in probability of −0.093 points. Similar calculations can be made for the other score margin states and support the conclusion that there can exist asymmetric costs of opposing team goals, which therefore can affect the underlying supply of infractions and own-team penalties.Footnote 5 In light of these various factors that can affect the penalty-calling decisions of referees, our empirical methodology analyzes the data at the “shift” level, the smallest unit of observation possible in hockey. Our data includes information on the score margin at the time of the penalty, along with a host of additional shift-level factors that can potentially contribute to discretionary penalty (henceforth called penalty) events. A player’s shift occurs each time he enters and exits the ice during the game. The game of hockey is characterized by rapid play and constant flow, with players entering and exiting the ice often, in exchange for other, more well-rested, players. In some instances, there is a stoppage in play that allows multiple players to simultaneously enter and exit the ice together in exchange for a new group of players. Many times during a game, a player enters and exits the ice during the play, called “changing on the fly.” In some instances a player’s shift is ended when they are called for a penalty event, and therefore the ending of some player shifts are censored by penalty events. Our empirical methodology accounts for this censoring of player shift by analyzing the player-shift data using duration models, which estimates the rate at which penalties are called for each player–referee ethnicity combination. Similar to the data used by Price and Wolfers [2010], our data only identifies the referees who officiated the game, and not the identity of the specific referee that called the foul/penalty. Different than the Price and Wolfers [2010] methodology that uses a continuous model to represent the fraction of opposite-race refereeing crew relative to the player, we specifically identify each of the various ethnic-specific combinations of refereeing crews. In doing so, rather than identifying whether or not discrimination exists among opposite referee–player races/ethnicities, we identify the specific referee ethnicity that is discriminating, along with the player ethnicity they are discriminating against, which has obvious benefits for making policy assessments.Footnote 6 The remainder of the paper is organized as follows: The next section discusses the data, presents the empirical models, and discusses the test results relating to ethnicity discrimination in penalty calling. The final section discusses various possible inferences that can be ascertained from the test results and provides some concluding remarks.",4
41,1,Eastern Economic Journal,23 December 2013,https://link.springer.com/article/10.1057/eej.2013.44,Foreign Aid and the Culture of Contracting,January 2015,Christopher J Coyne,Claudia R Williamson,,Male,Female,Unknown,Mix,,
41,1,Eastern Economic Journal,24 February 2014,https://link.springer.com/article/10.1057/eej.2014.2,Water Cooler Ostracism: Social Exclusion as a Punishment Mechanism,January 2015,Brent J Davis,David B Johnson,,Male,Male,Unknown,Male,"The financial news media often highlights the importance of a “happy workplace” [Gostick and Elton 2012]. Happy workplaces may be constructed through the encouragement of social activities outside the workplace. Many firms encourage happy hours, softball teams, and other social interactions in hopes that these events will foster a team mentality [Paton 2005]. The lengths employers will go to secure a happy workplace is often surprising. CNN reports that Seagate Technology spent over 3 million dollars on a single team-building exercise [O’Brien 2008]. Although this example is extreme (and rare), a more relatable illustration occurs nearly every year in the form of office Christmas parties. From the manager’s perspective, the reasoning behind scheduling these events is straightforward — replacing unhappy workers is expensive, costing the company as much as 20 percent of the employees’ yearly earnings [Lucas 2012]. If turnover can be prevented, the company can save money. Making employees happier (while saving money) is a worthy endeavor. However, doing so by encouraging social interactions introduces the possibility of other headaches (e.g., social punishments) that can negatively impact workplace productivity. Our paper investigates one of these potential headaches: social ostracism. We do so using a laboratory experiment. Although previous studies examine different forms of non-monetary punishment, none to our knowledge examine the role potential social ostracism (with monetary incentives) plays as a factor that can influence the provision of a public good. This study adds to the growing literature investigating non-monetary punishments. On one hand, social interactions allow for a variety of non-pecuniary punishments, but on the other also introduce the possibility of counter punishments (e.g., name-calling and loafing — idling time away.). As such, it may be reasonable to conjecture that the introduction of decentralized non-pecuniary punishments, which are inherent within social interactions, may lead to efficiency losses. Alternatively, team-building exercises may prevent some of the negatives associated with socialization by constructing a group identity, and thus enhance the effectiveness of some social punishments. The outside world is rife with examples where there is potential for social ostracism. For example, public goods are often produced in circumstances where there is an absence of a manager with the authority to punish free-riders. Likewise, the group may not have the authority to remove free-riders from the activity generating the public good (i.e., place of employment or athletic team). Nevertheless, the group possesses the option of punishing the free-rider with various forms of non-pecuniary punishment, with ostracism from the group’s social activities being one form. In the case of social ostracism, the ostracized group member continues to participate in the activity generating the public good but may be removed from the social aspects of the group (e.g., ignored at the water cooler). Furthermore, social ostracism may naturally occur through the endogenous formation of cliques that social loafers may be excluded from. Social ostracism is of special interest as it potentially creates a sense of alienation [Thompson 2011] and is a form of punishment that most people have experience with, being commonly exercised (and suffered through) during childhood [Masters 1986; Owens et al. 2000; Dixon 2007]. A growing literature examines punishment and social approval mechanisms within public good experiments. Although there are exceptions, economists have yet to study ostracism in great detail. Yet psychology has already pointed to negative emotional feelings from ostracism,Footnote 1 and also examines ostracism in naturally occurring instances [Boehm 1999; Williams et al. 2000; Wiessner 2005].Footnote 2 Clearly, multiple behaviors may result from ostracism. A socially ostracized individual may reduce their contribution in comparison to their prior contribution as a form of counter punishment [Nikiforakis 2008]. Alternatively, an individual may increase their contributions to regain the group’s approval and thereby be re-admitted to the group’s social activities. We extend this line of research using an economic experiment. We place subjects in production teams that have a social component. We then provide them a mechanism by which they can be removed from the social component. A key difference between this study and earlier economic studies of ostracism is that subjects always remain in the production aspect of the group but can be ostracized from a social component of the group — this is the main contribution of this paper. We find that subjects participating in a voluntary contribution mechanism (VCM) with possible ostracism vote to ostracize cohorts who contribute little to a public account. In addition, potential social ostracism does not increase contributions to a public account but rather attenuates selfish behavior in the second stage. We also find that subjects who are more active participants in the chat stage are less likely to be punished for low contributions. Previous studies have shown that formal ostracism and full removal from groups can improve cooperation and sustain public good levels. When ostracism becomes solely social we find that the effect is weaker. Our work suggests that social interactions that introduce possible social punishments may indirectly hurt production as those who are punished may further reduce their individual production. Yet employers may observe production gains if voluntary social interactions are encouraged after mandatory social interactions. Our results provide evidence that “happy workplace” initiatives might not only increase morale but also increase the effectiveness of certain social punishments.Footnote 3",10
41,1,Eastern Economic Journal,12 December 2014,https://link.springer.com/article/10.1057/eej.2014.17,Economics of Bankruptcy,January 2015,Erin Crockett,,,,Unknown,Unknown,Mix,,
41,2,Eastern Economic Journal,19 January 2015,https://link.springer.com/article/10.1057/eej.2014.78,Intellectual Incest on the Charles: Why Economists are a little bit off,March 2015,David Colander,,,Male,Unknown,Unknown,Male,"I could go on, but my goal in this paper is not to argue that modern mainstream economics is just a little bit off, but rather to tentatively accept that it is, and offer a reason why. That reason is intellectual incest. Modern mainstream economics is a bit off as a result of too much inbreeding. Specifically, my argument is that the gene pool of economists in the replicator dynamics of the profession is too small to prevent undesirable recessive traits from showing up in mainstream economists from time to time. Let me start by considering how big the economic intellectual gene pool is. Each year about 25 newly minted Ph.D. economists have a shot at making the top echelons of the economics profession. There are probably another 50 or so graduates who have a small shot at making it. So the intellectual genealogy of these 75 economists a year is central to understanding the profession, and its foibles. That elite group determines the future direction of the profession, just as the monarchies of old did. These elite are the super peers; they set the agenda, and provide the role models for the others. The problem is, like monarchies of old, there’s a lot of inbreeding that has reduced the intellectual gene pool of this group to a size that cannot avoid serious inbreeding. A diverse intellectual gene pool would have those graduates coming from different traditions so that undesirable recessive traits of any one tradition are less likely to be expressed in the offspring. So the reason economists are a bit off, and go off on DSGE fads, and too often seem to not reflect an educated common sense, is that the intellectual reproduction of this group does not have enough diversity. Intellectual inbreeding allows recessive traits and tendencies, which in the original breeding stock were not all that harmful, to become dominant. That’s what happened with DSGE macro. It reflects a recessive trait in its daddy, Robert Solow. DSGE macroeconomics (which includes just about all mainstream modern macro) is a mutant Solow strain, attributable to one of his recessive traits. That trait was a strong, dare I say excessive, aversion to informal models, which resulted in an almost visceral reaction to informal presentations of models, and a similarly strong preference for small semi-rigorous models that fit the real world. When those traits are bred with an Arrow–Debreu, Samuelson trait expressing a strong preference for highly rigorous formal general equilibrium models, the result is modern DSGE macro. With their other almost off-the-chart abilities, and with their almost unbounded educated common sense, Solow and Samuelson could compensate for their recessive traits, thereby coming up with useful models and policy insights. (Debreu, similarly, could recognize the limitations of his model and know enough to not relate his work directly to real world problems.) Their judgment allowed them to limit the applicability of the models. For example, Solow, writing about how he used his growth model and why he objects to how modern economists are using it, stated: “I restricted the applicability of the model to tranquil trajectories without stormy intervals … [and] I immediately warned the reader of the possibility of aggregative short to medium run imbalances that would not fit in to the model.” Modern macro followed Solow on the focus on formal modeling, but not on the judgment about use of models. The result was modern macro. So whereas Solow’s judgment gene compensated for his recessive anti-informal modeling gene, and made him into an eminently reasonable economist, it did not in his intellectual offspring. Solow is the daddy of modern macro, and he has the good judgment to disown it. The Piketty problem shows a different result of the intellectual incest. It reflects top economist’s differential treatment of their offshoots. If one of their own (Piketty is trained in an MIT tradition, and taught at MIT for 2 years) raises a policy issue they believe should be raised, they pull their punches when assessing it, and convey a sense that the contribution is larger than it is. For example, had a heterodox economist, or other outsider, written that “economics has yet to get over its childish passion for mathematics,” as Piketty did, the elite would have stopped reading and paned the book. With Piketty, they overlooked it and numerous similar gratuitous statements.",10
41,2,Eastern Economic Journal,24 February 2014,https://link.springer.com/article/10.1057/eej.2014.3,"Bargaining Power, Labor Market Institutions, and the Great Moderation",March 2015,Aaron Pacitti,,,Male,Unknown,Unknown,Male,"Between 1984 and 2007 — the period known as the “Great Moderation” [Bernanke 2004] — inflation rates remained low and stable. This improvement in macroeconomic performance is traditionally attributed to three causes: structural changes in the economy, such as improvements in technology and communications, improved financial market efficiency,Footnote 1 and market deregulation; improved monetary policy, whereby stabilization policy became more efficient and caused an inward shift of the Taylor curve [John Taylor 1998];Footnote 2 and a reduction in the frequency and severity of adverse supply shocks. Bernanke [2004] suggested that improved monetary policy was the main factor in reducing inflation rates, while Stock and Watson [2003]; Cogley and Sargent [2003], and Ahmed et al. [2002] argued that the main cause was less severe supply shocks. However, the inflationary effects of labor market institutions — the laws and conventions that determine the type of employment available and the relationship between firms and their employees — and alternative measures to the unemployment rate of labor’s bargaining power are ignored in these analyses. This paper argues that rising costs of job loss — a more comprehensive measure of labor’s bargaining power — and shifting labor market institutions have significantly affected inflationary outcomes during the Great Moderation, and, more recently, the Great Recession. The first institutional shift is rising employment insecurity, which reduces workers’ ability to bargain for higher wages. The second institution is that of the social bargain [Cornwall 1990; 1994], which can be thought of as the amount of cooperation in labor-management relations, and can reduce workers’ willingness to demand higher wages if they receive non-wage compensation, such as pensions. This paper builds on, refines, and extends the analyses of Matthews and Kandilov [2002], who used the cost of job loss in a Phillips curve model to examine the inflationary process during the new economy, and Setterfield and Lovejoy [2006], who argued that the short-run tradeoff between inflation and unemployment can be improved through rising insecurity or a greater social bargain. The current study uses annual data for the 1960–2010 sample and finds robust empirical evidence that the low inflation achieved during the Great Moderation had severe distributional biases and was partly the result of institutional shifts that depressed workers’ bargaining power, thereby reducing their ability to demand and secure wage increases. Once employment insecurity and the social bargain are appropriately proxied by strikes and the part-time share of employment, and the share of private employees covered by an employer-provided pension, respectively, they always improve the in-sample fit of the models, are insensitive to alternative specifications of the Phillips curve, and improve forecasting accuracy. These results imply that full employment is not incompatible with price stability, as labor market institutions can mitigate inflationary pressures even at low levels of unemployment. However, the results also suggest that normative economic policy should be fully considered due to the distributional biases of the outcomes.",5
41,2,Eastern Economic Journal,24 February 2014,https://link.springer.com/article/10.1057/eej.2014.6,Variation in Job Creation and Destruction across the States Through Boom and Bust: Could Minimum Wage Matter?,March 2015,Suzanne H Clain,,,Female,Unknown,Unknown,Female,"The topic of economic growth has long fascinated economists. Acquiring an understanding of the determinants of economic growth has been a goal of both theoretical and empirical researchers. These researchers have focused on long-term growth as well as short-run fluctuations in growth; growth in terms of output, income, and employment; and growth for firms, industries, countries, and regions of countries. An understanding of the determinants of such growth has often been touted as vital for policymakers and business leaders. If economic growth were a primary objective, then actions aimed at promoting factors positively associated with growth would be priorities, if the benefits outweighed the costs. Sensitivity to the issue is particularly heightened, when economic downturns occur and growth stalls or is unexpectedly slow. This study adds to the body of research that has explored the variation in growth among the 50 US states. It contributes to previous efforts by investigating the determinants of both job creation and job destruction, inasmuch as these are the building blocks of employment growth. By including minimum wage as one of the possible factors affecting job creation and destruction, the study contributes to the body of research on minimum wage legislation as well. For the sake of timeliness, the focus of the study is the pattern of experiences within the past decade.",
41,2,Eastern Economic Journal,24 February 2014,https://link.springer.com/article/10.1057/eej.2014.8,The Economics of Community Gardening,March 2015,Amelia Garrett,Michael A Leeds,,Female,Male,Unknown,Mix,,
41,2,Eastern Economic Journal,24 February 2014,https://link.springer.com/article/10.1057/eej.2014.9,Entry Discrimination in the NHL: Evolution and the KHL Effect,March 2015,Tom Christie,Marc Lavoie,,Male,Male,Unknown,Male,"While salary discrimination in the National Hockey League (NHL) and in other major sports leagues has been studied extensively over the last decades, the same cannot be said of entry discrimination, which has given rise to less interest. This is obvious from past surveys of discrimination in team sports [Kahn 2000; Longley 2006]. Indeed, whereas entry discrimination in baseball and then in basketball did attract some attention, the topic has probably been studied more systematically in the case of ice hockey [Leadley and Zygmont 2005]. The topic has been resurrected, at least as far as the NHL is concerned, by a recent book, called Discrimination in the NHL: Quebec Players Sidelined, published in 2010 by Bob Sirois, a former NHL player. Sirois argues that it is much more difficult for French Canadian hockey players to make it to the NHL than it is for English Canadians, and that indeed American and European players have faced similar barriers to entry. The book was given a considerable amount of attention, because Sirois, besides being a former player, is also the former owner of a franchise in the Quebec Major Junior Hockey League (QMJHL) and a former player agent, and thus was well known in hockey circles. Also, players or former players are usually prudent about such touchy subjects, and hence avoid being dragged into such controversial discussions. Sirois almost entirely escaped presenting anecdotal evidence, preferring to offer a series of intriguing statistics that compared Francophone and Anglophone Quebeckers, under the assumption that players coming from the same province ought to have the same exposure to NHL scouts and the same hockey training. From these, Sirois [2010] concluded that French Canadians had suffered from entry discrimination, at the time of the draft and when final rosters are decided. Indeed, it has been shown in econometric studies that go back to the 1980s and the early 1990s that French Canadian ice hockey players are underrated in terms of their draft ranking. The results achieved when testing entry discrimination all pointed in the same direction: performance differentials in favor of French Canadian forwards or defensemen are consistently observed. Similarly, European players of the early 1990s also turned out to be underrated in terms of their draft ranking [Lavoie 2003], and hence it is interesting to find out whether Europeans still appear to be underrated 20 years or so later. Lavoie [1989] has argued that discrimination was more likely when there was more uncertainty around the assessment of performance or future performance. This is also the point of view of Sirois [2010, p. 30], who argues that: “Talent scouting is much more a question of intuition, feeling or even gut instinct. Myths, prejudices, stereotypes and favouritism are an integral part of each National Hockey League draft.” Some of Sirois’s statistics, however, revealed an evolution in the trends that he had observed since the 1970s, and hence one may wonder whether conditions have changed over the last two decades, thus justifying a new study based on more recent data. First, one could argue that the growing financial implications of making good choices and finding low-cost quality players, so as to remain below the payroll cap that exists for each NHL team, should induce NHL management to pursue complete, unbiased evaluation of prospective talent. It could also be argued that new technology must have modified the scouting process over the last 10 or 15 years [Kimelman 2009]. Advances in portable video and the internet make the relay of highlight reels simpler, faster, and less expensive. Players have a greater percentage of their developmental years that is being filmed and a larger number of their years are statistically recorded. This provides a massive, easily accessible, source of information that should help making the scouting process more efficient and transparent, thus reducing subjective appraisals and the role of prejudices (based on mother tongue, for instance). These advances in technology should also help to improve scouting in far-away regions, such as in Europe. Thus, one should observe an improvement in the way in which European players are being drafted, since the latter have also been shown in the past to suffer from a form of entry discrimination. However, another recent phenomenon may make NHL teams more reluctant to draft European players, more specifically Russian players. The 2004–2005 NHL lockout (as well as the 2012–2013 one) gave a boost to European hockey leagues, as several NHL hockey players moved there for a season or part of the season. In particular, the KHL, or Kontinental Hockey League, which replaced the former Russian Superleague, is now regarded as the top European hockey league. The league has 28 teams, situated in seven countries, most of the teams however being located in Russia or in the countries that used to be components of the former Soviet Union. The KHL pays nearly competitive salaries given its less heavy playing schedule. For instance, for the 2013–2014 season, while the average payroll in the NHL is approximately $57 million, it is $17.3 million in the KHL.Footnote 1 However, taking into account that the players get paid to play only 52 regular-season games in the KHL, versus an 82-game season in the NHL, one could argue that this is equivalent to a $27.2 million payroll. Thus, while salaries are approximately three times lower in the KHL, they are only approximately two times lower per regular-season game. Looking only at the salary of the seven top players, KHL stars also earn twice less than NHL stars. What is clear however is that salaries in the KHL far exceed those paid by the NHL farm clubs that operate in the American Hockey League (AHL).Footnote 2 Already the likes of Marcel Hossa, Alexei Yashin, Alexander Radulov, and more recently Ilya Kovalchuk and Alexander Burmistrov have moved back to the KHL, and there have been whispers of more imported Russian talent leaving the NHL and returning to their homeland. About two-thirds of the KHL players are Russians. The fear that drafted Russian players may decide never to try out the NHL is likely to decrease their draft value and induce general managers to pick good Russian juniors at a much later draft rank, thus giving rise to apparent discrimination against them — something that could be called rational entry discrimination. We will call this the “KHL effect.” The paper is structured as follows. First, there is a description of how the current data set was constructed and an outline of the methodology used to test for the presence of entry discrimination in the 2009–2010 data; second, we recall past studies that relate the scoring performance of French Canadians relative to English Canadians for a given draft rank, and we present new results using the same method; third, we discuss entry discrimination in a broader sense, examining the situation of American and European players, eventually making use of a separate Russian player category. In the appendix, an ultimate set of regressions is proposed to examine the implications of using career games played as some measure of performance.",4
41,2,Eastern Economic Journal,29 September 2014,https://link.springer.com/article/10.1057/eej.2014.13,Recent Fracturing in the US Economy and Society,March 2015,Frederic L Pryor,,,Male,Unknown,Unknown,Male,"In a fractured economy the highest-income group pulls away from the rest of the population and becomes more distinct along many dimensions, for instance, income, education, health, housing, or values. Although both fracturing and inequality must be taken into account in social social and economic analysis, fracturing is different from overall inequality, where the focus is on the entire income distribution. For instance, a society can become more unequal and poverty can increase if there is a fall in income in poorest 10 percent, even though fracturing is barely affected.Footnote 1 Although fracturing often occurs when inequality increases, it is important to consider it separately because it concentrates attention on the elite, a focus with important implication for policy decisions and the type of leadership in the society and economy since, as shown below, elite values are different from the rest of the population. In the discussion below, I focus in most cases on differences between the top 10 percent of income earners and the remaining 90 percent of the population. Is fracturing in the US worth worrying about? Between 1983 and 2005 fracturing did not seem a problem for most, at least regarding income: for instance, average Americans were increasingly optimistic about their chances for upward economic mobility. When asked whether it is possible to start poor, work hard, and become rich, a New York Times survey showed that those answering yes rose from about 40 percent in 1983 to 60 percent in 2005 [Hertz 2006]. However, a more recent New York Times poll shows a sharp decline in such optimism [Shapiro 2013] and alarmed commentators in the public media have begun to question the reality of income mobility, the alleged weakening of class lines, and the supposed commonality of interests of low- and high-income groups. A number of studies have recently appeared providing evidence on fracturing along various dimensions — income, wealth, residential location, education, health, and, in the social sphere, values and opinions. The purpose of this essay is to review these various types of fracturing, to see if they are related and to gain some idea about possible future trends. The discussion begins with an examination of the fracturing of income and wealth over time. Such trends have led to fracturing in other aspects of life, such as residential location, education, and health. I then turn to the social sphere and show some divergences over time of opinions and attitudes between the high-income earners in the population and the remainder of the population. Such empirical evidence suggests that in many respects, fracturing of America has recently been increasing and that the process appears to be self-feeding, so that it will most likely continue in future years. At the end I briefly examine whether fracturing has yet led to increased class warfare or the alleged decline in morals, casting doubt on both conjectures.",1
41,2,Eastern Economic Journal,24 March 2014,https://link.springer.com/article/10.1057/eej.2014.14,Is Income Growth in the United States Pro-Poor? A State-Level Analysis,March 2015,Edinaldo Tebaldi,Jongsung Kim,,Unknown,Unknown,Unknown,Unknown,,
41,2,Eastern Economic Journal,16 June 2014,https://link.springer.com/article/10.1057/eej.2014.31,Class Size Matters: Heterogeneous Effects of Larger Classes on College Student Learning,March 2015,Timothy M Diette,Manu Raghav,,Male,Male,Unknown,Male,"Class size is potentially important because it may affect the overall learning experience of students. Smaller class size encourages greater student–faculty interaction, lowers the cost to the professor of assigning intensive writing projects, and increases adoption of innovate teaching techniques such as active learning exercises in the classroom. However, smaller class sizes are expensive for colleges and universities to maintain. Increasing class size is one of the most straightforward methods to implement to reduce the cost per pupil for colleges and universities. Many public colleges and universities are increasing class size to address financial constraints imposed by reductions in support from state funds, diminished endowments, and reduced donations from organizations and individuals. The recent financial crisis forced both public and private schools to reevaluate the level of funding drawn from endowments. But on the other hand, to stay attractive in the teacher labor market, college costs have been pushed up due to professors teaching fewer classes and increased personnel salary and benefit costs. In addition, some schools have increased the number of admitted students to increase revenue. Because of all these reasons, average class size has increased. There is mixed evidence on whether increasing class size reduces the quality of education at the college level [Kennedy and Siegfried 1997; Arias and Walker 2004; Kokkelenberg et al. 2008; Bandiera et al. 2010]. Our paper extends the current literature to the context of liberal arts colleges. Liberal arts colleges advertise themselves as being distinct from larger colleges and universities by offering small classes, easy access to professors, and a particularly strong focus on undergraduate teaching. The unique environment may limit the generalizability of previous studies that focus on large public schools or exclusively economics or business school classes. We estimate a model of grades, which is dependent on class size and other covariates. After controlling for student characteristics, course characteristics, department, semester, time, and faculty fixed effects, we find that increasing class size is harmful to students. In addition, we follow Bandiera et al. [2010] and examine whether class size effects are heterogeneous across students. We find that students who are already at the greatest risk — first-year students and those with low SAT scores — are more impacted by larger class sizes. Our findings suggest that colleges should be mindful of the explicit trade-off when determining the size of classes.",15
41,2,Eastern Economic Journal,24 November 2014,https://link.springer.com/article/10.1057/eej.2014.65,On the Go: De-Mystifying Gross Output,March 2015,Mark Skousen,,,Male,Unknown,Unknown,Male,"Colander mentions the history of GO. Let me give a little more background. My original work, The Structure of Production [Skousen 1990] was built on the shoulders of two giants, Wassily Leontief and his development of input–output tables [Leontief 1966], and Friedrich Hayek and his Austrian stages-of-production model [Hayek 1935]. Both won Nobel prizes for their work in these areas. Hayek made no attempt to measure his “triangles”; his Prices and Production was a purely theoretical work. Leontief’s development of input–output tables was a major advance in putting numbers on individual industries, and showing the “hierarchy of interindustrial dependence,” but he showed little interest in an aggregate figure such as GO as a macroeconomic tool [Leontief 1966, pp. 14–15, 162]. His I–O data was compiled every 5 years, and only in the 1990s, after Structure was published, did the BEA begin measuring “Gross Output by Industry” on an annual basis. Even then, it was 2–3 years behind, and few economists or journalists paid attention. Now all that is changing with the up-to-date quarterly data being published.",3
41,2,Eastern Economic Journal,18 March 2015,https://link.springer.com/article/10.1057/eej.2014.18,"The Battle of Bretton Woods: John Maynard Keynes, Harry Dexter White, and the Making of a New World Order",March 2015,Brittany A Baumann,,,Female,Unknown,Unknown,Female,,
41,2,Eastern Economic Journal,18 March 2015,https://link.springer.com/article/10.1057/eej.2014.19,The Great Inflation: The Rebirth of Modern Central Banking,March 2015,Robert C Winder,,,Male,Unknown,Unknown,Male,,
41,2,Eastern Economic Journal,18 March 2015,https://link.springer.com/article/10.1057/eej.2015.5,List of Reviewers 2014,March 2015,,,,Unknown,Unknown,Unknown,Unknown,,
41,3,Eastern Economic Journal,27 May 2015,https://link.springer.com/article/10.1057/eej.2015.32,An Economist with Attitude on Steroids: Remembering Gordon Tullock,June 2015,David Colander,,,Male,Unknown,Unknown,Male,,
41,3,Eastern Economic Journal,23 March 2015,https://link.springer.com/article/10.1057/eej.2015.6,Benefits of Education at the Intensive Margin: Childhood Academic Performance and Adult Outcomes among American Immigrants,June 2015,Deniz Gevrek,Z Eylem Gevrek,Cahit Guven,,Unknown,Male,Mix,,
41,3,Eastern Economic Journal,14 July 2014,https://link.springer.com/article/10.1057/eej.2014.20,Does Month of Birth Affect Individual Health and Educational Attainment in Iceland?,June 2015,Thorhildur Ólafsdóttir,Tinna Laufey Ásgeirsdóttir,,Unknown,Female,Unknown,Female,"An association between month of birth and outcomes such as health, earnings, and educational attainment has been documented in numerous studies. Individuals born in the first quarter of the year (January−March) have been found to have less schooling and earnings [Angrist and Krueger 1991; Plug 2001; Chesher 2007; Hoogerheide et al. 2007]. Proneness to diseases such as schizophrenia [Castrogiovanni et al. 1998; Tochigi et al. 2004], type 1 diabetes mellitus [Willis et al. 2002], and epilepsy [Procopio et al. 2006] is greater for those born in January−May. Results from a study on mortality, measured in the Northern Hemisphere, show that those born in the autumn (October−December) live longer than those born in the spring (April−June) [Doblhammer and Vaupel 2001]. Numerous other factors have been related to month of birth, such as shyness [Gortmaker et al. 1997], left-handedness [Martin and Jones 1999], autism [Gillberg 1990], dyslexia [Livingston 1993], family income [Kestenbaum 1987], and reproductive output [Huber et al. 2008]. Several hypotheses have been put forward to explain this seasonality in outcomes by month of birth, including natural and social factors. Among the most prominent theories are those regarding nutrition and exposure to illness during the fetal period, compulsory schooling laws, relative age at school start, and parents’ socioeconomic status. The empirical analysis in this study is aimed at examining these four hypotheses using Icelandic data. We present results from a descriptive analysis of month of birth in relation to self-assessed health (SAH) and educational attainment among women in Iceland. Such an analysis has not been done before. The final answer on the effect of birth month on later outcomes is unlikely to be provided with a single study, but as the literature grows, some theories are less favored, whereas others become more convincing. Thus analyses within communities, which vary according to the hypothesized reasons for month-of-birth variability, shed light on the probability of those hypothesized reasons. The paper’s contribution to the literature is the use of data from Iceland, a country that differs with respect to social, environmental, and institutional factors from places from which previous studies originate. This is important, since birth seasonality may be influenced by all of those factors. A predictably different pattern in outcomes from previous results would be expected with the Icelandic data if relative age at school start or compulsory schooling laws were truly the underlying mechanisms. However, if parent’s socioeconomic status is the explanatory mechanism the outcome pattern would be expected to be similar in the current study as in previous research, although attenuating effects should be accounted for. The fetal-origins hypothesis rests on seasonal variations in nutrition and illnesses and is therefore subject to the period explored. Even though such seasonal variations may be less pronounced in Iceland because of less variation in weather, one would not expect the outcome pattern to be qualitatively different from previously reported results from other countries. A separate literature review is offered on seasonality in birth frequency among populations in Appendix A.",2
41,3,Eastern Economic Journal,27 May 2015,https://link.springer.com/article/10.1057/eej.2014.21,Who Takes Advanced Placement (AP)?,June 2015,Benjamin P Scafidi,Christopher Clark,John R Swinton,Male,Male,Male,Male,"Economists and other social scientists have long been interested in analyzing the sources of differences in student achievement between various groups of students. Jencks and Phillips [1998] reported that differences in test scores between African-American and white students had narrowed considerably after the 1954 US Supreme Court decision in Brown v. Board of Education declared that governments could no longer legally establish separate schools for black and white students. Johnson and Neal [1998] found that differences in basic cognitive skills in Language Arts and Mathematics as measured by the Armed Forces Qualifying Test explained a large portion of the differences in later wages and earnings between whites and African-Americans. After decades of declines, the gap between the academic achievement of African-American and white students has remained large. Moreover, in recent years the declines in achievement gaps have stagnated or slowed [Magnuson and Waldfogel 2008]. Recent research has discovered that the gap in academic achievement between low-income and high-income students is large and — in contrast to the black–white achievement gap — has widened significantly since the 1950s [Reardon 2011]. Given the large role that cognitive skills appear to play in determining wages, earnings, and other life outcomes and given the large — and sometimes growing — achievement gaps that exist between different groups of students, it is worth analyzing potential sources of achievement gaps. One potential source of achievement gaps is differences in access to Advanced Placement (AP) courses in high school. If low-income students and students from historically disadvantaged racial and ethnic groups have less access to AP courses than other students, this differential access may be a source of achievement gaps between students and may have adverse effects on their labor market and other outcomes. This paper analyzes access to AP with a special emphasis on differences across racial and income groups and geographic differences. The AP Program sponsored by the College Board has been creating curricula and tests for advanced high school courses since 1955. AP courses are designed to be more challenging than typical high school courses and potentially lead to course credit for college [Willingham and Morris 1986]. High schools can select to teach any of the 34 AP subjects the College Board currently offers, provided that they obtain College Board approval. To label a course as an “AP” course means that the high school obtains approval of the syllabus and teacher by the College Board and has a class size that is typically smaller than non-AP courses. Over the past generation, American policymakers have made increasing enrollment in AP courses a national priority. In 1989, the governors of the 50 American states and then-President George H.W. Bush met in Charlottesville, Virginia and created, for the first time, national goals for education. Their effort was called Goals 2000. The third of these national education goals stated that “By the year 2000, American students will leave grades four, eight, and twelve having demonstrated competency in challenging subject matter ….” One specific metric to measure progress toward this third national education goal was the number of students in AP courses [National Education Goals Panel 1999, p. vi]. In recent years, national leaders from both major American political parties have advocated for greater enrollment in AP courses. For example, in his 2006 State of the Union Address then-President Bush [2006] said, “Tonight I propose to train 70,000 high school teachers, to lead advanced-placement courses ….”. And, US Secretary of Education Arne Duncan recently said, “It is no secret that I am a huge fan of AP” [Duncan 2010], and “I am also especially encouraged by increasing enrollment in advanced placement classes as one indicator of high school rigor” [Duncan 2011]. In addition, leaders of the business community have expressed the opinion that greater enrollment in AP courses is an important step toward improving the quality of the American workforce. Furthermore, they use AP enrollment and success as measures of workforce quality [US Chamber of Commerce 2011]. Overall, the United States has seen significant increases in enrollment in AP courses. In the six decades since the birth of the AP Program, the number of schools offering AP courses has increased from under 900 to over 15,000 [College Board 2007; Klopfenstein and Thomas 2009]. According to the College Board [2012a], “In May 2011, nearly two million students representing more than 18,000 schools around the world, both public and nonpublic, took 3.4 million AP Exams.” Many, however, have an additional goal regarding the AP program. On its website, the College Board — the purveyor of AP courses — states that “equitable access” to AP courses is an important goal. Their statement of support of equitable access pays particular attention to access for students who are racial or ethnic minorities who have traditionally been “underserved” or are from low-income backgrounds: The College Board strongly encourages educators to make equitable access a guiding principle for their AP programs by giving all willing and academically prepared students the opportunity to participate in AP. We encourage educators to: Eliminate barriers that restrict access to AP for students from ethnic, racial, and socioeconomic groups that have been traditionally underserved. Make every effort to ensure their AP classes reflect the diversity of their student population. Provide all students with access to academically challenging coursework before they enroll in AP classes Only through a commitment to equitable preparation and access can true equity and excellence be achieved.[College Board 2012b] It is this access issue regarding the AP program that is the subject of this paper. Specifically, we analyze the likelihood of taking AP Economics for different types of students. Given that national leaders, business leaders, and leaders in economic education have the goal of increasing enrollment in AP courses, we analyze AP course-taking and pay particular attention to students who have been “traditionally underserved.” Using administrative data from two cohorts of students in Georgia public schools, we estimate a series of regression models that analyze course-taking in AP Economics in various ways. Georgia data are well suited for examining course-taking in AP Economics because all public school students are required to take economics in high school, and a good measure of prior student achievement in mathematics is available. Prior achievement in mathematics has been found to be a strong predictor of success in high school economics [Ballard and Johnson 2004; Clark et al. 2012]. In what we consider to be a naïve empirical specification, that has only student demographic characteristics as regressors — and is similar to virtually all the prior literature on AP course-taking — we find large differences in AP Economics course-taking across groups. Asian students, white students, and students from higher income backgrounds are much more likely to be enrolled in AP Economics relative to other types of students. However, once we control for prior achievement in Geometry, African-American and Hispanic students are actually overrepresented in AP Economics relative to white students — this overrepresentation is large relative to the percentage of students overall in AP Economics and statistically significant. Conditional on prior achievement in Geometry, low-income students are underrepresented in AP Economics, but the estimated difference is reduced by just over one-half relative to our naïve specification. Nevertheless, the difference between low-income and high-income students remains large and statistically significant. The likelihood that a low-income African-American or Hispanic student takes AP Economics is virtually identical to that of a high-income white student all else equal, including prior achievement in Geometry. On the basis of our results, an important reason why African-American, Hispanic, and low income students are less likely to be enrolled in AP Economics relative to white and Asian students is prior achievement in Geometry. Males and females are about equally likely to be enrolled in AP Economics — even though males have much higher prior achievement in Geometry. Thus, controlling for prior achievement, males are less likely to take AP Economics relative to females, all else equal. Smaller high schools and high schools outside of the 20-county region defined as metropolitan Atlanta are much less likely to offer AP Economics. African-American, Hispanic, and especially Asian students are more likely to attend high schools that offer AP Economics. In addition, African-American, Hispanic, and students from low-income backgrounds attend schools that increase their likelihood of taking AP Economics. Our results are consistent with the empirical findings in Conger et al. [2009] — the only other study that, to our knowledge, controls for prior achievement when analyzing AP course-taking. While we confirm the results in Conger and colleagues using data from a different state, we also add to the literature by pointing out the important role that geography plays in access to AP — students in urbanized areas outside of metropolitan Atlanta and especially students in rural areas have far less access to AP than students who live in metropolitan Atlanta, even after controlling for school size. The differences in access to AP Economics by geography are large. The rest of this paper is organized as follows. The next section contains an explanation of critiques of the AP program, including the issue regarding access for traditionally underserved student populations, and a review of the relevant literature. The section that follows explains why the state of Georgia is well suited for analyzing the issue of AP course-taking and also describes our data. A discussion of our empirical approach follows. The final two sections contain our empirical results and concluding remarks, respectively.",3
41,3,Eastern Economic Journal,19 May 2014,https://link.springer.com/article/10.1057/eej.2014.22,"What You Do in High School Matters: High School GPA, Educational Attainment, and Labor Market Earnings as a Young Adult",June 2015,Michael T French,Jenny F Homer,Philip K Robins,Male,Female,Male,Mix,,
41,3,Eastern Economic Journal,05 May 2014,https://link.springer.com/article/10.1057/eej.2014.23,External Financing and the Survival of Black-Owned Start-Ups in the US,June 2015,Yunwei Gai,Maria Minniti,,Unknown,Female,Unknown,Female,"In the United States, the last 30 years have seen a significant increase in the number of self-employed blacks [Fairlie 2004]. Yet, the percentage of self-employed white workers in the United States continues being about three times that of black workers [Robb and Fairlie 2007], and that ratio has remained relatively stable for the last 100 years [Fairlie and Meyer 2000]. In addition, studies have shown that self-employed blacks earn less than whites [Borjas and Bronars 1989], that black-owned businesses tend to be less profitable than white-owned businesses [Robb and Fairlie 2007], and that the rate of exit from self-employment for blacks is about twice that of their white counterparts [Fairlie 1999].Footnote 1 What explains this difference? Köllinger and Minniti [2006] have shown that the under-representation of blacks is not due to a lack of entrepreneurial propensity. In fact, they find that African Americans are more likely to try starting a business than whites, but that they are significantly less likely than white Americans to actually start a business or own an established one that survives in the market beyond the initial start-up process. Their study suggests that the lack of participation in self-employment among African Americans is not the result of a lack of tradition as hypothesized in early writings on race [Myrdal 1944], but it is likely due, instead, to higher failure rates and/or to the existence of barriers in access to resources. Studies on the differences in entrepreneurship rates between racial groups have stressed geographical concentration, reliance on co-racial markets, replacement in markets abandoned by indigenous businesses or large firms, and discrimination [Fairlie and Robb 2008]. In addition, when focusing specifically on differences between black Americans and other racial groups, particular attention has been paid to the lower average personal wealth held by black entrepreneurs [Fairlie 1999], and to the possible existence of discrimination in the credit market. Bradford [2003], for example, estimated that in the mid-1990s, black business owners held on average about a third of the assets held by other groups. Similarly, Blanchflower et al. [2003] provided evidence that black-owned businesses in the United States experienced higher loan denial probabilities and paid higher interest rates than white-owned businesses even after controlling for differences in creditworthiness and other factors. Robb and Fairlie [2007] have suggested that blacks are less likely to have the financial resources necessary to start a business and, therefore, are more exposed to the risk of failure than whites. Finally, Bates [1991] found that black failure rates would be similar to those of whites if blacks were to receive the same amount of external financing. Taken together, this evidence suggests that undercapitalization may be responsible for a significant portion of the differences in performance and exit rates observed between black- and white-owned businesses. The goal of our paper is to investigate whether the white–black gap in entrepreneurship has financial causes that go deeper than personal net worth and discrimination in the credit market. We fill this gap in the literature by testing whether obtaining commercial financing reduces the exit rate of black-owned start-ups differently than that of start-ups owned by other racial groups. Specifically, the objective of the paper is to understand whether commercial financing has different effects on the survival of black- and white-owned businesses after they have obtained commercial financing. In other words, we compare the effect of commercial financing across black-owned businesses and, separately, across white-owned businesses. Our intuition is that having obtained external financing should decrease the probability of failure in general, but that the marginal contribution of external financing should be different between black- and white-owned businesses. Specifically, if black-owned start-ups have, on average, less informal financing than white-owned start-ups, their relative financial position will be weaker and the need for commercial financing more pronounced. Therefore, after correcting for quality of the business idea and human capital of the entrepreneur, we would expect access to commercial financing to increase the likelihood of survival for black-owned start-ups more than for white-owned start-ups because it will compensate their initial financing disadvantage. In essence, we want to test whether obtaining financing makes a real difference for the probability of survival and whether this difference is distributed asymmetrically across racial groups. We use data from the Kauffman Firm Survey (KFS) to test for the possible asymmetric effect of commercial financing. Since the survival of a start-up is largely determined by the owners’ human capital, we use propensity score matching (PSM) method to address the endogeneity problem of credit worthiness, and panel data and survival models to account for the fact that survival rates change significantly with the age of the business. Unexpectedly, our results do not support our intuition and suggest that, while the use of commercial financing reduces the exit rates of new firms in general, that reduction is not significantly different across racial groups. We suggest that this result may derive, at least in part, from unobserved heterogeneity linked to the owner’s start-up capital and, likely, to other beneficial externalities that access to informal investors produces. Our paper contributes to the literature in several ways. First, we distinguish the effects of commercial financing from those of non-commercial financing since not all types of financing have the same impact on start-ups’ survival odds. Second, we link directly the types of financing available to entrepreneurs (and the survival of the start-up) to the owners’ human capital. Third, unlike previous studies that have focused on established businesses, we look at new businesses in their difficult inception stage. This is important since it allows us to avoid selection bias and to answer questions concerning survival rates by looking at the appropriate population.",6
41,3,Eastern Economic Journal,22 September 2014,https://link.springer.com/article/10.1057/eej.2014.24,"Bricks, Mortar, and Wedding Bells: Does the Cost of Housing Affect the Marriage Rate in the US?",June 2015,Simon W Bowmaker,Patrick M Emerson,,Male,Male,Unknown,Male,"Anecdotal evidence and findings from surveys indicate that individuals consider housing market factors when weighing the decision to marry. For example, an Ipsos MORI poll, conducted in 2008 on behalf of the Civitas think tank in the UK, of people aged between 20 and 35 years found that seven in ten wanted to marry and a quarter of those surveyed stated that they had not yet married because of financial circumstances; lacking the funds required to buy a home was one factor cited. However, only a small number of academic studies have examined the extent to which the state of the housing market (in particular, the cost of buying property) affects entry into marriage. This comparative neglect is somewhat puzzling, particularly in light of evidence that homeownership rates for married couples are higher than for singles and cohabiting couples [Henretta 1987; Rindfuss and Van den Heuval 1990; Clark et al. 1994; Dowling 1998; Mulder and Wagner 1998; Rowlands and Gurney 2001; Feijten and Mulder 2005], and that entry into marriage and the purchase of housing often coincide closely with each other [Clark and Dieleman 1996; Mulder and Wagner 1998; Van Riper 2006], which raises the question of whether a high cost of housing has the potential to constrain marriage. This paper examines the relationship between the cost of housing and the rate of marriage in 2,450 US counties over the period 1970–1999. Data from the US Census are used to construct a measure of the burden of housing costs (the ratio of the value of owner-occupied housing to per capita income), while the marriage rate figures are taken from the National Center for Health Statistics (NCHS) and privately obtained from Vital Statistics units on a state-by-state basis. This appears to be the most comprehensive data set on marriages in the United States that has been collected and analyzed at the US county level. Controlling for economic and demographic variables, it is found that the burden of housing costs appears to play a role in the marriage decision. A higher ratio of the cost of owner-occupied housing to per capita income in a given county is associated with a lower marriage rate. The analysis is also extended to include a measure capturing the relationship between the cost of owning housing and the cost of renting and marriage. Evidence is reported that suggests that the greater the difference between the annual cost of owning housing and renting as a proportion of per capita income in a county, the lower the marriage rate. The paper proceeds as follows. The second section incorporates the burden of housing costs into a standard economic model of marriage. The third section reviews previous empirical studies that have examined the relationship between the housing market and marriage. The fourth section describes the paper’s panel data and estimation methods. The fifth section contains the main results, robustness checks, and an extension of the analysis. The final section provides a summary and conclusion.",1
41,3,Eastern Economic Journal,20 April 2015,https://link.springer.com/article/10.1057/eej.2015.20,Cumulative Effects on Weight Due to an Initial Occupational Choice as a Blue Collar Worker,June 2015,Bogdan Nedanov,Charles R Link,,Male,Male,Unknown,Male,"The goal of this paper is to estimate the long-term effects of occupational choice on obesity. Once a person has chosen an initial occupation, he or she is likely to be placed on a particular trajectory of labor, socioeconomic and health outcomes, which could ultimately impact her overall state of health decades later [Fletcher 2012]. A particular type of job may have an effect on an individual’s future job mobility, stream of earnings, wealth, and retirement outcomes. The exact impact of an occupation on health is thought to operate through multiple channels, some of which are considered to be direct and some indirect [Kelly et al. 2011; 2014]. For instance, job related stress and exposure to hazards are likely to directly affect a person’s health. Likewise, according to Fletcher, income, availability of health insurance and the “psycho-social” aspects of the job such as peer effects are considered to indirectly impact health. The general direction of some these effects may not be entirely clear. For instance, one of the main hypotheses underlying our analysis is that manual work can potentially cause health deterioration by physically stressing the body. On the other hand, one of the most commonly stated reasons for the recently observed growth of obesity has been the gradual shift towards a sedentary lifestyle, often starting at the workplace. Manual work exposes an individual to a higher degree of physical activity, which could potentially be health improving [Lakdawalla and Philipson 2007; Gomis-Porqueras et al. 2011]. Studying the relationship however is further complicated by the endogenous nature of occupational choice and self-selection. Individuals are not randomly assigned to blue or white collar jobs, but instead choose their initial occupation. It is very possible that unobservables such as ambition, upbringing, and risk aversion may have an effect on a person’s choice of work. Case and Deaton [2003] and Fletcher and Sindelar [2009] argue that if manual work has negative impacts on health, it is likely that individuals who are less likely to experience the health deteriorating effects will be the ones to self-select into such occupations, thus downwards biasing the key coefficients. Our paper contributes to the existing literature by expanding on the work by Kelly et al., who estimated the cumulative effects of initial occupational choice on adult obesity using the Panel Study of Income Dynamics (PSID). Ours is one of only a few papers which explore this important relationship and thus provides some interesting comparisons. The consistency of the results across datasets is tested by estimating a recursive bivariate probit model, using the National Longitudinal Survey of Youth (NLSY). The results will indicate whether their results, based on multiple cohorts, hold when using a sample consisting of a single age cohort as is the case with the NLSY and with more recent obesity data. Several different estimation techniques are used in order to test the consistency and the sensitivity of the results to each approach. The bivariate probit is expanded through the use of more instruments available only in the NLSY, which further aids in identifying the occupational choice variable, as well as through the inclusion of multiple controls not present in the PSID. In contrast to Kelly et al., our instruments are quite strong at predicting occupational choice. The section ‘Literature review’ provides a survey of studies relevant to our research. The section ‘The model’ sets forth our model of initial occupational choice and weight gain many years later in life. The section ‘Data’ discusses the data while the sections ‘Initial occupation blue collar and years of blue collar work’ and ‘Other variables of interest’ present the results. Section ‘Conclusions’ provides a summary and conclusions.",
41,3,Eastern Economic Journal,02 February 2015,https://link.springer.com/article/10.1057/eej.2014.44,The Relationship between Net Migration and Unemployment: The Role of Expectations,June 2015,Robert Baumann,Justin Svec,Francis Sanzari,Male,Male,Male,Male,"Labor market theory has long posited that economic factors induce migration across regions; see Lowry [1966] and Harris and Todaro [1970] for early expositions. This migration helps reallocate labor to those regions where it is most valued and away from regions where it is least valued. According to the Census Bureau, about 11.6 percent of all Americans changed residences between 2010 and 2011.Footnote 1 Most of these moves are within the same county, but many Americans migrate out-of-state each year. The same report finds that about 1.6 percent of Americans, or about 4.89 million Americans, moved out-of-state between 2010 and 2011. One important driver of migration is unemployment. Several empirical papersFootnote 2 posit and test whether people immigrate to regions with low unemployment and emigrate from regions with high unemployment because either they found a job in the low unemployment area or they believe their chances of finding a job are higher. However, this logic presents two problems. First, migration has not eliminated large and persistent unemployment differences across regions, for example Appalachia and some inner-cities. Both of these examples are particularly curious because of their close proximity to lower unemployment areas, which suggests that migration costs (both pecuniary and psychic) should be relatively low. Second, as we detail in the next section, several empirical migration papers do not confirm the above intuition. In this paper, we present an alternative explanation. We believe unintuitive empirical results stem from model misspecification. Specifically, we show that migration does not respond to the observed level of unemployment, per se, but rather people migrate due to changes in their expectations of unemployment across regions. The previous literature, in failing to account for these expectations, effectively averaged the effect of unemployment observations that did and did not change expectations. To formalize this insight, we develop a parsimonious theoretical model of migration. This Harris and Todaro style model assumes that a nation is composed of two regions, each of which is subject to an unemployment shock. The unemployment shock represents the probability that a resident in that region is unemployed. If the resident is unemployed, then she earns nothing; if she works, the resident earns her marginal product of labor. The residents of the nation are endowed with initial probability models that they believe characterize the regions’ shock processes. In this environment, an equilibrium is defined as the distribution of population across regions such that no resident wants to migrate. This condition implies that the expected wages in the two regions must be equal to each other. As we will show, the initial population distribution depends on the residents’ expectations of one region’s unemployment rate relative to the other. Given the initial equilibrium, we then consider two alternative scenarios. In the first scenario, we assume that each region is hit by an unemployment shock that falls within the residents’ expectations. As we show, this event has no effect on migration because the residents’ expectations do not change. In the second scenario, one region is hit with a level of unemployment that is outside of the residents’ expectations. As this state of the world was not believed possible, the residents are forced to update their expectations about the shock process. This change in expectations then induces residents to migrate until the expected wages are once again equalized. Using this theoretical model, we derive three testable predictions. First, the level of unemployment only affects migration if it alters expectations. If, instead, a shock occurs that does not change expectations, then migration should not occur. It is for this reason, we contend, that the previous literature, which typically ignores expectations, did not find a consistent impact of unemployment on migration. Second, if the shock improves (worsens) the residents’ expectations about region i’s unemployment shock, then residents immigrate into (emigrate from) region i. Third, unemployment shocks that cause greater changes in expectations lead to larger migration responses than shocks that cause smaller changes in expectations. In our empirical section, we test these three predictions on US state-level data from 2000 to 2010. To do this, we must differentiate between shocks that are within people’s expectations and shocks that are outside. As these data are not readily available, we have created a simple empirical test that attempts to distinguish between these two types of observations. An unemployment shock is defined as within expectations if it is sufficiently close to past observations, where “close” is measured by k standard deviations from the mean of the past data. An unemployment shock is defined as outside expectations if it falls outside of that range. If k is large enough, then the shock can plausibly be considered unexpected, as that unemployment rate is far away from what has been observed historically.Footnote 3 Given these definitions, we find strong evidence that unemployment observations outside of expectations have a much larger impact on migration than shocks within expectations. In particular, for a median population state, if the unemployment rate rises by 0.1 percentage points but the new value remains within expectations, then the state should expect to lose 22 people in the following month. Conversely, for the same increase in unemployment, if the new value moves outside of expectations, then the state should expect to lose over 25 times that many people in the following month. Further, we show that unemployment rates that are above expectations induce out-migration, while unemployment rates that are below expectations induce in-migration, just as the theory predicts. These results are robust to a number of alternative specifications. Our findings also suggest that unemployment shocks that are further away from people’s expectations cause a greater impact on migration than shocks that are closer to (but still outside of) people’s expectations. Finally, we find no evidence that a correlation between our expectations variable and large unemployment changes is creating a spurious result.",2
41,3,Eastern Economic Journal,27 May 2015,https://link.springer.com/article/10.1057/eej.2014.67,The Cultural and Political Intersection of Fair Trade and Justice: Managing a Global Industry,June 2015,Teresa L Cyrus,,,Female,Unknown,Unknown,Female,,
41,4,Eastern Economic Journal,25 May 2015,https://link.springer.com/article/10.1057/eej.2015.31,Economic Theory Has Nothing to Say about Policy (and Principles Textbooks Should Tell Students That),September 2015,David Colander,,,Male,Unknown,Unknown,Male,"My interest in this article is not with economics but with economic pedagogy: What blend of theorems and precepts should economists focus on in the principles of economics course. There is no one answer to that question, and, as long as professors make it clear whether they are teaching theorems or precepts, the course can be taught in various ways. Unfortunately, all too often theorems and precepts become blended, and students think they are learning non-debatable theorems when they are actually learning debatable precepts. The quiz questions that I started out this article with were posed as theorems, and as theorems, they had no implications for policy. Had they been presented as precepts, “true” answers would be possible, but only if all the qualifying nuances — the direct and implicit assumptions and ancillary judgments embedded in the model that led to the policy result — were taken into account in the question itself. Looking at the history of economic pedagogy, we can see that teaching has shifted between the two. Up until the 1950s, the majority of principles textbooks in economics were designed to teach precepts. Solow nicely described the textbooks of the time as “civilized and discursive — sensible discussions of policy as seen by an economist.” [Solow 1997] Because the texts were focused on teaching precepts, they were quite different from our principles books today. [See Colander 2005]. They had few graphs, and were mostly loose narratives about the relation of economic theory to policy. (Francis Edgeworth described Marshall’s writings, which used that approach as the “zigzag windings of the flowery path of literature.” [Edgeworth 1925, p. 288]) Exams reflected that nuanced vagueness. They were generally comprised of essay questions that had, in principle, many different answers; generally there was an expected answer but, ideally, what the examiners were looking for was an understanding of the nuance and an ability to reason from stated premises, not a specific answer. Teaching nuanced precepts proved difficult to carry out in practice. The principles textbooks of the time tried, but they had mixed success. “High level” texts such as John Stuart Mill’s or Alfred Marshall’s, blended numerous qualifications into their discussion and did a reasonable job. They presented a few theorems, which they verbally qualified, and had sensible, but highly debatable, discussions of complex issues. On the basis of those books, it would be very hard for students to arrive at a definite policy conclusion based on economics alone. The lesser known principles texts, which dominated the US principles market, were far less successful about blending theorems and precepts. Their discussions often turned into self-righteous moralizing. Consider, Francis Wayland’s Principles, one of the leading US texts in the late mid 1800s. In discussing savings, he writes “It is obvious, upon the slightest reflection, that the Creator has subjected the accumulation of blessings of this life to some determinant laws. Every one, for instance, knows that no man can grow riche, without industry and frugality.” [Wayland 1837 (1875)] Another leading US textbook in the early 1900s, Seligman [1905 (1929)] was a bit more successful. He writes “It is not competent to argue from internal free trade to international free trade” but that “in the main, then, the conclusion would seem to be that under certain conditions a protective policy is relatively defensible.” How students were to interpret “in the main” and “relatively defensible” is unclear, and that ambiguity shows one of the problems with trying to teach precepts; the presentations of policy become either wishy washy, saying anything can result, or they convey the sense that students are expected to take the “master’s” (i.e. the textbook author’s) view on policy as definitive. The difficulty of teaching primarily precepts in the principles texts was recognized, and there were attempts to avoid them. One of the earliest of these attempts was Pantelioni’s [1898] Pure Economics, a principles book designed to teach theorems, not precepts. Pantelioni argued that precepts were too subjective to teach effectively, and that economists should concentrate on teaching beginning students what he called pure economics, which meant teaching students theorems that were not meant to be directly applied to policy. Pantelioni focused on teaching economic principles without the moralizing or the judgments inherent in precepts. His book was not widely adopted, and US principles books up until the 1950s primarily focused on teaching precepts. The primary focus on precepts changed in the 1950s when Paul Samuelson’s book (Samuelson, 1948) transformed the way principles of economic was taught. Instead of teaching precepts, Samuelson’s book taught a blend of theorems and precepts that were tied together with the general equilibrium welfare economics that had developed in the 1930s. In presenting economic theory and policy this way he followed the economics profession, which had gravitated away from the Classical policy approach and had started directly relating theory to policy through a newly developed separate branch of economics — welfare economics. The hope was that welfare economics would allow economists to directly relate their theorems to policy, thereby eliminating the need to separate out theorems and precepts. Unfortunately, the attempt failed, and welfare economics faded away as a separate field of economics. The reason it failed was that the attempt violated Hume’s Dictum that you cannot derive a should from an is. Shoulds only follow from other shoulds. The initial economic welfare theory focused on Pareto Optimality and efficiency but it was quickly recognized that, without value judgments, it was impossible to move from theorems to policy. To get around the problem formal welfare economics moved to a social welfare function (SWF) approach, in which economist’s policy job was to apply society’s social welfare function to their applied policy analysis. Given the appropriate SWF, it could theoretically integrate values into economist’s policy analysis. The problem was that practically the social welfare function could not be implemented. A full specification of the Social Welfare Function required value decisions on a wide array of questions that went far beyond economics, and any tractable SWF was embedded with numerous implicit value decisions that eliminated any generality of the results. The SWF approach hid, but did not solve, the “values” problem. In addition, there was no way of unambiguously arriving at a social welfare function from individual utility functions without violating seemingly desirable aspects of the selection mechanism. [Arrow 1950] So, practically, the social welfare function approach quickly recognized was a dead end for practical policy. As early as 1957 Jan v. Graaff, in his famous study of welfare economics, recognize this, stating that “the possibility of building a useful and interesting theory of welfare economics — that is, one which consists of something more than the barren formalisms typified by the marginal equivalences of conventional theory — is exceedingly small.” [v. Graaff 1957, p. 169] By the 1970s, the impossibility of welfare economics avoiding the need to separate theorems and precepts was well understood by economic theorists, and as a result economists abandoned formal welfare economics as a practical guide to policy. Instead they used a variety of heuristics that embodied numerous, generally unstated, value judgments. Economists, such as Sen [1970] recognized the problem, and in their writings provided a deep understanding to the limitations and philosophical foundations of policy discussions. Unfortunately, that understanding did not make it into the standard principles texts, which continued to connect theory and policy in ways that cannot be justified except perhaps in the name of “pedagogical simplification.” The result was the current situation — a situation in which theorems and precepts are often not distinguished in the texts, and principles students are often misled into thinking that economic theory leads to policy conclusions that it does not in fact lead to.",5
41,4,Eastern Economic Journal,23 March 2015,https://link.springer.com/article/10.1057/eej.2015.9,The College Fed Challenge Competition,September 2015,Alexandre Olbrecht,,,Male,Unknown,Unknown,Male,"This paper serves as an introduction to a symposium on the College-level Fed Challenge competition. The articles contained in the symposium describe the experiences of the college professors who teach and mentor student teams. The articles in the symposium present various forms of statistical as well as narrative evidence on the effectiveness of the competition in enhancing student understanding of monetary policy and macroeconomics. When teaching monetary economics, professors often struggle with balancing economic theory with institutional knowledge and real-world applications. Professors wishing to present students with timely and relevant applications must find ways to supplement standard textbook information. As will be clear from the articles that follow, the College-level Fed Challenge competition has been highly successful in helping students understand how theory plays out in the real world of monetary policymaking. The evidence of success presented here is supported by a broader literature on economic education on the benefits of such active learning see Salemi (2002), Silberman (1996) and Souitaris et al. (2007). Additional examples of this type of learning are described by the case study approach of Marks and Rukstad [1996], the games approach of Laury and Holt [2000], and the computer simulation of Lengwiler [2004]). In this paper, I describe the College Fed Challenge, which is designed to teach students about the implications and implementation of monetary policy in the United States, using a competitive environment to promote active learning. This program receives strong support from several Federal Reserve Banks, the Board of Governors of the Federal Reserve, and the Eastern Economic Association. Keeping in mind that each participating bank runs its program slightly differently, I will keep to a more generalized discussion.",
41,4,Eastern Economic Journal,23 March 2015,https://link.springer.com/article/10.1057/eej.2015.10,The College Fed Challenge: An Innovation in Cooperative Learning,September 2015,Cynthia Bansak,Julie K Smith,,Female,Female,Unknown,Female,"Active learning exercises, if structured well, can provide clear benefits for students in a wide range of in-class and experiential applications. In this paper, we document and provide guidelines for implementing the key elements of cooperative learningFootnote 1 in a course on the College Fed Challenge (CFC). While a course on the CFC is not taught at all institutions that participate in the CFC and many colleges and universities are in Federal Reserve districts that do not participate in the CFC, the exercises presented below can be adopted in a number of undergraduate economics courses including principles of economics, intermediate macroeconomics and money and banking. Our goal is to provide an outline to introduce a course on the CFCFootnote 2 and to give instructors a framework for introducing and enhancing elements of cooperative learning in their current classes by implementing similar projects.Footnote 3 Overall, we have witnessed the achievements of our students and have found that the benefits outweigh the costs to setting up and developing courses that involve cooperative learning exercises.Footnote 4 The paper is organized as follows. In section 2, we define cooperative learning and discuss the five key elements required so that students can successfully work together towards a common goal. The importance of cooperative learning and interdisciplinary background research and evidence of this approach is then presented in section 3. Section 4 and section 5 of the paper discuss the basics of the CFC and the structure of the course which includes summer preparation, mock presentations and mock question and answer sessions, respectively. In section 6, we provide instructions for a scaled down variation on this active learning exercise that can be introduced as a module in a larger class such as Money and Banking or Intermediate Macroeconomics. We present empirical evidence from our student evaluations that demonstrate that students learn more and excel in this type of learning environment in section 7. We conclude in section 8.",3
41,4,Eastern Economic Journal,13 April 2015,https://link.springer.com/article/10.1057/eej.2015.11,Teaching a Class Dedicated to the College Fed Challenge Competition,September 2015,O David Gulley,Aaron L Jackson,,Unknown,Male,Unknown,Male,"There are a number of regional and national level academic competitions that allow teams of students from colleges and universities to compete against their counterparts from other schools. Most students who participate in these competitions do so as an extracurricular activity. Recently, however, a small number of schools have begun to offer courses oriented around preparing students to participate in certain competitions. We follow up on Bansak and Smith [2011] and Brusentsev and Miller [2011] and examine the benefits to students, instructors, and schools of participating in a college-level academic competition that is conducted in conjunction with enrollment in a class specifically geared toward preparing students for the competition. We use our experience of teaching a course that prepares students to participate in the College Fed Challenge (CFC) to provide an example. Strictly anecdotal evidence suggests that although competition-based courses are quite time consuming for faculty and students, the results are well worthwhile for all involved: motivated, engaged students working very hard to achieve a common objective, a student centered learning environment, external validation of the quality of the school’s students, publicity to successful schools, and a highly rewarding professional experience for the instructors. This paper is organized as follows. First, we follow previous authors and discuss the benefits to students of participating in academic competitions. Second, we detail how conducting a class oriented toward preparing students to participate in a competition can enhance the experience and learning associated with participation. We use our recent experiences with a competition-oriented course as an example. Finally, we discuss some of the challenges of starting and maintaining a competition-based course.",5
41,4,Eastern Economic Journal,23 March 2015,https://link.springer.com/article/10.1057/eej.2015.12,Teaching an Economics Capstone Course Based on Current Issues in Monetary Policy,September 2015,Dean Croushore,,,Male,Unknown,Unknown,Male,"Many Capstone courses in Economics are organized around a research theme. My institution also followed such a theme for many years, but we found that detailed instruction on how to engage in research was not of much value for many of our students. So, we changed our structure so that students for whom doing a major research project would be valuable were directed to our honors program, and the rest would take a redesigned Capstone course. We considered many options for such a course, such as a topics course, a course based on reading classic Economics articles, and a course based on reading symposia from the Journal of Economic Perspectives. Ultimately, we settled on a course based on the analysis of current issues in monetary policy. The inspiration for this concept came from our participation in the College Fed Challenge, which is an intercollegiate competition, organized by a number of Federal Reserve Banks around the country. In the competition, teams of students from different universities present their views about how monetary policy should be conducted. The contest is judged by practicing macroeconomists, usually Federal Reserve economists. Engaging in the competition requires students to gain in-depth knowledge about different sectors of the economy and current issues in monetary policy. Our implementation of the Fed Challenge idea in a Capstone course is designed to use the structure and main outcomes of the Fed Challenge, without having students engage in the intercollegiate competition, but rather an internal competition. It provides students an introduction to research, without having to devote the whole course to a semester-long writing project. And the course can handle our needs for assessment of students and assurance of learning in easy ways. Our objectives for our students to accomplish in the Capstone course are to gain additional knowledge about economics, especially macroeconomics and monetary policy, to learn how research is done by engaging in a small-scale project, to improve their writing ability and presentation skills, to learn about ethical issues in Economics, and to broaden their base of knowledge about foreign economies and international interdependence. The course allows our Economics department to satisfy a number of goals, as well. In the course, we are able to assess and improve students’ knowledge, writing skills, and presentation skills. We increase their knowledge in three areas: macroeconomics and monetary policy, economics and ethics, and international issues. We also show them how to do research with a small project, which can lead to greater interest among students in research than if they did a larger project. Because we use the course to assess our students’ achievements and knowledge after 4 years, it gives us a useful tool for evaluating our program, which is required by accrediting agencies. Research on economics capstone courses was propelled by Siegfried et al. [1991], who argued that such a capstone would greatly enhance the effectiveness of the Economics major. Hansen [1986] described a set of proficiencies for economic majors and later refined it [Hansen 2001] to include six main capabilities: (1) access existing knowledge; (2) display command of existing knowledge; (3) interpret existing knowledge; (4) interpret and manipulate economic data; (5) apply existing knowledge; and (6) create new knowledge. Ensuring that every student who earns an economics major has these proficiencies requires an entire curriculum, not just one course. Presumably many of Hansen’s proficiencies are accomplished in a number of economics courses. But the Capstone course can be used to provide students the opportunity to work on the last two proficiencies (applying existing knowledge and creating new knowledge), as Salemi and Siegfried [1999] recommend, and allow Economics departments to assess their students for all six proficiencies. In addition to these proficiencies, many Economics departments have additional goals for their students. For example, in some universities such as ours, students can earn their degrees through either the School of Arts and Sciences or the School of Business. So, the department must meet the learning objectives of each school. Recently, our business school adopted learning goals requiring our graduates to: (1) be able to analyze and think critically to solve complex economic problems; (2) be effective communicators; (3) be able to understand global economic and diversity issues; and (4) be ethical leaders and decision makers. Associated with each of these goals were a set of objectives for the students. So, not only do we have goals for accomplishments in Economics that we expect of our graduates, we also need to fulfill the general goals set forth by the business school.",7
41,4,Eastern Economic Journal,13 April 2015,https://link.springer.com/article/10.1057/eej.2015.13,The Educational Value of the College Fed Challenge Competition,September 2015,Vera Brusentsev,Jeffrey Miller,,Female,Male,Unknown,Mix,,
41,4,Eastern Economic Journal,20 October 2014,https://link.springer.com/article/10.1057/eej.2014.48,"The Financial Biography of an Economist: Income, Saving, and Wealth (Mostly Pension Wealth)",September 2015,Ronald G Bodkin,Kathleen M Day,,Male,Female,Unknown,Mix,,
41,4,Eastern Economic Journal,08 December 2014,https://link.springer.com/article/10.1057/eej.2014.75,Banks as Accelerators of the Circulation of Money,September 2015,Laurent Le Maux,,,Male,Unknown,Unknown,Male,"During the last century, the quantity theory, Old view, and monetarism developed the money multiplier approach and maintained that commercial banks are unique among financial institutions in that they increase the quantity of money at will, and, all things considered, act as creators of money [Fisher 1911; Friedman 1959; Pesek and Saving 1968]. Since the New view and the integration of banking in the theory of finance, the pendulum has swung to the opposite extreme [Gurley and Shaw 1960; Tobin 1963; Fama 1980]. Banks are henceforth considered by contemporary banking theory as financial intermediaries among others and their monetary role has been denied. An alternative approach standing between these two extreme views was suggested almost 300 years ago. Retrospectively, in his Essai sur la Nature du Commerce en Général, Richard Cantillon at one and the same time challenges the money multiplier approach and grants to banks a significant monetary role.Footnote 1 The Essai propounds a twofold theoretical proposition on bank issuing of debts convertible into money at face value to the bearer on demand. The first proposition is the law of reflux in the form of convertibility, which means that banks cannot issue more than demand debts that the public wants to hold. The first proposition was restated by classical monetary theorists in the late eighteenth and nineteenth centuries, and has been documented by the secondary literature [Skaggs 1991; Glasner 1992; Le Maux 2012]. The second proposition, connected to the first, states that banks contribute to accelerating the circulation of money through the issue of demand debts and then considers banks as accelerators of the circulation of money. The second proposition is a very particular contribution from Cantillon to monetary and banking theory. I shall refer to it as the “banking approach of circulation” (regarding banks as accelerators of the circulation of money) in opposition with the “quantity approach of banking” or “money multiplier approach” (viewing banks as creators of money).Footnote 2 Cantillon’s banking approach of circulation has been largely overshadowed by the Old and New views and is rarely mentioned in literature. Without any explicit reference to the Essai, Wicksell [(1898) 1936, (1906) 1935] expounds a similar proposition when he deals with the “virtual” acceleration of money. Nevertheless, as will be seen, Wicksell’s analysis is too broadly formulated, encompassing all manner of credit and not specifically bank credit. However, the term “virtual” is the most appropriate and I shall adopt it to designate the accelerated circulation of money brought about by banks. Later, Holtrop [1929, pp. 506–7] and Rist [(1938) 1940, pp. 38–9, pp. 69–71] point out the importance of Cantillon’s banking approach of circulation and link it with the idea of the virtual velocity of circulation formulated by Wicksell. In subscribing to the money multiplier approach, Schumpeter [1954, pp. 319–21] is maybe the sole who explicitly tried to refute Cantillon’s proposition. Within the quantity theory, Allais [1987, pp. 533–4] criticizes Rist’s — in fact Cantillon’s — view, which considers “that the credit mechanism does not lead to the money creation, and that its only effect is to increase the velocity of circulation of basic money. […] Mathematically, the two formulations are equivalent, but only the [quantity] interpretation is of real significance in terms of economic understanding.” This Allais’s statement is precisely open to discussion. Before beginning the substantive discussion, two definitions — explicitly or implicitly given in the Essai — need to be emphasized. First, by money, Cantillon means the silver or gold specie coined at an official price at the Mint. Nowadays, the ultimate money is not specie but the inconvertible liabilities issued by the central bank — the fiat money. Cantillon did not refer to the fiat money standard when expounding the banking approach of circulation, but let us keep in mind that his proposition is independent on whether money is metallic or fiat. As a second definition, a bank is a fractional reserve institution holding money in reserve and, thus, an institution issuing demand debts such as banknotes or demand deposits convertible at face value into money and partially backed by it. Theoretically, Cantillon argues that the form of demand debt is irrelevant and it is only a practical matter [Essai, p. 305] and, most importantly, he draws a clear-cut distinction between money and demand debts. These definitions have been rediscovered by Rolnick and Weber [1997, p. 1311] and imply “that money should be divided into two mutually exclusive categories: objects that represent a convertibility promise by, or claim on, the issuer and objects that represent no convertibility promise or claim. For convenience, we refer to the nonconvertible, unclaimed objects as primary money, and the convertible, claimable objects as secondary money” (original italics). Respectively, I refer to money and demand debts. In order to discuss the monetary role of banking, this paper briefly comes back to the law of reflux first held by Cantillon and contrasts it with the quantity approach of banking. Then, it specifies two kinds of circulation drawn by Cantillon termed here the “monetary approach of circulation,” which is related to the physical circulation of money, and the “banking approach of circulation,” which is related to the virtual circulation of money. Furthermore, two channels of virtual circulation of money provided by banks are considered, namely “externalization” and “internalization,” and their characteristics concerning the liquidity risk borne by banks. Finally, the paper provides an analysis of comments on Cantillon’s banking approach of circulation.",1
41,4,Eastern Economic Journal,13 October 2014,https://link.springer.com/article/10.1057/eej.2014.38,"Exploring Job Satisfaction by Sexual Orientation, Gender, and Marital Status",September 2015,Karen Leppel,Suzanne Heller Clain,,Female,Female,Unknown,Female,"Economic research related to sexual orientation has grown over the past two decades, as data supporting such research have become more available. Researchers have cited evidence of various forms of workplace discrimination based on sexual orientation. In addition to experiencing earnings inequities, members of sexual minorities have been verbally and physically abused and had their workplaces vandalized [Badgett et al. 2007]. It seems likely that such experiences would reduce the job satisfaction of gay men, lesbians, and bisexuals relative to heterosexual men and women. Sexual minorities have also been fired and denied employment and promotions [Badgett et al. 2007]. Thus, their options have been restricted and their jobs may not fit their skills and aptitudes as well, nor be as satisfying, as the jobs they would have had in the absence of labor market discrimination. Researchers working in areas other than sexual orientation have uncovered a significant role of job satisfaction in explaining important aspects of worker performance. In particular, overall job satisfaction is strongly associated with absence frequency [Scott and Taylor 1985] and is a powerful predictor of quits [Freeman 1978; Clark 2001]. Organizations whose sexual minority employees quit because of dissatisfying, hostile environments lose investments made in those employees and incur recruiting, hiring, and training costs for replacements. Consequently, insofar as discrimination reduces the job satisfaction of sexual minorities, policies directed at reducing that discrimination may be expected to improve organizational outcomes. With both individual and institutional benefits at stake, it is therefore important to understand whether the job satisfaction levels of gay men, lesbians, and bisexuals do in fact differ from those of heterosexual men and women. However, very little of the economic research on sexual orientation has explored differences in job satisfaction. In this paper, we use US data to contribute to the limited existing evidence concerning the relationship between job satisfaction and sexual orientation. Differences based on gender and marital status are taken into consideration. Our results show that gay men and lesbians who are married experience lower job satisfaction compared with similar heterosexual men and women. We find no significant difference in the job satisfaction of unmarried gay men and lesbians compared with similar heterosexuals; we also find no evidence of job satisfaction differences in bisexuals and heterosexuals, when other variables are held constant. These findings are somewhat different from those of other researchers. In order to be clear about the connection between our research efforts and those of other researchers, we begin with an examination of the relevant literature.",5
41,4,Eastern Economic Journal,29 September 2014,https://link.springer.com/article/10.1057/eej.2014.46,Multipartner Fertility and Child Support,September 2015,Terry-Ann L Craigie,,,Unknown,Unknown,Unknown,Unknown,,
41,4,Eastern Economic Journal,30 June 2014,https://link.springer.com/article/10.1057/eej.2014.32,Student-Athletes? The Impact of Intercollegiate Sports Participation on Academic Outcomes,September 2015,P Wesley Routon,Jay K Walker,,Unknown,Male,Unknown,Male,"As of the 2010–2011 academic year, there were more than 444,000 student-athletes playing for over 18,000 college teams [NCAA 2011]. While some students who play sports at the collegiate level will have the opportunity to play sports professionally after graduation, it is not statistically likely. The likelihood of continuing as a professional athlete after participation in a varsity collegiate sport is highest for college baseball players at 9.7 percent. When considering men’s basketball, football, ice hockey, and soccer, along with women’s basketball, no individual sport exhibits a greater than 2 percent take-up rate into professional play [NCAA 2012]. The vast majority of student-athletes will compete in the same labor markets as other graduates where their athletic abilities are perhaps less important than their academic education. With the large amount of time spent practicing and competing along with personalized attention from the coaching staff, it is possible, indeed likely, student-athletes accumulate different skill sets and achieve different academic outcomes. This study uses propensity score matching (PSM) techniques to explore and quantify the effects of participation in intercollegiate athletics on a variety of outcomes. Outcomes include measures of college GPA, on-time graduation, satisfaction with the college experience, and performance on graduate school entrance exams. The choice to participate in intercollegiate athletics proves to be a significant determinant of some college outcomes. Specifically, we find small, negative impacts on different measures of academic success for student-athletes across sports played, gender, race, and pre-college academic ability. We are not the first to study the academic effects of intercollegiate sports participation. Maloney and McCormick [1993] utilize OLS estimation techniques on data obtained from a single public university, estimating the impact of sports participation on individual semester GPA results. They find negative GPA impacts for student-athletes. This negative effect is larger for “marquee” sport athletes, (American) football and basketball players, with these negative impacts being even larger during the sports season. Fizel and Fort [2004] also analyze the impact of sports participation on GPA, using data from Penn State University. Their findings are much in line with Maloney and McCormick [1993], showing larger GPA deficits for athletes participating in marquee sports and lesser academically qualified athletes being more likely to change majors. Robst and Keil [2000] study the relationship between college athletic participation and academic performance, using data from a single Division III university. They find non-transfer athletes have higher GPAs than non-athletes, while transfer athletesFootnote 1 have grades similar to non-athletes, and that all athletes exhibit higher graduation rates than non-athletes. Other studies focus on longer term impacts of participation in college athletics, trying to determine effects on post-graduation wages and labor market decisions. Long and Caudill [1991] use maximum likelihood techniques and data from the Cooperative Institutional Research Program (CIRP) to estimate the impact of college sports participation on post-graduation income, finding a premium for male athletes and no impact for females. Henderson et al. [2006] study college athletic participation and its impact on post-graduation income using the same data as Long and Caudill [1991]. Their findings point toward wage premiums in business, military, and manual labor occupations, although athletes are also found to be more likely to become high school teachers, which pay relatively lower wages. Olbrecht [2009] utilizes Baccalaureate and Beyond data and quantile regression to estimate wage impacts from college athletics for scholarship athletes and finds there are different effects across the wage distribution. In this study, we re-examine the impacts of intercollegiate sports participation on academic outcomes. The propensity score weighting methodology utilized allows us to construct control groups made up of non-athletes otherwise very similar to student-athletes across a large set of characteristics. We are able to build upon prior work in this area by utilizing a larger, richer, and more varied data set including students from over 400 institutions of higher education. Our data allow for analysis of additional outcomes and a greater degree of control than previous studies. We are also able to delineate between the impacts of participation in marquee (major revenue producing) vs non-marquee sports and also delineate by ethnicity, gender, and pre-college academic ability (as measured by SAT scores) to determine whether impacts of participation in college sports differ across demographic groups and student ability.",5
41,4,Eastern Economic Journal,22 June 2015,https://link.springer.com/article/10.1057/eej.2014.52,What’s Right with Macroeconomics?,September 2015,Peter M Summers,,,Male,Unknown,Unknown,Male,,
41,4,Eastern Economic Journal,30 March 2015,https://link.springer.com/article/10.1057/eej.2015.19,International Handbook on the Economics of Migration,September 2015,Roger White,,,Male,Unknown,Unknown,Male,,
42,1,Eastern Economic Journal,26 November 2015,https://link.springer.com/article/10.1057/eej.2015.52,Liberal Arts Macro Economists are Becoming an Endangered Species,January 2016,David Colander,,,Male,Unknown,Unknown,Male,"Liberal arts macro professors have not always been endangered. Thirty or forty years ago, standard macro theory blended pedagogical, methodological, and historical issues into macro theory, making macroeconomics more undergraduate professor research friendly. Then standard macroeconomic theoretical research was based on IS/LM analysis, as was pedagogy. Standard macro econometric research still included activities such as estimating consumption functions and money demand functions — activities that one could have an honors students do. Undergraduate macro professors could be active participants in the standard macroeconomic theoretical and policy debates. That has changed. Standard macro is now dynamic stochastic general equilibrium (DSGE) analysis. Theoretical and applied macro econometric research has become so technical and specialized that it is beyond what can reasonably teach in an undergraduate liberal arts school. For macroeconomic theory, this is a gain; macro theory is beginning to come to grips with the complexity of the macro economy. But it is not a gain for undergraduate teaching of macro. The problem is exacerbated by the fact that graduate training in macro is not designed to prepare graduate students to become undergraduate professors of economics who combine both research and undergraduate teaching. Graduate economics training in macro is designed to prepare students for full-time research positions at a graduate university or a Central Bank. The result is a very small pool of highly qualified macro-research-focused candidates from standard programs whose goal is to teach macro at a liberal arts school. While the pool is small, it is not zero. There are always a few graduate students who want to teach at a liberal arts program where they can integrate undergraduate liberal arts teaching with their research. So they accept jobs at liberal arts schools. Unfortunately few of them survive to tenure.",
42,1,Eastern Economic Journal,28 July 2014,https://link.springer.com/article/10.1057/eej.2014.37,"Global Evidence on Obesity and Related Outcomes: An Overview of Prevalence, Trends, and Determinants",January 2016,Nadia Doytch,Dhaval M Dave,Inas Rashad Kelly,Female,Unknown,Female,Female,"Obesity is a significant public health challenge facing the United States. Until relatively recently, however, obesity in the United States was fairly rare. During the early 1980s, for instance, prevalence in most of the surveyed states (based on the Behavioral Risk Factor Surveillance System, Centers for Disease Control and Prevention — CDC) was less than 10 percent, and the obesity rate was only about 1–2 percent at the turn of the 20th century [Komlos and Brabec 2011]. However, between 1980 and 2000, the prevalence of obese adults doubled to 34 percent, and the prevalence of obesity for children almost tripled to 17 percent.Footnote 1 Obesity has been identified as a significant risk factor for morbidities such as diabetes, coronary heart disease, stroke, high blood pressure, and several cancers related to the colon, breasts, and the prostate. It is a leading cause of preventable mortality, contributing to approximately 100,000–400,000 deaths annually in the United States, and imposing an economic burden of US$140–215 billion in direct (health-care expenditures) and indirect (productivity losses) costs [Wolf and Colditz 1998; Hammond and Levine 2010; Cawley and Meyerhoefer 2012]. The explosive growth in obesity over a relatively short span of 20–30 years indicates that the weight gain may be fundamentally a result of individual choices in response to the economic environment rather than a shift in individuals’ genetic endowment.Footnote 2 The economics literature has highlighted several factors that may have played a role in the increase in obesity. These include shifts in time constraints associated with changing labor supply and a higher female labor force participation (LFP) rate, higher levels of income, technological progress in food production, home production, and market production, increasing prevalence of fast-food restaurants, advertising of fast food and snacks, potential shifts in time preferences, declining smoking rates,Footnote 3 changing prevalence in major depression, and shifts in health insurance [Chou et al. 2004; Rashad 2006; Chou et al. 2008; Zhang and Rashad 2008; Dave and Kaestner 2009; Kelly and Markowitz 2009; Lakdawalla and Philipson 2009; Andreyeva et al. 2011; Dave et al. 2011]. Some of these possible causes — for instance, technological progress, changes in time and income constraints, and changes in the markets for health care and health behaviors — are associated with economic development. In fact, several studies attribute the increase in global obesity to the advent of capitalism [Wells 2012], and PLOS Medicine recently put together a series on “Big Food” in June of 2012 (see http://www.ploscollections.org/article/browseIssue.action?issue=info:doi/10.1371/issue.pcol.v07.i17), defined as “the multinational food and beverage industry with huge and concentrated market power.” It is argued that although economic development has its benefits, “food systems are not driven to deliver optimal human diets but to maximize profits” [Stuckler and Nestle 2012]. Hence, these potential underlying drivers of the obesity epidemic are not necessarily unique to the United States and the upswing in obesity rates is not just confined to the United States. Obesity has emerged to be a “global epidemic” [World Health Organization (WHO) 2000]. The WHO report views obesity as a significant and growing risk to population health in an increasing number of countries. For many of these countries, obesity has overtaken some of the more traditional problems such as undernutrition and infectious diseases as the most important causes of morbidity and mortality [WHO 2000]. Despite the large literature that has studied obesity in the United States, the underlying and distant causes of the weight gain and some of the associated economic consequences are less well understood [see, for instance, Grossman and Mocan 2011; Dave 2012]. It is even less clear whether, and if so, to what extent these same factors identified for the United States are potentially in operation worldwide. However, deciphering the causes and consequences of the global obesity epidemic, and mounting effective public health interventions, requires a comprehensive accounting of the global scope, scale, and trends in obesity. This overview has also been largely missing in the literature, understandably driven by a dearth of consistent data for multiple countries over long periods. The WHO (2000) report, for instance, is limited to a handful of countries across geographic regions, such as OECD countries [Su et al. 2012], and is particularly limited in its presentation of secular trends. It is also dated in that most of these trends are available only up to the early 1990s for most countries discussed in the report. Trends in obesity, at least for the United States, have somewhat plateaued in recent years; hence, the earlier trends reported by the WHO may also not be reflective of more recent changes. Moreover, data for developing countries are limited in nature and need to be examined more closely; current data indicate that the rising obesity prevalence in these countries in the past several decades has occurred at a much faster rate than that in developed nations [Popkin 2004]. The WHO report also does not present information on outcomes related to obesity nor on the proximate inputs into obesity. The latter limitation is particularly salient given that it is important to understand whether any potential increase in obesity is being driven by an increase in caloric intake or a decrease in caloric expenditure (activity levels), or both. This study addresses these critical gaps, assembles data from a multitude of sources, and provides the most comprehensive and updated cross-national evidence on the prevalence and trends in indicators of obesity, related outcomes such as cholesterol and glucose levels, and proximate inputs into obesity such as caloric intake and physical activity. This evidence spans a diverse set of 228 countries, across the full spectrum of economic development, over 29 years. Specifically, the study has four aims. First, we rigorously document the prevalence and trends in BMI and other obesity-related outcomes, such as glucose and cholesterol levels, as well as a proximate input into obesity — caloric intake, based on models that account for country fixed effects. These models control for all time-invariant country-specific heterogeneity, and make it feasible to statistically test for inter-regional differences in these trends. Second, we estimate conditional trends in BMI also based on the country fixed effects models, and which further account for observable country-specific factors such as real GDP per capita and LFP. Third, we estimate the effects of caloric intake and physical inactivity (as a proxy measure for caloric expenditure) on BMI, drawing on the structural conceptual model of BMI being a function of energy balance. These estimates inform whether the observed trends and inter-regional differences in BMI are driven by changes in caloric intake and/or caloric expenditure. Finally, we also present analyses that inform on some of the indirect drivers of global BMI growth over the past three decades. We specifically focus on changes in real per capita GDP and LFP, and assess how these factors have affected caloric intake, physical inactivity, and subsequently BMI. The estimates from this study indicate that BMI has been steadily increasing across world regions and across the spectrum of economic development, yet considerable heterogeneity remains by gender and across regions. Moreover, the world has experienced decreases in cholesterol levels, suggesting that countries are managing to contain and curb at least some of the adverse effects of the obesity epidemic, though glucose levels have risen concurrently with BMI. Changes in real GDP and LFP can partly explain the increase in BMI through their impact on caloric intake and physical activity levels.",11
42,1,Eastern Economic Journal,09 June 2014,https://link.springer.com/article/10.1057/eej.2014.30,The Macroeconomics of Emission Permits: Simple Stylized Frameworks for Short-Run Policy Analysis,January 2016,Arslan Razmi,,,Male,Unknown,Unknown,Male,"Economics as a profession has, perhaps belatedly, started taking seriously the environmental aspects of policies. An inordinate proportion of that attention has focused on microeconomic analysis of efficiency issues based on representative agent optimization. As pointed out by Daly [1991], this lack of “environmental macroeconomics” is obvious when one looks at textbooks. Environmental issues influence and are influenced by aggregate economic actions along many dimensions. In a world of the future where emission permits are likely to be internationally traded in growing volumes, how do these transactions affect exchange rates, relative prices, and the balance of payments? How do alternative short-run stabilization policies affect a country and the rest of the world both in terms of output and emission costs? Recent literature has begun to pay attention to some of these macroeconomic issues. Heyes [2000] develops a modified closed economy IS-LM model that incorporates an environmental constraint. This constraint takes the form of a curve labeled EE, along which the rate of re-generation of the environment exactly offsets its use, so that the available stock is unchanging. Producers substitute between the use of the environment and capital in production. Raising the interest rate on borrowing for physical capital, therefore, encourages substitution toward less capital-intensive but more environment-intensive production methods. Within this framework, Heyes explores the effects of fiscal and monetary policies. Considering expansionary fiscal policy, for example, the absence of an automatic adjusting mechanism means that an increased level of output and interest rate leaves the economy suspended at a point where the stock of environment is continuously shrinking. This leads Heyes [2000] to conclude that fiscal policy must be complemented by monetary policy to sustain a given stock of environment.Footnote 1 Sim [2006] argues that an automatic adjustment mechanism does exist that would ensure a stable stock of the environment in Heyes’s model. Consider a scenario where the stock is continually degrading. In such a situation, Sim argues, public health is likely to deteriorate. This, in turn, is likely to lead to declining productivity and a halt to output growth. The economy, in other words, hits a natural ceiling, creating conditions for redemptive action to restore the environment. Decker and Wohar [2012] explore Heyes’s model but assuming complementarity rather than substitutability between physical capital and the environment. This changes the relative roles of fiscal and monetary policy. The papers cited above have made useful contributions to the literature on environmental aspects of macroeconomic policies. The treatment of the issue, however, appears to be unsatisfactory in the sense that there is a tension inherent in the framework used. Insofar as the framework assumes underutilized resources, predetermined stocks of physical capital, and fixed prices, the frame of reference must be short run. The assumption of substitution between factors in production and that of a steady state level of the environment, on the other hand, imply long-run closures. A more plausible treatment may be to assume complementarity in the short run, as in Decker and Wohar [2012], but substitutability in the long run, as firms develop alternatives to current production methods.Footnote 2 Moreover, it is arguably more appealing to relax the assumption of a steady state stock of environmental capital for short-run analysis. There appears to be no reason to treat the stocks of capital and environment asymmetrically in this regard. Finally, these papers assume a closed economy thus neglecting issues arising from international trade in goods and assets. This paper tries to develop some of these themes starting with a closed economy but then shifting to a more generalized open economy context. Throughout, I assume fixed goods prices, underutilized resources, a policy-determined amount of tradable emission permits, and — unlike the earlier cited literature — predetermined stocks of both physical and environmental capital, in line with the short-run focus of the analysis. Emission permits have a dual nature, acting as inputs to production for firms and as tradable assets — representing claims on the stock of environment — for speculators. The presence of money and bond markets gives the model an IS-LM flavor, or rather an IS-MP one with the difference of course that emission permit trading is incorporated. Asset demands depend on the rates of return, although trading in emission permits introduces output as a determinant in a non-standard manner. The second section considers an economy that is hermetically sealed from the rest of the world. Thought experiments are carried out with fiscal policy and issuance of emission permits employed as temporary stabilization instruments. The next section shifts the focus to an open economy that is a price taker in the markets for emission permits and produces goods that are imperfect substitutes for foreign goods. Fiscal policy loses efficacy as an instrument for stabilization in this case. The final section extends the imperfect substitutes model to consider a more general two-country world with flexible exchange rates. Feedback effects between the asset and goods markets lead to interesting consequences. In particular, expansionary fiscal policy inflates both countries through repercussion effects while an expansion of emission permits issued, by contrast, turns out to be a beggar-thy-neighbor policy. Thwarting the possible use of the latter by countries facing unemployed resources may, therefore, require global supervision for macroeconomic reasons. In a sense, this paper is an exercise in futuristic thinking. The world considered is one where emission permits are being widely used and traded across countries.Footnote 3 While such a state of affairs at the global level may perhaps lie in the future, some regions are already moving in that direction. The European Union’s Emission Trading Scheme (EU ETS), involving 27 EU countries, is by far the most significant example. The scheme is now in its third phase. The emission allowances (European Union Allowances (EUAs)) issued under this scheme can be banked and traded across time. Figure 1 plots the movement of EUA spot prices since the beginning of Phase 1 (2005–2007). At least two interesting features stand out. First, the price collapsed to almost zero in 2007 at the end of Phase 1 since auctions could not be banked across phases.Footnote 4 Second, the price again fell dramatically in 2008–2009, following the problems in the European economy and the accompanying recession. Thus, the ability to trade claims across time and demand fluctuations emanating from the real side of the economy both matter for allowance prices. I incorporate these features in the following analysis.Footnote 5 Monthly average spot prices of the EUAs, 2005–2011.Source: European Environment Agency (author’s calculations based on http://www.eea.europa.eu/data-and-maps/figures/eua-future-prices-200520132011/eua-future-prices-200520132011-excel-file/at_download/file). Throughout this paper, the emphasis is on simplicity of treatment rather than comprehensiveness, and I consistently eschew paraphernalia in favor of conciseness. The idea is to suggest possible ways to incorporate environmental concerns in the analysis of aggregate (but simple) economies rather than carrying out policy experiments in exhaustive detail.",1
42,1,Eastern Economic Journal,28 July 2014,https://link.springer.com/article/10.1057/eej.2014.36,"The House Vote to Overturn the Moratorium on Offshore Drilling: Jobs, PACs, Ideology, and Spills",January 2016,Leo H Kahane,,,Male,Unknown,Unknown,Male,"Of the many points of debate leading up to the 2008 US Presidential election, one prominent issue was whether or not the United States should increase offshore drilling for oil and natural gas. This debate was an extension of a long-standing view by US Presidents and policymakers dating back to the 1970s that “energy independence” should be a long-term goal for the United States, [Hughes and Flores-Macias 2012]. With regard to the specific issue of offshore drilling, a moratorium on drilling off the west, Atlantic, and Gulf coasts of the United States had been implemented by former President George H.W. Bush in 1990. This ban was later lifted for some areas by his son, President George W. Bush, in 2008 in response to large price increases for oil and gas. A debate ensued as to whether increased offshore drilling (into areas that remained under a drilling moratorium) should be pursued as part of the energy independence policy for the United States. The viewpoint of the Republican candidates John McCain (for President) and Sarah Palin (for Vice President), were made clear by Palin’s campaign slogan: “The chant is ‘drill, baby, drill.’ And that’s what we hear all across this country in our rallies because people are so hungry for those domestic sources of energy to be tapped into.”Footnote 1 Democratic Presidential candidate Barack Obama’s viewpoint during the 2008 campaign changed from one of opposition to increased drilling, to one of measured opening of coastal waters to drilling. Following his election, President Obama announced in March of 2010 the proposed opening of waters off the Atlantic, Alaskan, and Gulf of Mexico coastal areas to drilling — areas that had previously been under a drilling moratorium.Footnote 2 These plans, however, changed dramatically when on April 20, 2010 the Deepwater Horizon drilling rig, operated by BP, exploded killing 11 workers and ultimately spilling in excess of 4 million barrels of oil into coastal waters.Footnote 3 In response to this catastrophe, President Obama announced in December of 2010 the cancellation of the earlier plan to increase drilling and instead implemented a moratorium on drilling for the next 7 years. In opposition to President Obama’s extended moratorium of offshore drilling, Congressman Doc Hastings (R — WA) sponsored the bill, HR 1231, titled the “Reversing President Obama’s Offshore Moratorium Act.” The purpose of the bill was essentially to rescind the Obama moratorium on offshore drilling and to increase exploration in previously protected coastal waters.Footnote 4 The bill was eventually passed by the House on May 12, 2011 in a strongly partisan vote of 243 (222 of which were Republicans) in favor of the bill to 179 (170 of which were Democrats) opposing it.Footnote 5 Those who supported the bill stressed the view that increased offshore drilling would generate significant job growth in both the short and long run. Indeed, during an Energy and Mineral Resources Subcommittee hearing on three of Hastings’ bills (HR1229, 1230, and 1231) designed to increase offshore drilling, economist Joseph Mason provided testimony that these bills would create approximately 250,000 short-term jobs during the exploration and development phase, and about 1.2 million long-term jobs during the production phase.Footnote 6 Those in opposition of the bill cited mostly concerns about the environmental impact of potential oil spills, as well as the notion that focusing resources on the development and discovery of fossil fuels would divert attention away from development and discovery of alternative energy sources. In this paper I study the voting patterns in the House of Representatives on HR 1231 to determine the political and economic factors that may explain how legislators voted. This paper adds to the current literature in several ways. First, the topic is immediately relevant as efforts to open up coastal waters to greater offshore drilling are ongoing.Footnote 7 Second, while policymakers generally face a variety of forces when casting votes (e.g., political party platform vs constituent preferences vs campaign donors), this particular vote entailed one additional component, specifically the legacy of the BP oil spill and the harm it caused to the Gulf Coast areas. Faced with this disaster, legislators who may have otherwise voted in favor of increased offshore drilling may have been compelled to vote against HR 1231. Third, this paper is one of the very few to consider the timing of Political Action Committee (PAC) contributions on the voting behavior of legislators. General findings reveal that increases in PAC money from those who supported (opposed) the bill were associated with a higher (lower) probability that a legislator voted in favor of overturning the moratorium on shore drilling. I also find that the marginal effect of PAC contributions is greater for contributions made just before the vote in comparison with the marginal effect of total contributions received by legislators from the beginning of the year. Taken together, these findings provide further evidence in the political economy literature that contributions by PACs may be aimed at influencing the outcomes of particular votes as opposed to simply attempting to affect the outcomes of elections. The empirical results also show that two (opposing) measures of constituent interests were important determinants of legislator voting. Specifically, legislators from congressional districts located near the Gulf of Mexico, where much of the damage from the 2010 BP oil spill occurred, were less likely to vote in favor of the bill. On the other hand, legislators from districts with a higher employment in the oil extraction sector were more likely to vote in favor of the bill. The remainder of this paper is organized as follows. The next section lays out the political economy model used to explain legislator voting on the HR1231. A description of the data employed and the estimation results obtained is contained in the third section. The fourth section contains some concluding comments along with some suggestions for future research.",1
42,1,Eastern Economic Journal,11 August 2014,https://link.springer.com/article/10.1057/eej.2014.35,An Evaluation of Tax Credits for Residential Energy Efficiency,January 2016,Andre R Neveu,Molly F Sherlock,,Male,Female,Unknown,Mix,,
42,1,Eastern Economic Journal,18 August 2014,https://link.springer.com/article/10.1057/eej.2014.41,Discrimination and Information: Geographic Bias in College Basketball Polls,January 2016,Andrew W Nutting,,,Male,Unknown,Unknown,Male,"According to economic theory, discrimination is either taste-based [Becker 1957] or information-based, that is, statistical [e.g. Aigner and Cain 1977]. Taste-based discrimination occurs when economic agents possess a dislike of certain groups and potentially forgo economic benefits such as profits or wages to avoid them. Information-based discrimination involves economic agents possessing imperfect information about individuals, and therefore assuming they possess the mean characteristics of their group. There may be other forms of discrimination as well. Wolfers [2006] uses the term “mistake-based” discrimination to describe a group being treated differently because its mean characteristics are not correctly known.Footnote 1 This paper examines discrimination in the context of college basketball polls with the goal of distinguishing whether discrimination is taste-based or information-based. Using 1 year’s worth of college basketball polls and games, it first determines whether, in a given week, pollsters rank teams differently based on their geographic proximities to teams. Coleman et al. [2010] used this technique to show that media voters in the 2007 Associated Press (AP) College Football Poll voted teams from their home state and teams from conferences that include own-state teams to higher rankings than other teams.Footnote 2 Fixed effects estimations show that pollsters vote own-state teams and teams that are fewer miles away to better ranks than other teams, especially at the bottom of their polls. Second, this paper examines whether pollsters rank teams in close geographic proximity more accurately or less accurately than other teams. Assuming that ceteris paribus pollsters should aim to create a poll where better-ranked teams defeat worse-ranked teams, a finding that teams were more likely, for example, to lose when pollsters voted them to higher ranks would indicate that pollsters engage in taste-based discrimination favoring local teams. If instead empirical estimations showed that teams were more likely, for example, to win when own-state pollsters placed them to better ranks, it would suggest that pollsters possessed better information about teams in closer geographic proximity and therefore more accurately ranked them. In such a case, own-state and more-nearby teams voted by pollsters to higher ranks would win more often than other teams of the same rank. Ross et al. [2012], using a similar methodology but not utilizing data on individual pollsters, find that college football pollsters overrate teams from the Mid-American Conference and underrate teams from the Southeastern Conference and former Pac-10 Conference. Estimations show that pollsters rank own-state teams and teams that are fewer miles away to higher rankings than other teams. The effect is especially strong at the bottom of the polls. Estimations show no evidence that pollsters overrate teams in close geographic proximity. In fact, evidence suggests pollsters more accurately rank own-state teams and may actually underrate teams that are fewer miles away, especially at the top of their polls. There is also some evidence that pollsters overrate teams in local conferences at the top of their polls, but correctly award them better ranks at the bottom of their polls. This paper adds to the growing literature examining the sources of discrimination. Altonji and Pierret [2001] find that statistical discrimination in labor markets declines over workers’ careers because employers become better informed about individual workers’ productivity levels. List [2004] shows that discrimination against minority groups (women, racial minorities, and the older-aged) in negotiations over baseball card prices constitutes statistical, not taste-based, discrimination. Levitt [2004] finds that contestants on the television game show The Weakest Link exhibit taste-based discrimination against older contestants and statistical discrimination against Hispanic contestants (the latter by believing they are worse players than white contestants). Holzer et al. [2006] find that employment discrimination against black males is reduced when employers can conduct criminal background checks, suggesting such discrimination is statistical. Wolfers and Kumar [2008] discern that male analysts significantly under-predict the performance of firms with female CEOs, suggesting taste-based discrimination. Parsons et al. [2011] find that Major League Baseball umpires call more strikes when the pitcher is of the same race, and believe such discrimination is taste-based because it decreases when umpire monitoring, and therefore the costs of discrimination, rise. Gneezy et al. [2012] show that people are more likely to engage in taste-based discrimination against those who have characteristics — such as obesity and homosexuality — that are believed to be controllable. The remainder of this paper is organized as follows. The next section discusses the data set and details why this paper examines college basketball instead of college football. The section after that empirically determines whether geography affects basketball pollsters’ rankings. The penultimate section determines whether there is a relationship between pollster/team proximity and pollsters’ accuracy in ranking. The final section concludes.",1
42,1,Eastern Economic Journal,17 November 2014,https://link.springer.com/article/10.1057/eej.2014.62,The Effect of Public Transportation Accessibility on Food Insecurity,January 2016,Deokrye Baek,,,Unknown,Unknown,Unknown,Unknown,,
42,1,Eastern Economic Journal,17 November 2014,https://link.springer.com/article/10.1057/eej.2014.60,Lonely Highways: The Role of Social Capital in Rural Traffic Safety,January 2016,Matthew G Nagler,Nicholas J Ward,,Male,Male,Unknown,Male,"Fatalities from traffic crashes constitute a peculiarly rural public health problem. In 2010, rural areas accounted for only 19 percent of the population of the United States, but 55 percent of all traffic fatalities [National Highway Traffic Safety Administration (NHTSA) 2012]. In that year, the traffic fatality rate per vehicle mile traveled in the United States was 2.5 times higher in rural areas than urban areas [NHTSA 2012]. Correspondingly, the traffic fatality risk per capita and per vehicle mile traveled is higher in those states that have a higher percentage of travel on rural classified roads (see Figure 1). Internationally, the rural traffic fatality problem has reached what may be reasonably characterized as a crisis. Within the 13 countries of the Organization for Economic Co-operation and Development (OECD), 60 percent of all traffic fatalities occurred on rural roads in 1996, the most recent year for which international data were available, up from less than 55 percent in 1980 [OECD 1999]. The deaths, on order of 75,000 per year, have been associated with an estimated economic cost of US$135 billion per year. Whereas urban traffic fatalities in the OECD countries declined from 1980 to 1996, the number of rural traffic fatalities increased by 5 percent, suggesting that there is a rural traffic safety problem that “has been neglected over the years … is very serious … and clearly requires priority attention” [OECD 1999, p. 9]. Traffic fatality risk per capita and per vehicle mile traveled vs percentage of vehicle miles traveled on rural-classified roads, 2010. Note: Each dot represents a state. The analysis of rural traffic safety, in view of the current state of knowledge of the key factors that impact it, poses a puzzle. On the one hand, several factors, including risks posed by the rural physical environment, longer response times for emergency medical services, and a culture of risk-taking and risk-denial that leads to a greater tendency among drivers to engage in high-risk behaviors (e.g., speeding, alcohol use) combined with a decreased tendency to engage in protective behaviors (e.g., seatbelt compliance), contribute to higher fatality rates [Ward 2007; Eiksund 2009; Rakauskas et al. 2009; Coogan et al. 2010; NHTSA 2012]. On the other hand, rural communities are perceived as enjoying greater social capital — that is, stronger communal social ties — than urban communities [Weisheit et al. 2006]. The logic of this perception relates to parochialism: the notion that the restricted mobility and insularity promotes reputation, retaliation, and segmentation forces that raise the net benefits to individual behaviors that benefit the larger group [Bowles and Gintis 1998]. Recent studies by Nagler [2013a, 2013b] have connected social capital with improved traffic safety, providing as potential explanations the effects of social capital in increasing courteous driving behavior, encouraging conscientious vehicle choices, and promoting more efficient safety-law enactment and enforcement.Footnote 1 Why, then, if social capital’s protective effects exist equally in rural areas as elsewhere, are the rates of traffic fatality in rural areas so persistently high? In this paper, using data from the United States for the period 1997–2006, we build on the previously observed relationship between social capital and traffic fatality risk by examining the implications of this relationship within rural environments. We begin by exploring briefly whether there is truth to the notion that social capital is greater in rural areas; for this purpose, we examine a simple bivariate correlation between state-level social capital measures and various proxies for a state’s “ruralness.” Our tentative finding from this exploratory analysis is that social capital is in fact no higher in states that are “more rural.” We move on to the core of our investigation, in which we apply and extend the econometric approaches used by Nagler [2013a, 2013b] to examine whether social capital’s protective effects have equivalent force with respect to fatalities occurring on rural and urban roads. To probe the question further, we explore the potential mediating effects of specific crash types and contexts and of behavioral risk factors on the relative protective effect of social capital on rural and urban roads. We find that social capital has a significantly lower protective effect on rural roads than urban roads; in fact, the protective effect is altogether insignificant on rural roads. Potentially relevant differentials in crash-type (i.e., single- vs multi-vehicle crash) and crash-context (i.e., junction- vs non-junction-related) exposure between rural and urban road settings do not mediate this outcome. Rather, our findings are consistent with the possibility that the relative prevalence of certain risk factors, such as speeding, skew rural environments toward crash situations in which the critical safety factors are orthogonal to social capital influence. The next section introduces our data. We then proceed to describe our empirical strategy. Results are presented, and then subsequently discussed. A brief final section concludes.",1
42,1,Eastern Economic Journal,31 August 2015,https://link.springer.com/article/10.1057/eej.2015.24,"The Great Escape: Health, Wealth, and the Origins of Inequality",January 2016,Scott Carter,,,Male,Unknown,Unknown,Male,,1
42,1,Eastern Economic Journal,23 March 2015,https://link.springer.com/article/10.1057/eej.2015.15,Climate Change and International Trade,January 2016,Jonathan F Cogliano,,,Male,Unknown,Unknown,Male,,1
42,2,Eastern Economic Journal,26 February 2016,https://link.springer.com/article/10.1057/eej.2015.57,"Tools, Not Rules: Are We Teaching the Wrong Principles of Economics in the Introductory Course?",March 2016,David Colander,,,Male,Unknown,Unknown,Male,"Currently, the principles of economics are often presented in a way that does not embed the nuance in the principle so that when the students learn the principle, they also learn the nuance. It doesn’t have to be that way. In this section I list 10 standard principles of economics and then suggest a nuanced way to present them. I also provide a brief discussion of how the nuanced principle differs from the standard principle and why the nuanced presentation is preferable. In the table at the end of the discussion, I contrast my nuanced principles with the principles as they are often presented in the principles’ texts. The trade-off principle is central to economics. The nuance that needs to be incorporated into this principle is that the process of making trade-offs is much more complicated than can be presented in the beginning course in economics. It is because it is more complicated that economists’ cutting edge research in behavioral economics is working on expanding our understanding of real world decision making. Presenting choice analysis as a clear-cut and simple principle, as principles textbooks often do, is itself just a trade-off made by textbook writers. Students will have a much better sense of how to apply economics if they are told right from the get-go that the models they learn in economics miss many dimensions of decisions. A nuanced presentation of the trade-off principle would include a caveat that reminds students that once in a while, a win-win situation is possible. So my suggested nuanced trade-off principle is: (1) There is no such thing as a free lunch, but once in a while one can snitch a sandwich. The opportunity cost principle reinforces the “no free lunch” principle, and also reminds students to consider implicit, as well as obvious, costs (and benefits). But implicit costs and implicit benefits have many dimensions, and an appropriate use of economist’s cost-benefit framework needs to remind students that before they apply cost-benefit analysis, they need to take these many dimensions into account. The nuance that needs to be built into the principle is that the cost-benefit economic framework is a tool, not a rule. (2) Opportunity cost focuses economist’s cost-benefit framework on non-obvious as well as obvious tradeoffs, and serves as a useful heuristic for decision making. The rationality principle is fundamental to the teaching of economics. We all tell students that a rational decision maker takes action if the relevant benefits of taking the action exceed the relevant costs. What we don’t tell students is that defining rationality in a non-tautological way is really difficult, and the models we teach them lose many dimensions of people’s decision-making process. A key example occurs when the rationality principle is embedded in marginal decision making. The “benefits exceed costs” decision rule applies whether or not they are marginal costs or non-marginal costs. The “marginal” nature of the costs is relevant for decisions that involve marginal decisions, and that have the correct second-order conditions. Many real world decisions are not marginal decisions, in which case the standard marginal formulas don’t apply directly. Since, in reality, we have no way of defining what is rational, or what costs and benefits to a person are, the “rational choice” principle is best presented as a tautological framework that is extremely useful if treated as what it is: a useful heuristic — a tool, not a rule. (3) Framing a question in a cost-benefit lens is useful as long as the lens is seen as a tool, not a rule; the economy is a complex system requiring multiple lenses to capture its many dimensions. Good economists always qualify their models with “other things equal.” The incentive principle is also best seen as a tool not a rule. It is trivial to say that people respond to incentives. (Yoram Bauman (https://www.youtube.com/watch?v=LPUEgEdCTp8) wonderfully describes it as a tautological tautology.) The relevance of this tautology is embedded with the nuance: A nuanced presentation of the incentive principle would emphasize that price incentives — the incentives economic models focus on — are just one of a set of incentives; there are many other types of incentives as well. Moreover, the actual incentives that people respond to are embedded in institutions that the simple principles’ models don’t capture. So extreme care should be used when extending this principle to policy questions. A more nuanced incentive principle is: (4) Most people are driven by enlightened self-interest in achieving their personal and social goals. As the costs and benefits of achieving these goals change, behavior changes. Trade and specialization are central to economics, and the ability to trade is what Adam Smith said separated man from animals. But trade is best understood as part of a broader system of social cooperation; trades that occur in markets are only one form of cooperation among people. A nuanced “trade is good” principle would emphasize that cooperation can make society better off, and that trade in markets is an important way in which cooperation leads to coordination. It would also point out that cooperation inevitably requires difficult trade-offs. The nature of that coordination should be seen as having many dimensions, of which market trades are but one. (5) Trade is a form of cooperation and cooperation can make everyone better off. Complicated trades require complicated institutional and social foundations. This principle can lead students to miss the interconnection between markets and the government. The nuance that is missing is that legal and cultural support of the property rights upon which markets are built are necessary for complex markets to operate. Thus the government and markets are intricately entwined; they should not be seen as opposites. Effective markets exist where the difficult decisions about distribution of property rights have been answered by the society and government protects and supports the market. The efficient use of markets requires a deep level of planning by the government that involves creating an ecostructure conducive to markets and other bottom-up solutions to problems. (6) Markets offer a useful way for societies to coordinate actions. The efficient use of markets evolves over time. One aspect of government policy involves creating an ecostructure conducive to markets and other bottom-up solutions to problems. The standard presentation of the externality principle often makes it sound as if markets exist independently, and are not grounded within a cultural and political setting that is itself created by the government. The government and market are intricately entwined. Public policy involves much more than just correcting market outcomes. It involves influencing the evolution of markets so that top-down interventions into market outcomes are not necessary. There are arguments for and against the government intervention that extend beyond the model. Not only are there market failures and government failures, there are also failures of market outcomes and failures of government outcomes. (7) Government and the market are intricately entwined. Public policy involves much more than just correcting market outcomes. It can also involve influencing the evolution of institutions so that top-down interventions into market outcomes are less necessary. The “real” principle is designed to get students to look behind the veil of money and to think about real goods and services. Often the “real” principle is presented in a way that it is tautologically true and thus says nothing but that only when you produce more do you have more. The potential lack of nuance occurs when the focus of the “real” discussion is on goods and services only, as if material welfare were all that mattered. Economics is part of a broader set of social issues, of which the goods and services measured by GDP and productivity are only one aspect of society’s welfare. GDP, which is often used as a focus of macroeconomic policy, is not a good measure of these broader aspects of welfare. What is “real” is decided by people, not by accountants. (8) A country’s well-being depends on more that the material welfare that economics focuses on. The goods and services measured by GDP and productivity measures are only one aspect of social welfare. GDP misses many of these broader aspects. Often, the money illusion principle is presented as one in which increases in money cause increases in prices. Presented this way, the money illusion principle is not only not nuanced, it is seriously misleading. There are two serious problems with it. First, it does not describe recent experience. In recent years, the government has created enormous amounts of money, and prices have not risen. The second is that it directs students to see only the demand side, and leaves out the supply side. My alternative tautological statement that captures the problem of inflation is that “Prices rise with price setters have incentives to raise their prices.” This provides a more nuanced principle for students that directs them to think about both supply and demand forces in the aggregate economy. (9) Ultimately, the amount of real goods in the economy, not the amount of money in the economy, limits the amount that society can consume. Prices rise when sellers have incentives to raise their prices and demanders are willing to pay those higher prices. This principle is often presented as meaning that there is always an inflation/unemployment trade-off. Presented in that way it is a lousy general principle. Inflation and unemployment are intricately connected, but not in any simple relationship as this principle suggests. The macro economy is best seen as a complex system in which supply and demand forces are intertwined, and many different outcomes are possible. Economists don’t have good theories of the macro economy, and students should be told that. We have some general heuristic models that help guide policy, but they are highly imperfect. Often policies that are helpful in the short run are not helpful in the long run and vice versa. A more nuanced principle would emphasize that in the aggregate supply and demand forces are intertwined. So I would replace it with: (10) Aggregate supply and demand forces are intertwined; many possible outcomes are possible. Economists don’t have good theories of the macro economy.",6
42,2,Eastern Economic Journal,26 February 2016,https://link.springer.com/article/10.1057/eej.2015.58,The Tradeoff between Nuance and Clarity,March 2016,N Gregory Mankiw,,,Unknown,Unknown,Unknown,Unknown,,
42,2,Eastern Economic Journal,30 June 2014,https://link.springer.com/article/10.1057/eej.2014.34,A Comparison of Inequality and Living Standards in Canada and the United States Using an Extended Income Measure,March 2016,Edward N Wolff,Ajit Zacharias,Andrew Sharpe,Male,,Male,Mix,,
42,2,Eastern Economic Journal,20 October 2014,https://link.springer.com/article/10.1057/eej.2014.50,Globalization and the Labor Share in the United States,March 2016,Juann H Hung,Priscila Hammett,,Unknown,Female,Unknown,Female,"The distribution of income between labor and capital has long been an issue of interest to economists [e.g., see Smith 1776; Ricardo 1817; Keynes 1939; Kuznets 1933, 1959, 1966; Solow 1957, 1958; Kaldor 1961; Kravis 1962, 1968]. Even though that interest had subsided significantly since Kaldor [1957] observed that factor shares of income remain roughly unchanged over a long period, it has again resurged in the past two decades. Research interest in this issue has revived in part because the labor share appears to have been declining among industrial countries since the early 1980s. In the United States, that decline came later; but it too became noticeable since the early 1990s and especially after 2000. On the political-economy front, this downward trend in the labor share catches attention because of its negative implications for social cohesiveness and even political stability. On the analytical front, the development also poses two major challenges to economists. First, it means economists can no longer safely assume that a Cobb–Douglas production function with constant coefficients is a reasonable approximation for the aggregate economy [e.g., see Karabarbounis and Neiman 2012]. Second, given that labor income and capital income are subject to different tax rates, the loss of a long-term anchor for forecasting the labor share of income results in a higher uncertainty for the projection of government budget. For those reasons, research interest has risen in several related issues: Is there really a trend decline in the labor share since the late 1970s, or was it just a return to the “normal” from its unsustainable high level in the 1960s? If indeed there is a long-term decline in the labor share in most advanced countries, is it mainly driven by the rapid rise in globalization (including labor migrations, trade flows, cross-border investments, offshoring of services, and integration of global supply chains), as alleged by many? Or, was it mainly due to the technology shift toward labor-saving production? Those who regard globalization an important contributor to the decline in the US labor share have made the following arguments [e.g., see Rodrik 1997; Feenstra 2010; Spence and Hlatshwayo 2011; Alpert et al. 2011]. First, imports from low-wage countries have risen rapidly since the early 1980s, thereby lowering US firms’ price competitiveness in labor-intensive products. According to the theorem advanced by Stolper and Samuelson [1941], because the United States is labor-scarce relative to those low-wage countries, this shift in the composition of US trading partners will lead to the fall in both the real wage and the labor share in the United States.Footnote 1 Second, the increase in capital mobility not only allowed firms to move their production abroad more easily but also increased capital owners’ bargaining power in wage negotiations, thereby slowing the growth of jobs and wages in the home country. Third, US industries have increasingly been tapping lower-wage labor abroad through the global-supply chain, thereby eroding job growth and suppressing real wages. How valid are the above claims? Some economists — for example, Bhagwati et al. [2004] and Lawrence [2008] — did not agree that globalization was a major culprit for the fall in the labor share. Is it because their data no longer reflect the new reality? Against this backdrop, this paper examines the impact of globalization on the labor share of national income in the United States. We argue that globalization can influence the national labor share (NLS), even though its impacts first occur to the tradable sector. The argument includes a review of the two familiar channels — the Stolper–Samuelson channel and the rent-sharing channel. In addition, this paper illustrates a new channel — the technology-shifting channel — that is likely to have become important in this age of offshoring and global supply chain. We then use a data set comprising 18 industries from 1999 to 2009 to estimate the effect of globalization on the labor share in the US manufacturing sector, the most tradable sector in an economy. We find that that globalization had a net negative effect on the labor share in the US manufacturing sector, with negative effects of some globalization indicators dominating positive effects of other globalization indicators. We also find that total-factor productivity (TFP) growth had a negligible effect on the labor share during our sample period. A particularly interesting finding is that an increase in the offshoring of jobs (i.e., the number of workers in US affiliates abroad relative to the number of workers in US parents) yielded a modest net increase in the labor share in 2 years, though not in the first year. This result suggests that the offshoring of low-skilled jobs not only improved the pay of those high-skilled jobs retained at the parent companies, but also created other better jobs at home over time. Altogether, globalization is estimated to have lowered the manufacturing sector’s labor share by 3.1–5.8 percentage points between 1999 and 2009, amounting to 20–36 percent of the decline in the NLS in that period. However, this simple tally very likely underestimates the total effect of globalization on the NLS. This is because, by design, our estimations do not capture the indirect effect of globalization on the labor share of the non-tradable sector through the channels discussed in the paper. Even though this paper does not offer an estimate of those indirect effects, the existing literature suggests that they are negative and significant. The organization of the rest of this paper is as follows. The next section discusses three theoretical channels through which globalization may influence the labor share and whether their validities find support in the data. The section after that discusses why other factors — such as technology shift, the business cycle, and labor-market rigidity — may also influence the labor share. The penultimate section conducts the estimation and discusses empirical results. The final section concludes.",13
42,2,Eastern Economic Journal,02 June 2014,https://link.springer.com/article/10.1057/eej.2014.29,Third-Province Effects on Inbound FDI: Evidence from Chinese Provinces,March 2016,Mingming Pan,,,Unknown,Unknown,Unknown,Unknown,,
42,2,Eastern Economic Journal,25 August 2014,https://link.springer.com/article/10.1057/eej.2014.43,"Family Composition and the Benefits of Participating in the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC)",March 2016,Christina Robinson,,,Female,Unknown,Unknown,Female,"Societal concerns over the health issues faced by children living in low-income and impoverished families have grown substantially over the past several decades. These concerns are not without merit as recent research indicates that children living in low-income families are at greater risk than other children. They are, for instance, less likely to have private health insurance, more likely to have public insurance, and more likely to be uninsured than children living in other families. As a result they are less likely to receive physician care when ill and are also less likely to receive preventative or well-child care. At the same time, these children are more likely to live in food insecure households, be overweight or obese, have asthma, diabetes, ADHD or other learning disabilities, and to suffer delayed development of speech and motor skills [Magnuson and Votruba-Drzal 2009; Seith and Isakson 2011; Joyce et al. 2012]. If these issues are not addressed in childhood it is likely that low-income children will enter adulthood at a disadvantage, as the consequences of poor health (in childhood) are often persistent. Adults who experienced poverty as children are, for example, more likely to have arthritis, asthma, diabetes, heart disease, and hypertension [Magnuson and Votruba-Drzal 2009; Ziol-Guest et al. 2012]. They also tend to have completed fewer years of school, work fewer hours per year, and have lower annual (and lifetime) earnings [Duncan et al. 2012]. To help combat these tendencies there has been an expansion of governmental programs designed to reduce the number of children in poverty and improve the health outcomes of low-income children. These programs, however, are often costly with budgets that ranged from US$6.7 to $68.3 billion in 2010. Given the high financial costs of these programs and the increasing demand for assistance, the efficacy of such programs has become an issue of interest to policymakers. One program that has received a considerable amount of attention is the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC), which has seen a 27 percent increase in enrollment over the past decade [USDA 2011]. The program’s primary mission is to safeguard the health of pregnant/postpartum women, infants, and children under the age of 5 who live in households with a combined family income below 185 percent of the federal poverty level, and who are believed to be medically or nutritionally at risk. To meet its goals the program employs a three-pronged approach providing participants: access to vitamin and nutrient rich food bundles targeted to their unique dietary needs; nutrition counseling and education; and referrals for health-care services. Existing research indicates that WIC participation improves the nutritional quality of a child’s diet, reduces the incidence of nutrition related health problems, and reduces the probability that they are abused or neglected [e.g., Rush et al. 1988; Rose et al. 1998; Oliveira and Gundersen 2000; Wilde et al. 2000; Siega-Riz et al. 2004; Lee and Mackey-Bilaver 2007]. Although WIC’s focus is on promoting the health and nutrient consumption of those participating in the program, research has indicated that the benefits provided by WIC may be shared with non-WIC family members. The first evidence of benefit sharing was provided by Basiotis et al. [1998] who found that WIC families (as a unit) consumed a more nutritious diet than otherwise similar non-WIC families. More recently, older children (i.e., children over the age of 5) who lived with a WIC participant were shown to consume a healthier diet than those who did not [Ver Ploeg 2009; Woodward and Ribar 2012]. Living with at least one WIC participant was also shown to improve the overall health of teenage males [Robinson 2013]. Combined, these studies identify the presence of a benefit spillover and indicate that the active participant is not receiving the full nutritional benefit afforded to them by WIC. The present paper extends our knowledge of WIC spillovers by examining the impact siblings have on the health benefits received by WIC-enrolled children (i.e., those between 1 and 5 years of age).Footnote 1 Despite knowledge that benefit spillovers take place, the impact of the spillovers on a participating child’s health has remained ambiguous. This uncertainty is driven primarily by the fact that health is a composite measure determined by a variety of genetic, demographic, and environmental factors. Thus, the relationship between a family’s composition, a child’s WIC participation status, and their health is complex — depending only in part on the nutritional quality of a child’s diet. One possibility is that all enrolled children receive the same health benefit(s) from WIC, regardless of whether or not the foods provided by the program are shared with others. In this case the benefits of WIC would extend to non-WIC family members without imposing a cost on the recipient. Alternatively, the reduced consumption of WIC foods could result in the participating child receiving a smaller health benefit than children without siblings. Should this prove to be the case, the spillover is imposing a cost on the participant and policy alternatives designed to mitigate the adverse effects should be considered. It could also be the case that the spillover generates an additional benefit for WIC-enrolled children. Households turning to WIC for assistance are, by definition, experiencing a period of economic stress and are often food, housing, or energy insecure as well. These conditions have all been found to adversely affect a child’s physical health, social–emotional competence, and cognitive ability [Gershoff et al. 2007; Joyce et al. 2012]. If the sharing of WIC foods reduces the stress and hardship faced by the family the result may be a healthier, more stable, and more nurturing living environment for the enrolled child. This, in turn, could lead to an improvement in a child’s overall health, despite the fact that they are not receiving the full nutritional benefits of WIC participation. If the enrolled child’s health improves more than the health of children without siblings then the spillover generates an additional benefit. This ambiguity is an issue that has important policy implications and as such merits careful consideration. To this end, two separate, yet related analyses are conducted. In the first the sample is stratified based on the presence of a sibling, and the impact a child’s WIC participation has on their health is considered.Footnote 2 Results indicate that WIC participation increases the probability of an only child’s health improving and decreases the probability of their health worsening. WIC benefits do not have the same effect for children with siblings, which suggests that a family’s composition impacts the health benefits received by WIC participants. To further examine the impact a family’s composition has on the benefits received from WIC participation, the possibility that it is not only the presence of other children that matters but also the characteristics of those children is investigated. It is possible that siblings of different ages and different genders contribute differently to family dynamics and resource procurement, which in turn may influence the health benefits an enrolled child receives from program participation. Older children, for example, may be able to work and help provide for the family, in which case a (benefit) spillover may magnify the impact WIC benefits have on the participating child. Results from this part of the analysis indicate that WIC children who live with teenage males are more likely to experience an improvement and are less likely to experience a worsening in their health than children with teenage sisters or younger siblings of either gender. This suggests that the spillover identified by Robinson [2013] has a positive effect on the WIC recipient. Additional economic research evaluating how WIC benefit allocations can be adjusted to encourage similar outcomes in other families should be conducted and incentive compatible policies should be encouraged. Combined, the results from the two estimations described above indicate that the foods provided by WIC are not allocated as prescribed, which suggests that a family’s composition should be taken into account when benefit allocation decisions are made and WIC food bundles are designed. The remainder of this paper is organized as follows: the next section provides a brief outline of related literature. The third and fourth sections are (respectively) devoted to the estimation strategies described above and are followed by a section that discusses concern over possible self-selection bias. The final section offers concluding observations about the role family composition plays in the realization of health benefits from the WIC program.",3
42,2,Eastern Economic Journal,25 August 2014,https://link.springer.com/article/10.1057/eej.2014.45,Improved Targeting of Social Programs: An Application to a State Job Coaching Program for Adults with Intellectual Disabilities,March 2016,Melayne Morgan McInnes,Orgul Demet Ozturk,Joshua Mann,Unknown,Unknown,Male,Male,"Social programs are increasingly asked to find more efficient ways to allocate resources, but, practically speaking, how can this be done? Re-allocating resources to improve efficiency requires knowledge of who would benefit most. Economic program evaluations typically focus on estimating the effects of the marginal program dollar, but recent work on estimating the distributional treatment effects allows us to identify who wins and who loses from program participation [Heckman and Vytlacil 2005]. In this paper, we go one step further and use this modeling approach to estimate the gains from several counterfactual resource allocations. This kind of analysis can help policymakers identify whether there are feasible ways to make existing program resources go further. In a climate of tightening government budgets, we think this can be a very useful policy tool. We illustrate the potential for gains from redirecting resources using data from a state job coaching program that is designed to increase employment among adults with intellectual disabilities (IDs). IDs are disabilities that originate before age 18 that are characterized by significant limitations in both intellectual functioning and adaptive behavior (including everyday social skills and practical skills). Supported employment programs, including job coaching, have been encouraged under federal policy since the Developmental Disabilities Assistance and Bill of Rights Act (DDA) of 1984 and are present in every state. The goal of job coaching is to help individuals with severe disabilities find stable employment in integrated community settings rather than work environments that employ only people with ID. Job coaches provide a range of services (from individual skills assessments to on-site job training) to help overcome barriers to community employment. Job coaching programs have been shown to significantly increase employment [Cimera 2007] even after controlling for potential endogeneity of program participation [McInnes et al. 2010], but we do not know how these gains are distributed among current participants and eligible non-participants. To identify the distribution of treatment effects, we model the uptake of job coaching and employment outcomes. Our model allows for observationally equivalent individuals to have heterogeneous responses to job coaching. Since differences in expected gains from job coaching may also affect the likelihood of participation, we begin with the approach suggested by Aakvik et al. [2005]. They develop a model to analyze the effects of a vocational rehabilitation program that allows unobserved gains to affect the likelihood of program participation. This model allows us to estimate the counterfactual outcomes for everyone, as well as the distribution of the potential and realized gains. Using these estimates, we assess the employment effects of possible reallocation of job coaches. The optimal allocation provides job coaches to those who gain the most. Since gains may not be observable to program evaluators, we consider a second best scheme in which the program is assessed based on the employment success of participants. If administrators target employability rather than gains, there can be an unfortunate outcome in which the program appears to be successful because participants are employed but population employment is lower. Our simulations allow us to quantify the costs of using a second best scheme relative to the optimal one. In addition, we consider a well-intentioned policy objective of targeting those who are least employable. We use a unique data set collected in South Carolina from 1999 to 2005 for all individuals receiving any service from the Department of Disabilities and Special Needs (DDSN). The data includes information on individual characteristics (including IQ, age, gender, race, and an indicator for emotional or behavioral problems), participation in job coaching, and employment outcomes. Because the goal of job coaching is stable employment, our employment measure excludes jobs for short duration (less than 26 weeks) or very low pay (less than US$50 per week). The estimates from our model show that while the treatment effects from the job coaching program are positive and significant, they are not maximized. We find that the Average Treatment Effect (ATE) is greater than the Treatment Effect on the Treated (TT). This arises when the program is less likely to reach those with the most to gain from participation. For example, we find that individuals with emotional and behavioral problems are less likely to be employed and coached, but they gain more from coaching (everything else held constant). In our simulations, we estimate that employment can be increased by as much as 56 percent by a program administrator who can perfectly target gains. A more achievable goal of targeting based on observable characteristics is estimated to increase population employment by roughly 11 percent. To put these potential employment gains in perspective, a 50 percent expansion of the program as currently deployed (i.e., assuming no change in the way program participants are selected) would achieve a 12 percent increase in population employment. We begin by describing supported employment and job coaching in the section “Supported employment and job coaching” and then discuss our data and variables in the following section. The section “Model and estimation” describes the latent variable models of program participation and employment and our estimation strategy. The section “Estimation results” reports the estimation results, which are used to conduct the policy simulations described in the section “Policy simulations.” Our conclusions are discussed in the final section.",1
42,2,Eastern Economic Journal,20 October 2014,https://link.springer.com/article/10.1057/eej.2014.51,The Impact of Retirement on Smoking Behavior,March 2016,Padmaja Ayyagari,,,Unknown,Unknown,Unknown,Unknown,,
42,2,Eastern Economic Journal,19 May 2014,https://link.springer.com/article/10.1057/eej.2014.26,Are Fulbright Applicants Idealists or Opportunists?,March 2016,Carrie Gill,Corey Lang,,Female,,Unknown,Mix,,
42,2,Eastern Economic Journal,03 August 2015,https://link.springer.com/article/10.1057/eej.2015.39,Returns on Indian Art during 2000–2013,March 2016,Jenny R Hawkins,Viplav Saini,,Female,Unknown,Unknown,Female,"In this paper, we analyze price trends in an important emerging art market: the market for modern Indian art. Although the bulk of the paintings currently categorized as modern Indian art were produced between the early 20th century and the 1980s, the name for the category did not come into existence until the late 1990s. Until then, what is now known as modern Indian art was lumped together with antiquities and folk art in “mixed auctions.” With the exception of one auction in 1995, the major auction houses of Christie’s and Sotheby’s did not have regular auctions entirely devoted to modern Indian art until the early 2000s. However, around 1995, a consensus emerged among art critics and historians, recognizing modern Indian art as an independent artistic movement. Consequently, during 1995–2007, the average price of an Indian painting at auction increased from $6,000 to $44,000 [Khaire and Wadhwani 2010]. Currently, Indian art prices are considered roughly comparable to prices for art from Latin America, with several paintings fetching prices in excess of $2 million. Crucial to the establishment of modern Indian art as a recognized category was the formation of the firm Saffronart in 2000. Headquartered in Mumbai, with additional offices in London, New Delhi, and New York, Saffronart was the leading auctioneer of Indian artwork during 2000–2007, with a market share of at least 41 percent, compared to 25 percent each for Christie’s and Sotheby’s [Khaire and Wadhwani 2010]. From November 2000 to June 2013, the firm sold around $188 million worth of artwork. We use data on these sales to document the trend in the Indian art market by calculating a price index. We do so by estimating a hedonic regression, a methodology that has been employed extensively in the existing literature to study price trends in auctions for Western artwork.Footnote 1 Our work offers the first rigorous analysis of price trends in the Indian art market, as well as the effects on it of the global financial and economic crises of 2007–2009. The price of Indian art conveys relevant information about the demand and supply of an important cultural good for a country that by 2050 will become the most populous in the world. With the world’s two largest auction houses, Christie’s and Sotheby’s, along with Saffronart, now dedicating auctions solely to Indian art, market activity is growing. However, little research has investigated the development and outcomes of this market. In fact, to date no econometric analysis of the long-term price trend in the Indian art market has been done, and our paper fills this gap.Footnote 2 More broadly, our paper contributes to the research program on understanding the prices realized at art auctions all over the world. Ashenfelter and Graddy [2006] present a detailed survey of this literature. They emphasize that using an index to understand trends in art prices is useful in determining the profitability of buying art as an investment. The rate of increase of the price index can be taken to be a rate of return on holding art, which can then be compared to the rate of return on other assets. Campbell [2008] finds that art can exhibit low correlation with certain other asset classes, which can make investing in art part of a portfolio diversification strategy. Our full data set consists of 5,514 paintings and drawings by Indian artists that were auctioned by Saffronart during 2000–2013. The average painting sold for $47,617, and the total value of sold paintings is about $170 million. The record sale price in our data is $2.22 million for the painting Wish Dream by Arpita Singh. Saffronart’s auction method has two novel features. While the auctions are in the open-ascending format, they are conducted online, similar to eBay.com’s auctions.Footnote 3 Moreover, the auctions for all objects on a given day begin simultaneously, unlike typical in-house auctions. Prior to each auction, Saffronart conducts preview exhibitions in cities like London, New York, and Mumbai, where potential bidders can physically examine the paintings. Buyers can pay in either Indian rupees (INR) or US dollars (USD). Our analysis considers two types of Saffronart auctions: major auctions and absolute auctions. Saffronart annually conducts four major auctions; the average number of lots at a typical auction is 105. At these auctions, each artwork has a secret reserve price. Saffronart posts a low and high estimated price for each painting, and it is understood that the secret reserve price does not exceed the low estimate. In addition to the major auctions, which feature major works by mainstream painters, Saffronart also conducts several “absolute” auctions each year. Otherwise similar to the major auctions, these auctions usually contain minor works of major names, or works by less well-known artists. The main novelty of absolute auctions is they do not have reserve prices, so the object is always sold to the highest bidder. The mean winning bid within absolute auctions is $6,504, although 10 percent of the paintings sell for more than $10,000, with the highest observed winning bid of $184,000. While major auctions constitute the bulk of the data, absolute auctions account for about 16 percent of recorded sales. The absolute auctions sometimes have other artworks in the form of prints, photographs, sculpture, ceramics, digital art, calligraphy, and books. Since these diverse artworks vary in average value, demand, and supply, we focus only on the paintings and drawings (described henceforth as paintings) sold at these auctions. We further restrict our data to paintings from artists who sold 15 or more pieces during 2000–2013, which covers 72 artists well-represented in the data set. This list includes most major names in Indian art, as well as many lesser known painters. Restricting our data in this manner leaves 3,572 paintings successfully sold during November 2000 to June 2013, which we use for estimation. For additional details regarding Saffronart’s auction process and our data, we point the reader to our working paper, Hawkins and Saini [2014]. Art historians often categorize Indian painters in two distinct stylistic groups, modern and contemporary, although works by both are termed “modern art.” Modern painter usually denotes painters born before or around 1940 (India became independent in 1947) and includes artists like M.F. Husain, Tyeb Mehta, F.N. Souza, S.H. Raza, V.S. Gaitonde, Ram Kumar, Jogen Chowdhury, and others. The term contemporary denotes painters born after 1940; some important names are Atul Dodiya, Subodh Gupta, Jitish Kallat, Anju Dodiya, and Shibu Natesan. Of the 72 well-represented painters in our data, 36 happen to be categorized as modern and 36 as contemporary. Table 1 details statistics for sale prices by auction type (major or absolute) and artist type (modern or contemporary). Modern painters have the larger market share, accounting for 2,447 sales, as opposed to 1,125 for contemporary painters. Modern paintings also sold at a higher average price ($53,114 vs $35,660), and this difference is statistically significant (p<0.001). In addition to the sale prices and the low/high estimates, we have the following data for each sold painting: title, painter name, type of paint (e.g. oil, watercolor, ink), surface medium (e.g. canvas, paper, cardboard), height and width in inches, category (painting or drawing), style (e.g. figurative, abstract, landscape), and whether the painting is signed and/or dated. Further, we know the painting date for 2,949 of the 3,572 paintings in our sample. Table 2 presents some summary statistics. The dead variable takes value 1 if the painter is dead and 0 if alive. The signed dummy takes value 1 if the painting is signed and 0 otherwise. To measure uncertainty regarding the valuation of a painting, we divide the spread of the estimate (defined as high estimate minus low estimate) by the estimate (defined as the average of the low and high estimates), and take its log to arrive at log(spread/estimate). The larger this ratio, the less certainly the auctioneer (and the market) likely regards the value of the painting.",2
42,2,Eastern Economic Journal,22 June 2015,https://link.springer.com/article/10.1057/eej.2014.53,"Mass Flourishing — How Grassroots Innovation Created Jobs, Challenge and Change",March 2016,Anusua Datta,,,Unknown,Unknown,Unknown,Unknown,,
42,2,Eastern Economic Journal,22 June 2015,https://link.springer.com/article/10.1057/eej.2014.56,Legacies of the War on Poverty,March 2016,Sophie Mitra,,,Female,Unknown,Unknown,Female,,
42,2,Eastern Economic Journal,08 June 2015,https://link.springer.com/article/10.1057/eej.2015.16,Handbook of Research on Gender and Economic Life,March 2016,Erin E George,,,,Unknown,Unknown,Mix,,
42,3,Eastern Economic Journal,13 April 2015,https://link.springer.com/article/10.1057/eej.2015.7,Are Pornography and Marriage Substitutes for Young Men?,June 2016,Michael Malcolm,George Naufal,,Male,Male,Unknown,Male,"The United States has seen considerable demographic changes with respect to family arrangements over the last few decades. Between 1950 and 2010, the rate of marital formation dropped by 39 percent, with a 17 percent drop between 2000 and 2010 alone.Footnote 1 Today, the proportion of men between 25 and 34 years old who have never been married is more than six times higher than it was in 1970. For men between 35 and 44 years old, the increase has been more than fourfold. Marriages that do form are about twice as likely to end in divorce today as in 1950. Traditionally, one of the reasons to enter into a marriage was sexual gratification. But as options for sexual gratification outside of marriage have grown, the need for a marriage to serve this function is diminishing. The NIH reports that the fraction of 20 year-olds who have engaged in premarital sex grew by about 50 percent between the late 1950s and the late 1990s. Besides premarital sex, another option is consumption of pornography, which has become widely more accessible since the proliferation of the Internet. The degree to which these extra-marital options for sexual gratification substitute for marriage is an open question. Friedman [2000], among others, has suggested that one of the reasons that prostitution is so socially abhorrent is that it competes with women who seek a stable marriage. Although there is a small literature on the relationship between prostitution and marriage, there is almost no empirical work on the substitutability between pornography and marriage. This paper attempts to fill the gap. The proliferation of the Internet in the late 1990s and early 2000s was very rapid. The U.S. Census Bureau [accessed March 2015] reports that in 1996 only 9.4 percent of Americans had accessed the Internet at all within the 30 days prior to the survey. By 1998, 26.2 percent of Americans had an Internet connection in their homes. This grew to 50.5 percent in 2001 and in 2010 stood at 71.1 percent. A sizeable portion of the Internet is pornographic content. Forbes Magazine [2011] reports that, between July 2009 and July 2010, 13 percent of all Internet searches were for erotic content. About 4 percent of the million most heavily visited sites are pornography Websites. Estimates of the industry’s size vary, but US$3 billion a year for the United States is a commonly reported figure. Estimates of pornography usage vary as well, but Carroll et al. [2008] claim that 87 percent of young adult males use pornography.Footnote 2 As changes in the accessibility of pornography have occurred coincident to large changes in marital behavior, the causal relationship between the two is a natural question. Between 2000 and 2004, the General Social Survey (GSS) asked a series of detailed questions about Internet usage; it also records comprehensive demographic information, including marital status. Using these microdata we find that for young men there is a large degree of substitutability between Internet and pornography usage and marriage — heavy Internet usage generally, and use of pornography specifically, are associated with lower participation in marriages. We employ instrumental variables and a number of robustness checks, all of which suggest that this is a causal effect and not merely the endogenous relationship that married men are less likely to look at pornography, or some kind of unobserved selection issue that distinguishes men who use pornography from men who do not use pornography. We assert that increasing ease of accessing pornography is an important factor underlying the decline in marriage formation and stability. As policymakers seek to understand rapidly evolving family structures, technological change is surely an important element of these shifts. Specifically, to the extent that policymakers treat family arrangements as a control variable important for social welfare, and with a number of open public policy questions relating to Web access, understanding the underlying the connection between the two is critical. The paper proceeds as follows. We review the relevant literature on pornography and marriage and on extra-marital sexual substitutes for marriage generally. We then present a theoretical model of the relationship between the cost of accessing pornography, marriage formation, and divorce. We then proceed to discuss the data and methods and present the results. We conclude with a number of policy implications.",9
42,3,Eastern Economic Journal,17 November 2014,https://link.springer.com/article/10.1057/eej.2014.63,Marriage and Men’s Earnings: Specialization and Cross-Productivity Effects,June 2016,Sonia Dalmia,Claudia Smith Kelly,Paul Sicilian,Female,Female,Male,Mix,,
42,3,Eastern Economic Journal,01 December 2014,https://link.springer.com/article/10.1057/eej.2014.76,Relationship between Positive Attitude and Job Satisfaction: Evidence from the US Data,June 2016,Madhu S Mohanty,,,,Unknown,Unknown,Mix,,
42,3,Eastern Economic Journal,12 January 2015,https://link.springer.com/article/10.1057/eej.2014.64,Impact of Futures Trading on Spot Markets: An Empirical Analysis of Rubber in India,June 2016,Akanksha Gupta,Poornima Varma,,Unknown,Unknown,Unknown,Unknown,,
42,3,Eastern Economic Journal,30 March 2015,https://link.springer.com/article/10.1057/eej.2015.8,Comparing Standard Regression Modeling to Ensemble Modeling: How Data Mining Software Can Improve Economists’ Predictions,June 2016,Joyce P Jacobsen,Laurence M Levin,Zachary Tausanovitch,Female,Female,Male,Mix,,
42,3,Eastern Economic Journal,24 November 2014,https://link.springer.com/article/10.1057/eej.2014.61,Lotteries in Dictator Games: An Experimental Study,June 2016,David Owens,,,Male,Unknown,Unknown,Male,"Even the strictest adherents to the assumptions of neoclassical economics typically accept that concerns of equity and fairness can influence economic decisions. A body of literature has evolved on the topic of social preferences, involving theoretical, empirical, and experimental studies. The Dictator Game, in which a “Dictator” decides how much wealth to allocate to a passive “Receiver,” was developed by Kahneman et al. [1986], and has long been the workhorse for the experimental literature other-regarding preferences. Devoid of strategic and reciprocal concerns that complicate decisions in related games, a Dictator’s decision is seen as a direct reflection of her preferred distribution of resources. A number of articles have used experimental Dictator Games to explore other-regarding preferences. Early works showed that experimental subjects often sacrifice their own payoff to increase the payoffs of others.Footnote 1 The canonical model of Fehr and Schmidt [1999] is often cited to explain other-regarding behavior in the Dictator Game and similar settings, in which final outcomes determine preferences. With the existence of non-selfish preferences established in experimental settings, researchers turned to differences in giving behavior across settings and groups of individuals. Eckel and Grossman [1998] and Andreoni and Vesterlund [2001], among others, explore gender differences in giving behavior. Papers including Dana et al. [2007] highlight the role of the transparency of the Dictator’s decision. Eckel and Grossman [1996], Charness and Gneezy [2008], and Burnham [2003] highlight the importance of the identity of and information about the recipient of the dictator’s generosity. Oxoby and Spraggon [2008] establish the importance of the Dictator “earning” her role. These findings made clear that outcomes are not the only determinants of other-regarding behavior, and agents care about the manner in which outcomes are determined as well. Charness and Rabin [2002], among others, develop models of “intentions-based preferences,” which incorporate feelings such as reward and punishment, and thus aid in the interpretation of results of the Ultimatum Game, and other games involving strategic interaction. Bolton et al. [2005], Bolton and Ockenfels [2000], and Krawczyk [2011] also develop models of “procedural fairness,” in which agents have preferences over allocative procedures, rather than just an expectation over the resulting outcomes. The most straightforward application of the literature discussed above is to giving behavior, such as charitable contributions. Donating money to a charitable organization could be seen as a simple, one-to-one re-allocation of resources from a donor to a beneficiary. Of course, altruism is not constrained to the financial domain, as people often donate time and effort for the betterment of others, for example. Through slightly more abstraction, this line of research also contains insights for re-distributive and other social policies. Spending on public education, for example, can be seen as a transfer from the taxpayer to students. As education yields uncertain benefits at the individual level, increased spending does not directly improve the material well-being of students, but rather aims to improve the lottery of financial and life outcomes. Infrastructure projects often have a redistributive element as well, and both the associated costs and benefits involve substantial uncertainty. Many careers, such as social work, have public-service or charitable components. Some, such as careers in law enforcement, disaster relief, and the armed forces, explicitly involve shifting the risk of harm from one party to another. Even the straightforward act of giving to a charity involves uncertainty on behalf of the recipient, as it is often unknown how much of a donation will reach its final target, much less how it will affect her utility. Given the prominent role of uncertainty in many of its real-world applications, it is surprising how seldom it is incorporated into experimental studies. In a theoretical paper, Fudenberg and Levine [2012] note that distributional models cannot simultaneously explain preferences for both ex-ante and ex-post fairness. Experimental studies of altruism under uncertainty include Krawczyk and Le Lec [2010], in which each subject participates in three classes of Dictator Games. They observe more giving when the prize is deterministic, and among lotteries lowest when the process is “competitive,” in that the Dictator or the Receiver wins the prize. Cappelen et al. [2013] find that after the results of a lottery are known, Dictator’s decisions are largely driven by concerns of ex-ante procedural fairness, though ex-post distributional concerns also play a role. Ponti et al. [2013] find that Dictators consider both the time and risk preferences of Receivers when making decisions. In a different setting, Chakravarty et al. [2011] find that individuals make relatively risk-neutral choices on behalf of anonymous strangers even if they predict these strangers to be risk-averse themselves. In the paper most similar to our own, Brock et al. [2013] employ a within-subjects design to examine differences in Dictators’ altruism when assets are cash vs lotteries. They find that giving decreases when either the Dictator’s or the Receiver’s payoff is a lottery, and conclude both procedural and distributional concerns motivate Dictator behavior. This paper employs a between-subjects, 2 × 2 experimental design in which both the Dictators’ and Receivers’ assets are either cash or lotteries with a chance at winning a larger prize (US$40), while keeping the distribution of expected earnings to both parties identical across the four treatments. Our design differs from the existing literature in that we vary both Dictators’ initial wealth allocations and the price that they must pay to increase Receivers’ expected payoffs, from one decision to the next. In addition to testing the effect of lotteries on Dictators’ decisions, these design features allow us to test for differing wealth effects and price sensitivity.Footnote 2 Like Krawczyk and Le Lec [2010] and Brock et al. [2013], we find that subjects are willing to sacrifice their own chances in a lottery to increase the chances of others, but that they are less generous than when assets take the form of cash. Unlike the latter, we find them slightly more generous when recipients’ payoffs are lotteries. We also find much weaker wealth effects when Dictator assets are lotteries. These results are consistent with a distributional model of social preferences in which Dictators are risk averse over their own earnings, but not over the Receivers’. We also find some evidence that Dictators’ giving behavior is less price sensitive when their own asset is a lottery. Our results suggest support for pro-social donations and policies such that the welfare gains to the beneficiary is uncertain, but less support when the costs are uncertain. For example, donors may be reluctant to finance a project with uncertain costs, but willing make a donation equal to its expected cost. Further, our study suggests, weakly, that benefactors become less sensitive to the benefits received by the recipient if the cost to the benefactor is uncertain. The next section describes the experimental design and procedures employed to test our research question, and the following section provides a theoretical background. The penutimate section discusses the results of our study, and the final section concludes and discusses implications.",
42,3,Eastern Economic Journal,13 October 2014,https://link.springer.com/article/10.1057/eej.2014.59,House Prices and Bequests,June 2016,Edward C Hoang,,,Male,Unknown,Unknown,Male,"There exists a large literature conjecturing that house price appreciation alters economic behavior [see among many examples, Benjamin et al. 2004; Case et al. 2005; Campbell and Cocco 2007; Gan 2010; Hryshko et al. 2010; Dettling and Kearney 2014]. Since it has been widely observed that the value of a home represents a large share of household wealth and consumption expenditure is an important source of economic growth, much attention has been naturally centered on the wealth and collateral effects of house price growth on consumption behavior. However, this focus suggests that economic incentives are strengthened by house price appreciation and, therefore, raises the possibility that house prices affect other types of behavioral responses such as changes to altruistic bequest planning. The objective of this paper is to investigate the effect of changes in house prices on life insurance death benefits, which is an indicator of altruism and a specific source of bequests. Prior to the subprime mortgage crisis in 2007, the rapid increase in real US house prices starting in the late 1990s was instrumental in rekindling the traditional view that homeownership represents a mechanism for accumulating wealth. According to some economists, wealth accumulation is partly motivated by the desire to leave bequests and, therefore, increases in housing wealth may influence the amount of wealth households plan to bequeath. Assuming that bequests are normal goods, a wealth effect and/or home equity extraction effect could explain a positive correlation between house prices and bequests. This paper focuses on life insurance death benefits which are an important component in planning the transfer of resources and estates. Upon the death of the insured, these benefits can be used by beneficiaries to pay estate taxes on the transfer of assets, cover funeral costs associated with the death of the policyholder, maintain their standard of living, or simply contribute to their own accumulation of wealth. Recipients of life insurance death benefits may find it more beneficial to receive this type of bequest because of the liquidity it provides as opposed to the passing on of an illiquid physical asset such as housing. Additionally, life insurance death benefits are potentially exempt from estate taxes and can be used to pay the estate taxes associated with the transfer of the house. Life insurance death benefits are tax exempt if, for example, an irrevocable life insurance trust is established. Also, if a house is not intended to be transferred, the benefactor may choose to bequeath life insurance death benefits so that the beneficiaries could use the proceeds to purchase or make a down payment on their own home. Finally, if a home with an existing mortgage is passed on, life insurance death benefits may be used to pay off the mortgage balance. Among other benefits of passing on life insurance death benefits, if the home is passed on to one relative, life insurance death benefits can be passed to other heirs in distributing the benefactors wealth. If households experience wealth gains due to house price appreciation, they might respond by increasing the face value of their life insurance policies and, therefore, the death benefit. According to the American Council on Life Insurance, most life insurance holders pay only about $0.03 for $1 of coverage because of the low probability that an individual will actually die during their coverage period. This suggests that because of the relatively low cost of financing life insurance death benefits out of housing wealth gains, other resources intended to be transferred to heirs can also be financed by housing wealth gains. Figure 1 plots average 4-year growth rates in life insurance death benefits and house prices using annual US state level data from the American Council of Life Insurers (ACLI) and the Federal Housing Finance Agency (FHFA). As can be seen, the patterns of these two series are similar especially before the mid-1990s, and their growth rates have a correlation coefficient of 0.26. The positive correlation between these two series suggests that housing wealth may be partially driving the changes in life insurance death benefits, but this relationship is unclear due to the lack of empirical research on the matter. The aim of our analysis is to empirically examine whether increases in housing wealth are associated with changes in life insurance death benefits. Average 4-year growth rates. Notes: The figure plots average 4-year growth rates in life insurance death benefits paid out and house prices from 1977 to 2011 and for 50 US states. Life insurance death benefits and house prices are in 2011 dollars. Data for life insurance death benefits is obtained from the American Council of Life Insurers (ACLI) and house prices is obtained from the Federal Housing Finance Agency (FHFA). An investigation of the economic effects of house price appreciation on bequest behavior should begin by describing the wealth effects of house prices and the incentive effects of the home equity system. According to standard economic theory, households feel wealthier when the present value of their lifetime wealth increases due to house price appreciation. However, households feel poorer when there is a decline in their housing equity caused by a fall in house prices. Formally, the direct effect of house prices on behavior is characterized as a wealth effect. House price appreciation leads to an increase in home equity, which can stimulate bequest behavior in two different ways. First, households can use their home equity to obtain lower second mortgage loan rates, which ultimately saves them money in mortgage payments. Second, households can use the increase in their home equity to engage cash out refinancing, which serves as additional cash that appears on a household’s balance sheet. The cash obtained from refinancing, less commission fees, might be used to pay for home improvements and outstanding household debts. Also, some of the cash may be passed on to heirs in the form of bequests. Therefore, house price appreciation may be correlated with an increase in bequests through the cash obtained from home equity “extraction.” We use state-level panel data covering the 50 US states over the period 1977–2011 to examine the response of life insurance death benefits to changes in house prices. Exogenous shocks to both household wealth and the opportunity for home equity extraction are captured by growth in house prices which are constructed using the FHFA house price index at the state level. Using state-level house prices in our panel data regressions has an advantage over national house prices in isolating the effect of house prices because the latter may be correlated with the underlying economic environment which influences household bequest behavior. The use of state-level house price data in our panel regressions allows for the control of macroeconomic shocks through the inclusion of year fixed effects. The estimated coefficients obtained show that life insurance death benefits are responsive to changes in house prices. Specifically, life insurance death benefits increase by 10 percent in response to house price growth of 50 percent. This translates, in economic terms, to a $20 million increase in life insurance death benefits. Skinner [1989] proposes households will bequeath portions of their wealth gains due to house price appreciation to help future generations afford the purchase of their own homes. In the spirit of Ricardian Equivalence, Calvo et al. [1979] suggest that anticipated growth in future house prices is internalized by the current generation who in turn bequeath more financial resources to future generations. Current tax reductions on land implies future increases in land taxes and, therefore, the current generation will try to offset the increase in future taxes by transferring more financial resources to future generations. This paper builds on these conjectures by exploring whether wealth gains made possible by house price appreciation are passed on to future generations in the form of life insurance death benefits. Our results not only add to the literature on the effects of housing wealth, but also to the longstanding debate over whether the contribution of bequests to wealth accumulation is intentional or accidental [Kessler and Masson 1989; Dynan et al. 2002]. The source of accidental bequests may come from residual wealth left unconsumed by the current generation as a consequence of uncertain lifespans and imperfect markets for life annuities. Overall, we provide empirical evidence that house price growth have real effects on planned bequests in the form of life insurance death benefits. Our findings are robust to the inclusion of state demographic variables and differenced specifications based on short-term and long-term horizons. Specifically, the estimated coefficient increases when the difference interval widens. This suggests that house prices have a long-run effect on life insurance death benefits, and, hence, bequest behavior. The rest of the paper is as follows. The next section discusses life insurance death benefits and house prices. The subsequent section develops the empirical methodology. The latter section describes the data. The penultimate section discusses the empirical results. The final section concludes.",1
42,3,Eastern Economic Journal,22 December 2014,https://link.springer.com/article/10.1057/eej.2014.77,"Intertemporal Incentives, Equilibrium Selection, and Rational Investment Collapse",June 2016,Maciej K Dudek,,,Male,Unknown,Unknown,Male,"More than a decade ago, numerous emerging economies experienced foreign investment crises. Investors based in mature economies who had been generously funding overseas investment projects suddenly refused additional lending, and triggered unexpected currency revaluations and capital outflows. Moreover, it never became clear which macro events actually triggered the crises. However, in the ex post sense, it was obvious that numerous emerging economies had been severely mismanaged at the micro level. In this paper, we want to provide rationale for the occurrence of foreign investment crises. Specifically, we present a novel mechanism that leads to investment crises occurring endogenously in a fully rational equilibrium. Moreover, we show that empirically observed micro-mismanagement, an assumption taken as given in other contributions, can arise endogenously along the equilibrium path. We differ from most contributions that deal with investment crises both in the focus and the approach that we take. The vast existing literature typically assumes that foreign investors rationally assess the shape of a given emerging economy and then decide on the level of investment. Normally, foreign investors are assumed to rationally process all signals generated by a given emerging economy and then make their investment decisions. In other words, the literature assumes that an assessment of a given country’s fundamentals by the foreign investors is crucial for the occurrence of foreign investment crises. According to this view, a country experiences an investment crisis if investors decided that its business climate was poor. Moreover, typically, as in Angeletos et al. [2007] or Hellwig et al. [2006], formal modeling is reduced to a purely descriptive aspect contingent on the shape of the fundamentals. While we agree that the standard approach leads to numerous insights, we hold the view that it avoids the key questions of interest and it does not attack the problem at its root. In this paper, we attempt to fill the void. It is definitely the case that for crises to occur — we abstract here from the possibility of a sunspot-type equilibria emphasized in the second-generation models Obstfeld 1986] — in a rational equilibrium, at least some emerging economies must be characterized by weak fundamentals. In other words, we must as in first-generation models, for example, Krugman [1979], observe along the equilibrium path some form of misbehavior, some sort of substandard practices, on the part of agents in the emerging economies as, otherwise, it would always be optimal to invest. The literature normally assumes that either governments follow some forms of inconsistent policies first-generation models — or that businesses engage in substandard practices. However, this begs an obvious question: Why would a rational entity, be it a government or a professionally run company, follow self-destructive policies? Why would a government run an excessive budget deficit? Why would a given Thai company invest in risky projects with a negative net present value or in heavily overpriced land, or why would a given company hold a mismatched balance sheet? The literature assumes that somehow this simply happens and then proceeds to model a learning process that culminates in a crisis. In this paper, on the other hand, we want to explain all these suboptimal practices as realizations of rational actions taken in equilibrium. The focus of the literature on the problem of foreign investors who rationally interpret signals in their attempt to assess whether a given emerging economy is “OK” leaves a key dimension untouched. We argue that, not so much the perceptions of foreign investors, but rather the expectations of the domestic agents formed in regard to the behavior of the foreign investors are crucial. In other words, we show the answer to a different question — “will investors be able to invest at the current level for a long time?” — appears to be critical in the process of crisis formation. In that sense, we shift the focus from the expectations of foreign investors to the expectations of the domestic agents. It turns out that such a shift of perspective can explain both the occurrence of crises and the observed rational misbehavior of domestic agents along the equilibrium path. The modeling approach that we take in this paper utilizes standard tools. We build a stylized model of strategic interactions that, in its purest form, allows for multiple equilibria. Moreover, we assume that a Pareto-dominant equilibrium exists and is a cooperative one — all agents follow sound business practices — and is enforced by the standard set of trigger strategies. At the same time there exists a Pareto-inferior equilibrium with all agents engaging in substandard forms of behavior. When no foreign investment is present, domestic agents are assumed to coordinate on the Pareto-dominant equilibrium. At the same time, we assume that foreign investors find it optimal to invest when the Pareto-dominant equilibrium occurs as the outcome, and they prefer not to invest when the Pareto-inferior equilibrium occurs as the outcome. We show that a shift towards a Pareto-inferior equilibrium can occur endogenously and that it can lead to a foreign investment crisis. In particular, we argue that periods when the amount of available foreign funds is higher than normal despite the fundamentally positive role of foreign investment can be susceptible to crises. The reason is that the availability of foreign funds plays an analogous role to demand shocks described by Rotemberg and Saloner [1986]. Specifically, if agents expect the future availability of foreign funds to be lower than today’s, then again it might be optimal to take advantage of more abundant resources when they are present. However, if domestic agents attempt to take advantage of richer opportunities, then the punishment phase is triggered, and the domestic economy moves towards an inferior equilibrium where it is no longer optimal to invest, and the economy experiences a crisis. The result is a bit counter-intuitive, as we normally perceive the periods when the availability of foreign funds is high as a blessing as in those periods domestic agents should be able to attain more, but at the same time, by changing intertemporal incentives, higher than normal availability of foreign funds opens a route to an inferior equilibrium and to a crisis. The paper emphasizes the role of intertemporal incentives and makes a single policy prescription. According to the paper, crises can occur as long as the availability of foreign funds is not permanent. Thus, any policy that affects the expectations of the domestic agents and signals that there is a high probability that investment can be long lasting can be welfare improving. There are three additional sections in the paper. The following section outlines the underlying model. The subsequent section extends the analysis to a stochastic environment and presents a simple result, with crises occurring due to a shift in intertemporal incentives caused by abnormally high availability of funds. The last section concludes.",1
42,3,Eastern Economic Journal,20 October 2014,https://link.springer.com/article/10.1057/eej.2014.58,The Adoption of Constitutional Home Rule: A Test of Endogenous Policy Decentralization,June 2016,Jessica Hennessey,,,Female,Unknown,Unknown,Female,"Between 1875 and 1912, 12 states adopted constitutional home rule for municipalities. The adoption of home rule legislation allowed municipalities the option of writing their own charters and the ability to independently determine their desired structure and functions. The state-level choice to adopt home rule was not an exogenous decision; it was determined by underlying social, economic, and political changes in each individual state. While the state decision to adopt home rule is influenced by many factors, this paper explores one possible mechanism. I use a fiscal federalism framework to test whether municipal preferences for home rule have an effect on the state adoption of home rule. During this period, home rule was a right granted to municipalities in the state constitution and implemented by the state legislature.Footnote 1 In a home rule state, each municipality decides whether or not to draft its own unique home rule charter. Municipalities have the default option of adopting a standard organizational form provided under general state legislation. If we assume the state legislature reflects local preferences, states with more heterogeneity across municipalities may have a greater desire or need for home rule by municipal governments and should be more likely to adopt home rule. States that have more homogeneity across local governments are less likely to adopt home rule because localities are more likely to be satisfied with a uniform general law for municipalities.Footnote 2 I test the hypothesis by using a unique municipal-level data set to estimate a latent preference for home rule for each municipality and then compare the dispersion of these preferences across states. The results consistently show that states with more heterogeneous municipal-level preferences for home rule adopted constitutional home rule at the state level. The analysis of home rule presents a unique opportunity to investigate the endogenous determination of government structure. Often, empirical research either takes government structure as given or assumes that institutional change and policy determination are exogenous. Rodden [2004] points to several studies that seek to explain endogenous fiscal decentralization, all building off the predictions of Oates [1972]. For example, Panizza [1999] uses cross-country data to empirically identify variables that predict fiscal decentralization. Strumpf and Oberholzer-Gee [2002] analyze endogenous policy decentralization within the United States. The point of these exercises is to better understand the origins of these governmental structures. By overlooking how institutions and governance are endogenously determined, past research has sometimes ignored valuable information that can help explain why and how different government structure have evolved. This is true in the case of home rule, as previous scholars have not explored the determination of home rule, rather they have focused on how home rule may impact municipal governments’ choice of an efficient structure and an optimal set of public goods. Writing 10 years after the adoption of the home rule amendment in Minnesota, Charles P. Hall described the difference home rule had made for municipalities. He recognized its limitations, specifically that municipalities were still creatures of the state and could not supersede the general state laws. However, he boldly claimed that:
 Already the small municipalities are finding themselves better governed than before; the spirit of freedom, long confined, becomes a light in the community life: while other cities, less progressive, go lumbering on, under out-grown legislative grants. No municipality, though it be small in numbers, is deprived of the home rule privilege: thinking men and understanding voters there must be; but, with these present, the benefit may be secured. [Hall 1906, p. 7] Similar observations were made in other states. While scholars recognize the drastic changes municipal governments were able to make through home rule,Footnote 3 few have tried to place home rule within the context of legislative change. While the use of home rule will cause change, its arrival also reflects change. The advent of home rule points to changes in demand for different types of governmental structure during this dynamic period of municipal history. With only 12 of 48 states adopting home rule in the initial era of adoption from 1875 to 1912, it is evident home rule was only one of many viable solutions for how states structured a system of local government. This paper helps answer the question of why municipal home rule was a solution for certain state governments.",1
42,3,Eastern Economic Journal,08 December 2014,https://link.springer.com/article/10.1057/eej.2014.66,Schools of Thought and Economists’ Opinions on Economic Policy,June 2016,Luca De Benedictis,Michele Di Maio,,Male,Female,Unknown,Mix,,
42,3,Eastern Economic Journal,13 June 2016,https://link.springer.com/article/10.1057/eej.2016.11,The Economics Major is Not a Pipeline to Finance,June 2016,David Colander,,,Male,Unknown,Unknown,Male,"A primary reason the economics major is not a pipeline to finance or a business career is that it is not designed to be. Economics, like other liberal arts majors, is designed to provide students with general liberal arts skills. The economics major provides students with broad insights into how the economy works. It makes no attempt to teach students the specific skills and knowledge needed in business or finance. I am not saying that understanding economics is not useful in finance. Economics understanding is important for finance, but understanding economics is just as important for non-profits, government, or even the arts, as it is for finance. Employers know that the economics major is not designed to train students in the specifics of business or finance. They hire liberal arts economics majors nonetheless because what they are most interested in is that their new hires have solid general liberal arts skills. (The same holds true for all employers: business, government and non-profits alike.) Employers’ interest is in hiring students whom they believe can help them in their activities. They want employees who are self-starters, who can learn on their own, who can write, who can handle quantitative and qualitative reasoning, who are comfortable with critical thinking, who can deal with uncertainty, who can communicate verbally, and who play well with others in the sandbox (i.e. are comfortable working in teams). Those are all general liberal arts skills, which is why liberal arts colleges send so many students into finance. The entire liberal arts experience, not the economics major, prepares students for a career in finance or business. The fact that financial firms hire liberal arts economics majors, even though the economics major doesn’t teach them finance-specific skills, has an important implication. It means that there is no need to major in economics if a student wants to go into finance. Because enrollment pressures in economics lead to larger class sizes, other majors teach many general liberal arts skills as well or better than does an economic major. For example, a math major provides more training in quantitative reasoning; a philosophy major provides more training in qualitative reasoning, and a history or English major provides more training in writing and communicating. And just about any major provides better training in working in teams than do most economics majors. I could go on, but you get the picture. Finance firms are open to hiring a student with just about any major, as long as that student has strong liberal arts training combined with appropriate job-specific skills and knowledge.",1
42,3,Eastern Economic Journal,13 June 2016,https://link.springer.com/article/10.1057/eej.2016.12,An Alternative Model,June 2016,Brian Chezum,,,Male,Unknown,Unknown,Male,,
42,3,Eastern Economic Journal,07 September 2015,https://link.springer.com/article/10.1057/eej.2014.72,Predatory Pricing in Antitrust Law and Economics: A Historical Perspective,June 2016,Jenny R Hawkins,,,Female,Unknown,Unknown,Female,,
42,3,Eastern Economic Journal,13 April 2015,https://link.springer.com/article/10.1057/eej.2014.73,Behavioral Economics: A History,June 2016,John F Tomer,,,Male,Unknown,Unknown,Male,,
42,3,Eastern Economic Journal,04 May 2015,https://link.springer.com/article/10.1057/eej.2014.70,Social Security and Pension Reform: International Perspectives,June 2016,John R Moreau,,,Male,Unknown,Unknown,Male,,1
42,4,Eastern Economic Journal,20 June 2016,https://link.springer.com/article/10.1057/s41302-016-0005-x,Adam Smith at 240: A Symposium,September 2016,Jeffrey T. Young,,,Male,Unknown,Unknown,Male,"It
 is a pleasure to publish these four outstanding papers on Adam Smith’s enduring ability to stimulate creative thought in the modern discipline of economics. Three of the papers were presented at the Eastern Economics Association annual meetings in Washington, D.C. in February, 2016. Unfortunately, Samuel Hollander was unable to attend the meetings, but we are pleased to publish his paper on Smith as an ethical Utilitarian. Of course, 2016 is the 240th anniversary of the publication of Smith’s Inquiry into the Nature and Causes of the Wealth of Nations (WN). (I still ponder Kenneth Boulding’s question as to the scientific status of an “Inquiry”?) Hence the title of the Symposium. But why 240? Surely the Smithian desire for harmony, symmetry, and aesthetic beauty would suggest that we wait 10 more years? There is a simple answer to the question. When my colleagues in the Economics Department at St. Lawrence University joined the editorial team of this journal they asked me to put together this Symposium now, because in 10 year’s time I will have retired. However, Smith scholarship is a growth industry, and it is always appropriate to read and discuss Smith’s works. As Smith predicted, the division of labor within “philosophy” (science) has progressed into ever more narrow areas of specialization. Economists mostly do not read Smith today, and those who do have evolved into their own specialist sub-group. Given Smith’s iconic status, though, the editors of the journal and I thought exposing a general economics audience to some of the best contemporary scholarship on Smith, done by historians of economic thought who are also first-rate economists, could be a fruitful exercise. Perhaps the single most outstanding feature of modern Smith scholarship is the treatment accorded to The Theory of Moral Sentiments (TMS), Smith’s first, and arguably, better book. The interplay of morality and economics, and the desire to reunite them in the way Boulding characterized economics as moral science (Boulding 1969), therefore, are themes which run through all of these papers. Smith did not work with a strict positive/normative dichotomy, nor did he consider the selection of moral principles to be solely matters of personal taste. Self-interest, for example, was an attribute of the morally defined virtue of prudence, and as such was developed and exercised in a social environment of individuals seeking the deeper pleasures of harmony, not just externally through social approval, but internally through the approval of the impartial spectator, Smith’s term for the conscience. More specifically, we find many observations in Smith, which beg for further development. Consider the following two: This division of labour … is the necessary, though very slow and gradual consequence of a certain propensity to truck, barter and exchange one thing for another. Whether this propensity be one of the original principles in human nature, of which no further account can be given; or whether, as seems more probable, it be the necessary consequence of the faculties of reason and speech, it belongs not to our present subject to enquire. (WN I. 2. 1–2, p. 25; emphasis added; all references to Smith’s works are to the Glasgow Edition of the Works and Correspondence of Adam Smith). The difference of natural talents in different men is, in reality, much less than we are aware of; and the very different genius which appears to distinguish men of different professions, when grown up to maturity, is not upon many occasions so much the cause, as the effect of the division of labor. The difference between the most dissimilar characters, between a philosopher and a common street porter, for example, seems to arise not so much from nature, as from habit, custom, and education. When they came into the world, and for the first 6 or 8 years of their existence, they were perhaps, very much alike, and neither their parents nor playfellows could perceive any remarkable difference. About that age, or soon after, they come to be employed in very different occupations. The difference of talents comes then to be taken notice of, and widens by degrees, till at last the vanity of the philosopher is willing to acknowledge scarce any resemblance. (WN I.2.4, pp. 28–29). The division of labor is the central, unifying principle of the book, just as sympathy is central to TMS. In setting out his theory in systematic form, Smith begins each book with the core principle from which so much of the rest follows, and indeed both of our quotes come from the same chapter in Book I. David Levy and Sandra Peart have created a prolific and creative research program around the second, while Deirdre McCloskey builds on the first. Her paper is a plea to recapture something that Smith already knew, but did not elaborate on, namely, that exchange is a social activity which involves among other things the need to talk to one another. Thus, she chastises the modern theory of rational choice for ignoring the obvious fact the people have to communicate with each other in order to engage in trade. Since Smith chose not to delve into the relationship between reason and speech on the one hand and the origin of the division of labor on the other, she goes back to Smith’s Lecture in Jurisprudence to discover that there Smith taught his students at Glasgow University that it was the desire to persuade that led to the kinds of social intercourse that we think of as markets. McCloskey’s discovery of these insights, which Smith never published, was exciting to me, since my earliest work on Smith was an attempt to integrate the two books by following through on the idea that exchange and markets are social phenomena, and therefore, individual behavior is governed by the desire for approval, of which the satisfaction of having successfully persuaded someone to trade with you would be a specific instance. From here I argued that the terms of trade, mutually agreed to, would be considered just, since approval (successful persuasion) would rule out the possibility that one of the persuaded partners had been injured in the transaction (Young 1986). However, there is a dark side to human nature that Smith is well aware of. In later work (Young 2000), I pointed out that there is also the desire to dominate and enslave as we see here: For though management and persuasion are always the easiest and the safest instruments of government, as force and violence are the worst and the most dangerous, yet such, it seems, is the natural insolence of man, that he almost always disdains to use the good instrument, except when he cannot or dare not use the bad one. (WN V.i.g.19, p. 799). Here the tedious process of persuasion is replaced by one person lording it over the other. “Force and violence,” contrary to the happy world of Book I and the pleasure of concord of feelings,
 seem to be the default position of human nature (a point consistent with Smith’s Calvinist up-bringing). The important point is that the parties to these transactions are unequal. This, in turn, implies that the kinds of speech acts leading to voluntary exchange are taking place among equals. Thus, we “dare not” use force and violence when we encounter equals in situations where we want someone else to do something for us. Smith’s usage here is suggestive of the social context in which the desire for approval is always paramount. This, then, raises the whole set of issues surrounding Smith’s treatment of inequality and difference, the theme of the Levy and Peart paper. They have staked a claim to the concept of analytical egalitarianism, which they have shown is an underlying ethical commitment of Smith and the great nineteenth century classical liberals: Malthus, Ricardo, and J. S. Mill. In their Symposium paper they develop Smith’s idea of occupation-based sources of variation and difference in human character in contrast to Hume’s theory of innate national characters (later, the source of racial theories of difference) as an instance of Smith’s commitment to analytical egalitarianism. “But [says Smith] the understandings of the greater part of men are necessarily formed by their ordinary employments.” (WN V. i.f.50, pp. 781–782) From Levy and Peart’s epigraph we may safely add “character” to “understandings.” There are “races” and “factions” in Smith, but they are not rooted in genetics. They are the learned behavior of human beings whose mental capacities develop from childhood according to the sympathy principle of TMS. “Factions” develop in opposition to the public interest because the members attend to the “partial spectator,” praise within the group, not the impartial spectator, the desire for praiseworthiness. (TMS III.3.42-43, pp. 154–156) The latter, suggest Levy and Peart, could lead to the breakdown of factional, special interest politics. Amos Witztum’s paper is also concerned with the formation of character and different character types in Smith’s moral theory. Witztum notices a parallel between a common approach to the famous Adam Smith Problem among Smith scholars, and the way behavioral economists deal with experimental results of the Ultimatum Game. In both cases, the received model of self-interested agents is rescued by simply giving the agent social preferences. Witztum argues that this maneuver is neither an appropriate way of relating Smith’s two works nor a departure from the model of rational utility maximization. In probably the most ambitious and complex of the papers here, Witztum develops a Smithian model in which he identifies three distinct character types: self-interested agents who judge according to utility; other-regarding, benevolent agents who take the trouble to internalize the process of sympathetic identification with the others in the form of the impartial spectator; and malevolent agents who cannot successfully form a viable society. This, more general model, he claims is the way out of the Adam Smith Problem as well as the way out of game theory approaches to a socialized version of homo economicus. Witztum may thus be seen as responding to McCloskey’s plea to do human economics by treating humans as moral agents. As such they may not be playing games. They may bring to the table ethical commitments which do not fit any sort of utility maximization framework. One of the important implications of these papers is that Smith is now being taken much more seriously as a moral theorist in his own right. Smith used to be seen as a second rate Hume whose moral theory was studied primarily to shed light on Hume. These papers suggest that Smith’s moral theory stands as an important, fruitful contribution on its own merits. As in the case of any significant work in moral philosophy, Smith’s TMS is open to alternative interpretations. Samuel Hollander’s paper, for example, enters the debate over Smith’s place in the ethical Utilitarian tradition of Hume, Bentham, and the Mills. D. D. Raphael, one of the editors of the Glasgow Edition of the Theory of Moral Sentiments has long argued the contrary, that Smith is an anti-Utilitarian (Raphael 1972–1973, 2001, 2007). Amos Witztum and I have recently made our contribution to Rapheal’s position arguing that it is a mistake to view Smith’s moral theory as essentially Utilitarian (2013). Rather we view him as paving the way for Kant and the Kantian approach to morality, which historically is the primary competitor to Utilitarian ethics. The issues are too complex to rehearse here. Suffice it to say that Hollander, in his usual style of careful scholarship and close attention to the texts, has concluded that the positions of Hume, Bentham, and Smith are sufficiently close that if the former are agreed to be ethical Utilitarians, then Smith must be accorded the same honor. Hollander’s argument must be given careful consideration, but not here. Adam Smith’s work is not of purely antiquarian interest. All four of these excellent papers show the range and depth of modern Smith scholarship among historians of economic thought. All in their own way treat Smith the same way they would treat any contemporary work in economics. We read him to gain insight and understanding of the world we live in. And we criticize him from the same perspective. Smith scholarship is intimately bound up with deep issues of the interconnection between economics and ethics. My hope is that making this type of scholarship available in a general economics journal will help economists to consider the entirety of Smith’s works and to find inspiration for creative new research programs which will humanize the discipline along Smithian lines.",
42,4,Eastern Economic Journal,24 August 2016,https://link.springer.com/article/10.1057/s41302-016-0007-8,“Adam Smith did Humanomics: So Should We”,September 2016,Deirdre Nansen McCloskey,,,Female,Unknown,Unknown,Female,,13
42,4,Eastern Economic Journal,28 June 2016,https://link.springer.com/article/10.1057/s41302-016-0004-y,Group Analytics in Adam Smith’s Work,September 2016,David M. Levy,Sandra J. Peart,,Male,Female,Unknown,Mix,,
42,4,Eastern Economic Journal,24 August 2016,https://link.springer.com/article/10.1057/s41302-016-0006-9,"Experimental Economics, Game Theory and Das Adam Smith Problem",September 2016,Amos Witztum,,,Male,Unknown,Unknown,Male,"Until not long ago, Adam Smith was seen by many as the forefather of the neo-classical paradigm where the self-interested behaviour of individuals unintentionally translates into social benefits through the working of the invisible hand.1 However, developments in game theory raised doubts over this conclusion. Competitive interactions – captured by the idea of non-co-operative games2 – often lead self-interested rational agents into a prisoner’s dilemma where the co-ordinated outcome cannot be perceived as best serving the interests of the participants.3 Nor can such an outcome be easily attributed to market failure.4
 But evidence from experimental economics has shown that people do not tend to act in games according to the predictions of the self-interested rational utility maximiser.5 They seem to avoid the trap of the prisoner’s dilemma and the reason for this, it has been argued, is that agents are considerably more socially conscious than is normally implied by the standard conception of homo-economicus. Therefore, competitive interaction does indeed lead to a co-ordinated outcome which is socially desirable provided that agents are socially minded rather than purely self-interested. But this idea too, has been attributed to Adam Smith.6 So, Smith is both the father of the unintended beneficial outcome of self-interested behaviour and the father of the (presumably intended) socially beneficial outcome of competitive interaction between socially minded individuals. What is, of course, common to both these approaches – and by implication, the focus of Smith’s guardianship – is the idea that whatever motivates individuals, social benefit will always arise from a system of decentralised decision making.7
 This juxtaposition of the purely self-interested agent from the neo-classical core and the socially minded individual from experimental economics as the legitimate heirs of Adam Smith is, of course, reminiscent of Das Adam Smith Problem where the question arose about Smith’s conception of the human character. Are individuals socially minded people as they seem to be depicted in The Theory of Moral Sentiments (TMS) or are they self-interested or even selfish as they seem to be depicted in The Wealth of Nations (WN)? In this respect, there is indeed no surprise at all that both sides in the current debate seem to find their origins in Smith’s writing. But for Smith, of course, this was not a problem at all, and the main question is not whether there is a choice to be made between the ‘own regarding’ behaviour of individuals and the ‘other-regarding’ aspects of it. Instead, the question is really about the relationship between these two components of the human character. Notwithstanding the ongoing debate about this problem among Smith’s scholars, the received view about its resolution is indeed an attempt to accommodate both aspects of the human character rather than to choose between them. Therefore, it is argued, people can care about their own affairs (their self-interest), while at the same time be socially minded.8 Economic analysis – the subject of the WN – is therefore confined to the examination of only one aspect of the human character assuming that all of these individuals have a clear and active ‘other-regarding’ capacity. By the same logic, the TMS is focused on the study of those other-regarding aspects of the human character.9
 There are evidently serious problems with this approach.10 For one, if there are multitudes of ways in which individuals can care for themselves and there are multitudes of ways in which their ‘other-regarding’ sentiments are manifested, it is difficult to see how the pursuit of one’s own interest can be examined independently of the nature of the other-regarding aspects of one’s character and vice versa. But there is an even more puzzling question which arises from the fact that whatever may motivate individuals it does not seem to have any impact on the way they interact with each other in the economic sphere. Whether they are self-interested or socially minded, they will always interact (and hence, act) competitively with the others.11
 To a great extent, the critique of neo-classical economics embedded in experimental economics is very much in line with the received view of how to resolve Das Adam Smith Problem. But while it is suggested that the self-interested individual of the WN is not devoid of sympathy – the social principle behind the TMS – it does not offer any insight into how would the fact that the Brewer employs sympathy in his relationship with the others affect his pursuit of his own interest. In other words, the received view simply claims that self-interested people are capable of feelings as other would had they been in their place so there is no contradictions between being social and pursuing one’s interest. What is intriguing about the contemporary debate is that both the self-interested agent of Arrow-Debreu and the socially minded agent of experimental economics are rational utility maximisers.12 This can be seen in two different ways. It can either be seen in line with the received view by saying that the way people act in their pursuit of their own affair does not preclude their ability to have social sentiments. Or as an improvement on the received view by saying that the correct depicting of the rational utility maximiser should be based on the fact that they have social values. The first approach is unsatisfactory for the same reason that the received view is an unsatisfactory resolution of the problem. It is not enough to say that a self-interested person can have social sentiments; the question is whether or not this will have any effect on his or her behaviour. The second is unsatisfactory because it (a) assumes that the rational utility maximiser is an appropriate framework to analyse social sentiments and (b) because, like the received view, it leaves the question of how would the presence of social sentiments affect the way one acts in the pursuit of one’s own affairs unanswered.13
 In what follows I propose to follow three lines of investigation. Firstly, I propose to re-iterate my critique of the received view about the resolution of Das Adam Smith problem. I will argue that Smith never doubted the simultaneous presence of both the self-preservation drive and the social motive. Therefore, simply saying that people have these two aspects in their character does not add much to the understanding of his work or the world around us. Instead, I will show that not only there is a logical connection between the way the social motive is manifested and the way individuals pursue their own affairs, but this also has an influence on the way in which they form moral judgements. In connection with this, I will also explore the fallacy of using the framework of the rational utility maximiser to depict both selfish and socially minded behaviour. In the following section I will question whether the findings of experimental economics really give rise to the presumption of a degree of sociality which is not consistent with the notion of self-interest as is depicted by the neo-classical paradigm. The fact that such behaviour can be captured by the rational utility maximiser is already suggesting that we may be looking at the same type of individual used in standards theory. But there is much more to suggest that much of what experimental economists identify as social or moral behaviour is nothing but a form of self-interest. In the final section, I will examine whether the Smithian distinction between different types of character would lend itself to an analysis where agents interact competitively. I will argue there that competitive interaction is a form of behaviour which is typical of a particular type of character. Had people been genuinely social as is suggested by some of the experimental literature, they may not have behaved competitively; they would not, as it were, be playing games.",1
42,4,Eastern Economic Journal,28 June 2016,https://link.springer.com/article/10.1057/s41302-016-0003-z,Ethical Utilitarianism and The Theory of Moral Sentiments: Adam Smith in Relation to Hume and Bentham,September 2016,Samuel Hollander,,,Male,Unknown,Unknown,Male,"In his Principles of Morals and Legislation, Jeremy Bentham designates as the most influential of principles opposed to that of utility the principle which “approves or disapproves of certain actions, not on account of their tending to augment the happiness, nor yet on account of their tending to diminish the happiness of the party whose interest is in question, but merely because a man finds himself disposed to approve or disapprove of them: holding up that approbation or disapprobation as a sufficient reason for itself, and disclaiming the necessity of looking out for any extrinsic ground” [Bentham (1789) 1982, pp. 21, 25)]. Insertions made in 1819 by Bentham himself into a lengthy note to the Principles surveying objectionable alternatives to utility – the insertions were first printed in the Bowring edition of The Works [Bentham (1789) 1859, p. 8n] – specify the authors intended: “One man [Lord Shaftesbury, Hutchinson (sic), Hume, &c.] says, he has a thing made on purpose to tell him what is right and what is wrong; and that it is called a moral sense: and then he goes to work at his ease, and says, such a thing is right, and such a thing is wrong – why? ‘because my moral sense tells me it is’” [Bentham (1789) 1982, p. 26]. One is initially taken aback by the inclusion of David Hume amongst those appealing to “a moral sense” rather than utility, since A
Fragment on Government – where Bentham first stated his axiom that “It is the greatest happiness of the greatest number that is the measure of right and wrong” [Bentham (1776) 1859, p. 227] – describes Book III of Hume’s Treatise
on Human Nature 1740 (“Of Morals”) as the text from which he had learned “that the foundations of all virtue are laid in utility”, and that “utility was the test and measure of all virtue”, notwithstanding “a few exceptions” discerned by Hume which Bentham himself dismissed as such [p. 268n]. “I felt”, he famously declared, “as if scales had fallen from my eyes”. It is, in fact, precisely because Bentham was so attracted by Humean utility that he regretted features that seemed to be in conflict.1 Bentham was unnecessarily concerned since Hume, we shall see, did not intend by his “moral sense” – or “conscience” as he sometimes expressed it – an innate ability to sense what is right and wrong independently of circumstances; to the contrary, the moral sense was put at ease by observing conduct reflecting concern with the general good, that is satisfying the benevolent sentiment of “sympathy”. Presumably, Hume’s admired utility analysis is not attended to in the survey of the literature Bentham provided in his note to Morals and Legislation because he is there concerned with formulations opposed to the utility principle in ethical evaluation. But how are we to interpret his silence regarding The
Theory of Moral Sentiments? Would he not have mentioned Smith amongst the non-utilitarian ethicists if he understood TMS in this manner, especially since he never hesitated to protest Smith’s appeals in the Wealth of Nations to “natural liberty”? This possibility will immediately be discounted by those who maintain that Smith rejected “ethical utilitarianism”. I shall briefly review a selection of modern commentaries to this effect, having in mind a recent allusion to “a fairly standard view among philosophers that TMS is a book in the Utilitarian tradition” [Witztum and Young 2013, p. 573]. One might equally well talk of a “fairly standard view” to the contrary. Lionel Robbins expressed himself reticently regarding the utilitarian component in TMS relating to personal ethics, though he clearly had his doubts. The Utilitarian doctrine itself – “the habit of judging actions and policies by their consequences rather than by reference to some intuitive norm”, contrasting with that of the Continental metaphysicians – Robbins attributed to the entire classical school “from its beginnings in Hume’s Essays right through to Cairnes and Sidgwick”, holding good “even for Adam Smith whose explicit moral philosophy had a somewhat different complexion” [Robbins 1970, p. 56]. Adam Smith is singled out as “the only possible exception” to the general run of “classical” economists, for he “had a moral philosophy of his own which in some respects appears to be in contrast with the utilitarian outlook” [Robbins 1952, p. 178]. With the terminology in mind of “Deistic philosophy” and of Naturrecht embellishing even the analysis of the market in the Wealth of Nations, Robbins cautioned against judging that work by reference to the Theory of Moral Sentiments “rather than by examining the merits of the arguments by which they are supported in the contexts in which they appear” [p. 25; also p. 48]. Bonar maintained that “Adam Smith is less Utilitarian than Hume; regard for consequences is always secondary to immediate regard to virtue for its own sake” [Bonar 1922, p. 169]; and “immediate sympathy is… antecedent to any consideration of utility, personal or social” [p. 165]. Macfie argued to similar effect: “So far… as Smith defined intrinsic value, he placed it especially in virtue and beauty. Utility itself had only instrumental value, except in so far as it partook of the beautiful in the ‘system’. And the exercise of any virtue had ‘many agreeable effects’. In this sense, good or pleasing consequences were essentially associated with and coloured by the exercise of the values from which they were derived” [Macfie 1967, p. 48]. “Smith then”, Macfie concludes, “was not a utilitarian, even in Hume’s sense. Utility for him was not basic”. More recently, Montes has elaborated this theme, his purpose being “to show that if Hutcheson and Hume prepared the ground for Bentham, Smith paved the road for some of Kant’s ideas on ethics”, referring to evaluation of moral good by reference to intention rather than consequences [Montes 2004, p. 114]. He prefers, however, to say that Smith “is not a proto-utilitarian” rather than that he is an “antiutilitarian”, while (like a successful general occupying all positions) he allows that “some passages in the TMS definitively have a pre-utilitarian tone”, and that “the importance [Smith] attributes to motives does not necessarily exclude the significance of effects” [pp. 115–117]. It is Smith’s position whereby “the sentiment of approbation always involves in it a sense of propriety quite distinct from the perception of utility” that “completely invalidates any attempt to describe Adam Smith as a forbear of utilitarianism” [p. 117]. Raphael asserts strongly that “[f]ar from being a utilitarian, Adam Smith was a severe critic of utilitarianism in many parts of his ethics and Jurisprudence” [Raphael 2007, p. 46]. Similarly, “Hume is by and large a utilitarian. Adam Smith is an antiutilitarian, indeed a natural-law theorist …” [Raphael 1972/1973, p. 88]; and he reiterates the theme that while Smith “was prepared to allow that moral actions do in fact tend, as a whole, to promote the general happiness, and that this is the end intended by God”, he opposed Hume’s view “that utility is the one and only standard of right action. In practice, he argued, the thought of utility has a subordinate role in the formation of moral judgement”; for while “the pleasure which attends the thought of utility” is taken into account, it is the least important “in its contribution to the final judgement of approval” [Raphael 1985, p. 38]. In brief, “we do not in practice decide what is right by reference to utility”. Along these lines, Raphael and Macfie wrote in their bicentenary edition of TMS: “Smith continually insists that considerations of utility are the last, not the first, determinants of moral judgement. Our basic judgment of right and wrong is concerned with the agent’s motive, not with the effect of his action. Our more complex judgements of merit and demerit, justice and injustice, depend on the reactions of gratitude and resentment to benefit and harm respectively, not simply on the benefit and harm themselves” [Smith (1759) 1976, p. 13]. Moreover, “even though the pleasant or painful effects of action are relevant to the moral judgement passed upon it, they are primarily the effects of this particular action upon particular individuals, not the more remote effects upon society at large. Considerations of general social utility are an afterthought, not a foundation” [pp. 12–13]. Elsewhere in their commentary, the editors represent Smith simply as “an opponent of utilitarianism” [p. 305n]. Whereas Haakonssen allows that utility in TMS “is a real source of moral judgement, although a secondary one”, which, only when afterwards recognized, “comes to have an influence on moral judgement and behaviour” [Haakonssen 1981, p. 73], he also insists that “Smith’s concept of utility cannot be taken in any Benthamite sense of uniform happiness” [p. 135]. He allows too that, for Smith, “the laws of justice are useful in the sense that they serve as a means to an end, the end being the public interest”, but maintains that “this idea of ‘means-utility’…is clearly different from the idea of utility which we find in the later utilitarian theorists. For them utility is more or less identified with pleasure or happiness of a kind, and is thus the end towards which actions should aspire” [pp. 40–1]. Thus, utility considerations are recognized in the context of justice, but “do not form the foundation of justice [since] social utility is rarely thought of by the bulk of mankind”, but pertain rather to “how philosophers…interpret human morality” [p. 88]. To the same effect, Witztum and Young conclude that “under no circumstances can Smith be considered as part of the Utilitarian tradition. For this to happen we should have seen the search for happiness as a motivator of human actions and of human moral judgment. We found neither” [Witztum and Young 2013, p. 600]. For what Smith intended by “utility” was either “the simple colloquial notion of usefulness or, the more complex notion of social usefulness or, a pleasure from harmony” – alluding to Smithian “propriety” – “rather than the idea of “happiness” in any form”. These authors further maintain that colloquial “utility” is “foreign to utilitarianism” [p. 602]. They allow that “utility maximization is [for Smith] appropriate in some cases”, although – like Haakonssen – only as a matter of “philosophical judgment”, and this they take to be “consistent with Smith’s non-utilitarian moral theory and non-utilitarian theory of individual motivation” [p. 596; on the “philosophical spectator”, see also p. 600]. Muller too perceives a fundamental clash with Bentham: “While Smith’s deistic humanism had judged commercial society according to the standards of the civilizing project, Bentham’s utilitarianism eliminated the possibility of standards beyond sensual pleasure for judging character. Indeed, it tended to discredit qualitative distinctions, to make them appear as intellectually suspect and morally sinister, and promoted a model of moral thinking which tries to do without them altogether” [Muller 1993, p. 190]. It is a feature of Raphael’s account that “utilitarianism” supposes right behaviour by an individual to be behaviour motivated by a desire to enhance social happiness: “the proper standard is maximum promotion of the general happiness”, not his own [Raphael 1985, p. 38]. McCloskey also perceives Smith as opposed to Hume and utilitarianism, but her conception of the category is diametrically opposed to that of Raphael. Thus, she contends that Smith “sharply opposed, the reduction of what is good to what causes pleasure, that is, utilitarianism”, though “not quite in the form of the ‘chaos of precise ideas’” [McCloskey 2009, p. 4];2 but it was opposition to the opinion of those – including Hume as Smith understood him – who maintained that “virtue consists in prudence” or, more precisely, “prudence only”. McCloskey in fact regards “prudence suffices” and “greed is good” as synonymous; and she represents nineteenth-century utilitarianism – and later the “new” welfare economics – as “attempting to build judgments about the economy” on the basis of an identification of virtue with “prudence only”, with justice taken as sheer taste. If all are benefited, or could be benefited, the proposed policy is good. That is all ye know of ethics, and all ye need to know” [pp. 4–5]. Her perception of utilitarianism incorporates the simplistic textbook reading of the Wealth of Nations whereby people guided by self-interest nonetheless act (unwittingly) to enhance social wellbeing, for she cites Smith’s “book on prudence” to the effect that “what is prudence in the conduct of every private family can scarce be folly in that of a great kingdom” [Smith (1776) 1976, p. 457]. Other virtues, she writes, are recognized by Smith elsewhere, especially “temperance” and “justice” in Smith’s Lectures, while TMS allows a range of “moral sentiments” divorced from any sort of self-interest, and there Smith’s “grounds for opposing utilitarianism” are based on observed behaviour similarly divorced [McCloskey 2009, p. 7]. In much the same vein, Hanley refers to Smith’s “argument against utilitarianism” in TMS designed “not simply to counter the claim that the proper standard for evaluating actions is their capacity for utility-maximizing effects”, since “Smith does not deny the goodness of utility maximization, but rather suggests that to posit utility maximization as either the sole or the ultimate standard of ethical value … precludes a comprehensive understanding of the multiple phenomena involved in moral judgment” [Hanley 2009, pp. 68–69]. Smith’s concern, in brief, was “to resist the reductionism characteristic of utilitarianism”, since – in Smith’s terms – “it seems impossible that the approbation of virtue should be a sentiment of the same kind with that by which we approve of a convenient and well-contrived building; or that we should have no other reason for praising a man than that for which we commend a chest of drawers” [Smith (1759) 1976, p. 188]; again: “any theory of our admiration of [men’s] virtue must not only account for their utility, but also for “the unexpected, and on that account the great, the noble, and exalted propriety of such actions” [p. 192]. We shall evaluate the foregoing opinions as we proceed, noting at this point that any concessions they allow to the presence in Smith of a utility dimension insist on its secondary character. My own approach emphasizes the qualifications both Hume and Smith made to their respective doctrines, and the complexity of Bentham’s position, the utility dimension to TMS emerging as a result in much brighter colours. As for modern commentaries, my results corroborate Raynor’s position “that there is much in TMS that Hume could heartily accept. It is not so very surprising that Hume composed a complimentary notice of Smith’s book” for the Critical Review of 1757 [Raynor 1984, p. 64]. Any understanding of TMS as hostile to the utility principle would have to account for this positive reaction. Other moderns forwarding this sort of reading, if not in every detail, would include: Rawls, who places Smith within a Shaftesbury–Hutcheson–Hume–Bentham tradition [Rawls (1971) 1999, p. 20n]; Campbell, according to whom “[d]espite all that Smith has to say against utility as the explanation for the ordinary person’s moral and political attitudes, his own normative moral and political philosophy turns out to be, in the end, a form of utilitarianism…. Utility, or the production of happiness, is … the principle by reference to which he judges that both the natural moral sentiments and the system of natural liberty are desirable” [Campbell 1971, pp. 205–206], or again: “Utility is … very much the meta-principle for Smith. It is to be found at the basis of his whole moral outlook”, although Campbell opines that “it operates most typically at the level of contemplation, when men adopt a God’s-eye-view of society, enter into His universal benevolence and feel admiration and approval for what they observe” [p. 219; also Campbell 1975, pp. 76–77]; Schneewind, who argues that, notwithstanding his criticisms of Hume, “[u]tility does play a major part in Smith’s view of morals” [Schneewind 1998, pp. 390–391]; and Alvey who recognizes the utilitarian dimension in Smith’s discussion of “the virtue of prudence” [Alvey 2003, p. 60]. Finally, my study amply confirms Rosen’s case for a close Hume–Smith connection [Rosen 2000, 2003], which includes a denial that whenever Smith in TMS “discusses utility, he intends to diminish its importance” [Rosen 2000, p. 82]. Indeed, Rosen properly maintains that “in several respects Smith gives greater scope and importance to utility than Hume”. Of prime importance, of course, is the “utilitarian” standard itself. As for the alleged absence in TMS of the Benthamite idea of “happiness”, we shall show that the “real happiness of human life” constitutes Smith’s justification for population expansion – implying no less than the greatest happiness of the greatest number. In any event, when Smith talks of the “welfare of society” it is (we shall show) frequently “happiness” that he intends, as with Bentham who made use of synonyms for happiness, including “public interest” and “general good”. And the remarks in A
Table of the Springs of Action relating to “self-regarding prudence” – considered as a virtue when perceived in terms of pleasure and pain – in response to “want, need, demand, exigency, necessity” [Bentham (1815) 1859, pp. 208, 211] point away from the contention, noted above, that the “colloquial” notion of “utility” as “usefulness” is “foreign to utilitarianism”, since the colloquial version would fall within “necessity” and perhaps even “exigency”. Furthermore, the view of Smith as “non-Utilitarian” reflects in many instances an unjustifiably narrow reading of Bentham, while Muller’s view of Bentham is a caricature [Hollander 2015, pp. 34–42]. And while confirming Rosen’s interpretation of the Smith–Hume relation, I would qualify his reading of the Smith-Bentham which asserts, following Schneewind, that “Bentham firmly rejected the idea of moral sentiments as the foundation of morals” [Rosen 2000 p. 101; 2003, p. 81]. It is a “moral sense” understood as precluding the utility criterion of ethics that he firmly rejected.",2
42,4,Eastern Economic Journal,28 September 2016,https://link.springer.com/article/10.1057/eej.2014.79,Benefits and Costs of the Erie Canal: A New View,September 2016,Donald F Vitaliano,,,Male,Unknown,Unknown,Male,"Described as the “Eighth Wonder of the World” in the 19th century, the Erie Canal initiated an era of widespread canal building during the pre-Civil War era in the United States, and proved to be an important engine of economic growth. Why conduct a benefit-cost analysis? Given the current interest in infrastructure investment, it may prove instructive to evaluate ex post a project widely seen as an extraordinary success in its time. The existing literature is dated and inadequate in several ways. Fishlow’s [2000] survey of 19th century canals is based on the work of Segal [1961] and Rubin [1961]. Segal, whose estimates of canal benefits are shown below to be grossly exaggerated, considers only the 1837–1846 period, in spite of the fact that these canals operated for decades. And instead of considering the 10 New York canals as an integrated public sector transportation investment, Segal lumps the Erie and Champlain into a group of ten “heavily utilized” canals located in various states; and three of the other New York canals are considered separately as part of a dozen canals deemed to have “failed.” Selection bias is an obvious concern. In addition, economic historians have focused on the financial profitability of the Erie Canal [Fishlow, op. cit., pp 561–564] or, have measured non-financial benefits using a “cost saving” approach rather than the more conceptually appropriate consumer and producer surplus [Segal 1961; Fogel 1964; Fishlow 1965]. The paper is organized as follows: the next section gives a brief description of the canal system under investigation, followed by a discussion of the existing literature. The conceptual framework for benefit estimation is then presented, followed by empirical estimation of willingness to pay, followed by canal costs. The choice of discount rate is next; then a simulation analysis of net present values (NPVs). A concluding section closes the paper.",2
42,4,Eastern Economic Journal,28 September 2016,https://link.springer.com/article/10.1057/eej.2014.80,Pump up the Volume: Income Risk and Counter-cyclical Asset Trading,September 2016,Gian Domenico Sarolli,,,Male,Unknown,Unknown,Male,"As the changing relationship between volume and output has not been fully explored, this paper addresses three related issues in macro-financial analysis: the structure of equity trading volume in the United States, the relationship between equity trading volume and aggregate output, and finally the risk-sharing characteristics of households. First, the paper documents a structural break in the 1970s for all variables related to equity trading (volume, value of volume, turnover, real value of turnover). Second, the paper documents the change in correlations between equity volume and aggregate output, whereby correlations became significantly negative after 1975. Third, the paper proposes an exercise using a DSGE model calibrated to household characteristics and aggregate economic variables using incomplete markets. These results have three implications for the study of asset markets. First, the structural break may suggest that the liberalizations in the market have allowed for more risk sharing as greater numbers of participants enter the market. Second, the negative correlations further suggest that agents are trading (directly and indirectly) to mitigate changes in income. Finally, the DSGE model provides a base model to capture the reason for some of the trading, namely labor income variations. Taken together, this points to a change in the data and provides a motivation to explain the change. Historically, the dynamics of trading volume have received less attention than equity prices in economic literature. Lo and Wang specifically comment in their seminal work on trading volume “… quantities have received far less attention, especially in asset pricing literature” [Lo and Wang 2009]. This has been due to the reliance on the Lucas Tree model of asset trading, where markets are complete and thus all trading occurs in period zero [Lucas 1978]. The use of incomplete markets models, whereby agents do not have the full complement of Arrow-Debreu-type securities has expanded the modeling of volume beyond the initial states. In fact, both Telmer [1993] and Heaton and Lucas [1996] use the incomplete markets framework extensively to analyze risk sharing, though they generally concern themselves with an analysis of equity prices and the equity premium. Furthermore, in a recent paper, DeJong and Espino [2011] use a dynamically complete market to model equity turnover explicitly in a neoclassical production economy. My paper will extend the methodology of Heaton and Lucas to demonstrate the applicability of a simplified incomplete markets endowment economy for the study of the counter-cyclical trading phenomena that DeJong and Espino observe in the US. This paper draws upon two main strands of literature: the literature on incomplete market models and the empirical literature on the aggregate dynamics. Heaton and Lucas [1996] is the most similar in methodology to this paper, in that they use incomplete markets models and idiosyncratic shocks to analyze equity trading between agents. Their results confirm intertemporal trading and explain some of the equity premium. This paper extends their results by using the insight of Storesletten et al. [2001] to provide evidence of the importance of labor income shocks on risk sharing among agents and an analysis of volume rather than price. Also, rather than using the “auctioneer algorithm” developed by Heaton and Lucas, this paper employs a variation of Judd et al. [2002] which provides a solution strategy for solving DSGE models with incomplete markets. The second strand of literature is related to empirical research on cyclical dynamics. The most important of which is DeJong and Espino [2011] which documents the relationship between equity turnover and aggregate output. The paper below extends this research by using multiple filtering methodologies and examines various measures of trading volume. The first paper that examines cyclical dynamics in GDP is Granger and Hatanaka [1964], which uses spectral analysis to examine the US economy and is extended by this paper to trading volume. There is a strand of literature that includes Mankiw and Zeldes [1991] and Ait Sahalia et al. [2004], which examine PSID data to differentiate behavior between stockholders and non-stockholders. This paper uses the characteristics of stockholders to calibrate the labor income variation and thus calculate the risk sharing between households who participate in the stock market. Finally, there is a strand of literature that looks at the ability of financial markets to increase the risk-sharing capabilities of its participants as in Arestis and Demetriades [1999]. This analysis contributes to the literature in the following ways. First, the documentation of a structural break in several measures of post-war equity trading volume (from 1946 to 2005) that occurs in the 1970s. There are a paucity of studies that perform structural break analyses on four measures of equity trading: equity volume, equity turnover, value of volume (volume multiplied by price), and real value of volume. Three potential changes in the US market may have contributed to this structural change: the initiation of electronic trading (notably the formation of the NASDAQ exchange), the elimination of fixed commissions on the NY Stock Exchange on May 1, 1975, and the passage of the law facilitating the creation of individual retirement accounts (known as 401(k) accounts). Furthermore, changes in international dynamics may have also contributed to the changing behavior of the markets. The second result of the paper is an analysis of the cyclical characteristics of equity trading both before the structural break and after the structural break noted above. Specifically, the relationship between aggregate output and equity trading showcases a significant and negative correlation for equity trading after 1975. This potentially points to a change in the relationship between agents trading on the exchange and the overall economy. We can term this negative correlation counter-cyclical trading. This result confirms the findings of DeJong and Espino [2011] and extends their results to several measures of filtered data on equity volume. They find that unfiltered turnover is asynchronous with GDP and that a model with wealth variation can capture 29 percent of the level of turnover and 22 percent of the volatility. While they analyze the lead-lag patterns of trading volume, this paper will concentrate on the contemporaneous correlations. The third result of this paper is the use of a simplified endowment economy model with incomplete markets, which is able to match the negative sign of the counter-cyclical trading as a portion of the magnitude of the correlation. The model includes two assets and generates trade through the incorporation of labor income shocks. The results of the model, calibrated to US data, is able to match 77 percent of the correlation between US GDP and turnover. Furthermore, the model is able to account for a portion of the cyclical trading volatility by matching 23 percent of the standard deviation of turnover as well as 97 percent of the standard deviation of value of turnover. This result establishes one of several components of trading motivations by households. The combination of incomplete markets (whereby agents are unable to hedge all of their risks) and shocks to their optimal income stream is the mechanism by which this model (and models such as in Heaton and Lucas [1996]) are able to produce trading volume through time. Specifically, the model presented below operates under the assumption that during downturns some agents lose (gain) income relative to their optimal consumption path and thus are induced to sell (buy) assets to smooth their income.",
42,4,Eastern Economic Journal,28 September 2016,https://link.springer.com/article/10.1057/eej.2015.1,Long-Term Trends in Intra-Financial Sector Lending in the United States (1950–2012),September 2016,Juan Antonio Montecino,Gerald Epstein,Iren Levina,Male,Male,Female,Mix,,
42,4,Eastern Economic Journal,28 September 2016,https://link.springer.com/article/10.1057/eej.2015.2,School Size and Student Achievement: Does One Size Fit All?,September 2016,Laura M Crispin,,,Female,Unknown,Unknown,Female,"Many aspects of the educational process are directly related to the size of the school, though there are tradeoffs involved. For instance, larger schools tend to offer a broad range of courses, while teachers in larger schools may be less able to monitor and interact with individual students. School size may also reflect differences in location or demographics, where disadvantaged students frequently attend large, urban schools, for example. This motivates two important questions: What does school size represent in the educational process? And is there an ideal size at which the advantages outweigh the disadvantages? If school size only reflects differences in student demographics or location, then policies to improve student outcomes through changes in school size are likely to be ineffective. If school size reflects differences in educational inputs such as course offerings, then policies to change the size of the school may be needlessly costly because less costly alternatives such as expanding offerings in small schools via school consortiums or teleconferencing would also be effective. To explore the role of school size in the educational process, I estimate the relationship between school size and student achievement in math using a sample of 10th grade public high school students from the National Education Longitudinal Study of 1988 (NELS:88). I estimate a series of value-added education production functions (VAEPFs) in which school size enters as a proxy variable intended to represent the aspects of education that vary with school size. The first specification in the series includes only school size, thus representing the combined effect of all inputs that vary with school size. Subsequent specifications include school size along with different combinations of observable inputs (e.g., course offerings, socioeconomic status, and teacher quality), essentially changing what school size represents as additional inputs are included in each subsequent specification. In the final specification, I include the complete vector of background variables and educational inputs, and any remaining relationship between school size and achievement is due to the unobserved factors that vary with school size (i.e., factors that are difficult to quantify such as school climate and organization). An important feature of my estimation strategy is that I estimate these production functions separately by urban status, dividing students into urban, suburban, and rural samples based on school location. This is because different aspects of education are emphasized in different locations, which may affect the relationship between size and achievement. As an example, historically, rural areas are known for strong community involvement with the school [Tyack 1974], and so large rural schools may have a more engaging school climate than a comparably sized suburban school. In addition, school size is relative to location, where a “large” urban school is typically much larger than a “large” rural school. In urban and rural areas, I find that achievement is the highest in the smallest and largest schools in all specifications, indicating that school size has a U-shaped relationship with achievement. These results suggest that unobservable factors including (but not limited to) an engaging school climate associated with relatively small schools, and organization and teacher specialization associated with relatively large schools are important aspects of education. In contrast, in suburban areas, I find that school size has an inverted U-shaped relationship with math achievement that becomes negligible once schooling inputs are included in the production function. This suggests that any benefits from larger suburban schools are primarily due to teacher quality and the associated increase in course and extracurricular offerings. My findings directly contrast the findings from most prior studies that use a non-linear function of school size. These studies tend to find that school size has an inverted U-shaped relationship with achievement and conclude that the peak — typically near the middle of the school size distribution — represents the “ideal” school size [e.g., Lee and Smith 1997; Bradley and Taylor 1998; Foreman-Peck and Foreman-Peck 2006].Footnote 1 Though these studies may intend for school size to represent difficult-to-quantify factors such as school climate, organization, and offerings, most include few controls for student background, other educational inputs, or observables that vary with school size [e.g., Lee and Smith 1997; Schneider et al. 2006/2007]. Therefore, it is impossible to determine which remaining factors make the “ideal” school size ideal and unclear what educational inputs policymakers should target to improve student outcomes (other than changing school size itself ). Further, many do not control for location, and almost none estimate separately by location.Footnote 2 To illustrate the importance of location in the analysis and to reconcile my findings with those of previous studies, I use a specification similar to that used in previous studies (i.e., I pool urban, suburban, and rural schools and control for fewer inputs) and I do find an inverted U-shaped relationship between school size and achievement. This shows that by pooling all locations, variation by location is incorrectly restricted, yielding the misleading conclusion that there is one size that fits all.",2
42,4,Eastern Economic Journal,28 September 2016,https://link.springer.com/article/10.1057/eej.2015.21,A Note On Piketty’s Logic,September 2016,Frederic L Pryor,,,Male,Unknown,Unknown,Male,"
Piketty’s [2014] recent and widely selling book Capital in the 21st Century has given rise to a number of responses [for instance, Acemoglu 2014; Galbraith 2014; Krusell and Smith 2014; Taylor 2014; Rognlie 2014; Colander 2015] outlining many factors that he did not include in his formal model. Many of these analyses focus on Piketty’s “second law of capitalism.”Footnote 1 In contrast, this note focuses on the basic logic of one of his major arguments, namely that if the GDP growth rate is lower than the profit rate, the share of property income will increase as a share of national income and, presumably, the income distribution will become increasingly unequal. Although at first sight, this proposition seems obvious, Piketty does not provide a formal proof and I show below that it rests on some assumptions that deserve our attention. This note has three parts. First, I look at the basic algebra of his model and explore the various possible factors underlying a rising share of property income. Then I look briefly at some relevant empirical evidence for the United States to determine the validity of some of his assumptions. Finally I discuss some future economic trends in the United States that support his argument.",
42,4,Eastern Economic Journal,28 September 2016,https://link.springer.com/article/10.1057/eej.2016.7,A Minimum Guaranteed Jobs Proposal,September 2016,David Colander,,,Male,Unknown,Unknown,Male,"A guaranteed jobs program can take many different forms depending on the nature of the minimum job being guaranteed. The primary purpose of the guaranteed jobs is to provide individuals with a method of translating their time into income. The goal is not to provide individuals with a deeply satisfying job. The reality is that most low-wage market jobs are not deeply satisfying. The equilibrium wage rate for deeply satisfying jobs is likely zero or more likely negative. To give you an idea of how a guaranteed minimum jobs program might be structured, and what it would cost, let me outline some characteristics of what I see as a compromise proposal designed to take into account conservative’s concern about costs of the program and progressive’s concern about providing an acceptable minimum safety net. The design characteristics of a basic minimum jobs program might be the following:Footnote 1
 The government in its role as employer of last resort will provide a guaranteed minimum job for eligible citizens The pay for the job will be US$10.00 an hour, and workers will be allowed to work for up to 40 hours per week. This will provide everyone in the labor force with the ability to earn a minimum income of $400 per week, or $20,800 per year The guaranteed job will consist of a variety of mental and physical tasks that can be easily monitored either online or within existing institutions. The mental tasks might be activities such as keyboarding practice, doing arithmetic exercises on an internet training program, doing exercises in word processing, doing spread sheets, reading reports and summarizing them, or completing online courses. The physical tasks might include doing specific exercises, digging holes and filling them, moving weights, and other similar type activities. The goal of these activities is not to produce usable output for society, but to provide work activities by which people can translate their time into income. To the degree possible, these working experiences will provide training in skills and an introduction to customs and behavior useful in the real-world market jobs The guaranteed minimum job will require the worker to go to work at standard working hours — variations of an 8 hour day — and meet the normal job requirements that low-wage private and government employers impose on their employees — dress codes, behavior codes, such as showing up on time, demonstrating the appropriate attitude, and being responsible. Individuals who do not meet these standards can be fired Whenever possible the guaranteed job would be designed to help people learn new skills that would make them more productive and provide certification of any increase in productivity. For example, for those who have not graduated from high school, the job activities might prepare them to get their GED (General Educational Development) degree. A number of skill certifications might be created, which guaranteed job workers will be encouraged to earn on the job to provide them with credentials that will help them find better-paying jobs in the normal job market Government will contract with for-profit, non-profit, and governmental firms and organizations to monitor and mentor the workers in the set of specified activities. These organizations will receive a stipend for monitoring the jobs, and possibly be allowed to have the person work for them at no cost for up to one-fifth of the hours they work. Likely contractees would include existing small manufacturing and retail firms, non-governmental organizations, and governmental organizations, such as schools, that have space and the ability to undertake this monitoring. The guaranteed workers would be a bit like interns for them. Depending on monitoring feasibility, there might also be virtual jobs, in which the guaranteed job and the monitoring are done over the internet Ideally all citizens willing and able to work would be eligible for the guaranteed program, but practically, eligibility will likely have to be limited. For example, initially it will likely be necessary to restrict eligibility to non-full-time students; otherwise the program would likely be overwhelmed by students wanting summer work. To be feasible, the program will also likely have to be limited to US citizens; if open to immigrants, and undocumented residents, it would likely be too costly",3
42,4,Eastern Economic Journal,11 August 2016,https://link.springer.com/article/10.1057/s41302-016-0014-9,Response,September 2016,David Neumark,,,Male,Unknown,Unknown,Male,,
42,4,Eastern Economic Journal,28 September 2016,https://link.springer.com/article/10.1057/eej.2015.26,The Status Quo Crisis: Global Financial Governance after the 2008 Meltdown,September 2016,Marcella Lucchetta,,,Female,Unknown,Unknown,Female,,
42,4,Eastern Economic Journal,28 September 2016,https://link.springer.com/article/10.1057/eej.2014.68,The IMF and Global Financial Crises: Phoenix Rising?,September 2016,Miguel D Ramirez,,,Male,Unknown,Unknown,Male,,
42,4,Eastern Economic Journal,28 September 2016,https://link.springer.com/article/10.1057/eej.2015.18,After Occupy: Economic Democracy for the 21st Century,September 2016,Aaron Pacitti,,,Male,Unknown,Unknown,Male,,
43,1,Eastern Economic Journal,02 December 2016,https://link.springer.com/article/10.1057/eej.2015.4,Foreign Entry into US Manufacturing by Takeovers and the Creation of New Firms,January 2017,Zadia M Feliciano,Robert E Lipsey,,Unknown,Male,Unknown,Male,"The United States became a magnet for international direct investment flows in the form of foreign acquisitions of existing US firms and establishment of new US firms by foreigners starting in the late 1980s. This inflow reached its peak in 2000, declined sharply, and recovered, although not quite to the previous level in 2006–2008. Most of these inflows have been in the form of acquisitions of US firms by foreign owners (see Figure 1).Footnote 1 New US business enterprises established by foreign firms and foreign investors (greenfields) are less common but remain an important part of foreign direct investment (FDI). Investment outlays in manufacturing in the United States by foreign direct investors (Millions of 2006 dollars). 
Source: US BEA, US Department of Commerce, www.bea.gov, publicly available data. Although the United States has for a long time been an advocate for the free flow of FDI in general, the growth of FDI from abroad into the United States, and particularly the entrance of foreign firms via takeovers of existing US firms, led to fears that skills and technologies that were the basis for US comparative advantage and for US security were being lost to foreign countries. That concern led to the establishment of the Committee on Foreign Investment in the United States (CFIUS) in 1975 and to various measures to tighten regulations in later years, summarized in Jackson [2010]. While the earlier regulations were confined to issues of security, the current ones broaden the bases for requiring scrutiny of takeovers. One criterion introduced in 1993 is that the acquirer “… is controlled by or acting on behalf of a foreign government …” (p. 6). In 2007, Public Law 110–49 added bases for review that were broader and edged closer to what might be thought of as economic, rather than strictly security considerations. They included such considerations as “the potential effects on United States critical technologies” and “the potential effects on United States critical infrastructure, including major energy assets” and “the long-term projection of the United States requirements for sources of energy and other critical resources and materials” (p. 13). Given the doubts about FDIs, it is important to understand the characteristics of the ultimate beneficial owner (UBO) in both takeovers of existing US firms and newly established foreign firms. It is our purpose to investigate the relationships between foreign takeovers and new foreign business formations and the revealed comparative advantage of the country of UBO and US comparative advantage of industries where the investments are made. To this end, this paper analyzes a data set on all foreign takeovers of existing US firms and new foreign-owned firms in the United States from 1988 to 2006, collected by the Bureau of Economic Analysis (BEA) of the US Department of Commerce. Firms are grouped by 3-digit Standard Industrial Classification (SIC) and country of location of the UBO abroad.Footnote 2
 Although there is a literature on the relationship between comparative advantage and foreign takeovers in the US, most of this research is based on data from private sources such as Thomson Financial Securities, which exclude takeovers of small firms and the value of asset acquired. Moreover, Thomson Financial Securities data do not include new foreign firms. Previous research has analyzed characteristics of foreign takeover targets and new establishments of US multinationals abroad. This is the first study to analyze the relationship between UBO comparative advantage and newly established foreign firms in the United States. Our data are quite complete, including many small firms and firms not publicly traded. Because of the completeness of the BEA data, our research provides new insights on foreign takeovers and new foreign establishments in the United States.",6
43,1,Eastern Economic Journal,02 December 2016,https://link.springer.com/article/10.1057/eej.2015.22,The Immediate Hardship of Unemployment: Evidence from the US Unemployment Insurance System,January 2017,Mark Stater,Jeffrey B Wenger,,Male,Male,Unknown,Male,"The Great Recession was an exceptionally painful economic period in the United States, both in its severity and duration. After the housing market went bust and financial markets began to implode in late 2007 and early 2008, the unemployment rate rapidly increased from 5.0 percent in April 2008 to a high of 10.0 percent in October 2009, the first time it had reached 10 percent in more than 26 years, then remained above 9.0 percent for nearly 2 more years [Bureau of Labor Statistics 2014]. It took yet another year to go below 8.0 percent, and another 15 months to go below 7.0 percent. Finally, in September 2014, the unemployment rate fell below 6.0 percent for the first time in more than 6 years. During this period of remarkable economic stagnation, it is estimated that nearly 9 million jobs were lost in the United States [Center on Budget and Policy Priorities 2014]. Aside from the sheer number of people who were unemployed in this episode, the rising incidence of long-term unemployment became an important concern. In March 2008 only 16.9 percent of the unemployed in the United States had been unemployed for 27 or more weeks; this share rose to a high of 45.3 percent in April 2010 and remained above 40.0 percent for another two and a half years [Federal Reserve Bank of St. Louis 2014a]. Even as the unemployment rate fell below 6.0 percent in late 2014, the percentage of the unemployed with durations longer than 27 weeks was still above 30.0 percent, almost double what it was before the recession. Likewise, the share of the unemployed with relatively short durations (5–14 weeks) fell from 34.7 percent in June 2008 to a low of 20.6 percent in April and July of 2010 and took almost another 4 years to finally rise above 25.0 percent [Economagic 2014]. The distinction between unemployment durations shorter and longer than 26 weeks is noteworthy because this is the amount of time after which unemployment insurance (UI) benefits are typically exhausted in the United States. However, labor market conditions were so poor from 2008 to 2013 that Congress voted to extend to UI benefits 11 times, with the extensions at one point allowing up to 99 weeks of UI benefits [IVN 2014]. The last benefit extension elapsed in December 2013, leaving over 1 million people without UI. Although the unemployed who exhaust their UI benefits face potentially serious losses in income and welfare, less is known about what happens to their well-being in the earliest phases of unemployment. One possibility is that workers have financial cushions to absorb the shock of a brief period of unemployment, or that they use the additional non-working time to satisfy a pent-up demand for leisure, such that welfare does not fall early in the unemployment spell. Alternatively, unemployment may be a sufficiently severe shock that welfare begins to fall immediately when one becomes unemployed. By examining the reservation wages of unemployed workers during the time they wait to apply for UI benefits, we are able to examine the immediate effects of unemployment. This is because, as shown by Shimer and Werning [2007], the after-tax reservation wage can be used as a monetized welfare metric as it measures the take-home pay, or potential consumption, which makes the worker indifferent to working vs remaining unemployed. As the waiting time to apply for UI benefits is usually relatively short, the question of how it affects the reservation wage provides insight on how welfare changes over the early part of the unemployment spell, a question that has not been previously investigated in the literature. This sheds light on the nature of the job search process in the very early stages of unemployment, as stationary job search models predict a constant reservation wage over the duration of unemployment, whereas non-stationary models predict a falling reservation wage. We use US Department of Labor data on over 20,000 UI benefit applicants from 2002–2009, and examine how the waiting time to apply for benefits affects the reservation wage. We find that the waiting time has a negative effect, suggesting that welfare falls very early in the spell of unemployment and that the search environment is non-stationary even in its initial stages. Our results, based on a finely grained and highly compressed duration measure, are among the first to describe the time path of the reservation wage in the first few weeks of unemployment.",
43,1,Eastern Economic Journal,02 December 2016,https://link.springer.com/article/10.1057/eej.2015.23,Estimates of the Frisch Elasticity of Labor Supply: A Review,January 2017,Charles Whalen,Felix Reichling,,Male,Male,Unknown,Male,"In choosing how much to work, people respond to incentives that are partly determined by federal fiscal policies. The effects of changes in federal taxes and spending on economic output will depend in part on the responsiveness of the supply of labor to those changes in policy. This article assesses the literature on that responsiveness in the case of a temporary change in after-tax compensation. In particular, the article reviews the literature on the Frisch elasticity. Approaches to estimating the responsiveness of labor supply vary according to whether changes in after-tax compensation are viewed as permanent or temporary. The responsiveness to a permanent change in after-tax compensation — a tax cut that would permanently reduce marginal tax rates, for example — can be measured by the substitution and income elasticities.Footnote 1 The responsiveness to a one-time, temporary change in after-tax compensation is described by the Frisch elasticity, which is the sum of the substitution elasticity and a measure of people’s willingness to trade work for consumption over time.Footnote 2
 Estimates of the Frisch elasticity can be generated using data on changes in individuals’ hours of work or data on changes in the total hours worked in the economy. Estimates based on data for individuals — micro data — vary substantially not only with the assumptions made in the estimation process but also with the individuals studied. For example, estimates of the Frisch elasticity are often different for women and men as well as for younger and older workers. Estimates based on changes in the total hours worked in the economy — macro data — are generally much larger than those based on micro data. Estimates of the Frisch elasticity also depend on whether changes in after-tax compensation affect workers’ decisions about how many hours they work (the intensive margin) or decisions to work or not (the extensive margin). Although estimates vary widely, we find that the estimates of the Frisch elasticity most relevant for fiscal policy analysis range from 0.27 to 0.53 (with a central estimate of 0.40). Using that range, we conclude this article by illustrating how different Frisch elasticities affect the responsiveness of labor supply to changes in fiscal policies. The illustration shows that estimation of the Frisch elasticity can have a significant influence on analyses of the economic effects of such policy changes.",26
43,1,Eastern Economic Journal,02 December 2016,https://link.springer.com/article/10.1057/eej.2015.30,Charitable Contributions of Time and Money: A Multivariate Sample Selection Approach,January 2017,Steven T Yen,Ernest M Zampelli,,Male,Male,Unknown,Male,"Current law allows itemizing taxpayers to deduct their charitable contributions from adjusted gross income in calculating federal tax liabilities. For the 5-year period 2010–2014, the Joint Committee on Taxation (JCT) [2010] has estimated that this tax preference cost the US Treasury over US$200 billion. The Congressional Budget Office [CBO 2011], Cordes [2011], and Galle et al. [2012] have conducted analyses of various options for changing the federal government’s tax treatment of charitable donations and provided estimates of the impacts of these changes on both the level of charitable donations and on the cost to the federal government. In estimating such impacts, the tax price elasticity of charitable contributions is a parameter of crucial importance. The aforementioned studies provide estimated impacts using a low tax price elasticity of −0.5 and a high tax price elasticity of −1.0. Historical estimates of the tax price elasticity of charitable donations [Peloza and Steel 2005], however, span a wide range of values from 0 [Taussig 1967] to −7.07 [Robinson 1990] and come mostly from studies that model only charitable contributions of money [e.g., Clotfelter 1980; Brown 1987; Duquette 1999; Auten et al. 2002]. Exceptions to this include Brown and Lankford [1992], Andreoni et al. [1996], and Apinunmahakul et al. [2009] who model charitable donations of money and time simultaneously. Most importantly, all of the aforementioned research concentrates on the decision of how much to give to charity (the levels decision), either ignoring the selection decision of whether to give to charity or not, or assuming implicitly that it is governed by the same stochastic process which drives the levels decision. In this paper, we use the multivariate sample selection system developed by Yen [2005], which extends Heckman’s [1979] bivariate selection model, to improve upon previous empirical approaches on the topic in two important respects: (1) the binary (selection) and level outcomes are determined by separate stochastic processes and (2) the error terms of all selection and level equations are allowed to be correlated, which, in turn, accommodates potential feedback among all binary and level decisions on charitable donations. Our price elasticity estimates for monetary donations to both religious and secular charities are considerably greater than one in absolute value and suggest that changes in the tax treatment of charitable contributions may lead to substantial changes in the levels of both types of charitable giving. The remainder of the paper is organized as follows. A brief history of the tax preference for charitable donations along with a review of the relevant past research on charitable contributions are provided in the next section. The section after that and the following section present our theoretical framework and econometric model, respectively. Data are described in the subsequent section, with results presented and discussed in the penultimate section. The final section summarizes the paper and offers some concluding remarks.",
43,1,Eastern Economic Journal,02 December 2016,https://link.springer.com/article/10.1057/eej.2015.34,"Predicting First-year Law School Performance: The Influences of Race, Gender, and Undergraduate Major",January 2017,John Fordyce,Lisa K Jepsen,Ken McCormick,Male,Female,Male,Mix,,
43,1,Eastern Economic Journal,02 December 2016,https://link.springer.com/article/10.1057/eej.2015.36,Inflation Persistence Before and After Inflation Targeting: A Fractional Integration Approach,January 2017,Giorgio Canarella,Stephen M Miller,,Male,Male,Unknown,Male,"Since first adopted in New Zealand in 1990, inflation targeting (IT) has become the focal point of extensive research, especially since a growing number of central banks, in both developed and emerging countries, have shifted toward using IT to implement their monetary policy. A number of studies have emerged that attempt to measure the impact and effectiveness of IT on economic performance — lowering the average inflation rate, raising the average rate of output growth, and lowering inflation and/or output growth volatility [Ball and Sheridan 2005; Batini and Laxton 2007; Gonçalves and Salles 2008; and more recently, Brito and Bystedt 2010; Fang et al. 2010]. Another strand of empirical literature examines the successfulness of IT in reducing inflation persistence. Inflation persistence is an unobservable variable that plays an important role in the conduct of monetary policy. Given an inflationary shock, inflation returns to its target more quickly (i.e., inflation exhibits less persistence), the more effectively the monetary authorities can reduce inflation fluctuations, all else equal [Fuhrer 1995].Footnote 1 Since more persistent inflation takes longer to adjust to an inflation shock, the monetary authorities may need to implement a policy response to bring inflation back to its equilibrium level. On the other hand, with low inflation persistence, inflation reverts to its initial level more quickly after a shock occurs. In such a case, the response to the inflation shock may not require active monetary policy. In the worst case, inflation may follow a random walk [i.e., an I(1) process] making it impossible for central banks to bring it under control. In the best case, inflation may follow an I(0) process, implying that it reverts to its initial level soon after a shock occurs.Footnote 2 As a consequence, the optimal timing and size of monetary policy crucially depend on the knowledge of how shocks affect the dynamics of inflation. This is particularly important for countries with an IT regime. The IT literature maintains as a central proposition that establishing well-anchored inflation expectations supports a successful IT policy. Well-anchored expectations enable the central banks that adopt IT to maintain stability of output and employment in the short run, while ensuring the stability of prices in the long run. Erceg and Levin [2003] and Orphanides and Williams [2005] show that inflation proves less persistent if agents know more about central bank objectives. In the imperfect credibility model of Erceg and Levin [2003], the lack of credibility on the inflation target provides an important source of inflation persistence. Similarly, in the model of Orphanides and Williams [2005], well-anchored long-run inflation expectations lead to less persistent inflation than if the public remains uncertain about the long-run inflation objective. Recently, Yigit [2010] shows that inflation persistence declines if IT succeeds in reducing the heterogeneity of inflation expectations. Thus, the analysis of inflation persistence is particularly important for countries with an IT regime since the decline of inflation persistence reflects the improved credibility of monetary policy and suggests that inflation expectations have become better anchored than previously was the case. Siklos [1999]
Kuttner and Posen [2001], Pétursson [2005], Benati [2008], and Yigit [2010], however, provide mixed evidence as to whether IT reduces the persistence in inflation rates. A large number of studies in the existing literature model inflation as an autoregressive process and measure persistence using different metrics, such as the integer order of integration, the half-life of responses to shocks, the largest autoregressive root, and the sum of the autoregressive coefficients.Footnote 3 By contrast, this paper focuses on an alternative parameterization of inflation dynamics, examining inflation persistence within the context of a long-memory I(d) model, where d is the order of fractional integration. The I(d) approach provides a more powerful framework to detect persistence than the standard unit-root analysis. From an integer order of integration perspective, inflation exhibits either a stationary, I(0), or a random walk, I(1), process. In the first case, shocks do not persist. They are transitory and mean-reverting. In the second case, shocks are permanent and non-mean-reverting. The developing interest in I(d) reflects the growing awareness of the limitations of the I(1)-I(0) framework and the increasing interest in models used to describe long-memory phenomena, and patterns of mean-reversion and responses to shocks that do not exist in conventional unit-root analysis.Footnote 4
 This paper examines the relationship between the adoption of IT and inflation persistence. We employ fractional integration methods to characterize the behavior of inflation dynamics for 13 OECD countries that switched to IT in 1991 or earlier: Australia, Canada, Chile, Iceland, Israel, Mexico, New Zealand, Norway, South Africa, South Korea, Sweden, Switzerland, and the United Kingdom. For each country, we estimate the persistence of inflation in the respective pre- and post-IT periods, and then proceed to test for equality of the persistence parameters across these two periods. We find that (i) inflation exhibits a fractional integration behavior over the entire sample period and the pre-IT period, and, in most cases, in the post-IT period, (ii) the adoption of IT coincides with a structural break in the inflation series, (iii) significant variations and asymmetries exist in inflation persistence across the IT countries, (iv) only about half of the countries in the sample experience significantly lower persistence in the IT period, (v) in the IT period all inflation series exhibit mean reversion and stationarity, while mean reversion and stationarity as well as mean reversion and non-stationarity are present in the pre-IT period, and (vi) for one country, South Africa, persistence increases, switching from stationarity to non-stationarity, after the adoption of IT. 
Yigit [2010] comes closest to our work. He shows that long memory in inflation series can reflect heterogeneous inflation expectations. Other explanations include money supply persistence, aggregation across heterogeneous production by firms, or aggregation of individual prices into an index of prices. Yigit [2010] then argues that the adoption of IT can reduce the heterogeneity of inflation expectations by reducing the extent of long memory, something that would not occur if the other explanations caused long memory in the inflation series. Yigit [2010] estimates the fractional integration parameter in the inflation process before and after the regime switch to IT using an autoregressive fractionally integrated moving average (ARFIMA) model with and without a generalized autoregressive conditional heteroskedasticity (GARCH) error structure. Our methodology differs from Yigit [2010]. We estimate inflation persistence using the modified log periodogram (MLP) method proposed by Kim and Phillips [2006, 2000] and Phillips [2007]. The method is robust to AR(1) and MA(1) specifications [Choi and Zivot 2007], and, unlike the ARFIMA approach, is semiparametric and, therefore, does not require the specification of the ARMA model.Footnote 5
 The MLP method extends the basic log periodogram regression to include the important case of the unit root. That is, much applied econometric work use the log periodogram regression originally developed by Geweke and Porter-Hudak [1983]. But, its validity is limited to the stationary region. Conversely, the MLP regression proposed by Kim and Phillips [2006, 2000] and Phillips [2007] extends statistical estimation and inference to the non-stationary region and, therefore, can appropriately test a fractionally integrated process I(d) against both I(0) and I(1) processes. The remainder of the paper is organized as follows. The next section describes the data and presents results of unit-root and stationarity tests. The conflicting nature of the findings of these tests suggests that neither the I(0) nor the I(1) models prove entirely appropriate for the inflation data. The subsequent section briefly recalls the fractional integration approach for measuring inflation persistence and describes the MLP proposed by Kim and Phillips [2006, 2000] and Phillips [2007], which we apply to the inflation series. The latter section reports the estimates of the persistence parameters based on the entire sample and the two subsamples defined by the date of adoption of IT, and presents the findings of the Hassler and Olivares [2008] and Kumar and Okimoto [2007] tests of the null hypothesis of no change in the fractional parameter d. This, in turn, enables us to formally answer the main question of whether inflation persistence changes after the adoption of IT. The final section summarizes the main results and provides concluding remarks.",10
43,1,Eastern Economic Journal,02 December 2016,https://link.springer.com/article/10.1057/eej.2015.3,Dissimilar Relations Between Income and Environmental Quality for Open Economies in a Growth Model,January 2017,Bidisha Lahiri,,,Unknown,Unknown,Unknown,Unknown,,
43,1,Eastern Economic Journal,02 December 2016,https://link.springer.com/article/10.1057/eej.2015.40,"Educational Quality Matters for Development: A Model of Trade, Inequality, and Endogenous Growth",January 2017,Joshua D Hall,,,Male,Unknown,Unknown,Male,"The distribution of income across individuals and across countries has long been an important issue in economics and has come to the forefront of policy discussions across the development spectrum. In his seminal contribution, Kuznets [1955] suggests inequality within a country will rise in the early stages of development where investment in physical capital is the engine of growth, yet decline in latter stages as human capital becomes the primary growth mechanism. Empirically, however, within-country income inequality grew substantially in much of Latin America between the late 1970s and early 2000s only to decline in the majority of countries between 2002 and 2010. East Asian countries experienced a different trend with declining inequality followed by a subsequent increase during the same period.Footnote 1 Inequality across countries, measured by the per capita GDP relative to the United States, diverged Latin America and Africa on average, while East Asia experienced convergence.Footnote 2 This paper both links income inequality both within and across countries, and also presents a mechanism that can explain the varied dynamics of inequality among countries and regions. The paper presents a rich, theoretical model that shows the quality of education is a major source of the observed trends in inequality across developing countries. This endogenous growth model contributes by focusing on how both the rate of technological progress and the quality of education interact in the formation of human capital and its distribution. Trade liberalization between a developed and a developing country sparks a change in the rate of technological progress both in the short run and long run. The change in technological progress alters the level of human capital but the direction is dependent on the quality of education. Ultimately the distribution of income changes as well both in steady state and the dynamic transition. Specifically, inequality within developing countries fall if there is a higher quality of education and the country as a whole converges in comparison to the developed country. The reverse is true for countries with a lower quality of education. Skill-biased technical change and globalization are utilized to explain the dynamics of within-country inequality, but the specific mechanisms to account for differences among countries vary among the previous literature. Galor and Tsiddon [1997], Greenwood and Yorukoglu [1997], Caselli [1999], Lloyd-Ellis [1999], Aghion et al. [2002], and Aghion [2002] focus on technological revolutions that give rise to an increase in demand for skilled workers, and thus put upward pressure on their relative wage. Acemoglu [1998] argues a sharp (exogenous) increase in the supply of skilled workers raises the return to innovations targeted at skill-intensive sectors which leads to an increase in their relative wage. Galor and Moav [2000] introduce the idea that the rate of technological progress determines the relative demand, and reward, for skilled labor. While these papers emphasize the relationship between technology and inequality, this paper focuses on why the dynamics of inequality differ among developing countries, namely the quality of education. The globalization argument [see Wood 1994] stems from the Stolper–Samuelson theorem where the reduction in impediments to trade with skill-scarce countries increases the relative demand for skilled workers in the skill-abundant countries, and therefore raises the skill premium. However, the theory also suggests trade liberalization decreases the relative demand and rewards for skills in less developed countries, which is not consistent with empirical evidence. Dinopoulos and Segerstrom [1999, 2006], Sener [2001], Acemoglu [2003], Grieben [2005], and Zeira [2007] provide more unified models of technology and trade that avoid the pitfalls of the Stolper–Samuelson theorem. Ripoll [2005] develops a general equilibrium model of trade and finds that initial conditions, such as the skilled–unskilled labor ratio, are important to the dynamics of income inequality following trade liberalization. The present model also accounts for income inequality across countries. This paper suggests that the quality of education can provide an explanation for the diverse dynamics of inequality in developing countries. In a general equilibrium endogenous growth model with North–South trade in intermediate goods, the dynamics of inequality depend on the interaction of technological progress, and the ability of workers to adapt to new technologies. The ability of the workers to adapt is a function of the quality of education, which characterizes the incentives to invest in human capital. Trade liberalization, then, alters the rate of technological progress in a non-monotonic way which drives an initial increase in income inequality, and cross-country divergence, if the quality of education in the developing country is low. If the quality of education is high, however, trade liberalization can reduce income inequality over the transition path, again through its impact on the rate of technological progress. Defining human capital and its relationship with the rate of technological progress, the quantity of education and the quality of education is fundamental to this model. The formation of human capital is motivated by three empirical observations. First, individual earnings increase with ability [Griliches and Mason 1972; Murnane et al. 1995]. In this model, individuals are differentiated by ability in which higher innate ability reduces the cost of attaining secondary education. Second, technological progress increases the relative return to education [Ferguson 1993; Bartel and Sicherman 1999]. This feature is captured in the model by assuming that the education premium is an increasing function of the rate of technological progress. A rise in the rate of technological progress increases the education premium for workers with secondary education, which increases the overall effective human capital. On the intensive margin, an increase in the rate of technological progress increases the relative productivity of skilled workers, while on the extensive margin, a rise in the rate of technological progress induces more unskilled workers to make the discrete decision to attain additional education and become skilled. Third, Bartel and Sicherman [1998] show that an increase in the rate of technological progress increases the need to (re)train workers, especially low-skill workers. To capture this effect, the number of efficiency units of unskilled labor supply is assumed to be decreasing in the rate of technological progress. Faster technological change, therefore, reduces the supply of effective human capital by reducing the time unskilled workers spend in wage earning production. Instead, unskilled workers spend more time adapting to the new technologies. This echoes the formulation modeled in Galor and Moav [2000]. The time cost, however, is mitigated by a higher quality of education. Taken together, faster rates of technological change induce competing effects on effective human capital. Faster technological growth raises the return to additional education which serves to increase effective human capital on both the intensive and the extensive margins, while the loss of labor supply of unskilled workers reduces the level of human capital. The strength of each effect depends on the quality of basic (unskilled) or additional (skilled) education. Overall, effective human capital can rise or fall during periods of increased technological change depending on the quality of education. The model produces thresholds for the quality of education, above (below) which an increase in the rate of technical progress enhances (reduces) the effective human capital. This threshold is found to be important in explaining the diverse patterns of income inequality. The transitional dynamics of income inequality following trade liberalization depend on the quality of education in the developing country (the South). In cases in which the South has a high quality of education (above the threshold), trade liberalization induces a U-shape dynamic transition of within-country income inequality between steady states. In this case inequality within the South will fall in the short run. In cases in which the South is below the threshold quality of education, inequality follows an inverted U-shape transition following a reduction of trade barriers, and rises in the short run. Empirical observations, as noted below, echo these diverse patterns and the model concludes that a faster rate of technological progress increases inequality, unless the quality of education is sufficiently high. Furthermore, the model predicts greater divergence for countries with a low quality of education relative to those with a higher quality of education at the time of trade liberalization. Allowing for heterogeneity in the quality of education across countries makes the model’s predictions consistent with the differential dynamics of inequality between East Asia and Latin America. In the long run, the model shows Southern-originating trade liberalization leads to: (1) a greater rise of income inequality within developing countries with relatively low quality of education; and (2) a greater convergence for countries with high quality of education relative to other developing countries with a lower quality of education. The model is also consistent with wage dynamics of developed countries. For example, the model predicts a growth in income inequality despite an increase in the supply of educated workers, the decline in the average wages of unskilled workers, along with an increase in inequality within skill cohorts. The key contribution of this paper is to present a mechanism that can account for a wide array of income inequality dynamics. The remainder of the paper is organized as follows. The next section details the empirical motivation; the section after that introduces the model; the following section solves for the steady-state and transitional dynamics; the penultimate section presents and discusses the implications of Southern and Northern trade liberalization on the distribution of wages; and the final section concludes and discusses further empirical implications of the model.",2
43,1,Eastern Economic Journal,24 October 2016,https://link.springer.com/article/10.1057/s41302-016-0076-8,Trade and Environmental Quality in African Countries: Do Institutions Matter?,January 2017,Mina Baliamoune-Lutz,,,Female,Unknown,Unknown,Female,"In recent years, many African countries have increased exports to the world, particularly to China. In some cases, this trend has also been accompanied by an increase in inward FDI, especially in resource-rich countries. At the same time, a large number of countries are still faced with the challenges of reducing poverty and unemployment. Both the rise in exports and the fight to reduce poverty could suggest possible threats to environmental sustainability in Africa. The World Bank defines environmental sustainability as “[e]nsuring that the overall productivity of accumulated human and physical capital resulting from development actions more than compensates for the direct or indirect loss or degradation of the environment” [World Bank 2008]. The United Nations (UN) Development Goal (MDG) 7 is specifically about ensuring environmental sustainability. Target 7a is to “integrate the principles of sustainable development into country policies and programs, and to reverse loss of environmental resources.” The first two indicators associated with this target (as well as with target 7b) are (1) proportion of land area covered by forest, and (2) total CO2 emissions per capita and per $1 GDP (in purchasing power parity values). This paper examines the impact of trade and institutions on environmental quality (environmental sustainability)1 in African countries and explores whether political institutions matter to the trade–environment relationship. We investigate whether trade has a significant effect on deforestation and pollution, and whether political institutions mitigate this effect. To do so, we use data covering the period 1990–2008 and variables that are directly related to two indicators identified by the UN as indicators associated with MDG 7: net forest depletion (NFD) and CO2 (carbon dioxide) emissions. More specifically, using the Arellano–Bover system GMM (GMM-SYS) estimator, we examine the effects of trade on environmental quality, distinguishing between the effects of trade volume, export of forest products, and fuel exports. We also examine the impact of institutional quality, focusing in particular on the interplay of political institutions and trade. The empirical results indicate that fuel exports seem to have a negative effect on environmental quality in Africa. Furthermore, political institutional quality has a direct negative impact on the environment and an indirect positive effect through its interaction with trade. Interestingly, we find that polity — an indicator of political institutions — has a U relationship with environmental quality when we use NFD. We obtain support for the well-known environmental Kuznets curve (EKC) in the case of pollution (CO2 emissions) but not in the case of NFD (deforestation). We also derive evidence suggesting that urbanization has a positive and linear impact on CO2 emissions and a U-shaped link with deforestation. The remainder of the paper is organized as follows: In “OVERVIEW OF THE EMPIRICAL LITERATURE” section, we present a brief review of the relevant literature. “VARIABLE SELECTION AND METHODOLOGY” section describes the variables and methodology. We present and discuss the estimation results in “ESTIMATION RESULTS” section. “SUMMARY AND DISCUSSION” section provides policy discussion and concludes.",3
43,1,Eastern Economic Journal,16 November 2016,https://link.springer.com/article/10.1057/s41302-016-0078-6,Reforming the Affordable Care Act,January 2017,David Colander,,,Male,Unknown,Unknown,Male,"Economists know how to hold down costs: Design the health care system so that it follows the golden rule of economics: Him who pays makes the rules. Costs of health care will be contained only if they are directly connected to what the person or organization paying for that health care is paying. That means that if government pays, experts appointed by government decide what health care will be available to individuals, and how much government will pay for it. If individuals pay, then individuals decide; if insurance companies pay, then insurance companies decide. The US health care system is an unmanageable blend of all three of these in which who decides and who pays are largely disconnected. Sophisticated progressives who championed the ACA understood the golden rule of economics; they recognized that the ACA would make the health care system more of a mess. They supported it nonetheless because (1) they believed it would improve accessibility for some low-income individuals, and (2) they saw it as a path to a government single-payer system in which medical costs were financed by general government revenue. They believed that if (1) government were the single primary payer for health care; (2) there was a board of experts who decided what health care people would get; and (3) government could bargain with providers to hold costs down; eventually the changes brought about by the ACA would lead to a reasonably efficient and fair health care system. Once that happened, total costs of our health care system would be reduced and medical care would be more accessible to poor people than it is now. They based this belief on the fact that some version of a single-payer system is used by most advanced countries, and, in terms of holding down costs, and providing broad access to medical care for all, these systems work much more efficiently than the current U.S. system, albeit with many well-known problems.3
 The second way of holding down health care costs is what is sometimes called the market solution, but a better name for it, which contrasts it with the single-payer system, is a multi-payer system. Like the single-payer plan, a multi-payer plan meets the golden rule of economics. The difference is that the main source of cost control comes from individuals because it is individuals, not government, who are paying, and it is individuals, not government, who are making decisions about what type and how much health care to consume. In a multi-payer health care system, the majority of individuals pay for their own health care, either directly, or indirectly through insurance that they buy and pay for. I do not like to call a multi-payer health care system a “market system,” because health care is not a normal market good. There are two reasons why. First, unlike most goods, society sees everyone as having a right to a minimum level of health care regardless of income. That decision means that the distribution of health care cannot be left solely to the market. A second reason is that any reasonable multi-payer system will have many non-market attributes because health care consumers often lack the information, and know they lack the information, needed to make reasonable health care decisions. An unregulated market system where medical care providers are primarily concerned with monetary profit, not with providing quality health care at affordable prices, is not a reasonable system since it leaves many susceptible to snake-oil medicine. Thus, any realistic multi-payer system will involve an important role for government, non-profit, and for-benefit provision or subsidization of health care to some portion of the population.4 The policy debate is not about market versus government provision. The debate is about whether the central institutional structure emphasizes bottom-up consumer control, or top-down government control. Currently, the U.S. is on a path to a single-payer system, not because that makes the most sense, but because the current U.S. health care system is structured in a way to essentially make it impossible for a multi-payer system in health care to work. The reason is that a reasonable multi-payer system requires (1) consumer cost transparency of both medical care and medical insurance, and (2) relatively equal bargaining power for all consumers. Our current system has neither. A reasonable multi-payer system requires that the price charged individuals for medical care and health insurance reflects the economic cost of providing that care and insurance. Our current system does not do that. Instead, it obscures costs of both health care and insurance in ways that are strongly biased against a multi-payer system.",
43,1,Eastern Economic Journal,25 October 2016,https://link.springer.com/article/10.1057/s41302-016-0085-7,Health Care: Multi-Payer or Single-Payer?,January 2017,Dhaval Dave,,,Unknown,Unknown,Unknown,Unknown,,
43,1,Eastern Economic Journal,02 December 2016,https://link.springer.com/article/10.1057/eej.2014.54,West African Agriculture and Climate Change: A Comprehensive Analysis,January 2017,Richard B Dadzie,,,Male,Unknown,Unknown,Male,,
43,1,Eastern Economic Journal,02 December 2016,https://link.springer.com/article/10.1057/eej.2014.69,Worldly Philosopher: The Odyssey of Albert O. Hirschman,January 2017,Robert Prasch,,,Male,Unknown,Unknown,Male,,
43,2,Eastern Economic Journal,19 January 2017,https://link.springer.com/article/10.1057/s41302-016-0090-x,Introduction to the Symposium on Agent-based Modeling,March 2017,Christopher S. Ruebeck,Leanne J. Ussher,Jason M. Barr,Male,Female,Male,Mix,,
43,2,Eastern Economic Journal,22 February 2017,https://link.springer.com/article/10.1057/eej.2016.2,Elements of Dynamic Economic Modeling: Presentation and Analysis,March 2017,Leigh Tesfatsion,,,,Unknown,Unknown,Mix,,
43,2,Eastern Economic Journal,21 September 2016,https://link.springer.com/article/10.1057/s41302-016-0073-y,Matching Impacts of School Admission Mechanisms: An Agent-Based Approach,March 2017,Shu-Heng Chen,Connie Houning Wang,Weikai Chen,Unknown,,Unknown,Mix,,
43,2,Eastern Economic Journal,22 February 2017,https://link.springer.com/article/10.1057/eej.2016.6,The Consequences of Social Pressures on Partisan Opinion Dynamics,March 2017,Shyam Gouri Suresh,Scott Jeffrey,,Male,Male,Unknown,Male,"The evolution of opinions in a polity is greatly influenced by partisan politics. The precise nature of this partisan influence can be explained through a variety of mechanisms. Members with opinions that diverge greatly from their party may switch parties depending on the extent of their party loyalty. Members of a party may interact more frequently with other members of their own party, thereby leading to opinions coalescing in partisan ways. This effect of opinions within a party influencing each other through interactions would be magnified if members of a party felt socially pressured to conform to their party line in public conversations. In addition, partisan ideologues who adopt a hardline stance on issues could have a disproportionate influence on the opinions of their fellow party members. We construct a parsimonious agent-based two-party model that incorporates the above features in the context of the United States (US) politics. We find that the socially acceptable range of opinions in each party and the level of partisanship prevalent in the polity play a role in determining outcomes such as party memberships, median opinions, polarization, and the marginalization of extreme opinions. Many empirical studies suggest that political parties influence public opinion in the US. For instance, Noel [2013, p. 159] finds that opinions on abortion were considerably more uniform across members of both parties in 1980 whereas by 2000, opinions became considerably more polarized along partisan lines. Similarly, there is evidence for partisan influence in the context of gay marriage as well. Dimock et al. [2013, p. 6] uses Pew Research Center’s survey data to show that despite increasing support for gay marriage among Republicans and Democrats between 2001 and 2013, the gap in the support for same sex marriage increased by 10 percentage points over this time. Another recent example comes from Dunlap and McCright [2008], who find a widening gap between Democratic and Republican opinion on climate change between 1997 and 2008. With regard to overall opinions, Dimock et al. [2014, p. 19] find that the gap between the median Democratic opinion and the median Republican opinion on the ideological consistency scale (based on 10 separate issues) has widened considerably between 2004 and 2014. The systematic nature of these observed partisan polarizations of opinions on various issues suggests a potential common underlying model for opinion dynamics. Further, there is evidence to indicate that political elites shape voter attitudes within parties. For example, Brulle et al. [2012] find that while actual weather events and research publications do not influence public opinion on climate change noticeably, cues such as public statements from the political elite make a significant impact. More generally, according to Carmines and Stimson [1986, p. 904], “visible changes in elite behavior serve to redefine party images, to affect emotional response to the parties, and ultimately to realign the constellation of voter issue attitudes and party identification.” Noel [2013, p.158] also provides evidence to support the claim that as Congressional leaders become more polarized, public opinion polarizes as well, leading to parties adopting more extreme stances based on this public opinion. Lebo et al. [2007] analyze the relationships among party unity, legislative success, and seats in Congress, highlighting the necessity for overall party coordination on given issues. The relationship they find is one where political elites influence the public opinions of individuals, and political parties set their agenda around this shifting public opinion. Opinion formation is also influenced by social pressures. Several fields have studied various aspects of this social pressure. In political science, the term “spiral of silence” that originates from Noelle-Neumann’s [1974] work on public opinion, refers to a situation where people find themselves agreeing with the public opinion instead of voicing their own beliefs that leads to collective decisions based on these falsely supported views. Glynn et al. [1997] conduct a meta-analysis of related empirical literature and find small but statistically significant support for the effects of the spiral of silence. In economics, Kuran [1987a, 1987b; 1991] spearheads the literature on the process of “preference falsification” whereby people may publicly misrepresent their preferences because of social influences. In social psychology, according to O’Gorman [1986, pp. 335–336], “the term ‘pluralistic ignorance’ was coined by Floyd H. Allport and first appeared in 1931 in a book [Katz et al. 1931] he wrote with Daniel Katz … Allport conceived of pluralistic ignorance as a special version of the illusion of universality … [T]he individual frequently is not in a position to know the actual views, behavior, and experiences of other individuals … [and] make[s] unwarranted assumptions about others, and as a result impressions of universality often become … patterns of pluralistic ignorance.” Taylor [1982] conducts an empirical analysis on American environmental politics to study the interconnections between the theory of pluralistic ignorance and the theory of spiral of silence and he finds evidence in favor of both these theories. All three concepts — spiral of silence, preference falsification, and pluralistic ignorance — stem from the same notion of social influence. The “Overton Window”Footnote 1 is also related to these three phenomena. Named after Joseph Overton of the Mackinac Center who introduced the idea, the Overton Window is defined as the range of politically acceptable options for a politician to support for electoral success. Although the Overton Window theory developed by the Mackinac Center does not explicitly consider the partisan nature of the window, we believe that politicians face party-specific constraints. Thus, our first extension of this concept is to suggest that the Overton Window is party specific. Further, based on the social pressure literature cited above, we extend the concept to model the Overton Window as a party-specific restraint that also applies to individuals’ public opinions. Although our use of this term is at a slight variance with the original definition, we find this concept to be visually helpful in understanding the constraints on ideas that individuals can espouse publicly. Therefore, in our conception, the Overton Window imposes a party-specific message discipline on the public opinions in the polity. To study opinion formation and the consequences that tight message disciplines create, we use an opinion dynamics model. DeGroot [1974] conducted seminal work in the field of opinion dynamics by constructing an analytical model, where the opinions of individuals are influenced by the opinions of their peers. As in most other mathematical models where a positive weight is attached to the opinions of others, DeGroot [1974] finds that all opinions reach consensus eventually. Acemoglu et al. [2013], DeMarzo et al. [2003], and Acemoglu and Ozdaglar [2011] also focus on the long-run properties of opinion dynamics using analytical models. However, Acemoglu and Ozdaglar [2011, p. 45] acknowledge, “opinion dynamics away from this very long run limit are often important and interesting but more difficult to study.” As our focus is on the short and intermediate aspects of opinion dynamics rather than the long-term equilibrium,Footnote 2 we use agent-based modeling rather than an analytical approach as it allows us to capture these dynamics explicitly in an environment with heterogeneity. The agent-based approach has been adopted by numerous studies on opinion dynamics. Two prominent agent-based opinion dynamics models include the Bounded Confidence model [Deffuant et al. 2000; Hegselmann and Krause 2002] and the Relative Agreement model [Deffuant et al. 2002, 2013; Meadows and Cliff 2012]. Other recent work in this literature includes Acerbi et al. [2009], and Huang and Wen [2014]. The latter authors incorporate social psychology into their model and connect their results to pluralistic ignorance. For a more detailed literature review of agent-based models of opinion dynamics, refer to Sobkowicz [2009]. Our model adds to this body of literature on opinion dynamics by capturing the effects of social pressures that arise from tight message discipline within political parties. We find that our simple framework can explain certain features of recent data such as the influence of parties on individual opinions and the increasing polarization of the polity.",4
43,2,Eastern Economic Journal,31 August 2016,https://link.springer.com/article/10.1057/s41302-016-0012-y,The Complexity of Coordination,March 2017,Davoud Taghawi-Nejad,Vipin P. Veetil,,Unknown,Unknown,Unknown,Unknown,,
43,2,Eastern Economic Journal,18 October 2016,https://link.springer.com/article/10.1057/s41302-016-0075-9,The Future of Agent-Based Modeling,March 2017,Matteo G. Richiardi,,,Male,Unknown,Unknown,Male,"Although there are early antecedents, it is now at least two decades that agent-based (AB) models have been introduced to economics.1 It is therefore time to ask whether they have marked an impact on the way economics, and especially macroeconomics, is done, and what their future prospects look like. This is all the more relevant given the debate on macroeconomic modeling which was prompted by the Great Recession, and Ricardo Caballero’s suggestion that macroeconomics should be in “broad-exploration” mode [Caballero 2010]. Forecasting the future is notably a difficult exercise, but a common perception is that AB models have “a bright future past them”: they have been charged with high expectations, especially from outside the mainstream literature, which they somewhat failed to live up to. With respect to mainstream economic models, AB models trade off individual sophistication (in expectation formation and decision making) with complexity in the interaction structure and richness in the institutional details. The rational expectations (RE) hypothesis at the core of mainstream economic models, with its strong consistency requirements – all actions and beliefs must be mutually consistent at all times – requires simplistic models. This is replaced in AB models by the assumption of partial information and limited computational ability, which brings in milder evolutionary requirements: corrections must take place, through learning, selection or reactions in the environment. The resulting increased flexibility in model specification, however, has downsides: (i) assumptions are sometimes deemed arbitrary and disconnected from the literature, suggesting a return to the sort of anarchy that was lamented before the RE revolution [Wickens 2014], (ii) models often exhibit too many degrees of freedom, and are therefore non-falsifiable, (iii) models often lack a sound empirical grounding; when present, this is often limited to some ad hoc calibration [Grazzini and Richiardi 2015]. Other common critiques point to the fact that (iv) models are oftentimes poorly documented and hardly replicable [Leombruni et al. 2006], and (v) writing an AB model requires quite a lot of programming skills; code is often not re-usable and projects are not incremental [Leombruni and Richiardi 2005]. A search on EconLit returns 5,705 articles for Dynamic Stochastic General Equilibrium (DSGE), Real Business Cycle (RBC) and New Keynesian (NK) models since 1980, and only 1,062 hits for AB models (Figure 1).2
 Number of articles retrieved in the EconLit database (all types of publications) searching for “Dynamic Stochastic General Equilibrium,” “Real Business Cycle,” “New Keynesian,” “Agent-based,” variants and acronyms, 19/1/2015 Does this graph suggest that AB modeling is ready for take off, as the new consensus in macroeconomics in the early 2000s, or that it has already leveled off? In this paper I argue that in order to reap all the benefits of the AB approach, a change in strategy is required, and this allows us to address all the critiques listed above. Abandoning the consistency requirements of RE equilibrium removes the need for solving the models as one single block: in principle AB models can be fully modularized, with endless possibilities for recombination and extension. However, this feature has not been exploited so far. Existing AB models are mostly one-off exercises which do not travel across research groups and whose “working life” does not usually extend beyond the grant that originated them. Code is not re-used, except possibly by the authors themselves; alternative assumptions are not tested and results are not generalized. In short, knowledge accumulates at a slow pace. The reason for this “modeling individualism,” – as we might call it, is to be searched in the struggle to win the “modeling race,” in line with existing incentives in terms of (short-term) publications and funding opportunities. Conversely, by fully exploiting the modular nature of AB models, a common computational environment can be built, where different behavioral assumptions, markets, and institutional frameworks can be implemented as separate simulation modules. Classification of and experimentation with different behavioral assumptions, from choice heuristics to learning and expectation formation, will then provide a firm foundation for the modeling assumptions made (critique i above); comparison of different specifications will allow to reduce the number of parameters to a minimum and focus on lean structures (point ii); adoption of common computational interfaces will allow the development of standard simulation-based estimation procedures (point iii) and will directly address points (iv) and (v). The remaining of the paper is structured as follows. “The Great Recession Poses a Serious Challenge to Mainstream Macro” section suggests that mainstream macroeconomic models are not “systemic” models, and are thus not suited for analyzing endogenous changes in the economic structure, like those we have experienced during the Great Recession. “DSGE Models Cannot Become “Systemic” Models” section discusses why these models cannot be further extended in order to provide such a vital development. “Macroeconomic Research in ‘Broad-Exploration’ Mode” section explains why AB models, by departing from the strict requirements of RE equilibrium analysis in favor of an evolutionary approach, can provide a valid alternative. “The Vision for a New Modular Macroeconomic Science” section elaborates on the “double dividend” that can be obtained from moving away from RE, as models can in principle be fully modularized. “Challenges” section discusses the main challenges that a new Modular Macroeconomic Science faces, as agents have to share common ontologies (an ontology is a part of the agent’s knowledge base that describes what kind of things an agent can deal with and how they are related to each other). “Practical Steps” section suggests a practical roadmap for testing the new modular approach and build a Modular Macroeconomic Simulator, developed using a readily accessible simulation platform. Finally, “Conclusions” section discusses why the modular approach has not yet materialized in AB modeling, and why the AB research community should actively engage in a collaborative effort to help the prediction of a future Modular Macroeconomic Science become true.",12
43,2,Eastern Economic Journal,28 July 2016,https://link.springer.com/article/10.1057/s41302-016-0001-1,"Property Tax Limits, Balanced Budget Rules, and Line-Item Vetoes: A Long-Run View",March 2017,John A. Dove,,,Male,Unknown,Unknown,Male,"A significant amount of scholarship has been devoted to understanding fiscal institutions and constraints imposed on various governmental jurisdictions. Much of this research has revolved around state, local, and federal comparisons within the United States.1 Three constraints in particular have received much of this scholarly attention. These include state tax and expenditure limits (TELs), state balanced budget requirements (BBRs), and the gubernatorial line-item veto (LIV). The majority of this literature has found that each of those institutions has had an important effect on a number of fiscal outcomes across states, including revenues, expenditures, and borrowing costs. However, these studies generally focus around the so called “tax revolts” of the 1970s and after, when a number of states, through legislative action or initiative and referenda, passed stringent caps on the tax and spending authority of state governments. Although an important period in the history of state fiscal policy, most of these studies miss the richness involved in the development of these fiscal limitations. This is especially important as TELs, BBRs, and LIVs actually began developing in some form over 150 years ago [Rodriguez-Tejedo and Wallis 2012]. Further, although some of these early constraints are thought to have had little to no impact on state fiscal outcomes, no formal empirical work has been undertaken to investigate this conjecture. Given this, the purpose of this paper is several folds. First, the article adds to the institutional evolution of these fiscal constraints with a brief discussion of their historical development and the political context within which they emerged and were implemented through the mid-nineteenth and into the early twentieth centuries. Interestingly, the vast majority of these limits were imposed constitutionally (and subsequently are the ones analyzed in this paper) rather than statutorily. This brief history should provide important information for future scholars to draw out about why it is that these safeguards persisted and slowly transformed in the ways that they did through the twentieth century. Second, this paper is also an attempt to go beyond mere anecdotal conjectures that, by-and-large, these constraints had little effect on state fiscal outcomes by undertaking an empirical investigation that explores their actual impact on several important indicators of state fiscal policy over the long run (covering the period 1830–1920). These indicators specifically are state revenues and expenditures. Overall, early TELs (which were actually property tax limits or PTLs) appear to have had the most significant impact by decreasing both state revenues and expenditures, BBRs actually increased revenues while having a lesser effect on expenditures, whereas LIVs seemed to have a much less pronounced effect on both. These results are robust to a number of specifications and add important nuances about the development and persistence of these important early state fiscal constitutions. This paper adds to several important strands of the literature, which includes recent developments on the fiscal effects of state TELs, BBRs, and LIVs. Although early work on many of these issues showed mixed results at best regarding the effectiveness of these institutions [see: Abrams and Dougan 1986; Lowery 1983; Bails 1982, 1990; Joyce and Mullins 1991; Mullins and Joyce 1996 in regard to TELs; and Carter and Schap 1990; Endersby and Towle 1997; Alm and Evers 1991; Nice 1988; Gosling 1986 in regard to the LIV], later scholars, incorporating both panel data and accounting for potential endogeneity within the analysis, have to some extent shown that each of these constraints can at times perform consistent with their stated goals Current literature would also suggest that state BBRs have had the most profound effect on state public finance. Specifically, Bohn and Inman [1996] show that strict BBRs consistently reduce state deficits, which are even more pronounced with the presence of the LIV. Poterba [1994] notes that states with BBRs augmented with TELs are better able to withstand fiscal shocks and more rapidly recover from unexpected deficits. Most of these results are largely corroborated by Alesina and Bayoumi [1996], Bails and Tieslau [2000], and Primo [2006]. Specifically, Primo [2006] suggests that BBRs are relatively more effective in states with an elected judiciary, as legislatures may be more likely to appoint judges who are sympathetic toward deficit spending. Additionally, Hou and Smith [2006] create several indices of state BBRs based on political and technical limits included in their language, which are used by Smith and Hou [2010] to empirically evaluate which sets of rules matter. They find that technical rules (such as debt limits and no carry over rules) have a more significant effect on a state’s fiscal performance than do political rules (such as requirements that a governor submit or pass a balanced budget). Further, more recent literature that explores TELs would suggest that these limits are, in general, effective at reducing expenditures or increasing budgetary surpluses, though it may depend on the particular makeup of the given constraint. In regard to state TELs, Bails and Tieslau [2000] show a negative and significant effect of the presence of a TEL on state expenditures when analyzing a panel of states from 1969 to 1994. Shadbegian [1996] finds TELs slow the growth of government expenditures. Rueben [1997] notes that, when accounting for the endogenous nature of TELs, tax revenues are roughly two percent lower. Many of these results have also been supported by other studies [see: New 2010; Bae and Gais 2007; Shadbegian 1998]. Additional and relatively recent work has analyzed under what circumstances TELs limit overall government growth. Theoretically, Seljan [2014] suggests that TELs can only be effective when limited-government preferring agents oversee political actors. Further, Amiel et al. [2014] show that the extent to which a TEL is effective depends on its overall restrictiveness, while Kousser et al. [2008] suggest that no single TEL is completely effective at limiting government growth, though some such as Colorado’s TABOR and Washington state’s I-601 appear to act as relatively better constraints. In regard to LIVs, Bohn and Inman [1996] show that its presence leads to lower deficits. Further, Dearden and Husted [1993] suggest that LIVs result in a budget which is closer to that preferred by the governor. However, of the three fiscal institutions, it would appear that the effect that the LIV has on state public finance is the most ambiguous, and may be used to achieve political ends. For example, Abney and Lauth [1985] find evidence to suggest that LIVs are used more often when government is divided, and especially when a Republican governor faces a legislature controlled by Democrats. Also of interest is the interconnectedness of these institutions and how the presence of one constraint may dampen or strengthen the impact of another [Crain and Miller 1990]. A final body of research encompasses the historical development of many of these constitutional constraints. Here the evidence overwhelmingly suggests that early procedural debt limits (PDLs) emerged as a result of perceived excesses in state borrowing and “taxless finance” for infrastructure investment (dubbed internal improvements) through the 1830s, which culminated in default and repudiation by eight states and one territory as a result of the Panic of 1839 and ensuing depression [Wallis 2005].2 As PDLs developed states began to modify budgetary accounting standards in order to circumvent those constraints, which led to the imposition of additional restrictions [Rodriguez-Tejedo and Wallis 2012]. The remainder of the paper is structured as follows: ""Historical Overview"" section provides a historical overview and political context within which many of these constraints developed. ""Data and Model Specification"" section discusses the data to be employed within the empirical exercise as well as the empirical model. ""Results and Interpretation"" section lays out the results and a brief discussion of those results, while ""Conclusion"" section concludes.",4
43,2,Eastern Economic Journal,23 June 2016,https://link.springer.com/article/10.1057/s41302-016-0002-0,"Liquidity, Price Behavior, and Market-related Events",March 2017,Ran Lu-Andrews,John L. Glascock,,,Male,Unknown,Mix,,
43,2,Eastern Economic Journal,22 February 2017,https://link.springer.com/article/10.1057/eej.2015.51,"Abortion, Contraception, and Non-Marital Births: Re-Interpreting the Akerlof-Yellen-Katz Model of Pre-Marital Sex and Shotgun Marriage",March 2017,Saul D Hoffman,,,Male,Unknown,Unknown,Male,"In a famous and widely cited article,Footnote 1
Akerlof et al. [1996] proposed a novel explanation of the increase in the proportion of births that were non-marital among US women in the late twentieth century.Footnote 2 Most prior research had emphasized economic incentives in the form of overly generous welfare benefits [Murray 1984] and/or the relatively bleak marriage market prospects facing some women, especially those in minority communities [Wilson 1987]; for empirical efforts to assess these arguments, see Duncan and Hoffman [1990], Moffitt [1992], and Lundberg and Plotnick [1995]. Akerlof, Yellen, and Katz (hereafter, AYK) correctly noted the weak explanatory power of these explanations and, instead, emphasized a change in the social norm concerning the responsibility of a single male to the unplanned pregnancy of his unmarried partner. AYK presented a theoretical model of the “negotiation process” between single men and women about pre-marital sex and the man’s responsibility in the event of a pregnancy and then showed how those negotiations and the resulting social norm plausibly changed with the introduction of more reliable female-controlled contraceptives (i.e., the pill) in the 1960s and the legalization of abortion in 1973. In the AYK model, contraception and abortion counter-intuitively increased the proportion of births that are non-marital by reducing male commitment via “shotgun marriage” to a pregnant partner. The policy implications of this model have had a very curious life. AYK are very specific that an “attempt to turn the technology clock backward … would almost surely be both undesirable and counterproductive” (p. 314). But despite this, their paper is regularly cited in conservative policy writing as support for doing exactly that. For example, social conservatives have used the article and its conclusions as evidence in support of Catholic teachings on restricting access to contraceptives, abortion, and sex education; see, for example, Wilcox [2005] and Eberstadt [2014]. The article is also prominently cited by abstinence-focused advocacy groups; see, for example, testimony by the Executive Director of the Abstinence & Marriage Education Partnership opposing a sex-education bill in the Illinois State Legislation [Phelps March 2013], recent conservative critiques in The National Review of the Affordable Care Act for its coverage of FDA-approved contraceptives [Pfundstein 2011; New 2013], and a discussion in the Washington Post about the recent sharp decline in the teen birth rate in Colorado [Sullivan 2014]. All of these articles emphasize the conclusion of AYK about the negative role of contraception and abortion, and then argue that the solution is to restore a more traditional social order by restricting access to contraception, comprehensive sex education, or abortion. The reason that AYK’s paper has been able to be used in this way reflects what I argue is a particular and, in retrospect, perhaps unnecessarily narrow modeling assumption. Like any theoretical model, the AYK model simplifies reality in order to identify, isolate, and illuminate critical elements and relationships. In this case, the critical simplification is to classify women into two types, based exclusively on the cost to them of a pregnancy, where the cost is understood as the impact of a pregnancy on a woman’s net utility or well-being. I show below that the conservative policy implications follow directly from this assumption. This particular assumption was, however, only one of several that could have been used to generate the central result about the effect of abortion and contraception on non-marital births. One could just as well — or better, in fact — identify the two groups of women in terms of their monetary and/or psychic costs of utilizing contraception or abortion and/or their effectiveness in doing so. In this paper, I do precisely that, by revising the assumptions of the AYK model. I show that the central results of the revised models are exactly the same as in AYK, but the interpretation and policy implications are completely different. In my reformulation, clear concrete policies other than “rolling back the clock” are indicated as a way to assist single women and thereby potentially reduce the incidence of non-marital births. I first review the basic AYK model and its implications. Then I present several alternative versions based on differences in the costs of contraception and/or abortion or in the efficacy of use. This provides a clearer and more plausible result that is more descriptive of contemporary pregnancies to single mothers and more appropriate for policy debate and action. As re-interpreted, the AYK model is no longer subject to distortion of its core message, but rather suggests clear, specific policies.",
43,2,Eastern Economic Journal,09 January 2017,https://link.springer.com/article/10.1057/s41302-016-0089-3,How to Market the Market: The Trouble with Profit Maximization,March 2017,David Colander,,,Male,Unknown,Unknown,Male,"Milton Friedman, of course, understood all these arguments, and his argument in favor of profit maximization is best interpreted as a rhetorical argument to be understood in context. By that I mean that his argument for profit maximization was not a general argument supporting profit (or its shareholder value derivative) maximization as a normative imperative for firms. Rather it was a more narrowly directed argument against the then dominant lay view that corporation executives had a social responsibility to do good separately from conducting their business in a socially responsible manner. Friedman opposed this view, and Smith, Mill, and Marshall would have likely opposed it as well. The reason they would have opposed calls for “social responsibility” is Smith’s warning that I outlined above; such calls are very likely subterfuges designed to “deceive and even to oppress the public.” Social responsibility is a highly ambiguous concept ripe for misuse. Whose concept of social responsibility should firms follow? Government’s? Executive’s? Owner’s? Worker’s? Consumer’s? A government, or executive-imposed, social responsibility goal would likely undermine the offsetting selfishness aspect of markets. For example, social responsibility was often interpreted as paying existing workers higher wages and benefits, rather than as holding wages and benefits down. That helps the workers with jobs at the firm, but it hurts both other workers who would like to work for that firm at the existing wage, and consumers who would end up paying higher prices for the firm’s goods. Such a “higher wage” interpretation of “social responsibility” can be seen as a way of discouraging the competition that was necessary for the “offsetting selfishness” role of competitive markets to work. Market competition forces firms to take that social responsibility to other workers and consumers into account, and to balance it with their social responsibility to existing workers. This does not mean that corporations should not be socially responsible. It just means that social responsibility is a much more nuanced concept than supporters of corporate social responsibility often portray it. Friedman wanted to point that out. A careful read of Friedman’s article shows how he added qualifications that gave him wiggle room for his argument that firms should maximize profits. He writes: “there is one and only one social responsibility of business — to use its resources and engage in activities designed to increase its profits so long as it stays within the rules of the game, which is to say, engages in open and free competition without deception or fraud.” (My italicizing) Notice that his qualifications limit the argument to firms that are in an open competitive market, and to firms that do not engage in deception or fraud. These qualifications modify much of what people object to when they object to large sustained profits made by firms. An economy that has “open and free competition” would not have large and sustained profits — and “no deception and fraud” would make it so that firms are fulfilling actual desires of consumers, not taking the often easier path to “profits” by misleading people for the firm’s not the consumers’ benefit. So all Friedman is actually saying is that a version of society’s definition of social responsibility is already built into the “rules of the game” and that calls for additional social responsibility are often attempts to escape those rules. That is a reasonable argument that is debatable, but defensible.",1
43,2,Eastern Economic Journal,09 January 2017,https://link.springer.com/article/10.1057/s41302-016-0088-4,Response to How to Market the Markets: The Trouble with Profit Maximization,March 2017,Aswath Damodaran,,,Unknown,Unknown,Unknown,Unknown,,
43,2,Eastern Economic Journal,22 February 2017,https://link.springer.com/article/10.1057/eej.2015.43,Model Building in Economics: Its Purpose and Limitations,March 2017,Jason Barr,,,Male,Unknown,Unknown,Male,,
43,2,Eastern Economic Journal,22 February 2017,https://link.springer.com/article/10.1057/eej.2015.42,Experimenting with Social Norms: Fairness and Punishment in Cross-Cultural Perspective,March 2017,Pierre Lacour,,,Male,Unknown,Unknown,Male,,2
43,2,Eastern Economic Journal,22 February 2017,https://link.springer.com/article/10.1057/eej.2015.41,The Rise and Fall of Global Austerity,March 2017,Nina Quinn Eichacker,,,Female,Unknown,Unknown,Female,,
43,3,Eastern Economic Journal,11 April 2017,https://link.springer.com/article/10.1057/s41302-017-0096-z,Heterogeneity in the Labor Market: Ability and Information Acquisition*,June 2017,Solomon W. Polachek,,,Male,Unknown,Unknown,Male,"Ted Schultz, one of the early proponents of human capital, was an agricultural economist. Jacob Mincer [1974] using cross-sectional 1960 US Census, p. 93. See Polachek and Kim [1994] and Peseran and Tosetti [2011] for exceptions that get at a limited number of slopes. In Ben-Porath’s framework individuals create human capital in accord with a Cobb–Douglas production function \(Q = \beta H^{b}\) where H is one’s stock of human capital at time t and Q is the amount of human capital produced. The parameters b and β depict ability because individuals with greater b and β produce human capital more quickly at lower costs, resulting in steeper earnings trajectories. PDT’s Appendix A contains the derivation. The genetic algorithm (GA) is a recently available parallel processing optimization routine originally developed by Holland [1975]. The GA technique optimizes numeric strings using genetic reproduction, crossover, and mutation concepts [Goldberg 1989]. It is more prone to converge at a global optimum compared to Newton–Raphson hill-climbing algorithms which rely on a point-to-point gradient-based search [Dorsey and Mayer 1995]. PDT used a version of GA written by Czarnitzki and Doherr [2009]. As will be illustrated later, these two questions are related to employee–employer bargaining. See Papadopoulos [2015] for a recent survey. Here I use the adaptation of Polachek and Yoon [1987] described by Gaynor and Polachek [1994]. The same conclusion would be reached if offers expire quickly. Thus, one can construe depreciation as sooner offer expirations instead of skill depreciation.",1
43,3,Eastern Economic Journal,11 August 2016,https://link.springer.com/article/10.1057/s41302-016-0013-x,Natural Capital and Wealth in the 21st Century,June 2017,Edward B. Barbier,,,Male,Unknown,Unknown,Male,"An important contribution of Thomas Piketty’s book Capital in the Twenty-First Century [2014] is to document the rise in the wealth–income ratios over 1970–2010 for eight high-income economies—the United States, Japan, Germany, France, the United Kingdom, Italy, Canada, and Australia. For each of these countries, the wealth–income ratio has increased from 200–300  percent in 1970 to 400–600z in 2010. More extensive analysis of these ratios is provided by Piketty and Zucman [2014, henceforth PZ], who also develop a wealth accumulation model to explain the observed capital–income trends. To construct these ratios for 1970–2010, PZ use official national accounts for each country, following the U.N. System of National Accounts (SNA). Wealth is defined conventionally as market-value “national wealth,” which can be decomposed into domestic capital (including land and real estate) and net foreign assets.1 Income is “net-of-depreciation national income,” which is the sum of gross domestic product and net foreign income, less any domestic capital depreciation. Similarly, the national saving flow that adds to wealth is also measured net of capital depreciation. Finally, to explain the return to the high wealth–income ratios over 1970–2010 for the eight richest economies, PZ develop a variant of the constant-savings rate neoclassical growth model of Solow [1956] and Swan [1956], and establish that the long-run capital–income ratio β is equal to the (net) national savings rate s divided by the growth rate g in (net) national income. However, the SNA approach to national accounts does not include the depreciation in natural resources essential to domestic production and income, such as fossil fuels, minerals, and forests. These resources are important sources of “natural” capital, and the value of their net depletion should also be deducted from annual income and savings [Arrow et al. 2012; Hamilton and Clemens 1999; Hartwick 1977 and 1990; Solow 1986; World Bank 2011]. The rationale is intuitive: if we use up more of energy, mineral, and forest resources to produce additional economic output today, then we have less natural capital for production tomorrow; thus, net national income and savings today should also account for any natural capital depreciation. In this paper, I make two additional contributions to PZ’s analysis of wealth–income ratios over 1970–2010. First, I show that their one-good wealth accumulation model can be extended to allow for natural capital depreciation, although I develop this model using standard intertemporal optimizing behavior rather than assuming a constant (net) savings rate. This extension leads to two key indicators: the net national saving rate adjusted for natural capital depreciation \(s_{t}^{*}\) indicates the annual change in wealth (inclusive of natural capital) relative to net national income (adjusted for natural capital depreciation); and the ratio of this saving rate with respect to the long-run average annual growth in adjusted net national income per capita \({{s_{t}^{*} } \mathord{\left/ {\vphantom {{s_{t}^{*} } {\bar{g}^{*} }}} \right. \kern-0pt} {\bar{g}^{*} }}\) indicates how annual changes in adjusted net wealth relative to income compare with long-run growth over some defined time period of T years. Second, using data from the World Bank [2016], I apply these two indicators to examine the impacts of depreciation of key natural resources, such as fossil fuels, minerals, and forests, on the accumulation of adjusted net wealth over 1970–2013 for the eight largest economies analyzed by PZ and also Piketty [2014]. For comparison, I examine trends over 1979–2013 in \(s_{t}^{*}\) and \({{s_{t}^{*} } \mathord{\left/ {\vphantom {{s_{t}^{*} } {\bar{g}^{*} }}} \right. \kern-0pt} {\bar{g}^{*} }}\) for low- and middle-income economies. The main findings are that, although over the past four decades the rate of natural capital depreciation has been on average five times larger in developing countries than in the eight rich countries, in low- and middle-income economies other forms of capital investments have largely compensated for the rising natural capital depletion that has occurred since the late 1990s. In contrast, in the rich countries, the rate of adjusted net savings has converged to the rate of natural capital depreciation. As documented by PZ, over the past 40 years there may have been substantial accumulation of wealth relative to income in these economies, but as this accumulation has proceeded, natural capital depreciation is being compensated less and less each year by net increases in other forms of capital. The overall implications are that, given that stocks of natural resources are depleted for current production and wealth accumulation, a measure of national wealth that excludes natural capital depreciation likely exaggerates the actual increase in an economy’s wealth over time, especially in those countries where accumulation of other forms of wealth is failing to compensate for diminishing natural capital.",5
43,3,Eastern Economic Journal,27 June 2017,https://link.springer.com/article/10.1057/eej.2016.10,How the Wealth and Credit Channels in Monetary Transmission Affect Consumer Durables and Housing: A Dynamic Stochastic General Equilibrium Approach,June 2017,Menelik Geremew,,,Unknown,Unknown,Unknown,Unknown,,
43,3,Eastern Economic Journal,29 June 2016,https://link.springer.com/article/10.1057/s41302-016-0011-z,The Native-Born Occupational Skill Response to Immigration within Education and Experience Cells,June 2017,Emily Gu,Chad Sparber,,Female,Male,Unknown,Mix,,
43,3,Eastern Economic Journal,27 June 2017,https://link.springer.com/article/10.1057/eej.2015.54,Religion and Labor: An Examination of Religious Service Attendance and Unemployment Using Count Data Methods,June 2017,Neil R Meredith,,,Male,Unknown,Unknown,Male,"The Great Recession has sparked a renewed interest in the relationship between unemployment and religion.Footnote 1 Existing empirical evidence on the relationship between unemployment and religiosity is scarce. Preliminary cursory analysis of monthly national data on the US unemployment rate, US average weeks unemployed, and US weekly religious service attendance, such as in Figures 1 and 2, suggests that there may or may not be a link between unemployment and that dimension of religiosity. Additional analysis in Table 1 using data from this study yields conflicting results regarding whether persons who regularly attend religious services and who are unemployed in a current wave of data are more or less likely to find work by the next wave of data than persons who do not attend religious services. Percent attending worship services nearly every week or more and the unemployment rate. 
Notes: A simple correlation between the percent of men attending nearly every week or more and the unemployment rate for men shows no statistically significant relationship. At the same time, a simple correlation between the percent of women attending worship services nearly every week or more and the unemployment rate for women reveals a positive statistically significant relationship. More specifically, the coefficient is 0.87 with standard error of 0.44. In other words, a 1 percentage point increase in the unemployment rate is correlated with a 0.87 percentage point increase in the percent attending worship services nearly every week or more. 
Sources: Worship attendance data come from the General Social Survey and are for years 1972–1978, 1980, 1982–1991, 1993, and biannually from 1994–2012. Unemployment rate data are annual from the Current Population Survey for years 1972–2012. Results are similar when aggregating all individuals together. Percent of Americans who attend worship services at least weekly and the US average weeks unemployed. 
Notes: A simple correlation between the percent of men attending nearly every week or more and the average weeks unemployed for men indicates a negative statistically significant relationship. The coefficient estimate of −0.19 with standard error 0.11, more specifically, implies that a 1 week increase in weeks unemployed is associated with a 0.19 percentage point decrease in the percent attending worship services nearly every week or more. Similarly for women, a simple correlation between the percent of women attending worship services nearly every week or more and the unemployment rate for women implies a negative statistically significant relationship. More specifically, the coefficient estimate is −0.30 with standard error of 0.09. In other words, a 1 week increase in the unemployment rate is correlated with a 0.30 percentage point decrease in the percent attending worship services nearly every week or more. 
Sources: Worship attendance data come from the General Social Survey and are for years 1972–1978, 1980, 1982–1991, 1993, and biannually from 1994–2012. Average unemployment duration data in number of weeks are annual from the Current Population Survey for years 1972–2012. Results are similar when aggregating all individuals together. To provide new evidence on this topic, I conduct the first count data estimates of the relationship between frequency of religious service attendance and unemployment. This study is also the first to consider the relationship between the duration of unemployment and the frequency of religious service attendance. More broadly, I seek to add to a growing body of literature that explores the relationship between religious activities and labor market outcomes. In addition, this study incorporates two panel data sets to evaluate younger and older working-age individuals.Footnote 2 Individuals in different life stages may have disparate relationships between the frequency of religious service attendance and unemployment. This paper is organized into five sections. The next section discusses the relevant literature and hypotheses for this study. The third section reviews the data set, the measures utilized, and covers the econometric estimation. The fourth presents the results. The fifth and final section contains conclusions and recommendations for further research.",1
43,3,Eastern Economic Journal,11 October 2016,https://link.springer.com/article/10.1057/s41302-016-0010-0,Do Smoking Bans Improve Infant Health? Evidence from U.S. Births: 1995–2009,June 2017,Jia Gao,Reagan A. Baughman,,,,Unknown,Mix,,
43,3,Eastern Economic Journal,14 November 2016,https://link.springer.com/article/10.1057/s41302-016-0079-5,Changes Over Time in the Relationship of Obesity to Education Accumulation,June 2017,Tim Classen,,,Male,Unknown,Unknown,Male,"This section reviews the potential pathways through which education accumulation and obesity may be related, considering both causal directions and the potential for unobserved factors that affect both observed outcomes. Levels of educational attainment may vary by adolescent weight status for a variety of reasons.5 There exist several potential avenues by which obesity could cause variation in education levels. Both the in-school social and longer-term economic consequences of adolescent obesity may cause obese students to obtain lower levels of education. Discrimination or social isolation from classmates or teachers during the primary and secondary years of schooling may limit obese students’ desire to continue education beyond the compulsory level and serve as a causal factor in students dropping out of high school.6 Future returns to investments in education, including the sizeable tuition costs of attending college, may be reduced either due to obese workers earning lower wages or a shorter lifespan over which to earn income given the strong correlation between adolescent and adult obesity (and subsequent health problems such as diabetes and heart disease). As well, parents may invest less in an obese child’s post-secondary education due to parental discrimination, as postulated by Crandall [1995]. While elevated weight levels may potentially affect cognitive functioning (through comorbidities such as sleep apnea), the evidence from previous research finds mixed evidence for significant differences in academic achievement between obese and non-obese students.7 These factors would all indicate ways in which obesity may cause reductions in educational attainment, but it is also possible that obese adolescents would have an incentive obtain relatively higher levels of education due to the need to be economically self-sufficient in the case that they remain single as adults if obesity creates penalties in marriage markets as has often been found (but usually only for females).8
 It is also possible that the direction of causality in the observed relationship between obesity and education results from increased investments in education causing reduced rates of obesity. This could result from education investments providing individuals with information that allows for more efficient health production (or, alternatively, to better understand the influential role of certain inputs in health production). As well, increased investments in education generally produce higher adult incomes that provide budget constraints that would permit healthier foods and produce better health outcomes (e.g., lower likelihoods of obesity). These are all potential explanations for increased education causing a lower likelihood of weight problems, but it is also possible that investments in education could increase the risk of obesity. Studying is necessarily a sedentary activity, so increased effort toward school work may increase obesity risk (if studying substitutes for physical activity, rather than other sedentary activities). The food environment during early years of college (with an unlimited supply of food at a marginal price of zero on a meal plan) may also increase obesity risk. The above explanations represent potential causal pathways by which obesity may be related to human capital investments via observable measures, but it is also possible that relationships between weight status and education result from correlations in other observed and unobserved characteristics. Due to the substantial positive correlation in weight between parents and their offspring (whether attributed to genetics or shared environmental factors),9 obese children are more likely to have obese parents who may have fewer monetary resources to invest into their children’s education when labor market outcomes are negatively influenced by obesity. As the NLSY data indicate, women with less education are more likely to have overweight and obese offspring. Thus, the families that are most likely to have difficulty paying for college are also the most likely to have obese children (who are more likely born to obese parents). While credit constraints may limit investments in post-secondary education among relatively poor families, such constraints will force lower-income families to invest relatively more in children with better future wage prospects. As well, characteristics unmeasured in most datasets may explain both adolescent obesity and education accumulation. Individuals who heavily discount the future are likely to invest less in education, while also being more prone to health characteristics such as obesity that may reduce longevity. Unobserved parental effort toward obese offspring (relative to their non-obese siblings) may also explain any observed relationships between weight status and education.",5
43,3,Eastern Economic Journal,21 September 2016,https://link.springer.com/article/10.1057/s41302-016-0074-x,"Eliminating the Fractional Part of Currency from Circulation (Pennies, Cents, Paras, Kopeikas). An Analytic Study",June 2017,Marko Rašeta,,,Male,Unknown,Unknown,Male,"Serious scientific debate on the topic of elimination of one penny coin from the U.S. coinage system started with a paper by Lombra [2001] . He ran a large-scale simulation of the effects of rounding cash transactions in convenience stores to the nearest nickel. Based on the results of the simulation, Lombra deduced that if a one penny coin would cease to exist we would see an accumulation of no less than $318 million and no more than $818 million. This money would come directly from the pockets of U.S. citizens, and refers to it as the ‘rounding tax’. Chande and Fisher [2003] investigated the corresponding question in Canada. They assumed that goods are taxed, and also ran a simulation. They deduced that taxation ‘almost’ removes the bias and that the effects of rounding would be much less severe. Furthermore, Whaples [2007] offers further evidence for eliminating the one penny coin from the U.S. coinage system. Contrary to the three researchers above he uses real data from seven U.S. states. Among other things he deduces that a ‘rounding tax’ would ‘essentially’ disappear in the case where ‘all goods were taxed’. Finally, in an almost immediate response to Whaples, Lombra [2007] adjusts his arguments from 2001. He launched a new simulation with taxes included this time and deduced that ‘rounding tax’ would still be very significant indeed, and would cost Americans between $250 million and $350 million per year. The main idea here is that even a small bias against a customer will accumulate to a huge sum once you multiply it by 70 billion, which he uses as an estimate of the yearly number of cash transactions in the U.S. Moreover, in all of the above studies, researchers explored whether retailers would exploit the absence of penny in order to make more money, but no real analytical study was presented. A fractional part of the currency has been removed from circulation in many countries around the world. This includes Canada, Serbia, Russia and Hungary, to name just a few. The problem is that rounding to the nearest fractional unit yields accumulation, as Lombra’s papers show; hence, if we are to get rid of the fractional parts of money we must proceed with caution. This paper is organised as follows. In the first part we will show that strategic pricing is a reality through a statistical study we have recently conducted in Serbia where we have analysed the accumulation in the local supermarket chain ‘Maxi’. In the second part we will offer a common solution to this global problem, which we will illustrate in the case of the U.S. We will introduce stochastic rounding procedures which will turn out to be an extremely efficient tool against the ‘rounding tax’. It will be impossible for retailers to take advantage of their customers and vice versa, even in the absence of pennies, no matter which strategies they use.",
43,3,Eastern Economic Journal,27 June 2017,https://link.springer.com/article/10.1057/eej.2015.56,"The Classics, Keynes, and the Keynesians: A Unified Formalization",June 2017,Lucas Llach,Pablo Schiaffino,,Male,Male,Unknown,Male,"With the so-called Great Recession starting in 2007, macroeconomics has turned its attention back to its big question of the economy’s adjustment, or lack thereof, to full employment. In both academia and the public sphere the crisis has highlighted deep differences between the “saltwater” and “freshwater” traditions — a debate partly overlapping with the Keynesian/Classical divide. The aim of this article is to present a simple, microfounded, intertemp`oral model that can accommodate the main traditional arguments for and against the idea that market forces automatically lead to full employment. Although historically these arguments were advanced with different modeling strategies (e.g., with and without microfoundations, and with and without an explicit intertemporal framework), we show that they can be presented as differing just in the value or behavior of certain parameters in one single, unified model with explicit solutions. To be more precise: we show that changing the values or behaviors of certain parameter(s) of one single, unified model is enough to obtain the diverse and sometimes contrasting results that comparative statics produce in the endogenous variables of each one of the most typical macroeconomic models. There are essentially four cases in the model we present, each corresponding to a well-known textbook macroeconomic model. We label our cases, without a claim of correspondence with the — often ambiguous — usage of these terms: (1) Classical, with flexible wages and prices guaranteeing full employment; (2) Keynes’ Liquidity Trap, akin to Krugman’s [1998] presentation of the liquidity trap; (3) Sticky-Price Keynesian, leading to a traditional IS–LM; and (4) Sticky-Wage Keynesian, which corresponds to the Aggregate Supply-Aggregate Demand textbook model. We do not claim that each of our cases captures every aspect and argument of each of these models (arguments which, anyway, are not exactly consensual). We argue, rather, that the results that comparative statics produce on the endogenous variables in our cases mimic those arising in the corresponding macro model. What are the advantages of constructing a simple, unified formalization of the sort we propose? First, it emphasizes that empirical observations appearing to favor one or another theoretical tradition do not necessarily imply an endorsement of a whole intellectual edifice (or, more accurately, the rejection of others) but are compatible with one single macro model facing a certain constellation of parameters. It could well be the case that in diverse historical circumstances these parameters (reflecting, e.g., the flexibility of wage adjustments or the state of aggregate demand) behave differently or adopt different values. The message from a unified formalization is that these differences do not necessarily justify a clash of models or modeling strategies: a simple intertemporal, microfounded macro model can accommodate what appear to be very diverse empirical observations or predictions depending on the specific values or behavior of these parameters. A second motive for this unified formalization in a simple setting has to do with the teaching of macroeconomics. The cases in this simple model are presented graphically to coincide with the textbook formulation of typical Classical and Keynesian models. These models are often taught as coming from different methodological quarters: microfounded vs ad hoc; instantaneous vs intertemporal; and so on. The message of this article for the teaching of economics is that a single, microfounded modeling strategy is enough to present the most widely used textbook macroeconomics models. We keep the model in the simplest possible form so that we can arrive at explicit, easy-to-read expressions for the solutions of the main endogenous variables (production, employment, prices, wages, and interest rates) under each set of assumptions for the parameters. We present alternative assumptions concerning the following :(1) the flexibility of product prices; (2) the flexibility of nominal wages; and (3) the government’s monetary policy targets. “Shocks” to the economy are presented as changes in parameters that affect current demand. Of course, we do not claim that these four cases capture every major argument for or against the economy’s automatic adjustment to full employment, nor important variants within each of these traditions. Non-competitive market structures, demand spillovers, expectational or informational problems, the investment accelerator, secular stagnation, the financial channel, or the motives behind price or wage rigidities are some of the major elements that have been part of this debate but we omit. Many other works compare in a unified framework diverse traditions in macroeconomic thought, stressing various traits as the dividing lines between competing models. Leijonhufvud’s [2000] “Swedish Flag” scheme classifies macroeconomic traditions according to the emphasis placed on nominal or real forces in both the types of shocks and the transmission mechanisms. Phelps [1990] points to different types of price and wage rigidities and diverse beliefs on the way expectations are modeled. Tobin [1993] discusses the different approaches of the Classics, Keynes, and other Keynesians to macroeconomic adjustment in the credit and labor markets. In these works there is, however, no unified algebraic model supporting the competing arguments. A recent contribution presenting Keynes’ view in the context of a microfounded model is Dos Santos Ferreira [2014]. To the extent that we make different views depending on the characteristics of the parameters of one same model, our presentation is analogous to the “elasticities debate” between monetarists and Keynesians, in which each camp held a particular view on the slopes of the IS and LM curves. The article proceeds as follows: in the next section we present the basic setup of the model, including an incidental detour regarding the Keynesian cross. In the section after that we consider the Classical case: flexible prices with a full employment equilibrium. In the following section we deal with the Liquidity Trap case: flexible prices with no full employment solution. The penultimate and final sections cover, respectively, the IS–LM case (sticky pricesFootnote 1) and the AD–AS model (flexible prices but sticky wages). Our baseline model is a two-period general equilibrium model, with standard assumptions: many identical households with decisions over, and utility depending on, consumption and liquidity in both periods; profit-maximizing firms in a competitive market deciding employment and production given prices and wages; and a government that spends its resources in both periods. There are some assumptions that let us keep the model in a very simple form:
 Households supply labor inelastically. There exists a non-linear monitoring cost, implying increasing short-run marginal costs of production and downward-sloping short-run labor demand, despite constant marginal productivity of labor. There is no capital or investment. Can this be consistent, for example, with a “Keynesian” world? As stated in the introduction, the aim of the article is not to replicate every argument behind each traditional macro model but to show that one single model can capture the results these models have on endogenous price and quantity variables.Footnote 2
 Money is assumed to provide direct utility to households. This is a shortcut consistent with different psychological motives for holding money, such as “liquidity services” or precaution, and standard in many macro models including “liquidity trap” models such as that of Eggertsson and Woodford [2004]. Again, we do not claim this represents the actual motives given by different schools for holding money; it is sufficient for our purposes for endogenous variables (including the quantity of money demanded) to behave as they do in those traditional models. The government finances itself solely through seignorage. Lump sum taxes could be added without loss of generality, but seignorage needs to be a part of the story as long as the government prints money. The model is solved under certainty. “Shocks” are illustrated through comparative statics: changing the value of a parameter starting from an equilibrium obtained under another constellation of parameters, including crucially expectations at Period 1 of parameters at Period 2. The economy is populated by a large number of identical households with the following utility function, where subscripts indicate the time period. (The two periods could well be interpreted as “short run,” and “long run,” as in the work by Eggertsson and Krugman [2012]): where β is the intertemporal discount factor, α the marginal utility of nominal money holdings, weighted by the level of those holdings, and p

t
, M

t
, and c

t
, respectively, the price level, monetary holdings, and consumption in period t. Households may carry wealth from Period 1 (today) to Period 2 (tomorrow) through bonds. Money is not just given away for free by the government, as in Friedman’s “helicopter money”; it has to be purchased from the government in exchange for private income in Period 1. The reason for avoiding “helicopter money” is that in such case a pure monetary policy is impossible. When the government’s helicopter drops money, a monetary and a fiscal policy are acting in combination: an increase in the money supply and a transfer of nominal resources to the private sector (which in principle, and depending on the kind of model, may or may not lead to an increase in real purchasing power). If i is the nominal interest rate and y

t
 the real income (from all sources) in period t, the budget constraints of Periods 1 and 2 combine to obtain: The household maximizes its utility function subject to the intertemporal restriction stated above. The first-order conditions for consumption and money holdings in both periods are: where Y is the real intertemporal income given by Y=y
1+(y
2/(1+r)), r is the real interest rate (1+r)=(1+i)/(1+π) , where (1+π)=p
2/p
1 , and z is a sum of parameters: z=1+β+α+αβ. We assume that government gets its revenue (T) from issuing money in both periods, i.e. Why do we not include taxes? They would complicate the algebra without adding much substance. Essentially, if the government creates (or increases) a lump-sum tax, private intertemporal income falls, and present and future consumption diminish proportionally. The net effect on current demand would be positive (negative) if and only if the share of government intertemporal resources spent “today” (term s below) is higher (lower) than the share of intertemporal private consumption spent “today.” On the other hand, seignorage is impossible to be ignored in a general equilibrium model. Note that government revenues equal the difference between total output and total consumption in (2), the cost to private agents of holding money. Substituting (5) and (6) into (7) yields and dividing both terms by p
1, we get the following: Plugging equations (3) and (4) into (9), The interpretation of (10) is straightforward: the higher the present value of consumption, the higher is government’s revenue from people holding money. And, given the assumed preferences, the present value of consumption is linear in Period-1 consumption. Total demand in the economy is given by the sum of household consumption and government spending, as there is no real investment. We are interested in possible economic disruptions in Period 1; hence, we concentrate our analysis in today’s aggregate demand: We assume that the government spends in Period 1 a share “s” of intertemporal revenues.Footnote 3 This is the main fiscal policy parameter: given the level of intertemporal public resources (seignorage), the impact of fiscal policy on current aggregate demand is given by parameter s. Using equation (10), the expression for public spending is: g
1=sαc
1(1+β). Introducing it to the aggregate demand in (11), we get the following: where 0⩽s⩽1. We are close to getting a final expression for aggregate demand. Replacing equation (3) into (12) and using y=y
1+(y
2)/((1+r)), we arrive at the following: where x=1+sα(1+β). This is, in essence, the insight of the textbook Keynesian cross: income depends on income. Through its effect on consumption, income (y
1) influences current aggregate demand (y
1
d) without taking into account the markets for labor, credit, and/or money; at the same time, income and demand have to be equal if there is no inventory accumulation. As Figure 1 shows, the presence of the multiplier is straightforward: any change in government spending in Period 1 (s, a part of z above) will increase income proportionally more. There are, however, a couple of differences with the traditional Keynesian cross: (1) there is no investment, which in Keynes [1964] is the crucial determinant of production, as consumption depends on income; and (2) in contrast, there are other “autonomous” influences in today’s expenditure apart from the fiscal policy variable, namely: tomorrow’s expected income and parameters related to time and liquidity preference. The Keynesian cross. Re-arranging in order to get an expression for y
1 yields where v=(1+α+β+αβ)/(1+sα(1+β)) − 1. Equation (14) is the aggregate demand of the model and the equilibrium value of Period-1 income given (y
2) and the others parameters of the model. Intuitively, aggregate demand depends positively on tomorrow’s expected income (y
2) and the share of intertemporal government proceeds spent today (s), and negatively on the real interest rate (r) and the valuation of future consumption (β).Footnote 4 Using (12) we can also express equilibrium consumption in Period 1 as follows: The analogous expression for c
2 is In both periods there is a number n of firms producing a homogenous good. Firms demand labor, and households supply inelastically an amount of labor collectively equal to  in both periods. The representative firm chooses the optimal quantity of labor to maximize profits, with a linear production function y=al: where l is the total labor input used in the production process, a
1 is productivity in Period 1, w is the nominal wage and b (indexed through p
1) can be thought of as supervision cost that rises linearly more with output.Footnote 5 Labor demand for each firm is as follows: Replacing (18) into the production function yields the aggregate supply: The model is, then, described by the following 5 equation systems 
Equations (20), (21), (22), (23) and (24) solve for {w
1, w
2, p
1, p
2, i}. Notice that the AS=AD that holds for Period 2 can be dropped here because of Walras’ Law. As we shall see, however, for some combination of the parameters the system (20)–(24) has no solution; when this is the case, there is no full employment equilibrium. In those cases we will define the (non-full employment) equilibrium of the system as the vector {w
1, w
2, p
1, p
2, i} with all five prices ⩾ 0 that (1) satisfies Equations (20), (21), (22), (23) and (24) and (2) minimizes , that is, that which minimizes current unemployment. We choose the labor market as the one where we admit a disequilibrium, when there is no full employment solution, precisely because we are looking for conditions in which full employment might not prevail (it is also empirically natural to assume that adjustment in the labor market is less smooth than that is in the money or goods’ market). If more than one vector leads to equilibrium in all markets but the labor market, we define the one with less unemployment as the “equilibrium” to avoid any arbitrary element in departures from unemployment.",
43,3,Eastern Economic Journal,07 June 2016,https://link.springer.com/article/10.1057/s41302-016-0008-7,"Playing at Acquisitions. By Han Smit and Thras Moraitis. Princeton University Press, Princeton and Oxford, 2015. 172pp., $55.00. ISBN: 978-0-691-14000-1.",June 2017,Jennifer W. Kelber,,,Female,Unknown,Unknown,Female,,
43,3,Eastern Economic Journal,23 June 2016,https://link.springer.com/article/10.1057/s41302-016-0009-6,"Planetary Economics: Energy, Climate Change, and Three Domains of Sustainable Development. Edited by Michael Grubb, Jean-Charles Hourcade, and Karsten Neuhoff. Routledge, New York, 2013. 548pp. $70.00. ISBN: 970-0-415-51882-6.",June 2017,Mimi Houston,,,Female,Unknown,Unknown,Female,,
43,3,Eastern Economic Journal,11 August 2016,https://link.springer.com/article/10.1057/s41302-016-0015-8,"The Evolution of the Property Relation: Understanding Paradigms, Debates, and Prospects. By Ann E. Davis. Palgrave Macmillan, New York, 2015. 271pp., $120.00. ISBN: 978-1-137-35210-1.",June 2017,Melvin L. Cross,,,Male,Unknown,Unknown,Male,,
43,3,Eastern Economic Journal,27 June 2017,https://link.springer.com/article/10.1057/eej.2015.25,What Does the Minimum Wage Do?,June 2017,Veronika Dolar,,,Female,Unknown,Unknown,Female,,
43,4,Eastern Economic Journal,02 October 2017,https://link.springer.com/article/10.1057/eej.2015.46,Do Relational Goods Raise Well-Being? An Econometric Analysis,September 2017,Simona Rasciute,Paul Downward,William H Greene,Female,Male,Male,Mix,,
43,4,Eastern Economic Journal,02 October 2017,https://link.springer.com/article/10.1057/eej.2015.49,Significant Placebo Results in Difference-in-Differences Analysis: The Case of the ACA’s Parental Mandate,September 2017,David J G Slusky,,,Male,Unknown,Unknown,Male,"The Patient Protection and Affordable Care Act (ACA), signed into law by President Obama on March 23, 2010, includes a provision mandating that as of September 23, 2010 most young adults must be allowed to stay on their parents’ health insurance until age 26. Several recent papers study the potential effects of this early provision, making important contributions. Their results are intuitive: an increase in the share of individuals with dependent insurance coverage [Cantor et al. 2012b; Sommers and Kronick 2012; Akosa Antwi et al. 2013; O’Hara and Brault 2013; Sommers et al. 2013; Chua Sommers 2014], a decrease in the uninsurance rate [Cantor et al. 2012b; Sommers and Kronick 2012; Akosa Antwi et al. 2013; Mulcahy et al. 2013; O’Hara and Brault 2013; Jhamb et al. 2015], a decreased likelihood of delaying or not obtaining care because of cost [Sommers et al. 2013, Barbaresco et al. 2015], a decrease in out of pocket costs [Chua and Sommers 2014; Busch et al. 2014] an increased likelihood of having a usual source of care [Sommers et al. 2013; Barbaresco et al. 2015], increased labor market flexibility [Akosa Antwi et al. 2013], a drop in share of uninsured emergency room (ER) visits [Mulcahy et al. 2013], improved self-reported health [Carlson et al. 2014; Barbaresco et al. 2015], increased use of inpatient and mental health resources [Saloner and Lê Cook 2014; Akosa Antwi et al. 2015], an increase in dental coverage [Han et al. 2014; Shane and Ayyagari 2015], a decrease in ER visits [Hernandez-Boussard et al. 2014; Akosa Antwi et al. 2015], and increase in premiums for health insurance plans that cover children [Depew and Bailey, 2015]. Econometrically, all of these studies use an age–time difference-in-differences strategy, which has also been used to investigate the impact of other health insurance public policies with sharp age-cutoffs [Levine et al. 2011]. Cantor et al. [2012b], using the years 2005–2010, compares those age 19–25 to those age 27–30, before and after the 2010 implementation of the parental insurance mandate. Unfortunately, this approach does not satisfy the crucial assumption for a difference-in-differences analysis, which is that in the absence of treatment the average outcomes for the affected and comparison groups would have followed parallel trends [Bertrand et al.2004 (hereafter BDM); Abadie 2005]. If this condition is not satisfied, the difference in average trends between the affected and comparison groups in the affected time period can confound the effect of the policy, or even suggest a substantial one when none exists. While this critique could be applied to any health insurance public policies with sharp age-cutoffs (e.g., Medicare, S-CHIP), it is especially applicable to the ACA’s parental mandate because of the age-specific labor market turmoil that was occurring during its enactment and implementation. Specifically, over the past few decades, the United States has undergone substantial shifts in the structure of its labor force [Card and Lemiuex 2000; DiCecio et al. 2008]. Crucially, these shifts have had differential age effects [Bell and Branchflower 2011; O’Higgins 2012; Congressional Budget Office 2004], especially during the Great Recession of 2007–2009 [Dunn 2013; Lazear and Spletzer 2013], contaminating any age–time difference-in-differences analysis. For example, Figure 1 shows the proportion of individuals who are employed, split into those affected by the expansion of parental coverage under the ACA (those aged 19–25) and comparison age groups not affected (16–18, 27–29). This data is smoothed over the 5 months before and after the interview month to reduce seasonality. The gray vertical line represents the earliest period to include data from the enactment of the federal mandate which is October 2009. The gaps are because of missing data between the end of one panel and the start of the next and these gaps are larger because of the smoothing. Share employed, SIPP. 
Notes: Each point is the average of itself and 5 months before and after. Weighted. Each box is a sample “affected” placebo period where the trends are not parallel. The vertical line at October 2009 is when data from the enactment period begins to be including in the moving average window. If this were an appropriate application of a difference-in-differences strategy, then while the two groups would have unequal step changes directly after the implementation, the subsequent trends would be the same. However, the two lines to the right of the vertical line are not parallel, suggesting substantial labor market differences between those two groups. Furthermore, one can see in the black boxes that there are many time periods where the two lines have non parallel trends. To quantify whether these trends are parallel or not one can regress the probability of being employed on month-year-group dummies and then run a joint F-test of whether the coefficients for each month-year until enactment (i.e., through February 2010) are equal across groups (e.g., January 1997 16–18 and 27–29=January 1997 19–25, February 1997 16–18 and 27–29=February 1997 19–25). For 186 months in the full data set, the F-stat is 1.65*** (P<0.0001), suggesting that the two trends are not equal. Repeating the analysis with quarter-year-group dummies (dropping quarters without 3 months of data) gives an F-stat of 2.45*** (P<0.0001) for 59 quarters, which is consistent.Footnote 1
 
Figure 2 shows the same picture for the share of individuals working full-time workers (greater than 30 hours/week). This is arguably more relevant to health insurance coverage as usually only full-time employees receive benefits. Analogous to Figure 1, there are periods in the black boxes which have substantially non-parallel trends, lending intuition to the claim that young adults of different ages are not ex-ante equivalent. The analogous F-stats are 1.65*** (P<0.0001) and 2.78*** (P<0.0001). Share working full time, SIPP. 
Notes: Each point is the average of itself and 5 months before and after. Full time= >30 hours/week. Weighted. Each box is a sample “affected” placebo period where the trends are not parallel. The vertical line at October 2009 is when data from the enactment period begins to be including in the moving average window. As many young adults have health insurance coverage (or the option of it) through their employer, this changing labor market also makes it possible that there were group-specific trends in insurance outcomes during the affected time period that would confound any estimates of policy impacts. For example, if full-time employment growth were lower for the affected age than the comparison age because of overall labor market conditions, then these individuals would comparatively be losing their own employer-sponsored coverage and switching to parental insurance wherever possible, independent of any change in mandated insurance availability. Failing to properly control for this would bias estimated effects away from zero as they would include both the impact of the mandate and the impact of the differentially changing labor conditions. It would be extremely challenging to ascertain whether the conclusions in the literature from a pre-2010/post-2010 difference-in-differences are the result of the ACA or from differential trends. Therefore, in lieu of this I will perform earlier-in-time “placebo” regressions tests on three of the most prominent papers on this topic: Sommers and Kronick [2012] (hereafter SK); Cantor et al. [2012b] (Cantor); and Akosa Antwi et al. [2013] (AMS).Footnote 2 These papers fostered an entire field of research, and provided rapid, positive feedback on the early consequences of the ACA. Their ingenuity did the field a great service. My falsification tests will use each paper’s specification, shifting their temporal windows (i.e.,, 2005–2010, 2008–2011) backward in time 1 year at a time.Footnote 3 Using this approach, I find that the differential age–time health insurance and employment effects that appear after the ACA is implemented also appear in other time periods, even with a narrower age bandwidth, thus undermining the conclusion that these effects are causal outcomes of the ACA. Rather, they may be a consequence of the age-differential changes in labor market in the United States.Footnote 4
 To attempt to mitigate these differences, I use multiple conservative difference-in-differences statistical methods: adjusting for serial correlation [BDM], adjusting for intra-group correlation [Donald and Lang (hereafter DL) 2007] and also weighing the comparison ages to create synthetic control groups [Abadie and Gardeazabal 2003]. While useful in many contexts, none of these methods substantially reduce statistically significant placebo results in this paper. I also drastically reduce the age bandwidth which does allow me to more reliably test the hypotheses in the literature. However, only some of the health insurance results remain statistically significant, whereas none of the labor supply results are. A Monte Carlo analysis using the health insurance results confirms this conclusion. Already, because of the influence of earlier version of this paper, two of the most recent working papers on this topic [Akosa Antwi et al. 2015 and Barbaresco et al. 2015] include a reduced age bandwidth either as their main specification or as a robustness test. Still, many other new papers in the literature continue to use broad-age bandwidths with limited if any robustness checks.",32
43,4,Eastern Economic Journal,02 October 2017,https://link.springer.com/article/10.1057/eej.2015.44,The Returns to College Education — An Analysis with College-Level Data,September 2017,Philip R P Coelho,Tung Liu,,Male,,Unknown,Mix,,
43,4,Eastern Economic Journal,02 October 2017,https://link.springer.com/article/10.1057/eej.2015.48,"Maternal Life Satisfaction, Marital Status, and Child Skill Formation",September 2017,Dimitrios Nikolaou,,,Male,Unknown,Unknown,Male,"Whereas much is known about income and family structure as factors that influence child skill formation — with children of more affluent and married families outperforming children of less affluent and non-married families [e.g., Hill et al. 2001; Dahl and Lochner 2012] — less is known about the role of happiness. Maternal happiness is important for child development not only because it affects parenting preferences, but because it can also affect the choice of spouses, both of which subsequently determine childhood investments. For example, happier mothers may increase the quantity and quality of child investments or may avoid conflict-ridden relationships to ensure child exposure to a constructive familial environment. This suggests that happier mothers may be more likely to choose and marry partners who will positively contribute to the production of child skills. This positive selection into a marital status will further enhance maternal happiness, which will, subsequently, have a positive effect on child investments. Due to this link between marital status and life satisfaction, the effects of family structure on child skill formation should be separated from the effects of happiness, which leads to the question: is the marriage effect in existing work, in fact, a happiness effect? To identify which part of the marital status effect on child outcomes can be attributed to maternal happiness, I use data for UK children ages 3–7 from the Millennium Cohort Study (MCS) and a three-equation model for maternal happiness, marital status and a value-added, child skill production function. Because maternal happiness is affected by unobserved characteristics that may also affect child outcomes, I use lagged weather conditions and the change in life satisfaction since child birth as exclusion restrictions in the life satisfaction model to provide exogenous variation in contemporaneous life satisfaction. Similarly, because mothers select into a marital status based on unobserved preferences marital status is also endogenous. Exogenous variation in marital status comes from region-year-age variation in male incarceration rates at the time period before the mother started her relationship with the father of the child. Moreover, I account for the latent nature of child skills — a cognitive test score and a battery of six behavioral scores (conduct problems, emotional symptoms, hyperactivity/inattention, peer problems, prosocial behaviors, and self-regulation skills) — and uncover their underlying distribution using item response theory (IRT), which distinguishes my analysis from previous studies that rely on an aggregated, summed measure of child behaviors. Despite that the family structure literature documents a positive association between marriage and child outcomes [e.g., McLanahan and Sandefur 1994; Crawford et al. 2011], marriage per se might not be the only issue, but whether the parent(s) is happy or not. This distinction is crucial on the grounds that part of the beneficial effects of marriage on child development may be driven by maternal happiness. Although some prior studies examine the relationship between marriage and life satisfaction [e.g., Stutzer and Frey 2006; Zimmerman and Easterlin 2006], they do not address their simultaneous determination. In addition to isolating the effect of marital status on child outcomes net of maternal life satisfaction effects, the paper contributes to identifying the direct link between life satisfaction and child outcomes. Evidence about the effects of life satisfaction on child outcomes is scarce. The only study addressing the causal effect of interest is Berger and Spiess [2011] who show that maternal life satisfaction leads to decreases in behavioral problems and increases in cognitive performance of young children in Germany. However, they do not take into account marital status, so their estimates may reflect positive marriage effects. I estimate distinct happiness and marriage effects that differ by child outcome. Maternal happiness promotes only non-cognitive skills. For example, a 10 percent increase in maternal life satisfaction is predicted to increase social skills by an amount equivalent to increasing average annual household income by £38,000. Marriage promotes cognitive skills and select non-cognitive skills. A change from single parenthood to marriage is predicted to increase cognitive skills by the same amount as £25,000 in annual income; however, marriage is predicted to lower conduct problems by the same amount as an income reduction of £51,000. This asymmetry in the estimated effects of interest suggests that promoting only marriage or only maternal happiness will lead to shortages in the accumulation of different types of skill. My finding that marriage has a significant effect on child skills suggests that pro-marriage policies have merit. But because life satisfaction is also independently important for child development, I conclude that a happy and healthy marriage is important, and that life satisfaction is one of the main avenues through which non-married mothers can produce good quality children.",2
43,4,Eastern Economic Journal,02 October 2017,https://link.springer.com/article/10.1057/eej.2015.47,Fracking and Labor Market Conditions: A Comparison of Pennsylvania and New York Border Counties,September 2017,Kelly Hastings,Lauren R Heller,E Frank Stephenson,,,Unknown,Mix,,
43,4,Eastern Economic Journal,02 October 2017,https://link.springer.com/article/10.1057/eej.2015.37,It’s My Party and I’ll Vote How I Want to: Experimental Evidence of Directional Voting in Two-Candidate Elections,September 2017,Thomas Knight,Fan Li,Lindsey Woodworth,Male,,,Mix,,
43,4,Eastern Economic Journal,02 October 2017,https://link.springer.com/article/10.1057/eej.2015.45,On the Nominal and Real Currency Devaluation Nexus in European Transition Economies,September 2017,Salah A Nusair,,,Male,Unknown,Unknown,Male,"An important policy issue facing countries, especially transition and less developed countries, is the effectiveness of nominal currency devaluation (under a fixed exchange rate system) or depreciation (under a flexible exchange rate system) in bringing about real currency devaluation or depreciation. It is argued that nominal devaluation would be effective in improving the trade balance only if it leads to real devaluation [Bahmani-Oskooee and Gelan 2007]. Indeed, currency devaluation has been an important policy tool that is used to improve trade balances of countries. In fact, currency devaluations have been an important part of the International Monetary Fund’s stabilization programs that many countries have implemented [Edwards 1986]. In particular, they have been used to improve international competitiveness, boost exports, increase employment, and improve the trade balance of the devaluing country. According to conventional trade theory, devaluation increases the volume of exports by making exports cheaper in foreign currency terms and decreases imports by making imports expensive in domestic currency terms, thus improving the trade balance. However, nominal devaluation may have negative effects as it makes imports more expensive in domestic currency terms, which may contribute to domestic inflation and, hence, loss of international competitiveness. This suggests that the inflationary effects of nominal devaluation may wipe out any favorable effects that the nominal devaluation may have on the trade balance [Bahmani-Oskooee 2001]. In this context, international competitiveness is measured by changes in the real exchange rate, which is the nominal exchange rate adjusted for changes in prices in the home and foreign countries. If a country’s real exchange rate is rising over time, it means its goods are becoming more expensive relative to its competitors, and thus the country is losing its international competitiveness. Therefore, the argument that nominal devaluation improves the trade balance is based on the assumption that it leads to real devaluation. Thus, an important policy issue facing countries is the effectiveness of nominal currency devaluation or depreciation in producing real devaluation. If nominal devaluation is effective, then it could be used as a policy tool to improve the international competitiveness and the trade balance of the devaluing country. The objective of this paper is to examine whether or not nominal devaluations lead to real devaluations in a group of European transition economies.Footnote 1 To this end, the paper employs quarterly data for 17 European transition economies over the period 1994:1–2013:2. The economies are the Central and East European (CEE) countries (Bulgaria, Croatia, Czech Republic, Macedonia, Hungary, Poland, Romania, Slovakia, and Slovenia), the Baltic countries (Estonia, Latvia, and Lithuania), and the Commonwealth of Independent States (CIS) countries (Armenia, Georgia, Moldova, Russia, and Ukraine). The choice of the countries is based upon data availability. The data are collected from the International Monetary Fund (IMF)’s International Financial Statistics Online Database and the Eurostat Database, and include the nominal and real effective exchange rates defined such that a decrease reflects a real depreciation of the domestic currency. All variables are expressed in logarithmetic forms. In addition, over the sample period, five out of the seventeen countries have experienced different exchange rate regimes.Footnote 2 Namely, between 2004 and 2005, Estonia, Lithuania, Slovenia, Latvia, and Slovakia joined the Exchange Rate Mechanism (ERM II)Footnote 3 and then Slovenia and Slovakia adopted the euro in 2007 and 2009, respectively. In general, in terms of their exchange rate regimes that existed in the pre-ERM II period, the five countries can be split into two major groups: the “peggers” and the “floaters” [Mirdala 2013]. The “peggers” are Estonia, Latvia, and Lithuania, and the “floaters” are Slovakia and Slovenia. Specifically, the Estonian kroon joined ERM II on June 28, 2004, and observed a central rate of 15.6466 to the euro with a standard fluctuation band of±15 percent. The kroon left ERM II when Estonia adopted the euro on January 1, 2011. The Lithuanian litas joined ERM II on June 28, 2004, and observed a central rate of 3.45280 to the euro with a standard fluctuation band of±15 percent. The litas left ERM II when Lithuania adopted the euro on January 1, 2015. The Slovenian tolar joined ERM II on June 28, 2004, and observed a central rate of 239.640 to the euro with a standard fluctuation band of ±15 percent. The tolar left ERM II when the country adopted the euro on January 1, 2007. The Latvian lats joined ERM II on May 2, 2005, and observed a central rate of 0.702804 to the euro with a standard fluctuation band of ±15 percent. The lats left ERM II when the country adopted the euro on January 1, 2014. The Slovak koruna joined ERM II on November 28, 2005, and observed a central rate of 38.4550 to the euro until March 19, 2007 when it was revalued to 35.4424 to the euro until May 29, 2008 when it was revalued again to 30.1260 to the euro while maintaining a standard fluctuation band of ±15 percent. The koruna left ERM II when the country adopted the euro on January 1, 2009. A commonly held view in the literature is that real exchange rates behave differently under different exchange rate regimes (for instance, fixed vs floating regimes). In particular, the behavior of real exchange rates exhibits substantial and systematic differences under fixed and floating regimes, with much more short-term volatility under the floating exchange rate regime than under the fixed rate regime [see, e.g., Mussa 1986; Stockman 1983]. In view of that, and since the five countries (Estonia, Lithuania, Slovenia, Latvia, and Slovakia) have experienced different exchange rate regimes over the sample period, ranging from fixed to floating, this paper will also provide an insight as to whether different exchange rate regimes have different impacts on the link between nominal and real devaluations. To do so, we carry out the analysis on the full sample and on sub-samples corresponding to the different exchange rate regimes that the five countries have experienced. Table 1 provides a summary of the exchange rate regimes that the five countries have experienced over the sample period. To achieve the objective of this paper, the time-series properties of the variables are examined using the augmented Dickey–Fuller [ADF, Dickey and Fuller 1979] and Phillips and Perron [PP, 1988] unit root tests. The results suggest that the variables in some cases are integrated of different orders (I(1), and I(0)). These findings justify the use of the conditional autoregressive distributed lag (ARDL) model of Pesaran et al. [2001]. This model, known as the bounds testing approach, is a single cointegration approach that can be implemented regardless of whether the variables are I(1), I(0), or a combination of both. The results for the full sample show that nominal devaluation leads to real devaluation in the short run in all but Estonia. However, this short-run effect lasts into the long run in only Armenia, Bulgaria, Croatia, Georgia, Hungary, Poland, Romania, and the Czech Republic. The results for the sub-samples corresponding to the different exchange regimes for Estonia, Lithuania, Slovenia, Latvia, and Slovakia suggest that different exchange rate regimes may have different impacts on the link between nominal and real devaluations. The paper contributes to the literature in the following aspects. First, the paper uses quarterly data over the period 1994:1–2013:2 to examine the relationship between nominal and real devaluations for a large number of European transition economies that play an important role as members of the Eurozone and/or European Union (EU) and as trade partners as well.Footnote 4 Second, it employs the ARDL cointegration test that can be used regardless of the integration or cointegration properties of the variables. Using this test avoids the pre-testing problems associated with utilizing cointegration procedures, such as those of Engle and Granger [1987] or Johansen [1988], which require all variables be integrated of the same order. Third, it provides an insight as to whether different exchange rate regimes have different impacts on the relationship between nominal and real devaluations for a sub-group of our sample countries. Fourth, given the limited number of studies examining the relationship between nominal and real devaluations for European transition economies, this paper is an attempt to add to this literature. European transition economies provide an interesting case to study since their exchange rates have experienced, due to the transition process, sharp nominal and real depreciation followed by real appreciation. Thus, the motivation for selecting these economies is their peculiar situation and the limited number of studies examining the relationship between nominal and real currency devaluations for these countries. In the course of the transition process, which started in most countries in the late 1980s and early 1990s, transition economies faced many challenges as they moved from one economic system (centrally planned) into another (market driven). These challenges, as summarized by the IMF [2000], represent the main ingredients of the transition process and include: liberalization of markets to allow prices to be determined in free markets; achieving macroeconomic stabilization to bring inflation under control; restructuring and privatization to create a viable financial sector; and establishing legal and institutional reforms to introduce appropriate competition policies [IMF 2000].Footnote 5
 Moreover, since these economies are relatively small, open economies, depending on exports to promote economic growth, changes, and developments in their exchange rates may adversely affect their trade flows and, hence, economic growth. An important stylized fact regarding the development of the exchange rates of these economies is their initial undervaluation, misalignment, and appreciation due to the transition process [Bahmani-Oskooee and Kutan 2009]. Prior to the transition process, currencies of transition economies were believed to be strongly over-valued and substantial currency devaluation was considered to be a necessary pre-condition to support the liberalization of markets and to bring domestic prices closer to the world prices [Halpern and Wyplosz 1997]. The transition process was initially accompanied by very high inflation rates and real depreciation of most currencies of transition economies [Taylor and Sarno 2001; Papazoglou and Pentecost 2004]. The initial depreciation was then followed by a significant, continuing secular real appreciation [Taylor and Sarno 2001]. Figure 1 provides a plot for the behavior of both the nominal and real effective exchange rates over the sample period. Logarithm of nominal and real effective exchange rate behavior. Solid line: nominal effective exchange rate. Dotted line: real effective exchange rate. While for some currencies the substantial real appreciation may be partly explained as a return of the currencies to their equilibrium values following the initial depreciation, for other currencies, it may be related to real factors (such as efficiency gains and rising productivity stemming from structural reforms) as a consequence of the liberalization process [Halpern and Wyplosz 1997]. This trend appreciation caused by real factors is the well-known Balassa–Samuelson effect, which states that fast-growing economies experience real currency appreciation. However, real appreciation could also be due to nominal shocks, such as fiscal and monetary policies [Barlow 2003]. At the early stage of the transition, the economies faced high and volatile inflation rates. As a result, they were forced to manage their exchange rates to control inflation and to achieve domestic price stability. Therefore, foreign exchange interventions were regular and aimed at preventing or slowing down the real appreciation [Sideris 2006]. However, at a later stage, achieving competitiveness, measured by the real exchange rate, played an important role. To achieve competitiveness, some countries undertook a series of official devaluations, others switched to more flexible exchange rate policies, and others established independent central banks [Kočenda and Valachy 2006; Bahmani-Oskooee and Kutan 2009]. Thus, most transition economies began their transition with a sharp nominal and real currency depreciation followed by real appreciation as domestic inflation exceeded subsequent nominal depreciation over the course of the transition [Brada 1998]. The remainder of this paper is organized as follows. The next section provides literature review. The subsequent section presents the methodology. The penultimate section reports the results and the final section concludes.",1
43,4,Eastern Economic Journal,02 October 2017,https://link.springer.com/article/10.1057/eej.2015.53,Missing Men: Determinants of the Gender Gap in Education Majors,September 2017,J Farley O Staniec,,,Unknown,Unknown,Unknown,Unknown,,
43,4,Eastern Economic Journal,02 October 2017,https://link.springer.com/article/10.1057/eej.2016.1,Formula Scoring Reconsidered,September 2017,Ellen Sewell,,,Female,Unknown,Unknown,Female,"The multiple choice test format was reportedly introduced by William A. McCall in 1920 [Slakter 1968].Footnote 1 From the beginning, a correction for guessing, or formula scoring, was advised in response to concerns that correct answers based on luck are indistinguishable from those based on knowledge.Footnote 2 The standard correction deducts a portion of the question’s point value for an incorrect response with no points awarded or deducted for an omitted response. For a question with k response options, the simplest correction is a deduction of 1/(k−1) times the question’s point value for an incorrect response. This will be referred to as a standard correction.Footnote 3 The expected return for random guessing on such a question would be zero.Footnote 4 So in the case where the student either knows the correct answer with certainty or randomly guesses (the knowledge-or-random-guessing model), the corrected score provides an unbiased estimate of true knowledge. The scoring of tests with a correction (or penalty) for guessing, a practice known as formula scoring, has recently fallen from favor. The College Board, previously a staunch supporter of the practice, eliminated formula scoring on its advanced placement (AP) tests in 2011 and plans to do so on its Scholastic Aptitude Test (SAT) in 2016. The alternative, rights scoring, makes no distinction between an incorrect response or an omission. Rights scoring is used on the American College Testing (ACT). The scoring policy varies for the vast assortment of other standardized tests. And while the multiple choice test format is popular with college instructors, many do not incorporate a correction for guessing. So while a consensus concerning the best practice has failed to fully emerge, it appears that there is a move towards abandonment of formula scoring. A prevailing concern has been that the ranking of students under formula scoring captures not just true knowledge but also risk attitude. Yet past empirical work suggests that this is not the case because test items were omitted under formula scoring only when the alternative was random guessing [Lord 1963; 1974].Footnote 5 Such behavior would be consistent with the maximization of the expected score and risk neutrality. However, economic studies find that when decision makers are confronted with uncertainty, they typically exhibit risk-averse behavior.Footnote 6 A risk-averse student who has enough knowledge to assign unequal subjective probabilities across response options would omit any item where the positive expected return would be insufficient to compensate for the risk. Furthermore, a more risk-averse student would be expected to omit a greater number of items, and ones with a greater probability of being answered correctly, other things equal. In this case, the ranking of students could differ between rights and formula scoring [Albanese 1988]. The purpose of this paper is not to condone or condemn formula scoring. Instead, this study seeks to align the expectations of economists that students are risk averse with previous studies documenting risk-neutral behavior in response to formula scoring. The underlying problem is that it is difficult to isolate the impact of formula scoring using actual test data because there is no indication of why an item was answered or omitted. This paper uses an experimental approach to isolate the impact of the potential penalty by bypassing the assignment of subjective probabilities to the response options. The majority of subjects did initially exhibit risk-averse behavior. A target or threshold score was then introduced in an attempt to align this finding with previous empirical evidence of risk-neutral behavior. This study finds that the introduction of a threshold score does induce the typical subject to behave in a less risk-averse manner, a finding that may have application well beyond the issue of formula scoring. Finally, the impact of coaching in preparation for tests with a penalty for guessing was investigated. Students reported a wide variety of advice but it was found to have little effect on their behavior.",
43,4,Eastern Economic Journal,02 October 2017,https://link.springer.com/article/10.1057/eej.2015.55,An Introduction to Understanding and Teaching Within-Cluster Correlation in Complex Surveys,September 2017,Humberto Barreto,Manu Raghav,,Male,Male,Unknown,Male,"This note describes and explains how to access code written in Excel, Stata, and R to teach the concept of within or intra-cluster correlation, which is usually the most complicated element of complex surveys. Highlights include clear presentation of the data generation process (DGP), simulation to demonstrate sampling distributions, and emphasis on the estimated standard error (SE) as a random variable. Most real world surveys such as the Current Population Survey are collected using a complex design that includes cluster sampling. Please refer to Barreto and Raghav [2013a] for intuition and instructions to correctly use survey data that is collected with unequal probability of selection. Here we will focus on cluster sampling and its appropriate econometric analysis. As the use of complex survey data from large publicly available sources is becoming more common, it is increasingly important to learn about this topic not only for graduate students and academic researchers, but also for undergraduate students so that they can also learn to correctly use data from cluster sampling. The examples, explanations, code for Excel, Stata, and R, sample exercises, and learning objectives will reduce the cost of including this topic in an undergraduate course. These materials can also be used outside the classroom by undergraduate students and research assistants to learn this topic on their own time and pace, with some help from a professor. As undergraduate research in various forms using real world data is becoming more common, even undergraduate students need to know best practice methods when using data generated by complex survey designs. Keeping in mind our intended audience and the purpose of this note, we have eschewed a mathematically rigorous approach consisting of a variety of abstract modeling scenarios and error structures cast in formal language. Instead, we use a single, concrete within-cluster correlated error DGP (in the family of cluster-specific random effects models) based on Moulton [1990]. We further restrict the analysis by making the ordinary least squares (OLS) estimator the main focus of the paper. Our goals include explaining why OLS estimated SEs (which we call classic SEs) do not do a good job in estimation; why robust SEs, which correct for heteroscedasticity, do not perform better either; and how cluster SEs, which give good results with large samples, correctly incorporate the error structure. We include, as optional, advanced material in the Excel workbook, explicit matrix derivations of the various SEs, including exact SEs. To access these materials, visit: www.depauw.edu/learn/stata and refer to Barreto and Raghav [2013b] (click on ClusterPaper.pdf). All files are freely available. A step-by-step manual for using the Excel, Stata, and R versions of the simulations is provided, including additional pedagogical support and references in the complex survey literature. We welcome all extensions and will include them on the website.",
43,4,Eastern Economic Journal,23 March 2017,https://link.springer.com/article/10.1057/s41302-017-0094-1,Economists Should Stop Doing It with Models (and Start Doing It with Heuristics),September 2017,David Colander,,,Male,Unknown,Unknown,Male,"I wouldn’t be writing about this problem if I didn’t have, or at least think I have, a solution. My solution is simple: policy economists should stop thinking of themselves as applied scientists and start thinking of themselves as engineers.5 Engineers don’t see themselves as doing it with models; they see themselves doing it with heuristics. They define heuristic as “anything that provides a plausible aid or direction in the solution of a problem but is in the final analysis unjustified, incapable of justification, and fallible.” Heuristics includes the formal and semiformal models that economists tend to think of as models, but they also include highly informal models, intuition, and anything else that might lead to an answer to the question one is trying to answer. The very name, “heuristic” emphasizes the limitations of these aids and thus makes it less likely that economists will fall prey to the Ricardian Vice. I discovered the importance of heuristics to engineers as I explored engineering methodology and how much it emphasizes heuristics. Billy Vaughn Koen, one of the few engineers who have written about engineering methodology, explains: “Everything the engineer does in his role as an engineer is under the control of a heuristic. Engineering has no hint of the absolute, the deterministic, the guaranteed, the true. Instead it fairly reeks of the uncertain, the provisional and the doubtful. The engineer instinctively recognized this and calls his ad hoc method: doing the best you can with what you’ve got ‘finding a seat of the pants solution,’ or just muddling through.” That is a good description for what goes on in the basement. Koen notes four signatures of a heuristic: it does not guarantee a solution; it may contradict other heuristics; it reduces the search time in solving a problem; its acceptance depends on the immediate context instead of on an absolute standard. Unlike economists, who tend to see themselves as applied scientists, engineers see themselves as problem solvers; they are not limited by scientific methodology. Instead, they see themselves using “state-of-the-art” (SOTA) heuristics. These SOTA heuristics are constantly changing, as engineers learn-by-doing. So whereas scientific methodology tends to be rigid and similar across subfields of the science, engineering methodology is looser, evolving, different in different subfields, and in a constant state of flux.",6
43,4,Eastern Economic Journal,13 March 2017,https://link.springer.com/article/10.1057/s41302-017-0095-0,"The Solution is Better Science, Not Less Science",September 2017,Diego C. Nocetti,,,Male,Unknown,Unknown,Male,"For example, Blinder [1997] summarized his experience in central banking as one involving “as much art as science.” Blanchard [2006] and Mishkin [2007] have expressed similar views, depicting the practice of monetary policy making as integrating scientific methods with informal methods that permit efficient decision making in rapidly changing environments, while the reaction of central banks around the world to the global financial crisis and the great recession has presented clear, renewed evidence of those views (see, e.g., Blinder et al. 2016; Yellen 2016). See, for example, Basu’s [2016] exposition as chief economist of the World Bank. The underlying motivation for economists is to persuade the public, aligning the public’s beliefs with the economist’s knowledge and policy preferences. There is nothing wrong with this, especially given that economists are typically better informed in economic matters than the average citizen. The trouble arises when economists try to persuade the public in a deceptive way.",
43,4,Eastern Economic Journal,05 September 2017,https://link.springer.com/article/10.1057/s41302-017-0102-5,Book Review of Economics Rules by Dani Rodrik,September 2017,Nicholas Mangee,,,Male,Unknown,Unknown,Male,,
43,4,Eastern Economic Journal,02 October 2017,https://link.springer.com/article/10.1057/eej.2015.27,Getting a Ph.D. in Economics,September 2017,Derek Pyne,,,Male,Unknown,Unknown,Male,,
43,4,Eastern Economic Journal,02 October 2017,https://link.springer.com/article/10.1057/eej.2015.29,What Have We Learned? Macroeconomic Policy after the Crisis,September 2017,Cindy Jacobs,,,Female,Unknown,Unknown,Female,,
44,1,Eastern Economic Journal,05 December 2017,https://link.springer.com/article/10.1057/s41302-017-0105-2,The Changing Pattern of Stock Ownership in the US: 1989–2013,January 2018,James M. Poterba,,,Male,Unknown,Unknown,Male,"The Survey of Consumer Finances (SCF) is the primary source of information on the balance sheets of US households. This triennial survey, conducted routinely since 1983 under the auspices of the Federal Reserve Board, over-samples high-income, high-wealth households that account for a large fraction of total financial wealth. This makes it the premier data source for assessing asset holding patterns. Bricker et al. [2014] present key findings from the 2013 survey.1 The household is the primary unit of observation in the SCF. Focusing on households avoids the need to determine whether stock owned by married couples is held jointly or is owned by only one member of the couple. Table 1 presents summary statistics on stock ownership from each wave of the SCF between 1989 and 2013. The first column shows the percentage of households owning stock “directly,” i.e., without any intermediary such as a mutual fund or retirement plan. This percentage rose sharply between 1995 (15.2 percent) and 2001 (21.3), but declined in subsequent years. It was 13.8 in 2013. The run-up in direct stock ownership during the late 1990s was short-lived. The second column shows the percentage of households holding an equity mutual fund outside a retirement account. This entry includes mutual funds that the household reports as investing exclusively or primarily in stocks, as well as “balanced funds” that invest in both stocks and bonds. This form of stock ownership also shows rapid growth from 1989 (7.2 percent) through 2001 (17.7), but subsequent decline to 8.2 percent in 2013.2
 The last column of Table 1 shows the percentage of households owning stock through retirement accounts. These accounts, both employer-sponsored 401(k) plans and Individual Retirement Accounts created by individuals, expanded rapidly during the 1990s. Stock ownership through this channel more than doubled, from 18.6 percent to 37.5 percent, between 1989 and 2001. The growth has been modest since then; this percentage peaked at 45 in 2007 and was 43.3 in 2013. This reflects a decline in the rate of expansion of the number of households covered by such plans. The Board of Governors of the Federal Reserve’s 2016 SCF Chartbook [2017] reports that the percentage of households with such accounts grew from 37.1 in 1989 to 52.8 in 2001. It has fluctuated in a small range since then: 49.9 (2004), 53.0 (2007), 50.4 (2010), 49.2 (2013), and 52.1 (2016). Most, but not all, of these accounts hold equities. There is some duplication of ownership across the rows of Table 1. Households that hold stock directly may also hold stock through a retirement plan, so the percentage of households owning stock in some way is not the sum of the three rows. Table 2 addresses this issue. The first and second rows, for direct stock ownership and ownership through equity mutual funds in taxable accounts, are the same as those in Table 1. The third row in Table 2 reports ownership of common stock through variable annuity products and other taxable accounts distinct from equity mutual funds. This form of ownership, while uncommon in 1989, was reported by 5.2 percent of households in 2013. The fourth row summarizes the percentage of households who own stock through one or more of the channels described in the first three rows. It shows that ownership of stock in taxable accounts rose by nearly ten percentage points (20.4–31.1) between 1989 and 2001, and then declined by almost as much, to 21.0, by 2013. The next row describes ownership through defined contribution pension accounts, and row six summarizes ownership through trusts, which are of modest importance: 1.3 percent of households in 2013 report owning stock in this form. Finally, the seventh row presents the overall percentage of households owning stock. This rises from 31.9 percent in 1989 to 53 percent in 2001 and falls back to 48.8 percent in 2013. The entries in Table 2 offer insight into the factors that contributed to the broad rise, and then plateau or decline, in stock ownership. More than half of the increase in the percentage of households owning stock between 1989 and 2001 was accounted for by owners whose only equity positions were in retirement saving plans. This suggests that the diffusion of retirement saving plans drew new investors into the stockholding population. Direct stock ownership and equity mutual fund ownership contributed the other half of the rise in stock ownership during the 1990s. Although both of these sources of ownership have declined more recently, the relatively stable share of households owning stock through retirement accounts, which rose by one percent between 2001 and 2013, has avoided a sharp decline in the overall ownership rate. One important feature of stock ownership, emphasized in Poterba and Samwick [1995], is that many households hold relatively small equity positions, while a small group of households account for a large share of the value of household stockholding. The second panel of Table 2 is like the first, but it defines “stock ownership” as holding equities worth more than $50,000 ($2013). Only 21.8 percent of US households held more than this amount of stock in 2013, less than half of all stock owners. There are differences across mode of ownership in the size distribution of ownership positions. Two-thirds of the households that own stock directly own stock worth more than $50,000, compared with only one-third of those holding stock through tax-deferred accounts. To describe the value of stocks held in different channels, not just the number of holders, Table 3 reports the mean and median values of stockholdings among those who own stock. The tabulations illustrate the skewness of the stockholding distribution. Mean holdings in retirement accounts and in equity mutual funds outside retirement accounts are five times the median. For directly held stock, the mean is more than ten times the median. The last column of Table 3 shows the relative importance of aggregate stock ownership through different channels in 2013. While only about 20 percent of households own stock outside retirement accounts, either in a taxable account or an equity mutual fund, these holdings represent more than half of the total stock held by households.",1
44,1,Eastern Economic Journal,30 August 2016,https://link.springer.com/article/10.1057/s41302-016-0069-7,Impact of Ethnic Civil Conflict on Migration of Skilled Labor,January 2018,Julie Christensen,Darius Onul,Prakarsh Singh,Female,Male,Unknown,Mix,,
44,1,Eastern Economic Journal,09 January 2018,https://link.springer.com/article/10.1057/eej.2016.5,"Social Dividends, Entrepreneurial Discretion, and Bureaucratic Rules",January 2018,Douglas W MacKenzie,,,Male,Unknown,Unknown,Male,"Market Socialists (e.g. Oscar Lange) argued that socialist officials could simulate market pricing through trial and error by adjusting prices as inventories rise or fall. Austrians [e.g. Lavoie 1985] claim that the lack of genuine entrepreneurship in simulated markets render simulated markets illegitimate. Lange allegedly overlooked the role entrepreneurs play in the market process. Austrians and many Marxists believe that simulating markets conflicts with the true socialist aim of overturning the results of market competition.Footnote 1
 Recent research on socialism largely ignores social dividends and bureaucratization. Social dividends are residuals over factor costs, divided among socialist citizens as equal owners of capital. Bureaucratization means rigid conformity to rules and disregard for consumer sovereignty by officials. Substitution of social for private dividends transfers control of capital from private financiers to either central authorities or bureaucrats. Decentralized bureaucracies face the problem of mutual plan coordination. Central authorities need not coordinate separate plans, but cannot comprehend capital costs in a modern economy. All forms of socialism lack financial markets as a means of planning investment. This paper argues that social dividends and bureaucratization are central to the critique of socialism developed by Ludwig von Mises and F.A. Hayek. The first part of this paper contrasts financial markets and bureaucracies. The second part asserts that Dickinson and Lange admitted that state planning is arbitrary, but also argued that private investment in financial markets works no better. The third part examines John Roemer’s proposed socialist stock market, whereby stock ownership is private but equalized. Roemer addresses agency issues, but does not adequately address investment planning problems. The final section reevaluates the original calculation debate, and suggests new avenues for research.",2
44,1,Eastern Economic Journal,09 January 2018,https://link.springer.com/article/10.1057/eej.2016.3,Covenant Marriages: Increasing Commitment or Costs?,January 2018,Amanda J Felkey,,,Female,Unknown,Unknown,Female,"The economic research devoted to understanding how marriage and divorce legislation affects family formation and dissolution is plentiful. Beginning with Becker’s [1981] Treatise on the Family, economists have considered outcomes in marriage markets throughout the world, estimated the value of being married, and debated extensively over how divorce legislation affects divorce rates [Peters 1986; Allen 1992; Gatland 1997; Friedberg 1998; Wolfers 2006]. With an eye toward how policy affects the well-being of couples as well as any children they may have, economists continue to make strides in understanding how the social and legal environment in which couples find themselves shapes their decisions to enter and exit marriage. This paper adds to this literature by exploring the effects of a relatively new choice and potential marital commitment device, the covenant marriage. State governments began considering the covenant marriage option in 1997 when Tennessee, Kansas, and Louisiana all had covenant marriage bills proposed and debated. In that year Louisiana was the first state to make covenants a legal option. Since 1997 the covenant option has been formally considered at least once and often repeatedly by more than half the states in the United States. Most of these proposals were in the early 2000’s, but this option has been formally put forth as recently as 2012 by Alabama, 2013 by Oklahoma, and 2015 by Texas. Proposed covenant marriage bills have passed one but not both houses in Oregon, Georgia, and Texas; and covenant marriage bills have been adopted in four states: Louisiana, Arizona, Arkansas, and, most recently, Oklahoma. Figure 1 illustrates the states that have considered the covenant option, and highlights the states used in this analysis (the three states where covenants are available and the three states used as controls). States by consideration of covenant marriage legislation. Proponents of the covenant marriage option contend divorce rates will decline because individuals will find themselves in stronger, happier unions. And states that have adopted the covenant option have generally done so to combat high divorce rates. However, the effects of incorporating covenant marriages into the menu of legal relationship types may not be that straightforward. First of all, whether or not the unions are “stronger” depends on whether or not covenant marriages are an effective commitment device [Felkey 2011], and increase the cost of divorcing enough to deter couples from doing so. Second, covenant marriages not only affect those couples who opt into them, but they also alter the relationship opportunity set for everyone else. This means the costs and benefits associated with engaging in all other relationship types changes and the covenant option may have unintended consequences for the marriage and divorce rates among those considering or already in a traditional marriage. So even if covenant marriages are merely an option and one that is not very often utilized, the effects on marriage and divorce rates may not be negligible. The goal of this paper is to document the effects of the covenant marriage option on family formation and dissolution. That is, does covenant marriage legislation increase or decrease divorce and marriage rates? This paper will first theoretically explore the costs imposed by covenants, both on those who opt for this type of marriage, as well as those who do not [Drewianka 2004]. Second, in order to determine whether the covenant option is effective in its goal of decreasing divorces, this paper empirically examines how divorce and marriage rates change, taking into consideration (1) covenant availability [Friedberg 1998], (2) how long covenant marriages have been an option [Wolfers 2006], and (3) the rate of their utilization. The empirical analysis uses administrative data from six states: Louisiana, Arizona, and Arkansas, which have had the covenant marriage option since 1997, 1998, and 2001, respectively, and Mississippi, Oklahoma, and Texas, which did not have the covenant option during the time period analyzed in this paper. The remaining sections of this paper are laid out as follows: The section “Theoretical effects of the covenant marriage option” details theoretically how the covenant marriage option affects the decisions to marry and divorce, even for those not using covenants. The section “Data and measurement” describes the data and measurements used. The section “Empirical effects of the covenant option — availability, longevity and utilization” documents empirically the effects of this new type of union on divorce and marriage rates. The section “Direct effect of covenants in Arkansas” measures the effects of the direct costs of covenants in Arkansas. The final section remarks on the implications of this paper’s findings for state marriage and divorce policy.",
44,1,Eastern Economic Journal,09 January 2018,https://link.springer.com/article/10.1057/eej.2016.4,Bringing the Effects of Occupational Licensing into Focus: Optician Licensing in the United States,January 2018,Edward J Timmons,Anna Mills,,Male,Female,Unknown,Mix,,
44,1,Eastern Economic Journal,25 August 2016,https://link.springer.com/article/10.1057/s41302-016-0070-1,Seniority Wages in the National Hockey League,January 2018,James A. Brander,Edward J. Egan,,Male,Male,Unknown,Male,"The economics of any professional sport are dependent on the salary structure of that sport. Some aspects of salary structures have been extensively studied, such as discrimination on the basis of ethnic identity. However, relatively little academic attention has been paid to seniority wages – the phenomenon that younger players are typically paid less relative to performance than older players. Seniority wage structures have important consequences for competitive advantage in sports, particularly if a salary cap is in place, as in the National Hockey League (NHL). If younger players earn less relative to performance than older players, then teams with a favorable supply of good young players can afford more high quality players under a given salary cap than can a team with older players. The first main objective of this paper is to estimate the structure of the seniority wage profile in the NHL. We show that performance-adjusted salary rises sharply with age over most of the relevant age range, peaking at about age 32 for forwards. Our second main objective is to explain the reasons for this pattern, focusing on the league-imposed contractual environment as a determinant of the NHL seniority wage profile. Professional sports leagues have special features that make the determination of salaries particularly interesting. Specifically, the NHL consists of individual firms (teams) that are concerned with their own profits and that compete with other teams both on and off the ice. The league as a whole, however, coordinates the activity of individual teams in an effort to produce an appealing product that promotes league-wide profitability. The determination of player salaries illustrates the importance of these two decision-making levels. Individual player salaries are set through bargaining between a team and an individual player (or his agent). However, the structure of salaries depends in large part on the overall collective bargaining agreement between the NHL players’ association and the league. One innovation of this paper is the ability to distinguish between the effects of the league-wide contractual structure on salary and player-specific and team-specific considerations, including individual performance, age, and tenure with a specific team. The remainder of this paper is organized as follows. The next section contains a literature review and conceptual framework, followed by sections describing the data, presenting our regression analysis of NHL salaries, and providing concluding remarks.",3
44,1,Eastern Economic Journal,09 January 2018,https://link.springer.com/article/10.1057/eej.2016.9,FDI and Trade Policy Openness in Sub-Saharan Africa,January 2018,Godfred William Cantah,Gabriel William Brafu-Insaidoo,Abass Adams,Unknown,Male,Unknown,Male,"Foreign Direct Investment (FDI) as an important element of globalization, is a major driver of economic transformation, employment creation, technological improvements, and eventually economic growth. FDIs plays a vital role of meeting the development, foreign exchange rate, investment, and tax revenue needs of developing countries [Quazi 2007]. Specifically, FDI can play a significant role in Sub-Saharan Africa’s (SSA) development agenda by enhancing domestic savings, employment creation and growth, integration into the global economy, transfer of technologies, enhancement of efficiency, and raising skills of local manpower [Anyanwu 2006; Dupasquier and Osakwe 2006]. Despite the importance of FDI in promoting economic growth and development agenda, SSA countries continually attract low level of FDI. Over the years, the inflow of FDI into SSA compared with other regions has been low. Example, FDI inflows between 1980 and 1989 was 2.6 percent of the world average, 1.9 percent in the period 1990–1999; and 3.2 percent in the period 2000–2009. During the same periods, the Asian region received 14.2, 19.1, and 19.1 percent, respectively, of the total world average FDI inflows [Anyanwu 2011]. Though FDI inflow to Sub-Sahara Africa increased from US$29.5 billion to $36.9 billion in 2011, it is still relatively very low compared with other sub regions in the world with similar level of development as indicated in Figure 1. Most of these inflows have largely concentrated in Nigeria, South Africa, and Ghana. Nigeria alone accounts for over one-fifth of all FDI flows to the continent [UNCTAD 2012]. As indicated in the Figure 1, most of the FDI inflows into the developing world mostly finds its way into East and South Asia, followed by Latin America. Inflow into SSA though increasing, is still relatively low. FDI inflow as a percentage of GDP in three selected regions. 
Source: UNCTAD (2014). A report by UNCTAD [2012], indicated that inflows of FDI to Africa declined in 2011 for the third successive year. An important question that arises from this is: Why cannot Africa attract much FDI? The answer to this question is very important to both economists and policymakers. The quest to find answers to this question over the past decades has culminated into a large body of empirical literature that seeks to identify the determinants of FDI inflows into Africa. These studies have identified a wide range of determinants of FDI in the sub region [Seim 2009]. These comprise of macroeconomic factors (market size, openness, human capital, labor costs, cost of investment, trade deficit, exchange rate, total tax rate, inflation, budget deficit, domestic investment, external debt, government consumption expenditure, and energy use), business environment and institutional variables. Apart from market size however, there is still no strong consensus as to what variables are more robust determinants of FDI inflows. Although results vary in the empirical literature, openness is one of the determinants identified as being more likely to be robust when compared with other potential determinants of FDI [Seim 2009]. A large number of empirical works have therefore focused on the influence of trade openness on FDI inflows into Africa. Majority of these studies concentrated on revealed openness’ (ratio of exports and imports to GDP) on FDI inflows (see: [Asiedu 2013]; Kandiero and Chitiga 2006; Seim 2009; Anyanwu 2011]. 
Lloyd and MacLaren [2002] noted that one major difficulty associated with the revealed openness is that the figures in the numerator and the denominator are in nominal prices. However, over a period of time the prices of goods and services that are produced domestically and those produced internationally may diverge as a result of variations in the exchange rate or other relative price movements. They further argued that a country may have a high trade to GDP ratio because the country is endowed with natural resources which are mainly exported or because the size of the economy is relatively small or perhaps due to high preference of residence for foreign goods and not necessarily as a result of low trade restrictions. Though some countries attract FDI based on the availablity of natural resources, recent trends indicate that the flow of FDIs to countries that are relatively less endowed in natural resources seem to be higher than countries with relatively greater natural resource endowment. Hence, the availability of natural resources may not be the only means to an end for attracting FDI. Thus access to foreign markets and the size of the internal market can probably affect the trade to GDP ratio. This implies that geographical position, conditions and the level of income might be correlated with the openness measure. Also, larger countries (in terms of economy size) might have a lower measure of openness due to their level of income and a higher level intra-national trading activities. This is believed to also affect the flow of FDIs [Seim 2009]. On the other hand, policy openness takes into account barriers to trade, correcting for structural features and price distortions. Sachs and Warner [1995], introduced a similar measure of trade openness using a dummy which was based on the fraction of years between 1965 and 1985 that a country was integrated with the global economy (a country is said to be integrated if it maintained reasonably low tariffs and quotas, and did not have an excessively high black market exchange rate premium). However, with the onset of trade libralisation in almost all African economies and given the level of progress in most economies, one will be unable to draw a clear distinction between an open economy and a closed economy based on the suggessted measures provided. Again dummy variables have the tendency to capture other policy effects which may not necessarily be the level of trade openness. Hence as contribution to the literature on the effect of trade openness on FDI inflows in Africa, this paper developed a trade openness variable from the doing business indicators and the World Development Indicators that is published by the World Bank, within the circles of policy openness to analyze the effect of trade openness on the flow of FDI to Africa. The study is based on panel data set from 2006 to 2013 for 28 SSA economies. This paper differs from previous studies on FDI in Africa based on the fact that it uses a new data set from the doing business indicators of the World Bank to create an index that captures the cost and ease with which trading activities take place within an economy to capture trade openness within the circles of policy openness. The rest of the paper is organized as follows: we examine the trade openness variable, this is followed by an exploration of the theoretical and empirical literature related to the determinants of FDI in Africa, this followed closely by a description of the econometric model employed and discusses the empirical approach and data used. The econometric results and analysis as well as the findings presented after the econometric model. The study ends with the conclusion.",18
44,1,Eastern Economic Journal,09 January 2018,https://link.springer.com/article/10.1057/eej.2016.8,Banking Concentration and Financial Stability. New Evidence from Developed and Developing Countries,January 2018,Mohamed Sami Ben Ali,Timoumi Intissar,Rami Zeitun,Male,Unknown,Male,Male,"In recent decades, financial instability has become a major source of concern worldwide. The proliferation and recurrence of financial crises since the 1980s, which affected both developed and developing countries, as well as the socio-economic costs they generated are the main reasons for this concern. A significant component of this concern lies in the central role of banks at the heart of countries’ growth dynamics. Banking activity has undergone dramatic changes in terms of banks’ structure, status, and regulations in a competitive and changing environment. Financial deregulation endorsed the market entry of non-banking financial institutions. In addition, bank deregulation caused significant structural changes that impacted the fragility of financial systems. The idea that emerges highlights the importance of banking concentration and the creation of stronger banks to have more stable financial systems. As a result, a broad movement of mergers and acquisitions has emerged around the world, and banks dramatically decreased in number but increased in size. Arguments that emerge in the literature have not, however, all documented a positive effect of concentration on financial stability. The empirical literature dealing with this relationship presents two possible strand of literature in the sense that the concentration may promote stability [Beck et al. 2006; Evrensel 2008], as it can also be a source of instability [Boyd et al. 2006; Shehzad et al. 2009 and Uhde and Heimeshoff 2009]. The first strand of literature suggests the existence of a positive relationship between market concentration and financial stability in the sense that more concentrated markets allow banks to earn higher profits, which will serve as a buffer against unexpected or potential negative shocks. On the contrary, the second strand of literature suggest a negative relationship between market concentration and financial stability in the sense that market power can induce banks to charge high interest rates to borrowers. Therefore, borrowers will take excessive risks and this will in turn raise the default risk that is considered as a financial destabilizing effect. It is worth noting that most of the previous studies examined the direct effect of banking concentration on financial stability and financial crises. Except for a few studies [Bretschger et al. 2012, among others], the indirect effect of concentration on financial stability has not been extensively investigated. However, there are several potential indirect effects of concentration on financial stability. No serious study has investigated how these channels act in crisis and in normal periods. The current study takes further steps and intends to fill this gap by examining the direct and indirect effects of banking concentration on financial stability for a sample of 156 developed and developing countries during the period 1980–2011. Three distinct motivations, thus contributions, drive our study. First, this study intends to bring new insight to the current empirical literature in this field. Second, it considers a panel of 156 developed and developing countries, which will deliver more robust empirical results and helps comparing our results with the existent literature. Third and more importantly, the study examines the existence of the direct and indirect effects of banking concentration on the likelihood of a financial crisis during normal and crisis periods. This study considers the effect of concentration on financial stability. Given potential simultaneity, it would be also interesting in future research to develop another study on the hypothesis under which such causality may occur. The remainder of the paper is organized as follows. The next section covers the relevant literature on the banking concentration and financial stability nexus. The methodology, data description, and estimation procedures are discussed after that. The subsequent section presents the empirical results. The last section concludes and presents some policy implications.",14
44,1,Eastern Economic Journal,09 January 2018,https://link.springer.com/article/10.1057/eej.2015.33,Ideology and Dissent among Economists: The Joint Economic Forecast of German Economic Research Institutes,January 2018,Ha Quyen Ngo,Niklas Potrafke,Christoph Schinke,,Male,Male,Mix,,
44,1,Eastern Economic Journal,09 January 2018,https://link.springer.com/article/10.1057/eej.2015.50,Rational Bias in Inflation Expectations,January 2018,Robert G Murphy,Adam Rohde,,Male,Male,Unknown,Male,"Expectations about price inflation play a central role in modern macroeconomic analysis. They are important for understanding how households and firms make saving, spending, and investing decisions, and are a key input into negotiations for labor contracts and the pricing of financial instruments. Central banks track them for comparison with internal forecasts and targets for inflation. The ability of monetary authorities to achieve price stability depends on an accurate understanding of inflation expectations. A concern sometimes raised by policymakers is whether inflation in highly visible products such as food and energy might overly influence the public’s perception of inflation. For example, discussion at the Federal Open Market Committee meeting in June 2008 clearly illustrates this concern: Participants had become more concerned about upside risks to the inflation outlook — including the possibility that persistent advances in energy and food prices could spur increases in long-run inflation expectations
 Some noted that the increase was greatest for short-term survey measures of households’ inflation expectations, which may be influenced disproportionately by consumers’ perceptions of changes in the prices of food and gasoline. [Federal Open Market Committee 2008, pp. 6–7] This concern is not a recent one, having featured prominently during policy discussions about supply shocks during the 1970s.Footnote 1
 In an interesting paper, Van Duyne [1982] considers policymakers’ concerns that food price inflation may overly influence the public’s expectations of inflation. He uses a simple model of the inflation process to illustrate how such “bias” can represent rational behavior.Footnote 2 Van Duyne is unable to reject the hypothesis that the weights implied by his model are equal to the actual expenditure shares of food and other items in the consumer price index. He concludes that “contrary to the conventional wisdom of policymakers, consumers appear not to bias their expectations toward food prices” [Van Duyne 1982, p. 420] Similar to Van Duyne, our paper explores whether prices of items that individuals frequently purchase and that often are quite volatile, such as food and energy products, play a larger role in the formation of inflation expectations than their expenditure shares would indicate. We show how it may be rational for individuals to assign relative weights to price increases in the food and energy sectors that are larger than those sectors’ shares in consumer expenditure. To illustrate this potential “bias” in inflation expectations, we develop a simple three-sector dynamic aggregate demand-aggregate supply model of the economy with gradual price adjustment in the “core” (non-food, non-energy) sector and flexible prices in the food and energy sectors. Serial correlation of shocks to food and energy prices allows individuals to gain an understanding about future shocks, possibly making it optimal for individuals to place more weight on the movement of prices in these sectors, as past movements of these prices may help predict future inflation.Footnote 3
 Our framework extends Van Duyne’s paper in three important ways. First, we model aggregate demand as a function of the real interest rate and incorporate a Taylor-type policy rule for the nominal interest rate, whereas Van Duyne expresses aggregate demand as a function of real money balances and assumes a money supply growth rule. Second, we allow for a differential optimal response by the monetary authority to inflation across sectors rather than assume a single response to overall inflation. Third, in our empirical analysis we test whether or not any observed bias in the formation of expectations about inflation is rational, whereas Van Duyne tests only for the presence of such bias but not whether it is rational. We estimate the model using survey data on expected inflation from the Federal Reserve Bank of Philadelphia.Footnote 4 In contrast to Van Duyne, our results show the weights implied by the model for constructing expectations of inflation differ from the expenditure shares of food and energy prices in the Consumer Price Index (CPI) for the United States.Footnote 5 In particular, we find food price inflation is weighted more heavily and energy price inflation is weighted less heavily. But importantly, we cannot reject the hypothesis that these weights reflect rational behavior in forming expectations about inflation. Our analysis validates concerns raised by policymakers that expectations might not be well anchored with respect to some commodity price shocks, such as those to food prices. As a consequence, policy may need to be calibrated carefully to prevent such shocks from becoming embedded in expected inflation. Several recent papers have considered the response of inflation expectations to commodity price movements and reach conflicting conclusions. Counter to our findings, work by Trehan [2011] suggests that households are more sensitive to both food and energy prices in forming inflation expectations than they are to core measures of inflation that exclude those items. He uses correlation analysis to show that survey measures of inflation expectations are more closely related to inflation in food and energy items than to core inflation. Experimental evidence presented by Georganas et al. [2014] finds that perception of the economy-wide inflation rate is influenced by the frequency with which goods’ prices are observed, consistent with our results for food prices but not energy. A recent paper by Arora et al. [2013] shows that expected inflation responds strongly to (what they term) “explosive deviations” of overall inflation (often driven by energy prices) from core inflation. On the other hand, work by Verbrugge and Higgins [2015] finds energy price shocks are much less important in determining inflation expectations than are other macroeconomic variables, in line with our results and consistent with evidence in Bernanke [2007] that inflation expectations have become better anchored with respect to energy prices in recent decades. All of these papers use non-structural methods in their analyses and none explicitly test whether the observed response reflects rational behavior.Footnote 6 By contrast, we develop a structural model of the economy and use it to test directly whether inflation expectations respond rationally to food and energy price movements. The paper proceeds as follows. We first develop a simple dynamic model of the economy to show how individuals may optimally overweight or underweight food and energy prices in forming expectations about overall inflation. We then present tests of the model’s predictions using survey data on expected price inflation providing support for rational bias in inflation expectations. Finally, we summarize our findings and offer suggestions for further research.",5
44,1,Eastern Economic Journal,09 January 2018,https://link.springer.com/article/10.1057/eej.2014.71,Housing and the Financial Crisis,January 2018,Rachel Bogardus Drew,,,Female,Unknown,Unknown,Female,,
44,1,Eastern Economic Journal,09 January 2018,https://link.springer.com/article/10.1057/eej.2015.17,"Bank Stability, Sovereign Debt and Derivatives",January 2018,Jia Liu,,,,Unknown,Unknown,Mix,,
44,2,Eastern Economic Journal,25 August 2016,https://link.springer.com/article/10.1057/s41302-016-0016-7,"Don’t Do As I Do, Do As I Say? Evidence on the Inter-Generational Transmission of Financial Attitudes",April 2018,Andrew Gill,Radha Bhattacharya,,Male,Female,Unknown,Mix,,
44,2,Eastern Economic Journal,30 August 2016,https://link.springer.com/article/10.1057/s41302-016-0071-0,Will a Decline in Smoking Increase Body Weights? Evidence from Belarus,April 2018,Aliaksandr Amialchuk,Kateryna Bornukova,Mir M. Ali,Male,Female,Male,Mix,,
44,2,Eastern Economic Journal,11 October 2016,https://link.springer.com/article/10.1057/s41302-016-0077-7,The Fortunes of War and Aircraft Manufacturer Stock Returns: The Case of the Korean War,April 2018,Stephen Ciccone,Fred R. Kaen,Huimin Li,Male,Male,Unknown,Male,"Defense industry firms earn profits by manufacturing and selling war material. Defense firm shareholders earn returns through dividends and capital gains. So, how do the prospects for and fortunes of war affect the profitability of defense firms, the returns to their shareholders, and the price of their common stock? These questions can be approached in a number of different but ultimately related ways. One approach is to examine the accounting profitability of the companies and compare it to non-defense industries and to war and non-war years. A second approach is to examine how stock prices of defense firms react to news about important political and military events surrounding and during the years of hostilities. And, a third approach is to examine whether the returns earned by defense firm shareholders during the duration of a war are more or less than expected given the risk level of defense firm equities. We use all three approaches to examine the profitability and financial market performance of aircraft manufacturers during the Korean War. We focus on the aircraft industry during the Korean War because of the industry’s transition from a dire financial condition following World War II to, arguably, a dominant role in the country’s defense plans at the outset of the Cold War. When World War II ended, defense industry expenditures dropped precipitously. Fesler [1947] reports that military procurement dropped from $5 billion in March 1945 to an average monthly rate of $1.1 billion in the fourth quarter of 1945 with the aircraft industry experiencing exceptionally large declines. Monthly airframe weight production fell from 103 million pounds in 1944 to 3 million pounds at the end of 1945 and remained weak through early 1950 despite some increases in production following the Finletter Commission report, Survival in the Air Age [1948], that called for government support of the aircraft firms to ensure that they would exist if needed for a future war. The report noted that “[The] military establishment must be built around the air arm … [and that] … our military security must be based on air power and that we need a much stronger air establishment than we now have.” Subsequently, the outbreak of the Korean War and heating up of the Cold War ushered in a new era of defense spending and defense spending priorities as the need for support of the industry became generally acceptable. As expressed by The Aircraft Industries Association: “The year 1950 will probably go down in aviation as the one in which the industry, greatly aided by the Korean crisis, finally won its three-decade campaign for adequate peace-time air defenses.” [Simonson 1960, p. 379]. According to Simonson, between 1949 and 1953, annual output of military aircraft increased from $1,781,000,000 to $8,511,000,000 and employment increased from 281,000 to 779,100 employees. Furthermore, the U.S. government financed two-thirds of the total $3,528,000,000 needed to produce the aircraft. We consider the following questions. (1) How did the Korean War profitability of aircraft manufacturers compare to the pre-war years of 1947 through 1949/1950 and to the median profitability of the largest manufacturing firms during the war years? (2) How did aircraft manufacturer stock prices react to important military and political events during the war years? (3) How did aircraft manufacturer risk-adjusted stock returns compare to the market returns between 1947 and 1953? We conduct our investigation and interpret our findings within the context of long-standing controversies about the profitability of defense contractors at the expense of others. Such controversies have permeated U.S. history from the American Revolution to the current day as described in “The Historical Context: A Brief Summary of the Controversies” section. We choose the aircraft industry because we believe it represents the purest heavy industry during the Korean War in terms of its relation to wartime manufacturing. Its rapid decline in the post-World War II period meant that it could greatly benefit from another prolonged conflict. The aircraft industry also experienced rapid technological advances. The jet fighter, lightly used only near the end of World War II (and primarily by the Axis), was a major innovation in the Korean War. The wide use of the F-86 Sabre, produced by North American Aviation, was considered instrumental in the competition with Soviet fighter jets for air superiority [Dwyer 1999]. Commercial travel on jets did not begin until 1958, several years after the Korean War [Lombardi 2008]. In contrast to aircraft, the advances in tank and light vehicle technology were not as rapid. The tanks employed were often modified from World War II leftovers. Many new tanks were built at the Detroit Arsenal Tank Plant, which was owned by the U.S. Government and operated by Chrysler [Bos and Talbot 2001]. Most other tank and light vehicle manufacturers tended to be car manufacturers like Ford, GM, and Chrysler, which despite wartime restrictions, were still thought of primarily as civilian-oriented companies [Nelson 2014]. We find that the accounting profits of aircraft manufacturers improved substantially relative to the pre-war years and to the median of the largest 500 U.S. corporations and that their annualized stock returns were greater than the overall market during the war. We also find that aircraft stocks responded favorably to events that implied a continuation of the war and negatively to events that implied a reduction in or a cessation of hostilities. However, based on the Carhart four-factor alpha, the returns were not excessive given the portfolio’s risk level.",1
44,2,Eastern Economic Journal,17 November 2016,https://link.springer.com/article/10.1057/s41302-016-0080-z,The Cleansing Effect of Offshoring in an Analysis of Employment,April 2018,Jooyoun Park,,,Unknown,Unknown,Unknown,Unknown,,
44,2,Eastern Economic Journal,15 November 2016,https://link.springer.com/article/10.1057/s41302-016-0081-y,Prudence and Different Kinds of Prevention,April 2018,Mario Menegatti,,,Male,Unknown,Unknown,Male,"The concept of prevention was introduced for the first time by Ehrlich and Becker [1972], under the term “self protection,” as the effort exerted by an agent in order to reduce the probability of suffering a loss. In the original Ehrlich and Becker framework, this effort was treated as occurring in the same period as when the agent bears the risk of loss. Under this assumption, the action of prevention is contemporaneous with the risk that the agent tries to prevent. For instance, if I want to reduce the probability of contracting an infection I take hygienic measures during the contagion period. Similarly, if I want to reduce the probability of fire, I take care when handling flammable material. This time framework, describing prevention in a one-period model, characterized all papers in the literature for a long period [e.g., Dionne and Eeckhoudt 1985; Briys and Harris 1990; Julien et al. 1999; Chiu and Henry 2005; Eeckhoudt and Gollier 2005; Huang 2012; Huang et al. 2015; Crainich et al. 2016]. In a more recent paper, Menegatti [2009] suggested for the first time that, in many cases, the effort exerted to reduce the probability of suffering a loss is related to a risk which may occur in the future rather than in the present. Menegatti introduced thus the idea that prevention can be made in advance rather than be contemporaneous with the possible loss. For instance, if I want to reduce the probability of an infection I undergo vaccination before the period of contagion starts. Similarly, if I want to reduce the probability of a future fire accident, I can update the fire system now.1 Starting from Menegatti’s paper, this idea, characterized by the analysis of a two-period time framework, has been widely studied in recent years, generating a new strand in the prevention literature [e.g., Menegatti and Rebessi 2011; Eeckhoudt et al. 2012; Menegatti 2012, 2014a; Courbage and Rey 2012; Xue and Cheng 2013; Hofmann and Peter 2016; Courbage et al. 2015; Nocetti 2015; Wang and Li 2015; Wang et al. 2015].2
 Starting from the result that risk aversion does not have a clear effect on prevention [Dionne and Eeckhoudt 1985], the literature studied the role of prudence in determining the optimal level of prevention both in the models examining contemporaneous prevention and in the models analyzing advance prevention. With reference to contemporaneous prevention, Eeckhoudt and Gollier [2005], comparing the optimal choice of a risk averse either prudent or imprudent agent with the choice of a risk neutral agent, obtained results which can be summarized as follows: [Eeckhoudt and Gollier 2005] In case of contemporaneous prevention: If the optimal level for the probability of loss occurrence for the risk neutral agent is larger than 1/2, then a risk averse and prudent agent exerts less effort than the risk neutral agent. If the optimal level for the probability of loss occurrence for the risk neutral agent is smaller than 1/2, then a risk averse and imprudent agent exerts more effort than the risk neutral agent. Similarly, with reference to advance prevention, Menegatti [2009] derived the following conclusions: [Menegatti 2009] In case of advance prevention: If the optimal level for the probability of loss occurrence for the risk neutral agent is larger than 1/2, then a risk averse and prudent agent exerts more effort than the risk neutral agent. If the optimal level for the probability of loss occurrence for the risk neutral agent is smaller than 1/2, then a risk averse and imprudent agent exerts less effort than the risk neutral agent. It is clear that Results 1 and 2 above show opposite findings for contemporaneous and advance prevention.3 In fact, under the same conditions involving the threshold 1/2 for the probability of loss occurrence chosen by the risk neutral agent, Eeckhoudt and Gollier [2005] show that prudence reduces contemporaneous prevention and imprudence increases it, while Menegatti [2009] shows that prudence increases advance prevention and imprudence reduces it. Eeckhoudt and Gollier [2005] and Menegatti [2009], respectively, analyze the effect of prudence/imprudence on contemporaneous prevention and on advance prevention, considering each of the two kinds of prevention alone. The aim of the present work is to analyze how prudence/imprudence affects the optimal levels of advance and contemporaneous prevention in a context where both kinds of prevention are used together. To the best of our knowledge, this analysis is new to the literature. In fact, a recent paper by Hofmann and Peter [2015] is the only work studying a model which introduces contemporaneous and advance prevention together. That work, however, focuses on the optimal choices of the risk neutral agent and of the risk averse agent and does not analyze the effect of prudence/imprudence.4
 Lastly, a recent paper by Denuit et al. [2016] provides a possible decomposition of the effects of contemporaneous prevention in different kinds of trade off. The last part of the present work proposes an application of this analysis to the case where an agent exerts effort both in advance prevention and in contemporaneous prevention. The paper proceeds as follows. Next section presents the model. We then introduce some restrictive conditions which will be alternatively used to derive the results. After this we derive the results and we then discuss their interpretation. Finally, we present a possible decomposition of prevention effects and we then conclude.",3
44,2,Eastern Economic Journal,19 October 2016,https://link.springer.com/article/10.1057/s41302-016-0082-x,A Model of Institutional Complementarities in Ancient China,April 2018,Haiwen Zhou,,,Unknown,Unknown,Unknown,Unknown,,
44,2,Eastern Economic Journal,14 November 2016,https://link.springer.com/article/10.1057/s41302-016-0083-9,Hysteresis in a Three-Equation Model,April 2018,Thomas R. Michl,,,Male,Unknown,Unknown,Male,"To economize on exposition we will stick to the basic assumptions, notation and even some terminology of the widely-used macroeconomics text by Carlin and Soskice [2015] with some exceptions. Variables that are dated will carry a time subscript only when needed for disambiguation. We will suppress the t in representing variables (so \(z_{t-1}\) will be written \(z_{-1}\)). Parameters will have identifying numerical subscripts, and it will generally be clear that these are not time signatures. To further economize on notation, we assume unit labor productivity so that output, y, and employment are identical (aside from units of measure). This makes the real wage, w, equivalent to the wage share. The labor force is assumed to be constant, so that output and unemployment move inversely and we needn’t consider the unemployment rate explicitly at all. The inflation process revolves around bargaining between workers (either individually as in an efficiency wage setting or collectively through trade unions) and firms over the real wage. We will linearize the wage-setting and price-setting curves and use some notation that differs from the C/S text as follows: The innovation here, borrowed from Stockhammer [2008], lies in the time-dependent terms \(B_t\) and \(C_t\), which capture the effects of wage aspirations and mark-up norms. In this paper, we will assume that initially these are normalized to zero, or \(B_0=C_0=0\). We will adopt the convention that the model is in a long-run equilibrium in period 0, so that the values of important variables like output, the real wage, etc. in period 0 can be taken as benchmarks. The question we address is how the model responds to an aggregate demand shock or a change in the inflation target in period 1. The idea is that workers form aspirations about the wage they desire based on their experience in the labor market. If employed workers receive a wage higher or lower than the wage to which they feel in some sense entitled, they revise their aspirations. We take the bargained real wage, \(w^{WS}\), to represent this reference point. If workers receive a real wage that is higher than the wage for which they have bargained and if this gap persists over time, they will revise their wage bargain upward in subsequent periods. (Note that workers are not forming aspirations by comparing their current real wage to past wages.) We provide an example of this scenario below. Similarly, firm managers consult the normal level of the mark-up in formulating their pricing plan. If they experience an actual mark-up that is higher or lower than the current norm, the mark-up will be revised accordingly. In this treatment the mark-up is not simply a reflection of profit-maximizing pricing under conditions of imperfect competition (i.e., an expression of the elasticity of product demand), but also reflects (as post-Keynesians have long argued) social factors such as class struggle, the growth objectives of managers, and normative behavior. Lavoie [2014, Ch. 3] provides extensive discussion of this approach. Because the mark-up that is actually received reflects the prevailing real wage, the firm’s mark-up norms can also be explained in terms of the real wage. If the prevailing real wage is less than the real wage associated with the target mark-up, \(w^{PS}\), the actual mark-up must exceed the target mark-up and if this mismatch persists managers will be inclined to revise upward their mark-up norm in subsequent periods.Footnote 2 We will provide an example of this scenario below as well. The rate of inflation will stabilize when the level of output reaches \(y_e\), which we will call the equilibrium level of output. It is defined by consistency between the demands of workers and firms, or \(w^{WS}=w^{PS}\). Notice that equilibrium output is potentially time-dependent: We will assume that the shift parameters are initialized to be zero so any dynamic process that leads to non-zero values for the shift parameters will change the equilibrium level of output. These equations form the basis for the expectations- or inertia-augmented Phillips curve (take your pick) in which the lagged inflation rate affects current inflation: The slope of the Phillips curve, \(\alpha \), mirrors the parameters of the wage and price setting equations in a transparent way. Here we allow for the possibility that inflation expectations are anchored by the inflation target, \(\pi ^T\), chosen by the central bank, with \(0 \le \chi \le 1\) measuring the extent of anchoring. There is considerable evidence that over the last two decades, as inflation-targeting has become the prevailing form of monetary policy (and perhaps because of this trend), inflation expectations have become increasingly anchored and less responsive to variation in the actual inflation rate; see International Monetary Fund [2013, Ch. 3] and the citations therein. Following Stockhammer 2008 we will assume that when the system operates away from equilibrium output the actual real wage lies somewhere between \(w^{WS}\) and \(w^{PS}\) owing to a stable lag structure. There are lags between when prices adapt to money wages and when money wages adapt to prices. The actual real wage is a weighted average of the two wage targets and obeys where \(0 \le \phi \le 1\) is the weight. One justification for this formalization is that it can make the model consistent with the evidence that real wages are procyclical, even if, for example, the price setting real wage is countercyclical because of the behavior of the mark-up over the cycle. More importantly here, we would like to leave open the possibility that both the workers’ wage aspiration and firms’ mark-up norm can evolve over time. The important point is that when the system operates away from equilibrium, both workers and firms will change their aspirations and norms. We formalize this with the following equations of motion, using the \(\Delta \) operatorFootnote 3 to indicate a first-difference: The hysteresis-generating mechanisms kick in whenever the level of output deviates from the currently prevailing equilibrium and as a result, the equilibrium level of output evolves according to an equation that will play a central role in the dynamics of the system: The parameter \(\theta = \phi \sigma + (1-\phi ) \psi \) is a weighted sum that captures the strength of the two separate hysteresis-generating mechanisms. This parameter is the fulcrum for the hysteresis mechanism in this model. We will restrict \(\theta \) to be strictly less than one for obvious reasons, so that the current equilibrium output level stays between the previous levels of equilibrium and actual output. To visualize the hysteresis-generating mechanisms, consider a temporary negative demand shock (for example, a one-period shift in the intercept term of the IS equation) that lowers output below equilibrium. The system goes from point A on Figure 1 to point B. This will reduce the prevailing real wage depending on the magnitude of \(\phi \), but it will reduce the bargained real wage by more as workers are in a weaker bargaining position. (The actual real wage always lies between \(w^{WS}\) and \(w^{PS}\).) Over time the workers’ wage aspirations will tend to rise since they are receiving a wage in excess of the wage warranted by their (low) bargaining power, shifting the WS curve upward. This increase in wage aspirations occurs despite the fact that workers are receiving a real wage lower than they enjoyed in the past in the initial equilibrium; workers are comparing the actual wage to the bargained wage in forming aspirations. A prolonged slump increases the aspiration factor and the mark-up norm, shifting the WS-PS schedules and reducing the equilibrium level of output The firms’ mark-up norm will also tend to rise, which means that the relevant parameter in the price-setting equation (C) will tend to fall, shifting the PS curve downward. After the shock, the real wage will be lower than the real wage reflecting the mark-up norm so that the prevailing mark-up will exceed the norm as discussed above. This scenario is presented in Figure 1, with the parameter \(c_1=0\) so that the PS curve is flat for simplicity. Taken together these changes in the wage- and price-setting functions will tend to lower the equilibrium level of output, which is the essence of hysteresis. To determine under what conditions the mechanisms generate true hysteresis (i.e., permanent reductions in output and employment) as opposed to persistence (temporary reductions in equilibrium output), we need to incorporate them into a fully specified model. Note that we have not built in any asymmetries, such as worker resistance to reductions in their wage aspirations. Thus, the hysteresis mechanisms work in both directions (raising equilibrium output in a boom), and this is a key to understanding some of the results below. In Figure 1, it is clear that real wages are depressed by the hysteresis mechanisms but the model is flexible enough to accommodate other outcomes for the wage. For example, if the price-setting curve slopes downward (\(c_1<0\)), wages can rise if the price-setting curve shifts by less than the wage-setting curve. In any case, however, the hysteresis mechanisms depress equilibrium output after a negative shock.",9
44,2,Eastern Economic Journal,18 September 2017,https://link.springer.com/article/10.1057/s41302-017-0103-4,How to Solve the Trade Problem: The Generalized Buffett Countertrade Plan*,April 2018,David Colander,,,Male,Unknown,Unknown,Male,"To gain insight into why a border tax might make sense, it is useful to modify the canonical theoretical trade model, which assumes away financial issues, into a model that deals explicitly with them. We can create such a model by hypothesizing an economy in which there is a requirement that agents in the model “be real.” Specifically, all agents exporting or importing in the model are required to meet an additional constraint — they must see that their export or import (calculated in value-added terms) is matched by an offsetting import or export. When agents meet this constraint, the model becomes the equivalent to a barter model, and the offsetting transactions capture the financial feedback on to the real economy, which is being obscured by the canonical barter model. Thus, in what might be called a “realized” (by which I mean “made real”) model of trade, every export or import, however, complex, is institutionally required to be reduced to the equivalent of barter trade at the time it is made.3
 In this “realized” model, financial issues are not assumed away; they are offset by the agents themselves. Thus, there can be no export-led growth, or import-led stagnation because exports and imports are required to be balanced.4 To relate this model to the real world, we have to envision the real world as having a different institutional structure than our current one. Specifically, in this case, it would have an extra market within which agents “realize” the financial effects of their real trades. This market equalizes trade imbalances as economists’ canonical trade model assumes happens. Thus, if exchange rate adjustment fails to equilibrate trade, this market would.",
44,2,Eastern Economic Journal,06 September 2017,https://link.springer.com/article/10.1057/s41302-017-0100-7,On Global Imbalances and International Monetary Policies,April 2018,Pablo Schiaffino,,,Male,Unknown,Unknown,Male,"As an economist living in Argentina, I can take the country as a good of example of free trade and protectionism implemented within a short period of time. Both extreme positions went wrong. During the 1990s, Argentina experienced a free trade policy which increased aggregate productivity and enhanced growth. Growth was not matched with equality. At the begging of the decade, unemployment was around 7% and the Gini coefficient was 0.44. By 2001, unemployment rose to 18% and the Gini coefficient increased to 0.55 (see Gasparini and Cruces [2008]). Given Argentina’s comparative advantages, two significant factors, namely the country’s economic specialization (due to a free trade-friendly policy) and a technology catch-up (due to general economic reforms) led to workers losing their jobs. The story was that the income effect of “specialization” would generate a wealth effect that eventually materialized at the aggregate level. Employment would at first dip but then recover as new sectors emerge similar to England’s Industrial Revolution. The final expansion of the aggregate demand due to the wealth effect never occurred. Why? One possibility, as David clearly states, was that part of the benefits and rents from the period had not been invested but taken abroad to developed countries like the USA. Since the 1960s, Argentina has been restricted with an institutional factor (sometimes called Distributive bid) that blocks the path to convert a portion of profits into long-term investment (see Gerchunoff and Rapetti [2015]). With this institutional restriction binding, arguing for a general policy of free trade does not make sense since the mechanism leading to our desirable conclusion is broken. The final outcome is lower economic activity, higher unemployment and current account deficit. Much more recently (2011–2015), Argentina adopted a more protectionism-driven pattern. Has it worked? No. Some new unusual policies were applied: since exports, as in the past, were in the downward trend again, new measures were implemented to avoid a trade balance deficit. The “new economy” was not a product of economics books. Blackberry was forced to move its production to the local territory in order to keep on doing business in Argentine soil. Similarly, in 2011, the Argentine Government made deals with different automobile companies to settle the “unbalanced trade balance in the sector.” BMW agreed with the authorities that in order to offset its trade balance, the company would export processed rice and automotive components. Other companies in the sector closed similar deals. Nissan exported soybean meal, soybean oil and biodiesel through third parties. Mitsubishi sold balanced feed, peanuts and premium mineral water abroad, while Porsche compensated its imports with exports of wines and olive products. In the majority of the cases, the car sector did not increase exports but rather reached agreements with firms that had already been exporters. In the broad picture, between 2011 and 2015 exports plummeted and the trade balance surplus vanished.1
 The cost for being a more diversified and hermetic economy was lower output and higher price level. Loss of competitiveness arose, affecting the development of economic activity and total exports.",
44,2,Eastern Economic Journal,23 March 2018,https://link.springer.com/article/10.1057/eej.2014.55,Closing the Deficit: How Much Can Later Retirement Help?,April 2018,Lauren Schmitz,,,,Unknown,Unknown,Mix,,
44,2,Eastern Economic Journal,23 March 2018,https://link.springer.com/article/10.1057/eej.2014.57,Austerity: The History of a Dangerous Idea,April 2018,Mark Setterfield,,,Male,Unknown,Unknown,Male,,
44,3,Eastern Economic Journal,18 July 2017,https://link.springer.com/article/10.1057/s41302-017-0097-y,The Body Mass Index Assimilation of US Immigrants: Do Diet and Exercise Contribute?,June 2018,Sukanya Basu,Michael A. Insler,,Unknown,Male,Unknown,Male,"Obesity rates have risen across all industrialized countries in recent decades [OECD Obesity Update 2014]. The rates of obesity in the USA are the highest in the developed world [Food and Agriculture Organization of the United Nations Report 2013].\(^{1}\) Obesity is now considered an epidemic in the USA [Ogden et al. 2007]. There are many dimensions to the economic costs of obesity. First, there is the direct medical spending for diagnosis and treatment of obesity-related health conditions. Obesity is the leading cause of many health problems including diabetes, hypertension, cardiovascular disease, and cancer. About 300,000 preventable deaths can be attributed to obesity-related diseases annually [National Institute of Health 2014]. The estimated annual medical cost of obesity in the USA was $147 billion in 2008 [Finkelstein et al. 2009].\(^2\) In addition to direct medical costs, there are productivity losses from absenteeism and decreased effort at work. Cawley [2004] finds that obese individuals earn lower wages compared to lighter members, and there is heterogeneity by sex and ethnicity. Ricci and Chee [2005] estimate $3.9 billion losses from decreased effort at work of obese workers. Other economic costs of obesity include increased transportation costs of moving overweight individuals. Finally human capital accumulation, in the form of quality and quantity of schooling, can be affected [Hammond and Levine 2010]. In the context of the US obesity epidemic, it is important to study the health of immigrants as their numbers increase and they adopt the American lifestyle.\(^3\) According to the US Census, nearly 15 million new immigrants were added to the US population between 2000 and 2010. Movement across borders can necessitate “acculturation” whereby foreign-born individuals may abandon traditional healthier diets and adopt easily available processed foods in the USA. Additionally the stress of assimilation can affect health behaviors such as exercise and activity. A well-documented facet of immigrant health assimilation, seen in most developed host countries, is the “healthy immigrant effect”: Upon arrival, immigrants are healthier than natives. This health advantage erodes as they continue to live in the host country [Stephen et al. 1994]. Antecol and Bedard [2006] and Park et al. [2009] show that body mass index (BMI)\(^4\) and obesity fit the framework in which immigrants enjoy an entry-level advantage but converge to worse native levels. An unhealthy BMI is an outcome of “energy imbalances.” This involves eating too many calories and not getting enough physical activity [US Surgeon General’s Call to Action to Prevent and Decrease Overweight and Obesity 2001]. In this paper, we use four cross sections of the National Health and Nutrition Examination Survey (NHANES) spanning 2003–2010 to examine food habits and physical activity changes over immigrants’ years of stay. The objective is to assess which dimension can better explain their BMI trends. Using assimilation models, we track both immigrants’ consumption of the major food groups and their proclivity for physical activity at work or leisure over time; for each outcome, we compare immigrants’ levels to natives’. Our regression analyses are stratified by young, middle, and old age groups because diet and activity assimilation patterns could differ due to age-specific metabolic activity and health conditions. The findings of the diet and exercise assimilation models, and their ability to explain immigrants’ BMI assimilation are mixed and vary by age group. A combination of increased fat consumption and reduced physical exercise at work is consistent with BMI assimilation of the middle-aged immigrant group. We also observe increased consumption of fats over the duration of stay in the USA for the oldest age group; however, biological weight gain might be different within this group. For the youngest group of immigrants, we do not see significant dietary assimilation along broad nutrient lines. Given that this group exhibits increased physical activity at leisure which is inconsistent with their apparent BMI assimilation, we posit that closer scrutiny of their consumption of processed and fast food may explain their BMI convergence. In fact, we find significant assimilation in saturated fats for this age group but not other kinds of fats, when calories are held constant, indicating that the distribution of foods may change over time. We also examine the role of income in determining the adoption of “worse” foods. We find that poorer immigrants exhibit greater convergence to unhealthy native eating habits. Home country conditions also affect the diet behaviors of different immigrant groups heterogeneously: Non-Mexican immigrants increase their fat consumption to native levels, unlike Mexican immigrants who traditionally have high-fat content diets. If “gains” in obesity erode the health of immigrants, both privately and publicly funded healthcare costs for the foreign-born population can increase. Immigrant households can suffer from human capital and labor productivity losses. The results in this paper also indicate that policies targeting the health of immigrants or the obesity epidemic in general should consider heterogeneous responses by age, income, and ethnic subgroups.",1
44,3,Eastern Economic Journal,01 November 2017,https://link.springer.com/article/10.1057/s41302-017-0104-3,Lucas and Hume on Monetary Non-neutrality: A Tension between the Logic and the Technique of Economics,June 2018,Simon Bilo,,,Male,Unknown,Unknown,Male,"Besides being a medium of exchange, money is a medium of change. And changes in the quantity of money have real effects on economies in a number of ways. It is perhaps because of this multiplicity of real effects, as Humphrey [1991] and Subrick [2010] argue that the term “non-neutrality of money” is imprecise, which means every discussion of it requires proper qualifications. I focus on a specific meaning that one can trace back to Hume [(1752) 1987a, b, c], among others.1 Hume notes that monetary shock enters the economy at different points and it takes time before initial changes in particular people’s money balances and expenditures reach everyone. The monetary change affects relative prices and redistributes resources in the transition toward final equilibrium. In spite of the theory’s soundness and empirical relevance, academic interest in it significantly declined over the past 50 years, after its prominence during the first half of the twentieth century. But to avoid having to rediscover valid and potentially useful ideas, economics should be able to incorporate older ideas with these properties within new technical frameworks of analysis. The prominence of Hume’s theory before the Second World War can be confirmed by looking at important contributions of the time, such as those by Robertson [(1922) 1961, pp. 74–77], Mises [(1924) 1971], Keynes [1930, pp. 89–94], Robbins [(1934) 1971], Schumpeter [(1934) 1983], and Hayek [(1935) 1967]. A good illustration of the subsequent fading of interest in the theory is how Milton Friedman’s views developed. Initially, Friedman [1961, pp. 461–63] uses Hume’s theory to explain the time lags between monetary-policy actions and changes in output. While Friedman in 1969 [pp. 4–7] still accepts the theory, he abstains from elaborating on it. Finally, in his later works, such as those in 1972 [p. 15] and 1987 [p. 10], he argues that Hume’s theory is not supported by evidence, a point also raised by Chari [1999, p. 6]. Friedman’s declining interest in the theory is not an exception. After all, the important models of the 1960s and 1970s, such as Samuelson and Solow’s [1960] Phillips curve, Friedman’s [1968] money illusion, and Lucas’s [1972] monetary misperception, are all driven by changes in the average level of prices rather than by changes in relative prices and therefore do not rely on insights from Hume’s theory. Only a few outliers in the top professional journals during the 1960s and 1970s belie my observation. Among these outliers, Cagan [1966, pp. 229–30, 1969] and Allais [1974, pp. 311–15] use the theory to explain the liquidity effect. Hayek [1969, pp. 277–82] argues from the theory that changes in the money supply lead to changes in relative prices and have an effect on output. Morgenstern [1972, pp. 1184–85] briefly calls for economists to pay attention to the theory. Wagner [1977] discusses how self-interested policy makers try to change relative prices through monetary policy, and Bordo [1983] and Perlman [1987] revisit Hume in exercises in the history of economic thought. The empirical literature on relative-price dynamics that started to develop in the second half of the 1970s, however, suggests the decline of interest in Hume’s theory was not justifiable. The literature also contradicts Friedman’s [1972; 1987] criticisms of the theory on empirical grounds in at least three ways. First, Bordo [1980] finds that changes in the money supply change relative prices across industries. Fischer et al. [1981] support Bordo’s findings by pointing to the positive relationship between changes in the money supply and relative-price variability, as measured by the variance of relative prices. These findings are consistent with Hume’s theory because if the money supply increases by increasing the balances of some individuals, it only purchases a subset of all goods. Correspondingly, the new money affects the prices of these initially purchased goods relatively sooner, which means relative prices change across industries and overall relative-price variability increases. The second type of supporting evidence relates to the fact that changes in the money supply tend to change the average level of prices. If an increase in the money supply also leads to an increase in relative-price variability, as Hume’s theory predicts, the price level should be positively related to relative-price variability. This relationship has been confirmed by, among others, Vining and Elwertowski [1976], Parks [1978], Fischer et al. [1981], Debelle and Lamont [1997], and Lastrapes [2006]. Third, Hume’s theory is consistent with the rightward skewness of the distribution of shocks to individual prices during inflationary periods, as documented by Vining and Elwertowski [1976] and Ball and Mankiw [1995]. As I already mentioned, the theory starts with the assumption that new money is injected into the economy through the cash balances of some individuals, who likely spend it on a limited number of goods. As a result, it should be only a few prices that push the inflation rate upward, which can then explain the skewness of the distribution of shocks to individual prices. All this evidence, however, has not had a significant effect on the leading academic discussions of monetary non-neutrality. The lack of interest in Hume’s theory has lasted through today. Notable exceptions include exercises in the history of thought by Wennerlind [2005], Paganelli [2006], Berdell [2010, pp. 216–17], and Schabas and Wennerlind [2011, pp. 218–20]. Further, Anthonisen [2010] shows how injections of new money across geographic space change relative prices and real output. And Williamson [2008, 2009] discusses the injections in the context of segmented asset markets and segmented goods markets. Exceptions aside, in popular models, changes in the money supply do not affect relative prices through changes in the distribution of the money supply as in Hume’s theory. Rather, relative prices in these models are affected in two other ways. First, a monetary shock changes relative prices through a version of “time-dependent” staggered pricing decisions [Calvo 1983], as is well illustrated by the recent contributions of Christiano et al. [2005], Atkeson et al. [2010], Engel [2011], and Guimaraes and Sheedy [2011]. The second currently popular way of accounting for monetary non-neutrality and related changes in relative prices is “state-dependent” pricing decisions in menu-cost models such as those in Golosov and Lucas [2007], Gertler and Leahy [2008], Nakamura and Steinsson [2010], and Midrigan [2011]. These time-dependent and state-dependent models, considered from a Humean perspective, do not substantially differ from each other. As I explain below in more detail, money itself is the main source of its own non-neutrality in Hume’s theory. The conclusions follow with only a few additional assumptions. The inherent non-neutrality has roots in Clower’s [1967, p. 5] dictum that only “money buys goods and goods buy money; but goods do not buy goods.” This restriction on which transactions can take place and when they can take place starts with the fact that every piece of a given money supply is at any time possessed by a particular person. Until this person transfers it to someone else, it cannot be used by anyone else in their own transaction. It is this restriction, arising from the assignment of property rights to each piece of the money supply — whether the money is tangible or intangible — that limits where and when each particular piece can be used once it is injected into the monetary system. That limit is what I have in mind when I call money its own source of non-neutrality. This view of non-neutrality as an inherent feature of the money supply contrasts with time-dependent and state-dependent models, which impose special assumptions of non-neutralizing frictions on certain aspects of a world with otherwise-neutral money. As the non-neutrality is inherent to the money itself from the perspective of Hume’s theory, it cannot be arbitraged away and therefore affects market-clearing equilibrium responses to monetary shocks, as I show below in the context of a general-equilibrium framework. Thus, the inherent non-neutrality of money itself and the pattern through which it changes the economy take center stage in Hume’s theory. The main actors in time-dependent and state-dependent models, however, are frictions in particular markets and individuals. The problem with these newer models is that they implicitly restrict scientific inquiry. Although they, like Hume’s theory, describe step-by-step changes in relative prices, the changes are exogenous: the monetary authority cannot cause changes in relative prices, although it does influence the price level. Rather, the given landscape of markets and given knowledge of individuals determine relative-price changes in response to a given monetary shock. The time-dependent and state-dependent models thus restrict analysis in at least three ways. They discourage economists from asking how and why the monetary authority chooses where it injects the new money. They deter economists from investigating how monetary expansion changes relative prices. And they constrain economists from investigating the long-run effects of different types of injections of the new money as Bilo and Wagner [2015] argue. The way contemporary economics has shunted aside Hume’s theory raises concerns about how it fails to incorporate valid old theories into new frameworks, and it raises eyebrows because this is not the first time it happened. Hicks [1967, pp. 156–63] notes that David Ricardo did not incorporate Hume’s theory into his monetary analysis either. Unfortunately, it seems new frameworks are built for purposes that sometimes make them incompatible with old ideas. Such incompatibility, as Krugman [1984, pp. 261–62] and Blanchard [2003, p. 24] suggest, might tempt scholars to reject old ideas for reasons other than those of merit, such as convenience. If nothing else, we should be aware of the possibility of such rejections and understand how they happen. In the following sections, I use Lucas’s discussion of Hume’s theory in his Nobel lecture [Lucas 1996] as a case study of how economists can reject a valid older theory, like Hume’s, when they examine it from the perspective of a more-recent technical framework of economic analysis. In this case, Lucas argues that he rejects Hume’s theory because the theory is incompatible with a general-equilibrium framework. I argue that the theory is in fact consistent with a general-equilibrium style of theorizing with intertemporal substitution and utility maximization, and that Lucas does not reject the theory for his stated reason. Instead, Lucas actually rejects Hume’s theory because his own analysis assumes changes in the money supply are evenly distributed across a group of identical agents who are the only ones spending money in the given period [Lucas 1996, pp. 672–77]. The assumption does not allow us to discuss the differences in patterns in which money enters the economy — differences that are at the core of Hume’s theory — which can explain why Lucas struggles to see the theory as something more than an outdated “disequilibrium dynamics” even though it is not. This, of course, does not mean valid older theories, including Hume’s, cannot be rejected by some economists for other reasons than the difficulty of their translation into more recent frameworks. Even in Ricardo’s case, as Hicks [1967, p. 162] hypothesizes, the straitjacket of the “static equilibrium method” was only one of the reasons, the other being Ricardo’s fear that “crude inflationists” might decide to use Hume’s theory to support ongoing monetary expansion. In looking at one prominent economist’s views on Hume’s theory, my discussion shows that the useful older theory may have been rejected because it was incompatible with a new technical framework. While it is impossible to generalize conclusively about the standing of Hume’s theory within the profession, my finding supports the conjecture that the incompatibility might be at least partly responsible for why the profession dismissed Hume’s theory during the period after the Second World War.",1
44,3,Eastern Economic Journal,15 February 2017,https://link.springer.com/article/10.1057/s41302-017-0092-3,A Monetary Explanation for the Recession of 1797,June 2018,Nicholas A. Curott,Tyler A. Watts,,Male,,Unknown,Mix,,
44,3,Eastern Economic Journal,20 February 2017,https://link.springer.com/article/10.1057/s41302-017-0091-4,Can Price-Level Targeting Reduce Exchange Rate Volatility?,June 2018,Nestor Azcona,,,Male,Unknown,Unknown,Male,"Since New Zealand’s central bank first introduced its inflation target in 1990, many other countries have adopted that monetary policy regime.1 Among other things, inflation targeting is credited with providing transparency to monetary policy decisions and anchoring inflation expectations [Mishkin et al. 2001], which can help reduce inflation volatility. One important caveat of inflation targeting is that, when a central bank uses monetary policy to keep inflation close to its formal target, its ability to use monetary policy to influence the exchange rate is limited. For example, since Brazil, Chile, and Mexico adopted an inflation target, those countries have sterilized their occasional foreign exchange interventions [Schmidt-Hebbel and Werner 2002]. As a result, those operations have been less effective and their exchange rate volatility has been comparable to that of inflation-targeting countries that let their currencies freely float. Although these and other central banks with inflation targets explicitly signal that they are primarily concerned about price stability, they may also be worried about excessive exchange rate volatility.2 Calvo and Reinhart [2002] find that many emerging economies exhibit what they call “fear of floating” and take measures to stabilize their exchange rates, including sporadic foreign exchange interventions, different types of fixed exchange rate regimes, and even abandoning the domestic currency altogether.3 The most obvious cost of excessive exchange rate volatility is uncertainty about future exchange rates, which is detrimental for international trade and finance. Exchange rate volatility may also be problematic in countries where it is common for assets and liabilities to be denominated in different currencies. Fluctuations in the exchange rate may also contribute to increased volatility of output and consumption.4
 Given that the goals of inflation and exchange rate stability are often in conflict, the purpose of this paper is to study whether a price-level targeting regime would be able to achieve a better trade-off between the two than the current inflation-targeting regimes. In other words, can a price-level target help reduce exchange rate volatility without sacrificing the central bank’s goals for inflation and output? Under an inflation-targeting regime the central bank conducts monetary policy with the objective of minimizing deviations of the inflation rate from a chosen inflation target (say, 2 percent annually). Such objective can be attained through changes in a benchmark interest rate, which would influence the aggregate demand and in turn affect the inflation rate. It is possible that a central bank with an inflation target may also take into account other factors, such as the output gap, when conducting monetary policy. One well-known example is Taylor’s [1993] description of US monetary policy in terms of a simple rule that determines the federal funds rate as a function of its natural rate, deviations of inflation from an implicit target of 2 percent, and deviations of output from potential output. Despite the widespread use of inflation targeting, several economists have studied the potential benefits of replacing inflation targets with targets for the price level. Currently, there are no central banks with a price-level target, although there is the historical precedent of Sweden in the 1930s [Berg and Jonung 1999]. Under a price-level target, the central bank’s objective would be to minimize the deviations of the price level (as measured by the CPI or another price index) from a pre-announced target. This target could be adjusted over time so that on average the inflation rate is positive.5 For example, the price-level target could be raised every month by an annualized rate of 2 percent. To understand the difference with respect to an inflation target of 2 percent, consider the different responses of a central bank after inflation exceeds 2 percent (shaded area in Figure 1a) under the two alternative regimes. In both cases the central bank would respond by raising interest rates to contain inflation. Once the inflation rate returns to 2 percent, the central bank with an inflation target (IT) does not need to take any further action: It is not expected to correct past deviations of inflation from its target. By contrast, under a price-level target (PT) the central bank would not be satisfied with a return of inflation back to 2 percent. The temporarily higher-than-2 percent inflation rate has caused a permanent increase of the price level above the desired path. In order to fulfill its goal, the central bank must respond to a period with inflation above 2 percent by causing a period of inflation below 2 percent. Although in both regimes inflation becomes stationary, the price level is only trend-stationary under a price-level target.6 That makes future price levels much more predictable, especially at longer horizons. This reduction in uncertainty would represent a clear benefit, since it would encourage long-term contracts and investments. The problem with price-level targeting, as argued by Fischer [1994], is that returning the price level to its path would require a very aggressive contractionary policy, which would cause additional volatility of inflation and output in the short run. Svensson [1999], however, shows that a price-level target could offer a “free lunch” in the sense that, under certain conditions, it could actually yield less volatility of output and inflation than an inflation target. This implies that a price-level target may be superior even if the public is more concerned about output and inflation stability than about price-level stability. Unlike previous work on price-level targeting, Svensson’s model relies on an endogenous policy rule and rational expectations. Under those assumptions, price-level targeting yields lower inflation volatility if supply shocks are sufficiently persistent. Further work showed that price-level targeting may outperform inflation targeting also under a broader set of scenarios. In a simple model, Cover and Pecorino [2005] show that output persistence is not a necessary condition for the superiority of price-level targeting. The benefits of price-level targeting in New Keynesian models are emphasized by Vestin [2006]. Such models are characterized by forward-looking price-setters. Under that assumption it is easy to see why price-level targeting may be superior: Since higher-than-average inflation today means lower-than-average inflation in the future, the expectation of low prices in the future will moderate the increase in prices today (as in the shaded area in Figure 1b). As a result, the commitment to return to the price-level path in the future reduces the effort needed by the central bank to bring the price level back to its path today. The role that price-level targeting plays in determining inflation expectations is also emphasized by Eggertsson and Woodford [2003]. They demonstrate that a policy that is purely forward-looking is unable to stimulate the economy once the central bank reaches the nominal interest rate’s zero lower bound. But a price-level target, which is history-dependent, can be effective in those situations. A commitment to return to the price-level target should increase inflation expectations and reduce the real interest rate, staving off deflation. Interestingly, Ball et al. [2005], which regard the New Keynesian Phillips curve as implausible, find that price-level targeting is the optimal policy in a sticky-information model. Colletti et al. [2008] extend the study of the benefits of price-level targeting to a two-country open economy model and found that in that case a price-level target may also yield a better trade-off between inflation and output volatility, especially in response to terms-of-trade shocks. Dib et al. [2013] show that price-level targeting may also be better at reducing the distortions created by nominal debt contracts. The conclusion from these studies is that price-level targeting may be superior to inflation targeting in terms of output and inflation volatility under very different assumptions about the structure of the economy and the nature of shocks. Price-level paths after unexpected increase in inflation While most of the literature comparing price-level targeting and inflation targeting has focused on the implications for inflation and output volatility, there is a compelling reason to consider the effects of a price-level target on the exchange rate. The nominal exchange rate depends on the real exchange rate and the price levels in each country. Under a price-level target, the domestic price level and other nominal variables become trend-stationary.7 Therefore, domestic shocks that have no permanent effects on real variables, including the real exchange rate, would not cause any changes in the expected future value of the nominal exchange rate. Given the forward-looking nature of the exchange rate, that might reduce its volatility in the short run as well. While international investors may be primarily concerned about the volatility of the nominal exchange rate, international trade is affected by the real exchange rate. In practice both exchange rates move closely together in the short run, which may be attributed in part to the sluggishness with which prices adjust. Therefore, the reduction in nominal exchange rate volatility may also reduce the volatility of the real exchange rate. This study uses Canada as a benchmark for small open economies with inflation targets. A small-scale dynamic stochastic general equilibrium model of the Canadian economy is estimated and used to compare the response of the nominal and real exchange rates under four alternative monetary policy rules: two Taylor rules that target the inflation rate and two Taylor rules that target the price level. The results show that, when the economy is confronted with supply shocks, price-level targeting rules are able to reduce the volatility of inflation, the nominal exchange rate, and the real exchange rate while maintaining the same volatility of output. In the case of demand shocks price-level targeting also reduces the volatility of inflation, but its effect on nominal and real exchange rate volatility depends on the persistence of the shock. Therefore, a price-level target may or may not reduce exchange rate volatility depending on the characteristics of the shocks in the economy. In the case of Canada, it would have reduced real exchange rate volatility by a modest amount and nominal exchange rate volatility by a significant amount. While certainty about future nominal and real exchange rates is typically considered beneficial for long-term investment decisions, this paper does not delve into the question of whether adopting a price-level target, and its impact on exchange rate volatility, would be welfare-improving for the nation as a whole. Obstfeld et al. [1998] address one way in which exchange rate volatility has a negative effect on welfare. In their model, prices are set in advance and cannot be changed ex-post in response to shocks. That implies that higher exchange rate volatility makes the relative price between home and foreign producers more uncertain. Faced with increased uncertainty for its own goods demand, each producer/household responds by raising prices, which reduces income and utility from consumption but increases their leisure. As a result, world income, consumption, and trade fall. Obstfeld and Rogoff don’t consider the negative effect of exchange rate volatility on international capital flows because, for tractability, their model assumes a balanced current account at all times. Nevertheless, the main welfare gains from adopting a price-level target may come from reducing output and inflation volatility. Therefore, this paper investigates whether a price-level target may reduce exchange rate volatility without sacrificing output and inflation stability. The remainder of the article is organized as follows. First I describe the small open economy model and discuss the estimation of the model’s parameters using Canadian data. Then I compare the behavior of output, inflation, and the nominal and real exchange rates under the actual inflation-targeting regime implemented by the Bank of Canada and under three alternative Taylor rules, including two price-level targeting rules. Finally, I summarize the results in the “Concluding Remarks” section.",1
44,3,Eastern Economic Journal,26 March 2018,https://link.springer.com/article/10.1057/s41302-018-0107-8,Tort Reforms and the Gender Distribution of Physicians,June 2018,Hsueh-Hsiang Li,Alexandra Bernasek,,Unknown,Female,Unknown,Female,"There is a large body of literature on the effects of malpractice risk on the labor supply decisions of physicians [Helland and Scholwalter 2009] and on the effects of tort reforms at the state level that reduce malpractice risk on the supply of physician services [Kessler et al. 2005]. However, none of these studies examine possible gender differences in responses to the legislative changes. Extensive empirical evidence has shown that men and women differ in their responses to pecuniary incentives, career mobility, occupational choices, and attitudes toward risk. All of these may lead us to expect gender differences in response to differences in malpractice risk across states. This paper seeks to fill a gap in the literature by taking advantage of what is effectively a natural experiment — exogenously driven changes in tort laws across states — to explore how differences in the occupational risks physicians face across states may affect the gender distribution of physicians across states. A priori it is difficult to predict what the nature of the gender difference may be, if there is one, making this an interesting empirical question. A crucial assumption that underlies the literature on the relationship between tort reforms and physician supply is that physicians are risk averse. This leads to the hypothesis that physician supply will respond positively to tort reforms. A number of studies have sought to empirically test that hypothesis and have found mixed results (discussed in details in “Literature review and motivation” section). The motivation for this paper came initially from the findings from another literature on gender and risk aversion. There is extensive empirical evidence that women are more risk averse than men in a variety of economic activities [Hersch and Kip Viscusi 1990; Powell and Ansic 1997; Jianakoplos and Bernasek 1998; Schubert et al. 1999; Bajtelsmit et al. 1999; Sunden and Surette 1998; Bernasek and Shwiff 2001; DeLeire and Levy 2004; Croson and Gneezy 2009]. In a related literature on the willingness to trade off wages for occupational risks among risk-averse workers, there is also empirical evident that finds women to have less tolerance for those risks [Rosen 1986; Hersch 1998; Bonin et al. 2007]. Connecting these empirical findings across the various literatures leads us to hypothesize that, all other things equal, if women are less risk tolerant than men, we would expect to observe a higher representation of female physicians in reform state since tort reforms reduce the legal exposure to malpractice risks for physicians. Meanwhile, accompanying the reduced legal risks, earnings might be expected to decrease due to reduced risk premiums when tort reforms are passed. Because men are more responsive to pecuniary incentives than women [Zafar 2013; Wiswall and Zafar 2015], we might also expect to observe stronger responses to the changes in wages by male physicians. Once again all other things equal, this may lead male physicians to be more likely to move out of reform states into non-reform states, with female physicians moving in the opposite direction. It is established that career mobility is stronger among men than among women [Fitzenberger and Kunze 2005] and that the career mobility accounts for a large fraction of earnings growth [Topel and Ward 1992]. As a result, we might observe an outflow of male physicians from reform states to non-reform states for the higher wage premiums for risks and a muted migration pattern among female physicians. On the other hand, the marital status of workers may affect interstate mobility because migration for a dual-career household is more costly than migration for an individual. Given that, compared with female physicians, a higher fraction of male physicians are married [Hinze 2004], we might expect female physicians to be more mobile than male physicians. Overall, for a variety of reasons it seems reasonable to hypothesize there will be gender differences in the supply response of physicians in response to tort reforms. It also seems reasonable to hypothesize a relative increase in female physician representation in the reform states compared with the non-reform states. Given, however, the multiple dimensions along which gender differences may come into play, the gender distribution of physicians across states in response to tort reform is an empirical question. In interpreting our results we are mindful that any explanation for observed gender differences is suggestive rather than deterministic. This paper contributes to the literature in several respects. First, we decompose the impact of major tort reforms on physician labor supply by gender and age group that links to the broader literature on physician labor supply. Also, we contribute to the literature on gendered labor outcomes by investigating whether male and female physicians respond differently to these major tort reforms. We find that a cap on non-economic damages increases female representation among young physicians by 1.297 percentage points (or 3.2 percent) in reform states, but there is no effect on physicians 35 years old or older. The change in the gender composition among young physicians across reform versus non-reform states is primarily driven by male physicians entering the non-reform states. The paper proceeds as follows: section “Literature review and motivation” provides a review of the literature, section “Data and empirical specification” describes the data and empirical specifications, section “Results” analyzes the results and checks the robustness of the results, section “Dynamic effects” presents the dynamic effect of the tort reform, section “Falsification test” presents the results of a falsification test, section “Discussion” discusses our findings, and section “Conclusions” concludes.",
44,3,Eastern Economic Journal,13 March 2017,https://link.springer.com/article/10.1057/s41302-017-0093-2,The High Costs of Large Enrollment Classes: Can Cooperative Learning Help?,June 2018,Tisha L. N. Emerson,Linda K. English,KimMarie McGoldrick,Female,Female,Unknown,Female,"In recent decades, colleges and universities have responded to tightened budget constraints and the increased challenges of allocating scarce faculty time across a diverse range of research, teaching, and administrative responsibilities by increasing class size. The field of economics is no exception to this trend. In 1995, for example, average class size for principles courses taught at research universities was 162, but by 2001 the average size had increased to 185 students. Colleges granting only associate degrees saw even larger percentage increases as the average principles class size increased from 30 to 45 over this same period [Becker and Watts 2001, p. 449]. While more recent studies suggest that the increases in class size have leveled off to some extent in recent years [Watts and Schaur 2011], relatively large class sizes in principles courses remain an important strategy for many colleges and universities in managing enrollment fluctuations. Although large enrollment courses potentially reduce costs to higher education providers and – in some cases – may enable faculty to specialize in either research or teaching activities, academics have voiced many concerns related to large enrollment classes. Concerns include the effects that larger classes have on the choice of teaching methods and their efficacy, perceptions and behavioral responses of students enrolled in large classes, difficulties associated with administration and logistics in large enrollment classes, and students’ evaluations of instructors of large classes. While such concerns have led instructors in other disciplines to incorporate techniques designed to facilitate student engagement, learning, and information retention into their pedagogical approaches [e.g., Cooper 1995; MacGregor 2000; Deslauriers et al. 2011; Ferreri and O’Connor 2013], most instructors of large introductory economics courses continue to primarily (or exclusively) utilize lecture-based instruction. Watts and Schaur [2011] report that – across institutional type – the median percentage of class time spent lecturing remained at 83 percent from 1995 through 2010 with few instructors at any type of institution making use of active teaching and learning methods. This fact is unsurprising given perceptions among economics instructors about the high cost of implementing active learning approaches and uncertainty about the success of such approaches in the economics classroom [e.g., Becker and Watts 2001; Salemi 2002; Herbert et al. 2003].1
 To advance our understanding of the effectiveness of active learning approaches in large enrollment courses, the current study uses a quasi-experimental research design to examine whether cooperative learning techniques can mitigate some of the negative consequences of increased class sizes. Using data from both small and large enrollment principles of microeconomics courses (taught by the same instructor using identical class materials over the course of several semesters), we examine differences in student learning and perceptions about the course across sections that varied only in the usage of cooperative learning. We find that while large enrollment sections attain lower levels of achievement (as measured by course score) than those with smaller enrollments, this effect is at least partially mitigated by the cooperative learning treatment. Overall, students in large enrollment sections report lower average levels of satisfaction and learning than students enrolled in smaller-sized classes. Comparing only the (large and small enrollment) sections engaged in cooperative learning activities reveals that the use of cooperative learning activities eliminates the negative effects of increased class size on student perceptions. The following section provides background information regarding the possible problems associated with large class size, as well as the potential for cooperative learning to moderate the problems experienced in large enrollment class settings; this is followed by an outline of the data and methodological framework; we present the results from the empirical analysis and offer some concluding thoughts in the remainder of the paper.",5
44,3,Eastern Economic Journal,18 July 2017,https://link.springer.com/article/10.1057/s41302-017-0099-9,Paid Parental Leave and Female Faculty Retention,June 2018,Nicholas G. Rupp,Lester A. Zeager,,Male,Male,Unknown,Male,"In a study of parental leave policies in 21 high-income countries, Ray et al. [2010, p. 204] find that all the countries provide protected job leave for mothers, but “Australia and the United States guarantee mothers no paid time off after childbirth.” Seven countries (Germany, Sweden, Norway, Greece, Finland, Canada, and Japan) provide 6 months or more of full-time equivalent paid leave for mothers and the remaining 16 countries provide 11 weeks or more.1 Ray et al. [2009, p. 1] report that in the U.S., While about 60 percent of workers are eligible for FMLA-related leave [small employers and short-tenure workers are excluded], only about one-fourth of U.S. employers offer fully paid ‘maternity-related leave’ of any duration, and one-fifth of U.S. employers offer no maternity-related leave of any kind, paid or unpaid. Ray et al. [2010, p. 210] note that the absence of paid leave for new mothers is rare globally, A 2005 International Labor Organization report on 166 countries found that 97% provided some payment to women on maternity leave. Only five exceptions were found: Australia and the United States — and Lesotho, Papua New Guinea and Swaziland. Yet some US employers provide this benefit, at least to certain workers. Many universities provide paid parental leave for faculty. Drago and Davis [2009, p. 2] find that among Big-10 universities,2 “The modal or typical university provides around six weeks of pay during new parental leave.” Such policies are also adopted by private-sector employers. The Working Mother [2012] “100 Best Companies” average 28,378 workers, with over half (51.3%) of them women. One-fourth of these companies offer 10 or more weeks of fully paid maternity leave, and the average for all 100 companies is 7.24 weeks. Netflix announced in 2015 that it would offer up to one year of paid maternity leave for mothers or fathers during the first year after a birth or adoption of a child.3 Still, according to the National Compensation Survey, only 13 percent of full-time workers had access to paid family leave in 2012 [Van Giezen 2013, p. 2], though this figure was up from 2 percent in 1992–93. The US labor force has undergone a massive demographic change since 1950 due to a steady increase in the labor force participation rate of females, rising from 33.9 percent in 1950 to 60.2 percent in 2000. For women ages 25–34, the increase was even greater, from 34 percent in 1950 to 76.3 percent in 2000 [Toossi 2002, p. 22]. Over the same period, technological advances substantially increased the demand for skilled labor and rising education levels permitted women to move into skilled occupations. By 2000, women had attained parity with men in terms of high school completion rates and had almost closed the gap at higher levels of education [Bauman and Graf 2003, p. 5]. In the competition to recruit women from the new talent pool, employers offered new benefits to assuage their concerns — flexible schedules for pregnancy, childbirth or family illness — and to improve female retention [Waldfogel et al. 1999; Oyer 2008]. Providing paid leave has benefits and costs for an employer. To operate efficiently, the employer will weigh the benefits against the costs to find the optimal amount of paid leave. To motivate the problem, we consider a public employer, East Carolina University (ECU), the third largest institution in the University of North Carolina (UNC) system, which experienced severe budget cuts by the North Carolina legislature after the Great Recession of 2007–2008. These cuts gave urgency to operating efficiently, and ECU decided to reduce its paid parental leave length from 15 to 12 weeks in July 2011 over the objections of faculty, especially females. A large public university has advantages for studying the effects of paid parental leave. Universities employ a highly diverse faculty. Some academic fields offer the possibility of generating substantial revenue streams from clinical practice or other sources, which must be forgone during the parental leave, while others offer few of these opportunities. Nonacademic employment opportunities also differ by field; hence, turnover rates naturally vary across fields. Universities often fill faculty lines at the entry level, where the applicant pool typically includes women in the childbearing years. Our analysis focuses on these women, because parental leave can be especially important to them. Finally, a public university permits greater access to data for calibrating the key parameters. In the calibration exercise, we explore the cost effectiveness of paid leave at one university and the impact of the reduction in paid leave on young female faculty retention. More than two decades ago, Brown and Woodbury [1995, p. 1] noted that, “Over the last 15 to 20 years, colleges and universities have paid increasing attention to attracting and retaining faculty women.” In this paper, we explore the role of paid parental leave in female faculty retention. Economists have long recognized the importance of managing labor turnover. Douglas [1918] was among the first to consider the problem. The need to control labor turnover is one possible explanation for the cyclical patterns of unemployment [Yellen 1984]. Mincer [1988] uses the human capital literature to illuminate the forces influencing labor turnover.4 Evan and MacPherson [1996] account for differences in the turnover rates for large and small firms using the characteristics of pension benefits, demonstrating that fringe benefits as well as wages affect turnover rates. Erosa et al. [2002] attribute the persistent gender differences in US labor turnover rates to fertility decisions, which are related to the parental leave issue.5
 Second section specifies the problem for an employer determining the optimal amount of paid leave to offer employees. Third section explores the implications for the characteristics of workers who receive paid leave and the optimal length of leave. Fourth section illustrates the analysis with a calibration exercise using aggregate data from ECU. Fifth section summarizes our findings.",3
44,3,Eastern Economic Journal,06 June 2018,https://link.springer.com/article/10.1057/eej.2015.28,Rational Econometric Man,June 2018,Richard Fowles,,,Male,Unknown,Unknown,Male,,
44,3,Eastern Economic Journal,12 April 2018,https://link.springer.com/article/10.1057/s41302-018-0108-7,On the Irrelevance of Formal General Equilibrium Analysis,June 2018,Dave Colander,,,Male,Unknown,Unknown,Male,"General equilibrium theory is impressive, but, for an applied economist such as myself, general equilibrium proofs are largely irrelevant. They are wonderful exercises in logic, but they have little to do with the real world. The complexity vision of the economy sees all aspects of the economy as continually evolving. The policy focus is on the actual economy, not a hypothetical one. The actual economy does not need a proof of existence; it can be assumed to exist with deeper questions of existence left to philosophers. Moreover, a proof related to a particular institutional structure is of little use, since the economy is assumed constantly evolving and adjusting on many levels. If the conditions for equilibrium, stability and optimality currently do not exist, forces are set in motion that change those conditions. If perfect competition is unstable, institutions will adjust into a system that is more stable. If fully rational individuals are incompatible with existence, the nature of rationality and tastes will evolve, so that it is. If the equilibrium in the economy is perceived by most agents in the model as horrendous, institutions will evolve as agents attempt to head toward a perceived better equilibrium. Western capitalism does not depend on general equilibrium theory. It would still exist if the theory is or is not provable, or even if general equilibrium theory had never been developed. An applied economist’s role is to understand that evolution and to make suggestions about how changing agent’s actions might make it work a little better. In summary, when one envisions the economy as a complex evolving system, as I do, general equilibrium proofs just are not very important, because having a formal theory is not very important.",2
44,3,Eastern Economic Journal,17 May 2018,https://link.springer.com/article/10.1057/s41302-018-0109-6,Comments on Dave Colander’s “On the Irrelevance of Formal General Equilibrium Analysis”,June 2018,Ross M. Starr,,,Male,Unknown,Unknown,Male,,
44,4,Eastern Economic Journal,06 September 2017,https://link.springer.com/article/10.1057/s41302-017-0101-6,"Income, Education, and Three Dimensions of Religiosity in the USA",September 2018,Imam Alam,Shahina Amin,Ken McCormick,Unknown,Unknown,Male,Male,"Economic development is often associated with a decline in the influence of religion. Development brings formal education, the application of science to industry and agriculture, and other trappings of modern societies; these inculcate cause-and-effect thinking that erodes belief in supernatural forces. As our understanding of how the world works advances, there is less space for acts of God. Thinkers as diverse as David Hume, Auguste Comte, Herbert Spencer, Emile Durkheim, Karl Marx, Sigmund Freud, Thorstein Veblen, and Max Weber have all made this point. All of these figures expected that economic development would weaken the hold of religion. The issue is important. As Alfred Marshall famously wrote, “the two great forming agencies of the world’s history have been the religious and the economic” [1950, p. 1]. If the advance of the economic is associated with the retreat of the religious, then social evolution will proceed differently than if there is no such association. We investigate the relationship of income and education to three different dimensions of religiosity in the USA, holding a variety of other socioeconomic factors constant. The three different dimensions of religiosity are: attending religious services, participating in religious practices, and religious education activities. Our contribution is threefold: (1) We examine three different dimensions of religiosity; (2) we use the American Time Use Survey (ATUS) and the Current Population Survey (CPS) from 2007 to 2012. As we explain in the data section below, these data are much more accurate than conventional survey data; (3) we employ a two-part model. The first part is a probit model and includes everyone in the survey. It tells us how socioeconomic factors relate to the likelihood of engaging in religious activities. The second part of the model is an OLS regression that includes only those who have spent time on religious activities. This group is a subset of the whole sample. The OLS regression tells us how socioeconomic factors relate to the amount of time religious people spend on religious activities; the amount of time spent can be considered a measure of the intensity of religiosity. To our knowledge, no one has used the ATUS or a two-part model to investigate the relationship of income and education to these dimensions of religiosity in the USA. We find some evidence that rising income is associated with a reduction in all dimensions of religiosity. People are somewhat less likely to be religious as income increases. In addition, religious people spend less time performing all religious activities as their incomes rise. With respect to education, however, our results are decidedly mixed. People with an associate’s, bachelor’s, or master’s degree are more likely to be religious than people with no high school degree. But among religious people, additional education is associated with less time devoted to religious activities. We were also able to investigate the relationships of other variables to religiosity. The complete results are reported below, but two things stand out: (1) We find that while women are more likely to be religious than men, religious women do not spend more time on religious activities than religious men. (2) We find that immigrants are more likely to be religious than natives, but among religious people, there is no significant difference in time spent on religious activities between immigrants and the native born. Azzi and Ehrenberg [1975] claim to have made “the first systematic attempt by economists to analyze the determinants of religiosity” [p. 27]. Azzi and Ehrenburg extend Becker’s [1965] analysis of the allocation of household members’ time to include participation in religious activities. They investigate the relationship between income and both church membership and church attendance. They hypothesize two different motives for religious activity. One is the “salvation motive”; the second is for “purely social reasons.” Using a multi-period utility-maximizing model of household behavior and a data set that is “notoriously poor in quality” [p. 39], they find income has a negative relationship to church membership rates while church attendance first increases and then decreases as family income rises. Since Azzi and Ehrenberg, a series of papers on the topic have been published in economics [Iannoccone 1998]. These papers investigate how economic factors influence religiosity. Many of them are additional empirical tests of Azzi and Ehrenberg’s model. Long and Settle [1977] use data from a survey of Wisconsin heads of households. They do not find any significant relationship between frequency of church attendance and non-wage income, and only weak support for a relationship between wage income and church attendance. Ehrenberg [1977] uses data from a survey of Jewish people. He finds that as the husband’s wage increases, synagogue attendance decreases. But he finds that as total family income increases, both spouses’ participation in Jewish clubs increase, as does the probability of the family being a synagogue member. Ulbrich and Wallace [1983] find that total family income, wages, and education are not significantly related to church attendance. Neuman [1986] uses data on 700 male workers in Israel. She finds that religious activities decline with the wage rate. She also finds that years of schooling are negatively related to religiosity. Branas-Garza and Neuman [2004] look at the relationship between schooling and religiosity for Spanish Catholics. They find that education has a “marginal positive effect” [p. 18] on religiosity. Aleksynka and Chiswick [2013] use the European Social Survey to compare the religiosity of immigrants to Europe with that of the native born. They find that religiosity decreases as years of formal schooling rise, but only up to a point. After that, religiosity rises with more schooling. They also find that religiosity is negatively related to household income. There is another thread of research that uses macro-data to investigate religiosity. Barro and McCleary [2003] test the secularization hypothesis using a variety of data sources. The secularization hypothesis suggests that rising income and more education will reduce the influence of religion on people’s lives. They examine 61 countries and measure religiosity in seven different ways. In six of the seven regressions, per capita GDP is significant and has a negative effect on religiosity. [See also McCleary and Barro 2006]. Lipford and Tollison [2003] use state-level US data to explore the relationship between income and religious participation. They find a negative relationship between income and religiosity. In summary, studies that use individual-level data generally find mixed results regarding the relationship of religiosity to income and education. In macro-level studies, higher income is associated with less religious participation. There is a need for both more work and better data. Our paper generally follows Azzi and Ehrenberg’s time allocation approach. We give special attention to the relationship of income and education to time devoted to religious activities. One important contribution of our paper to the literature is that we use actual time spent on religious activities in the USA using the American Time Use Survey. Ehrenberg [1977] notes that the theory is about the use of time allocated to religious activities, but most studies (except for Neuman [1986]) use some variation of church attendance or intensity of religious activity as the measure of religiosity.1 The problem is that these variables “may not be perfectly correlated with total time devoted to religious activity as it ignores such dimensions as prayer and other religious activity in the home, volunteer work for religious organizations, and membership in, or time devoted to, other church related organizations, including social clubs” [Ehrenberg 1977, p. 416]. As will be seen later, our measure of religiosity includes all of the activities mentioned by Ehrenberg [1977] and counts them in minutes. Thus, we have a more accurate measure of religiosity that to our knowledge has not been used before. We assume that individuals derive satisfaction both from the consumption of goods and from practicing religious activities.2 We also assume that consumption goods and religious activities are normal goods. An individual’s preferences can be denoted by the utility function: where C represents consumption goods and R represents religious activities. We assume that C is a function of a non-religious composite good (x) and the time allocated to its consumption (h). We assume that C is continuously differentiable and concave. C can be expressed as: 
R gives the “consumption value” of religious participation. Like Azzi and Ehrenberg, we assume R is continuous and an increasing, concave function of time (r) devoted to religious activities. where r = r
1 + r
2 + r
3. Thus, r is the sum of time spent attending religious services (r
1), participating in religious practices (r
2), and on religious education (r
3). Equations (1)–(3) comprise a well-defined maximization problem—maximize U with respect to the decision variables x, h, and r. subject to (1) P

x

*x = V + wl (budget constraint) and to (2) T = h + r + l (time constraint), where h, r, l ≥ 0 
V is non-labor income, w is the wage rate, l is the number of hours of work, P

x
 is the price of the consumption good, and T is the 24 h available in a day. Following the literature, we assume that V, w, P

x
, and T are defined exogenously. The maximization problem allows us to write the Lagrangian function: Assuming that an interior solution exists, the first-order conditions for the constrained optimum yield a system of simultaneous equations. The solution to the system gives the optimum values of x, h, and r in terms of the exogenous variables. Because T is exogenous, finding h and r also determines l, the number of hours worked. The optimality conditions enable us to see how religious participation changes as the exogenous variables change and thereby establish some testable hypotheses. In general, the time constraint means there is a trade-off between time spent on religious activities and time spent on other activities. Our two main variables of interest are income and education. With respect to income, we can predict how changes in non-labor income or an individual’s wage rate affect time devoted to religious activities. By assumption, participation in religious activities is a normal good, so an increase in non-labor income (V) will lead to an increase in religious participation, that is, We cannot make an unambiguous prediction regarding the sign of (δr/δw) because there are two opposing forces at work. A change in the wage rate has both an income-compensated own substitution effect (δr/δw)* and an income effect. Hence, In Eq. (7), the second term is positive; as income increases the demand for all normal goods increases, meaning it increases the demand for religious activities. But the first term is negative. A wage increase makes time spent on religious activities more expensive. The substitution effect of a wage increase thus reduces the demand for religious activities and increases hours of work. Due to these two opposing forces, the net effect of a change in the wage rate (δr/δw) cannot be predicted a priori. The secularization hypothesis implies that the marginal utility of religious activity will decline as education increases. Yet as Glaser and Sacerdote [2008] note, the returns to education increase with networks and other forms of social capital. In that case the marginal utility of religious group activities should rise as education increases. Hence, the effect of additional education on religiosity cannot be predicted a priori. We use the same control variables other studies have used. These include gender, age, marital status, employment status, immigration status, race, household size, the presence of children under 18 in the household, and region of the country.",1
44,4,Eastern Economic Journal,12 April 2017,https://link.springer.com/article/10.1057/s41302-016-0087-5,"Culture, Institutions, and Firm Performance",September 2018,Hasan A. Faruq,Margaret L. Weidner,,Male,Female,Unknown,Mix,,
44,4,Eastern Economic Journal,06 July 2018,https://link.springer.com/article/10.1057/s41302-018-0110-0,The Effect of Labor-Management Complementarities on Production and Efficiency When Management Is Paid but Labor Is Not Paid,September 2018,Kelly E. Carter,,,,Unknown,Unknown,Mix,,
44,4,Eastern Economic Journal,09 December 2017,https://link.springer.com/article/10.1057/s41302-016-0086-6,Do Unions Increase Labor Shares? Evidence from US Industry-Level Data,September 2018,Andrew T. Young,Hernando Zuleta,,Male,Male,Unknown,Male,"Economists no longer embrace this simplistic view of labor unions. At least since Olson [1971], characterizations of collective bargaining have been more sophisticated than as mere class struggle. And rather than simply aiming towards “as large a share as possible,” economists take seriously unions’ potential role in enhancing productivity [Freeman and Medoff 1979, 1984]. By providing agency services, unions may increase both the supply of and demand for firm-specific human capital [Malcomsen 1983; Faith and Reid 1987; Skovsgaard and Sena 2005]. Still, unions may play an important role in the distribution of income across productive factors. And changes in the factorial income distribution have been of increasing concern to economists and policymakers in recent decades. In particular, labor share of income has been trending downward in OECD countries [OECD 2012] and in the US in particular [Elsby et al. 2013]. Piketty [2014] has drawn considerable attention to the decline in labor share, arguing that it may be inherent to capitalist economies.1 Other researchers have offered complementary (though less sweeping) explanations for falling labor shares, including decreases in labor bargaining power associated with globalization and decreased unionization. Regarding globalization, examples include Harrison [2005], Guscina [2006], and Schneider [2011]. There are several ways in which unions may affect labor share. First and foremost, they may affect labor share through collective bargaining activities aimed at increasing wage rates. Alternatively, “higher union wages [may] combine with substantially lower employment and thus to a lower rather than a higher labor’s share” [Macpherson 1990, p. 143]. Furthermore, union workers may receive higher wages at the expense either of profits or of non-union wages. In the former case, one expects a higher labor share; in the latter case, an unchanged or even decreased labor share. Furthermore, by providing agency services and encouraging the accumulation of firm-specific human capital, unions may yield productivity gains. Labor share may reflect these productivity gains in the form of higher wages and/or increased employment.2
 The relationship between unions and labor share will also depend on the substitutability between labor and capital. When unions put upward pressure on wages, firms may be able to easily substitute towards more capital-intensive production. Alternatively, labor demand may be relatively inelastic if substitution possibilities are poor. The elasticity of substitution may be a key parameter determining both the size and sign of the union–labor share relationship. We examine this relationship using panel data from 35 US industries over the 1987–2011 period. We report that union membership is positively and significantly related to labor shares. Starting from the average labor share in our sample, an increase in union membership by 1 standard deviation is associated with a 6–12 percentage point higher labor share. However, we also report that the effect is smaller (and perhaps zero) for manufacturing industries in particular.",12
44,4,Eastern Economic Journal,18 July 2017,https://link.springer.com/article/10.1057/s41302-017-0098-x,Navigating Disaster: An Empirical Study of Federal Assistance Following Hurricane Sandy,September 2018,Laura E. Grube,Rosemarie Fike,Virgil Henry Storr,Female,Female,Male,Mix,,
44,4,Eastern Economic Journal,04 April 2018,https://link.springer.com/article/10.1057/s41302-018-0106-9,Are Remittances Good for Your Health? Remittances and Nepal’s National Healthcare Policy,September 2018,Brian Chezum,Cynthia Bansak,Animesh Giri,Male,Female,Unknown,Mix,,
44,4,Eastern Economic Journal,01 August 2018,https://link.springer.com/article/10.1057/s41302-018-0111-z,Using Network Centrality to Inform Our View of Nobel Economists,September 2018,John H. Huston,Roger W. Spencer,,Male,Male,Unknown,Male,"There is a long literature attempting to measure and rank the significance of individual economist’s contributions. That is a worthwhile endeavor, as knowing which economists have the most import or influence exposes which ideas have spread to other fields, have affected policy or have been most fertile in leading to further breakthroughs. As in ranking journals or institutions, developing systematic ways of ranking economists allows comparisons with more complete data and a more transparent set of criteria. The vast majority of the ranking literature examines the number of articles written by an author or the number of citations those articles received. That is an appropriate approach since the primary way economists influence each other is through a network of articles and citations. The limitation of this methodology is that it only captures one channel of import; the influence an economist has on other economists. A broader gauge of significance must also measure an economist’s legacy in affecting public dialog concerning important issues which, in turn, can affect policy. The network connecting economists and society has historically been more difficult to identify, much less quantify, and thus has been omitted from analysis. With the rise of the internet, there is the opportunity to observe and evaluate the effect of individual economists on the world beyond economics. Researchers are beginning to use internet metrics to provide alternative assessments of import. This paper is unique in combining analyses of both the traditional network of papers and citations, and the broader network of internet links to measure individual economist influence. Our original interest in this topic came from comparing the contributions of Nobel Prize-winning economists, and we have stayed with that sample because they are an ideal group to test this new methodology: They span a wide range of economic ideas, they cover a long time period, and being familiar to most economists, they are of great interest. This paper applies the concept of network centrality to ranking those Nobel Economists. Centrality is a measure of the importance of nodes in a network. Nodes that are more “eigenvalue central” have more links and more important links to them. This concept has important applications to multiple types of networks including social, computer, transportation and biological networks. One prominent example is the PageRank algorithm which creates a centrality-based ranking of internet websites and is the basis of the Google search engine. There are two types of networks featuring Nobel Laureates. One is the internet which includes web pages about Nobel Prize winners. Those Laureates whose webpages link to more sites and to more important sites are more influential. Algorithms similar to PageRank are used to measure the centrality of those web pages. In addition, Laureates are linked to other economists through their scholarly articles. Centrality in that network means that those with more articles and more heavily cited articles are more influential. We combine measures of centrality in these two very different networks to arrive at an overall ranking of 76 Nobel Prize-winning economists.",2
45,1,Eastern Economic Journal,10 October 2018,https://link.springer.com/article/10.1057/s41302-018-0125-6,Introduction to the Symposium on the Extended Effects of the Affordable Care Act,January 2019,Reagan Baughman,Dhaval Dave,Angela Dills,,Unknown,Female,Mix,,
45,1,Eastern Economic Journal,01 October 2018,https://link.springer.com/article/10.1057/s41302-018-0119-4,Effects of the Affordable Care Act on Health Behaviors After 3 Years,January 2019,Charles Courtemanche,James Marton,Daniela Zapata,Male,Male,Female,Mix,,
45,1,Eastern Economic Journal,05 October 2018,https://link.springer.com/article/10.1057/s41302-018-0124-7,The Impact of Medicaid Expansion on Household Consumption,January 2019,Helen Levy,Thomas Buchmueller,Sayeh Nikpay,Female,Male,Unknown,Mix,,
45,1,Eastern Economic Journal,12 October 2018,https://link.springer.com/article/10.1057/s41302-018-0123-8,Impact of the ACA’s Dependent Coverage Mandate on Health Insurance and Labor Market Outcomes Among Young Adults: Evidence from Regression Discontinuity Design,January 2019,Barış K. Yörük,Linna Xu,,Male,Unknown,Unknown,Male,"In the USA, the uninsurance rate among young adults aged 19–25 remained as high as 33.9% in 2010, compared to only 7.8% among children 0–18 years old and 22.1% among the 26–64 years old population (Kirzinger et al. 2013; CDC, National Center for Health Statistics 2010; Current Population Reports 2010). Over 10 million, or nearly one in three young adults aged 19–26 are not covered by health insurance (Holahan and Kenney 2008). There are several reasons why young adults are less likely to be covered under health insurance. From the supply side, young adults are more likely to take part-time jobs without any employer-sponsored insurance (hereafter, ESI), a more economic and low-cost way to gain health insurance coverage. Few young adults are eligible for public health insurance programs such as Medicaid or Medicare (Holahan and Kenney 2008). From the demand side, young adults have relatively lower demand for health insurance due to lower health risks and less attention to health problems. A lack of health insurance reduces young adults’ likelihood to utilize healthcare services when needed (Anderson et al. 2012) and increases the likelihood of suffering from financial burdens or health deficits because of higher healthcare costs (Himmelstein et al. 2005). Expanding health insurance coverage among young adults has been a significant economic and social concern. Many policies have been proposed to reduce the uninsurance rate among young adults. One of the most recent and important one is the Affordable Care Act’s (hereafter, ACA’s) dependent coverage mandate. Effective on September 22, 2010, this was also one of the earliest effective mandates under the ACA. Before the ACA, states also implemented dependent coverage mandates. However, the ACA’s dependent coverage mandate removed all restrictions of the state-level policies and allowed young adults to be covered under their parents’ health insurance plan until their 26th birthday. Early studies of the ACA’s dependent coverage mandate find a significant increase in insurance rates among 19- to 25-year-olds (Antwi et al. 2013; Sommers et al. 2013; Barbaresco et al. 2015). These papers use difference-in-difference (hereafter, DD) to identify the effect of the ACA’s dependent coverage mandate on several different outcomes. The DD-type models heavily rely on the parallel trend assumption. However, this assumption may fail if 19- to 25-year-olds are different in unobservable ways than those who are 26 or older. In this paper, we use an alternative approach and focus on short-run effects of the ACA’s dependent coverage mandate. In particular, we exploit the discrete change in health insurance coverage rates at the 26th birthday due to the ACA’s dependent coverage mandate and investigate the impact of this policy on insurance coverage rates and labor market outcomes among young adults. To the best of our knowledge, our paper is the first to provide RD estimates of the effects of the ACA’s dependent coverage mandate on the probability of taking different job types such as temporary jobs, seasonal jobs and self-employment. We also supplement the literature by investigating heterogeneous effects of the ACA’s dependent coverage mandate on young adults with different baseline health status.",10
45,1,Eastern Economic Journal,16 October 2018,https://link.springer.com/article/10.1057/s41302-018-0121-x,Effects of the 2010 Affordable Care Act Dependent Care Provision on Military Participation Among Young Adults,January 2019,Pinka Chatterji,Xiangshi Liu,Barış K. Yörük,Female,Unknown,Male,Mix,,
45,1,Eastern Economic Journal,27 September 2018,https://link.springer.com/article/10.1057/s41302-018-0115-8,The Relationship Between Health Insurance and Early Retirement: Evidence from the Affordable Care Act,January 2019,Erkmen Giray Aslim,,,Unknown,Unknown,Unknown,Unknown,,
45,1,Eastern Economic Journal,22 September 2018,https://link.springer.com/article/10.1057/s41302-018-0116-7,The Effect of the Affordable Care Act on Entrepreneurship among Older Adults,January 2019,James Bailey,Dhaval Dave,,Male,Unknown,Unknown,Male,"Owing to the tax deductibility of employer-provided health insurance benefits and the absence of universal public coverage, employment and health insurance coverage remain tightly linked in the USA. A large majority of the non-elderly obtains health insurance coverage through their employers,Footnote 1 and fear of losing access to affordable coverage has been found to hinder job mobility, a much-studied phenomenon known as “job-lock” (Gruber and Madrian 2004; Currie and Madrian 1999; Madrian 1994). Previous work has also found substantial evidence that some Americans are deterred from self-employment because they are concerned about losing their employer-based health insurance (Wellington 2001; Gurley-Calvez 2011). About 1 in 9 workers in the USA is self-employed (Hipple 2010), which, though sizeable, is still among the lowest across the OECD countries.Footnote 2 Availability of low-cost coverage options apart from one’s job would be expected to affect employment outcomes, specifically reducing job-lock, increasing job mobility, and potentially impact transitions between wage employment and self-employment (Gruber 2000). The Affordable Care Act (ACA) introduced several reforms aimed at increasing insurance coverage and improving the market for individual insurance, with one objective being to promote entrepreneurship.Footnote 3 We assess whether, and the extent to which the ACA has made progress toward this objective using a quasi-experimental research design applied to data from the American Community Survey. As potential entrepreneurs are deterred because of concerns over their ability to maintain health insurance when they start a company, it is important to understand whether and to what extent the ACA solves this problem, as well as the extent to which further reforms would be needed to keep the employer-based health insurance system from distorting the labor market and slowing the formation of new businesses. Market frictions that impede entrepreneurship may have adverse implications for efficient matching between worker skills and work and lead to labor market inefficiencies.Footnote 4 Entrepreneurs must often rely on the market for individual insurance, rather than the better-functioning large-group insurance market available to large employers; before the ACA, policies on the individual market were more expensive on average and were often completely unavailable to those with pre-existing conditions (Pauly and Lieberthal 2008). The ACA does appear to have substantially increased the use of health insurance, with coverage of all non-elderly American adults rising from 79.6% in 2013 to 87.7% in 2016, and coverage of self-employed non-elderly American adults rising from 70.7% in 2013 to 81.8% in 2016.Footnote 5 Because many of the main ACA provisions took effect in January 2014, it is only now becoming possible to study their effects; previous work on the ACA and the labor market could only investigate the effect of relatively minor early provisions such as the dependent coverage mandate (Bailey 2017; Bailey and Chorniy 2016). Ours is one of the first attempts to study how the main provisions of the ACA have affected entrepreneurship, along with Heim and Yang (2017), Duggan Goda and Jackson (2017), and Blume-Kohout (2018). Our use of the 2000–2016 American Community Survey provides a large sample with 3 full years of post-ACA-implementation data, which may explain why our estimates are larger and more statistically significant than those of previous work. The ACS also enables another key contribution, the use of several definitions of entrepreneurship in addition to the standard binary self-employed definition. In addition to contributing to the very limited evidence base on the link between the ACA and entrepreneurship, in this study we specifically focus on older adults. This focus is salient for several reasons. First, if employer-sponsored health insurance is a constraint against entrepreneurial activity, this deterrent effect is likely to be strongest among older adults. Marginal valuation of health insurance increases with age, and, in the absence of reforms, market frictions in the non-group market may make it especially costly and difficult for older adults to obtain health insurance coverage outside of their jobs prior to their becoming eligible for Medicare. Second, older adults have consistently higher rates of entrepreneurial activity, as proxied by self-employment, relative to younger adults. About 16% of adults ages 60–69 are self-employed, compared with 10% of adults ages 27–59 (American Community Survey 2000–2016). Many older workers transition from wage employment to self-employment prior to exiting the labor force completely, and self-employment may also offer some older workers a way to transition back into the labor market after an adverse health shock or job-loss. Given the high prevalence of self-employment and the role that it plays in providing older adults flexibility in labor market participation, it is important to understand potential incentives and impediments to self-employment for this group. Finally, the focus on older adults allows us to exploit Medicare’s age-related eligibility threshold as a natural experiment. Older adults who are eligible for Medicare by virtue of being over 65 years of age should not be affected by the ACA’s market reforms, comprising a plausible counterfactual for older adults who are not yet Medicare-eligible (and thus more likely to benefit from the ACA’s expansion of coverage and reforms to the non-group market).Footnote 6 We use this natural experiment within a difference-in-differences framework to identify plausibly causal effects of the ACA on measures of entrepreneurial activity among older adults.",15
45,2,Eastern Economic Journal,27 September 2018,https://link.springer.com/article/10.1057/s41302-018-0118-5,Retaliatory Antidumping by China: A New Look at the Evidence,April 2019,Thomas Osang,Jaden Warren,,Male,,Unknown,Mix,,
45,2,Eastern Economic Journal,03 September 2018,https://link.springer.com/article/10.1057/s41302-018-0113-x,Re-Thinking Debt Burden: Going with the Flow?,April 2019,Vipul Bhatt,Andre R. Neveu,,Male,Male,Unknown,Male,"In recent years, the sustainability of the US Treasury debt has come to the forefront of both political and economic analysis. Many observers of the US economy have expressed concerns about rising Treasury debt levels, and the adverse effects recent increases may have on future economic growth.Footnote 1 An important element of this debate is the choice of an appropriate macroeconomic indicator of a country’s ability to service its public debt.Footnote 2 A widely used measure in both academic as well as policy discourses is the concept of the “debt-to-GDP” ratio which is obtained by dividing a nation’s public debt, a stock, with the gross domestic product (GDP, henceforth), a flow. In the literature, it is commonplace to associate a higher level of this ratio with negative growth prospects for an economy (Bohn 1998; Cecchetti et al. 2011; Checherita-Westphal and Rother 2012). In this paper, we argue that the commonly used debt-to-GDP ratio is a poor interpretation of debt burden, and leads to misguided concerns about the level as well as the evolution of public debt burden over time.Footnote 3 These concerns are captured by Robert Shiller (2011) who noted, “The fundamental problem that much of the world faces today is that investors are overreacting to debt-to-GDP ratios, fearful of some magic threshold, and demanding fiscal-austerity programs too soon. They are asking governments to cut expenditure, while their economies are still vulnerable.” We propose an alternative measure of US public debt burden, which incorporates duration of the debt and hence is a more appropriate measure for studying both the level and the evolution of the public debt burden.Footnote 4 The first problem with the debt-to-GDP ratio stems from the difficulty of meaningfully interpreting the magnitude of this ratio for making statements about the debt burden of an economy. Taking the USA as an example, the national debt is measured in dollars at a point in time (e.g., as of December 2018), whereas GDP is measured in dollars per unit of time (e.g., every year, quarter, or month). As a result, the ratio itself has units of time over which the GDP is measured. For instance, at an annual frequency, such a measure has the interpretation of the number of years it would take a country to pay its debt under the unrealistic scenario of allocating the entire GDP toward the debt repayment. Further, the choice of a year as the time frame for public debt payment is inherently arbitrary and does not align with reality. To illustrate this issue, we plot the marketable US debt-to-GDP ratio in Fig. 1.Footnote 5 We observe that at the end of 2017 this ratio stood at 72.4 percent. The only interpretation of this number is that approximately nine months of production of the US economy in 2017 would need to be dedicated toward debt repayment. The magnitude of this ratio is not a direct reflection of the actual debt burden at the time.Footnote 6 Despite this problem of interpretation, the debt-to-GDP ratio has been widely used a measure of debt burden and several studies have investigated its impact on macroeconomic indicators such as economic growth. We review this literature below and provide an empirical example. One contribution of our paper is to provide a single measure of public debt burden that incorporates duration of the debt and can be interpreted as a percentage of any amount of production. While there is no one way to refer to the debt burden or debt service, our proposed measure is akin to the debt service ratio and is a better measure in that it reflects economic conditions and/or policy decisions. Debt-to-GDP versus DD-to-GDP Ratio The second drawback of the debt-to-GDP ratio is that it does not account for the duration of the debt outstanding, and thus ignores a country’s ability to manage the terms of repayment by influencing the maturity of its debt. Theoretically, a government facing high debt levels may either increase or decrease the maturity of its debt (Alesina et al. 1990; Missale and Blanchard 1994; Giavazzi and Pagano 1990). Therefore, the nature of this relationship is an empirical question. Many studies have found that the relationship between maturity and debt-to-GDP ratio is generally positive for the USA (de Haan et al. 1995).Footnote 7 In this paper, we find a time-varying correlation between maturity and the debt-to-GDP ratio for the USA.Footnote 8 Further, in recent years the US Treasury Department has been able to extend the maturity of its debt in response to the 2008 financial crisis when debt rapidly accumulated. Between October 2008 and the end of our sample in April 2018, the Treasury extended the average maturity of outstanding marketable securities by approximately 43 percent (Fig. 2).Footnote 9 We argue that a country that can extend the maturity of its debt by spreading out payments toward their debt across longer periods is less burdened than those who cannot. Hence, a measure of a country’s payment capacity that ignores this channel will be a poor indicator of the debt burden. Duration and maturity including and excluding bills In this paper, we aim to broaden the discussion on the macroeconomic implications of public debt in two dimensions. First, an appropriate analysis of debt burden should account for possible changes in the duration of debt over time. There have been duration shifts in the past, based on government decisions about financing. For example, between late 2001 and early 2006 the US Treasury ceased issuing 30-year debt. During this period, the overall maturity fell nearly 20%, and duration declined about 5% reflecting a period of rising interest rates. Because duration can shift independently of the debt level—which was rising at the time—our proposed measure is providing new information that cannot be summarized in a debt-to-GDP ratio. Using detailed historical data on bond issues provided by the US Treasury, we first compute duration as an annual measure and use it to compile a monthly time series measuring the debt-to-duration ratio (DD). The DD measure is obtained by dividing total debt by duration to estimate an annual burden of debt as though it were evenly distributed over each year of planned repayment. This is a flow measure of debt and is measured per year, like GDP. Second, we propose the DD-to-GDP ratio as an alternative to the conventional measure of debt-to-GDP for gauging the payment capacity of an economy. The DD-to-GDP ratio is unit-less and represents a closer approximation to the percentage of national income that would need to be allocated to debt service. Unlike the debt-to-GDP ratio, our measure of debt burden is sensitive to changes in both maturity and interest rates and does not require assigning an arbitrary unit of time for repayment of national debt. More importantly, by incorporating duration in its computation, our measure captures an important aspect of public debt financing where a government may be able to change the duration of its debt. For these two reasons, we believe that our measure is more appropriate measure of debt burden when compared to the debt-to-GDP ratio. We use our measure of debt burden and document several findings of interest for the US economy. First, the recommended measure of debt burden (DD-to-GDP ratio) correlates well with the more conventional debt-to-GDP ratio as indicated by a correlation of 0.96 between 1997 and 2018. However, the overall correlation masks significant changes in the relationship between the two measures described here later in the empirical section. Second, we show that our recommended measure of the debt burden paints a very different picture for the US economy than the conventional debt-to-GDP ratio. For instance, the debt-to-GDP ratio fell from around 40% in April 1997 to 26% in June 2001 and had risen back to 35% by September 2008 (Fig. 1). In contrast, the DD-to-GDP ratio fell from approximately 0.13 in mid-1997 to a low of 0.074 in June 2001, before rising to about 0.11 in September 2008. In September 2008, debt began growing at a faster rate than GDP such that the debt-to-GDP ratio more than doubled to 74% by the end of the sample in 2018. During the same time period, the DD-to-GDP ratio rose along with the total amount of outstanding debt to just over 0.17 at the end of the sample in 2018. Although the DD-to-GDP ratio does increase from the start of the sample, the difference is much smaller in magnitude than the increase in debt-to-GDP. Based on our findings, the standard rhetoric of an unsustainable debt burden deduced primarily from the debt-to-GDP ratio seems misplaced. Third, using the projected levels of maturity and debt from the Treasury Department we document the anticipated evolution of the DD-to-GDP ratio in the coming years. We find that depending on future interest rates and the mix of debt issuance, the DD-to-GDP ratio should be stabilizing over the next decade. In fact, there is a scenario with a roughly stable mix of debt issuance and coupon rates over the next 10 years, where we could expect the US debt burden as measured by the DD-to-GDP ratio to remain near levels seen at the end of the sample. By using our proposed flow measure of debt, researchers can use one number to assess debt burden across countries and more accurately determine the relationship between high level of debt and economic performance. We illustrate the usefulness of our measure by investigating the relationship between DD-to-GDP ratio and economic growth for the USA. We find that a rising debt-to-GDP ratio is related to slower growth. However, a rising duration of public debt can counteract the negative growth effects. Our results have significant implications for the discourse on debt burden in general, and the economic analysis of the rising debt in particular. Although the debt-to-GDP ratio is a quick way of measuring a country’s debt burden, we believe that it misses the important role of long-term interest rates and maturity in the determining the actual incidence of debt burden for an economy. The following sections provide a brief review of the related literature, and then present our conceptual framework. We then describe the data and compare our measure of debt burden with the debt-to-GDP ratio and provide an empirical exercise on the relationship between duration and growth. The final section concludes.",
45,2,Eastern Economic Journal,14 January 2019,https://link.springer.com/article/10.1057/s41302-019-00133-8,Inflation in Argentina: Analysis of Persistence Using Fractional Integration,April 2019,Mateo Isoardi,Luis A. Gil-Alana,,Male,Male,Unknown,Male,"Throughout most of its recent and not so recent history, the Argentinian economy has been characterized by high inflation rates, not only with respect to the levels that are usually considered acceptable in a stable economy, but also with respect to other countries, both in the region and across the globe. Such is the magnitude of the problem facing the country that in recent years it has been clearly situated among the three economies with the highest levels of annual inflation, when the international tendency is to present moderate or even low levels. But again, this is not new. For at least the last 70 years, Argentina has regularly reported two-digit annual inflation rates, including some years in which three-digit inflation was reached, and even 2 years in which (hyper)inflation levels far exceeded 1000%. In this context, inflation in Argentina seems to have become the norm, while the years in which there was no inflation or there were acceptable levels seem to be the exceptions. Or in other words, inflation in Argentina seems to have a rather important persistence component. Thus, the objective in this paper is to study the statistical characteristics of the inflation series of Argentina to verify whether, as expected, there is evidence of persistence and eventually determine the degree of it. There are several papers in the literature that have analyzed the persistence of the inflation series in Argentina. For instance, D´Amato et al. (2007), D’Amato and Garegnani (2013) and Pastor Rueda (2014) have obtained evidence that the inflation series of Argentina are persistent, with the reservation of indicating that the degree of its persistence changes if it is considered a constant mean or variant in time. On the other hand, Capistrán and Ramos-Francia (2006), in a broader study involving several countries in Latin America, have found indications of inflationary persistence for Argentina which has declined over time. All these works have two aspects in common, in addition to the results they have reached. The first one is the extension of the series of inflation that they use for their study, since they are concentrated in the period that stretches from the 1970s to the middle of the first decade of the twenty-first century. The second one, and which is more important, is that the calculation of inflationary persistence is performed by adding the autoregressive components of the AR(p) model defined for the series of inflation, following both Andrews and Chen (1994) and Marques (2004). It is in these points where the main difference lies with the approach used in this work which will enable us to contribute to the existing literature. First, the study will be carried out on a larger sample, ranging from 1943 to mid-2017, which is the period for which official statistics exist. Then, in this paper, the persistence of the inflation series in Argentina will be studied through the use of fractional integration, a process that is more flexible than the ARMA processes, which represent I(0) or stationary processes, and the ARIMA processes, or I(1), that is to say, those processes that present a unit root. In this way, in the fractional integrated models, or ARFIMA (p, d, q) models, the integration component “d” can assume any real value between 0 and 1 or even above 1. Thus, these models can identify different degrees of memory of the series, with respect to the influence that past behavior possesses in the posterior values. Moreover, and unlike other measures of persistence that use short-term parameters—autoregressive or moving averages—the ARFIMA (p, d, q) models analyze the long-term behavior of the series. Thus, in these models, when speaking of persistence, we speak of long memory, which, according to Lemus and Castaño (2013), “[…] is usually understood as the existence of a not insignificant dependence between observations that are distant from each other for long periods of time,” where “[…] the degree of memory and stationarity of the process is defined by the fractional differentiation parameter d, which takes values ​​in a continuous range of real numbers.” These results reinforce the main hypothesis of this study which is that the time series of inflation in Argentina have statistical persistence or long memory, in terms of fractional integration. Furthermore, the study of the statistical persistence of the inflation series in Argentina may yield results that can be translated into economic policy implications. Thus, determining that the series is persistent, or has long memory, would indicate that not only does the series depend in a non-negligible way on values more or less removed from the variable in the past but that it can also present mean reversion, that is, that the values of the variable will tend to return to its mean value after a shock. Thus, a recommendable policy would be to avoid provoking sudden shocks to try to lower inflation, since inflation would return to its average value in any case and the costs in terms of other variables (production, employment, etc.) could be too high and, ultimately, sterile. This paper is structured as follows: “Historical Context” section describes the evolution of inflation in Argentina during the period under analysis, from a historical perspective. “Literature Review” section briefly reviews the literature regarding the study of the persistence of inflation in general and in the particular case of Argentina. The explanation of the methodology used, that is, the theoretical aspects of ARFIMA (p, d, q) processes, is the subject of “Methodology” section. “Data and Empirical Results” section presents the data and the results obtained in the study. Finally, “Concluding Comments” section presents the main conclusions.",4
45,2,Eastern Economic Journal,25 September 2018,https://link.springer.com/article/10.1057/s41302-018-0120-y,On the Relationship Between Income Inequality and the Shadow Economy,April 2019,Aziz N. Berdiev,James W. Saunoris,,Male,Male,Unknown,Male,"The causes and consequences of inequality in the distribution of income have garnered a fair amount of attention from academics, policy makers, and the public at large (see Dabla-Norris et al. 2015). Although some level of income inequality may be efficient to provide growth-enhancing incentives that promote savings and investment in capital and technologies, too much inequality potentially leads to the disadvantaged having less access to education, healthcare and other opportunities, or in extreme cases, civil unrest and political instability that destroy incentives for wealth creation.Footnote 1 While the evidence relating economic growth and income inequality is inconclusive (see Ostry et al. (2014) and the references therein), the negative consequences of inequality have led many nations to attempt to correct income disparity through redistributive programs financed by a progressive tax system. According to The Economist, “[T]oo often high-tax welfare states turned out to be inefficient and unsustainable. Government cures for inequality have sometimes been worse than the disease itself.”Footnote 2 Consequently, individuals react to this government failure by “voicing” their discontent in political elections or “voting with their feet” by “exiting” the jurisdiction (see Hirschman 1970). Whereas exiting a country might be considered very costly, an alternative to escape high taxation is for individuals to retreat to the unofficial or shadow economy (see, e.g., Schneider and Enste 2000).Footnote 3 The shadow economy exists in all countries to varying degrees and offers an alternative to official sector production and employment. Schneider et al. (2010) estimate the size of the shadow economy to be approximately 33% of GDP worldwide with shadow activities amounting to 17% of GDP in high-income OECD countries and as high as 40% in Latin America and Sub-Saharan Africa. The literature argues that the shadow economy weakens political institutions by for example undermining the government’s ability to collect taxes that is essential to supplying public resources to the general population (Schneider and Enste 2000; Gërxhani 2004). In addition, the existence of the shadow economy causes official statistics, which are relied upon by policymakers, to be misrepresented (Feige 1989; Schneider and Enste 2000). Yet, research also highlights that the shadow economy may advance economic development by for example promoting entrepreneurial activities (Asea 1996; Schneider and Enste 2000; Dell’Anno and Solomon 2008). Dell’Anno and Solomon (2008) show that the shadow sector provides employment to unemployed individuals from the formal sector. Thus, the income earned in the informal sector may be spent in the formal sector, thereby boosting aggregate demand (Schneider and Enste 2000; Gërxhani 2004). The purpose of this study is to better understand the dynamic interrelationship between income inequality and the shadow economy. Specifically, does income inequality affect the shadow economy, or does the shadow economy affect income inequality? Or, is it possible that the shadow economy and income inequality reinforce each other? For example, individuals at the lower end of the income spectrum may choose to migrate underground for its increased flexibility and autonomy, or they may be forced underground due to the limited opportunities available to the disadvantaged in the official economy because of their low skills and poor education. Moreover, the prevalence of the shadow economy may contribute to a growing income inequality if shadow agents become trapped in the low-wage shadow sector: thus, the shadow economy may depress earnings over time. Alternatively, the shadow economy may reduce inequality as individuals exploit the opportunities provided by the shadow economy to learn new skills or start a business that results in an increase in income mobility. We therefore simultaneously estimate the linkages between the shadow economy and income inequality to account for the possible bidirectional causality between these variables (see, e.g., Rosser et al. 2000, 2003; Ahmed et al. 2007). It is also possible that the association between the widespread prevalence of the shadow economy and income inequality is a process that develops over time. For example, the process that unfolds following the implementation of redistributive polices that raise taxes thereby encouraging migration to the shadow economy requires a dynamic analysis to describe changes in shadow economy occurring over time.Footnote 4 For example, Gutiérrez-Romero (2010, p. 26) argues that “inequality in wealth causes a large informal sector; however, a large informal sector increases inequality as the difference between the rents of those engaged in the formal and informal sectors deepens over time.” Consequently, studies that focus only on the static steady-state relationship between income inequality and the shadow economy provide only a limited understanding of this inherently dynamic relationship. Finally, we account for the official sector and its effect on the shadow-inequality nexus.Footnote 5 In the short run, the counter-cyclical nature of the shadow economy—i.e., by absorbing the excess demand and supply in the official sector over the business cycle—affects both the official sector and inequality. For instance, high demand for labor during a booming economy promotes upward income mobility and reduces the attractiveness of the shadow economy. In contrast, during a recession low-income individuals with little savings rely disproportionally on the shadow economy for consumption of goods and services and to supply their labor. In the long run, however, evolving institutions impact this relationship, as the shadow economy has the ability to alter institutions that impact factor accumulation and encourages new markets (Asea 1996). Accordingly, the interrelationship among inequality and official and unofficial production can differ over the short run and long run. To better understand the dynamic interrelationship between the informal economy and income inequality we use cross-country data of 144 countries observed over the period from 1960 to 2009 and employ a panel vector autoregressive (VAR) model (Holtz-Eakin et al. 1988; Love and Zicchino 2006). The panel VAR methodology enables us to estimate the dynamic endogenous relationships between income inequality and the shadow economy while controlling for the influence of the official economy. In particular, we use impulse response functions to illustrate the complete dynamic adjustments from the initial shock to the long-run steady state of the shadow economy, the official economy and income inequality (Holtz-Eakin et al. 1988; Love and Zicchino 2006). The results reveal considerable dynamics underlying the relationship among the shadow economy and income inequality. We find a bidirectional positive relationship between the two, namely, higher income inequality promotes the spread of the shadow economy whereas the development of the shadow economy contributes to income inequality. Overall, these results withstand a variety of robustness checks, including alternate ordering of the variables, alternate measure of income inequality, accounting for institutional quality, alternate measure of the shadow economy and removing the effects of outliers. Turning to possible policy implications, it is vital to treat the interactions between income inequality and the shadow economy jointly in forming effective policies to inhibit the rising inequality in the income distribution and the prevalence of the shadow economy. The remainder of the paper is structured as follows: the second section describes the relationship between the shadow economy and income inequality; the third section details the panel VAR model whereas the fourth section describes the data; the fifth section gives the results of the baseline model along with sensitivity analyses; and the final section provides concluding remarks.",20
45,2,Eastern Economic Journal,16 November 2018,https://link.springer.com/article/10.1057/s41302-018-00127-y,Bubbles and Broad Monetary Aggregates: Toward a Consensus Approach to Business Cycles,April 2019,Cameron Harwick,,,,Unknown,Unknown,Mix,,
45,2,Eastern Economic Journal,13 February 2019,https://link.springer.com/article/10.1057/s41302-019-00136-5,The Job Search Intensity Supply Curve: How Labor Market Conditions Affect Job Search Effort,April 2019,Jeremy Schwartz,,,Male,Unknown,Unknown,Male,"Whether or not individuals search more intensely for work during an expansion is important to our understanding of how the labor market evolves over the business cycle. Despite this, whether job search intensity is pro- or counter-cyclical is still an open question. However, intuitively during expansions the unemployed may anticipate a higher income in the near future from finding work quickly, and as a result, consume more of their savings while unemployed. If leisure and consumption are complements workers may then devote less of their time endowment to their job search. This offsetting effect, if dominate, may result in search intensity decreasing during expansions. To date, the few empirical investigations of this issue have provided mixed evidence on whether search intensity is pro- or counter-cyclical, suggesting the relative magnitude of these effects is unsettled. Consequently, modelers of the labor market have little empirical evidence to support a pro-cyclical search intensity assumption. Further, policy advice using these models could be unreliable. This paper seeks to enhance our knowledge of the business cycle by answering the question, “Does search intensity rise or fall with labor market conditions?” To determine whether or not search intensity is pro-, counter-, or a-cyclical, this paper first outlines a theoretical model with three main features (1) complementarity between leisure and consumption, (2) precautionary savings by workers, and (3) the ability for search effort and macro-labor market conditions to be substitutes or complements in the job search process. In this context, rather than constraining search intensity to be pro-cyclical, as many search models do, search intensity’s relationship with the business cycle is ambiguous. I take this model to the data and estimate its parameters structurally, which allows for easy interpretation of the estimated parameters and their implications. For instance, results indicate that search intensity and labor market conditions are complements in the process of finding work. Despite this complementarity, the estimated parameters imply only a weak pro- to an a-cyclical relationship. This calls into question whether assuming strong pro-cyclicality in a macro labor model is appropriate. Understanding how search intensity varies over the business cycle is critical to developing models of the labor market and using them effectively for policy evaluation. Mortensen’s (1976) and Pissarides’s (2000) search frameworks have been the basis for much of the research in this area. In standard search models, exerting job search effort is more likely to yield employment during good times versus bad. As a result, these models implicitly assume worker’s search intensity is pro-cyclical. 
Veracierto (2002), however, shows that pro-cyclical effort is not consistent with the empirical regularities of a counter-cyclical unemployment rate. This is because greater effort in expansions decreases the unemployment rate, but more entrants into the labor force increases the unemployment rate. The model implies that unemployment is a-cyclical, which puts into doubt whether models that include the pro-cyclical effort assumption should be used for policy analysis. This also motivates Shimer (2004), who addresses the technical aspects of how search influences the job-finding rate, and shows that search intensity is positively related to macro conditions only for those with less than an 80 percent probability of finding a job during their search period. In more recent work, Hornstein and Kudlyak (2016) show that whether search is pro- or counter-cyclical depends on the elasticity of search with respect to labor market tightness, with a large elasticity implying a counter-cyclical search intensity. To examine how search intensity varies with labor market conditions, I extend the model used by Lentz (2009) to estimate the optimal UI benefit in Denmark. Lentz’s (2009) model allows workers to save and borrow up to some binding constraint. Thus, a worker with a high likelihood of finding employment may choose to borrow from future consumption, while those who are unemployed during a poor labor market may ration their scarce wealth. I extend the model to allow for the impact of labor market conditions on the productivity of search intensity to range from no effect (a relation I refer to as job search substitutes) or a positive impact (a relation I refer to as job search complements). The flexible nature of the model allows me to identify how search intensity moves with changing labor market conditions, as well as to inform our intuition on why these movements occur. The model implies that the relationship between labor market conditions and search intensity is determined by three different effects. The first can be thought of as a substitution effect. If improving labor market conditions makes job search effort more effective, the case of job search complements, then the opportunity costs of not searching grows. Workers consequently substitute away from expensive leisure and toward greater search effort. The next effect is an income effect. As labor market conditions improve, it becomes more likely one will find a job, which increases a worker’s expected lifetime income. As a result, workers can use more of their wealth earlier in their unemployment spell, mitigating any necessary reductions in consumption while out of work. If leisure and consumption are complements, this will increase desired leisure time during unemployment and reduce time devoted to searching for work. The final effect is a self-insurance effect. If workers respond to improving labor market conditions by saving less today, they will have less wealth for longer spells. With less future wealth, the gain, measured in discounted utility terms, between getting back to work and continuing without employment grows. This larger gain incentivizes greater job search effort. If job search and labor market conditions are perfect job search complements in the process of finding a job, search effort will increase when the labor market improves, as the substitution effect always dominates. However, if these factors are perfect job search substitutes, search intensity may decrease, if the income effect dominates. Finally, when labor market conditions and search intensity are neither perfect complements nor perfect substitutes, it is possible that for some range of labor market conditions, search intensity will increase, while in others it may decrease as in Fig. 1. Measures of search intensity over the business cycle. Note: Search methods are the average number of methods used by the unemployed in the National Longitudinal Survey of Youth 1997 (NLSY97), conditional on using at least one method. Several empirical studies try to establish whether job search is pro-, counter-, or a-cyclical. An early descriptive paper from Finegan (1981) examines the extensive margin of whether to search and finds it to be pro-cyclical. Other papers use the search methods of job seekers as proxies for search intensity. Böheim and Taylor (2001), using the British Household Survey, find that search intensity is inversely related to local unemployment rates, and Bachmann and Baumgarten (2012), examining data on a variety of search methods using international data, also conclude that there is a negative correlation between search intensity and the unemployment rate. Kuhnen (2010) confirms these findings with a micro data set of MBA graduates, who are shown to apply for fewer jobs during bad versus good labor markets. The pro-cyclical finding in these papers suggests that the self-insurance and substitution effects might dominate. In contrast, however, Doiron and Gorgens (2009), using a data set of Australian graduate students, find search intensity decreases with improving labor market conditions. With time use data, Aguiar et al. (2011) find that workers increase the time they spend searching for work during recessions. Mukoyama et al. (2018) combine American Time Use Survey with data from the much larger Current Population Survey, to develop a measure that uses both time use data and data on the methods used to search for work. The authors confirm the Aguiar et al. (2011) finding that search is counter-cyclical. Faberman and Kudlyak (2016) is one of the few papers that considers leisure and consumption complementarity to motivate the possibility of counter cycle effort and find that online applications increase when local unemployment rates are higher. These papers suggest that it might be the income effect that dominates, resulting in counter-cyclical search intensity. This conflicting evidence leaves theorists still in the dark as to whether a pro-cyclical search effort is an appropriate assumption, and policymakers uncertain on how workers may respond to labor market policies at different points in the business cycle. To provide further evidence on the effect of labor market conditions on search intensity, this paper estimates the structural parameters of the search model the next sections outline. I use the NLSY97, which provides detailed work histories for 9000 individuals, along with numerous demographic (age, gender, education, etc.) variables, and identify whether search intensity increases or decreases with labor market conditions. To estimate the model, I use a structural approach, similar to Lentz (2009), where, given set of parameters and independent variables for a non-unemployment spell, the model identifies the unobserved amount of search intensity, and associated probability of finding a job. In order to estimate the structural parameters, I take a Maximum Likelihood approach that relates the implied hazard rates to the observed non-employment spell lengths. There are a variety of reasons a structural approach in this context is beneficial. First, one of the goals of this paper is to provide guidance on whether a pro-cyclical search assumption is warranted when modeling the labor market. Thus, a direct connection to the parameters estimated and the parameters of a standard search model is desirable. Second, a structural approach allows for estimation of the important model parameters, in this case the elasticity of substitution between labor market conditions and search effort. Third, estimation that is rooted in a theoretical model makes explicit the justification of any functional forms or nonlinearity in the estimation. In contrast, in a reduced form approach, the econometrician must somewhat arbitrarily assume a linear, log, quadratic or other relationship between labor market conditions and search effort. Finally, rather than using a proxy for search effort, such as the number of search methods, the approach taken in this paper allows the data to determine the degree of importance of each search method employed. To leverage data on the methods of search individuals in the NLSY97 use to find work, I take an approach similar to that of Bloemen (2005) and assume that the probability of reporting a specific search method increases with the unobserved search intensity predicted by the model. Thus, rather than assume, as do other papers in the literature, that there is a linear relationship between the number of search methods and search intensity, the method I employ allows the data to determine this relationship. As a by-product of this approach, the estimates reveal which methods have the highest marginal impact on the job-finding rate. 
Bloemen (2005) uses this approach with the goal of determining how search intensity influences labor market transitions, rather than its relationship with the business cycle. The author finds that the elasticity of job applications, with respect to the exogenous portion of the job arrival rate, is 0.12. However, by ignoring the role of wealth accumulation and by assuming workers maximize income, the income effect and self-insurance effect are not present in Bloemen’s (2005) model. Thus, it is not clear if Bloemen’s (2005) model could serve as a test of whether search intensity is pro- or counter- cyclical. An additional important difference with Bloemen’s (2005) paper is that, rather than the exogenous portion of the arrival rate being solely a parameter to be estimated, this paper utilizes data on occupational labor market conditions to identify the relationship between labor market conditions and search intensity. The results indicate that individual search intensity and macro labor conditions are complements in the job search process. While the model allows for search intensity to increase or decrease at different ranges of labor market conditions, search intensity is always increasing in the plausible range. However, simulations reveal that movements in search intensity across the business cycle are quite limited. I also find that sending out resumes and looking at advertisements have the largest effect on the exit rate from unemployment while contacting placement centers, unions, or schools and other miscellaneous methods have the lowest marginal impact on the probability of finding a job. In addition to improving our understanding of the business cycle broadly, these results are important to policymakers as well. For instance, in reaction to the weak labor market during the Great Recession, Congress increased the number of weeks one can collect unemployment insurance (UI) benefits from the standard 26 weeks to a maximum of 99 weeks, at a cost of $226.4 billion (Whittaker and Isaacs 2014). Several papers take a partial equilibrium approach to determine whether such a policy is appropriate. Kiley (2003), Sanchez (2008) and Kroft and Notowidigdo (2011) find that UI should be more generous when the labor market becomes slack. Schwartz (2012) and Landais et al. (2010), however, take a more general equilibrium approach where an increase in UI generosity impacts wages and lowers job creation. As a result, Schwartz (2012) finds that any increase in benefits during a recession should be limited, while Landais et al. (2010) finds that UI benefits should be relatively less generous. These papers, however, do not allow for savings and, as a result, ignore the self-insurance and income effects, implicitly assuming pro-cyclical search effort to reach their conclusions. Understanding how search effort varies over the business cycle may also be critical to policy evaluations and recommendations. The remainder of the paper proceeds as follows: Section two, “A Two-Period Model,” provides the intuition for the infinitely lived agent model I estimate, using a simplified two-period setting. Section three, “A Dynamic Search Model,” describes the dynamic model I use to estimate unobserved search intensity, and section four, “Estimation Strategy,” discusses the estimation approach. I describe the data sources in section five, followed by the results in section six. Section seven makes concluding remarks.",
45,2,Eastern Economic Journal,10 October 2018,https://link.springer.com/article/10.1057/s41302-018-0126-5,"Class Size, Course Spacing, and Academic Outcomes",April 2019,Kevin P. Belanger,Angela K. Dills,Kurt W. Rotthoff,Male,Female,Male,Mix,,
45,2,Eastern Economic Journal,17 December 2018,https://link.springer.com/article/10.1057/s41302-018-00130-3,How Much will a Universal Basic Income Plan Cost?,April 2019,David Colander,,,Male,Unknown,Unknown,Male,"In thinking about tax systems, people have a tendency to conflate marginal and average taxes. That conflation can be reduced by picturing a non-proportional tax system, such as our current progressive tax system, as consisting of two components – a proportional tax component that would exist if all households paid a constant marginal tax rate on all their income, not just the rate they pay on income above a certain level, and a lump sum tax or subsidy component, that is paid on non-marginal income. A proportional income tax system starting at zero tax at zero income would have no lump sum component; the marginal tax and the average tax are equal with a proportional tax. A progressive tax has a negative lump sum component, which can be the result of a basic income grant or progressive rates that increase as income increases. Under a progressive income tax system, the average tax a household pays is below the marginal tax it pays. Thus, for example, the average personal tax rate for high-income households in the USA is about 24%, but their marginal tax rate is about 40%. That means that a household with a million dollar income can be thought of as getting a $160,000 lump sum subsidy from our current system, compared to what it would have paid had it been faced a 40% proportional tax. Regressive tax systems work in the opposite direction. If the income tax was regressive, then the average tax paid by higher-income households would be above their marginal tax; they can be thought of as paying a lump sum tax in addition to what they would pay if their marginal tax rate were their average tax rate. These lump sum subsidies and taxes play a major role in determining the amount of revenue collected from an income tax system. Separating the tax in this way helps to highlight the difficulty of redistributing income with progressive taxes. The more progressive the income tax system, the greater the lump sum subsidy to the rich, and the higher the system’s average marginal tax rate must be for a progressive tax system to collect the same revenue as a proportional tax system with that same marginal tax rate. Put another way, one of the costs of providing benefits to low-income households either through low marginal rates, or through a basic income grant, is that one must also provide a greater lump sum subsidy to richer households. This means that to maintain the same revenue from the income tax, you have to raise the marginal tax rate high-income households face by significantly more than you lower the marginal tax rate on the poor in order to cover that lump sum subsidy. [The number of households in each income group, and their incentive effects also play important roles in designing an efficient progressive income tax. In Colander (1979) I explore such design considerations.] This way of looking at the income tax is useful when thinking about the universal basic income plan because it changes the focus of the debate from costs to design characteristics. Universal basic income plans significantly increase the progressivity of the income tax system. That means that they also significantly increase the lump sum subsidy that the rich receive from the income tax compared to a proportional tax. To keep tax revenue constant, that subsidy has to be offset through higher marginal taxes somewhere in the system regardless of how one measures costs. With this background, let us consider Widerquist’s argument that gross cost is not relevant when considering basic income plans. While logically correct, the argument is misleading because it does not capture the lump sum subsidy element of any basic income grant provided to all households. That lump sum subsidy is better captured by the gross costs and is what critics are likely thinking of when they argue that basic income plans are infeasible. While the basic income grant would not require government to take in any additional revenue, it would require it to change the tax schedule so that it collects the $1.8 trillion in taxes it currently collects in a way that is consistent with the average household starting with a tax credit of $20,000 and paying the same amount in taxes as they pay now. At the current progressive tax rate schedule, which has low-income individuals paying low marginal rates, that means essentially that all people earning less than about $150,000 will not pay any net tax, and that households with income over $150,000 will not pay any taxes on the first $150,000 they earn. To achieve that end, marginal tax rates would have to be significantly increased. This involves more than raising sufficient revenue to pay the net costs of the income grant. While it is true that to cover the lump sum subsidies to themselves, high-income households’ net tax payments would not have to be increased, their marginal tax rates will have to rise so that they can pay themselves the lump sum subsidy. How much marginal tax rates would have to be raised to make the plans feasible depends on how marginal tax rates are raised. I suspect that many supporters of basic income plans hope that we can do this by increasing marginal tax rates on the rich and raising the marginal tax rates on the poor. So say we leave current marginal tax rates at what they are for households with income below $150,000 and raise the marginal tax on households with income above $150,000 so that it is high enough to bring in the $1.8 trillion we currently collect. To make up for the lost revenue from the approximately 35% of the revenue that was previously collected from those with income below $150,000, the marginal tax rate the rich pay would need to rise to cover that the net transfer, but also to cover the lump sum subsidy that all individuals receive. That would require extremely high marginal rates, with all the accompanying widespread evasion and avoidance, as well as having negative incentive effects. I suspect that it is such a plan that critics have in mind when they say the plan is infeasible. The basic income plan becomes more feasible the more willing designers are to have high marginal rates at low-income levels—in other words, the more willing they are to add regresivity to the existing rates to offset the progressivity created by the basic income grant. What would make a basic income grant most feasible is if we increase the regressivity of the income tax significantly, by which I mean that we raise the marginal rates on low-income households. Indeed, Widerquist (2017) in his calculation of the cost of a basic income plan assumes a 50% marginal tax rate on income up to $50,000. The reason for having such high marginal rates on low-income households is not because one wants low-income households to pay more; the reason for doing it is that it reduces the lump sum subsidy that the progressive system provides to the rich, and thus lowers the amount that the marginal rate must increase on high-income households to bring in the needed revenue. The issues I raise here are well known by economists, but they are usually not expressed in these terms. Instead of speaking about the need for a regressive tax structure if one is going to provide benefits to the poor, economists normally speak about phase-out rates. This makes some sense because most of the discussions of phase-outs have involved not general tax rates, but rather the phasing out of targeted tax credits, exemptions and deductions. When existing phase-out rates are integrated with marginal rates, our current tax system becomes much messier in terms of its progressivity, with the net progressivity changing with different level of incomes and different circumstances of taxpayers. For example, the earned income tax credit has a phase-out rate of about 21% as income increases from $17,000 to $24,000 so the marginal tax rate for households in this income bracket who receive the earned income tax credit is not 15% but 36%. For a universal basic income grant that applies to everyone, the idea now being discussed, separating the phase-out rate from the marginal rate does not make sense. Since the plan is universal, the phase-out rate is part of the marginal rate. So when one states that high phase-out rates at low-income levels are needed to reduce the cost of the plans and make them feasible, one is saying that our tax system will have to be more regressive. With that background, let us consider the debate about costs of the basic income plan. The highest cost estimates result from an implicit assumption that phase-out rates will be zero. But that is not what proponents have in mind. They implicitly are assuming that there will be an initial relatively high phase-out rate. By making the tax system more regressive, the amount that government will have to raise marginal taxes on high-income individuals will be lowered. With the highest phase-out rates, the costs can be close to what proponents argue the plan will cost. Let us consider an extreme case that may help make the issues clear. Let us say that in the above example, the government places a 100% marginal tax rate on household income up to $20,000, and that at $20,000 the tax rate falls to the 10% rate that it is now. That reduction would significantly lower the cost of the basic income program. Of course, a 100% tax rate would have significant incentive, evasion, and avoidance effects for low-income households. Essentially, it would mean that households earning less than $20,000 would likely choose to exit the legal labor market. The reason they exit is not because they do not want to work; the reason is that the marginal rate of 100% means that they will earn nothing from working. Many households earning above $20,000 will also choose to withdraw from the legal labor market. The reason is that while the marginal tax rate for income above $20,000 might be 10%, for a decision to take a job or not, the relevant rate affecting their choice must include the 100% tax. For example, someone who took a job earning $25,000 a year would receive a net gain of about $4500, which translates into an effective tax rate of working or not working of about 70% (even though he faces a marginal tax rate of 10% for decisions about whether to extend the number of hours worked).",
45,2,Eastern Economic Journal,04 February 2019,https://link.springer.com/article/10.1057/s41302-019-00134-7,The High Costs of UBI are Not Financial: They are Real,April 2019,Pavlina R. Tcherneva,,,Female,Unknown,Unknown,Female,,1
45,3,Eastern Economic Journal,03 January 2019,https://link.springer.com/article/10.1057/s41302-018-00131-2,Evidence on the Effect of Political Platform Transparency on Partisan Voting,June 2019,Isaac Duerr,Thomas Knight,Lindsey Woodworth,Male,Male,,Mix,,
45,3,Eastern Economic Journal,05 October 2018,https://link.springer.com/article/10.1057/s41302-018-0122-9,Restrictions on Short-Term Capital Inflows and the Response of Direct Investment,June 2019,Richard J. Nugent III,,,Male,Unknown,Unknown,Male,"Capital flows management is an important branch of the international economics literature. The Research Department at the IMF has published several working papers that serve as the analytical basis for the policy guidance series on capital flows discussed in introduction.Footnote 6 The IMF’s revised policy guidance recognizes the capacity for capital controls to address macroeconomic and financial stability concerns in the face of inflow surges, only after all macroeconomic and exchange rate policy options have been exhausted (Ostry et al. 2011). The IMF’s revised position on capital controls highlights the substantial trade-offs that a policymaker must consider when evaluating a capital control. These trade-offs remain unsettled in the literature on many of the objectives that have been associated with capital controls. Forbes et al. (2015) find little support that capital controls are effective in any of 15 objectives. Forbes et al. (2015) however do not consider restrictions at the granular level like short-term capital inflows, nor do they study the influence of capital controls on direct investment. Several studies have found that capital controls push the maturity composition of external capital flows toward more long-term flows such as direct investment, but do not have an effect on the volume of external capital flows (Montiel and Reinhart 1999; Carlson and Hernandez 2002; Alfaro et al. 2005). Many studies have also found that capital controls improve growth resilience or reduce the vulnerability to crisis (Qureshi et al. 2011; Gupta et al. 2007; Edwards and Rigobon 2009; Pyun and An 2016), but there is evidence that this comes at the expense of efficiency losses and market discipline (Cardarelli et al. 2010; Forbes 2005b). Finally, there is an important branch of this literature which has looked at the costs of capital controls at the firm/investor level, where lower returns, higher cost of firm financing, and lower investment are significant (Alfaro et al. 2017; Forbes 2005b; Forbes et al. 2016). The contribution of this paper adds to a subset of papers which have studied the economic consequences of capital controls on particular asset categories as opposed to broadly defined capital control indices (see e.g., Eichengreen and Rose 2014; Alfaro et al. 2017; Asiedu and Lien 2004). In a closely related paper, Dell’Erba and Reinhardt (2015) show using an event study that restrictions on short-term debt flows decrease the likelihood of banking debt surges but increase the likelihood of financial sector foreign direct investment surges. The results presented here indicate that slower-growing countries with capital controls on short-term capital inflows receive higher levels of foreign direct investment. This result is similar to Dell’Erba and Reinhardt (2015) and is consistent with Cordella (2003), where foreign lenders in a theoretical economy find it profitable to invest in an emerging market if and only if the emerging government imposes taxes on short-term capital inflows. In Cordella (2003), the policy imposed on short-term capital inflows can prevent bank runs, increasing expected returns to investing and in turn increasing the volume of capital flows.",1
45,3,Eastern Economic Journal,28 March 2019,https://link.springer.com/article/10.1057/s41302-019-00138-3,Capitalist Views and Religion,June 2019,Adam T. Jones,Lester Hadsell,Robert T. Burrus Jr.,Male,Male,Male,Male,"The influence of culture on economic decisions has garnered renewed interest among economists (Alesina and Giuliano 2015; Guiso et al. 2006). The association between religion and economy is one such important cultural link (Iyer 2016; Guiso et al. 2003). Beginning with Weber’s hypothesis that economies of northern Europe grew faster than those of southern Europe due to the influence in the north of Protestant religion, the investigation has since evolved into a wider exploration of the multiple pathways religion may take to influence both macro-economies and individuals’ economic attitudes and actions. Although the Weber hypothesis has been difficult to substantiate (Iannaccone 1998), increasing empirical support exists for a connection between religion and a variety of market-related economic attitudes and behaviors, including those related to saving, investment choices, trust, charitable contributions, government intervention, and evaluation of the fairness of markets (Benjamin et al. 2013; Iyer 2016). In this paper, we examine the influence of religious affiliation and intensity of religious activity (religiosity) on support for markets at the individual level. We allow for religion to affect an individual’s views through two channels: parental affiliation as a proxy for latent moral underpinnings and current affiliation as a measure of conscious beliefs, while controlling for the intensity of religious involvement. Our contribution comes from improvement on prior studies in three areas: data collection; measures of religion and religiosity; and measures of market views. Many prior studies use data collected by face-to-face interviews which may suffer from social desirability bias, the phenomenon that occurs when responses are tilted toward cultural norms and expectations. Such misreporting has been observed in surveys of voting habits, charitable contributions, and religious activity (Cox et al. 2014; Presser and Stinson 1998; Smith 1998). Conti and Pudney (2011) report large differences in responses between face-to-face and self-completion survey modes. These studies note, for example, that while church attendance (and belief in God) reported in conventional phone and face-to-face surveys has been relatively stable over the past four decades, observed attendance is in decline. Religious beliefs and activities as well as affiliation are particularly susceptible (Cox et al. 2014). To address this issue, we use an innovative platform to collect data by anonymous computer-based surveys. As discussed in more detail in “Research Design” section, Amazon’s “Mechanical Turk” is an online freelance marketplace increasingly being used by academic researchers as an inexpensive, reliable source of data for surveys and experiments. Researchers are able to gain access to broader samples, filtering respondents based on quality of prior work, a distinct advantage over many traditional sources of data. Prior studies also tend to neglect certain religious affiliations. This may be important if there are substantial differences between them in market-related views. Arbuckle and Konisky (2015) note variations among Protestant denominations in terms of views of the natural environment and McCleary (2007) details the primary differences between major Protestant denominations (as well as other religions) in salvific merit (concern with salvation; avoidance of Hell or similar place), providing support for our separation of Protestant denominations in this study. This disaggregation proves important in explaining Protestant–Catholic differences in support for markets in our sample. Additionally, we include multiple affiliations and non-affiliations not included in prior studies, such as Mormon, atheist, agnostic, and “spiritual but not religious.” Prior studies also narrowly measure degree of religiosity, whereas we gather multiple measures of religiosity, used individually in our regressions and combined into an index. Our third contribution is the creation of two indexes. Single-attribute measures of the relationship between religion and market-related attributes (such as trust or charitable contributions or views of poverty) are more susceptible to measurement error than multiple combined measures (Nunnally and Bernstein 1994; Gliem and Gliem 2003). Our Capitalism Index is a comprehensive gauge of market attitudes, one that combines 10 measures into a single metric. Any individual’s level of support for markets is complex and is unlikely to be wholly captured by one’s view on a single attribute. Further, the seven-point Likert-type responses used in this survey allow for meaningful variation in responses, compared, for instance, to the two- or three-point choice set used in prior research. An index created as the summation of 10 such seven-point statements provides for greater variation and nuance. Likewise, our Religiosity Index is a more comprehensive measure of religious beliefs and actions, combining multiple measures of belief and action into a single metric. The overarching question we seek to address concerns how religious affiliation and intensity of religious belief are associated with overall support for markets; that is, are those who are more religious also more supportive of laissez-faire, free-market capitalism? Ancillary questions concern how certain demographic attributes are related to support for markets. For example, under certain conditions females have been found to favor less competition (more cooperative behavior). Will this translate into less support for free-market capitalism? Other variables examined include income, age, and early life household structure. Knowledge of the relationships between such variables and support for free markets provides insight into the potential macro-effects of the shifting role of religion in America and changing demographics. Furthermore, knowledge of these relationships potentially informs discussions of disagreements (among the general public as well as in the economics profession) about public policy such as trade and taxes. The role of religion in shaping views toward markets is part of a larger query into the formation of economic beliefs and attitudes as both the public and professional economists have a wide range of views regarding the proper role of market forces and the extent of government intervention in market economies (Fuller and Geide-Stevenson 2007; Blendon et al. 1997; Alston et al. 1992). Our findings have implications for abstract matters such as understanding the origin of differing views of markets and for practical areas such as public policy. For instance, our findings of a positive correlation between religiosity and support for free markets and the widely recognized decline in religious affiliation and certain measures of religiosity (Pew 2015) may have implications for the social sustainability of capitalism as an economic structure. Pew (2015) reports, for example, that those unaffiliated with a religion rose from 16 to 24% between 2007 and 2014, as the percentage of Catholics and Protestants fell. Our own findings, reported below, indicate a massive movement out of organized religion (as measured by change from parental affiliation to respondent’s current affiliation). The change in the religious landscape is part of a cultural shift. Millennials, who as a group exhibit lower levels of religious affiliation (Pew 2015), are seemingly more accepting of socialism than capitalism (Rampell 2016) and, while age may attenuate the approval of socialist economic systems, the decline in the importance of religion may suggest the support for free-market capitalism may wane.",5
45,3,Eastern Economic Journal,02 January 2019,https://link.springer.com/article/10.1057/s41302-018-00132-1,"A Note on School Quality, Educational Attainment and the Wage Gap",June 2019,Srikanth Ramamurthy,Norman Sedgley,,Unknown,Male,Unknown,Male,"Significant progress had been made in closing the wage gap between blacks and whites in the USA. By 1940, the relative wage, measured by the black–white weekly earnings ratio for men aged 25–34, had risen to about 50%, and by 1980 it had risen to 80% (O’Neill 1990; Couch and Daly 2002). Progress in closing the wage gap ceased by 1980 and US census data suggests it actually widened since the beginning of the great recession. A key research question is the extent of the relative importance of premarket factors versus market-based discrimination in explaining the history of the wage gap. For example, empirical evidence suggests that, in addition to labor market discrimination, premarket factors such as the degree of school segregation, resources devoted to improving school quality, years of education, life-expectancy, as well as family background variables are important premarket determinants of wages (Card and Krueger 1992a, b; Maxwell 1994; Neal and Johnson 1996; Carneiro et al. 2005; Castello-Climent and Domenech 2008). However, even with similar input measures of school quality between schools with and without large minority populations, black students continue to have lower graduation rates from high school and are less likely to attend college (Cameron and Heckman 2001). In 2000, among adults 25 years and older, 88% of whites attained a high school diploma compared to 79% of blacks. Similarly, 28% of whites had received a Bachelor’s degree compared to 17% of blacks (US Census Bureau 2002). By 2013, 20% of black students completed a 4-year degree compared to 40% of white students (US Census Bureau 2013). It is important to note that educational attainment, while acknowledged as a significant factor in explaining the differences in labor market outcomes, is typically assumed to be an exogenous premarket factor. It is, therefore, independent of market-based discrimination as well as part of the wage gap unexplained by premarket experiences. This theoretical paper explores the link between the quality of education received and educational attainment in a framework where school choice is made with forward-looking expectations of labor market outcomes. In a related paper, Coate and Loury (1993) (henceforth, CL93) provide a model with endogenous skills development based on expected labor market discrimination. Our model is distinct from CL93 in that we model an equilibrium wage gap that results from statistical discrimination rather than a taste for discrimination. The mechanism at work in our paper is poor school quality, a likely result of residential segregation, and its impact on the incentive to invest in human capital. CL93 focus on the enforcement of antidiscrimination policy and the way enforcement impacts the incentives to attain skills. The papers are complimentary in the sense that the mechanism in one of the papers is likely to remain a concern even if that in the other is fully addressed. The two key features of our model are that (a) both quality of schooling and educational attainment serve as signals for productivity and (b) years of schooling is the individual’s decision variable and, hence, endogenously determined. The central result of the model is that, in the face of anticipated statistical discrimination, years of schooling is a positive function of school quality. In light of the stark differences in the quality of schools attended by black and white students in the USA, this result provides further insights into the black–white education gap, and, to an extent, the resulting wage gap.",1
45,3,Eastern Economic Journal,24 April 2019,https://link.springer.com/article/10.1057/s41302-019-00139-2,Measuring the Effect of Policy on the Demand for Menthol Cigarettes: Evidence from Household-Level Purchase Data,June 2019,Esteban Petruzzello,,,Male,Unknown,Unknown,Male,"Tobacco smoking is the leading cause of preventable death in the USA, killing nearly half a million Americans each year (Surgeon General’s 2014 Report). It is also responsible for substantial health-related economic losses, in terms of both medical cost and lost productivity (Xu et al. 2014). Additionally, secondhand smoke causes severe health problems for those exposed to it (Surgeon General’s 2014 Report). Policies at all levels of government are constantly being implemented to address this public health issue. At the federal level, the 2009 Family Smoking Prevention and Tobacco Control Act gave the Food and Drug Administration (FDA) the power to regulate the manufacture, distribution, and marketing of tobacco products in order to protect public health (FDA Tobacco Control Act of 2009a). With this authority, the FDA has banned flavored tobacco productsFootnote 1 effective September 2009 (with the exception of menthol) and is currently considering a complete ban on menthols, which account for 30% of the cigarette market [FDA Flavored Tobacco Fact Sheet], about 4 billion packs at a prices ranging from $5 to $10 in 2017, around 25 billion dollars per year on average (CDC fact sheet). The Act did create the Tobacco Products Scientific Advisory Committee (TPSAC, established in 2010), which analyzes the impact of menthol cigarettes on public health. In 2011, the TPSAC issued a report on the public health impact of menthol cigarettes. The document analyzed the population harm associated with the toxicity of the product, the intensity of its use, and its prevalence. The main conclusion of the report is that menthol cigarettes have a particularly adverse impact on public health in the USA and that their removal would be beneficial (TPSAC’s Report 2011). More specifically, the report states that it is biologically plausible that menthol makes cigarette smoking more addictive and that cessation is less likely to be successful among smokers of menthol cigarettes. Additionally, the report highlights the correlation between the availability of menthol cigarettes and regular smoking among youth and states that menthol cigarettes may contribute to the initiation and persistence of cigarette smoking as they may have a greater reinforcing effect. The FDA also received a document with the industry perspective on the public health impact of menthol cigarettes. That document states that there is no scientific basis to support the regulation of menthol cigarettes any differently than non-menthol cigarettes and that a ban on menthol cigarettes would result in significant countervailing effects, including a large illegal market with negative consequences for public health (The Industry Menthol Report 2001). The FDA is currently reviewing all of the available evidence (the Committee’s report and recommendation, the industry report, and reviews by external scientists) in its consideration of a potential ban. The Tobacco Control Act does not set a required deadline or timeline for the FDA to act on the recommendations provided by the Committee in the report. Some lawmakers have called on the FDA to ban menthols, New Jersey considered legislation banning menthols in early 2018, and the city of San Francisco has approved a menthol ban on May 2018. In November 2018, the FDA stated it will advance a Notice of Proposed Rulemaking that would seek to ban menthol in combustible tobacco products.Footnote 2 At the same time, various survey data suggest that menthol cigarettes may be intrinsically different from non-menthols. Some studies based on the Tobacco Use Supplements of the Current Population Survey have addressed the substitutability of menthol and non-menthol cigarettes. A question in the 2010 Tobacco Use Supplements of the Current Population Survey asked nearly 3000 menthol smokers: “If menthol cigarettes were no longer sold, which of the following would you most likely do?” The answers: 39% of menthol smokers said they would quit, 36% said they would switch to non-menthol cigarettes, and 8% said they would switch to another tobacco product (Hartman 2011). Tauras et al. (2010) examine previous Tobacco Use Supplements and report that smokers do not find menthol and non-menthol cigarettes to be close substitutes for one another. Moreover, they find that non-menthol cigarettes are less of a substitute for menthol cigarettes than vice versa. Levy et al. (2011) and Delnevo et al. (2010) analyze the same data and report that menthol smokers are more likely to make quitting attempts, but are less likely to quit. This paper contributes to the policy debate by investigating the impact of intervention policies (taxes and public smoking restrictions) on menthol cigarette smoking in light of a potential ban on menthol cigarettes being under consideration. I use detailed, high-frequency household-level purchase data for the 2006–2010 period to estimate demand models that take into account the addictive nature of smoking. My baseline setup consists of a cigarette demand model that captures one of the main features of addiction, reinforcement: greater past consumption of an addictive good raises current consumption. I also incorporate a second feature of addiction: the existence of withdrawal costs. When smokers’ nicotine levels in a given period are zero (or substantially lower than the habitual levels), they can experience a physical discomfort that affects their purchase decisions. I analyze this cost by estimating a specification of the model that captures the effect of temporary smoking cessation on the purchases of the following period. The high frequency and richness of these panel data are crucial for the characterization of these underlying addiction dynamics. I estimate the model for both menthol and non-menthol cigarettes and find the following results. First, demand for menthol cigarettes is less price sensitive than for non-menthols. On average, a price increase of one dollar reduces non-menthol consumption by about a pack per month, but only reduces menthol consumption by approximately half a pack. This suggests that public intervention in the form of higher cigarette taxes could be less successful at curbing menthol smoking.Footnote 3 Second, menthol cigarettes are less addictive than non-menthols, as evidenced by the estimations of the two features of addiction. The reinforcement effect for non-menthols is 30% (for example, a pack smoked in the past month accounts for 0.3 packs smoked in the current one), while it is only 20% for menthols. Moreover, for non-menthol smokers, the withdrawal cost is responsible for the purchase of 0.8 packs on the month following the temporary cessation, while it is only 0.3 for menthol smokers. These estimations are addressing the addiction to nicotine; menthol smokers can also be addicted to the menthol flavor, and my results are consistent with this possibility that has been suggested by clinical psychologists. Equipped with this basic framework, I then study the effectiveness of public smoking restrictions in curbing cigarette consumption. While these restrictions are primarily aimed at reducing exposure to secondhand smoke, they may also reduce the demand for cigarettes by creating an environment where smoking becomes increasingly more difficult (they increase the hassle cost associated with smoking) and also by shifting social norms. However, it could be the case that smokers maintain their cigarette consumption even in the presence of restrictions. My analysis takes advantage of the fact that there are states with no smoking restrictions during the 2006–2010 period and that the states that placed restrictions did so at different moments of time. This variability in time and space is exploited to identify the causal effect of smoking restrictions on cigarette purchases, controlling for time-invariant household characteristics and temporary shocks that are common across households. Specifically, households in states that lack smoking restrictions serve as controls for those in states that do have them. The results show that public smoking restrictions are ineffective in reducing menthol smoking. Notably, this is not the case for non-menthol cigarettes, as smoking restrictions reduce average non-menthol cigarette purchases by more than half a pack per month.Footnote 4 Lastly, I examine the determinants of smoking cessation. I estimate a discrete-time hazard model of the probability of cessation that includes both time-invariant household characteristics and time-varying determinants, controlling for duration dependence. I find that higher prices are associated with a higher probability of quitting, but only for non-menthol smokers. I also find that smoking restrictions do not affect the likelihood of quitting. These findings, together with the results provided above, indicate that current policies are ineffective in curbing menthol smoking and suggest that if policy-makers continue to pursue an overall reduction in smoking of menthol cigarettes, a complete ban might be required, though other policies and regulations, such as advertisements, education, and risk awareness, may also be helpful to curb menthol smoking. The rest of the paper is organized as follows. The next section provides a brief description of the data and a descriptive analysis of the main variables. Methodology and Results section introduces the baseline reinforcement model, discusses the differential impact of smoking restrictions, and presents the cessation model and results. The last two sections present additional findings, robustness checks, and the conclusion.",
45,3,Eastern Economic Journal,22 September 2018,https://link.springer.com/article/10.1057/s41302-018-0117-6,Shared Experience and Third-Party Redistribution,June 2019,David Chavanne,Kevin A. McCabe,Maria Pia Paganelli,Male,Male,Female,Mix,,
45,3,Eastern Economic Journal,30 August 2018,https://link.springer.com/article/10.1057/s41302-018-0112-y,Personality Traits and Low Wealth at Retirement,June 2019,Meryl Motika,,,,Unknown,Unknown,Mix,,
45,3,Eastern Economic Journal,15 March 2019,https://link.springer.com/article/10.1057/s41302-019-00137-4,"Domenico Delli Gatti, Giorgio Fagiolo, Mauro Gallegati, Matteo Richiardi and Alberto Russo (eds): Agent-Based Models in Economics: A Toolkit",June 2019,Jason M. Barr,,,Male,Unknown,Unknown,Male,"In the years after the financial meltdown of 2007, economists were wringing their hands about how they failed to predict both the crisis and the subsequent Great Recession (Colander et al. 2014). In the immediate aftermath, there were calls for new methodological approaches to macroeconomics (Farmer and Foley 2009; Krugman 2009). And, after a period of debate and discussion, the question of the appropriate methodology has, once again, subsided. For the most part, economists have returned to their standard workhorse of New Keynesian stochastic general equilibrium models (DSGE) (Galí 2018). But for those looking to understand alternative modeling methods, Agent-Based Models in Economics: A Toolkit, offers an excellent introduction on how to incorporate heterogeneous agents and institutions, networks, and learning into macroeconomic models.",
45,4,Eastern Economic Journal,19 August 2019,https://link.springer.com/article/10.1057/s41302-019-00143-6,The Macroeconomics of Pascal’s Wager,October 2019,Paul Shea,,,Male,Unknown,Unknown,Male,"In 1670, the French philosopher Blaise Pascal posthumously published his famous argument in support of a religious lifestyle in the text Pensees. Pascal’s Wager posits that if one is uncertain about the existence of an afterlife, then it is rational for her to choose to be religious. If she chooses to be religious and is wrong, then she faces no negative consequences after her death. If she chooses to be non-religious and is mistaken, however, then she forgoes salvation and instead risks eternal damnation. Several interesting criticisms have been made against Pascal’s Wager including that God does not care about the specific belief, only that it be sincerely held, that religion is not a choice, and that the probability of God existing is zero (Hajek 2003). The general idea is, however, reasonable from the perspective of a rational expected utility-maximizing agent.Footnote 1 From a macroeconomic perspective, Pascal’s Wager has one important flaw: the idea that eternal damnation (including the opportunity cost of salvation) results in an infinite welfare loss. Continuity of preferences suggests that a rational agent would be willing to swim in a lake of molten hellfire, provided that it causes no lasting damage, for a very small but positive amount of time in exchange for some earthly pleasure. The instantaneous welfare loss of hell is thus finite. Another example comes from Dante’s The Divine Comedy, arguably the most famous depiction of hell. Dante’s Inferno includes nine, increasingly unpleasant, circles of hell, which requires that at least the first eight result in finite disutility. If agents use a discount factor less than one, then hell is an infinite geometric series that converges to a finite level of disutility which is increasing in the discount factor. Although there are other arguments for why the disutility of hell is finite (Hajek 2003), I am unaware of any prior work that bases this argument on discounting. This paper adds Pascal’s Wager to a very long-run growth model where time allocated to religion reduces the time that is available for production, and where agents’ preferences evolve based on a genetic learning algorithm.Footnote 2 I then examine the joint determination of religion, wealth, and agents’ discount factors. The main result is that that agents choose a level of religion greater than the amount which, all else equal, maximizes their evolutionary fitness. Furthermore, the resulting discount factor is less than the value that maximizes evolutionary fitness in a model without religion. The model is best suited for a pre-industrial economy. In such a setting, standard theory suggests that evolution favors the prosperous. Skjaervo et al. (2011) write that “wealth and status covary with lifetime reproductive success in pre-industrial human populations.” Skjaervo and Roskaft (2015) further find that wealth promotes fitness primarily by making widowed spouses more likely to remarry. Malthusian theory posits that increases in wealth led to increases in population that allowed per capita income to remain stable in the long term. Although there has been much speculation that this relationship may no longer hold in modern societies, Nettle and Pollet (2008) show that while more education reduces the number of offspring, increased wealth continues to result in more offspring. Religion poses obvious economic costs. Time devoted to worship, prayer, preaching, etc., could be used to produce and thus increase one’s genetic fitness. Churches could be converted into farms or factories, and animals could be bred or consumed instead of being used for sacrifice. Why evolution has not therefore selected less religious agents is a puzzle that has attracted the interest of the public, biologists, and economists. A major literature proposes that evolution does directly select more religious agents and seeks to identify subtle evolutionary benefits of religion that trump the obvious costs. Much of this literature is in evolutionary biology. Hamer (2005) suggests that the evolutionary benefits of religion may include reduced stress, increased optimism, and the ability to overcome setbacks.Footnote 3 His work resulted in a Time Magazine cover story titled “the God Gene.”Footnote 4 Hill and Pargament (2003) argue that religion may improve both physical and mental health. Other arguments consider group selection, where religion increases social cohesiveness or deters disruptive behavior.Footnote 5 Stark (1997) , for example, proposes that religion may act as a method of community risk sharing. Economics has also proposed scenarios where religion may yield net economic benefits. Barro (2004) writes that religion may be seen as “spiritual capital,” which encourages attributes “such as honesty, work ethic, thrift, and openness to strangers.” Berman (2000) develops a model where high levels of religion “signal commitment to the community, which provides mutual insurance to members.” Iannaccone (1992) models religion as a “club good” where seemingly irrational rituals and behavior serve to prevent individuals from joining who then choose minimal involvement. Berman and Iannaccone (2006) suggest that religious sects may efficiently provide social services when governments fail. In his survey of the economics of religion, Iannaccone (1998) notes that many religions prohibit unhealthy behaviors such as drinking, smoking, and the use of illicit drugs. In an empirical study, MacCulloch and Pezzini (2010) find that religion may reduce the propensity for revolutionary tastes. This paper’s results are compatible with all of these theories which suggest a direct evolutionary advantage to religion. “Simulations section” considers a version of the model where religion enters the production function, thus directly benefiting wealth and genetic fitness. The mechanism of this paper, where patience and religiosity coevolve, then causes additional religious activity beyond the level which maximizes wealth and fitness. I rely on a genetic learning algorithm that determines only one parameter, the discount factor, and does not directly consider religion. The wealthier the agent, the more likely that she successfully reproduces. I use a simple growth model where agents maximize their utility, not their evolutionary fitness. In addition to choosing their savings rate, agents also choose how much of their time to devote to religious activities. More religion reduces the time that an agent may spend producing and therefore reduces their wealth and probability of reproducing. More religion also reduces the perceived probability of eternal damnation, but this has no direct effect on reproduction. As in most growth models, a higher discount factor induces a higher savings rate that, all else equal, increases wealth. If the average discount factor is initially very low, then the discounted value of everlasting hellfire is also low and the population will choose a low level of religion. Low discount factors deter savings and wealth accumulation, and the net economic benefits of a higher discount factor are positive. Through reproduction, mutation and other operators, the genetic algorithm selects agents with higher discount factors and more wealth. As the average discount factor increases, however, so does the discounted cost of hell. Agents thus choose more religion even though, all else equal, religion reduces evolutionary fitness. Crucially, the genetic learning algorithm is not selecting more religion, it is selecting a higher discount factor. Increased religion is a side effect. Gould and Lewontin (1979) refer to this type of result as a “spandrel,” defined as a by-product of evolution rather than a direct result of it. The dynamics of the model also work in the opposite direction. Suppose that all agents have discount factors just below one. The cost of hell is therefore exceptionally large. Agents rationally choose to devote most of their time to religion, and their wealth and genetic fitness suffer. In this case, the genetic learning algorithm selects a lower discount factor. A society’s religious fervor depends on both the strength of its convictions and the consequences of disbelief. Increasing either parameter increases the equilibrium level of religion and reduces the average level of wealth. The genetic algorithm thus selects a lower average discount factor, which mitigates the economic costs of heightened religiosity. The model thus predicts that more religious societies exhibit less patience and less wealth once convergence has occurred. I also consider the impact of a shock to religious beliefs. When they become less intense, agents respond by immediately spending less time on religion. Afterward, the genetic algorithm selects for more patient agents, which then allows religion to partially recover toward its initial levels. The short-run impact on religious activity thus overshoots the ling run effect. The paper is organized as follows. “A Simple Growth Model” section develops a simple growth model. “Selection and the Genetic Learning Algorithm” section develops the genetic learning algorithm. “Simulations section” discusses simulations that demonstrate the model’s dynamics. “Conclusions and Extensions” section discusses potential extensions and concludes.",
45,4,Eastern Economic Journal,06 August 2019,https://link.springer.com/article/10.1057/s41302-019-00141-8,"Long-Run Expectations, Learning and the US Housing Market",October 2019,Daniel L. Tortorice,,,Male,Unknown,Unknown,Male,"Given the recent boom and bust in housing markets, there is renewed interest in understanding the determinates of US house prices. In this paper I begin by noting that housing market data present several puzzles that are present in asset market data more broadly, especially equities. First, there is evidence of excess volatility in house prices. The standard deviation of the price-to-rent ratio is 15% and the standard deviation of housing returns is 6%, while the standard deviation of the underlying housing rents is only 2.3%. Second, housing returns are significantly positively autocorrelated. Finally, the price-to-rent ratio is negatively correlated with future returns and rent growth and housing returns show evidence of time-varying volatility. While housing as an asset is in many ways different than equities, the similarities between these empirical facts and facts in the aggregate US equity market motivate me to examine the equilibrium housing price in a general equilibrium asset pricing framework.Footnote 1,Footnote 2 I begin with a standard consumption-based asset pricing model and use a log-linear approximation to the Euler equation as in Campbell (1993) and Restoy and Weil (2011) to solve for the equilibrium house price. The equilibrium house price then depends on future expectations of fundamentals (housing preferences) and consumption. I show that this model is unable to explain the facts outlined in the previous paragraph. I then examine the ability of two assumptions to bring the rational expectations model closer to the data. First, I allow for prices to adjust slowly to their fundamental value. Second, I assume the agent does not know the true model for housing preference shocks. Specifically, they are unsure whether preference shocks are permanent or temporary. They use a Bayesian learning model as in Cogley and Sargent (2005) to learn whether the preference process is trend stationary or difference stationary. Their beliefs change over time depending on how well each model fits the data. While the true process is (trend) stationary, the agent does not know this. He puts excessive weight on the difference stationary process, overreacting to temporary changes in market preferences. Two features of the housing preference processes make this learning significant. The first is the well-known fact that unit root and near unit root processes are very difficult to tell apart in small time series sample (Cochrane 1988; Stock 1991). As a result, the agent will almost always put some weight on the difference stationary process even if the true process is trend stationary. Additionally, after a random sequence of shocks which moves housing preferences away from their long-run trend the agent will put additional weight on the difference stationary model. Second, analogous to the analysis of the permanent income hypothesis (Deaton 1992) if the individual believes the true process is a unit root process, then shocks are permanent. As a result, they will react strongly to news about fundamentals and return volatility will increase. The sticky price assumption and the learning mechanism allow the model to better match the data, though the success is somewhat limited. Learning amplifies volatility over the rational expectations benchmark. I find that the learning model generates four times the amount of volatility in both housing returns and the HP-filtered house price as compared to the rational expectations model. On the other hand, the learning model does not amplify the standard deviation of the price-to-rent ratio over the rational expectations model, and the price-to-rent ratio is five times as volatile in the data than in the model. Sticky prices allow the model to match the autocorrelation of housing returns, though learning dampens this autocorrelation. The learning model improves more substantially over the rational expectations model when we examine the role the price-to-rent ratio has in predicting future returns and rent growth. Consistent with the data, learning creates a negative correlation between the price-to-rent ratio and future returns and rent growth. The rational expectations model predicts essentially a zero correlation between the price-to-rent ratio and future returns and a positive correlation between the price-to-rent ratio and future rent growth. Additionally, the learning model generates time-varying volatility as in the data. The learning model generates excessive kurtosis of returns. Data from the learning model are consistent with the positive autocorrelation of squared residuals from an AR(1) regression of returns and the estimation of GARCH effects in US data. The rational expectations variant of the model predicts these values should all be zero. The mechanism by which learning brings the model closer to the data is straightforward. First, since the house price is the present discounted value of future fundamentals (housing preferences), house price volatility is directly affected by the nature of the process for preferences. If preferences are trend stationary, then shocks to preferences have only a temporary effect on fundamentals and they quickly return to trend. As a result, the present discounted value of fundamentals does not change very much. On the other hand, if the true process is non-stationary, shocks have permanent effects, and the present discounted value of fundamentals responds more to the shocks. The result is that house prices are more volatile under the non-stationary process than the trend stationary process. Allowing the agent to entertain the possibility that fundamentals are non-stationary results in him reacting more strongly to shocks to fundamentals than in the rational expectations world where the process for fundamentals is trend stationary and the agent knows this. Consequently, learning is able to amplify volatility. This same mechanism explains why the model exhibits time-varying volatility. Since the agent’s probability weight on the non-stationary model varies over time, the magnitude of his reaction to shocks will vary over time as well, creating time-varying volatility. The learning mechanism is also able to generate return predictability with the price-to-rent ratio. This result occurs because agents are incorrectly forecasting future fundamentals. After a series of random but positive shocks to fundamentals, housing preferences drift away from trend. As a result, the agent puts more weight on the non-stationary model versus the stationary model. These beliefs lead him to overestimate the persistence of today’s positive fundamentals. As a result, the price rises and the price-to-rent ratio is high. But then fundamentals return to trend, leading the agent to revise down his belief in the non-stationary model, lowering the house price and leading to lower returns. As one can tell, the main goal of this paper is understanding changes in house prices. While the goal of better understanding house price volatility could be interesting in and of itself, there are also important practical reasons to study the question. As first emphasized by Shiller and Weiss (1999), the currently 20 trillion-dollar investment in residential housing is one that is very difficult to hedge. Few if any derivatives exist to mitigate the risk of house prices fluctuations. As a result, housing for many consumers remains a highly illiquid, geographically undiversified, leveraged investment. Understanding the drivers of house price volatility then helps us to understand the nature of risks that many households face on their largest source of wealth. Additionally, the financial system, especially the banking sector, is often exposed to house price risk through mortgage lending. A better understanding of house price volatility contributes to our understanding of the fragility of the financial system. Many papers have tried to explain the recent US housing boom and bust in rational expectations models using various institutional features and frictions in the housing market, like changes in down-payment requirements (Chu 2014; Chambers et al. 2009; Iacoviello and Pavan 2013; Corbae and Quintin 2015; Garriga and Schlagenhauf 2009; Titman et al. 2014; Chatterjee and Eyigungor 2009; Favilukis et al. 2017). While these models have considerable success explaining the volatility of house prices and returns, they generate return predictability in housing returns in an unsatisfactory way. To the extent there is return predictability, agents are aware of this predictability and therefore expectations of future returns are low when the price-to-rent ratio is high. This result is in contrast to expected returns in my model and data on survey expectations discussed below.Footnote 3 One of the first papers to examine data on house price expectations is Case and Shiller (2003). They find that home buyers have unrealistic expectations concerning future house price increases, predicting double digit increases annually over the next 10 years. Households also are unlikely to view housing as a risky investment. Case et al. (2012) replicate these results and argue that long-run expectations (10-year) and house price expectations are the primary driver of the house price boom. Piazzesi and Schneider (2009) argue for the presence of momentum traders in the housing market, agents who are always optimistic about price changes. Foote et al. (2012) present evidence that even industry financial analysts were bullish about house prices even at the 2006 house price peak. Based on this empirical evidence, many authors have examined the implication of relaxing rational expectations for house price dynamics. In fact, given the shortcomings of rational expectations model to match the volatility of house prices, Glaeser and Gyourko (2006) and Glaeser et al. (2008) argue that deviations from rational expectations and models of learning may be fruitful avenues of research as do Piazzesi and Schneider (2016). Glaeser and Nathanson (2017) study a model where agents neglect to consider the forecasts of other agents when forecasting future prices. Burnside et al. (2016) consider a model where, as in the current paper, learning about long-run fundamentals is essential. However, in their paper learning comes from social dynamics as opposed to observation of fundamentals. Bolt et al. (2014) generated boom and busts in house prices through an heterogeneous agent model where the agents endogenously switch between different price forecast rules. Adam et al. (2012) consider a model where agents learn about house price growth in an open economy model. Caines (2015) incorporates the Adam et al. model into a model where expectations affect both the demand and supply sides of housing. He finds that a substantial fall in the mortgage rate can replicate key fact of the recent US housing boom. Gelain and Lansing (2014) consider learning in an asset pricing-based model of housing where agents learn about rent growth using a misspecified model and extrapolative expectations. Similarly, Granziera and Kozicki (2015) explore the role of bubbles and extrapolation for explaining the volatility of house prices. All these models increase volatility of the price-to-rent ratio, and the Gelain and Lansing paper also generates predictability in house price returns. However, this paper differs from these in important ways. First, the paper seeks to endogenously explain both the predictability and the time-varying volatility in housing returns as well as amplifying volatility.Footnote 4 Secondly, I present a novel model of learning where agents are unsure about the true process for fundamentals and change their beliefs based on how accurately each model captures the data. As such, the model here provides a theoretical justification for extrapolative beliefs as opposed to simply assuming extrapolation exogenously. Finally, agents make significant mistakes about their long-run expectations (as opposed to their short-run expectations) consistent with the results in Case et al. (2012, 2014).Footnote 5 Finally, while it might seem simplistic to model housing in a purely asset pricing framework as opposed to a supply and demand framework, there is a large literature that takes the asset pricing approach.Footnote 6 For example, Piazzesi et al. (2007) model housing jointly as an asset and a consumption choice in an otherwise standard consumption-based asset pricing model. They show that housing increases the risk premium and predicts excess returns in equity markets. Lustig and Van Nieuwerburgh (2005) reach a similar conclusion in a model where housing is an important source of collateral. Flavin and Nakagawa (2008) explore how the illiquidity of housing influences the stochastic discount factor in a consumption-based asset pricing model. Ayuso and Restoy (2006) apply the asset pricing framework of Restoy and Weil (2011) and show that a large part of the fluctuations in Spanish house prices cannot be explained with observed fundamentals. The present paper uses the asset pricing framework of Restoy and Weil (2011) to explain house prices, but differs from the above papers by focusing on the volatility and predictability of housing returns and considering a learning-based model of expectation formation. The rest of the paper proceeds as follows. Section two discusses the data and the key empirical facts. Section three outlines the model and section four explains its calibration. Section five gives the main model results, and section six demonstrates the robustness of the results to alternative parameter specifications. Section seven concludes.",1
45,4,Eastern Economic Journal,14 May 2019,https://link.springer.com/article/10.1057/s41302-019-00140-9,The Effect of Foreign Direct Investment on Economic Growth,October 2019,Sergey Kondyan,Karine Yenokyan,,Male,Female,Unknown,Mix,,
45,4,Eastern Economic Journal,24 September 2018,https://link.springer.com/article/10.1057/s41302-018-0114-9,Exploring the Social-Architecture Model,October 2019,Alan G. Isaac,,,Male,Unknown,Unknown,Male,"This paper explores and improves the social-architecture model—an agent-based macromodel that emphasizes the role of constraints over the role of choice. This emphasis aligns the model with the zero-intelligence-trader models of Becker (1962) and Gode and Sunder (1993). It also reflects the influence on Wright (2009) of the methods of econophysics, which is the economic study of the statistical equilibria that emerge from the weakly structured interactions of large numbers of heterogeneous agents (Yakovenko and Rosser 2009; Jovanovic and Schinckus 2013). These traditions favor imposing mild distributional assumptions on behavior subject to uncontroversial constraints (such as budget constraints), seeking thereby to minimize the dependence of model predictions on specific behavioral theories. Despite the lapse of more than half a century since Becker’s paper and a spate of papers and books in the past two decades, behaviorally agnostic approaches to economic modeling remain novel to many economists. The social-architecture model of Wright (2009) marries the desire for behavioral agnosticism to the concerns of macroeconomics. It is an agent-based macromodel of monetary exchange and labor market allocation. Drawing on the research on zero-intelligence agents (Gode and Sunder 1993; Farmer et al. 2005; Othman 2008; Wright 2008; Cottrell et al. 2009; Ladley 2012), macroeconomic outcomes in the social-architecture model emerge from the interactions of many weakly coordinated and behaviorally volatile individuals. These agents are constrained by the institutional structure in which they interact, including search-based labor markets and institutions of monetary exchange. In contrast to mainstream macromodels, the social-architecture model does not impose an equilibrium-trading constraint on agents, nor do agents embody unbounded cognitive abilities.Footnote 1 New approaches to macroeconomics generally emerge tentatively and partially formed; they do not step forth fully armored from the head of Zeus. In this paper, we explore some of the strengths and weaknesses of the social-architecture model. This exploration is intended to produce friendly amendments to the model. In the process, we touch upon a few methodological issues. For example, we attempt to expose some tensions between behavioral agnosticism and the concerns of macroeconomics, which leads us to questions about the methodological justification of some of the model’s key assumptions and simplifications. (The term behavioral agnosticism is explained below.) However, our core focus is on the model, its implementation, and its purported successes in encounters with the data. This leads us to some difficulties with the original formulation of the social-architecture model, and we discuss how to mitigate these difficulties. In particular, we expose an unemployment problem in the social-architecture model, and we propose a mitigation. We also demonstrate that constituents of the model that might reasonably be presumed to be essential in fact have little effect on the model’s results. In agent-based macromodels, the evolution of the macroeconomy emerges from the interactions of many heterogeneous agents. Agent heterogeneity is a fundamental feature of these models; there is no “representative” agent. For example, in any period some agents may be unemployed, some may be employed, and some may be employers. Individual-level (microeconomic) interactions and the global (macroeconomic) state of the economy influence the evolution of the employment status of each agent. Since individual-level interactions affect individual outcomes, the history of each agent is fundamentally idiosyncratic. Nevertheless, we may detect persistent structure in the resulting macroeconomy, often in the form of stylized facts about statistical equilibria. Researchers have repeatedly demonstrated that unpredictable microlevel interactions can produce predictable macrolevel regularities. In the 1990s, Gene Stanley and other physicists applied the tools of statistical mechanics to economics, a method now known as econophysics.Footnote 2 Just after the turn of the century, econophysicists achieved unequivocal recognition by the mainstream economics profession (Gabaix et al. 2006). Their work has two core methodological emphases: an empirical search for stylized economic facts (in the form of stable statistical distributions), and related theoretical attempts to produce similar market-wide or economy-wide outcomes from the interactions of many heterogeneous and individually unpredictable agents. Their models eschew the standard theoretical toolkit of mainstream economists. In particular, they discard the assumption of essentially homogeneous, informed, substantively rational agents in continuous equilibrium. Instead, agents are fundamentally heterogeneous, and the mainstream reliance on imposed mechanical equilibrium is discarded in favor of emergent statistical equilibria (Yakovenko and Rosser 2009). Around the same time, economists displayed increasing interest in agent-based economics and behavioral economics, which also discard such mainstream assumptions. Agent-based social science blossomed with the publication of the monographs of Epstein and Axtell (1996) and Axelrod (1984, 1997). As in the econophysics literature, the agent-based literature evinced relatively little interest in characterizing the psychological state of individual agents. In striking contrast, behavioral economists—heavily influenced by the work of Kahneman and Tversky—were intensely interested in achieving a more realistic understanding of individual agents. Despite these deep differences, there are also important similarities. Agent-based economics and behavioral economics both reject the substantive rationality and equilibrium-trading constraints imposed on the agents in typical mainstream macroeconomic models.Footnote 3 By substantively rational agents, we mean agents who are computationally unlimited yet act exclusively in response to the constraints and payoffs that can be explicitly characterized by modelers. In saying that agents face equilibrium-trading constraints, we mean that agents are unable to act in response to disequilibrium signals in the economy. (For example, all trades must take place at market-clearing prices.) Additionally, both groups are inclined to agree with Lachmann (1943) that expectations “have to be regarded as economically indeterminate,” so that efforts to link subjective and unobservable expectations to objective and observable economic data is quixotic at best. The growth of agent-based economics and behavioral economics has focused attention on the emergent outcomes when interacting heterogeneous agents follow rule-of-thumb behaviors. Literature in this vein includes the research on zero-intelligence agents, which traces back to Becker (1962) but became truly influential with the work of Gode and Sunder (1993).Footnote 4 In such models, budget constraints and institutional features are more important determinants of aggregate outcomes than individual perceptions and goals.Footnote 5 Indeed, researchers who embrace behavioral agnosticism often model agents as choosing randomly among their feasible behaviors. Underpinning this approach is a suggestion that fundamental heterogeneity in the environments and constraints faced by individuals implies that systemic outcomes diverge from those implied by a representative agent (Aoki 2002). Researchers found that interactions between heterogeneous, weakly coordinated, unpredictable individuals can produce stochastic equilibria that display macrolevel regularities in the form of distributions over possible macrolevel outcomes. Economists calling for more attention to the characteristics of such stochastic equilibria include Steindl (1965), Foley (1994), Aoki (1996, 2002), and Mirowski (2001). Attempts to apply insights from statistical mechanics to the concerns of macroeconomics are often traced to the monetary models of Drǎgulescu and Yakovenko (2000).Footnote 6 This line of research proposes that a statistical treatment of monetary exchange can improve our understanding of actual macroeconomic outcomes. In the present paper, we explore a more recent and more explicitly macroeconomic example from this literature: the social-architecture model of Wright (2009). Wright argues that zero-intelligence agents can provide useful implicit microfoundations for macroeconomic modeling. In stark contrast to the usual methods of mainstream macroeconomic theory, implicit microfoundations impose neither rationality nor equilibrium. Wright (2009) finds that the social-architecture model produces distributions over a variety of macroeconomic variables that bear tantalizing resemblances to those exhibited by real economies. (We explore some examples below.) Instead of characterizing individual behavior in terms of substantively rational optimization, the social-architecture model turns to simple behavioral rules and allows for substantial behavioral randomness. This modeling strategy does not embody a claim that real individuals ignore goals or fail to formulate them. Rather, it is a behavioral agnosticism that acknowledges that our understanding of the individuals in a macroeconomy is too vague and incomplete to support speculation about their idiosyncratic goals and particular circumstances. Using randomness to represent our ignorance is a standard practice. Modeling consumer behavior is far more complex than modeling a role of a single die in carefully controlled circumstances, yet a simple stochastic model is usually the first choice model of die rolling. It is implausible that consumer behavior is better understood than die rolling. Yet mainstream macroeconomic models attempt to characterize all relevant preferences and constraints and then invoke cognitively superhuman preference optimization in order to uniquely determine consumer behavior. Such explicit microfoundations appear epistemologically presumptuous when contrasted with the epistemological modesty of behavioral agnosticism. Behavioral agnosticism eschews the “pretence of knowledge” embodied in characterizing individuals as essentially homogeneous and readily comprehensible. Additionally, agent-based models typically do not impose market equilibrium. Instead, as in the real world, market exchange takes place in real time by a process of discovery that involves out-of-equilibrium trades. In contrast, exchange in mainstream macroeconomic models is equilibrium-constrained, effectively treating price discovery as transpiring in imaginary time. For the purposes of the present paper, the social-architecture model has certain crucial features: replication is ensured by the availability of the source code, and its choice of methods allows us to explore the compatibility between the concerns of macroeconomists and the goal of behavioral agnosticism. Since Wright (2009) discusses model details and provides an implementation in code, we only briefly review the model details.Footnote 7 The social-architecture model is an agent-based computational macromodel of a monetary economy. There are no exogenously imposed agent types, yet different agents may be in fundamentally different states. (For example, an agent may become an employer, an employee, or unemployed.) An agent is essentially characterized by its attribute values and a few behaviors that are defined in the context of an economy. The key behaviors are job search, consumption, labor, and management. An agent’s state comprises the values of five core attributes: money (the transactions medium and unique store of value), an employer identifier, the value of the last wage received, a reservation wage (which determines which job offers will be accepted), and a list of employee identifiers. Figure 1 illustrates this basic summary of the nature of an agent.Footnote 8 Each agent behavior transforms the economy to a new state, which is indicated by having an economy both as the input and output of each behavior. (An economy is a collection of agents plus a macroeconomic state, as discussed below.) Attributes and behaviors of an agent To support this structure, we provide each agent with a unique identifier. This is an index on the closed integer interval [1..N], where N is the number of agents. The set of employees is empty if the agent is not an employer. An agent is unemployed if it has no employer and has no employees.Footnote 9 At each point in time, an agent can be in one of three different employment states: unemployed, employed, or employing. We will refer to an agent as a jobless agent, a worker, or an employer based on this employment state. This class structure is endogenous and can vary over time, which connects the model to the work of Roemer (1982) and its ramifications such as Eswaran and Kotwal (1986) or Henderson and Isaac (2017). However, class in the social-architecture model is not strictly endowment-based, and it evolves dynamically. An agent may transition from being unemployed to being a worker or even to being an employer. Aspects of the social-architecture model recall labor-focused macromodels, particularly those in the tradition of Mortensen (1970) that emphasize search unemployment. Entrepreneurs play a fundamental role in organizing production by providing working capital and hiring employees. Employment matches are determined by random search. Jobless agents search to find a position. In any period, a jobless agent may fail to find a match, thereupon remaining jobless for another period. The reservation wage falls during jobless spells, in accord with the empirical evidence (Addison et al. 2013; Brown and Taylor 2013). Agents search for positions by sampling employers. This recalls Mortensen (1970), where “the supply of labor available to a particular establishment in a given period is limited to the stock of its own employees plus the flow of those unemployed participants who happen to inquire about job vacancies.” However, the social-architecture model more realistically allows the employed to engage in job search. Employed agents may search in an attempt to improve their position; a worker need not leave a job in order to search for a new one. The state of an economy comprises its microstate (that is, the state of each agent) and its macrostate. In the social-architecture model, there is a single macroeconomic attribute: the level of latent demand in the economy. (See the discussion of firm revenues, below.) All other macro characteristics are determined by aggregation over the microstate. The timescale of simulation is 1 month per iteration. The evolution rule for the model produces a new state of the economy each month by sequentially updating a collection of agents. Each of these agents sequentially applies the following behaviors (if applicable): job search, consumer spending (which adds to latent demand), laboring (which generates firm revenue, depending on latent demand), and firm management (which includes wage payment and job-separation decisions). Such behaviors naturally change the agent’s state, but they also affect other agents and the macrostate. The outcome is a new state of the economy (including a new state for that agent and a new level of latent demand). Job search is a behavior of workers and the unemployed, who search for a potential match and accept a position whenever the wage offered exceeds the reservation wage. (Employers do not seek to become employees.) An agent may search for a better (that is, higher paying) match with any firm, even with the current employer. Whenever a jobless agent’s search proves fruitless, that agent’s reservation wage falls. If a worker finds a position that offers a better match (that is, a higher wage), the agent’s reservation wage rises to the new wage level. Job-search behavior produces an updated economy, which can include adjustments to wage expectations. An agent’s reservation wage rises with an accepted offer or falls during unemployment spells. This means that a worker’s reservation wage will ratchet upward until the worker faces unemployment, at which point it starts to decline.Footnote 10 No workers are fired at this stage—that happens during the firm-management stage—but an unemployed worker may remain unemployed. We may summarize job-search behavior as follows:Footnote 11 
Search:
 if the agent is an employer, do nothing; otherwise: randomly pick a potential employer; if there are no potential employers, do nothing; otherwise: negotiate a wage offer with the potential employer; compare the wage offer (\(w_o\)) with the reservation wage (\(w_d\)) and accept the wage offer if it is adequate; adjust the reservation wage (according to Table 1). Although the social-architecture model emphasizes random behavior subject to constraints, job search includes non-random behaviors. Most obviously, employed and unemployed agents always engage in job search, and they only accept offers that exceed their reservation wages. This means that the model incorporates wage-ladder effects, where an agent who remains employed experiences upward wage ratcheting. This role of the reservation wage is a virtue of the model.Footnote 12 A desire to incorporate such realistic details suggests limits on how much macroeconomists can aspire to a strict behavioral agnosticism. Even in the social-architecture model, which takes behavioral agnosticism extremely seriously, the microfoundations are not entirely implicit. Each recourse to an economic specification of behavior (beyond raw randomness subject to constraints) may be considered to be a methodological deviation from pure behavioral agnosticism. Our exploration of job-search behavior suggests that such deviations will often be desirable. As a practical matter, modelers will find that an emphasis on constraints and institutions does not entail complete liberation from the need to think economically about behavior. Even for researchers inclined toward behavioral agnosticism, modeling decisions must be judged pragmatically: which aspects of behavior can we usefully neglect, and which must we attend to in some detail? Once we introduce the criterion of usefulness, we must debate methods on the same pragmatic terrain as other macroeconomists. While it is a commonplace that macroeconomic models must be unrealistic, we must nevertheless ask whether any particular lapse of realism promotes our research goals or proves fatal to them. (As the saying goes, “all models are wrong but some are useful.”) Job-search behavior involves subroutines for employer selection, wage negotiation, reservation-wage adjustment, and firm affiliation.Footnote 13 The first three of these subroutines specify key behavioral assumptions of the model. In particular, we are confronted with the following questions: how do agents search for positions, what does wage negotiation look like, and what influences an agent’s reservation wage? Although our answers must in some sense be transparent when implemented in code, the social-architecture model intentionally leaves the answers to such questions relatively opaque at the level of individual goals and constraints. That is to say, whenever plausible, the social-architecture model relies heavily on randomness in the description of behavior. In the pursuit of behavioral agnosticism, the model relies heavily on the standard uniform distribution. This is often justified as a reliance on Bernoulli’s principle of insufficient reason. Even so, and even within its uncontroversial constraints (for example, budget constraints), behavior in the social-architecture model is not purely random. Consider a few examples of behavioral assumptions of the model that raise theoretical questions. A firm with more working capital is more likely to be chosen as a potential employer. Wage negotiations lead to offers that are (uniformly) between one and two times the job seeker’s reservation wage. The reservation wage rises in response to a good offer but falls in response to unemployment. In pointing out that such assumptions are behavioral, we in no way wish to imply that they are poor choices. Rather, we wish to document such drifts away from purely statistical reasoning toward economic reasoning in order to provide a starting point for a more explicit discussion of the potential for marrying behavioral agnosticism with the interests of macroeconomists. Such behavioral assumptions reflect a compromise between the goals of parsimony, plausibility, and performance. Such compromises are coextensive with the compromises of any economic theorizing. One virtue of agent-based modeling is that maintaining stock-flow consistency becomes almost trivial, since model transactions are generally explicit. In the social-architecture model, all transactions involve the unique medium of exchange. All expenditures require money, and all firm revenues are the receipt of money. Such a cash-in-advance constraint on transactions is common in the econophysics tradition and in the mainstream macroeconomics literature (Clower 1967; Lucas 1980; Lucas and Stokey 1987; Drǎgulescu and Yakovenko 2000).Footnote 14 In an interesting deviation from the standard theoretical treatments, however, the social-architecture model allows some temporal slippage between expenditures and receipts. These are mediated by a macroeconomic state variable, which we call latent demand.Footnote 15 As a result, expenditure decisions may not immediately produce receipts for firms: expenditure contributes directly to latent demand, but firm revenues accrue from effective demand.Footnote 16 One attraction of this approach lies in its support for motivational agnosticism. Specifically, it allows us to avoid speculation about how a consumer will chose a particular firm for particular consumption expenditures. The introduction of latent demand allows us to discard the counterfactual assumption—common in many macromodels—that a consumer faced with unchanging circumstances (including relative prices) will continually purchase an unchanging bundle of goods and services. The social-architecture model goes even further than this: it does not attempt to characterize each consumer’s diachronic allocation of expenditure among available options. Contrast this with complex shopping models and evolving consumer-firm networks, as found in the agent-based models of Gaffeo et al. (2008), Neveu (2013), or Ashraf et al. (2017). The use of latent demand offers a gain in realism as well as simplicity. Realistically, a consumer’s discretionary expenditures may be big or small from month to month, and may be allocated to a few firms one month and many firms the next. As we should expect from our previous methodological discussion, the social-architecture model shies away from specifying the details of how each consumer allocates expenditures among firms. The latent-demand modeling strategy proves simple and attractive. At the level of the individual consumer, only total spending is explicitly characterized. Each period, a consumer’s expenditures are differently allocated among the goods and services produced by existing firms, but this is mediated by latent demand. We may summarize a consumer’s expenditure behavior as follows. Consume: set expenditure to a random fraction of wealth decrement wealth by the amount of the expenditure increment latent demand by the amount of the expenditure Next, a firm-revenue rule determines how latent demand becomes effective demand: some portion of latent demand is realized as revenue by a firm. Workers are agents of firms; they generate the firm’s revenue (for example, via sales effort). Except for the unemployed, each agent has a shot at garnering a portion of latent demand for a firm. The resulting allocation of effective demand is thereby employment-weighted. Since large firms have proportionally greater revenue generating capacity, the social-architecture model imposes no ex ante size limitation on firms. The firm-revenue rule can be summarized as follows. 
Hustle:
 if the agent is unemployed, do nothing (no revenue is generated); otherwise: set effective demand to a random fraction of latent demand augment the wealth of the firm owner by effective demand decrement latent demand by the allocated effective demand The final agent-processing stage characterizes firm-management behavior: wage payment, or employee firing. Employers pay employees in the order hired; employees are let go if there is not enough money to pay them. (They are paid nothing at all in this case.) If all employees depart, the firm dies, and the employer enters the ranks of the unemployed. This firm-management behavior brings us again to the tension between behavioral agnosticism and economic theorizing. How agnostic is it to pay employees in the order hired? To reiterate, such a question need not imply a criticism of the assumption, which seems reasonably descriptive (Foulkes 1980). Rather, such questions highlight the tension between implicit theorizing and the desire for behavioral agnosticism. Would it be more behaviorally agnostic to pay employees in random order? Would it be less plausible to first fire the most expensive of these equally qualified (in the expected-revenue sense) individuals? Or should workers be let go without regard to their previous tenure, as in Neveu (2013)? We find that the goal of behavioral agnosticism is simply too vague to provide substantial guidance at crucial points in model formation. It is at such points that the methodological guidance of mainstream microfoundations—which discard relatively vague notions of behavioral plausibility and conformance to stylized facts in favor of the implausibly optimal pursuit of counterfactually stable goals subject to an unrealistically precise characterization of current conditions—may seem a relief. Setting aside these concerns for the moment, we summarize firm-management behavior in the social-architecture model as follows. 
Manage:
 if the agent is not an employer, do nothing; otherwise: iterate through employee list (in order hired) and pay wages owed as long as working capital is not exhausted;Footnote 17 employees that the firm cannot pay become unemployed. These four agent-processing stages—job-search behavior, spending behavior, revenue generation, and firm-management behavior—determine all changes in the state of the economy. Call this sequence of actions the agent schedule. We must process agents sequentially, as explained below, and each agent always follows the same schedule. From a functional-programming perspective, the agent schedule is the composition of the four agent-processing stages, which Fig. 1 lists as agent behaviors. Recall that the timescale of our simulation is 1 month per iteration. We therefore call the evolution rule for the economy the one-month rule. We implement one-month rule as follows. The agent schedule effectively takes two arguments, an economy and an agent, and returns an updated economy. Therefore, given an economy and a list of agents, we can fold the processing rule over the list of agents to produce the new end-of-month economy. Each month, we update the economy by applying the agent schedule to N agents.Footnote 18 Since agents are interdependent, they must be processed sequentially, each in the context of an updated economy. For example, the consumption expenditure of an employee depends on the wage payment of the employer. Similarly, the behavior of an employee will generally change the state of the employer. The one-month rule is our core state-transition rule for the economy, from which we can build up trajectories of arbitrary length. For example, given an initial state of the economy, we can produce a trajectory for one year by means of 12 repeated applications of the one-month rule. Correspondingly, one simulation run constructs a trajectory of economies from a given initial economy. We specify the length of the simulation in years. When we want an annual description of a trajectories, we use the last month of each year for stock variables, and changes from end-of-year to end-of-year for flow variables. Macroeconomic data are for the most part produced by aggregating over the microstate. However, the model also includes a purely macroeconomic variable that cannot be produced by aggregation of the microstate: latent demand. (Recall that latent demand is spending that has been chosen by consumers but not yet allocated to firms.)",
45,4,Eastern Economic Journal,12 August 2019,https://link.springer.com/article/10.1057/s41302-019-00146-3,Destabilizing Balance Sheet Effects in the New Consensus Model,October 2019,Emiliano Libman,,,Male,Unknown,Unknown,Male,"Nowadays, a flexible exchange rate regime combined with Inflation Targeting is regarded as the best policy option for small open economies. It is often suggested that the capital account should remain open and that a short-term nominal interest rate should become the dominant policy tool. Additionally, fiscal and income policies should not be used to achieve macroeconomic stability. Perhaps the most surprising fact is that this extremely simple policy mix seems to work well, despite the turbulences associated with the 2007–2008 Subprime meltdown, and considering countries of very different income levels and economic structures. Central banks often implement strategic purchases and sales of foreign exchange, but these interventions are regarded as a small departure from a basic script, and the idea that adopting a sensible monetary policy alone is a powerful tool to achieve macroeconomic stability, remains in place. It was not until recently that the question of whether monetary policy should pay attention to financial stability issues has become a main concern for central banks (see Vredin 2015; Woodford 2012). In an open economy set-up, financial stability is closely related to the question whether the exchange rate should be allowed to float freely. There is also a growing recognition that monetary policy in the USA has sizeable spill-overs into other countries (see for example Rey 2016), and some economist from the IMF have suggested that monetary policy should be complemented with other policies (Ostry et al. 2012). However, the implications for macroeconomic stability of exchange rate fluctuations under of Inflation Targeting are relatively less explored.Footnote 1 Latin American countries provide interesting case studies. The region is well-known for his long record of high inflation and currency crises. Several countries have been using Inflation Targeting for more than 15 years (for instance Brazil, Chile, Colombia, Mexico, and Peru). The economic performance of Latin American countries with Inflation Targeting was far from perfect, but during the last years, these countries experienced relatively fast growth, declining poverty rates, and low and stable inflation (Moreno-Brid and Garry 2016). Even more impressive, the pass-through from exchange rates to prices has declined dramatically (International Monetary Fund 2016) and currency collapses were avoided. As it is documented for instance by Chang (2008) and by Céspedes et al. (2014), central banks in Latin American that pursue Inflation Targeting often departed from the main script. The administration of a short-term interest rate was combined with a reasonable dose of other policies, including interventions in the foreign exchange market, counter cyclical fiscal policies, income policies, and even some regulations of the capital account. This paper argues that the pragmatism of the central bankers in the region is no coincidence, once the relevant aspects of the economic structure of Latin-American countries are taken into account. Specifically, these countries are exposed to financial shocks and to important degrees of liability dollarization, while their net exports seem to be relatively price inelastic. In that context, a standard Taylor Rule combined with a flexible exchange rate may produce undesirable outcomes. To illustrate the point, we build a basic model to focus on the so-called balance sheet effect associated with exchange rate fluctuations. We then show that when such effect is important, exchange rate depreciationsFootnote 2 can exert a negative effect on output and employment and capital flows can behave in a destabilizing way. Post-Keynesians have always been suspicious of the idea of stabilizing output and inflation using monetary policy (Rochon and Setterfield 2007). It is possible to show why the New Consensus Model fails, using relatively simple models that are often used in the Post-Keynesian tradition (mostly for the purpose of criticism). For example, Libman (2018a) shows that when devaluations are contractionary, a rule that keeps the short-term interest rate fixed dominates a standard Taylor Rule. The main message is that monetary policy may not be the best policy tool to stabilize output and inflation. This paper also builds on a simple version of the New Consensus Model and proposes an extension that captures some of the features that characterize Latin American countries during the recent period: the reluctance of central banks to let the exchange rate to float freely. We achieve this by including the so-called balance sheet effect. This is a relatively standard extensions supported by the empirical literature, that it can be found for instance in IS-LM-BM “in the Pampas” (Céspedes et al. 2003). While these authors focus on a general set-up, we explore the dynamic in more details and we enrich the institutional aspects of model to incorporate a Taylor Rule and Inflation Targeting. Additionally, and following a recent literature that explores the connection between large exchange rate shocks and binding collateral constraints, we propose a simple extension of the main model to capture some of the asymmetries that often characterize monetary and exchange rate policies (such as fear of floating), as well as some boom and bust dynamics associated with capital flows. The main lesson of this paper is that using only the short-term interest as the main macroeconomic policy tool may be destabilizing, especially when the balance sheet effect is very strong and net exports are relatively price inelastic. Policies that avoid large exchange rate fluctuations, such as capital controls, and other tools to achieve stabilization, for instance income and fiscal policies should also be considered, and perhaps preferred than the mere manipulation of a short-term interest rate. This paper is structured as follows. After this introduction, “Literature Review” section discusses the related literature. “The Model” section presents the static block of the model, while “Dynamic Behavior” section explores the dynamics and stability issues, as well as some extensions of the basic framework. Finally, “Conclusions” section concludes.",
45,4,Eastern Economic Journal,20 August 2019,https://link.springer.com/article/10.1057/s41302-019-00147-2,Technology Gap and International Knowledge Transfer: New Evidence from the Operations of Multinational Corporations,October 2019,Nune Hovhannisyan,,,Female,Unknown,Unknown,Female,"There has been a significant increase in the levels of global trade in goods and services. Two components of this increase are noteworthy: Currently, global trade in ideas is reaching annual levels of $250 billion (World Development Indicators),Footnote 1 and trade in intermediate inputs comprises 57% of total trade in goods in OECD countries (Miroudot et al. 2009). The USA is a major seller of technology, accounting for around 50% of world royalties and license fee receipts (World Development Indicators), and trade in intermediate inputs in the U.S. accounts for half of total trade in goods (Miroudot et al. 2009). U.S. Multinational Corporations (MNC) are important conduits of technology transfer, with around two-thirds of royalties and license receipts coming from intra-firm transactions and approximately 60% of total trade within U.S. multinationals being trade in intermediate inputs (The U.S. Bureau of Economic Analysis). A MNC can transfer its technology to foreign affiliates in  disembodied form (know-how, industrial processes, computer software) or in embodied form (intermediate inputs). Flows of royalty and license receipts from affiliates to parents, for the use of intangible technology is evidence of disembodied technology transfer, while exports of goods for further processing from parents to affiliates can indicate embodied technology transfer. It is well known that technology transfer is an important determinant of long-term cross-country income, economic growth and convergence of countries. However, the mode of technology transfer in embodied versus disembodied form has a differential impact not only on access to current knowledge and economic growth, but also on innovation, economic welfare, and convergence. The history of the soft drink “Fanta,” which was invented by the German affiliate of the Coca-Cola Company, offers one example. Possessing the recipe for Coca-Cola but lacking all the required ingredients due to a shortage in World War II-era Germany, Coca-Cola Deutschland invented this new soft drink by using the only available ingredients instead. In addition, the mode of technology transfer might also affect the degree of knowledge spillovers from multinational affiliates to domestic firms, which improves the productivity of the latter.Footnote 2 What determines the mode of technology transfer within a MNC? This paper provides new evidence that the technology gap of U.S. MNC foreign affiliates, defined as their productivity compared to the productivity frontier, is associated with the decision of U.S. multinationals to export tangible goods versus intangible technology within the MNC. The example of Intel Corporation illustrates the hypothesis behind this paper. For 25 years, Intel Corporation has had plants in China where chips (intermediate goods) are shipped for assembly and testing. But in October 2010, the company announced the opening of a new wafer fabrication facility (fab) in China capable of using the blueprint to make the actual chips. At the same time, Intel announced the opening of a chip assembly factory in Vietnam (Takahashi 2010a, b). One of the reasons why Chinese affiliates of Intel Corporation currently receive technology in the form of blueprints while Vietnamese affiliates receive technology in the form of intermediate goods might be that the former are currently closer to the productivity frontier, while the latter are farther from the frontier. A panel data on the activities of U.S. multinationals in 46 host countries and across seven manufacturing industries are employed to analyze the relationship between the affiliate’s technology gap and the share of importing technology versus inputs. Focusing on the activities of U.S. MNCs is attractive as there is information on both the technology and input flows within firms. These data come from legally mandated benchmark surveys, conducted every 5 years by the Bureau of Economic Analysis (BEA), which enable the identification of U.S. parent-affiliate tangible and intangible technology transfers across FDI host countries and industries. The technology gap is measured as the deviation of the affiliate’s labor productivity from the parent productivity in the same industry and year. The main finding of this paper is that the technology gap is negatively related to the share of disembodied versus embodied technology transfer, with a 10% increase in the technology gap on average decreasing the share of licensing versus importing inputs by 1.5%. The significance of this paper stems from the realization that MNCs tend to share know-how with country affiliates that are more productive, but export intermediate goods to the less productive ones. The fact that affiliates which are far from the frontier receive technology in the form of goods and not disembodied ideas leads to policy implications that for developing less productive countries, the reduction in the technology gap would involve direct access to knowledge and ideas. This not only gives such countries access to current information, but also stimulates the creation of new knowledge which in itself is important for long-run economic growth and convergence. Possible channels through which developing countries can reduce their technology gap are through subsidizing research to build up their knowledge stocks, thus being able to absorb intangible ideas and investment in human capital.Footnote 3 The theory on multinational enterprises identifies horizontal and vertical directions for Foreign Direct Investment (FDI). Horizontal FDI arises when multinationals replicate their production in host countries to gain market access (Markusen 1984), whereas vertical FDI arises when different stages of production are fragmented to take advantage of differences in factor prices (Helpman 1984), intra-industry considerations (Alfaro and Charlton 2009), or international transaction costs (Keller and Yeaple 2013).Footnote 4 Country empirical studies have found that market sizes, country similarity, factor endowments, and barriers to trade are among the most important determinants of FDI, while country–industry studies find that these factors have a differential impact on FDI in various industries.Footnote 5 This paper contributes to the growing body of the literature on vertical production sharing within multinationals, where part of production takes place locally in affiliates while the other is imported from parents (Hanson et al. 2005; Fouquin et al. 2007; Keller and Yeaple 2013). Hanson and coauthors find that MNC foreign affiliate’s demand for imported inputs is higher in affiliate countries with lower trade costs, lower wages for less-skilled labor, and lower corporate income tax rates (Hanson et al. 2005). Keller and Yeaple (2013) formalize and empirically confirm that knowledge intensity is another important determinant for the location of intermediate input production, where it is more difficult to transfer technology in more knowledge-intensive industries. This paper differs from the work of Hanson and colleagues and Keller and Yeaple by employing a direct measure which differentiates between transfer of tangible intermediate inputs versus intangible technology from U.S. parents to affiliates. Some new literature has emphasized the importance of especially intangible technology transfer in contrast to goods transfer within vertical production sharing (Atalay et al. 2014; Ramondo et al. 2016; Cho 2018). A second body of the literature has documented the importance of productivity differences in subsidiaries of foreign companies for knowledge flows within MNCs.Footnote 6 Bjorn and coauthors find that the larger the technology gap, the more important the foreign parent as a source of codified knowledge, defined as patents, licenses and R&D (Björn et al. 2005). Their study used survey data for foreign firms in Eastern European countries, but did not include knowledge embodied in intermediate goods.Footnote 7 A related study by Driffield et al. (2010) finds that total factor productivity (TFP) of foreign affiliates in Italy is important for technology transfer from affiliates to parents (sourcing), but not important for technology transfer from parents to affiliates (exploiting). However, the survey used in this study is based on a binary response to whether there was transfer of scientific and technological knowledge from parent to affiliate, which does not distinguish between tangible (intermediate goods) and intangible (patents, licenses, software) forms. Using data on French multinationals, Fouquin et al. (2007) find that labor productivity of countries is positively associated with imported-input demand for affiliates in developed countries, but is negatively related for affiliates in developing countries. This paper adds to the first body of literature a relative measure of embodied and disembodied technology to empirical analysis of multinationals’ vertical production networks. In relation to the second body of literature, this paper explicitly identifies two forms of knowledge transfer within MNCs and highlights productivity differences of affiliates as an important factor in determining the mode of technology transfer. As the decision of transfer occurs within the firm, affiliate productivity may be endogenously determined by MNCs. This is addressed in the present study by using instrumental variables and various proxies for technology gap. Furthermore, across country and across year variation in labor productivity of affiliates of U.S. MNCs within the same manufacturing industry is used to identify not only the direction of the impact, but also parameter estimates. In addition, differences in industries are controlled for in multiple specifications. A limitation of this paper is the usage of aggregated country–industry level data due to inaccessibility of confidential firm-level data from the U.S. Bureau of Economic Analysis. The remainder of the paper is organized as follows. The next section highlights the theoretical foundation. “Empirical Methodology” section presents the empirical estimation strategy and discusses estimation issues. “Data” section details data sources, variable construction, and descriptive statistics. The results are presented in “Results” section. “Conclusions” section concludes.",1
46,1,Eastern Economic Journal,12 December 2019,https://link.springer.com/article/10.1057/s41302-019-00164-1,EEA Immigration Symposium Introduction,January 2020,Andrew J. Padovani,Cynthia Bansak,,Male,Female,Unknown,Mix,,
46,1,Eastern Economic Journal,19 August 2019,https://link.springer.com/article/10.1057/s41302-019-00142-7,September 11 and the Rise of Necessity Self-Employment Among Mexican Immigrants,January 2020,Chunbei Wang,Magnus Lofstrom,,Unknown,Male,Unknown,Male,"The September 11 terrorist attacks (9/11) have triggered an increase in anti-immigration sentiment and a tightening of immigration policies throughout the USA. At the federal level, while there has not been a comprehensive immigration reform, immigration enforcement has become much stricter, documented by sharply increased immigration raids and deportation across the country (Golash-Boza 2011). At the state and local levels, an increasing number of states and local governments have taken up immigration issues in their own hands to implement more stringent immigration laws such as the mandated use of E-Verify or adopting the 287(g) program.Footnote 1 Mexican immigrants, the largest undocumented immigrant population in the USA, have become the main target. Since these actions mainly target workplaces in the wage sector, studies find that they have negatively impacted undocumented immigrant’s job market outcomes such as wages and employment rates (Orrenius and Zavodny 2009; Amuedo-Dorantes and Bansak 2012, 2014; Bohn and Lofstrom 2013). Among recent literature that examines undocumented immigrants’ responses to such policy changes, Wang (2019) finds that, in response to the deteriorated job market opportunities, an increasing amount of undocumented immigrants (proxied by non-citizen Mexican immigrants) have become self-employed as an alternative to make a living after 9/11. The 9/11 event and the stricter immigration enforcement that followed not only increased the amount of self-employment among Mexican immigrants, but may have also dramatically changed the nature of self-employment among this group. This paper builds on Wang (2019) to examine the effect of 9/11 on the changes in the types of self-employment taken up by Mexican immigrants. Because they are more likely to be pushed into self-employment due to lack of wage-sector opportunities stemming from stricter immigration policy and increased discrimination, we hypothesize that the composition of self-employment among this group may switch toward more necessity self-employment and away from opportunity self-employment. The distinction between necessity and opportunity self-employment has gained popularity in the recent literature. Necessity self-employment is motivated by lack of options in the labor market and the need to make a living, whereas opportunity self-employment is motivated by profit opportunity. Such distinction helps better understand changes in business start-up rate and its relationship with the business cycle, the role self-employment plays in the economy, and the heterogeneity in self-employment performance. While the conceptual distinction has existed for a long time, its empirical measurement faces challenge because it requires information on the motivation of self-employment in the data. Fairlie and Fossen (2018) recently propose an operational measure to identify necessity vs. opportunity self-employment in nationally representative data such as the Current Population Survey (CPS). In particular, they distinguish the two types of self-employment based on prior work status: Necessity self-employment is defined as self-employment transitioned from unemployment, and opportunity self-employment as transitioned from wage-employment or not-in-the-labor-force status. They show that such distinction is “consistent with the standard theoretical economic model of entrepreneurship” and helps to reconcile the puzzling findings regarding the cyclicality of self-employment in the literature. These measures can be readily constructed using the CPS basic monthly data that are matched across any two consecutive months which provide information on month-to-month labor market transitions. This paper utilizes these recently developed definitions of self-employment to examine the impact of 9/11 on the changes in the nature of self-employment among Mexican immigrants. Using the difference-in-differences approach and data from CPS basic monthly data from 1996 to 2006, we find evidence that both necessity and opportunity self-employment rates have increased among Mexican immigrants after 9/11 compared to less-educated Whites (the control group). The magnitude of increase is much larger among necessity self-employment, consistent with the hypothesis that Mexican immigrants are more likely to be pushed into self-employment due to the lack of job opportunities. We point out that our difference-in-differences estimations capture the reduced-form total effects of 9/11, including all enforcement changes and potential increased discrimination that they trigger. Careful investigation of specific channels through which 9/11 affects our outcomes is by no means trivial and warrants careful future research. However, we do show that the effects are not observed on comparable immigrant groups that are either less likely undocumented or unlikely influenced by stricter immigration enforcement, providing indirect evidence that undocumented status and immigration enforcement may be the main mechanism. While there is some evidence that opportunity self-employment also increases, we show that the definition of opportunity self-employment may capture a significant amount of necessity self-employment when applied to Mexican immigrants, a group with very strong labor market attachment and facing extra labor market constraints due to stricter immigration policies. For example, Mexican immigrants who perceive higher risks of being detected as undocumented immigrants may transition into self-employment directly from the wage sector to reduce such risks, as such will be defined as opportunity self-employment but may not necessarily be pursuing profit opportunities. Because of their strong labor market attachment, Mexican immigrants may also transition from wage-employment to unemployment to self-employment within a very short period of time, which creates the illusion that they have transitioned directly from wage-employment to self-employment from one month to the next. The paper contributes to the literature in several important ways. First, this is the first paper to examine the effect of 9/11 and stricter immigration enforcement on the nature of self-employment among the largest immigrant group in the USA. We document a substantial rise in necessity self-employment among Mexican immigrants following 9/11. Understanding the type of self-employment and its change over time has important implications for studying the labor market performance and assimilation patterns of Mexican immigrants. For example, Lofstrom (2002) finds that self-employed immigrants assimilate at a much faster rate than their wage-employed counterparts. A change in the composition of self-employment toward the necessity type may diminish the role self-employment plays in immigrants’ assimilation process. Second, our paper documents a change in the self-employment patterns among Mexican immigrants triggered by the 9/11 event. This is important for understanding the overall trend of self-employment and its determinants among Mexican immigrants. Mexican immigrants’ self-employment decision is an important topic in the entrepreneurship literature because they historically have very low self-employment rates.Footnote 2 Considering the important role self-employment plays in improving the economic status of disadvantaged minority groups (Fairlie 2004; Lofstrom 2002), studies have devoted attention to understanding the causes of the lower self-employment rates among Mexicans, for example, see Lofstrom and Wang (2009). However, Davila et al. (2014) have documented a drastic recent change: from 1990 to 2012, the number of self-employed Mexican immigrants increased by more than five times, responsible for an exponential growth of Hispanic entrepreneurs in the USA. Our paper suggests that the stricter immigration enforcement after 9/11 is an important contributor to such a fast growth. In addition, while the growth appears to be encouraging, our findings suggest that it may not necessarily be a sign of improved labor market outcomes. Third, we caution the use of necessity and opportunity self-employment measures proposed by Fairlie and Fossen (2018) when applying to groups with strong labor market attachment and extra labor market constraints. In the case of Mexican immigrants, we show evidence that the definition of opportunity self-employment may actually capture a large amount of necessity self-employment.",4
46,1,Eastern Economic Journal,16 September 2019,https://link.springer.com/article/10.1057/s41302-019-00150-7,Marriage and Citizenship Among U.S. Immigrants: Who Marries Whom and Who Becomes a Citizen?,January 2020,Eva Dziadula,,,Female,Unknown,Unknown,Female,"Marriage is one of the pathways to permanent residency and naturalization in the United States; thus, it seems likely that marriage affects naturalization decisions. To understand the importance of marriage in shaping the naturalization choice, one must first understand more about the relationship between these two outcomes. I first attempt to identify whom the foreign-born marry. Specifically, do they marry a U.S.-born or a foreign-born spouse? Then, I examine whether the citizenship status of the foreign-born spouse, which signals a possible pathway to citizenship, influences whether the immigrant becomes a citizen. I hypothesize that in the naturalization decision of the immigrant, the citizenship status of the spouse plays a larger role than simply their place of birth. Two key challenges have faced researchers attempting to understand the role of marriage as a determinant of naturalization. First, prior to the 2008 American Community Survey (ACS), Census-type data suffered from a lack of adequate information on the timing of marriage and naturalization for both the respondents and their spouses. Therefore, the existing literature could not identify the spouse’s citizenship status at marriage, only at the time of survey. This is problematic due to the second key challenge, which is that the impact of marriage is difficult to identify because there is a potential for reverse causality. While marriage may provide a pathway to legal status and strengthen the commitment to the U.S. and thus provide another incentive to naturalize, being a citizen of the United States may also contribute to one’s desirability in the marriage market. Furthermore, factors such as commitment, loyalty, and presence of children may in fact be associated with both marriage and naturalization and result in simultaneous decisions, thus making causal inference impossible. While I cannot overcome these identification issues, I am able to use the richer ACS data to shed more light on the association between the citizenship status of spouses and naturalization. The existing literature shows that having a U.S.-born spouse is associated with an increase in the probability of naturalization (Chiswick and Miller 2009; Jasso and Rosenzweig 1986; Portes and Curtis 1987). However, all foreign-born spouses were treated equally regardless of their citizenship status. That presents a concern, since in the legal framework of U.S. immigration policy, foreign-born spouses who are naturalized citizens provide a pathway to legal status and naturalization identical to that offered by U.S.-born citizen spouses. Therefore, when determining the role of spouse in the naturalization decision, ideally naturalized citizens would be identified separately from foreign-born spouses who are not citizens and thus may not be able to aid in gaining legal status. Unfortunately, no visa or legal status information is readily available in publicly available data, and the citizenship status of the foreign-born spouses at the time of marriage was unknown. Utilizing the ACS data, which as of 2008 include the year of last marriage and the year of naturalization, I am able to not only identify the citizenship status of both spouses at the time of the survey, but also at the time of the most recent marriage. I expect the likelihood of naturalization to be higher in marriages where the spouse is a citizen, regardless of their place of birth, which was identified as a determinant in the existing literature. Empirically I confirm the hypothesis, and I show that the naturalization hazard of immigrants married to a citizen, U.S.-born or foreign-born, is more than double the hazard of immigrants with noncitizen spouses. In fact, I show that the increase in the naturalization hazard associated with foreign-born citizen spouses is significantly higher than the increase associated with U.S.-born spouses. The relative risk of naturalization among immigrants who married a foreign-born naturalized citizen is 30% higher than that of their counterparts with a U.S.-born spouse, and among immigrants with a foreign-born citizen spouse who became naturalized during marriage the hazard is approximately 70% higher. The estimates are consistent with Helgertz and Bevelander’s (2017) findings in Sweden and suggest that foreign-born couples pursue naturalization jointly. The results are qualitatively similar for both men and women. Thus, in the interest of clarity, I present only one set of empirical results and focus on immigrant women.",2
46,1,Eastern Economic Journal,22 October 2019,https://link.springer.com/article/10.1057/s41302-019-00153-4,Immigrant and Minority Homeownership Experience: Evidence from the 2009 American Housing Survey,January 2020,Kusum Mundra,,,Female,Unknown,Unknown,Female,"In spite of an upward trend in homeownership for immigrants and minorities in the USA, homeownership gap for both groups still persists compared to natives and whites, respectively.Footnote 1 Recent studies examining the homeownership for immigrants and various groups in the USA have found that not all groups increased their homeownership during the housing boom of the early to mid-2000s and the loss of homeownership during the housing bust leading to the Great Recession was also varied (Kocher et al. 2009; Mundra and Oyelere 2018; Kuebler and Rugh 2013; Gabriel and Rosenthal 2015 to name a few). According to the 2011 State of the Nation’s Housing, the homeownership gap between whites and Hispanics has fallen marginally from 28.8 percentage points to 26.9 and the white–black gap has stayed the same, around 28 percentage points, and that the housing demand in the USA will be driven by immigrants. As recent as 2018, State of the Nation’s Housing Report shows that home price-to-income ratios have been rising in the USA and for majority of the households housing cost as a share of their income is on the rise, particularly for low-income households. This paper examines the immigrant and minority homeownership experience during the housing boom and bust leading to the Great Recession using the 2009 American Housing Survey data. Given the existing homeownership gap and recognizing the advantages of homeownership, the U.S. Government has taken many steps toward promoting first-time homeownership in the country.Footnote 2 These initiatives include President Clinton’s National Home Ownership Strategy, the Campaign for Homeownership of the Neighborhood Reinvestment Corporation, the Community Reinvestment Act, and the Federal Housing Enterprise Financial Safety and Soundness Act (Wyle et al. 2001; Freeman and Hamilton 2004). The recent drive by the quasi-state owned entities like Fannie Mae and Freddie Mac in targeting home loans through secondary mortgage market to the low-income neighborhoods may be considered as an additional initiative (Frame and White 2005) in promoting homeownership for minorities and lower-income households. The aim of this paper is twofold. First, using the 2009 national sample of the American Housing Survey (AHS), this paper examines whether the first-time homeownership increased for immigrants and minorities during the expansion of the home loans and housing boom in the USA. Second, using the sample of households who have recently moved, this paper explores whether the recent housing bust was more severe for immigrants and minorities compared to the whites and natives, respectively, in the USA. There is increasing evidence that the recent housing bust and high foreclosure rates have been disproportionately tilted toward minorities—the exact group, who were meant to be served by the recent home credit expansion in realizing their homeownership dream.Footnote 3 AHS collects financial characteristics of the primary mortgage as well as the year the previous mortgage was obtained for households who have moved recently from homeownership to renting, which is important in examining whether exit from housing was higher for households who obtained mortgage during the peak subprime period.Footnote 4 This paper examines homeownership and home sustainability for both immigrants and minorities because homeownership experience for these two groups in the USA is similar. Both groups have experienced an upward trend in homeownership, though they both still face a significant homeownership gap. Moreover, a large proportion of minorities are immigrants in the USA. For example, more than half of Hispanics are immigrants and, therefore, one cannot examine the experiences of Hispanics without including their immigrant experience. Using a national sample, this paper will shed some light in understanding how minorities and immigrants fared compared to whites and natives, respectively. Section “Background on Recent Housing Experience” offers a brief background behind the two aims of this paper and lays out the vantage point of the current paper. Section “Data and Sample Description” explains the data and the sample used in this paper, and “Preliminary Descriptive” section the summary statistics. Section “Econometric Model: Cox Proportional Hazard Framework” discusses the proportional hazard framework. “Results” section discusses the findings in detail. Finally, the conclusion in section summarizes the findings and discusses the relevancy of the findings.",2
46,1,Eastern Economic Journal,31 October 2019,https://link.springer.com/article/10.1057/s41302-019-00156-1,Comparing the Ethnicity Proxy and Residual Method: Applications to the State-level DREAM Acts and DACA,January 2020,Xintong Liu,Yang Song,,Unknown,,Unknown,Mix,,
46,1,Eastern Economic Journal,12 September 2019,https://link.springer.com/article/10.1057/s41302-019-00149-0,Variations in Naturalization Premiums by Country of Origin,January 2020,Miao Chi,Michael Coon,,,Male,Unknown,Mix,,
46,1,Eastern Economic Journal,13 September 2019,https://link.springer.com/article/10.1057/s41302-019-00148-1,Second-Generation Immigrants’ Entry into Higher Education: Students’ Enrollment Choices at Different Types of Universities,January 2020,Vivian Carstensen,Roland Happ,Olga Zlatkin-Troitschanskaia,Female,Male,Female,Mix,,
46,1,Eastern Economic Journal,27 October 2019,https://link.springer.com/article/10.1057/s41302-019-00155-2,Estimating the Determinants of Remittances Originating from US Households Using CPS Data,January 2020,Nicole B. Simpson,Chad Sparber,,Female,Male,Unknown,Mix,,
46,2,Eastern Economic Journal,10 March 2020,https://link.springer.com/article/10.1057/s41302-020-00167-3,Urbanization and Its Discontents,April 2020,Edward L. Glaeser,,,Male,Unknown,Unknown,Male,"Discontent roils America’s most successful cities: high housing prices, allegedly racist policing, technological innovations like ride-sharing and apparent inequities in public schooling have all sparked fierce protests from New York to San Francisco. Why has urban triumph produced acrimony rather than joy?Footnote 1 Paradoxically, success often generates disputes. Rent-seeking only makes sense where there are rents to fight over. Throughout most of the 1970–2000 period, urban fortunes seemed to hang on a knife’s edge. Urban mayors emphasized core services that would prevent more urban flight, not redistribution (Peterson 1981; Ferreiro and Gyourko 2009). In a world of apparent urban plenty, activists believe that urban governments can focus more on social problems. Moreover, as Florida (2017) notes, there are many ways in which successful cities are failing their poorer inhabitants. Cities are productive, but they do not appear to enable upward mobility (Chetty et al. 2018; Glaeser and Tan 2020). The urban wage premium appears to have disappeared for the less skilled (Autor 2019). As the wealthy bid up the price of urban real estate, the poor must either pay higher prices or move elsewhere (Hsieh and Moretti 2019). Those prices may explain why poorer people no longer move disproportionately to cities with higher wages (Ganong and Shoag 2017). Everywhere, but especially in the developing world, urban mobility has been slowed by terrible levels of congestion (Kreindler 2020), and crime has reappeared as an urban curse in some cities, such as Chicago. In “Understanding Urban Resurgence” section of this paper, I review backdrop to our current urban discontent: the technological changes that led to urban resurgence. Following previous work (Gaspar and Glaeser 1998; Glaeser 2011), I argue that mid-twentieth century technological changes, such as the interstate highway system and containerization, were largely centrifugal, leading both industry and people to leave urban cores.Footnote 2 In more recent decades, technological change has become more centripetal. Globalization and mechanization have increased the returns to skill and innovation, and urban density abets knowledge accumulation and creativity. There is a strong complementarity between cities and skills (Glaeser and Resseger 2020) so that more skilled places have achieved more success and more skilled urbanites have experienced greater wage growth. The economic success and growth of cities in the developing world is even more impressive (Chauvin et al. 2017). But even as many cities have experienced robust economic growth, they have failed to generate upward mobility. The Opportunity Atlas created by Chetty et al. (2018) provides upward mobility by neighborhood across the U.S. As Glaeser and Tan (2020) document, upward mobility is lower in dense metropolitan areas and lower in denser neighborhoods within metropolitan areas. Many central city school districts appear to be performing poorly, and urban children often live spatially segregated lives. For some teenagers, the urban advantage in making markets may have become a curse because urban drug markets distract from education. Autor (2019) documents that the urban wage premium has largely disappeared for less skilled workers. In “Cities, the Less Skilled and the Vulnerable” section, I show that this change has occurred in less skilled cities that have experienced less success, but there is still a wage premium for less skilled workers who work in more skilled cities. As a whole, cities appear to perform poorly for less skilled workers partially because less skilled workers live disproportionately in less skilled cities. In developing world cities, weak rule of law means that more vulnerable citizens, particularly women, have difficulty forming reciprocally beneficial partnerships that enable them to benefit from urban collaboration (Ashraf et al. 2019). Anger about gentrification partially reflects the fact that housing price growth has often outpaced income growth, especially for less skilled workers. That increase in housing prices reflects both demand for consumer amenities by higher skilled workers and limits on housing supply. As I discuss in “Unaffordable Cities and the Limits of Urban Housing Supply” section, cities where housing supply is relatively fixed, such as San Francisco, create the sharpest conflicts as rich and poor battle over a zero-sum stock of homes. Historically, cities have been an escape route for the underemployed residents of rural areas, such as the African–Americans, who fled north during the Great Migration. Since 1980, migration rates have declined (Molloy 2011) and poor people no longer disproportionately move to higher wage areas (Ganong and Shoag 2017). Limited housing supply provides one potential explanation for this change, but cities sometimes also fail to provide particularly good jobs for outsiders with the wrong set of skills. The Highway Revolts of the 1960s and 1970s were the transportation equivalent of community opposition to new housing construction. As more drivers crowd into a fixed supply of city streets, those streets become congested and urbanites waste billions of hours stuck in traffic (Texas Transportation Institute 2019). Few cities have adopted the congestion pricing that could induce drivers to internalize the social costs of their motoring. Endless traffic jams are even prevalent in developing world cities, although Kreindler (2020) finds that congestion pricing in those areas would have only modest benefits because the quality of roads is so limited. As I discuss in “Failing to Tame the Demons of Density” section, urban disamenities, such as traffic, effectively limit urban success. In “An Interpretation” section, I discuss a simple hypothesis to explain these liked phenomena. Our recent urban success primarily reflects private sector productivity which has been enhanced by urban density. But there have been few matching improvements in the efficiency of urban government. Policing is the only area that seems to have generated real success and that success came at the terrible cost of mass incarceration. More generally, the public sector has failed to keep up with private sector led urban growth. A mismatch between private urban expansion and public capacity is not unusual in either our urban past or in the developing world today. Typically, cities grow and then the public sector needs to catch up by building infrastructure, enacting regulations and supporting the poor. This catch up may also happen today, but there are far more breaks on public sector change than existed in the past. New infrastructure is harder to build. Schools are harder to reform. An empowered and educated citizenry has created more checks on city government. Those checks have upsides. Communities are less likely to be haphazardly bulldozed to make way for urban renewal than they were in the past. Yet an urban government that protects insiders too stringently will end up having a public sector that lags many decades behind. Lengthy construction delays have further reduced public sector accountability, because the leader who starts a major project is unlikely to be around when it is finished. This hypothesis echoes the argument made by Sigmund Freud in Civilization and Its Discontents, which motivates the title of this essay. Freud was concerned with the conflict between individual desires and the rules that define civilized behavior. That conflict is minimized when individuals are taught to accommodate social rules and when the rules are limited to restricting the most harmful behavior. The urban discontent of today arguably reflects failures in both education and regulation that have made cities far less accommodating to the less fortunate. “Conclusion” section concludes with a discussion of the ways in which future research might improve current and future urban policy debates. We need more research on the consequences of gentrification. We need to better understand why urban mobility is low. We especially need to understand the barriers to making developing world cities more livable.",15
46,2,Eastern Economic Journal,12 November 2019,https://link.springer.com/article/10.1057/s41302-019-00157-0,Introduction to the Symposium on Urban Economics,April 2020,Jason Barr,,,Male,Unknown,Unknown,Male,,
46,2,Eastern Economic Journal,20 November 2019,https://link.springer.com/article/10.1057/s41302-019-00158-z,Chinese Hukou Policy and Rural-to-Urban Migrants’ Health: Evidence from Matching Methods,April 2020,Marta Bengoa,Christopher Rick,,Female,Male,Unknown,Mix,,
46,2,Eastern Economic Journal,22 November 2019,https://link.springer.com/article/10.1057/s41302-019-00159-y,Do Vouchers Protect Low-Income Households from Rising Rents?,April 2020,Ingrid Gould Ellen,Gerard Torrats-Espinosa,,Female,Male,Unknown,Mix,,
46,2,Eastern Economic Journal,05 December 2019,https://link.springer.com/article/10.1057/s41302-019-00160-5,Vision Zero: Speed Limit Reduction and Traffic Injury Prevention in New York City,April 2020,Kristin Mammen,Hyoung Suk Shim,Bryan S. Weber,Female,,Male,Mix,,
46,2,Eastern Economic Journal,18 November 2019,https://link.springer.com/article/10.1057/s41302-019-00161-4,Shadow Economies Around the World: Evidence from Metropolitan Areas,April 2020,Ceyhun Elgin,,,Male,Unknown,Unknown,Male,"The shadow economy, sometimes also called the informal, hidden, black, parallel, second, or underground economy (or sector), is defined by Hart (2008) as a set of economic activities conducted outside the framework of the bureaucratic public and private sector establishments. Similarly, Ihrig and Moe (2004) define it as the sector that produces legal services and goods without complying with government regulations, thereby operating without much (if any) government scrutiny. There are many other similar definitions and descriptions, such as Frey and Pommerehne (1984), Loayza (1996), Johnson et al. (1997), Johnson, Kaufmann and Zoido-Lobaton (1998a, 1998b), Thomas (1999), Fleming et al. (2000) Schneider and Enste (2000), Schneider (2005), Elgin and Oztunali (2012), and Medina and Schneider (2018). The last of these defines the shadow economy as the sector that includes all economic activities which are hidden from official authorities for monetary, regulatory, and institutional reasons. This definition, which is also the one used in the current paper, includes all types of economic activities that, if recorded, would contribute to the official GDP and thus ignores illegal, underground, or criminal transactions, as well as do-it-yourself, or other types of home production activities. Even though informality is a widespread phenomenon and poses serious social, economic, cultural, and political challenges worldwide, many issues about its nature and consequences remain largely under-explored or unresolved. For example, the previous research has failed to generate a consensus over how to measure the informal sector (Elgin and Oztunali 2012). Nevertheless, there is a significant amount of theoretical and empirical research investigating the causes and effects of the shadow economy. From the very early papers in the literature onwards (e.g., Mazumdar 1976), the shadow economy has been overwhelmingly defined and viewed as an urban phenomenon. Although a large fraction of rural workers may work without any social security or insurance, and can therefore theoretically be considered part of the shadow economy, it is much easier to incorporate their value-added and employment within national income statistics as they largely work in a single sector (agriculture). For this reason, the shadow economy has mostly been seen as an urban issue (Elgin and Oyvat 2013). A significant number of papers have investigated the relationship between the shadow economy and urbanization. Elgin and Oyvat (2013), for example, proposed an inverted-U curve relationship between urbanization and the size of the shadow economy. They argue that the share of the informal sector first increases in the early phases of urbanization before decreasing as urbanization continues. In support of this, Yuki (2007) found that high urbanization and shadow economy’s size are positively correlated in economies with initial land inequality, whereas the correlation is negative for economies with low land inequality. The informal sector poses various challenges for governments and society as it has an important reciprocal relationship with several key macroeconomic, institutional, social, and cultural variables. The literature has explored a range of effects of the shadow economy: on level and cyclicality of fiscal policy (Cicek and Elgin 2011), provision of social security, labor force participation behavior (Schneider and Enste 2000), income distribution (Hatipoglu and Ozbek 2011), the magnitude and duration of business cycles (Roca et al. 2001; Elgin 2012), the monetary base (Tanzi 1983), inflation and growth (Asfuroglu and Elgin 2016), and total factor productivity (D’Erasmo and Boedo 2012). This non-exhaustive list of previous research indicates that it is essential to accurately measure the size of the shadow economy for governments, policy-makers, and researchers to properly design and implement policies. As the number of papers in the growing literature on informality indicates, there has been increasing economic analysis of the shadow economy. Despite the development of various methods, the literature still lacks the significantly large datasets needed to make informality subject to robust (applied) policy analysis. All empirical studies of the relationship between urbanization and informality generally construct their measure of the shadow economy’s size using aggregate indicators of the overall economy. That is, even though the main subject of the analysis is urban informality, the lack of urban shadow economy size data series forces the researchers to use aggregate economy-wide measures of informality. The obvious reason for this lack of data is the difficulty in obtaining city-level economic or social variables to estimate shadow economy size. The present study aims to fill in this gap in the literature by using a slightly modified version of the calibrated two-sector dynamic general equilibrium (DGE) model (similar to the one used by Elgin and Oztunali 2012; and Elgin et al. 2019) to back out estimates of shadow economy size (as a percentage of GDP) in 56 metropolitan areas from 31 countries. I use metropolitan area-level data series from different sources to feed the DGE model. To the best of my knowledge, this is the first paper in the literature constructing shadow economy size estimates for a wide range of cities around the world. In addition to fully describing the backed-out dataset, I also provide some stylized facts concerning the trends of these estimates as well as some of its correlates. Finally, the model is also used to evaluate the effects of two policy tools: changing the tax burden and tax enforcement. The rest of the paper is organized as follows. In the next section, I thoroughly describe the methodology to construct shadow economy estimates. In section three, I apply this methodology to obtain and then present shadow economy size estimates for a panel of 56 cities. I then fully characterize the constructed dataset and evaluate two specific policy tools in a counterfactual analysis. The final section provides concluding remarks.",8
46,2,Eastern Economic Journal,21 November 2019,https://link.springer.com/article/10.1057/s41302-019-00162-3,"Explaining New Firm Survival: Is the Firm, Owner, or Agglomeration at Fault?",April 2020,Matt Saboe,Simon Condliffe,,Male,Male,Unknown,Male,"There is a wealth of research considering how founder characteristics, skills, and motivations affect entrepreneurial performance. Similarly, urban economists have intensely studied the source and benefits of agglomeration economies. Given that entrepreneurs do not establish their new ventures in a vacuum, this paper examines the role of heterogeneous environments in entrepreneurial success. Externalities such as knowledge spillovers, labor pooling, supplier networks, and an entrepreneurial culture could play an important role in determining a new firm’s chance of survival. Through increased competition and innovation pressures, surviving startups encourage existing firm improvements and future regional growth (Fritsch and Mueller 2004; van Stel et al. 2010). Some studies have examined new firm survival across regions (Delgado et al. 2010; Falck 2007). However, very few studies to our knowledge (Burger et al. 2011; Ebert et al. 2019; Tavassoli 2016; Neffke et al. 2012) consider how the various sources of agglomeration affect new firm survival. Understanding how new firm survival is affected by externalities is important for policy makers interested in entrepreneurship-focused growth plans. The extant literature on the intersection of new firm survival and regional characteristics has a few opportunities for further investigation. First, there are several sources of agglomeration that drive regional competitiveness, but some sources which could be particularly important for entrepreneurs have not been examined. For example, Ebert et al. (2019); Neffke et al. (2012); Tavassoli (2016) consider the role of standard Marshallian and Jacobian externalities, but do not consider agglomerations of small independent firms (i.e., Chinitz agglomerations) or agglomerations of entrepreneurs. These agglomerations may be more important to entrepreneurs as suggested by Glaeser and Kerr (2009). Second, none of these studies use US data, where agglomeration patterns and effects may be different. Third, the existing studies draw contradicting conclusions as to which sources of agglomeration are significant and whether or not they enhance survival chances. Fourth, many studies are unable to control for detailed individual firm and entrepreneur variables that are known to influence survival chances. This paper empirically estimates the effect of several sources of agglomeration on new firm survival while controlling for firm, entrepreneur, and regional variables using a discrete-time hazard model. We utilize the very rich longitudinal Kauffman Firm Survey of almost 5000 US startups from 2004 to 2011. This limited-access dataset contains a myriad of firm and entrepreneur variables including location. Unlike previous studies [e.g., Falck (2007)], we find that the overall hazard of entrepreneurs shutting down their new firms is not significantly affected by regional agglomeration factors, but instead firm and entrepreneur variables. However, after we group the new firms according to motivational variables such as high-tech, home-based, and non-employers, we discover heterogeneous effects of agglomeration. Specifically, regions with a greater share of small businesses are associated with greater survival chances of non-high-tech, non-home-based, and non-employer new firms. Conversely, the survival chances of high-tech entrepreneurs are higher in regions with greater university knowledge activity. Lastly, after separating closures, we find that a greater share of small businesses in a region is associated with a lower hazard of other exit. We contribute to the expanding literature on entrepreneurial performance (survival) and agglomeration in several ways. We examine the effects of several sources of agglomeration on new firm survival. We control for detailed firm and entrepreneur variables using a limited-access data set. Finally, our analysis explores entrepreneur survival across the USA.",3
46,2,Eastern Economic Journal,25 November 2019,https://link.springer.com/article/10.1057/s41302-019-00163-2,Skyscrapers and the Happiness of Cities,April 2020,Jason Barr,Jennifer Johnson,,Male,Female,Unknown,Mix,,
46,3,Eastern Economic Journal,16 December 2019,https://link.springer.com/article/10.1057/s41302-019-00165-0,Non-stop Love: A Study of Entry Barriers in the Airline Industry Using Policy Changes at Dallas Love Field,June 2020,Pukar KC,,,Unknown,Unknown,Unknown,Unknown,,
46,3,Eastern Economic Journal,19 August 2019,https://link.springer.com/article/10.1057/s41302-019-00145-4,Share of Household Earnings and Time Use of Women in Same-Sex and Different-Sex Households,June 2020,Michael E. Martell,Leanne Roncolato,,Male,Female,Unknown,Mix,,
46,3,Eastern Economic Journal,11 October 2019,https://link.springer.com/article/10.1057/s41302-019-00152-5,"Pathways Between Minimum Wages and Health: The Roles of Health Insurance, Health Care Access and Health Care Utilization",June 2020,Otto Lenhart,,,Male,Unknown,Unknown,Male,"The minimum wage has entered the national spotlight over the past few years, reaching a pinnacle in former President Obama’s 2014 State of the Union Address. Obama strongly advised Congress to raise the federal minimum wage from $7.25 to $10.10, arguing that this policy change would provide much-needed additional income for 28 million Americans and thereby contribute to a reduction in earnings inequality. While the State of the Union failed to change the federal minimum wage, which has remained unchanged since 2009, several states have raised their wage floors considerably higher in recent years. Between 2015 and 2017, 61 increases in state-level minimum wages were implemented across the USA. In July 2019, the House of Representatives passed and forwarded the Raise the Wage Act, which outlines a gradual increase in the federal minimum wage to $15 by 2025. While the US Senate appears to currently not have immediate plans to raise the federal minimum wage (Fernández Campbell 2019a), it seems likely that the minimum wage will continue to be an important topic of political discussions in the upcoming years. The majority of Democratic candidates for the 2020 presidential election support an increase to $15 per hour (Fernández Campbell 2019b). While economists have extensively analyzed the effects of minimum wages on employment (see overview by Neumark et al. 2014) and poverty (e.g., Card and Krueger 1995; Neumark and Wascher 2002; Burkhauser and Sabia 2007), uncertainty remains about how minimum wages affect labor market outcomes. In recent years, several studies have expanded the focus and examined potential effects of minimum wage increases on health outcomes of affected workers (e.g., Wehby et al. 2016; Horn et al. 2017; Averett et al. 2017; Lenhart 2017a). This study contributes to this literature by examining potential pathways through which minimum wage can affect the health of low-wage workers in the USA. Understanding the pathways through which minimum wages affect non-employment outcomes such as health will allow policymakers to design efficient policies that can improve the overall well-being of society. Based on economic theory, minimum wage increases can either improve health by providing a boost in income or worsen health if some people lose their employment (Wehby et al. 2016). Recent studies have shown mixed evidence on the effects of minimum wages on health outcomes – some researchers provide evidence for health improvements (Wehby et al. 2016; Lenhart 2017a, b; Reeves et al. 2017; Du and Leigh 2018), while others have found no or mixed effects (Averett et al. 2017; Horn et al. 2017; Kronenberg et al. 2017). Given that it might take some time before health changes are observable, especially among younger workers in low-wage jobs, evidence on potential pathways through which increases in minimum wages might affect present and future health outcomes can add to the existing literature and provide evidence for potential long-run effects on health. This study estimates difference-in-differences models to examine the role of three potential mechanisms underlying the link between minimum wages and health: (1) health insurance coverage, (2) health care access and (3) health care utilization. I analyze US data for the years 1989–2009, a period during which there were 295 changes to state-level minimum wages. Controlling for state-specific time trends, the analysis provides evidence that higher minimum wages increase health insurance coverage, health care access and utilization for low-educated working-age individuals. Specifically, this study finds that a 10% increase in minimum wages is associated with a 4.44% increase in individually purchased insurance coverage, which translates to a 1.86% increase when using a typical increase in minimum wages (2008–2009). Additionally, individuals are 2.19% less likely to not be able to afford necessary doctor visits and 0.65% more likely to have a routine health checkup following a 10% increase in minimum wages. When examining heterogeneous effects, I find that the results are the largest for low-educated individuals between the ages 18 and 29, a group that is most likely to be earning minimum wages.",7
46,3,Eastern Economic Journal,23 October 2019,https://link.springer.com/article/10.1057/s41302-019-00154-3,When Social Norms Influence the Employment of Women: The Case of Japan,June 2020,Sanae Tashiro,Chu-Ping Lo,,,Unknown,Unknown,Mix,,
46,3,Eastern Economic Journal,12 September 2019,https://link.springer.com/article/10.1057/s41302-019-00151-6,An Agent-Based Model of Ethnocentrism and the Unintended Consequences of Violence,June 2020,William D. Tilson,Thomas K. Duncan,Daniel Farhat,Male,Male,Male,Male,"Cooperation has economic advantages in the right circumstances. Some people prefer to interact only within their own ethnic group and hesitate or refuse to cooperate with outsiders (“ethnocentrism”). In-group favoritism can lead to suboptimal production since agents are unable to take advantage of the dissimilar skills and knowledge of the out-group. Inefficient allocation is also possible since out-group buyers and sellers are ignored or treated differently. In the modern global economy, the opportunity cost of ethnocentrism is rising as more out-group members are encountered with increased urbanization, international migration, improvements in transportation and improvements in communication technology. Eliminating in-group bias may substantially increase both the value of investment and the number of lucrative transactions, making this an important economic policy goal. This study focuses on a single factor which may exacerbate ethnocentrism: violence. It is reasonable to suspect that ethnocentrism causes violence, specifically conflicts between ethnic groups. However, we are curious to find out whether the effect can flow in the other direction as well: for violence to worsen ethnocentrism.Footnote 1 If it can, then policies associated with violence reduction have an unintended economic benefit (flowing through increased interaction within a diverse community). We evaluate the impact of violence on ethnocentrism, holding the impact of ethnocentrism on violence constant. Our analysis uses a computational simulation method known as agent-based modeling (ABM). The ABM method involves the construction of a virtual world populated by heterogeneous interacting agents. In our virtual world, agents make decisions about economic transactions with others using rules. Some agents have rules that are ethnocentric in nature, while others do not. We simulate worlds with and without added violence and watch to see whether the rules that embody ethnocentrism become dominant in the population over time. If the rules featuring ethnocentrism flourish more in the violent environments, the statement above is deemed feasible—violence worsens ethnocentrism. Typically in agent-based models, initial parameter values and final trends are then compared to real-world data to evaluate the quality of the model (via falsification, like in other modeling approaches). As we are only interested in feasibility in this study, we present the model, initial parameter calibrations and evolving trends only; the empirical comparison is left for future work. In the sections that follow, we provide a brief review of ethnocentrism and violence, describe the model, present the results and connect the model outcomes to policy.
",1
46,3,Eastern Economic Journal,12 August 2019,https://link.springer.com/article/10.1057/s41302-019-00144-5,Monitoring Intensity and Technology Choice in a Model of Unemployment,June 2020,Haiwen Zhou,,,Unknown,Unknown,Unknown,Unknown,,
46,3,Eastern Economic Journal,05 February 2020,https://link.springer.com/article/10.1057/s41302-020-00166-4,"Complexity and the Art of Public Policy: Solving Society’s Problems from the Bottom Up. By David Colander and Roland Kupers. Princeton University Press, Princeton, NJ and Oxford, UK, 2014. 310pp., $29.95. ISBN 987-0-691-15209-7.",June 2020,William D. Ferguson,,,Male,Unknown,Unknown,Male,,
46,4,Eastern Economic Journal,27 July 2020,https://link.springer.com/article/10.1057/s41302-020-00174-4,Political Environment and US Domestic Migration,October 2020,Selcuk Eren,Andrew W. Nutting,,Unknown,Male,Unknown,Male,"Migration is an outcome of comparing costs to expected benefits. Empirical research often captures costs and benefits via pecuniary economic measures in the labor and housing markets (e.g., Sjaastad 1962; Fields 1979; Molloy et al. 2011), but benefits can also be related to local amenities and services (Albouy and Lue 2015). People can differ in their preferences for locational attributes and use migration to geographically sort themselves (e.g., Graves 1979; Graves and Linneman 1979; Greenwood and Hunt 1989; Mueser and Graves 1995). For example, migration differs by education level and age, leading to sorting. College graduates move toward areas with better schools and transportation networks (Diamond 2016), large numbers of college graduates in the marriage market (Compton and Pollack 2007), and western geographic-specific amenities (Glaeser et al. 2014). Young, highly educated households move toward large urban areas (Couture and Handbury 2017) and areas with high-quality business environments (Chen and Rosenthal 2008). Couples nearing retirement move toward warm coastal areas (Chen and Rosenthal 2008) and away from areas with high residential property taxes (Shan 2010). This paper has two goals related to the study of migration and sorting. The first is to determine whether the political beliefs held by a location’s residents are correlated with its in-migration and out-migration. Previous research has shown that people derive utility from living among neighbors who share similar political beliefs (Hui 2013) and Americans prefer migrating to places where their political beliefs are more commonly held even when controlling for factors such as income and racial composition (McDonald 2011; Tam Cho et al. 2013).Footnote 1 Glaeser and Ward (2006) further show that political attitudes are not limited to a simple one-dimensional “left/right” metric, but exist along both economic and cultural/religious dimensions, and that the extent to which these dimensions contribute to political polarization changes over time. In this paper, we extend the literature by using interest group ratings of Congressional representatives to measure MSA political beliefs across two dimensions—economic attitudes and social attitudes—and to examine how they differently affect migration.Footnote 2 To our knowledge, we are the first to use different measures of economic and social attitudes, instead of a one-dimensional “liberal/conservative” or “Democrat/Republican” metric, to proxy for local political environment in a migration model. The second goal of this paper is to determine whether observed correlations between political environment and migration are explained by controlling for a handful of state-level economic and social policies (Cebula et al. 2016) and MSA-level labor market conditions. Migration to locales whose Congressional representatives espouse certain beliefs may be motivated by those locales having governmental policies or labor market conditions representative of such beliefs (Medoff 2002). For example, migration to MSAs represented by economically conservative Congressmen may be motivated by a desire to live under economically conservative local policies, such as lower tax rates or less regulation and therefore lower costs of living (Glaeser et al. 2005).Footnote 3 It is possible, though, that migration might be motivated by factors beyond such “meat-and-potatoes” benefits. Shared political sentiments could be a pure public good, prompting sorting similar to that in models where migration is motivated by differences in local public expenditures (Tiebout 1956) or race (Schelling 1971). Research shows individuals are more likely to engage in face-to-face political discussions with people who agree with their political preferences than they are to consume television or internet news that caters to their political preferences (Gentzkow and Shapiro 2011). This suggests that people prefer living around those who share their political attitudes, and might migrate to do so. Our results show, when controlling for MSA demographics, region, and weather, that from 2005 to 2010 college graduates aged 25–40 in the USA migrated to MSAs that were more economically conservative but socially liberal than the migrants’ Origin MSAs and the national average MSA. According to this metric, the most popular sizable places to move to were Tucson, Hartford, and Oakland, and the least popular were Scranton–Wilkes-Barre, Albany, and Pittsburgh. College graduates aged 41–60 showed migration patterns that were similar to, but weaker than, those of young college graduates. Non-college graduates aged 25–40 and aged 41–60 migrated away from MSAs that were more conservative, but there was no significant relationship between migration and either economic conservatism or social conservatism, perhaps because of multicollinearity. Much of young college graduates’ migration toward economically conservative MSAs is accounted for by a few state policy and local labor market controls. But their migration to socially liberal areas, particularly among the unmarried and childless, is less well captured by these controls, suggesting they migrate toward the public good of shared political sentiments. Results also show that despite having moved to MSAs that were more socially liberal, young college graduates moved to states with more geographically restricted abortion access. This suggests they moved to socially liberal enclaves in socially conservative states. Our analysis is based on the prevailing political interpretations of “conservative” and “liberal” that predated the 2016 presidential election campaign won by Donald Trump. According to some commentators, Trump “erased Republican conservatism” (Will 2016) by shifting the Republican party away from the theme of free markets to something more “protectionist and nativist” (Douthat 2016). Drutman (2017) argues that Trump won in 2016 by “winning over ‘populists’ (socially conservative, economically liberal voters) who had previously voted for Democrats.” Autor et al. (2017b) found that Trump did especially well in areas that had lost jobs to Chinese import penetration, where voters were perhaps less favorable toward the economically conservative policy of free trade. Despite the changes in America’s political environment, our results remain relevant because they document a significant early twenty-first-century relationship between politics and migration. Also, our finding that households of different education levels sorted into MSAs with different political environments contributes to our understanding of how the country became so unsuspectingly polarized in the years leading up to 2016 (Schaffner et al. 2016) and why so much of the prestige national press found Trump’s 2016 victory so shocking (Flegenheimer and Barbaro 2016; Sorkin 2016; Tumulty et al. 2016). The rest of this paper is organized as follows: “Data” section describes our data. “Estimation Strategy” section outlines our gravity model estimation strategy. “Results” section shows our results. “Conclusion” section concludes.",
46,4,Eastern Economic Journal,15 June 2020,https://link.springer.com/article/10.1057/s41302-020-00171-7,"Inclusiveness, Growth, and Political Support",October 2020,Richard Carson,,,Male,Unknown,Unknown,Male,"Can diverse economic and political change result from a change in a single underlying parameter? In this paper, I argue that it can if the parameter measures the ‘inclusiveness’ of a political system in an environment where governments obtain political support from two basic sources, wealth creation and income redistribution, and choose combinations of the two that maximize their support. Changes in these combinations—and thus in the way governments obtain support—cause multiple changes in social and economic variables that can change the very nature of societies. To gain political support from income redistribution, a government exploits differences in ability to supply support. Let ‘insiders’ be those with a relatively high ability to supply political support and ‘outsiders’ be those with a relatively low ability. Then government redistributes income from ‘outsiders’ to ‘insiders,’ contingent on insiders’ support behavior—e.g., by giving insiders monopoly or monopsony power or by taxing outsiders and subsidizing insiders. Insiders are therefore valued for their abilities to supply political support in return for rent rather than for their administrative/managerial/entrepreneurial talent. We denote this redistributed income by V, which is a form of rent (defined more precisely below), while competition for V by supplying political support is a form of rent seeking and represents an alternative use of resources to wealth creation. Below we interpret the evolution of growth in Japan (Beason and Patterson 2004) and in the former Soviet Union (Anderson and Boettke 1997), as well as the ‘Great Recession’ in the USA as consequences of political support from rent and rent seeking. The more a government relies on insiders for support, the greater will be the support derived from redistribution and rent seeking. This support, denoted by R, can take many forms—money, resources, campaign rallies, monitoring, delivery of votes, intimidation and repression of political rivals or opposition groups, etc., depending on the nature of the political system. The government uses V to pay for R and to motivate support for itself rather than for a rival. Baker (2015) and Saez (2014, p. 24) argue that rent seeking is the main driver of growing inequality in the USA since the 1970s. Examples of insiders include the Communist Party elite in nations where it rules and party elites in other countries, such as Japan, where a single party monopolizes power. In the USA, the upper 1% of the income distribution has outsized political influence owing to their ability to make outsized political contributions in an environment where money plays an important role in determining political outcomes. Because increases in rent and rent seeking can raise political support while lowering efficiency, both efficiency and equality can be low at the support maximum. A political system becomes completely inclusive when the difference between its insiders and its outsiders disappears, along with the exploitation of outsiders. Then efficiency is high. Outsiders lose from income redistribution, but can gain from wealth creation. Let Ψ be the share of outsiders in political support, which depends on the nature of the political system. This share is computed like the share of labor or capital in output, and we can talk about insider or outsider intensive support, depending on which group a government relies for most of its support. A political system with Ψ = 1—all the support comes from outsiders—is quite different from one with Ψ = 0—all the support comes from insiders—especially in the role played by competition. Changes in Ψ generate a spectrum of political systems. As Ψ rises, a government will rely less on insiders and income redistribution and more on outsiders and wealth creation, causing efficiency to increase. Differences in ability to supply support will become smaller and harder to exploit; expansion of voting rights with secret ballots and the development of a free press are usually part of this. As Ψ rises, the support of insiders becomes less important and that of outsiders more so until the distinction between the two vanishes, and support depends only on wealth creation. Thus, Ψ indexes inclusiveness. While the value of Ψ reflects the institutions and history of a given political system, governments can often change institutions and erode or augment their effectiveness. This changes Ψ. Because governments maximize their support, the quest for support links efficiency to inclusiveness and, more generally, differences in economic systems to differences in political systems. In Why Nations Fail, Acemoglu and Robinson (2012) also link efficiency to inclusiveness, although without defining inclusiveness in a quantifiable way. We can also think in terms of ‘targeting’ (Lizzeri and Persico 2001). In redistributing wealth to insiders, a government destroys part of this wealth by raising inefficiency. However, it gains political support by targeting wealth to people with a relatively high ability to supply support. Efficiency will increase if institutional changes make it harder for a government to target in this way, but a government that gains from doing so will resist such changes. A government maximizes its political support, U, subject to a short-run production frontier, TR, in R and useful output, Y. A unique TR is assumed to exist for any given amounts of labor and capital input and worldwide technological knowledge (on which more below). As economic growth occurs, U remains tangent to TR as TR shifts outward. Wealth creation means production of Y, which we take as numeraire. Only Y can yield utility from present or future consumption, the way in which ‘useful’ output is useful. We divide Y into profit from providing political support, G, and all other useful output, (Y − G). Rent extracted from outsiders to pay for support is V, which equals the cost of support, πRR, plus the profit from providing it, or V = G + πRR. Here, πR is the price of R in units of Y. To maximize its support, a government can choose R and G or Y and G. Then TR determines Y or R, along with πR, which equals the inverse of the marginal rate of transformation of this frontier. R, G, and πR determine V. We can think of V specifically as the rent to the market positions of suppliers of Y and also as the economic profit on Y. To retain these market positions and thus V, suppliers must pay πRR to cover the cost of their political support, leaving them with a net profit of G. This paper makes two contributions and in each case tries to give a positive analysis rather than a normative one. First, it derives a specific political support function, U, with Ψ as parameter and shows how changes in Ψ affect Y, R, and G, as well as market and political competition, efficiency, and growth, plus secrecy in government, corruption, and the rule of law. It also looks at the institutional requirements for inclusiveness to be high and the effect of these requirements on growth, given that governments maximize their political support. Second, it shows how the quest for support causes Ψ to change and how this affects efficiency and growth. This quest can lead to an inclusive polity, a kind of political invisible hand, or to a polity that is far from inclusive. Questions about inclusiveness and growth recall the long-running debate in the economics literature over whether democracy raises growth. The most recent entry, as of this writing, is a paper by Acemoglu et al. (2019), which answers with a resounding ‘yes.’ These authors also reference earlier works on the subject, several of which give negative answers. Here I note that inclusiveness differs from democracy and is neither necessary nor sufficient for democracy, although most inclusive polities are probably liberal democracies. Autocracies can be inclusive under internal or external pressure to be efficient. However, this requires a government to be able to rely on the support of outsiders without giving them guarantees of political rights in return. Such an arrangement is likely to be temporary. Most democracies are illiberal, rather than liberal (Zakaria 1997), and the former are generally not inclusive (see below for the difference between the two). The basic reason why inclusiveness raises growth is that it increases reliance on wealth creation for political support.",2
46,4,Eastern Economic Journal,26 May 2020,https://link.springer.com/article/10.1057/s41302-020-00169-1,Satisfaction and Self-employment: Do Men or Women Benefit More from Being Their Own Boss?,October 2020,Karen Maguire,John V. Winters,,Female,Male,Unknown,Mix,,
46,4,Eastern Economic Journal,05 July 2020,https://link.springer.com/article/10.1057/s41302-020-00173-5,What are the Long-Term Effects of Prenatal Air Pollution Exposure? Evidence from the BHPS,October 2020,Patrick Gourley,,,Male,Unknown,Unknown,Male,"The total effect of air pollution on human health is a central question for environmental economists. The benefits of pollution are easily calculable; when a factory is operating, the number of workers employed or goods produced can be accurately measured and tabulated. Costs are more opaque. Beyond observable damages to the local environment, air pollution can cause short- and long-term negative impacts on people that include increased childhood asthma rate (Currie and Walker 2011), increased mortality (Chay and Greenstone 2003), decreased life satisfaction (Barrington-Leigh and Behzadnejad 2017), and chronic disease (Chanel et al. 2016). Although some avoidance behavior may mitigate these risks, ultimately low air quality will have an impact on people’s health (Liu et al. 2018). The long-term effects of pollution are notoriously difficult to isolate (Greven et al. 2011). Locations with higher levels of air pollution have lower housing and rental prices (Chay and Greenstone 2005), leading to populations that may differ in pollution avoidance or other behaviors when compared to those that live in areas with better air quality. Through Tiebout sorting (Tiebout 1956; Banzhaf and Walsh 2008), those with a greater distaste for pollution may vote with their feet and emigrate, biasing estimates. Also, pollution needs to be isolated from other determinants of health at both the individual and location level. A plethora of factors may affect the health of a population as well as air quality. To that end, this paper examines the effects of prenatal air pollution exposure on adult outcomes. Using a national survey from the UK, the British Household Panel Survey (BHPS), location and time of birth are matched with air pollution data to determine prenatal air pollution exposure. Prenatal exposure is then combined with adult outcomes from the BHPS that could be impacted by air pollution. These include whether a respondent is disabled, employment status, wages, overall health status, and frequency of doctor appointments. The effects of in utero air pollution on adults and even children are difficult to determine largely because of deficiencies in many data sets. As stated by Almond and Currie (2011), “...many prominent data sets, such as the Current Population Survey, do not include information on where someone was born or precise date of birth. As a result, many interesting and policy-relevant experiments linked to a certain time and place may never be analyzed.” Combining the BHPS with air pollution data circumvents many of these issues.Footnote 1 One of the strengths of the BHPS is its inclusion of the district of birth and district of current residence of each respondent.Footnote 2 By controlling for both district of birth and current district of residence, I am able to isolate the effect of prenatal air pollution exposure from contemporaneous exposure. The BHPS data have been combined with air pollution data provided by the UK’s Department of Food, Environment, and Rural Affairs (DEFRA). As part of the CAA of 1956, pollution monitors were established across England beginning in 1961.Footnote 3 By the 1970s, hundreds of monitors from around the country were recording daily readings of sulfur dioxide and black smoke, a type of particulate matter. The monitoring network initially focused on urban areas, which is where the majority of BHPS respondents are located. An advantage of analyzing the effects of air pollution on health in the UK is the presence of the National Health Service (NHS), which is a single-payer healthcare system that covers all British citizens. Founded in 1948, by the beginning of the study period, the program had established the first nationally run single-payer healthcare system in the world. In other countries, citizens who are exposed to worse air pollution will have lower incomes and be more likely to be unemployed, resulting in worse health insurance coverage. With universal health insurance, however, it is reasonable to assume that all survey respondents had access to roughly the same quality of healthcare. I find that prenatal air pollution has a statistically significant effect on all of the studied adult outcomes. The average marginal effect of a standard deviation increase in average black smoke exposure over the course of a trimester is a 1.6% point increase in the probability that a worker is disabled or long-term sick. Aggregating over the entire country, this results in an additional 405,000 disabled workers. This corresponds to 16 billion pounds in lost wages alone per year. Higher pollution exposure during the second trimester also results in lower wages and worse health. This is some of the first evidence that prenatal air pollution exposure negatively impacts adult outcomes. The results are generally more attenuated than contemporaneous effects of exposure on adult health, but given the probable permanence of prenatal exposure, the magnitudes are still economically significant. The main contribution of this paper is the inclusion of both district of birth and district of current residence. It is possible that those born in areas with high pollution then move to other areas with poor environmental quality. Without knowing their current district of residence, it would be impossible to separate prenatal pollution exposure from contemporaneous effects. In an ideal setting, current air pollution exposure would be used as an additional control. Unfortunately, the UK air monitoring system slowly switched networks during the survey period, and the paucity of data and changing measuring methods do not make this option available. In lieu of this, current district of residence is used as a rough proxy for current exposure. By controlling for current district of residence, I am able to isolate any time-invariant district effects that would influence employment and health outcomes. Previous papers have overcome this difficulty by assuming location changes were orthogonal to prenatal air pollution exposure or used an instrumental variable approach. This, to my knowledge, is the first paper to have access to both birth and current district of residence. One remaining concern is that those who are born in a year with high air pollution, compared to their birth district average, are more likely to move to a district at a time when the destination district is also experiencing relatively high pollution. This would require an individual to know both the average and birth-year levels of pollution in their district of birth, as well as the average and current levels of pollution in their destination district. They would then need to choose where to live based on this information. I view this as unlikely, however, as respondents would not have access to current pollution levels on a day-to-day basis. They might be aware that some districts have more air pollution than others, but would not be able to identify yearly pollution changes in their district. Most of the respondents were born in the 1960s and 1970s, when air pollution was still not well understood by the general population, and air pollution readings were not readily accessible. More importantly, all specifications contain a vector of district of birth by half-decade variables. Therefore, even if a district changes slowly over time, any variation will be accounted for. This allows me to plausibly isolate prenatal air pollution exposure from other variables. The advantages of my approach over similar papers are fourfold. First, English districts are less than a tenth of the size on average of US counties; this enables more accurate measurement of pollution exposure than much of the existing literature. Second, the BHPS asks a much larger battery of questions than the US Census, allowing me to test a series of different outcome variables. These include work status, overall health, and wages. Third, the National Health Service removes concerns about health insurance access. Fourth, restricted BHPS data include the current district of residence, not just of birth. This allows me to use their current location as a rough proxy for current air pollution exposure. The remainder of the paper is organized as follows. ""Effects of Air Pollution"" section discusses the epidemiology of air pollution and previous economic research. ""Data"" section describes the data from the BHPS and air pollution network. ""Empirical Approach"" section details the empirical approach. ""Results"" section delineates the results. ""Conclusion"" section concludes.",2
46,4,Eastern Economic Journal,19 May 2020,https://link.springer.com/article/10.1057/s41302-020-00168-2,Income Comparison and Subjective Well-Being: Evidence from Self-Perceived Relative Income Data from China,October 2020,Han Yu,,,,Unknown,Unknown,Mix,,
46,4,Eastern Economic Journal,17 July 2020,https://link.springer.com/article/10.1057/s41302-020-00175-3,Consumer Perception of Food Expiration Labels: “Sell By” Versus “Expires On”,October 2020,Onur Sapci,Ayse Sapci,,Male,Unknown,Unknown,Male,"In the absence of a federal law, states have enacted many date label laws, which makes it difficult for consumers to identify the exact safe date to consume their food products.Footnote 1 Although not passed in the Congress, the Food Labeling Act of 2016 has aimed to end the consumer uncertainty about whether their food is safe to eat by standardizing date labels. Following this attempt, the Grocery Manufacturers Association and the Food Marketing Institute are currently working on standardizing the date labels to offer greater clarity regarding the safety of products and to prevent the food waste.Footnote 2 Despite the interest of policymakers and the food industry, there are few empirical studies to understand how consumers perceive different types of date labels and whether (or how much) they worry about the uncertainty that these labels create. Our paper contributes to this important discussion by studying whether consumers perceive different signals from “sell by” and “expires on” labels, and therefore are willing to pay a premium to avoid any uncertainty on expiration dates. There are more than 10 labeling choices in the US grocery market such as “sell by,” “expires on,” “best by,” “best if used by,” “freeze by,” and “freshest by.” Among the most commonly used labels, Leib et al. (2016) show that “sell by” and “expires on” labels are in the opposite spectrum in terms of safety perception of consumers. They find that “expires on” is the one that is associated with food safety most (54%), whereas only 7% of the sample in the national survey associates “sell by” labeling as an indicator of food safety. Therefore, consumers seem to receive a more certain signal about expiration dates with “expires on” labeling. Our paper exploits this distinction between the perception of “sell by” and “expires on” labeling to study the effects of uncertainty on consumer behavior. Scientifically, a passed “sell by” date does not necessarily indicate that the food is unsafe to consume.Footnote 3 In fact, most of the time, the product would still be safe within a certain time period; however, this kind of labeling provides an unclear perception on the actual expiration date. The experiment, therefore, is designed to investigate whether there is a valuation difference between “sell by” and “expires on” types of food labeling. Huffman and Tegene (2002) and Rousu et al. (2007) show that information treatments affect consumer behavior significantly in other markets such as genetically modified foods. One important result of food labeling misconceptions is food waste. Several studies find that consumers waste food products as they near the date posted on the labels for perceived food safety reasons (Miles and Frewer 2001; Leib et al. 2013; Newsome et al. 2014; Wilson et al. 2017). In particular, Leib et al. (2016) show that over one-third of US consumers always discard food close to or past the date on the label, and 84% do so at least occasionally. Consumer misperceptions about the meanings of labels also result in a higher rate of unsaleable food for retail stores (Leib et al. 2013) and adds to the already high food waste rate nationwide, with the average household spending more than $2000 annually on wasted food (Gunders 2012; Roe et al. 2018). The US Department of Agriculture’s 2016 report (USDA) estimates that 30 percent of food is lost or wasted at the retail and consumer level. The Food and Drug Administration (FDA) estimates in their 2019 report that confusion over date labeling accounts for approximately 20 percent of consumer food waste. Therefore, understanding the differences in the consumer perception of more certain (“expires on”) and uncertain (“sell by”) labels would help consumers eliminate any confusion and possibly help decrease the food waste. Although an exact expiration date gives consumers clearer perception about food safety than “sell by” date, it is not evident how much consumers value this information. For instance, “sell by” February 14 is a type of information one might find on a meat or dairy product. Since this kind of label does not indicate by what date the product is safe to be consumed after the day of purchase, it is possible that consumers prefer labels with more clear information such as “expires on February 18” even though both labels give about the same amount of life span for the product. Such a distinction in perception could lead to a substantially different consumer behavior and therefore would have a significant economic impact given the size of the food industry. The economics and marketing fields include a number of studies that examine food labeling and food safety to shed light on consumer behavior (for instance, Blend and Van Ravenswaay 1999; Nimon and Beghin 1999; Shogren et al. 1999; Johnston et al. 2001; Laroche et al. 2001; Corsi and Novelli 2002; Loureiro et al. 2002; Baltzer 2003; Huffman et al. 2004; Darby et al. 2006; Aguilar and Vlosky 2007; Batte et al. 2007; Thrasher et al. 2007; Fortin et al. 2009; Hamzaoui Essoussi and Linton 2010; Achtnicht 2012; Botzen and Van den Bergh 2012; Kotchen et al. 2013; Diederich and Goeschl 2014; McFadden and Huffman 2017; Wang et al. 2018; Berger 2019; Bernard et al. 2019; Fan et al. 2019; Prieto et al. 2019; Rihn et al. 2019; Ritten et al. 2019; Corrigan et al. 2020; Wei et al. 2020). Especially, most studies focus on willingness to pay (WTP, thereafter) premium on eco-labeling, organic labeling, and homegrown labeling.Footnote 4 Our paper also complements the literature on measuring WTP for reducing uncertainty. These studies, however, are in the context of non-market goods and health benefits (for example, Loomis and Ekstrand 1998; List and Gallet 2001; Bishop and Boyle 2019; Subroy et al. 2019; Berry et al. 2020) rather than in the food industry. This paper contributes to the literature by using experimental auctions to explore whether there is a WTP premium between different food expiration labeling types, and if so, how would uncertainty affect consumer perception and behavior. We use an experimental auction methodology in this paper to measure the WTP between different types of labeling, because people may overstate the amount they are willing to pay when asked hypothetical valuation questions relative to when real money is on the line. In fact, experimental auctions put people in an active market environment and address the non-market valuation by creating a market (Lusk and Shogren 2007). Our experimental auction design aims to quantify how much consumers prefer certainty on product expiration dates by exploiting the difference between “sell by” and “expires on” labeling. Our findings show that when consumers are asked to compare “expires on” and “sell by” labels in a scenario where two labels infer similar life spans, they prefer exact information on product expiration and pay 27% higher price for the same good with the “expires on” label than the “sell by” label. In other words, consumers have a WTP premium equal to 27% to reduce uncertainty in food labeling.",4
47,1,Eastern Economic Journal,04 January 2021,https://link.springer.com/article/10.1057/s41302-020-00182-4,An Overview of the Economics of Sports Gambling and an Introduction to the Symposium,January 2021,Victor Matheson,,,Male,Unknown,Unknown,Male,"Gambling likely predates recorded history. The casting of lots (from which we get the modern term “lottery”) is mentioned both in the Old and New Testaments of the Bible, most famously when Roman soldiers cast lots for the clothes of Jesus during his crucifixion. In Greek mythology, Hades, Poseidon, and Zeus divided the heavens, the seas, and the underworld through a game of chance. Organized sports also have a long history. The Ancient Olympic Games date back to 776 BCE and persisted until 394 AD. The Circus Maximum in Rome, the home of horse and chariot racing events as well as gladiatorial contests for over one thousand years, was originally constructed around 500 BCE, and the Colosseum in Rome began hosting sporting events including gladiator fights in 80 AD. Variations of the ball game Pitz were played in Mesoamerica for nearly 3000 years beginning as early as 1400 BCE (Matheson 2019). Given the prevalence of both sporting contests and gambling across many ancient civilizations, it is natural to conclude that the combined activity of sports gambling also has a long history. And, indeed it is widely reported that gambling was a popular activity at the Olympics and other ancient Panhellenic events in Greece and at the racing and fighting contests in ancient Rome. Problems associated with gambling were also widely reported. As early as 388 BCE, the boxer Eupolus of Thessaly was known to have paid opponents to throw fights in the Olympics. Rampant gambling in Rome led Caesar Augustus (c. BCE 20) to limit the activity to only a week-long festival called “Saturnalia” celebrated around the time of the winter solstice, while Emperor Commodus (AD 192) turned the royal palace into a casino and bankrupted the Roman Empire along the way (Matheson et al. 2018). Just as in the modern day, gambling was often looked down upon by societal leaders in antiquity. Horace (Ode III., 24; 23 BCE) wrote, “The young Roman is no longer devoted to the manly habits of riding and hunting; his skills seem to develop more in the games of chances forbidden by law.” Juvenal (Satire I, 87; 101 AD), well-known for coining the term “bread and circuses” wrote, “Never has the torrent of vice been to irresistible or the depths of avarice more absorbing, or the passion for gambling more intense. None approach nowadays the gambling table with the purse; they must carry their strongbox. What can we think of these profligates more ready to lose 100,000 than to give a tunic to a slave dying with cold.” (Matheson 2019).",4
47,1,Eastern Economic Journal,04 January 2021,https://link.springer.com/article/10.1057/s41302-020-00178-0,"Legalized Sports Betting, VLT Gambling, and State Gambling Revenues: Evidence from West Virginia",January 2021,Brad R. Humphreys,,,Male,Unknown,Unknown,Male,"On May 14, 2018, the Supreme Court of the United States (SCOTUS) declared the Professional and Amateur Sports Protection Act of 1992 (PASPA) unconstitutional. PASPA made sports betting legal in Nevada and illegal in almost every other state. Following this SCOTUS decision, the power to legalize and regulate sports betting devolved to states. A number of states legalized sports betting soon after the decision. Some, like West Virginia, passed laws legalizing sports betting before the SCOTUS decision. 17 states (Delaware, New Jersey, Mississippi, West Virginia, Rhode Island, Pennsylvania, New Hampshire, Rhode Island, Arkansas, New York, Illinois, Indiana, Iowa, Montana, Colorado, North Carolina, and Tennessee) legalized sports betting as of early 2020. Many others are currently considering legalization. States legalize sports betting in order to generate new tax revenues, generally in the form of license fees for the operation of sports books and taxes on net revenues earned from bookmaking. However, most states that legalize sports betting already receive substantial tax revenues from other forms of gambling, like lotteries, video lottery terminals (VLTs), and casino table gaming. Understanding the fiscal consequences of legalizing sports betting requires understanding the relationship between consumer spending on sports betting and spending on other types of gambling in casinos. Limited evidence exists on the impact of legalizing or expanding sports betting on consumer spending on other forms of gambling, referred to as cannibalization in the literature. A recent survey article summarizing evidence on gambling market cannibalization found no existing research on the impact of legalizing or expanding sports betting on any other type of gambling (Marionneau and Nikkinen 2017). Only one paper, Room et al. (1999), found evidence that the opening of a new casino reduced the frequency of consumer sports betting, based on a single casino opening in Canada and none analyzed the impact of opening sports books on other casino revenues. Three papers (Miers 1996; Forrest 1999; Forrest and Pérez 2011) analyzed the impact of expansion of lotteries on sports betting in the form of football pools in the UK and Spain. The evidence in these papers suggests that the introduction of lotteries substantially significantly cannibalized football pool betting. Another line of research exploits cross-state changes in the availability of gambling to assess the extent to which spending on one type of gambling cannibalizes other types of gambling. Papers in this area include Walker and Jackson (2008), Farrell and Forrest (2008), Paton and Williams (2013), and Cummings et al. (2017). Most of these studies employ relatively aggregated data at the annual state or county level. These studies generally report strong evidence of cross-state or region cannibalization in gambling markets. This paper addresses the lack of evidence on the effect of expanding sports betting on casino revenues from table games and VLTs using data from West Virginia. This paper is the first to develop causal evidence that consumer wagering on sports causes decreases in consumer wagering on VLTs. The paper contributes to the literature on cannibalization in gambling markets by exploiting unusual sources of exogenous variation in the availability of gambling at the individual casino level. It also contributes to the literature analyzing gambling tax revenues in the broader context of state tax revenues (Nichols et al. 2015; Walker and Jackson 2011) by focusing on changes in two different streams of gambling tax revenues not previously analyzed in this literature. West Virginia casinos opened five new sports books between September and December 2018. In the prior fiscal year (July 2017 through June 2018), the state generated $38 million in tax revenues from table games and $253 million in tax revenues from VLTs located in casinos. Results from an instrumental variables estimation approach that also exploit the staggered opening of sports books and the shut down of two sports books for nearly a year in a difference-in-differences framework indicate that each additional dollar spent on sports betting in casinos in West Virginia reduced revenues from VLTs in casinos by $3.96, representing an elasticity of VLT revenues with respect to changes in sports betting handle of 0.18 at the mean of the distributions. Sports betting had no effect on revenues from table gaming in casinos. In terms of forgone tax revenues, West Virginia taxes net VLT revenues at 53.5% and net sports book revenues at 10%. The reduction in VLT revenues caused by the legalization of sports betting resulted in $45.4 million dollars in forgone VLT tax revenues at the four race track casinos from September 2018 until March 2020 when all West Virginia casinos closed due to health concerns from the novel coronavirus. In return, legalized sports betting in West Virginia generated $2.6 million in new tax revenues over this period.",6
47,1,Eastern Economic Journal,07 January 2021,https://link.springer.com/article/10.1057/s41302-020-00181-5,The Effect of Recreational Gambling on Health and Well-Being,January 2021,Brad R. Humphreys,John A. Nyman,Jane E. Ruseski,Male,Male,Female,Mix,,
47,1,Eastern Economic Journal,04 January 2021,https://link.springer.com/article/10.1057/s41302-020-00179-z,Integrity Fees in Sports Betting Markets,January 2021,Craig A. Depken II,John M. Gandar,,Male,Male,Unknown,Male,"In its 2018 decision in Murphy v. NCAA, the US Supreme Court invalidated the then federal law that made professional and amateur sports betting illegal in all states but Nevada, Montana, Delaware, and Oregon. In the months that followed the ruling, 23 states legalized sports betting or were in the process of doing so. The estimated legal market for betting in the USA approaches $5 billion per year and is predicted to reach $8 billion by 2025 (Associated Press 2019). In anticipation and with an eye toward profiting from this newly legal market, professional and amateur sports leagues began lobbying for a so-called integrity fee to be placed on the handle, that is, the total amount bet, collected by legal sports books, and remitted to the league for the purpose of monitoring the behavior of players, referees, managers and trainers, fans, and others involved with the games, in the context of fixing game outcomes. Although there is always an incentive to manipulate the outcome of games when wagering is involved, in a system where the vast majority of betting occurs in the underground economy there is no way for professional leagues to collect an integrity fee. Therefore, while an integrity fee might provide sufficient resources with which a league could reduce the possibility of successful game manipulation, it appears that one of the motivations for installing an integrity fee in the legal betting market is the ability to collect a tax in the licit economy. This paper reviews the existing literature on integrity fees, models one possible ramification of an integrity fee on the efficiency of betting markets, and shows how this might motivate sports books to offer more half-point lines which eliminate the possibility of pushes. As a case study, we describe the characteristics of recent betting lines in four North American sports and how sports books and bettors might respond to integrity fees placed on the total amount bet rather than on the total amount retained by the sports book after paying out all winnings. The distinction is important for the efficiency of the betting markets but also the sustainability of the newly legalized sports gambling industry in the USA.",2
47,1,Eastern Economic Journal,08 January 2021,https://link.springer.com/article/10.1057/s41302-020-00180-6,College Football Referee Bias and Sports Betting Impact,January 2021,Rhett Brymer,Ryan M. Rodenberg,Tim R. Holcomb,Male,,Male,Mix,,
47,1,Eastern Economic Journal,18 June 2020,https://link.springer.com/article/10.1057/s41302-020-00172-6,Breaking Bad: When Being Disadvantaged Incentivizes (Seemingly) Risky Behavior,January 2021,John Gibson,David Johnson,,Male,Male,Unknown,Male,"We investigate how an individual’s initial position influences their subsequent decisions. Individuals who start from positions of relative advantage or disadvantage may respond very differently than those who face a more neutral initial position. While we view this as a general phenomenon, we demonstrate the underlying mechanisms using a stylized model and economic experiment which focuses on subjects’ labor market behavior in the presence of debt. Our model and experiment produce consistent results which suggest that debt exerts a non-monotonic effect on subjects’ willingness to accept an initial wage offer, with subjects assigned high and low debt levels being less willing to accept initial wage offers than their counterparts who were assigned more moderate levels of debt. In general, our problem only requires the following two ingredients: (i) individuals who differ in their relative position prior to the decision being made, and (ii) some force which limits the severity of downside risk no matter their decision. Given this structure, we could consider many different domains; however, let us consider an example involving labor markets and mortgages. To start, picture an individual who lost their job but must maintain a large mortgage payment to avoid foreclosure. If this individual is immediately offered a low paying job, should they accept or reject the offer? The answer is “it depends.” If the individual has a sufficient amount of savings to service his mortgage and support a lengthy job search (that is, they are in an advantaged position), then it might be rational to reject the job offer, avoid costly effort, and hope for a better offer in the future. Likewise, if the individual has no savings and the low wage is insufficient to cover the mortgage payment (that is, they are in a disadvantaged position), then there is little reason to accept. If they do, they will incur costly effort and still suffer foreclosure. Therefore, it may be rational for individuals starting from either advantaged or disadvantaged positions to reject the initial low-wage job offer. In contrast, individuals with moderate savings may find the low paying job advantageous as it helps them service their mortgage without depleting their savings as they search for a higher paying job. The previous example is a useful starting point for two reasons; first, mortgages are a very common form of debt that most people are familiar with and, second, existing empirical literature suggests that mortgage exposure influences labor market behavior in a way similar to that described in our example. For instance, Oswald (1996) and Blanchflower and Oswald (2013) both find a positive relationship between home ownership and increased wage selectivity. While issues regarding housing and labor market decisions necessarily introduce further questions regarding endogeneity and mobility, we simply present this as a plausible real-world example of our mechanism. It should also be noted that the type of behavior described above is not limited to individuals or households. For example, Williams (2014) finds a u-shaped relationship between a bank’s capital holdings and their risk profile, thereby suggesting that both under and overcapitalized banks engage in higher levels of risk-taking than banks with more moderate levels of capital. This is particularly true for larger banks who may receive implicit bailout guarantees due to their “too-big-to-fail” status.Footnote 1 Much like in the mortgage example, banks who find themselves in trouble are incentivized to increase their portfolio of risky assets because in the worst-case scenario the government will (likely) bail them out. To formalize the elements described above, we develop a stylized labor market model and test the model’s predictions using an economic experiment. The model we use is a modification of a wage selection model in which agents are exogenously assigned a debt which places them along a continuum of positions ranging from extremely disadvantaged (very high debt) to extremely advantaged (no debt).Footnote 2 Our theoretical predictions and experimental results are broadly consistent with the intuitive scenarios described above, with both low- and high-debt individuals being significantly more likely to reject the initial wage offer, and holdout for a higher wage in the next round, than their counterparts who are assigned moderate levels of debt. While we primarily consider debt and labor market decisions as an illustrative example, improving our understanding of how debt influences labor market decisions is highly relevant as approximately 80% of Americans have some form of large financial obligation (Pew Charitable Trust 2015a).Footnote 3 Due to insufficient savings, these financial obligations are often paid using labor income. For example, Pew Charitable Trust (2015b) reports that 41% of households in America do not have enough savings to cover a $2000 expense, and 1 in 3 Americans lack any savings.Footnote 4 In our model, agents differ in terms of their risk preferences and the level of debt they have been assigned. Agents make a single choice – to accept or reject an initial wage offer in the first period. Agents who accept the initial wage offer earn this wage in both periods. Agents who reject the initial wage offer remain “unemployed” during the first period, but may receive a better or worse wage offer when they enter the second period. All agents, regardless of whether they accept or reject the initial wage offer, must pay their debt entirely before they receive their earnings. We limit the downside risk to agents by allowing for a type of bankruptcy protection that forgives any unpaid debt that remains at the end of the second period and ensures that the minimum payout an agent can earn is zero.Footnote 5 Given that we are comparing the results of our model to the behavior of subjects in an economic experiment, we adopt a fuzzy acceptance rule, or area of indifference, where the likelihood an agent accepts or rejects the wage offer is a function of the distance between the offer and their theoretical reservation wage. Variation in assigned debt levels can be thought of as defining a continuum of positions, with those assigned very high debts being disadvantaged and those with no debt being advantaged. The simulation results from our model indicate that this continuum encourages a non-monotonic effect on agents’ willingness to accept an initial wage, with low- and high-debt agents being significantly more likely to reject the initial wage offer than their counterparts assigned more moderate levels of debt. Additionally, the non-monotonic relationship is found to be strongest when subjects are assumed to make decisions under a moderate area of indifference. Put simply, agents assigned high or low levels of debt are incentivized to take the “risky” action and reject the initial wage offer, while agents assigned a moderate level of debt are not. We test the theoretic predictions of our model using an economic experiment that is designed and parameterized to closely mimic the structure of our model. Subjects are assigned to one of five debt treatments, and they are each offered the same initial wage. Subjects are told the distribution of wage offers that will result in the second period and are asked to make a single decision, accept or reject the initial wage offer. In order to test the area of indifference, we assign subjects into groups based off of their general willingness to take risk and reported attentiveness. We hypothesize that the dip in wage selectivity will be most prominent in subjects who are highly attentive and report a moderate willingness to take risk (that is, they have a narrow region of indifference). The results of the experiment are consistent with the predictions of our theoretical model. Specifically, in the full sample, we observe a statistically significant (at conventional levels) dip in the likelihood of rejecting the initial wage offer for subjects assigned a moderate debt level. Therefore, just as in our model, low- and high-debt subjects are significantly more likely to reject the initial wage offer than their counterparts assigned more moderate levels of debt. Additionally, when we compare behavior across groups, we find the dip in wage selectivity is most prominent in subjects who are highly attentive and report a moderate willingness to take risk.",
47,1,Eastern Economic Journal,06 October 2020,https://link.springer.com/article/10.1057/s41302-020-00176-2,Do Students Discriminate? Exploring Differentials by Race and Sex in Class Enrollments and Student Ratings of Instructors,January 2021,Robert L. Moore,Hanna Song,James D. Whitney,Male,Female,Male,Mix,,
47,2,Eastern Economic Journal,05 January 2021,https://link.springer.com/article/10.1057/s41302-020-00186-0,"Politicizing the Mask: Political, Economic and Demographic Factors Affecting Mask Wearing Behavior in the USA",April 2021,Leo H. Kahane,,,Male,Unknown,Unknown,Male,"In the spring of 1918 the Great Influenza Pandemic, commonly referred to as the ‘Spanish Flu,’ had made its way to the shores of the USA.Footnote 1 While statistics related to the pandemic are scarce, the Centers for Disease Control and Prevention (CDC) reports that an estimated 500 million people, or about one-third of the world population became infected. The total number of deaths is estimated to be 50 million. For the USA, the estimated number of deaths is 675 thousand.Footnote 2 Shortly after the flu was identified in March of 1918 at an Army base in Kansas medical authorities urged the use of face masks to fight against the spread of the virus. San Francisco was the first US city to implement a mask-wearing ordinance, signed into law by Mayor James Rolph on October 22, 1918. Various other cities followed San Francisco’s example with their own mask-wearing laws. While compliance was the norm, there was opposition as some saw the ordinances as a “symbol of government overreach.”Footnote 3 This opposition to mandated mask wearing crystalized in 1919 with the formation of the ‘Anti-Mask League’ in San Francisco, which led protests against mask wearing. Dolan (2020) writes that the Anti-Mask League protests, “might be cloaking deeper ideological or political divides.” In other words, opposition to mask wearing during the Great Influenza Pandemic may have reflected both a disbelief by some that masks were effective in reducing the spread of the deadly virus, as well as an example of government’s infringement on one’s personal liberty. Fast-forward approximately 102 years and we find ourselves in the middle of another pandemic, the Novel Coronavirus Disease, or COVID-19. Data now are much more reliable and accessible in comparison with the Great Influenza Pandemic. On March 11, 2020 the World Health Organization (WHO) declared COVID-19 as being a global pandemic.Footnote 4 The WHO reports that as of August 16, 2020 the cumulative number of confirmed COVID-19 cases is estimated to be 21.2 million with 760 thousand deaths worldwide.Footnote 5 The CDC reports that for the USA, total cases come to over 5.46 million with deaths totaling just over 171 thousand as of August 19, 2020.Footnote 6,Footnote 7 As with the case of the Great Influenza Pandemic, health officials are urging all people to wear facemasks when they are in public and within six feet of another person. The efficacy of facemask wearing is not in dispute as research has shown that facemasks are effective in reducing the spread of the virus.Footnote 8 Yet, as it was in 1918, there is opposition to face mask mandates as protestors in places like Provo, Utah and Tulsa, Oklahoma gathered to oppose such mandates. These protestors have found an ally in US President Donald Trump who has ignored the CDC’s urging of the use of facemasks. President Trump has politicized the issue as he noted on April 3, 2020 that the CDC recommendations are voluntary and stating, “You don’t have to do it. They suggested for a period of time, but this is voluntary. I don’t think I’m going to be doing it.”Footnote 9 Later, during a September 29 debate with Joe Biden, Trump chided Biden for wearing a mask, noting, “Every time you see him, he’s got a mask. He could be speaking 200 feet away from them, and he shows up with the biggest mask I’ve ever seen.”Footnote 10 Further, given the Trump administration’s reluctance to put forward a national mask-wearing mandate, a collection of individual states have implemented laws requiring facemasks. As of August 17, 2020, thirty-four states and Washington D.C. have mask mandates. Of the sixteen states that do not have a facemask mandate, all have a governor who is a member of the Republican Party. There is evidence of a general, growing partisan divide between Democrats and Republicans in the US over the last 4 decades (Boxell et al. 2020). Further, survey results by the Pew Research Center (2017) show that the growth in this division has accelerated under Donald Trump’s presidency. Bordalo et al. (2020) notes that when the division between Democrats and Republicans grows this typically leads to greater political engagement (e.g., voting, participation in political campaigns, political contributions). Such polarization and increased partisan awareness would seem to create conditions where the views and the behavior of the president could have strong influences on those who support him. In this light, considering President Trump’s reluctance to impose a nation-wide requirement for wearing masks while in public, and his own unwillingness to personally wear a facemask, a question arises: has President Trump’s views and behavior regarding masks had an impact on the mask-wearing behavior of those who support him? The goal of this paper is to explore this potential linkage empirically. Using county-level survey data, collected by the firm Dynata at the request of the New York Times, econometric results show a significant, negative relationship between mask-wearing behavior and county-level voting for Donald Trump in the 2016 presidential election. Understanding this linkage is important as it highlights how partisan divisions and powerful influences by political leaders may lead to suboptimal decisions by individuals that are costly both economically and in terms of public health. The remainder of this paper is organized as follows. The next section provides some discussion of related literature. Section three follows with the empirical model employed and a description of the data used to test it. Section four contains the estimations results. Section five contains some concluding thoughts.",58
47,2,Eastern Economic Journal,07 January 2021,https://link.springer.com/article/10.1057/s41302-020-00183-3,Do Credit Unions Serve the Underserved?,April 2021,Pankaj K. Maskara,Florence Neymotin,,Male,Female,Unknown,Mix,,
47,2,Eastern Economic Journal,01 February 2021,https://link.springer.com/article/10.1057/s41302-021-00187-7,Unintended Consequences: Ambiguity Neglect and Policy Ineffectiveness,April 2021,Lorán Chollete,Sharon G. Harrison,,Unknown,Female,Unknown,Female,"And it came to pass...that he said, Escape for thy life; look not behind thee... lest thou be consumed... But his wife looked back from behind him, and she became a pillar of salt. Genesis 19: 17–26. Individuals do not always follow orders, even when the consequences may be highly detrimental to them. We may fail to adhere to the choices that a policy intended to elicit because of resource constraints, stubbornness, behavioral considerations, bounded rationality, and other factors that lead us to depart from obedience. If enough citizens disobey a policy mandate, the policy may end up bringing about unintended consequences. Our concept of unintended consequences denotes policy outcomes that differ from the goals of the policymaker. This definition builds on that of Merton (1936), who used the term ‘unanticipated consequences’ in sociology.Footnote 1 When a government enacts a novel policy that affects us, we may not even know in advance how we shall respond. Hence, in novel situations, some individuals have to construct their choices. Sometimes the new choice involves a simple re-optimization in light of an added budget constraint. At other times, the novel policy may end up creating a new market (Siebert 2003), or distort resource allocation when attempting to introduce a beneficial social norm (Antecol et al. 2018). It is well known from consumer theory that adding an extra constraint can modify choices in nontrivial ways, in particular if the good is a necessity or Giffen good, if there is failure of preference convexity, or if the previous optimum was at a corner solution (see chapters 2 and 3 of Mas-Colell et al. 1995). There is also a substantial psychology literature on preference construction.Footnote 2 This latter research emphasizes that in actual decision-making contexts, individuals often need to construct their preferences. The above considerations suggest an inherent ambiguity in the case of novel policies: Policymakers do not know the probability that individuals’ choices will respond to the policy in a particular manner, because the individuals themselves do not know. At times, policymakers nevertheless implement such novel policies, without accounting for the potential uncertainty in the probability of citizen compliance. For example, in this paper we will use the policy discussed in Davis (2008). In Mexico City, a desire to combat pollution led to a policy that mandated limits on days that each car could be driven. This resulted in citizens’ purchasing additional cars, which yielded an increase in pollution. Sometimes a policymaker has to solve a society-wide issue by either implementing a policy or else do nothing. If she implements the policy, she has to decide what she believes about citizens’ preferences or choices. In particular, she may assume that citizens’ spending patterns will roughly stay the same, and they will continue economic activity that is similar to what they did before. What she does not account for is that, in response to her policy, some citizens may materially change their choices and behavior. Evidently, this new behavior could encompass non-compliance with the law. However, even if citizens comply with the letter of the law, they may find legal ways to circumvent the spirit of the law (such as purchasing an extra car when there are limits placed on how much each car can be driven). We refer to policymakers’ disregard of uncertainty as ambiguity neglect. While aversion to ambiguity has been studied in a variety of related contexts (Dequech 2000), the topic of willful neglect of ambiguity has received rather less attention. The focus of our paper is on unintended consequences, as outlined above, and developed in our motivating examples of “Our Motivating Examples” section. Ambiguity plays a role in the following manner. In particular, ambiguity manifests itself when policymakers implement policy solutions for extant challenges (such as pollution), under the assumption that all citizens will follow the policies. In this regard, our model is related to the recent literature on unforeseen contingencies, as studied by Ghirardato (2001), Epstein et al. (2007), and Kochov (2018), among others. Our work is also related to previous decision theoretic literature on ambiguity (Gilboa and Schmeidler 1989; Schmeidler 1989). This literature sees ambiguity as a central concern. Building on such literature, we are motivated by policy situations such as the above. In such policy situations, the policy decisionmaker did not attend to ambiguity that arose from other actors (e.g., ‘disobedient’ citizens). The policymaker may neglect ambiguity for a number of reasons, such as career concerns (Brunnermeier et al. 2013), limited Bayesianism (Minardi and Savochkin 2019), and behavioral factors (Benartzi and Thaler 1995; Kahneman 2011). We further discuss the role of ambiguity neglect in “Discussion” section. Given our policy focus in the present paper, we incorporate a reduced form for capturing the role of ambiguity. Our model summarizes the above considerations by saying that the policymaker does not account for a particular type of ambiguity; namely, that the probability p of disobedience is unknown and potentially nonzero.Footnote 3 What should a policymaker learn from our model? The present paper indicates a basic mechanism through which policies can become ineffective and thereby generate unintended consequences. The mechanism is that by assuming that citizens will obey the policy, it runs the risk of backfiring. The reason is that some citizens may disobey the policy and therefore make the policy ineffective. The main lesson for policymakers is that before implementing novel policies, they should attempt to uncover potential sources of ambiguity. In particular, as discussed in Conclusion, policymakers can attempt to glean a sense of citizens’ responses to the policy beforehand, by conducting questionnaires about their expected behavior. Once the potential for ambiguity is perceived, then the policymaker will be able to express attitudes towards ambiguity, be they ambiguity averse, ambiguity neutral, or ambiguity seeking. Our goal in this paper is to present a simple model of unintended consequences in policy, which may serve as a starting point for future explorations of unintended consequences. One implication of this model is that before implementing new policies, governments and policymakers should attempt to elicit preferences from the citizens who will be affected. The model also lends itself well to being evaluated in empirical and experimental settings.Footnote 4 The rest of this paper proceeds as follows. In the next section, we discuss some examples of unintended consequences, along with some frameworks within which we place these examples. In “A Building Block for the Model” and “Our Model of Unintended Consequences” sections, we present our model and discuss its implications. In “Conclusion” section, we conclude.",1
47,2,Eastern Economic Journal,28 January 2021,https://link.springer.com/article/10.1057/s41302-020-00185-1,"Natural hazard, employment uncertainty, and the choice of labor contracts",April 2021,Lopamudra Banerjee,,,Unknown,Unknown,Unknown,Unknown,,
47,2,Eastern Economic Journal,24 May 2020,https://link.springer.com/article/10.1057/s41302-020-00170-8,The Islamic Rate of Return Versus the Nominal Rate of Interest: A Macroeconometric Model,April 2021,Ibrahim L. Awad,,,Male,Unknown,Unknown,Male,"Numerous Muslim religious scholars regard nominal interest as Riba (or usury) which is prohibited under the Islamic law. The prohibition of Riba extends to any payment or receipt of interest even among commercial banks or between a central bank and commercial banks (Fahmy 2006). The reasons for the prohibition of Riba, as reported by Muslim religious scholars, are as follows (Iqba and Mirakhor 1999)Footnote 1: (1) it is ex-ante, hence it lacks accuracy, (2) it is tied to a time period and the amount of the principle, regardless of the business outcomes, (3) its payment is guaranteed by the borrower, regardless of the directions of the price level and the economic activity, and (4) the borrower is obliged to pay it at maturity date, or otherwise the interest amount replicates. Yet, not all Muslim religious scholars recognize interest rates as Riba.Footnote 2 In Egypt, for example, there have been lots of conflicting Fatwa (or statements) in different times by Muslim religious institutions and Muslim religious scholars about whether interest rates are Riba or not (available in Arabic upon request).Footnote 3 In addition, the controversy about the legitimacy of the nominal interest rate occurs concerning the explanation of the Arabic word Riba which means ‘an increase’ or ‘an excessive increase’ over the principle. Under the definition of Riba as ‘an excessive increase’, a positive nominal interest rate is permissible (Noorzy 1982). Even with the definition of Riba as ‘an increase’, the definition itself is ambiguous, as it does not differentiate between ‘nominal’ increase and ‘real’ increase. The recognition of Riba as a real increase over the principle will permit the positive nominal interest rate, which will offset the rate of inflation. Indeed, such a fuzzy situation by Muslim religious institutions and Muslim religious scholars about the legitimacy of nominal interest rates has existed for decades in a large number of Arab and Muslim countries, which, in turn, produces counterproductive effects on the economic performance where most people are still confused about whether or not to use conventional banks. Moreover, with the existence of conventional banks along with Islamic banks or even Islamic windows inside conventional banks, the image is darkened where central banks are unable to use the same monetary policy instrument, i.e., short-term nominal interest rate, to manage both Islamic and non-Islamic banks.Footnote 4 In light of this, this paper investigates the question of “will the replacement of the nominal interest rate by the expected real rate of Islamic return have positive consequences on the macroeconomic performance?” In other words, the study examines the potential effects on the macroeconomy when the nominal interest rate is replaced by the Islamic real rate of return. Firstly, the study explores the aspects of the nominal interest rate compared with the Islamic rate of return. Secondly, the study examines the potential effects of the nominal interest rate and the Islamic real rate of return on macroeconomic variables including real GDP, demand for money, inflation rate, and real exchange rate. For this purpose, the study employs a descriptive methodology to explore the characteristic features of both nominal interest rates and Islamic rates of return. In addition, to estimate and compare the effects of the nominal interest rate and the Islamic rate of return on macroeconomic variables, the study adopts a dynamic small-scale macroeconometric model to be solved by a stochastic simulation under different scenarios for interest rate and Islamic rate of return. It goes without saying that the religious issue of whether the nominal interest rate is Riba or not is out of the interest of this paper. Specifically, this paper treats all religious diversified views on Riba on the same equal footing within a dynamic macroeconometric model and investigates their potential effects on the macroeconomy. The novelty offered by this paper includes three dimensions. (1) The topic is new where this paper attacks a highly controversial issue either in the field of conventional economics or in the field of Islamic economics. Under mixed financial systems that incorporate conventional banks and Islamic banks, the association between the nominal interest rate and the Islamic rate of return is crucial for the central bank to efficiently manage monetary policy. (2) The econometric methodology employed by this paper is relatively new for that kind of research. To the best of my knowledge, there is no previous study in the field of Islamic economics which attacks this issue using such a methodology. (3) The results of the study offer important implications for monetary policy makers and for settling the dispute among Muslim religious scholars. One important implication by this study is that “the zero Islamic real rate of return or equivalently the zero real interest rate” is the best option for monetary policy, especially with a dual banking system where conventional banks and Islamic banks are working together under the same rules of monetary policy. The remainder of this paper is as follows: second section explores the similarities between nominal interest rate and Islamic rate of return, third section describes the macroeconometric model and the behavioral relations between macroeconomic variables, fourth section discusses the results of stochastic simulations under the baseline model and the three model-scenario over the Islamic rate of return, and final section offers concluding remarks.",
47,2,Eastern Economic Journal,06 October 2020,https://link.springer.com/article/10.1057/s41302-020-00177-1,General Theorizing and Historical Specificity in the ‘Keynes Versus the Classics’ Dispute,April 2021,Teodoro Dario Togati,,,Male,Unknown,Unknown,Male,"There are five main reasons why we might regard Keynes as pursuing an ‘internalist’ approach to generality. For simplicity’s sake, we shall regard these reasons as ‘pillars’ of his generalist strategy stressing different properties of the GT. The first pillar is Keynes’s focus on the ‘intrinsic’ characteristics of a real-world ‘monetary economy’, which make it quite different from the ‘real wage economy’ abstraction underlying standard theory. As he himself put it: Moreover, the characteristics of the special case assumed by the classical theory happen not to be those of the economic society in which we actually live, with the result that its teaching is misleading and disastrous if we attempt to apply it to the facts of experience. (Keynes 1936, p. 3, my italics) More specifically, Keynes placed a lot of emphasis on characteristics, such as true uncertainty and conventional behaviour, as well as on institutions of modern capitalism such as the stock market, fiat money and elasticities of production and substitution, which explain both why this economy is irreducible to a barter economy (for example, the money interest rate cannot be put on a par with the ‘own’ rates of interest of other commodities) and a lack of effective demand is possible. In particular, I suggest that in the GT, this phenomenon is not ‘caused’ but simply ‘enabled’ by the above institutions. It is ultimately due to behavioural drivers, such as agents’ decisions to hold money rather than buy goods.Footnote 8 The second pillar of Keynes’s internalist approach is his belief that scientific analysis should be carried out in terms of ‘self-contained models’, and his main aim was to improve Pigou’s model. As he pointed out in his letter to Harrod, ‘Progress in economics consists almost entirely in a progressive improvement in the choice of models. The grave fault of the later classical school, exemplified by Pigou, has been to overwork a too simple or out of date model, and in not seeing that progress lay in improving the model’ (Keynes 1938, p. 296). Indeed, his revolution, establishing macroeconomics as an autonomous subject, can be regarded as the generalization of the Marshallian partial equilibrium model to the economy as a whole. Another pillar of Keynes’s strategy is the link between the generality of his theory and its ability to account for multiple equilibria referring to actual, ‘internal’ states of the economy: that is, those corresponding to different levels of unemployment, in contrast with standard theory associated with full employment: I shall argue that the postulates of the classical theory are applicable to a special case only and not to the general case, the situation which it assumes being a limiting point of the possible positions of equilibrium […] the classical theory is only applicable to the case of full employment, it is fallacious to apply it to the problems of involuntary unemployment. (Keynes 1936, pp. 3, 16) Keynes’s argument has two major implications. First of all, it implies the existence of a ‘direct’ relationship between theory and ‘external reality’ (see, for example, Pernecky and Wojic 2019, p. 773). In particular, he thought it was possible to define various employment states ‘objectively’ and to establish a one-to-one correspondence between such states and theories. Secondly, Keynes regarded his theory as covering the normal case of the economy, namely persistent high unemployment, in contrast with Pigou for whom unemployment is only a cyclical phenomenon due to short-lived ‘external’ shocks. The fourth pillar of Keynes’s internalist approach is his proposed view of rationality, broader than the standard one. While the latter stresses atomistic agents’ substantive rationality in abstract, oversimplified contexts, Keynes focused instead on how they actually behave in real-world contexts. As he clarified, especially in his 1937 summary article formulating the ‘practical theory of the future’, in order to make decisions in such contexts, agents are forced to rely upon conventional techniques, such as assuming that tomorrow will be like today or falling back on other agents’ views. It can be argued that in his works, Keynes mainly provided a descriptive account of such techniques,Footnote 9 which led him to capture essentially the ‘psychological’ dimension of conventions—i.e. the fact that they are formed through interaction of individual agents, each seeking to find reassurance in the behaviour of others—rather than their theoretical dimension. He failed to underline, for example, that they are not just ‘internal’ market structures; being forms of precarious knowledge, they also call for ‘external’ anchors, including institutional factors, to be sustainable through time.Footnote 10 Moreover, he took the aggregate propensities underlying his model as quite impenetrable ‘data’.Footnote 11 Keynes nevertheless regarded conventions as playing a key role in his theory. On the one hand, he described key variables, such as the monetary rate of interest and the money wage, as conventional features;Footnote 12 on the other, he used them to support his generality claim. He noted that standard theory is just one particular conventional technique; for example, he accused ‘…. classical economic theory of being itself one of these pretty, polite techniques which tries to deal with the present by abstracting from the fact that we know very little about the future’ (1937, p. 115). In particular, Keynes hinted at two different aspects of standard theory as a convention that should be distinguished: this theory can be regarded either as a technique used by economic agents to lull their disquietude in the face of uncertainty (it amounts to projecting the existing situation into the future ‘to save the face of rational men’) or a technique used by theorists themselves to lull the horror vacui of instability (for example, it amounts to relying on various ceteris paribus assumptions). In this case, standard theory could be regarded not as a true statement about the world or a substantive theory, but simply a useful heuristic assumption to describe complex behaviour. This is the sense in which Keynes himself treated the key standard assumption of maximizing behaviour in the GT. Indeed, one key implication of his principle of effective demand—seen as substantive theory accounting, so to speak, for agents’ ex ante behaviour—is that firms do not primarily act by trying to maximize profit (for example, they decide how much labour to hire on the grounds of expected demand, not by looking at the real wage and labour productivity, as in standard theory which takes maximizing behaviour as the key starting point of the analysis). Following his Marshallian background, Keynes used the optimizing assumption merely to characterize ex post equilibrium in his aggregate demand and supply model. In other words, he actually viewed firms as maximizers, but only in the context of the principle of effective demand as key driver of the analysis. The last pillar of Keynes’s internalist approach concerns his ‘orderly method of thinking’, according to which, in order to deal with complex reality, where the relevant factors are organically interdependent, a theorist cannot consider all variables at once but is compelled to make simplifying distinctions such as the isolation of the true causal factors and the ‘freezing’ of other variables, at least temporarily.Footnote 13 Keynes clarified his approach in chapter 18 of the book in particular, where he summarizes his ‘static’ analysis of the determinants of income (i.e. relating to equilibrium at a point in time) by making a distinction between two types of givens, which can be labelled respectively ‘primary’ and ‘secondary’. On the one hand, he singled out his ultimate independent variables as consisting of (1) the three fundamental psychological factors, the psychological propensity to consume, the psychological attitude to liquidity and the psychological expectation of future yield from capital-assets, (2) the wage-unit as determined by the bargains reached between employers and employed, and (3) the quantity of money as determined by the action of the central bank. (Keynes 1936, 247) On the other, he isolated the ‘given’ factors, which essentially correspond to the deep parameters of standard theory, that are preferences and resources: We take as given the existing skill, the existing quality and quantity of available equipment, the existing technique, the degree of competition, the tastes and habits of the consumer, the disutility of different intensities of labour and of the activities of supervision and organization, as well as the social structure including the forces, other than our variables set forth below, which determine the distribution of the national income. This does not mean that we assume these factors to be constant; but merely that in this place and context, we are not considering or taking into account the effects and consequences of changes in them. (ibid., 245) On this basis, one can understand why Keynes’s ‘orderly method’ supports his generality claim. While not focusing on growth explicitly, it can be argued that in principle, Keynes regarded effective demand as being relevant to discuss not just fluctuations but also growth.Footnote 14 One implication of his organicist perspective—in contrast with the ‘strict independence’ view underlying mainstream theory—is that when the passage of time is considered, the primary demand factors are not truly independent but can be shaped by interaction with the secondary factors. This means that aggregate demand represents both the cause and effect of key trends or phenomena, such as technological progress or population growth, namely changes in the factors that Keynes takes as ‘secondary’ givens. On the one hand, it is clear, for example, that without a sufficiently high propensity to invest there can be no technological progress. On the other, there is no doubting that the latter also influences aggregate demand.",2
47,2,Eastern Economic Journal,08 March 2021,https://link.springer.com/article/10.1057/s41302-021-00188-6,"Critiques, Ethics, Prestige and Status: A Survey of Editors in Economics",April 2021,Ann Mari May,Mary G. McGarvey,Mark Killingsworth,Female,,Male,Mix,,
47,3,Eastern Economic Journal,17 May 2021,https://link.springer.com/article/10.1057/s41302-021-00192-w,Routine-Biased Technological Change and Declining Employment Rate of Immigrants,June 2021,Younjun Kim,Eric Thompson,,Unknown,Male,Unknown,Male,"It is well documented how routine-biased technological change has shaped labor market outcomes (Autor et al. 2003). Computerization and automation have substituted for workers in routine task-intensive jobs and pushed them to other non-routine occupations, which has led to the employment polarization (Goos and Manning 2007). Cortes et al. (2017) report which demographic groups (e.g., gender and age) were likely to lose their employment from routine jobs. However, it is still less clear whether these impacts are the same for immigrant workers and native workers. Immigrant workers are not perfect substitutes for native workers due to their imperfect language skills (Peri and Sparber 2009). When computerization and automation substitute for workers in routine jobs, switching to non-routine cognitive jobs may be challenging for immigrant workers due to their lower English proficiency.Footnote 1 Language skills are important in non-routine cognitive jobs, such as managers, technicians, and professionals because these jobs require frequent communication activities to manage organizations or discuss new ideas (Chiswick and Miller 2010; Imai et al. 2019). Therefore, we expect that computerization and automation would decrease the employment-to-population ratio (employment rate, hereafter) of immigrants. This impact would be greater for immigrant workers with lower English proficiency. Switching from routine jobs to non-routine cognitive jobs would be more challenging for immigrant workers than for native-born workers. We test these hypotheses by estimating impacts of local labor-market technology adoption on change in the employment rate of foreign-born population over the last three decades (1980–2010). Following Autor and Dorn (2013), local labor-market technology adoption is approximated by the initial commuting-zone employment share in high routine-intensity-index occupations because commuting zones specialized in routine intensive occupations have greatest potentials to replace workers with computers and machines. In regressions, this share is instrumented by predicted employment share in high routine-intensity-index occupations in 1950 based on industry mix. We find that local labor-market technology adoption decreases the employment rate of the foreign-born population, but not of the native-born population. This impact is greater for foreign-born population with lower English proficiency. We also find that switching from routine jobs to non-routine cognitive jobs is more challenging for foreign-born workers than for their native-born counterparts. All these results are consistent with our hypotheses. Our study provides an explanation for the decline in the employment rate in the USA. Abraham and Kearney (2020) review various reasons why the employment rate is declining in the USA. Our study adds a new explanation that computerization and automation decrease the employment rate of immigrants. Our study also provides insights to the findings of Basso et al. (2020), a parallel study estimating impacts of computerization on immigrant employment. They find that immigrant workers promote native workers’ skill upgrade to non-routine cognitive jobs. However, our study suggests the opposite view that this may result from immigrant workers’ skill downgrade to non-routine manual jobs; switching from routine jobs to non-routine cognitive jobs is more challenging for immigrant workers than for native workers. Our study adds new evidence to the literature showing the importance of language skills in immigrant employment. Warman and Worswick (2015) found that recent immigrants in Canada have not benefited from wage increase in non-routine cognitive jobs due to their lower language ability than earlier immigrants. Hunt (2015) found that immigrants holding engineering degree earn less than their native counterparts due to their imperfect English. Our study suggests that language skills are important for immigrants when they switch from routine jobs to non-routine jobs. Such challenges due to imperfect language skills add nuance to broader findings on the economic contributions of immigrants. Hunt and Gauthier-Loiselle (2010) find that college-educated immigrants increase patenting. Peri et al. (2015) report that foreign-born STEM (science, technology, engineering, and mathematics) workers improve total factor productivity growth.Footnote 2 The next section explains how routine-biased technological change affects the employment rate and occupational employment. Section 3 describes our model and explains how to approximate local labor-market technology adoption. Section 4 presents estimation results, and Section 5 discusses the results.",2
47,3,Eastern Economic Journal,18 March 2021,https://link.springer.com/article/10.1057/s41302-021-00190-y,The Nonlinear Unemployment-Inflation Relationship and the Factors That Define It,June 2021,Andrew Keinsley,Sandeep Kumar Rangaraju,,Male,,Unknown,Mix,,
47,3,Eastern Economic Journal,04 January 2021,https://link.springer.com/article/10.1057/s41302-020-00184-2,Minimum Dropout Age and Juvenile Crime in the USA,June 2021,Md. Abdur Rahman Forhad,,,Unknown,Unknown,Unknown,Unknown,,
47,3,Eastern Economic Journal,27 May 2021,https://link.springer.com/article/10.1057/s41302-021-00193-9,Assessing the Employment Effects of California’s Paid Family Leave Program,June 2021,Samantha Marie Schenck,,,Female,Unknown,Unknown,Female,"Many American families have a difficult time balancing their obligations at work with their responsibilities at home. This is especially the case when a member of the family needs an increased level of caregiving, for instance after the birth of a child or when a family member is seriously ill. Governments around the world have passed legislation to make these difficult times easier for workers by mandating that employers provide paid family leave to their workers (Addati et al. 2014). However, the US federal government mandates only 12 weeks of job-protected leave through the Family and Medical Leave Act of 1993 (FMLA), which only covers approximately 60% of US workers due to extensive eligibility requirements and is unpaid (Klerman et al. 2012). The result is that US workers are often unable to take leave when they experience increased responsibilities at home or take shorter leaves than they would like (Klerman et al. 2012). Not only does this mean that many workers are dealing with a stressful change in their family situation, but they could also be facing the loss of their job or the possible financial hardship of taking unpaid leave (Horowitz et al. 2017).Footnote 1 For these reasons, many family leave advocates have argued for expanding the FMLA to include a paid provision. While national polling indicates that the majority of Americans from across the political spectrum support a paid family leave program, there is no consensus on how generous the program should be or who should pay for it (Mathur et al. 2017; Milkman and Appelbaum 2013). As a result, some states have chosen to take matters into their own hands and pass their own legislation. In September of 2002, California made history when it passed the nation’s first comprehensive paid family leave legislation (Bartel et al. 2014; Baum and Ruhm 2016; Milkman and Appelbaum 2013; Rossin‐Slater et al. 2013). The California Paid Family Leave Act (CAPFL) was the first law of its kind in the USA that provided employees with paid leave for the express purpose of bonding with a new child or to take care of a sick relative. A handful of states followed suit, and currently seven other states and the District of Columbia have passed their own Paid Family Leave legislation.Footnote 2 There are also several statewide campaigns across the country, which are gaining momentum, and paid family leave legislation was recently introduced in the US Senate (Bryant 2019; Noguchi 2019). While these political initiatives, on both the state and federal level, have sought to expand FMLA to include a paid provision, many have been unsuccessful or have stalled in the political process due to the strong opposition to another employer mandate (Milkman and Appelbaum 2013). Opponents argue that paid family leave mandates will place additional costs on employers, which may cause a decrease in employment and wages. They argue that governments should instead focus on “pro-growth policies” and other reforms that enhance efficiency and choice, leaving paid family leave benefits up to negotiations between the worker and their employer (Calder 2018; Pinsker 2019). It is one of the reasons why the USA continues to be the only industrialized country that does not provide paid maternity or family leave benefits on a national level.Footnote 3 Up until recently, it was impossible to study how US employers would respond to paid family leave mandates, since no paid leave mandates existed in the USA. However, now that individual states are implementing their own programs, it is now possible to study how mandated paid family leave has impacted the labor market outcomes in these states. This study treats the implementation of the California Paid Family Leave program as a quasi-natural experiment and uses several difference-in-difference, fixed-effects regression analyses on a panel dataset of business establishments in order to better understand how it impacted California’s labor market. To date, there have been very few studies on the labor market impact of California’s Paid Family Leave program from the employer perspective, and there have been no large-scale studies that have looked at the impact on establishment-level employment. Existing studies of the program have focused mainly on mothers and how the law impacted their employment and earnings after the birth of a child or have been small-scale studies on the attitudes of employers (Appelbaum and Milkman 2011; Baum and Ruhm 2016; Milkman and Appelbaum 2013; Rossin-Slater 2018; Rossin‐Slater et al. 2013). The gap in the existing literature is largely due to the difficulty of obtaining establishment-level employment data (Bartel et al. 2014). This study uses a unique dataset provided by the Equal Employment Opportunity Commission (EEOC) that contains employment records for a large panel of firms from across the USA over 10 years.Footnote 4 Using these data, it is possible to track establishments and their employment decisions over time, both before the California law was enacted as well as after. By doing so, it has the potential to expand our knowledge in this area by studying how California’s Paid Family Leave Program impacted a subset of employers and how it may have changed their employment decisions.",
47,3,Eastern Economic Journal,11 May 2021,https://link.springer.com/article/10.1057/s41302-021-00191-x,Wealth Inequality and the Financial Accumulation Process,June 2021,Alan G. Isaac,,,Male,Unknown,Unknown,Male,"Economists have long recognized that the distribution of wealth is remarkably unequal (Pareto 1897; Lorenz 1905). In the USA, the Gini coefficient of inequality currently exceeds 80%, and the top 5% of the wealth distribution hold more than half of total wealth (Cagetti and De Nardi 2008; Díaz-Giménez et al. 2011). Although high wealth inequality at the country level is common, there is substantial variability across countries. The smooth kernel histogram in Fig. 1 summarizes the distribution of 148 country Gini coefficients, thereby providing a nonparametric window on the variation across countries. Such divergent wealth inequality experiences have intrigued researchers and policy makers, who wish to understand or influence the level of wealth inequality. Data Source: Davies et al. (2009)  Wealth inequality across countries. An unweighted kernel density plot of wealth Gini coefficients for 148 countries exposes substantial cross-country diversity in wealth inequality. Mainstream economists have produced a large theoretical literature on wealth accumulation. This is typically rooted in refinements of the 1960s life-cycle model of consumption and saving (Ando and Modigliani 1963). From a life-cycle perspective, wealth accumulation is a by-product of intertemporal consumption smoothing: saving during peak-earning years buffers consumption in low-income years. Economists quickly recognized that the resulting models struggle to explain observed wealth inequality: they typically predict levels of wealth inequality that are too low (Atkinson 1971; Oulton 1976; Huggett 1996). The gap between these predictions and the empirical evidence is the wealth concentration puzzle, and its challenge to the life-cycle model persists (Cagetti and De Nardi 2008; Hubmer et al. 2016). Consumption smoothing does not appear to offer an adequate explanation of observed levels of wealth inequality. An alternative approach to understanding wealth inequality traces to the 1950s. Models of the financial accumulation process focus on the evolution of wealth that is implied by the compounding of random investment returns (Sargan 1957; Wold and Whittle 1957). In these models, high wealth is more likely to produce high investment income that when saved further increases wealth. (The term investment income denotes the returns generated by wealth.) This financial accumulation process underpins many individual-based computational models of the distribution of wealth (Yunker 1999; Levy and Levy 2003; Isaac 2008; Biondi and Righi 2019). These models discard life-cycle saving behavior in favor of simpler rule-based saving behaviors. Saving rules more easily accommodate some evidence on wealth accumulation, such as wealth increases by high-income households long after retirement (Land and Russell 1996; Smith et al. 2009). In stark contrast to life-cycle approaches, the simplest models of the financial accumulation process tend to predict levels of wealth inequality that exceed observed values. However, computational research has demonstrated that taxing and redistributing investment income can substantially dampen the wealth inequality generated by the financial accumulation process (Yunker 1999; Isaac 2008; Biondi and Righi 2019). Our paper adds to the computational research on the financial accumulation process. It extends a simple individual-based computational model in order to incorporate additional behavioral, cultural, institutional, and technological influences on long-run wealth inequality. Our simulation experiments demonstrate the potential importance of each of these influences. The paper is organized as follows. The ""Computational Model"" section  presents our computational model of the financial accumulation process. It then introduces a baseline parameterization of the model that reproduces the traditional results, described above. This baseline model provides the point of comparison for the computational experiments of the ""Simulation Experiments"" section. This paper describes four experiments, which demonstrate the sensitivity of the baseline results to individual behavior, cultural institutions, fiscal policy, and technological change. Specifically, we extend the baseline model to consider the consequences for wealth inequality of heterogeneous saving behavior, alternative marriage practices, wealth taxes, and the growth of labor income. We relate these extensions to the existing literature and demonstrate that each can strongly affect wealth inequality. The final section summarizes the experimental results and draws conclusions.",
47,3,Eastern Economic Journal,11 March 2021,https://link.springer.com/article/10.1057/s41302-021-00189-5,Concrete Strategies for Economics Tenure-Track Faculty and Their Mentors,June 2021,Jeffrey Wagner,,,Male,Unknown,Unknown,Male,"This paper proposes some concrete research and teaching strategies for tenure-track assistant professors of economics and their mentors, particularly at universities with a significant undergraduate teaching mission (in other words, with undergraduate teaching loads between 2-2 and 4-4). (The author’s teaching load is 3-2.) Mentoring in general, and mentoring with respect to the tenure-track, is increasingly common and is widely recognized as impactful.Footnote 1 However, conversations with assistant professors in economics and in several other disciplines over the years suggest that there is still great interest in how-to, concrete strategies for managing ambiguities along the tenure path.Footnote 2 Note that ambiguities remain even if tenure standards at one’s program/university are perfectly clear. This is because significant aspects of the tenure packet by definition do not materialize until quite late in the process. Two significant aspects are (1) the external review letters as measures of research effectiveness and (2) measures of teaching effectiveness. Managing these two ambiguities appears in this author’s experience and in the literature to be relatively overlooked by tenure-track faculty and their mentors until late in the process, resulting in tenure committees sometimes having difficulty securing external reviews and in evaluating measures of teaching effectiveness that may not be robust. To be sure, there are some terrific resources available to help tenure-track faculty manage these ambiguities. In economics, candidates are well-advised to review resources provided on the American Economic Association’s Committee on the Status of Women in the Economics Profession (CSWEP) website, https://www.aeaweb.org/about-aea/committees/cswep/programs/resources/tenure, including the Winter 2013 issue of the CSWEP Newsletter that focuses upon navigating the tenure process. The following eight concrete strategies—three for research, three for teaching, and two for synergistic research and teaching—are intended to complement the great suggestions featured there and in the greater literature in order maximize the likelihood that at the end of six years of a tenure-track (particularly at a teaching-intensive college or university), internal and external reviewers can more easily glean the strengths of the packet and vote enthusiastically. The suggestions may also be useful to non-tenure-track faculty; to faculty in other disciplines; and to associate professors planning for promotion to full professor.",1
47,4,Eastern Economic Journal,08 September 2021,https://link.springer.com/article/10.1057/s41302-021-00198-4,Human Suffering and Natural Experiments: How Empirical Economics can unmask the devastation of Covid-19,October 2021,Alexandre Olbrecht,,,Male,Unknown,Unknown,Male,,
47,4,Eastern Economic Journal,03 September 2021,https://link.springer.com/article/10.1057/s41302-021-00203-w,Do Disease Epidemics Stimulate or Repress Entrepreneurial Activity?,October 2021,Aziz N. Berdiev,James W. Saunoris,,Male,Male,Unknown,Male,"Researchers and policymakers alike have been interested in better understanding the economic effects of disease epidemics for quite some time (e.g., Becker 1990; Raddatz 2007; Bloom and Mahal 1997). These considerations have gained renewed interest with the advent of the current coronavirus pandemic (e.g., Jordá et al. 2020; Béland et al. 2020; Baldwin and Weder di Mauro 2020; Coibion et al. 2020; Fairlie 2020; Bartik et al. 2020). Adding to the related research on the novel coronavirus pandemic, we investigate the influence of disease epidemics on the prevalence of entrepreneurial activity. Understanding the interrelationship between entrepreneurship and biological disasters could shed light on entrepreneurial motivations especially in unprecedented circumstances such as the current coronavirus pandemic, thereby providing key implications for economic policy. The prevalence of disease epidemics has been a source of concern for many nations. Reasons for concern are that epidemics dampen output/production (e.g., Raddatz 2007), reduce labor force participation (e.g., Yu et al. 2020), hamper the provision of food assistance (e.g., Cardwell and Ghazalian 2020), and contribute to food insecurity (e.g., Mishra and Rampal 2020). For instance, the shutdown undertaken by governments, in both developed and developing countries alike, to contain/eliminate the severity of the current coronavirus pandemic has halted economic activity across many industries. Of course, while demand for certain goods and services persists, epidemics create additional demand pressures for alternate products (e.g., disinfectants, antibiotics, sanitizers, etc.). These demand pressures could prompt entrepreneurs to unleash their creative energies to satisfy this unique demand. In this paper, we analyze whether biological disasters create unique opportunities that encourage entrepreneurial activity. While previous research has looked at the relationship between natural disasters and entrepreneurship (Boudreaux et al. 2019), we uniquely examine the impact of biological disasters. Infectious disease epidemics are different from other natural disasters (e.g., geologic, climatic) due to the inherent externality generated by its contagiousness, which create unique uncertainties and challenges. To examine this novel relationship, we use a large sample of nations and consider various epidemics related to infectious diseases. Our focus on disease epidemics, considering both the incidence and severity of epidemics, and their influence on entrepreneurship is a unique aspect of the current paper. Diverse and decentralized knowledge in a market economy makes central planning less effective where entrepreneurs thrive because they can exploit the localized knowledge. Although entrepreneurs play a key role in an economy during times of natural disasters, there is an absence of research that analyzes the nexus between biological disasters and entrepreneurial activity. We thus attempt to answer the following question: does the prevalence of disease epidemics stimulate or repress entrepreneurial activity? Broadly speaking, entrepreneurs are important drivers of economic activity that promote economic growth and development and raise standards of living. Epidemics could impact various entrepreneurial opportunities, in particular, opportunities for recognition, discovery, and creation (Sarasvathy et al. 2003). For instance, incidence and diffusion of disease epidemics like the COVID-19 pandemic provide crucial opportunities for entrepreneurs to develop creative ways to thwart the spread of the disease through such things as a discovery of a vaccine, as well as to safely satisfy the demand for certain goods and services during a time of need. Of course, the media abounds with examples of the importance of entrepreneurs during times of epidemics and pandemics (https://www.cnbc.com/2020/04/15/hot-spots-of-innovation-as-a-result-of-coronavirus-pandemic.html). Yet, the crisis brought on by the virus could adversely affect the health of the population (Yu et al. 2020), thereby lowering the available supply of entrepreneurs. Besides analyzing the frequency of epidemics on entrepreneurship, we also uniquely capture the severity of epidemics related to transmission (the number of people affected by epidemics) and mortality (the number of deaths caused by epidemic). In doing so, the results of the analysis expand our knowledge on the nexus between epidemics and entrepreneurship. Using a large sample of countries, the results show that both the occurrence and severity of epidemics are associated with greater entrepreneurship, and these results are robust after accounting for alternate measures of entrepreneurship, additional covariates, simultaneity, outliers, and nonlinearities. Biological disasters classified as epidemics have direct relevance to the current COVID-19 pandemic, and the results of this analysis have important policy implications that help inform policy makers during the current coronavirus pandemic. Given that epidemics encourage entrepreneurship, nations would benefit from allowing the entrepreneurial function of the market economy to work to help prevent, mitigate, and contain the spread of epidemics. The layout of the remainder of this paper is organized as follows: “Theoretical Considerations and Hypotheses” section discusses the relationship between epidemics and entrepreneurship and outlines a set of testable hypotheses; “Data and Empirical Model” section describes the data and presents the empirical model; “Empirical Results” section discusses the baseline findings and offers a series of robustness checks; the final section summarizes our major findings.",2
47,4,Eastern Economic Journal,01 September 2021,https://link.springer.com/article/10.1057/s41302-021-00202-x,A Study of the Effectiveness of Governmental Strategies for Managing Mortality from COVID-19,October 2021,William Clyde,Andreas Kakolyris,Georgios Koimisis,Male,Male,Male,Male,"Since the COVID-19 outbreak began in early 2020, the governments of countries, states, and cities around the world have implemented various combinations and timings of strategies for balancing the health and safety of their citizens with the economic costs of those strategies and the disease itself. In doing so, these governments have effectively done hundreds of experiments, which are being studied to provide insights in determining the strategies that have been effective in reducing the number of COVID-19-related deaths and the strategies that have not. A vast number of such studies have been and are taking place, attempting to study either the effectiveness of individual policies or the overall aggregate effectiveness of the stringency of policies and not the portfolio of policies each considered individually. The goal of this study is to examine the effectiveness of a wide range of strategies employed by the OECD countries from mid-March through the end of October 2020 to determine which strategies are most effective in reducing COVID-19 related deaths so policymakers might make informed policy choices. Key elements in modeling epidemics and guiding policymakers include infection rates, mortality rates associated with infections, the ability and effectiveness of the policies, the medical system, and how society adapts to the changing dynamics of a pandemic, as well as other structural factors (Verity et al. 2020). When vaccines are absent—such as in the case of COVID-19 up to December 2020—governments try to limit social contact in order to flatten the curves of infection and mortality. This strategy has been adopted by most countries in the world. Additionally, institutional and demographic characteristics (average population age, percent of population with diabetes, gross domestic product per capita, etc.) can influence mortality dynamics both directly through the size of vulnerable populations, and indirectly through citizens’ perceptions and behavioral responses to stringent policies (Van Bavel et al. 2020). Age seems to consistently be the largest factor in mortality—people 65 and older account for most of COVID-19 related deaths in the US, though the Our World in Data (OWD-Oxford) site suggestsFootnote 1 that this might in part be due to the higher incidence of pre-existing health conditions that come with higher risk. The CDC has concluded that pre-existing conditions, such as heart disease, obesity, diabetes, kidney disease, and immune deficiency put people at higher risk of mortality and other factors, such as asthma, liver disease, hypertension, and pregnancy might put people at even greater risk; the OWD-Oxford site reports similar conclusions.1 The mortality rate for those with no pre-existing health conditions (0.9%)Footnote 2 is less than a tenth the rate for those with such conditions. Our paper focuses on the mortality rate instead of the case rate for two reasons. First, and most importantly, it is the mortality rate of COVID-19, as compared to other diseases, that distinguishes it and is the cause for concern. Although the number of US cases of influenza in recent yearsFootnote 3 is about the same as the number of confirmed US cases of COVID-19 over the past year,Footnote 4 the number of COVID-19 related deaths in the US over the past year is more than ten times the typical number of US deaths from influenza in recent years.Footnote 5 A second reason is that the data seems to be more reliable. While death is definitive and the cause of death more carefully determined, there seems to be a much broader range of practice in defining COVID-19 cases (Jinjarak et al. 2020). Governments have used a wide range of strategies executed with various timings to manage the number of COVID-19 cases and deaths. Government interventions may affect people’s mental health and can cause substantial economic and social costs. Therefore, it is crucial to investigate which government policies (or NPIs) are the most effective to combat COVID-19 or any other future respiratory epidemic. The challenging part in studying NPIs is that many countries rolled out several policies simultaneously; therefore, it is much more difficult to decouple the impact of each individual intervention. In terms of representative studies on NPIs, COVID-19 transmission and social mobility, Brauner et al. (2021), using data between January and May 2020 in 41 countries (34 European and 7 non-European), find that closing schools was highly effective, banning gatherings and high-risk businesses was effective but closing most other businesses had limited further benefit. Flaxman et al. (2020) study the effect of government policies in 11 European countries using data from February to early May 2020 and find that these interventions have been effective in reducing the transmission rate of COVID-19. Haug et al. (2020) rank government interventions for 226 countries worldwide, for the purpose of mitigating the spread of COVID-19. Aravindakshan et al. (2020) measure the impact of NPIs on social mobility and the resulting disease mitigation by exploiting the spatio-temporal variations in policy measures across Germany’s states. Hsiang et al. (2020), with data on local, regional, and national NPIs in six countries, find that anti-contagion policies are effective in reducing the growth of COVID-19 infections. Liu et al. (2021) find mixed evidence for the association between NPIs and reduced infection rates, with only school closure and internal movement restrictions showing unequivocal evidence of being associated with a decrease in the effective reproduction number of COVID-19. On the other hand, Courtemanche et al. (2020) find that shelter-in-place orders and closures of restaurant and entertainment-related businesses were effective in reducing the growth of COVID-19 cases, while bans on large social gatherings and school closures had no effect. Berry et al. (2021) explore the effects of shelter-in-place policies during an early period of the COVID-19 pandemic. Their study focuses on how these policies affect not only new cases and deaths but also mobility behavior and unemployment rates. According to their findings, the effects on infections, deaths, and unemployment appear moderate and insignificant. Amuedo-Dorantes et al. (2021) examine how the speed of adoption of NPIs affects mortality in the US and conclude that advancing the date of NPI adoption by one day lowers the COVID-19 mortality rate by 2.4%. Regarding the effectiveness of mask mandates, Bundgaard et al. (2021) discover through a randomized controlled trial in Denmark, that the mask mandate as a supplement to other public health measures does not reduce the COVID-19 infection rate by more than 50% in a community with relatively moderate social distancing. On the other hand, Karaivanov et al. (2021) investigate the impact of mask mandates and other NPIs on COVID-19 case growth in Canada and find that in the first few weeks after implementation, mask mandates were associated with a 25% decrease in the weekly number of COVID-19 cases. We contribute to the continuously growing literature on COVID-19, that deals with non-pharmaceutical interventions, by investigating the impact of seven government policies on new mortality growth rates across the 37 OECD countries. The application of policies and reporting of data varies widely across countries. We focus on policies implemented between March 15th and October 31st, studying the impact of those policies up to 70 days after policy implementation,Footnote 6 controlling for policy and structural factors subject to data availability and quality. For our analysis, we examine how the growth in the new deaths attributed to COVID-19 (7-day smoothed) per million for OECD member countries, reported by OWD-Oxford, is influenced by changes in seven policy variables (ranging from school and business closures to restricting internal travel), reported by CGRT-Oxford, controlling for population, economic, and health data (such as age, GDP per capita, and diabetes prevalence in the population). The lag time between a change in policy to possible change in the rate of deaths is accounted for by the policy variables 7 to 70 days. This range of lags was decided based on the CDC’s estimates of the time from exposure to symptom onset to death. Related research (Jinjarak et al. 2020) finds also effects from lags of 2 to 4 weeks. In addition to the ordinal values associated with each of the policies to indicate stringency of implementation (discussed in more detail in the “Data” section), each also has an associated binary “flag” variable to indicate whether the policy implementation was geographically targeted or general. We use the different levels of stringency of policy implementation in our study but do not use the flag variable data. A reasonable argument is that selective geographic implementation implied implementation only where it was needed, and therefore it was tantamount to general application in effect. Given the restriction of the dataset, it would not be possible to measure mortality rates by region, and therefore, we do not include the flag variables in our model. Furthermore, compliance, which may vary between and even within countries over space and time, may be a critical factor in determining the efficacy of many of these policies. A paper by Singh et al. (2021) analyzes the effect of introducing and lifting NPIs in the US on COVID-19 cases and shows that part of the variation in health outcomes is driven by variation in compliance (that is, reduction in mobility due to NPIs). In their paper, compliance is measured as mobility, whereas mobility is proxied as the percentage of mobile devices staying home out of total mobile devices on the county level. In addition to any direct effects the implementation of NPIs might have on COVID-19 cases and deaths, there is evidence that their implementation has indirect effects, notably through information channels that may alter perceived risk of contracting the disease. For instance, Cronin and Evans (2020) use structural break and difference in difference models and show that mobility, foot traffic, and social distancing were all reduced after the declaration of a state of emergency and before restrictions were in place. This implies that the decline can be explained by the private decisions of people to reduce the risk of contraction or transmission of the virus, because of the information received at the national level. Another study by Dave et al. (2020) reports that termination of Wisconsin’s social distancing order by the Wisconsin Supreme Court had little impact on social distancing practices or COVID-19 case growth, reinforcing the role of information channel in motivating people’s behavior. We focus on the OECD countries because there seems to be more consistency in application and compliance within and between those countries. Further study of the issue of compliance, as well as the impact of the information value of policy implementation in changing the risk perceptions, and therefore behavior of individuals, is beyond the scope of this paper. Our paper is structured as follows: “Data” section presents the data utilized; “Model Specification” section describes the model and methodology; “EstimationandResults” section provides discussion of results and analysis, and “Conclusion” section concludes.",1
47,4,Eastern Economic Journal,21 September 2021,https://link.springer.com/article/10.1057/s41302-021-00204-9,Do Governors Lead or Follow? Timing of Stay-at-Home Orders,October 2021,Bryan C. McCannon,,,Male,Unknown,Unknown,Male,"On December 31, 2019 China reported a cluster of pneumonia cases related to the Wuhan Wholesale Market and confirmed that these cases were due to the novel Coronavirus on January 7, 2020. Since then, the virus has spread around the world. The first case in the USA was on January 19, 2020 (Holshue et al. 2020). It has spread rapidly and led to unprecedented disruptions to daily life and the economy, not to mention the catastrophic loss of life. In the USA, the virus led to unprecedented policies enacted by state governors. Almost all states implemented Stay-at-Home orders requiring non-essential businesses to close and asking residents to shelter in place and social distance. The economic destruction of the Stay-at-Home orders cannot yet be fully measured, but can only be balanced by the number of lives saved by slowing the virus’s spread and keeping health care facilities from being over-run. Unfortunately, this novel pandemic provides the opportunity to assess gubernatorial decision making. While state governors received information and suggestions from national and international sources, they were free to act independently. The orders issued did not receive legislative approval or were subject to Federal mandates and restrictions. Some governors acted (relatively) quickly, while other governors delayed, and some never implemented these lockdowns. The goal of this study is to understand this variation in decision making. It is unclear whether the benefit to these orders outweighed the costs, and we may never know. The variation in decision making, though, provides the opportunity to ask whether governors in the USA are leaders or followers. That is, do they respond to citizen demands, preferences, and opinions in a responsive manner (i.e., following their citizens)? Alternatively, leaders can be expected to respond to a new situation by doing what is right, given the information they have. A leading governor’s choices, then, would not necessarily be conditioned on voter’s assessments. Followers are more likely to respond to opinion polls and popular opinion, leaders act regardless. The novel Coronavirus provides a novel opportunity to evaluate gubernatorial decision making. It is a never-seen-before event. Each governor is confronted with the same information (but potentially varying magnitudes of benefits and harms to their decision) and is free from Federal government intervention. The dilemma is how to gauge citizen concern. Public opinion polling is sparse as there was no comprehensive daily/weekly data collection spanning all states. Identifying sentiment by protests or other collective actions are not possible given the shelter in place requirements. The approach I follow here is to track Google searches for “Coronavirus”. Google provides daily search behavior separate for each state. The shutdown of businesses, churches, and schools left many at home leaving them the internet (and television) as their only access to the outside world. Google searching is a good proxy measurement for residents’ concern over the virus. I ask whether heightened interest in the virus early in its spread can explain which governors implemented the Stay-at-Home orders first. If states where searches are relatively higher earlier in the pandemic are those states that implement the lockdowns earlier, while late adopting states are those with later responsiveness by the citizens, then I argue this is evidence of governors as followers. If the implementation of the Stay-at-Home orders are unrelated to Google searching, then governors are more likely to be leaders. The question connects to the fundamental divergence of political competition as being either one of Downs (1957) or Citizen Candidates (Osborne and Slivinski 1996; Besley and Coate 1997). In the former, politicians do not express personal preferences but simply announce and take on the policies preferred by voters. Political competition can be expected to lead to policy representing the median voter (Black 1948) and, therefore, adjusts as the median voter’s preferences change. Politicians as citizen-candidates, on the other hand, enact policy based on their own preferences and knowledge. The central question there, then, is to what degree does policy diverge from what is preferred by the voters.Footnote 1 I find strong evidence that states with relatively more Google searches for “Coronavirus” in February and early March of 2020 are those states that implemented the Stay-at-Home orders earlier. This effect is not explained only by searches in week prior to the order, but rather are driven by how much concern residents of the state had prior to the dramatic events and closures of mid- and late-March. Further, how quickly a state issued the order is not related to cumulative death toll. Governors are followers. Exploring heterogeneous effects, I am unable to document differentiable impacts by partisan affiliation. Unlike reporting of distinctions between blue and red states, the relationship between the relative amount of internet searching and gubernatorial policymaking is basically identical for Republican and Democrat governors. In addition, I do not find a difference between those governors who are up for re-election in 2020 and those who are not. I do find, however, that governors of more economically free states are less likely to be responsive to resident concerns.Footnote 2 Finally, I find that the effect is smaller for governors who have higher approval ratings just before the pandemic hit. This suggests that popular governors and those in freer states are more likely to be leaders. This note contributes to the rapidly growing and presumably large future literature on the economic, legal, psychological, health, and political ramifications of the novel Coronavirus.Footnote 3 Importantly, it informs the literature on democratic accountability and specifically the literature on state governors’ decision making (Simon 1989; Lowry et al. 1998).",5
47,4,Eastern Economic Journal,13 September 2021,https://link.springer.com/article/10.1057/s41302-021-00201-y,Assessing the Impact of Covid-19 Pandemic on Emerging Market Economies’ (EMEs) Sovereign Bond Risk Premium and Fiscal Solvency,October 2021,Menna Bizuneh,Menelik Geremew,,Female,Unknown,Unknown,Female,"The Covid-19 pandemic has had a global economic impact. The reduction in economic growth for many countries has come as a result of reduced labor supply, higher production cost, higher temporary inflation, and reduced social consumption (Wren-Lewis, 2020). The subsequent stringent lockdown measures implemented by many governments to protect susceptible populations has further amplified the contraction in economic activity. The reduction in economic activity has severely damaged government budgets throughout the world causing an increase in debt because of swelling budget deficit resulting from plummeting tax revenues and increasing expenditures. Emerging Market Economies (EMEs) have not been immune to the increase in debt induced by the Covid-19 pandemic. However, pre-Covid-19 EMEs were already in debt distress and facing fragile access to international markets providing governments with limited fiscal space and capacity to react to external shocks. In the past few decades EMEs have increasingly turned to capital markets, both domestically and internationally, to meet their financing needs. Consequently, EME’s increased borrowing has resulted in a significant increase in sovereign debt. In particular, sovereign debt in emerging markets has grown remarkably since the global financial crisis where there was a large cumulative inflow of foreign capital into the debt marketsFootnote 1. Against this background and heightened uncertainty about the future vis-à-vis the end of the Covid-19 pandemic and its economic consequences, EMEs are facing economic challenges both in the short-and long-term. Many EMEs face both demand and supply side shocks resulting in depreciated currency against major currencies, reversal in capital flows and potential impacts to their current accounts. As such sovereign debt securities become a key method of funding. However, the economic fallout of the pandemic has made EMEs experience a ‘sudden stop’ of private capital flows at the onset of the pandemic in March 2020 (Buchheit and Gulati, 2021) and EME governments face higher bonds risk premiums associated with higher possibility of default due to more volatile capital flows (Tran, 2018). Although several studies have analyzed the impact of the Covid-19 pandemic on sovereign debt risk premium, none of the prior literature to our knowledge discusses the impact of the pandemic on sovereign bond risk for EMEs. Our study attempts to fill this gap in the literature by shedding light on the channels and transmission mechanism through which the Covid-19 pandemic prompted negative economic shocks on EMEs through raising their sovereign bond risk premium and increasing the probability of fiscal insolvency. The pandemic has affected all economies including advanced economies and developing economies in addition to EMEs. With regard to the sovereign bond risk premium of advanced economies, we do not see significant change in their sovereign risk premiumFootnote 2 as can be seen in Figure 3bFootnote 3. We found that the Year-on-Year percent (YoY%) change in the measurement of sovereign bond risk premium for advanced economies, represented by the G7 countries had a practically flat line in the first three quarters of 2020. The lack of impact of Covid-19 on sovereign bond risk premiums is even more apparent when compared to the risk premium YoY% change of EMEs in Figure 3a. Almost all EMEs in our sample experienced a spike in their risk premium in the first quarter of 2020. On the other hand, developing countries could face similar fiscal challenges as EMEs to mitigate the impact of Covid-19 on their economies. However, the lack of more frequent data and unavailability of macroeconomic data makes analyzing the impact of Covid-19 on sovereign bond premium for developing countries in a meaningful way difficult. The aforementioned details provide us with a strong motivation to concentrate our analysis on the effects of Covid-19 on EMEs’ sovereign risk premium. In addition, we focus this study on EMEs for three reasons. First, different from other groups, EMEs have less access to domestic currency denominated borrowing making them more vulnerable to sudden stops in credit. Second EMEs are at a younger stage of financial development which could result in a different mechanism through which sovereign debt impacts the real economy (Bernardini and Forni 2017). Finally, EMEs have limited capacity to effectively deploy monetary and fiscal policies in times of crisis. In EMEs both fiscal and monetary policies tend to be procyclical or acyclical. This feature of these policies denies EMEs of important macroeconomic stabilization tools (Coulibaly, 2012). This paper adds to two branches of the literature. First, this study contributes to the vast literature that exists on the factors that influence sovereign bond risk premium in the context of EMEsFootnote 4. Some analysis focus on country-specific fundamental as indicators of external financing (Hartelius, Kashiwase, and Kodres, 2008; Luengnaruemitchai and Schindler, 2007). Other studies examine the role country-specific fundamentals such as debt and fiscal variables, reserves, GDP growth, and interest rates of various maturities play in explaining sovereign bond spreads (Baldacci, Gupta, and Mati 2009; Comelli, 2012; Eichengreen and Mody 1998; Kamin and Von Kleist 1999]. In addition, some studies highlight the importance of external global factors in accounting for spread dynamics (Kamin and Von Kleist, 1999; McGuire and Schrijvers, 2003; Uribe and Yue, 2006). There are also studies that focus on sovereign bond risk in general (Andries et al., 2020; Ferrucci, 2003; McGuire and Schrijvers, 2003). We contribute to this literature through the analysis of the impact of a significant economic shock associated with a pandemic on sovereign bond risk premium in EMEs. Second, our work broadly relates to the fast-growing research that studies the economic implication of the Covid-19 outbreak and the subsequent economic shutdown policies and ‘de-globalization’ process, broadly termed as “Coronanomics” (Eichengreen, 2020)Footnote 5. We add the impact of the pandemic on sovereign bond risk premiums to the current literature which examines the impact of Covid-19 on economic growth (Gormsen and Koijen, 2020), production (Biron and Zhu, 2020; Leijen, 2020), the supply-demand doom loop (Fornaro and Wolf, 2020), supply chain (Baldwin and di Maurao, 2020; Wong, Lin and Jackson, 2020; Luo and Tsang, 2020; Goel et al., 2021), financial markets (Haddad, Moreira, and Muir, 2020), capital flows (Ribakova, Hilgenstock, and Fortun, 2020), exchange rates and financial stability/risk (Beck, 2020; Cecchetti and Schoenholtz, 2020; Cochrane, 2020), as well as debt crisis. There are a few papers that share our focus on the impact of Covid-19 specifically on EMEs (see Arellano, Bai, and Mihalache, 2020; Hevia and Neumeyer, 2020). However, to the best of our knowledge this paper is the first to focus on the impact of Covid-19 on emerging market economies’ sovereign bond spread. Focusing on 12 countries in four regions, our results show that the impact of the Covid-19 pandemic on sovereign bond risk premium comes through primarily macroeconomic channels such as GDP growth and external account indicators. In addition, EMEs with higher fiscal deficit to GDP ratio also experience higher sovereign risk premiums during Covid-19. Moreover, we find that political stability indicators and the great financial crisis of 2008-2009 have a statistically significant impact on sovereign bond risk premium. The rest of the paper is organized as follows. The next section presents a brief description of economic impact of Covid-19 on EMEs. See section ‘Data and Methodology’ describes the data and the methodology utilized and see section ‘Results’ provides empirical analysis. We conclude with final remarks in see section ‘Conclusion’",6
47,4,Eastern Economic Journal,30 August 2021,https://link.springer.com/article/10.1057/s41302-021-00199-3,"Household Income, Pandemic-Related Income Loss, and the Probability of Anxiety and Depression",October 2021,Julio Huato,Aida Chavez,,Male,Female,Unknown,Mix,,
47,4,Eastern Economic Journal,03 September 2021,https://link.springer.com/article/10.1057/s41302-021-00200-z,Education in Mathematics and the Spread of COVID-19,October 2021,Joshua Ping Ang,Tim Murray,,Male,Male,Unknown,Male,"In the USA, the policy response to COVID-19 was primarily left to state governments. In the absence of a vaccine or treatment for the virus, states instituted a series of non-pharmaceutical interventions (NPIs) to slow the spread of the virus, such as mask mandates, stay-at-home orders, and social distancing guidelines. The success of the NPIs that states implemented was largely dependent on compliance of individuals and businesses. Nagler et al. (2020) show that nearly 75% of adults were exposed to conflicting information about the severity of COVID-19 and the effectiveness of NPIs from media outlets and politicians (see Barker 2020, Collins 2020, and Schumaker 2020 as examples). For those exposed to conflicting information, accurately deciphering data and statistics published by the Centers for Disease Control (CDC), World Health Organization (WHO), and local public health agencies may have been difficult. A higher level of education is associated with better health (Conti et al. 2010; Cutler and Lleras-Muney 2010; Lleras-Muney 2005) and is an important factor in the adoption of preventative measures against infectious disease (Bawazir et al. 2018; Leung et al. 2003). However, education in mathematics and science would be necessary when evaluating the data and statistics prepared by public health agencies. Davis and Simmt (2003) showed that education in mathematics is crucial to understanding the complexity of science and epidemiology. While Ang et al. (2020) found that education in mathematics and science impacts the number of COVID-19 cases, they did not assess the relative role of mathematics and science on one another. Currently, data on science scores at the county level are not available, so in this analysis, we investigate how education in mathematics in young people (ages 18–25) contributed to the spread of COVID-19 in the USA using a county-level cross-sectional analysis. The Stanford Education Data Archive provides normalized standardized mathematics test scores for eighth grade showing the average level of mathematics comprehension at the county level. We find that a one-grade-level increase in mathematics comprehension (i.e., if a county’s eighth graders had an average level of comprehension of 9th graders) resulted in a 7–15% reduction in the number of COVID-19 cases. This suggests that education in mathematics for young adults was an important factor in reducing the spread of COVID-19 in the USA. Secondly, we find that the higher the vote share (e.g., the percent of the vote) for Joe Biden, a one-grade-level increase in math comprehension led to a greater reduction in the number of COVID-19 cases. While not a causal result, one interpretation of this result would suggest that while education in mathematics is important, political affiliation and the ability to synthesize information that came from political leaders were also factors in slowing the spread of COVID-19. These findings suggest that if states and localities place a greater emphasis on education in mathematics, it may help in preventing the spread of disease in a future public health crisis.",4
48,1,Eastern Economic Journal,19 November 2021,https://link.springer.com/article/10.1057/s41302-021-00208-5,Which Immigrants Promote Trade with Third Party Countries? On the Role of Geographic and Linguistic Proximity,January 2022,Oleg Firsin,,,Male,Unknown,Unknown,Male,"Since the seminal work of Gould (1994), there has been an explosion of interest in studying the connection between immigration and trade. Previously, it had been analyzed through the Heckscher–Ohlin model, which treated production factor trade and commodity trade as substitute processes (Mundell 1957), implying movement in the opposite directions for migration and trade. This meant that easier immigration would likely reduce trade. Gould’s work, which found that immigration increased trade to immigrant countries of origin, helped shift the analysis framework to that of immigrants as trade facilitators. Despite a large amount of additional evidence supporting the finding that immigration promotes trade, with an economically significant average elasticity of about 0.16 (Genc et al. 2011), the trade promotion role of immigrants has not yet become a significant part of immigration policy or trade policy discussions. Yet proper accounting for immigrants’ trade promotion effect is potentially important for policy formulation, since it is needed for more accurate understanding of the benefits and costs of immigration, of ways to reduce trade barriers, and, in case of the US, due to uneven public support for immigration and trade. Over 70% of Americans think of trade positively—as an opportunity for growth—while only 25% think of it as a threat (Jones 2018). In contrast, only 45% of Americans think of immigrants as good for the economy, whereas 52% think they harm the economy or make no difference (McCarthy 2017). A better understanding among the public of how immigration affects a process that is widely seen as economically beneficial could potentially elicit more support for local and national policies that reflect the trade promotion effect of immigration. This paper accentuates the importance of immigration in trade promotion discussions and the importance of trade-related effects in immigration policy considerations by highlighting and thoroughly investigating a scarcely explored direction of the immigration-trade link—the spillover effect on trade with countries geographically and linguistically proximate to immigrants’ countries of origin. The reason for the link can be understood by considering the two main channels that are generally used to explain the trade facilitation effect of immigrants—networks and knowledge/information. The former channel refers to reducing costs of searching for destination country business partners, negotiating and enforcing contracts by drawing on business and familial relationships as well as other contacts that immigrants may have in home countries; these relationships may be particularly important in countries with weak rule of law and institutions. The latter channel refers to information about legal, institutional and cultural aspects of the export markets as well as language ability, as immigrants can improve communication and logistics and reduce search costs for foreign market information; this channel is especially important for trade among countries with different predominant languages and those with most dissimilar legal, institutional and cultural environments. While the trade facilitation effect applies to both exports and imports, there is a lesser third channel—the home product preference effect—that applies only to imports. Although in nearly all empirical work to date the impact of immigrants has been assessed with regards to exports to or imports from their countries of origin, the above mechanisms suggest that the effect can also apply to trade with third party countries. Anyone with a network in or with relevant legal, institutional, cultural or other knowledge of a given country, or in possession of language skills that facilitate communication, can have a pro-trade effect with respect to that country. The network- and information-based connection of immigrants to third party countries is likely not uniform, however. To explore the likely directions of trade promotion, we focus on two aspects of how immigrants are related to the trading partner country—geographic and linguistic proximity of immigrant origin and export destination/import origin countries. We term immigrants from country j in host country h geographically proximate to trading partner c if countries j and c share a common border. This categorization is motivated by the fact that trade tends to be greater between countries that are closer (distance being the main barrier in the gravity equation) and share a common border, which also means more business and other connections between people in geographically close countries and those sharing a common border. Consequently, some of the immigrants from country j in host country h may bring with them business/social connections to residents of country c that borders country j, which may be drawn upon to facilitate trade with country c. Additionally, countries that are closer and share a common border may be more likely to share similar cultures, legal systems, institutional and market peculiarities (having often been part of the same country or governed by the same colonial power). Hence, the information channel may also be at work when it comes to promotion of trade with countries geographically proximate to immigrants’ countries of origin. We term immigrants from country j linguistically proximate to trading partner c if they come from a country with the same official language or speak the same language (whether native or non-native) as residents of country c. The motivation for considering language as one dimension of immigrant proximity and for including its three roles—official, native, and spoken non-native—is strong. Same official language has been found to increase trade between countries (Rauch and Trindade 2002; Aleksynska and Peri 2014; Egger et al. 2012; Blanes-Cristóbal 2008). Additionally, same official language between a country pair reduces the trade promotion effect of immigrants, as immigrant language skills become less relevant if the host country residents speak the same language. This is consistent with the empirical findings from Aleksynska and Peri (2014) and Kandogan (2005) that the immigration-trade elasticity is higher for linguistically more dissimilar countries. This highlights the role of immigrant linguistic/cultural capital in trade promotion, which may carry over to trade with countries other than the immigrant country of origin. For example, if Ecuadorians can promote exports to Ecuador through facilitation in negotiations and logistics/communication due to speaking Spanish, potentially so can Colombians, Spaniards, and others who speak Spanish. It is also possible that native speakers of the same language from different countries develop relationships that are stronger, in which case the effect of the same native language would be higher than non-native; this is consistent with the Melitz and Toubal (2014) finding that a higher probability of speaking the same native language increases bilateral trade between two countries even after controlling for the probability of speaking the same (native or non-native) language. The special role of native language brings forth the role of ethnic versus inter-ethnic spillovers. Leaving a more detailed discussion for later sections and the “Appendix,” we define ethnically close immigrants as those who speak the same native language as residents of the trading partner country, which carries with it a higher likelihood of cultural similarity and social networks as well as ease of communication. Consequently, we term the trade-promotion effect of immigrants with countries (other than their origin countries) hosting more speakers of the same native language the ethnic spillover effect. Accordingly, we define the inter-ethnic spillover effect as the promotion of trade by immigrants who do not share a native language with those in the trading partner country. We test whether the latter exists for those who share the same non-native spoken or official language with the trading partner country or are from geographically proximate areas. Since in a given host country there are immigrants who are neither linguistically nor geographically proximate to trading partner country c, we term them “distant.” Since they may promote trade with some countries, at least their own countries of origin, it is natural to ask whether this results in trade diversion from country c, which we also seek to answer in this study. Our empirical analysis is based on an extended gravity equation using US state exports and imports, by industry, to and from a large number of foreign countries, annually over a number of years (2002–2016 for exports and 2008–2016 for imports), which enables us to control for state–country trading pair fixed effects. Furthermore, we use enclaves-based shift-share instruments to address other threats to identification, such as those related to reverse causality or omitted variables related to both immigration and trade. We check the robustness of the results by omitting potentially influential countries and by using a Pseudo-Poisson Maximum Likelihood (PPML) estimator. The relative methodological rigor and extensiveness in terms of the trading partner countries and time period compares favorably to other studies and are observed in Table 1. The results of the empirical analysis provide evidence for the inter-ethnic spillover effect due to immigrant language skills by showing that immigrants who speak the same non-native language as residents of the trading partner country increase both exports to and imports from it, even if they hail from geographically distant countries with a different official language. This finding extends the literature by showing that third party (spillover) trade promotion effect of immigrants is not limited to ethnic diasporas, but also arises from immigrants across different countries speaking the same language, the most likely mechanism for which is communication/logistics facilitation and easier information acquisition. In contrast, immigrants from countries with the same official language but who do not speak the same language do not increase (conversely, decrease) trade. The implication of this finding is that the positive effect of a common official language on bilateral trade may be largely due to the correlation with shared spoken and/or native language. When it comes to the role of native language over and above spoken, it tends to have an additional export- but not import-promotion effect. This finding provides further evidence for an ethnic network effect found in Rauch and Trindade (2002), Giovannetti and Lanati (2015), and Felbermayr et al. (2010); at the same time, it hints that the trade promotion effect found in these three papers, may be due to the export part of trade, as they rely on bilateral trade volume and cannot distinguish between export- and import-promotion effects. In terms of the role of geographic proximity, immigrants who come from countries that border trading partner country increase exports, but only if they are also linguistically proximate, and they do not increase imports. Alternatively interpreted, geographic proximity increases the magnitude of the export promotion effect of the linguistically proximate immigrants. Lastly, we find that distant immigrants, those neither geographically nor linguistically proximate, tend to decrease trade, which raises the issue of trade diversion, which has been, for the most part, neglected by the immigration-trade link literature. However, the magnitude of the trade diversion effect tends to be much smaller than that of trade promotion. In the following sections, we discuss related literature, present descriptive statistics, outline the empirical methodology, discuss the results, and conclude.",
48,1,Eastern Economic Journal,22 July 2021,https://link.springer.com/article/10.1057/s41302-021-00195-7,Domestic Arrears and Financial Stability: The Role of Institutional Factors,January 2022,William Godfred Cantah,William Gabriel Brafu-Insaidoo,Eric Amoo Bondzie,Male,Male,Male,Male,"The growth and development of every economy in the twenty-first century largely depends on a stable and strong financial sector. The financial sector facilitates the mobilization of needed resources for production and also promotes trade and consumption in the country. Financial stability refers to the ability of an economy’s financial system to enhance and facilitate economic processes, manage risk and absorb financial and economic shocks (Schinasi 2004). Thus, an understanding of the causes of financial stability is key to enhancing sustained growth and development of the economy. The determinants of financial stability have been a concern to several researchers (Alshubiri 2017; Diaconu and Oanea 2014; Madi 2016; Ozili 2018; Vo et al. 2019) since the 2008 financial crisis which affected many developed and emerging economies. Interestingly, very few of these studies (e.g., Ozili, 2018) have considered Sub-Saharan Africa (SSA). However, several African economies in the last few years have experienced some form of banking crises which seem to have exposed the fragile nature of Africa’s financial system and the ability of the financial system to propel economic growth in the continent (Ozili 2018). The recent form of these frequent crises in the financial sector was the one experienced by Ghana between 2017 and 2019. During this period, the Bank of Ghana revoked the licenses of 7 commercial banks, 23 savings and loans companies and about 347 microfinance companies. In a related development, the securities and exchange commission of Ghana also revoked the operating license of 53 fund managers in the country. Theoretically, banks in developed countries largely focus on capital adequacy for financial stability. However, Brunnermeier et al. (2009) argued capital resources may not be enough to achieve banking stability in developing countries. Their argument is a result of several structural weakness in developing economies which makes the capital adequacy insufficient for stability. This viewpoint has been corroborated by Beck and Cull (2013), who also argued that structural and institutional failures seem to have weakened the effectiveness of the bank’s risk management techniques. Several African countries are known to be associated with a weak institutional environment (Ozili 2018) and are likely to be prone to a number of these crises (Table 1). In addition to the identified challenges in the literature, there is a growing phenomenon of accumulation of domestic arrears in SSA (International Monetary Fund—Africa Office 2019). The average stock domestic arrears accumulation in SSA countries averaged about 3 percent of GDP. Though the figure may seem small, most of these arrears are largely debt to domestic suppliers and creditors who depend on the domestic banking industry to pre-finance government projects (Diamond and Schiller 1993). Such delays in payment could increase the level of non-performing loans left on the books of the domestic banking industry which could affect the level of stability in the industry. It is important to note that the accumulation of domestic arrears is mostly associated with debt accumulation which could further increase the vulnerability of the domestic banking industry to shocks. Radev et al. (2009) and Pattanayak (2016) have attributed the accumulation of domestic arrears to weak public financial management and the lack of political commitment to agreed financial policies. This implies that the level of arrears accumulation largely depends on the institutional quality of the country. Economies with relatively high institutional quality are likely to adhere to strict regulations on financial policies and management practices. More so, high institutional quality implies a low level of corruption and a high level of government effectiveness is likely to reduce the level of domestic arrears accumulation. Since activities in the banking industry in SSA are highly linked to activities in the public sector, an analysis of the determinants of banking sector stability in SSA requires an understanding of the role of domestic arrears accumulation and institutional quality. The only known study on SSA (Ozili 2018) focused on the role of institutional quality in achieving banking stability. Other existing studies (such as Alshubiri, 2017; Diaconu & Oanea, 2014; Vo et al., 2019) have not examined the role of arrears accumulation in banking stability. Domestic arrears accumulation by the government could have a detrimental effect on private sector business (since they mostly borrow money from banking institutions to pre-finance government contracts) which could lead to a distressed banking industry and the overall growth of the economy. Thus, it is imperative to analyze the extent to which the accumulation of arrears by various governments across the subregion affects the stability of the banking sector. This paper, therefore, seeks to analyze the effect of domestic arrears accumulation on the stability of the banking sector in SSA countries. In this study, we hypothesize that the effect of arrears accumulation on the stability of the banking industry largely depends on the level of institutional quality. This is because economies with relatively effective institutions are less likely to accumulate arrears and even if they do, they are more likely to pay it early. This paper, therefore, employs a panel threshold regression to analyze the extent to which government effectiveness and regulatory quality moderates the effect of domestic arrears accumulation on the stability of the banking industry in Sub-Saharan Africa. The rest of the work is organized as follows: The second section reviews the related literature on banking stability followed by data and methodology for the study. The fourth section discusses the results obtained from the study and ends with conclusions and implications of the study.",2
48,1,Eastern Economic Journal,26 October 2021,https://link.springer.com/article/10.1057/s41302-021-00206-7,Are We Floating Yet? Duration of Fixed Exchange Rate Regimes,January 2022,Menna Bizuneh,,,Female,Unknown,Unknown,Female,"The choice of exchange rate regime is a central topic in international finance and one that has been debated extensively. The salience of exchange rate regime in the open economy context is driven by the “trilemma” which postulates that countries can have only two out of three policy options among free capital mobility, monetary policy autonomy, and exchange rate stability (Mundell 1961). The increased cross-border flow of capital coupled with recent global crises has garnered a renewed interest on how countries navigate this trilemma by choosing different policy combinations (Aizenman and Hutchison 2011). Exchange rate regime choice has long-lasting effects on macroeconomic outcomes including but not limited to economic growth, inflation, sustainable trade balance, and international capital flow (e.g. Cheng et al. 2013; Giannellis and Koukouritakis 2013; Kim and Hammoudeh 2013; Kodongo and Ojah 2012; Verheyen 2012). Therefore, a country’s choice of exchange rate regime as well as its decision to switch its exchange rate regime are important policy issues in open-economy macroeconomics. Since the collapse of Bretton-Woods in the 1970s countries have had various exchange rate regimes where many have exited fixed (pegged) exchange rate regimes in favor of flexible (floating) exchange rate regimes. The determinants of which types of exchange rate regimes should be implemented depending on a country's characteristics have been theoretically predicted (see Mundell 1961; McKinnon 1963; Rizzo 1998; Frankel 1999; Fischer 2001; Poirson 2001; Juhn and Mauro 2002; Von Hagen and Zhou 2007; Carmignani et al. 2008). The determinants of the choice of currency policy are argued from several perspectives including optimal currency areas, currency crises, and policy credibility. However, the existing literature could not identify a single generalizable variable as an unquestionable determinant of exchange rate regime choice. Most of the empirical studies undertaken in the estimation of the determinants of exchange rate regime choice so far have been of the probit and logit nature (see Kumar et al. 1998; Eichengreen et al. 1995; Klein and Marion 1994; Klein and Marion 1997; Masson and Ruge-Murcia 2005). These binary dependent variable models are unable to account for the time dependence that may be present in the decision to abandon a fixed exchange rate regime in favor of a flexible exchange rate regime. Ideally, the empirical model to be used in the estimation of the determinants of exchange rate regime choice should take into consideration two conditions: the possibility of time-dependence and the effect of intra-subject correlation. In this paper we argue that time is an important concept for the analysis of transition between exchange rate regimes. In particular, we argue that the probability of an exit from a particular exchange rate regime is likely to be determined by the time spent within a given regime. To this effect, we study the conditional probability of a particular exchange rate regime ending by adopting survival analysis for 178 countries. The duration of a given exchange rate regime is important in assessing currency stability. Exchange rate credibility depends not only on economic and institutional factors, but also on the time already spent in a regime for which a particular currency does not suffer from a speculative attack. Moreover, the time-dependence may be non-monotonic resulting in a probability of exit from an exchange rate regime, which increases during short duration but decreases for longer duration. We contribute to the literature in two ways. First, empirical studies that tried to address the duration dependence effects on the choice of exchange rate regimes are limited (e.g. Setzer 2004; Tudela 2004; Wälti 2005; Tamgac 2013). None of the empirical studies provide a comprehensive analysis for different types of economies (i.e. advanced, emerging, developing) using de facto exchange rate regimes. Previous studies focus on either a subset of countries or utilize a de jure exchange rate regime. We provide a comprehensive list of countries in our analysis and include an extended time period which considers two of the most recent major economic shocks: the 2008–2009 global financial crisis and the 2011 Eurozone sovereign debt crisis. Having a larger sample of countries provides broad evidence base about how countries choose their exchange rate systems. Second, analyzing multiple cycle data is important to address occurrence dependence in addition to duration dependence. However, when analyzing multiple exit (failure) data there is a potential for a lack of independence of the failure times.Footnote 1 In these types of studies, exit times are correlated within a cluster (subject or group), violating the independence of failure times assumptions required in traditional “time-to-first event” survival analysis. Yet, the approach that has been applied by previous studies is the Cox (1972) proportional hazard estimate which is the standard “time-to-first event” survival analysis (e.g. Wälti 2005; Tamgac 2013) resulting in possible information about lack of independence of failure times begin lost.Footnote 2 Correlated exit times from a particular exchange rate regime to another are important for they set up the platform for establishing the probabilities to possible realignment of exchange rate policy. Therneau and Grambsch (2000) suggest that for analyzing multiple cycle data, failure events should be classified according to two criteria: (1) whether they are recurrence of the same type of events, and (2) whether they have a natural order. As such, to explicitly address intra-country correlation, with ordered exits this paper uses marginal risk analysis, the Andersen and Gill [1982] and the Wei et al. (1989) models.Footnote 3 To our knowledge, no such empirical work has been carried out. Our results reveal that the probability of an exit from a fixed exchange rate regime is non-monotonically time-dependent. To control for country-specific heterogeneity, we include time-varying covariates in the Cox (1972) proportional hazard model. The findings from the semi-parametric approach suggest that factors, such as GDP growth, unemployment rate, openness, and government budget deficit affect the probability of a switch from fixed to flexible exchange rate regimes. Economic growth continues to be an impactful covariant through the marginal risk analysis in addition to net foreign asset position and inflation rate. The remainder of the paper is organized as follows: Classification of exchange rate regimes section presents various classifications of exchange rate regimes, while Duration of fixed exchange regimes and definition of exits section discusses durations and provides definitions of exits from a peg. Then Methodological discussion section examines the methodological approaches and Time-varying covariates section presents the time-varying covariates and their expected relationship with the probability of abandoning a peg. Empirical results section follows with empirical results and in Final remarks section we provide concluding remarks.",
48,1,Eastern Economic Journal,19 July 2021,https://link.springer.com/article/10.1057/s41302-021-00196-6,Inequality and the Interwar Gold Standard,January 2022,Weinan Yan,,,Unknown,Unknown,Unknown,Unknown,,
48,1,Eastern Economic Journal,28 June 2021,https://link.springer.com/article/10.1057/s41302-021-00194-8,"Does it Payoff to be Blond in a Non-Blond Neighborhood? Eye Color, Hair Color, Ethnic Composition and Starting Wages",January 2022,Elif S. Filiz,,,Female,Unknown,Unknown,Female,"There is a long-lasting interest in wage discrimination in labor economics literature. Along with gender and race, the physical attributes of an individual might also lead to wage discrimination. In the last three decades, there has been an increasing attention paid to the relationship between labor market outcomes and physical characteristics such as beauty, height and obesity. Economic studies show that beauty is positively related to labor market outcomes of individuals (Hamermesh and Biddle 1994; Biddle and Hamermesh1998; Harper2000; Hamermesh and Parker 2005; Mobius and Rosenblat 2006; Süssmuth 2006; Robins et al. 2011). In these studies, attractiveness is typically measured with ratings given by others, based on photographs or one’s self-reported beauty ratings. There are some other physical characteristics that are utilized in the literature. Persico et al. (2004) and Case and Paxon (2008) find that there is a wage premium for taller adults. Glied and Neidell (2010) show that good dental health, which is determined by exposure to fluoride, increases women’s earnings by approximately four percent. Goldsmith et al. (2007) find that the White/Black wage gap increases as skin color darkens. Hersch (2008) shows that, on average, new immigrants who have lighter skin colors earn more compared to immigrants with darker skin colors. Eye and hair color can also be used to represent the looks of an individual; in other words, they can be used as dimensions of attractiveness. For example, having blue eyes and blond hair represents a certain type of physical feature, which may be considered attractive in some cultures. There are some studies that focus on labor market outcomes for blond women. Using the National Longitudinal Study of Youth 1979 (NLSY79), Johnston (2010) finds that there is a wage premium for blond women. In an experimental design where waitresses are asked to wear blond, red and brown/dark-colored wigs, Guéguen (2012) finds that waitresses with blond wigs receive more tips from male customers. Price (2008) examines whether female fund-raisers’ hair color affects their fund-raising success. He finds that blond fund-raisers raise more money compared to brunette fund-raisers. There are several explanations of the wage premium due to physical attributes in economics and also psychology literature. In economic theory, employer discrimination, consumer discrimination and occupational sorting provide explanations for the wage discrimination due to physical attractiveness. The employers might want to hire more attractive individuals based on two reasons: productivity expectations and taste-based discrimination. They might assume that attractive individuals are more productive, or they might simply prefer to work with attractive colleagues, which is a taste-based discrimination (Becker-type discrimination). Another explanation involving the attractiveness premium is consumer discrimination that stems from consumers’ preference to interact with attractive workers. This type of discrimination might particularly be attributed to certain types of occupations that involve a high volume of consumer–worker interactions (for example servers, salespeople, etc.). Hamermesh and Biddle (1994) and Biddle and Hamermesh (1998); Glied and Neidell (2010) show some evidence of consumer discrimination, employer discrimination and occupational sorting. There are two main groups of theories explaining the attractiveness effect in the psychology literature. These are the socialization and social expectancy theories and fitness-related evolutionary theories. The socialization theory states that appearance creates stereotypes. Due to these stereotypes, people develop different expectations for attractive and unattractive individuals. In addition, people’s treatment of these individuals becomes differentiated.Footnote 1 One can link worker attractiveness and employers’ expectation of high productivity to social expectation theory. The roots of the effects of attractiveness start developing in childhood. During childhood, attractive children might receive preferential treatment from teachers, parents and peers. This can be explained by fitness-related evolutionary theories that relate attractiveness to good genes and differential parental treatment. Attractiveness, in this case, proxies health, quality and reproductive value. Thus, attractive children expected to be popular among their peers and teachers and receive more parental investment due to their presumably high reproductive value. This differential treatment of attractive children might lead to development of higher cognitive and non-cognitive abilities such as test scores, confidence, personality and social skills. Cognitive and non-cognitive skills contribute to human capital accumulation that, in return, is rewarded in the labor market (for example, Goldsmith et al. 1997; Heckman et al. 2006; Fortin 2008; Drago 2011). There are several studies supporting the claim that attractiveness leads to higher human capital accumulation through behaviors and traits acquired due to attractiveness. Mobius and Rosenblat (2006) show that physically attractive workers are more confident and have better communication skills. Pfann et al. (2000) who investigate productivity differences and firm-specific returns to beauty for executives show that the returns to beauty capital is shared between the executives and the firm. They find that the beauty capital has positive impact on the executives’ earnings and on the revenues of the firms. Kuhn and Weinberger (2005) find that men who have leadership positions in high school earn more as adults. In their study, Persico et al. (2004), find evidence that a wage premium due to being tall stems from social club associations in high school. Results in Case and Paxson (2008) show that the height premium is the result of the correlation between height and cognitive ability. In addition, there is some evidence that attractiveness increases academic performance (Cipriani and Zago 2011; Deryugina and Shurchkov 2015). Mocan and Tekin (2010) report a higher criminal propensity for less attractive individuals. They show that this result might be due to hindered human capital development in high school. In this paper, I investigate whether eye and hair color have an impact on the wages individuals earn in the first-job after their schooling (first-job wage). The literature investigating the effect of beauty or physical attributes on labor market outcomes focuses on wages people earn in their mid-thirties or examines the impact of these attributes on wages for high school or college graduates only. The first-job wage does not include premiums due to the human capital accumulated by job experience and/or on-the-job training. If there is a wage premium due to attractiveness because of employer perceptions, it might be observed more accurately at the first-job after schooling. Everything else held constant, two individuals with different physical characteristics, one attractive and the other not, might start at the same position with different wages, the attractive one having higher wages. In the beginning, employers might not be able to observe these two individuals’ actual productivity. However, they might believe that the attractive worker is more productive. In addition, they might engage in taste-based discrimination, resulting in a wage difference between these two otherwise equally skilled workers. However, in time, the employer might realize that the unattractive worker is productive and adjust his/her wage accordingly. Similarly, if the attractive worker is not as productive as the employer initially assumed, s/he might not get wage increases or experience a wage decrease. Thus, using first-job wage would eliminate the productivity differences and productivity gains obtained in the labor market (for example, on-the-job training). It would reflect the employers’ assumption about the individual’s productivity. I also investigate whether there is a wage premium or penalty for individuals who reside among people who have a similar/different physical appearance. I assume that people of the same ethnic origin will have the same or similar physical attributes such as hair and eye color. For example, the hair and eye color of people of African, Hispanic and Asian descents are dark. However, people of European descent are expected to have a greater variation in eye and hair color. If individuals have similar characteristics with their own ethnic group, they might not be considered as attractive. For instance, a woman with blond hair and blue/green eyes might be considered to be attractive in a county where Italian descendants are the majority of the population.Footnote 2 However, if this woman is of Scandinavian descent and residing in a county where Scandinavian descendants are the majority, she might not be considered as different or attractive since the general population in that county is very likely to have similar attributes. Thus, the majority of one’s own ethnic group in the county might matter. However, focusing only on whether one’s own ethnic group constitutes the majority of a given county may not be the most appropriate measure to investigate the match between an individual’s attractiveness and the county’s eye/hair color attributes, which is the proxy for perception of attractiveness. One’s own ethnic group might be a minority, but other ethnic groups in the county might have the same or similar features in terms of eye and hair color. For example, those of German ethnic heritage might be in the minority in a county. However, if people of Scandinavian descent are in the majority, the whole county will be identified by light-eyed (having blue or green eyes) and light-haired (having blond or red hair). In that case, although one’s ethnic group is a minority, s/he might have the same attributes, eye and hair color, of the major ethnic group. In that case, having blue eyes and blond hair might not be considered as different. I use three anthropological studies (Coon 1939; Hulse1963; Geipel 1969) to determine eye and hair color features of each ethnic group in a county and detailed ethnic origin information collected in the US Census. Based on the information obtained from these three anthropological studies, I classify each ethnic group as light-featured or dark-featured. If people of the ethnic group predominantly have blue/green eyes and blond/red hair, that group is defined as “light-featured.” If people of the ethnic group predominantly have brown/black hair and brown/black eyes, then that group is a “dark-featured” one. I use the detailed ethnicity information collected in Census to calculate proportions of people of light-featured and dark-featured ethnic groups in a county. The eye/hair color composition of the county, measured by the share of light-featured or dark-featured ethnicities might be endogenous, potentially affected by unobservable individual, ethnic and location characteristics. Individuals’ decision on where to reside might be affected by their personality and ethnic characteristics, as well as the characteristics of the county where they reside. There might be some ethnic values taught to individuals that might also affect this decision. For example, some ethnicities might be more traditional and attribute high importance to family ties leading individuals to reside in a county where they have many relatives. As a result, their decision about where to reside might affect their wages and the eye/hair color composition in the county. In that case, the coefficient estimates of the variable measuring the eye/hair color composition in the county, the share of light-featured or dark-featured ethnicity, would be biased. To overcome this issue, I use the tipping point analysis outlined in Card et al. (2008) to determine whether the county is light-featured or dark-featured. The tipping point analysis is based on Schelling’s (1978) study where he shows that social groups can affect the race/ethnicity composition of a neighborhood. In other words, individual preferences might change a county’s ethnic composition. Once the share of one group increases and passes a certain threshold, in other words the tipping point, the county might observe flight and avoidance of the other group. Following the fixed-point procedure introduced in Card et al. (2008) I estimate that the average tipping point in dark-featured ethnicities is 19.7. I use the county’s tipping point as the exogenous source of variation in that county’s ethnic composition. If a county’s dark-featured ethnicity population is higher (lower) than the tipping point, the density of dark-featured (light-featured) ethnicities is assumed to be higher in that county. In that case the county can be classified as “dark-featured” (“light-featured”). The identifying assumption is that the residents of counties that are just below the tipping point are similar to the ones just above the tipping point. The results from this analysis show the intent-to-treat effects of county eye/hair color features on wages. I find that having blond/red hair generates a four to six-percent wage premium. Results suggest that ethnic composition of the county has a differential effect on wages for individuals with light-hair. I find that in a dark-featured county, where ethnicities with brown/black hair are in the majority, there is a wage premium for females and white people with light-features (blond/red hair) compared to females and white people with dark-features (brown/black hair). To be specific, there is a nine-percent wage premium for females and a ten-percent wage premium for white people. There is no evidence of wage penalty/premium for individuals with light-features (dark-features) living in counties where light-featured (dark-featured) ethnicity shares are higher. The results are robust in alternative specifications where occupation and ethnicity fixed effects are included, and they are robust to alternative distance to the tipping point bandwidth selections. The rest of the paper proceeds as follows: Section 2 introduces the empirical specification, Section 3 provides the data and the descriptive statistics, Section 4 presents the results, Section 5 presents possible mechanisms and sensitivity analyses, and the last section concludes.",
48,1,Eastern Economic Journal,20 August 2021,https://link.springer.com/article/10.1057/s41302-021-00197-5,Why Try? The Superstar Effect in Academic Performance,January 2022,Rey Hernández-Julián,Christina Peters,,Unknown,Female,Unknown,Female,"When faced with a competitor who far outmatches them, the performance of most individuals appears to suffer. Economists have documented this effect of competing against a superstar across a handful of professional sports as well in experimental settings.Footnote 1 For example, Brown (2011) and McFall and Rotthoff (2020) find that during the period of Tiger Woods’ peak performances, professional golfers earned worse scores and engaged in riskier strategies when participating in tournaments with Woods, compared to tournaments in which he did not play. Sunde (2009), Lallemand et al. (2008), and Berger and Neiken (2014) document similar negative impacts of skill heterogeneity on performance in professional tennis and handball tournaments. Beyond sports, a contest experiment by Hargreaves Heap et al. (2015) finds that a high level of heterogeneity among teams in team competition induces both the superior and inferior teams to exert less effort. Furthermore, Peltier and Moreau (2011) provide evidence that the impact of a superstar can vary depending on the environment, by showing that bestselling books get a smaller market share online relative to offline sales. Our paper extends the findings from this tournament literature regarding superstars and increased heterogeneity into a new setting: academic performance in a classroom. Within the social science literature, student performance has typically been examined in the context of peer effects. However, while we do place our results within that literature, our main goal in this paper is to expand the tournament literature using a new context. Even though the depressed levels of performance identified in the tournament literature are becoming more widely documented, the mechanism driving the decline remains unclear and may depend on the empirical setting. Documenting evidence of a superstar effect in alternative settings, such as an academic environment, thus becomes important in enabling us to pinpoint probable mechanisms. Much of the tournament literature so far suggests an incentive effect is at work as a driving mechanism, arguing that a large skill differential appears to reduce effort on the part of the less talented player. In other words, when matched with an obviously superior opponent, individuals ask themselves, “Why bother to try?” In support of this mechanism, Baik (1994), Dixit (1987), and Lazear and Rosen (1981) show that in asymmetric, two-player theoretical games, both the stronger and weaker players exert less effort as the asymmetry between their skills grows. Players thus exert maximum effort only when they are evenly matched. Franke (2012) finds empirical evidence to support this model, showing that amateur golfers perform better in tournaments that use a scoring system adjusted for player handicaps (thereby providing a level playing field) compared to tournaments that tabulate unadjusted scores. Further, Coffey and Maloney (2010) present evidence that a psychological thrill of victory could push competitors beyond the effort levels that would be optimal for a rational decision-maker. In settings where an individual’s outcome depends not just on their own performance but also reflects the performance of their opponents (as in sports with match play), uneven matches may simply enable the superior performer to win more easily and by a wider margin, given his or her greater abilities and strengths (Sunde 2009). Thus, in contrast to manipulating incentives, it is possible that a capability effect is at work instead. In these settings, disentangling the relative magnitudes of the incentive and capability mechanisms will be difficult as long as effort remains unobserved. Using data from professional tennis matches, Sunde (2009) distinguishes between the mechanisms and finds empirical support for the presence of an incentive effect in unequal competitions. We document the superstar effect and test these competing mechanisms in a new context by looking at student academic performance. We define a superior performer as an individual with a cumulative GPA more than 1.5 standard deviations above the mean for their major. Similarly, a student with a GPA more than 1.5 standard deviations below the mean becomes an inferior performer. Using academic records from a large, public, open-enrollment university, we show that students who are in a classroom with a superior performer earn grades that are approximately 0.02 grade points lower than when they are in classrooms without those outliers (this difference is nearly 1% of the mean grade 2.8). The presence of an inferior performer appears harmful as well, with the same students scoring 0.02 points lower in a class with such a low outlier than one without. We also find suggestive evidence that these effects may be gendered, perhaps in an unexpected way: women appear to do especially worse in the presence of an inferior male performer, while men do worse in the presence of an inferior female performer. Beyond merely documenting this superstar effect, our data on superior and inferior outliers further enable us to explore the mechanism behind the relationship. In an academic setting, it is possible both for teachers to assign, and students to perceive, course grades as either individual contests or competitive ones, which means that either the capability or incentive effects (or both) may be at play. If the average student does not try as hard on coursework because he or she assumes that the superstar will earn the “A” regardless, then the incentive effect is the driving mechanism behind our observed superstar effect. However, an alternative explanation for our results could be a capability effect: in a competitive environment, the average student is relegated to a lower grade simply because his or her abilities are so far below the superstar. As discussed in Sunde (2009), both the incentive and capability mechanisms predict a negative effect of superior performers on the average individual. However, they provide opposing predictions for the effect of inferior performers. The capability mechanism asserts that uneven matches will enhance the performance of the favored opponent. In our setting, therefore, we would predict that high-achieving individuals would earn higher grades in the presence of especially inferior outliers. However, the incentive mechanism would predict that high-achieving individuals have reduced incentives to try when taking classes with an especially inferior performer; thus, the grades of high-achieving students should go down. Indeed, our empirical results suggest just that: students with normalized GPAs above the mean across the institution actually earn lower grades when they have an inferior peer outlier in their classes, compared to their grades in courses without such outliers. Moreover, we exploit differences between face-to-face classroom environments and online courses to find significant outlier effects in face-to-face courses only. In order for the incentive effect to work, students must be aware of classmate type (and the presence of outliers in the class). The capability mechanism has no such requirement. Because it is likely that students are more aware of classmate type in a traditional face-to-face classroom environment, the absence of a result in online courses is thus further supportive of the incentive effect. Consequently, similar to Sunde (2009), our results are most consistent with the incentive channel as the dominant mechanism behind the superstar effect. This paper extends the literature in three important ways. First, we conduct an empirical test of the superstar effect in an entirely new setting, which has several important implications. So far, economists have heavily relied on explicit tournaments to test for the presence of a superstar effect. We find that this effect also exists in a setting that is not a true tournament (though it may often give students the impression of one). Documenting this effect in an academic context further suggests that it affects many more people in settings with much lower stakes than merely a handful of professional sports tournaments. Second, our evidence suggests a particular mechanism behind the academic superstar effect, in that it appears that the presence of superstars (both superior and inferior) reduce incentives to perform well. Finally, our paper contributes to the literature on peer effects in education by distinguishing the effect of an extreme outlier peer as separate from the effects of ability-sorted or more average peer groups. Although the peer effects literature tends to find that high-achieving peers are beneficial, our results suggest that extraordinarily high-achieving peers and extreme heterogeneity of abilities in the classroom may actually have a harmful effect.",2
48,2,Eastern Economic Journal,05 January 2022,https://link.springer.com/article/10.1057/s41302-021-00207-6,Border Walls and Crime: Evidence From the Secure Fence Act,April 2022,Ryan Abman,Hisham Foad,,,Male,Unknown,Mix,,
48,2,Eastern Economic Journal,11 April 2022,https://link.springer.com/article/10.1057/s41302-022-00215-0,Environmental Regulation and Export Performance: Evidence from the USA,April 2022,Zinnia Mukherjee,Niloufer Sohrabji,,Unknown,Unknown,Unknown,Unknown,,
48,2,Eastern Economic Journal,02 April 2022,https://link.springer.com/article/10.1057/s41302-022-00216-z,Prolonged Protests and Student Achievement: Evidence from Political Unrest in Thailand,April 2022,Kawin Thamtanajit,,,Unknown,Unknown,Unknown,Unknown,,
48,2,Eastern Economic Journal,26 August 2021,https://link.springer.com/article/10.1057/s41302-021-00205-8,Assessing Learning in College Economics: A Sixth National Quinquennial Survey,April 2022,Cynthia Harter,Rebecca G. Chambers,Carlos J. Asarta,Female,Female,Male,Mix,,
48,2,Eastern Economic Journal,25 January 2022,https://link.springer.com/article/10.1057/s41302-022-00209-y,Effect of Political Quotas on Attributes of Political Candidates and Provision of Public Goods,April 2022,Chitra Jogani,,,Female,Unknown,Unknown,Female,"Under-representation on the basis of identity exists in many sectors across the world, including education, employment, and politics. To address the under-representation, policymakers often turn to affirmative action policies. But, affirmative action policies are controversial because of the fear that they may lead to candidates or employees of lower qualifications or ability. This controversy has been investigated more widely for affirmative action policies in education and employment opportunities (Bagde et al. 2016; Holzer and Neumark 1999; Francis-Tan and Tannuri-Pianto 2018).Footnote 1 In this paper, I investigate this controversy for a popular form of affirmative action policy in politics – political quotas. To assess the effect of quotas, I use political quotas in state elections, which have been in place for the past seven decades in the largest democracy, India. Various studies have focused on quotas in the local village council where the assignment of quotas is randomized, unlike the quotas in the state legislature (Chattopadhyay and Duflo 2004a, b; Bardhan et al. 2010; Dunning and Nilekani 2013). The quota in state elections exists for the historically disadvantaged groups, the Scheduled Castes and Scheduled Tribes, who comprise a quarter of India’s population. The quotas are implemented by “reserving” approximately a quarter of the total 4,120 electoral districts in India.Footnote 2 Reservation of an electoral district for Scheduled Castes (Tribes) stipulates only citizens belonging to the Scheduled Castes (Tribes) can stand for elections (for district’s representative in the state legislative assembly) in the district. Estimating the causal effect of quotas can be challenging as the assignment of quotas is not random, leading to endogeneity issues, such as quotas being correlated with economically poor electoral districts. But, the assignment of reservation status to constituencies by the Delimitation Commission in India provides a suitable empirical setting for tackling the endogeneity problem. The reservation status of an electoral district (or constituency) depends on the population share of the reserved groups. Although there is no explicit population cutoff, I exploit the procedure of reservation after the latest delimitation in a novel way to establish a discontinuous relationship between the share of the reserved population and the reservation status of constituencies. To understand if caste-quotas affect the qualifications and ability of a candidate, I study the following questions. First, I examine if quotas affect representation of candidates based on other desirable qualifications, such as honesty or competence. As a measure for desirable attributes, I use information on criminal charges, education level, and wealth of all candidates for state elections post the redistricting in 2008. Second, I study if there is a difference in the delivery of village facilities, such as schools, hospitals, and roads, between quota-bound and non-quota-bound areas. Finally, I use the gender of candidates to investigate if caste-quotas displace the other underrepresented group in politics - women. The primary findings on how quotas affect the attributes of candidates: Candidates standing for election from Scheduled Caste constituencies are less likely to have a criminal charge (4.4 percentage points); have lower assets (0.14 million USD or 76 percent lower); and have similar education levels to candidates in constituencies not reserved for the Scheduled Castes. Comparing the estimates with statistics on attributes for the overall population, the results suggest that the differences observed in the attributes of political candidates do not merely reflect the differences in attributes of the populations of reserved and unreserved castes. Second, in Scheduled Caste constituencies, more women seek election (5 percentage points more than in non Scheduled Caste constituencies), and more women win (8 percentage points). The effects are larger for candidates affiliated with political parties compared to candidates who contest independently (35 percent of candidates are independents). Based on the data on facilities in all villages in 2011 and using regression discontinuity, the results do not indicate a significant difference caused by reservation status in the availability of facilities, such as schools, hospitals, roads, and banks across constituencies. The size of the estimates implies any effect greater than a decrease in the availability of facilities in 4 percent of villages in a reserved constituency can be ruled out. Thus, the reserved constituencies are on par with similar constituencies that are unreserved. This paper relates to several literature. First, this paper contributes to the literature on the effect of affirmative action policies on the quality of candidates. Most studies in the affirmative action literature have focused on the effect of policies in college admissions and employment opportunities (Holzer and Neumark 1999; Bagde et al. 2016; Francis-Tan and Tannuri-Pianto 2018), whereas this paper studies a policy for political candidates who are in a crucial position of managing a state. Although the attributes that define a “good” politician are not clear, to the extent that having criminal charges can be considered a bad attribute and the level of education a good attribute, the reserved constituencies are better off in this respect.Footnote 3 Second, a concern in the affirmative action literature is that affirmative action policies targeting one minority group may displace people from other underrepresented groups (Bertrand et al. 2010). Additionally, the importance of studying the effect of quotas on more than one dimension of identity has received recent attention (Cassan and Vandewalle 2017; Munshi 2019). This question has been studied for gender quotas in local elections in India: Cassan and Vandewalle (2017) show that gender quotas for women in local village councils increase representation of women from lower caste, whereas, Karekurve-Ramachandra and Lee (2020) find the opposite evidence for local bodies in Delhi, India. This paper adds to the above literature by providing causal evidence that caste-quotas in the state legislature lead to more representation of women. Finally, this paper adds to the few studies on the effect of political quotas in state legislatures on provision of public goods (Jensenius 2015; Min and Uppal 2012). The effect of quotas can change over time, but the null effect obtained in this paper aligns with the findings in Jensenius (2015). Jensenius (2015) uses propensity score method to find no impact of reservation for Scheduled Castes on several development indicators during the period of 1971–2001, when there was a freeze in redistricting. The latest redistricting, which occurred after three decades, corrected for malapportionment within a state. This paper uses new electoral districts and regression discontinuity design to analyze a broader set of village facilities. To the best of my knowledge, this is the first paper using data on new electoral districts and the above identification strategy. It also establishes the null result on reservation for Scheduled Tribes as well, as the literature has found different effects of quotas for Scheduled Castes and Scheduled Tribes.Footnote 4",
48,3,Eastern Economic Journal,24 March 2022,https://link.springer.com/article/10.1057/s41302-022-00214-1,Introduction to the Symposium on Macroeconomic Research at Liberal Arts Colleges,June 2022,Julie K. Smith,,,Female,Unknown,Unknown,Female,,
48,3,Eastern Economic Journal,29 April 2022,https://link.springer.com/article/10.1057/s41302-022-00213-2,Public and Private Benefits of Information in Markets for Securitized Assets,June 2022,Matthew J. Botsch,,,Male,Unknown,Unknown,Male,"Did the option to resell newly-originated mortgages encourage lenders to relax their lending standards in the run-up to the financial crisis of 2007–08? In the years since, a voluminous academic literature debates this question: e.g. Mian and Sufi (2009), Keys et al. (2010), Bubb and Kaufman (2014), Adelino et al. (2016), Foote et al. (2021). Notwithstanding the academic debate, policymakers answered this question in the affirmative when crafting the Dodd-Frank Act of 2010. A major component of the legislation (Section 941) requires sellers of most asset-backed securities to retain at least 5% of the underlying asset. This credit risk retention requirement aims to correct the agency problem between originators of and investors in securitized assets. According to this story, originators planning to sell newly-originated mortgages exerted suboptimal pre-origination screening effort and originated a high volume of loans that subsequently underperformed. Dodd-Frank corrects this misalignment of incentives by requiring lenders to retain more “skin in the game.” However, given that markets for securitized assets had existed in the U.S. for decades prior to the financial crisis (Foote et al. 2012 Fact 5), we should expect purchasers in these markets to understand sellers’ incentives and respond accordingly. The separation of origination from ultimate exposure to default risk creates two distinct problems that distort the bank’s incentives differently. On the one hand, learning about its loan applicants enables a bank to identify and screen out non-creditworthy types prior to origination, improving the average quality of all loans that are originated. This is a public benefit of bank information production, since it results in a more efficient allocation of scarce credit and increases social welfare. Banks that plan to resell a new loan will exert less effort and produce less information than is socially optimal: this is the standard agency problem that many believe lay at the root of the mid-2000s housing crisis (e.g., Keys et al. 2010; Purnanandam 2011; Hartman-Glaser et al. 2012; Malamud et al. 2013), and that Dodd-Frank aims to correct.Footnote 1 On the other hand, learning about its borrowers might also allow the bank to selectively offer for sale loans only it knows are low-quality. This is a private benefit of bank information production, since the bank may profit from its informational advantage vis-à-vis unsuspecting purchasers, transferring but not increasing social welfare. The potential for adverse selection can hinder trade (Akerlof 1970) and might be particularly prevalent in secondary mortgage markets (e.g., Ashcraft and Schuermann 2008; Downing et al. 2009; Agarwal et al. 2012; Adelino et al. 2018). Rational purchasers will respond to these two problems differently. The agency problem is that banks bear 100% of the cost but only a fraction of the (public) benefit of acquiring information about their borrowers, so they acquire too little information. Strategic purchasers will respond by preferring to buy loans that are less costly to screen on the margin, encouraging bank information production. The adverse selection problem is that the private benefit to the bank of acquiring information about borrowers exceeds the social benefit, so banks will acquire too much information. Strategic purchasers will respond by preferring to buy loans that are more costly to screen on the margin, discouraging bank information production. This distinction between agency versus adverse selection problems, or between banks acquiring too little versus too much information about their borrowers, is often glossed over in popular discussions.Footnote 2 Testing the idea that secondary-market purchasers respond rationally to banks’ misaligned incentives requires a publicly-observable proxy for bank information costs. The empirical banking literature establishes that there is a tight link between information costs and lender–borrower distance (e.g., Berger et al. 2005). Building on this insight, Fig. 1 plots the private-label mortgage sale rate for loans within ± $10,000 of the Conforming Loan Limit (CLL) against lender–borrower distance in miles between 1990 and 2000, based on Home Mortgage Disclosure Act (HMDA) data of all U.S. mortgage originations. The graph shows that the private sale rate increases with distance for jumbo loans (above the CLL) but is decreasing or flat for conforming-size loans (below the CLL). I am the first to document this fact, to my knowledge. Since more distant borrowers are more costly to screen, this strongly suggests that the jumbo market is operating in a low-information equilibrium and the conforming-size market in a high-information equilibrium. That is, purchasers’ buying patterns discourage private bank information production in the jumbo market, consistent with an adverse selection problem, but encourage it in the conforming-size market, consistent with an agency problem. This evidence indicates that both problems are at play, and that it is not so obvious whether the option to resell assets encourages or discourages information production in equilibrium.Footnote 3 Private Sale Rates versus Lender–Borrower Distance. Notes: This figure shows the average number of mortgages sold to non-affiliate, private counterparties (= not Fannie Mae or Freddie Mac), as a percent of all 1–4 family, owner-occupied, home purchase mortgages originated in a continental United States MSA, within $10,000 of the Conforming Loan Limit (CLL), during 1990–2000. “Conforming” loans are below the CLL and “jumbo” loans are above the CLL. Loans within $1,000 of the CLL are excluded. Source: HMDA / author’s calculations In this paper, I endogenize the bank’s origination and information acquisition decisions in a setting where assets may be resold. Potential borrowers are indexed by their distance from the bank: as distance increases, bank screening costs increase and the quality of the average borrower declines. “Distance” may be interpreted literally, but the results will hold for any publicly-observable borrower characteristic that is correlated with the cost of information acquisition. In autarky, i.e. if the resale of loans is not allowed, banks will choose to acquire information about nearby borrowers, screen out the bad types, and originate loans for the good types. But far away from the bank, it is too costly to screen and credit supply shuts off for both types of borrowers. Good types who live in such banking deserts do not have access to credit. The economic motivation for secondary markets is to prevent this market failure; but the resulting agency and adverse selection difficulties may cause other problems. I develop a unified model that incorporates both of these frictions and derive conditions under which either a low- or a high-information equilibrium will occur. Which equilibrium arises depends on the values of parameters that may be interpreted as the public benefit of information production—improved average payoffs on all loan originations—versus the private (to the Bank) benefit of information production—the informational rent captured by the Bank, which is a transfer from the Purchaser that does not affect total social welfare. If the public benefit exceeds the private benefit, then the Purchaser’s equilibrium buy strategy is a decreasing function of distance. That is, the option to sell the asset on secondary markets incentivizes local, informed lending. On the other hand, when the private benefit exceeds the public benefit of learning, the Purchaser’s equilibrium buy strategy is an increasing function of distance. The option of a possible sale incentivizes distant, uninformed lending. In this latter parameterization, the Bank and the Purchaser choose to remain symmetrically uninformed about borrower quality in equilibrium. This provides an explanation for both the upward-sloping mortgage sale rate in the jumbo market and the downward-sloping mortgage sale rate in the conforming-size market seen in Fig. 1. Several recent papers have considered related problems. I differ from these papers by combining three elements that have not previously been studied together: I consider the interaction between adverse selection and moral hazard frictions; I endogenize the bank’s information acquisition decision; and I analyze lenders’ choice between two lending technologies, nearby and faraway. Chemla and Hennessy (2014) also consider the interaction between adverse selection and moral hazard frictions, but in their model the lender always observes a private signal, independently of whether it exerts pre-origination effort. Frankel and Jin (2015) study the effect of asymmetric information between local and remote lenders on competition, securitization, and default rates. Their model focuses on adverse selection and does not incorporate a moral hazard friction: as they write in their conclusion, “The relative importance of these two theories is an interesting open question” (p. 23). They further assume that remote lenders are exogenously less informed than nearby lenders, whereas I endogenize the information acquisition decision. An implication of these differences is that the option to securitize might encourage rather than discourage information production, when the moral hazard friction dominates. Vanasco (2017), building on the framework of DeMarzo and Duffie (1999), analyzes endogenous information acquisition with a non-separable signal and the option to resell. There is no distance variable in her model, whereas I analyze the interaction between information acquisition and lender–borrower distance, considering cases both where distance is exogenously assigned by Nature and where it is endogenously chosen by the Bank. Also, in her model the originator does not have the option not to originate. Explicitly modeling this decision highlights the key social benefit of secondary markets: in autarky, lenders will choose to originate fewer loans than is socially optimal.Footnote 4 Focusing on the low-information equilibrium, my model makes several predictions concerning ex post loan performance that are confirmed by the empirical literature. First, sold loans should perform worse than retained loans due to adverse selection.Footnote 5 Krainer and Laderman (2014), Rajan et al. (2015), and Elul (2016) all find evidence consistent with this prediction.Footnote 6 Mian and Sufi (2009) and Demyanyk and Van Hemert (2011) suggest that increased access to securitization after 2001 led to lower underwriting standards, increased lending in low-income-growth neighborhoods, and higher default rates.Footnote 7 Second, distant loans should perform worse than nearby loans due to increasing information acquisition costs. DeYoung et al. (2008) find that the small business loans of remote-lending banks default more frequently, and La Cava (2013) shows the same for retained mortgage loans. Third, the performance gap between sold and retained loans should diminish as lender–borrower distance increases. I am unaware of any papers that test this prediction. The rest of the paper is laid out as follows. In Section Empirical Evidence on Distance and Mortgage Securitization, I present further empirical evidence that the relationship between lender–borrower distance and secondary market mortgage sale rates depends on the economic setting; i.e., it is not universally positive or negative. This observation motivates writing down a model to understand whether mortgage securitization should theoretically increase or decrease with information costs. I present the model in three steps. First I set up and solve a game-theoretic model with agency frictions only in Section An Agency-Only Model of Mortgage Loan Sales. Next, I introduce asymmetric information into the model and solve for players’ equilibrium strategies, holding information acquisition costs fixed, in Section The Full Model with Fixed Auditing Costs. Finally, I allow costs to increase with lender–borrower distance in Section The Full Model with Distance-Varying Auditing Costs and analyze the comparative statics of players’ strategies and of the mortgage sale rate. Section Numerical Examples gives two brief numerical examples. In Section Extensions and Policy Implications I analyze several extensions of the model, including increasing the number of borrower types, endogenizing the Bank’s choice of distance between nearby and faraway customers, and hiding this decision from the Purchaser. At the end of this section I discuss the policy implications and analyze whether policymakers can improve economic outcomes by requiring lenders to retain a fraction of sold loans’ credit risk, à la section 941 of the Dodd-Frank Act. Section Conclusion concludes. All proofs are in Appendix C.",1
48,3,Eastern Economic Journal,14 June 2022,https://link.springer.com/article/10.1057/s41302-022-00217-y,Correction to: Public and Private Benefits of Information in Markets for Securitized Assets,June 2022,Matthew J. Botsch,,,Male,Unknown,Unknown,Male,"In Table 1 of this article, in column 2, the coefficient for ‘Growth rate of median HH income, 1990–2000 (%)' should have been ‘− 0.0002' instead of ‘− 0.00027'. Values ‘(0.037)' and ‘(0.067)' in columns 2 and 3, respectively, have been moved one row below. The original article has been corrected.",
48,3,Eastern Economic Journal,25 March 2022,https://link.springer.com/article/10.1057/s41302-022-00211-4,Currency Devaluation as a Source of Growth in Africa: A Synthetic Control Approach,June 2022,Florence Bouvet,Roy Bower,Jason C. Jones,Female,Male,Male,Mix,,
48,3,Eastern Economic Journal,14 March 2022,https://link.springer.com/article/10.1057/s41302-022-00210-5,Should I Stay or Should I go Now? The Effect of Bank Mergers on Bank–Firm Relationships in Japan,June 2022,Heather Montgomery,,,Female,Unknown,Unknown,Female,"This study examines whether bank mergers affect client firm relationships with the merging banks. Do bank mergers damage client firm relationships with their so-called main bank? If so, does it matter? This study empirically investigates the effect of bank merger announcements on the likelihood of bank–firm relationship termination, the amount of credit supplied to the client firm from the main bank, and growth in the firms’ total outstanding borrowings. With theory pointing to different possible outcomes, the answers to the queries posed above are empirical. However, data explicitly identifying bank–firm relationships are rare. The few existing empirical studies exploring bank–firm relationships have extrapolated the existence bank–firm relationships from individual bank–firm loan data. In general, however, information on individual loan contracts is not available. When researchers have been able to obtain individual bank–firm loan data, it has usually a sub-sample of data from small firmsFootnote 1. This study explores the effect of bank mergers on bank relationships with large, informationally transparent client firms which often maintain multiple bank relationships and presumably have access to alternative, direct, financing. Analysis of the full universe of publicly listed firms allows investigation of potential differences in the effects of bank mergers for different types of firms. This enables testing of the hypotheses in the theoretical literature that financial institutions may have perverse incentives to provide credit to bad firms—the soft budget constraint problem (Bolton and Scharfstein 1996)—while constraining good firms—the hold-up problem (Stiglitz and Weiss 1981; Sharpe 1990). Using a uniquely rich data set from Japan combining survey data on bank–firm relationships with information on bank merger announcements, financial statements filed by the banks and firms themselves and individual bank–firm loans, this study investigates whether large, informationally transparent firms “stay” with their declared main bank following a bank merger announcement or take advantage of the opportunity to “go” to a new lender. This study also explores the relationship between bank–firm relationship termination and other outcomes: the supply of loans from the former main bank and total outstanding loans made to the client firms. The empirical results to follow demonstrate that bank mergers do damage the affiliated firms’ bank relationships. Bank–firm relationships are more likely to be terminated and the client firms switch to a new main bank following a bank merger announcementFootnote 2. This is significant in and of itself, since there is a long literature demonstrating the importance of the main bank–client firm relationships. For small firms, one important benefit of a close relationship with a bank is the availability of credit (Petersen and Rajan 1994). But what about the larger, informationally transparent, listed firms in this study? Analysis of individual loan data between specific bank–firm pairs demonstrates that even for the large, publicly listed firms investigated in this study, when the relationship with the main bank is terminated, client firms experience a significant contraction in the credit supplied by the former main bank. Some firms, however, are able to compensate for the reduction in loans from their main bank with lending from other banks. Healthy firms that lose their main bank relationship are able to compensate for the reduction in credit supplied by the main bank with loans from other banks. In fact, in support of the theory of informational capture and hold up costs, outstanding bank loans actually increase for healthy firms that terminate their main bank relationship following a bank merger announcement. Unlike healthy firms, unhealthy firms that terminate their main bank relationship following a bank merger are not able to make up for the reduction in credit supplied by the main bank by turning to other banks. For unhealthy firms that terminate their main bank relationship, outstanding loans fall by nearly 30% in the three years following a merger announcement by their main bank. Thus, one important policy implication of this study is that through their impact on bank-firm relationships, bank mergers improve the allocation of bank financing. The rest of this paper is organized as follows. The next section gives some background on bank–firm relationships in Japan, the institutional setting of the research. The ""Data"" section discusses the data used in the empirical analysis, and the ""Empirical Methodology"" section the empirical methodology. In ""Results""  the results of that empirical analysis are presented. The final section, Conclusion, concludes.",
48,3,Eastern Economic Journal,04 April 2022,https://link.springer.com/article/10.1057/s41302-022-00212-3,"Intelligence, Religiosity, and Environmental Emissions",June 2022,Jay Squalli,,,Male,Unknown,Unknown,Male,"Blaise Pascal, a French mathematician, physicist, and religious scholar once said that “men never do evil so completely and cheerfully as when they do it from a religious conviction.” This quote highlights humanity’s dark history of destruction, violence, and atrocities in the name of religion. In fact, as much as religiosity can provide guidance to individuals in living a moral and ethical life, it is a powerful instrument that can shape the minds of individuals into forming distorted, irrational, and inflexible perspectives about the world in which they live. In particular, anthropogenic climate change, one of the most salient issues of our time, may go beyond profligate consumption, potentially extending to specific religiously motivated actions toward the environment. There is evidence that religiosity is associated with negative attitudes toward the environment (Hand and Van Liere 1984; Guth et al. 1995; Schultz et al. 2000; Truelove and Joireman 2009; Tsimpo and Wodon 2016)– attitudes generally echoed by the belief in dominion over nature.Footnote 1 Furthermore, negative correlation between intelligence and religiosity (Lynn et al. 2009; Nyborg 2009; Reeve 2009; Reeve and Basalik 2011; Cribari-Neto and Souza 2013; Ganzach and Gotlibovski 2013; Zuckerman et al. 2013; Webster and Duffy 2016) and between intelligence and environmental outcomes (Salahodjaev 2016b; Omanbayev et al. 2018) suggest that assessing the relationship between intelligence and the environment requires understanding the complex nexus between intelligence, religiosity, and environmental emissions. This paper hypothesizes that religiosity may not only influence actions toward the environment but also has the potential to moderate the relationship between intelligence and environmental emissions. The rationale behind this hypothesis lies in the expectation that religiosity, materializing in the strict attachment to scripture and the belief system of a religious group, can interfere with intelligent people’s ability to take pro-environmental actions. Figure 1 provides a simplified hypothesized depiction by showing bidirectional relationships between religiosity and intelligence and between intelligence and the environment (measured using environmental emissions). The figure also shows a unidirectional relationship between religiosity and the environment largely due to the absence of evidence or theory on how emissions ought to influence religiosity. The hypothesized nexus between religiosity, intelligence, and the environment Ideally, the link between intelligence, religiosity, and environmental emissions should be addressed using data at the household or individual level. However, to the author’s knowledge, no data comprised of similar objective measures are available at the individual level, thus forcing the analysis to make use of data at the US state level. The aggregate nature of these data can raise concerns that the estimates derived might not reflect individual or household behavior (Robinson 1950), thereby potentially rendering the analysis meaningless. This issue is usually dubbed as the ecological fallacy, “in which relationships between characteristics of individuals are wrongly inferred from data about groups” (Selvin 1958, p. 613). While aggregation bias is a legitimate concern and applicable to numerous cases, it is worth mentioning that empirical research providing support for the ecological fallacy has been largely based on partial correlation analysis, which has been found to be more sensitive to aggregation bias than multiple regression analysis (Lichtman 1974). This is especially important due to the latter’s ability to address intercorrelated independent variables and other confounding factors typically ignored in partial correlation analyses (Schwartz 1994). Furthermore, studies making use of data only available at a high level of aggregation and theoretically grounded specifications may yield more meaningful, useful, and precise estimates than those studies making use of disaggregated (limited) data and poorly specified models (Lichtman 1974; Schwartz 1994). In fact, in the absence of data at the individual or household level that provide the same insight as state-level data, aggregate data may be useful predictors of individual behavior (Freedman 1999). This is not to say that estimates at the US state level in this paper are indicative of individual or household behavior. This is undoubtedly a task that can only be established after conducting a similar analysis using more disaggregated data. Nevertheless, this is the first study addressing the nexus between intelligence, religiosity, and environmental emissions and should represent a stepping stone to future disaggregated research. This paper expects to make a number of important contributions. First, it avoids subjective measures of environmentalism such as those in previous research by using environmental emissions in lieu of expressed attitudes toward the environment. Second, the focus on religiosity rather than on religious affiliation helps shed light on the association between environmental outcomes and religious factors that are common across all represented religious denominations. Third, endogeneity arising from potential reverse causality between intelligence and environmental emissions and between religiosity and intelligence raises important identification concerns, which have been largely ignored in the previous literature and must be addressed using simultaneous equation modeling or instrumental variable analysis. This helps address the complexity of the relationship between intelligence, religiosity, and emissions and represents an important methodological contribution to the literature. Understanding this relationship is especially interesting for the USA, which is endowed with a high IQ (consistent with developed countries) and a peculiarly low rate (10.5%) of atheists (Lynn et al. 2009). It is also a place where more than 70% of the population identifies as Christian while being dubbed as a “nation of biblical illiterates” (Prothero 2009). Furthermore, the use of US state-level data is particularly advantageous due to their consistency and reliability especially in comparison with cross-country data. The fact that US states share common traits also allays concerns about unobserved heterogeneity typically encountered when using cross-country data. These combined elements make examining the USA a compelling case study of potential interest to policymakers. To this end, this paper is organized as follows. ""Background"" Section  provides the background information. ""Material and methods"" Section describes the data and methodology. ""Results""Section summarizes the empirical results. ""Discussion and conclusions"" Section  discusses the results and concludes.",
48,4,Eastern Economic Journal,12 June 2022,https://link.springer.com/article/10.1057/s41302-022-00218-x,"US Health Care Expenditures, GDP and Health Policy Reforms: Evidence from End-of-Sample Structural Break Tests",October 2022,Ben Brewer,Karen Smith Conway,Robert S. Woodward,Male,Female,Male,Mix,,
48,4,Eastern Economic Journal,30 June 2022,https://link.springer.com/article/10.1057/s41302-022-00219-w,Female Role Models and Labor Force Participation: The Case of the All-American Girls Professional Baseball League,October 2022,Margo Beck,Sara LaLumia,,Female,Female,Unknown,Female,"The rise in female employment between 1940 and 1950 has been well documented, and many contributing factors have been explored. As men left civilian jobs to serve in World War II, new employment opportunities opened up for women. Mulligan (1998) argues that the influx of women into the labor force in this era cannot be entirely explained by monetary incentives, and that a reduction in discriminatory attitudes toward female employment may have also played a role. This paper studies how one particular institutional change contributed to this pattern. We consider how exposure to a professional women’s sports league affected female labor market outcomes. In particular, we study the All-American Girls Professional Baseball League (AAGPBL). This league was established in 1943 and was active in several Midwestern cities for the next decade. We hypothesize that in cities with an AAGPBL team, residents had more opportunities to see a woman “doing a man’s job.” This may have altered social norms about the acceptable economic role of women. There are potential labor supply effects, with a possible increase in female willingness to enter the labor market, and potential labor demand effects, with employers perhaps increasing their willingness to hire women. We investigate the net effect of these changes on female labor market outcomes. We employ a difference-in-difference strategy, using decennial Census data to compare women’s labor market participation in the cities that had an AAGPBL team and in nearby control cities without an AAGPBL team. We find that the presence of an AAGPBL team is associated with a 1.8 percentage point increase in the female labor force participation rate, and with a 1.9 percentage point increase in the probability that a woman is employed. Effects are larger for married women and for women with less than a high school degree. We also find positive effects on other measures of female labor market activity. Decennial data are not ideal for uncovering a causal impact, particularly when teams were introduced to different cities in different intercensal years, and we cannot rule out all possible confounders related to the war effort. The greatest threat to our identification strategy is that AAGPBL teams were not randomly allocated to cities, and that female employment would have grown more in the treated cities than in the control cities even absent the introduction of the league. We address this concern in a variety of ways. We show that the treatment and control cities had parallel trends in female employment between 1930 and 1940, before the league’s existence. We carry out randomization inference, showing that our results are in the right tail of the distribution of results when a city’s AAGPBL status is randomly assigned. Ultimately, we cannot make a fully compelling case for causal interpretation. Instead, our results might be interpreted as bordering the divide between descriptive and causal evidence. To the best of our knowledge, no previous literature has considered the economic implications of the AAGPBL. Our findings add to the literature studying historical female employment trends and the effects of culture and role models on labor market outcomes. We also contribute to understanding the full set of factors generating the sharp rise in female employment during the World War II era.",
48,4,Eastern Economic Journal,02 July 2022,https://link.springer.com/article/10.1057/s41302-022-00220-3,"The Impact of Sporting and Cultural Events in a Heterogeneous Hotel Market: Evidence from Austin, TX",October 2022,Clay Collins,Craig A. Depken II,E. Frank Stephenson,Male,Male,Unknown,Male,"Local politicians and tourism promoters often point to tourist inflows associated with sporting and cultural events to justify public subsidies for the events. However, more than three decades of research on the economic impact of sporting events find scant evidence that hosting such events generates benefits commensurate with their public subsidies.Footnote 1 Of course, overpromised benefits are just one aspect of what Müller (2015) refers to as “mega-event syndrome.”Footnote 2 Although the research examined by Coates and Humphreys (2008) was nearly unanimous in finding little benefit from sports franchises, stadiums, and mega-events, research on this topic has continued. Among the new approaches are the use of granular data measured near where the events are held and in ever-shorter time periods. Examples include analyzing monthly sales tax revenues (Baade and Matheson 2001; Baade et al. 2008; and Coates and Depken 2011), daily airline passenger traffic (Baumann et al. 2009; and Baumann and Matheson 2016), daily crime rates (Billings and Depken 2011), and daily restaurant revenue (Depken and Fore 2020). Hotel occupancy data have also been analyzed to measure events’ economic benefits. Since the local economic impact of hosting a large event consists primarily of increased spending by visitors from outside the host city or region,Footnote 3 hotel occupancy data are well suited for analyzing the ability of various events to attract visitors. Lavoie and Rodriguez (2005) analyze monthly hotel occupancy data from eight large Canadian cities and find little evidence that NBA and NHL franchises attract large numbers of overnight visitors. More recently, daily hotel occupancy data have become available and have led to several papers examining sports or other events.Footnote 4 Collins and Stephenson (2016) analyze the effect of the National Association of Intercollegiate Athletics (NAIA) college football national championship on Rome, Georgia. They find a small positive economic impact of the event, including some evidence of guests arriving a few days before the game day, but little evidence of guests staying after the game day. Depken and Stephenson (2018) use daily hotel occupancy data to examine the effects of NFL games, NBA games, NASCAR races, several college sports competitions, and major conventions in Charlotte, North Carolina. They show that effects vary considerably across events, with relatively large effects associated with NASCAR races and a regional college basketball tournament, modest effects arising from NFL games, no effect resulting from NBA games, and substantial effects associated with conventions. The lead and lag variables included in their analysis provide little evidence of intertemporal spillover benefits for most events studied. Similarly, Heller et al. (2018) use daily hotel occupancy data to analyze the four national political party conventions held in 2008 and 2012. Host cities often claim large economic benefits from hosting political conventions. Heller et al. (2018) do find that conventions produce substantial bumps in hotel occupancy, but they point out that the effects are considerably smaller than host cities often claim. Using leads and lags before and after political conventions, Heller et al. (2018) also identify a “hangover effect” of reduced hotel occupancy immediately following political conventions. Such effects might be caused by the need to clean an arena or convention center after an event and should be incorporated into the overall assessment of economic impact associated with an event. Chikish et al. (2019) investigate the effect of basketball and hockey events held in the Staples Center in Los Angeles on local daily hotel demand. They find that the impact of events is highly concentrated around the Staples Center. Since hotels in the immediate vicinity of the Staples Center are exempt from hotel occupancy taxes, Los Angeles is forgoing substantial hotel-occupancy tax revenue that would be generated by professional basketball and hockey games. Using a similar approach to Depken and Stephenson (2018), Heller and Stephenson (2021) examine the Super Bowl’s effects on daily hotel occupancy in four host cities. Their key findings include that the effect varies across cities depending on “normal” hotel occupancy during the time of the year that the Super Bowl is played and that the Super Bowl tourist inflow may go to other parts of a region than the host city. Daily hotel occupancy data offer several advantages over monthly hotel occupancy data. First, daily data can better control for both time of year and day of week effects, thereby allowing the estimation of net rather than gross effects associated with the sports events. Obtaining the net effect requires capturing normal time of year and day of week occupancy rates to control for the crowding out of normal hotel occupancy (Porter 1999). Second, as indicated by descriptions of previous research using daily hotel occupancy data, daily data can incorporate variables for the days leading up to or following an event to better capture the duration of visitors’ stays. Event promoters sometimes suggest that hosting a large event such as the Super Bowl entices visitors to arrive several days before the event or stay several days after the event; the use of leads and lags can capture such effects. Third, daily data are cleaner than using month dummies when studying events such as World Cup tournaments, which normally span multiple months. Fourth, daily data might be able to detect small effects that would be lost in monthly data. For example, consider a one-day event that generates 2000 additional room nights. If the host city averages 20,000 rooms let per night, the 2000-room increase might be detectable using daily data because it would reflect a 10% increase in rooms let. However, the same 2,000-room night increase would be difficult to detect using monthly data since the city would average approximately 600,000 room nights per month (= 30 * 20,000). In this paper, we further refine recent research by examining the effect of various sporting events on the different tiers of hotels (e.g., economy vs. luxury).Footnote 5 Understanding whether events have heterogeneous effects across hotel types might be important for tourist bureaus attempting to attract the best events for their city. Knowing that an event tends to fill more hotels that are expensive could be valuable information when considering the costs and benefits of hosting an event. One reason is that hotel taxes are ad valorem, that is, assessed as a percentage of the nightly room rate. Thus, knowing which events increase occupancy at more expensive hotels provides information about which events generate the largest increase in hotel tax receipts. Another reason is that willingness to spend on hotels is positively correlated with willingness to spend on other things like bar tabs, restaurant meals, souvenirs, etc. (Marcussen 2011). Hence, events that yield more room rentals in high-end hotels likely also generate larger increases in visitor spending on other goods and services.",
48,4,Eastern Economic Journal,27 June 2022,https://link.springer.com/article/10.1057/s41302-022-00221-2,An Agent-Based Macroeconomic Model with Endogenous Intertemporal Decision Rules,October 2022,Adalbert Mayer,,,Male,Unknown,Unknown,Male,"Agent-based macroeconomics addresses important shortcomings of the standard Dynamic Stochastic General Equilibrium (DSGE) framework. One drawback of agent-based macroeconomic models is that they are usually subject to the Lucas critique.The behavioral rules of agents are determined exogenously, and agents use these rules to adjust their behavior to an altered economic environment. This paper presents an agent-based macroeconomic model with endogenous intertemporal decision rules, obtained through an evolutionary algorithm. Forward looking expectations are formed though the adaptation of decision rules to a repeating cycle of identical exogenous economic conditions. Like standard dynamic macroeconomic models, this framework uses utility optimization to generate micro-foundations for intertemporal decisions; and thereby can address the Lucas critique. A special case approximates a stochastic representative agent growth model, and it can replicate the corresponding rational-expectations intertemporal decision rules. At the same time, it offers the flexibility to model expectations formation with alternative information sets, making it possible to isolate effects of limited information or limited ability to process information. In addition, this agent-based model allows for explicit modeling of individual interaction, offering micro-foundations for market interactions and explicit consideration of agent heterogeneity. It can generate features of other agent-based macroeconomic models, such as, endogenous heterogeneity of agent specific variables, long-run money neutrality, and short-run real effects of monetary shocks. Combining endogenous intertemporal decision rules and explicit modeling of individual interaction bridges the gap between standard DSGE models and agent-based modeling. It illustrates that macroeconomic dynamics in response to a shock depend on the interplay of expectations formation and the micro-structure of the interactions between individual agents. DSGE models have emerged as the standard approach to think about macroeconomics after the 1970s. These models address the Lucas critique,Footnote 1 offer a rigorous internally consistent framework, and provide micro-foundations for some aspects of economic decision making. However, this framework has received scrutiny that has intensified in the wake of the great recession (for example Romer 2016). Common criticisms include the reliance on a representative agent,Footnote 2 the assumption that markets clearFootnote 3, and the use of rational expectations.Footnote 4 Agent-based macroeconomics describes economies as complex evolving systems with heterogeneous agents, limited rationality, and out of equilibrium interactions.Footnote 5 Dawid and Gatti (2018) provide an overview over agent-based macroeconomic models. They distinguish between larger-scale models with workers, firms, and a financial sectorFootnote 6 and smaller-scale models with two types of agents: workers and entrepreneurs.Footnote 7 The model presented here is a smaller-scale model. The approach of expectations formations presented here could be adapted for larger-scale models. While many agent-based macroeconomic models use exogenous intertemporal decision rules, concerns related to the Lucas critique, expectations formation, and adaptations of agent behavior to a changed economic environment have been recognized by agent-based macroeconomic modelers. Dosi et al (2020) analyze heterogenous expectations formation and action rules in an agent-based macroeconomic model with deep uncertainty. They consider heuristic rules of expectation formation and sophisticated approaches based on a type of learning process that can lead to rational expectations as in Evans and Honkapohja (1999). They find that using heuristic approaches instead of more sophisticated approaches of expectations formation may be individually rational, leading to smaller forecast errors and reduce macroeconomic volatility. Ashraf et al. (2016) address the Lucas critique by identifying the key parameter for their results, in their case the markup of price setting firms. Then they allow for adaptation of this parameter through a “survival of the fittest” approach, conceptually related to the evolutionary approach used here. Reissl (2020) introduces households that optimize their intertemporal consumption decision based on an adaptive estimate of their future budget constraint. In Salle (2015), households use an artificial neural network to form inflation expectations, leading to endogenous adaptation of expectations. Seppecher et al. (2018) use a selection process to endogenously determine the markup of firms in an agent-based macroeconomic model. Seppecher et al. (2019) use a similar process to endogenously determine debt strategies of firms. They contrast the resulting dynamics of individual and macro-behavior and show that this process can lead to substantial price fluctuations and magnify macroeconomic volatility. This paper is related to these approaches: agents adapt their decision rules to a changing economic environment to maximize an objective (here utility). However, this paper does not consider adaptive adjustment to a changing environment, but uses adaptation to repeated cycles of identical economic environments to generate forward looking expectations. Therefore, it is possible to approximate a rational expectations equilibrium with a special case of the agent-based model. Examples of using evolutionary algorithms to examine expectations formation in macroeconomic models include: Arifovic (1994) who examines evolutionary algorithms and rational expectations in a Cobweb model. Bullard and Duffy (1998, 1999), and Arifovic (1995) use evolutionary algorithms to model the formation of price expectations.Footnote 8 Evolutionary algorithms have been used to solve the representative agent stochastic growth model. Duffy and McNelis (2001) use an evolutionary algorithm to solve the model by exploiting intertemporal first-order conditions, they use it to replace other optimization tools. Sirakaya et al. (2006) use a simulation process similar to the one here, with the explicit optimization of lifetime utility, as a tool to find the rational expectations equilibrium in a representative agent growth model. The framework here builds on a simple agent-based macroeconomic model with two kinds of agents: entrepreneurs and workers. Entrepreneurs experience exogeneous productivity shocks, hire workers and invest in capital to produce one type of good. Similar to other agent-based models (for example Lengnick 2013), the resulting economy exhibits heterogeneity in agent-specific variables, even when agents are ex ante identical (the workers), or skewed distributions when the underlying difference in agents’ characteristics is not skewed (entrepreneur productivity); money is neutral in the long run, but changes in money supply affect real variables in the short run. The innovation of this framework is the modeling of intertemporal decision making. Workers make one decision: how to split their cash holdings (earned as wages) between purchasing consumption goods and reserves for future purchases. Entrepreneurs make two decisions: how to split their cash (earned by selling their output) between purchasing capital, purchasing goods for consumption, and reserves for future purchases. In standard agent-based models, such decisions are usually based on exogenously given rules (potentially functions of endogenously determined variables). Here, the decision rules are determined endogenously through an evolutionary algorithm. To keep the process transparent and due to computational constraints, I use simple decision rules: decision rules that are determined by a single parameter, or linear functions of a limited set of state variables. I start with random intertemporal decision rules and simulate the agent-based economy for a number of time periods (one generation). This results in a sequence of consumption levels and associated lifetime utilities for each agent. These lifetime utilities are used as the objective criterion in an evolutionary algorithm that updates the decision rules of agents. This process is repeated for many generations. Because each generation of agents faces the same economic environment, adaptation based on the outcomes of past generations leads to decision rules that mimic forward-looking optimization within each generation. A special case of the agent-based model makes it possible to compare the arising intertemporal decision rules to those in a standard representative agent growth model. I model an economy with only entrepreneurs, who produce using capital and their own labor, sell and buy the good for cash, and make the decisions to invest in capital, consume, or to hold cash for the future. This is equivalent to a cash in advance (CIA) economy with ex-ante identical agents. When the goods market clears and exogeneous shocks occur prior to the decisions, the optimal cash holding in a CIA economy is zero,Footnote 9 and the model approximates a representative agent growth model. For certain parameter values, the representative agent model with rational expectations has a closed-form solution. The agent-based model with the evolutionary algorithm used here is able to approximate this solution. While the endogenously determined decision rules can approximate the rational expectations solution, they can also be specified to reflect different (limited or wrong) sets of information. I simulate the dynamics of the quasi-representative agent model with a productivity or monetary shock for different sets of information used to form the intertemporal decisions. I show that the interplay between the information used to form the decision rules and the structure of the economy leads to distinctly different responses to shocks. For example, (the anticipation of) a monetary shock leads to a temporary decrease in capital levels when markets clear and to a temporary increase in capital levels when frictions in the goods market are explicitly modeled. This complex interplay between expectations and the structure of the economy is even more important when considering dynamics for the full agent-based macroeconomic model with entrepreneurs and workers. The responses to productivity or monetary shocks differ substantially when the market structure, or the information sets used to form the intertemporal decision rules are altered. The rest of the paper is organized as follows: First, I describe the agent-based macroeconomic model and the formation of the intertemporal decision rules through an evolutionary algorithm. Then, I examine the special case with only entrepreneurs and without workers, that approximates the representative agent growth model. Finally, I examine the full agent-based macro-model with endogenous intertemporal decision rules.",
48,4,Eastern Economic Journal,09 July 2022,https://link.springer.com/article/10.1057/s41302-022-00223-0,University Presidential Searches: An Empirical Examination of Internal Versus External Hiring,October 2022,James Monks,,,Male,Unknown,Unknown,Male,"Succession planning and internal grooming of mid-level management for promotion and elevation within an organization is commonplace in the for-profit corporate sector, yet is rather rare within non-profit higher education. For example, in an analysis of 2204 CEO successions from 1979–2014 across 1441 publicly traded US corporations, Shi and de Jong (2019) found that 61 percent were hired from within the corporation. Similarly, Khurana (2001) notes that over two-thirds of new CEOs of large, publicly held corporations are hired internally. On the other hand, I find that approximately one quarter of the presidents surveyed in the American Council on Education’s (ACE) 2017 American College President’s Survey reported that their immediate prior position was within the same institution. This result is remarkably consistent across institutional control. Twenty-five percent of public university presidents were hired internally, and 26 percent of private university presidents were hired from within the same institution. The practice of usually hiring external from one’s own institution is also remarkably persistent over time. Data from the ACE’s 2006 American College President’s Survey reveals that 26 percent of non-interim presidents were hired internally. It may be the case that there is strong preference among colleges and universities, and their search committees, for hiring a president with previous presidential experience. If this is the case, it is nearly impossible to hire from within one’s institution; instead, universities must lure a candidate from another institution to their corner office. This does not appear to be the case. My calculations from the same 2006 ACE survey reveals that 69 percent of four-year, non-profit universities had hired a first-time president, and only 27 percent of these first-time presidents were hired from within the same institution. Clearly, universities are willing to hire first-time presidents, but when doing so still reveal a strong preference for external candidates. As noted above, this is neither a new nor unrecognized phenomenon. In fact, in 2010, the Association of Governing Boards of Universities and Colleges (AGB) published a monograph advocating for greater levels of succession planning within higher education. In this monograph, they lament the lack of serious consideration often given to internal candidates, who have the potential to make outstanding university leaders. Rita Bornstein, the author of the AGB monograph and former Rollins College president, wrote, “Familiarity often works against internal candidates, who might be too well known by their peers to be viewed as charismatic or visionary, while distance enhances the allure of outsiders.”Footnote 1 Search consultant, Matthew Tzuker, points out, in reference to search committees that “everyone thinks there is a savior out there somewhere. There isn’t.” This perception is remarkably consistent with the opinion of Harvard Business School Professor Rakesh Khurana who reports in regards to corporate searches a “preference for glamorous external candidates over qualified insiders” and “insider candidates are routinely dismissed as unequal to the role of corporate savior or …’change agent’.”Footnote 2 This perception that external candidates have a panache and allure that internal candidates lack is not unique to higher education, as Khurana (2002) points out above. A fundamental difference between non-profit higher education and the corporate sector is the practice of faculty (employee) representation on senior-level search committees and of shared governance within higher education. Shared governance is defined by the AGB as “the process by which various constituents (traditionally governing boards, senior administrators, and faculty; possibly also staff, students, or others) contribute to decision making related to college or university policy and procedure.”Footnote 3 Gary Olson points out that shared governance is commonly understood to “connote two complementary and sometimes overlapping concepts: giving various groups of people a share in key decision-making processes, often through elected representation; and allowing certain groups to exercise primary responsibility for specific areas of decision making.”Footnote 4 It is the former of these realms, shared decision making through representation, that is most applicable to the process of selecting a new university president. It is common practice to have faculty representation on the presidential search committee in the USA. For example, 93 percent of institutions in a survey of university governance report faculty representation on presidential search committees (see Table 2). Through this process, the faculty have at least a representative voice and influence in the selection of the next president. Bornstein (2010) notes that, “In the academy, the concept of shared governance may be seen as antithetical to corporate-style succession planning. Faculty, alumni, and students all expect to have a voice in the selection of a new president.”Footnote 5 Invariably, an administrator who has been at a university for some length of time may have had to make some tough and unpopular decisions. In the process of making these decisions, this administrator may have “stepped on some toes” and angered some of these constituents, who then have voice and influence in the selection of the next president. Former University of Richmond president Richard Morrill notes that: It is typically hard for an inside person to generate wide approval across a whole faculty. Top academic administrators are usually the prime candidates, and they often have to make decisions that may for good reason favor one or several sets of programs, and the decisions come to be perceived negatively by those who do not fare as well in the quest for resources and recognition. Faculty who get involved in presidential search committees may hear from colleagues that an inside candidate does not have the interests of their school or field as a priority, or has made other unpopular decisions, and those views can discourage an inside person’s chances. Clark (2017) reports that the pipeline of talent for university presidents in the USA is “running dry,” as current presidents are getting older, serving fewer years in office, and surveys of provosts, the traditional pathway to a US presidency, find fewer are interested in becoming president. As presidential tenures shorten, the perceived need to launch a national search every time a president departs may impede institutional momentum. A DDI/Conference Board (2014) report concludes that it takes up to two years for externally hired candidates to catch up to those hired internally, in the corporate sector. In an environment where institutions are struggling to find qualified leadership, an implicit bias against internal candidates may prevent many institutions from hiring the best candidate for the presidency, if that candidate is already on the payroll. Additionally, preference for external candidates may prevent many highly qualified candidates from becoming a university president, if it requires them to leave their current institution and relocate to another campus. Despite the widely held preference for external candidates for university leadership little empirical analysis has been done on this topic.",
48,4,Eastern Economic Journal,20 July 2022,https://link.springer.com/article/10.1057/s41302-022-00222-1,The Roundness of Antiquity Valuations from Auction Houses and Sales,October 2022,Melissa Boyle,Justin Svec,,Female,Male,Unknown,Mix,,
49,1,Eastern Economic Journal,31 October 2022,https://link.springer.com/article/10.1057/s41302-022-00234-x,Introduction to the Symposium on Teaching Economics: Innovations in Times of Uncertainty,January 2023,Cynthia Bansak,Julie K. Smith,,Female,Female,Unknown,Female,,
49,1,Eastern Economic Journal,27 October 2022,https://link.springer.com/article/10.1057/s41302-022-00229-8,Econ-assessments.org: Automated Assessment of Economics Skills,January 2023,Douglas McKee,Steven Zhu,George Orlov,Male,Male,Male,Male,"High quality standard assessments of student knowledge and skills can be incredibly valuable to instructors, researchers, and students. A standard assessment allows instructors to measure student learning in their course across semesters to estimate the impact of any teaching changes they may have implemented. Instructors can look at average achievement, gaps between demographic groups, or performance on individual learning goals. They can also use assessments at the beginning of the term to see how well-prepared their students are for the course and to identify students that might benefit from extra attention. Standard assessments have become popular in other STEM disciplines as a tool for researchers to measure the impact of pedagogical innovations on student learning. At the time of writing, the Discipline-Based Science Education Research Center (DB-SERC) at the University of Pittsburgh lists 58 such assessments for Physics, 33 for Biology, 28 for Chemistry, 14 for Computer Science, and 5 for Mathematics. These assessments have enabled great progress in discipline-based education research. For example, researchers have used standard assessments to provide classroom-based evidence for the effectiveness of active learning (Freeman et al. 2014), peer instruction (Crouch and Mazur 2001), and the determinants of race and gender-based performance gaps (Salehi et al. 2019; Bottan et al. 2022). This research in turn greatly benefits students, as instructors make use of new evidence-based practices to improve student performance, reduce achievement gaps, and potentially increase diversity of the undergraduate economics student population. Until very recently, the only standard assessment appropriate for undergraduate economics courses was the Test of Understanding College Economics (TUCE) which tests introductory economics knowledge (Walstad and Rebeck 2008). The first version of the TUCE was published in 1967, but it has been revised several times since then, and the latest revision was released in 2007. The TUCE includes questions based on content commonly taught in introductory microeconomics and macroeconomics courses. Economics instructors have used the TUCE informally to measure learning in their classrooms and more formally to publish research on a variety of teaching innovations (e.g., Becker and Powers 2001; Coates et al. 2004; Emerson and Taylor 2004). We believe our new assessments and our automated system can enable even more instructors to carefully evaluate their teaching and answer important education research questions. In the next section, we provide an overview of the Cornell Suite of Economics Assessments, which currently includes tests appropriate for several commonly taught undergraduate courses. We then describe econ-assessments.org, a website with a fully automated system that allows instructors to give online versions of assessments to their students and generates detailed performance reports. We give advice on how instructors and researchers can best integrate these assessments into their courses in the following section, and we conclude by describing our future plans to extend econ-assessments.org with additional assessments and functionality.",
49,1,Eastern Economic Journal,27 October 2022,https://link.springer.com/article/10.1057/s41302-022-00232-z,The Effectiveness of Logical Distractors in an Online Module,January 2023,Diego Mendez-Carbajo,,,Male,Unknown,Unknown,Male,"The use of multiple-choice questions for purposes of assessing student learning is widely endorsed by the educational research literature across a range of domains and in a variety of settings. Gierl, et al. (2017) provide a concise timeline of the use and assessment of this educational testing technology. Reporting from the sixth national quinquennial survey on teaching and assessment methods in economics, Harter et al. (2022) find that multiple-choice questions remain the dominant base of examinations in introductory classes. In the field of economic education itself, Walstad (1998, 2006) has convincingly advocated for the use of multiple-choice items. They have been found to perform at par with essays in measuring achievement in the College Board’s Advanced Placement exams in economics (Walstad and Becker 2006) as well as in introductory business statistics classes (Lawrence and Singhania 2004). Along those lines, Buckles and Siegfried (2006) have argued for the use of multiple-choice questions to measure some elements of in-depth understanding in Bloom’s (1956) taxonomy. As Schaur et al. (2012) report the frequent use of multiple-choice questions across the broad variety of schools, departments, and instructors they survey, the good design of these items cannot be overemphasized. In fact, a sizeable body of the educational assessment literature has examined the impact of multiple-choice test design on student learning across disciplines. Carlson and Ostrosky (1992), Geiger and Simons (1994), and Vander-Schee (2013) have explored how item sequencing affects test scores in introductory economics, accounting, and marketing, respectively; Hadsell (2009) has studied the impact that test timing has on introductory finance exams; Miller, Asarta, and Schmidt (2019) have researched the strategic use of assignment deadlines in adaptive learning modules; Bresnock, Graves, and White (1989) have studied the implications of changing the response patterns in multiple-choice questions; and Jin, Siu, and Huang (2022) have researched the impact of random guessing in the selection of incorrect answers—also known as distractors. A great deal of the assessment yield obtained from multiple-choice questions is predicated on their proper design. A reader interested in mastering this skill is well served by the comprehensive advice on best practices provided by Miller, Linn, and Gronlund (2012). Probably because high value multiple-choice questions are time-consuming to write, introductory economics textbook test banks are “dominated by recognition and understanding questions” (Buckles and Siegfried 2006, p. 56). More worrisome is the finding by Hansen and Dexter (1997) that 75% of the questions from 10 auditing textbook test banks “had one or more [item-design] guideline violations” (Hansen and Dexter 1997, p. 94). Of particular interest to us are the best practices related to the design of distractors. A distractor is a plausibly incorrect answer to a multiple-choice question. Since the purpose of a distractor is to direct the attention of the uninformed test-taker away from the correct answer, a well-designed distractor can, all else being equal, help discriminate between low-ability and high-ability test-takers. Conversely, a poorly designed distractor is likely to confuse test-takers and thus handicap assessment efforts. In that regard, a particular type of distractors, the low production-cost family of “All of the above,” “None of the above,” or “Both option X and option Y are correct,” is to be used either sparingly or not at all (Miller et al. 2012; Walstad 1998; Frary 1991). Ideally, adding this type of logical distractor is expected to force the student to carefully consider all of the alternatives and increase the difficulty of the test items by decreasing the probability of randomly selecting the correct answer. However, Haladyna, Rodriguez, and Stevens (2019) emphasize that simply adding more distractors to a multiple-choice question does not make it perform better as an assessment instrument. Another source of concern is that the “Both option X and option Y are correct” distractor strictly violates the premise that “[multiple-choice] responses should be independent and mutually exclusive” (Walstad 1998) and avoid “a comparison and selection of alternatives” (Miller et al. 2012). However, in their comprehensive literature review focused on the design of distractors, Gierl, Bulut, Guo, and Zhang (2017) do not address this specific type of logical distractors. The purpose of this article is to study the educational assessment value of the multiple-choice logical distractor “Both option X and option Y are correct.” We are aware that violating, either strongly or weakly, the issue of mutual exclusivity in the design of multiple-choice options should altogether disqualify our proposed use of logical distractors. However, in our experience, the average instructor is not well versed in good test-design practices and we hope to bring those matters to their attention with this work. To the best of our knowledge, the empirical literature has not yet documented the impact that these distractors have on student learning or the discriminatory power they yield to assessments. Moreover, we are not aware of empirical research on its effectiveness in an online learning environment. Our contribution to the scholarship of teaching and learning is also original in two additional dimensions. First, we employ the assessment metric put forward by Attali and Fraenkel (2000) to document the discriminatory effectiveness of this particular type of distractor. Second, we examine the impact that this distractor has on the types of added value learning outlined by Walstad and Wagner (2016). We answer the following questions: Does the addition/omission of this type of distractor change the level of difficulty of a multiple-choice item? Is this type of distractor effective at discriminating between low-ability and high-ability students? Does the addition/omission of this type of distractor change the learning outcomes registered by students?",
49,1,Eastern Economic Journal,04 November 2022,https://link.springer.com/article/10.1057/s41302-022-00228-9,The FDIC Challenge in the Economics Curriculum,January 2023,Léonie Stone,,,Female,Unknown,Unknown,Female,"In the last decades, academic challenges have become increasingly popular in economics departments and business schools. Such competitions are excellent opportunities to engage in active, cooperative, and experiential learning as well as in developing valuable student skills from critical thinking to preparing effective presentations. A recent entry is the FDIC Academic Challenge. As my goal is to explain this competition and how it may be used as a classroom project, I note briefly that there is an extensive literature on cooperative and experiential learning, some specific to the use of academic competitions in economics classesFootnote 1. In this paper, I summarize the details of the FDIC Academic Challenge, highlight particular skills that can be developed, provide direction on progressing through the competition, suggest some methods for integrating the competition with the money and banking curriculum, and discuss student, university, and alumni response.",
49,1,Eastern Economic Journal,21 October 2022,https://link.springer.com/article/10.1057/s41302-022-00226-x,Developing Students’ Analytical and Data Presentation Skills Using Infographic Assignments in Principles of Macroeconomics and Economics of Immigration,January 2023,Eva Dziadula,,,Female,Unknown,Unknown,Female,,
49,1,Eastern Economic Journal,12 December 2022,https://link.springer.com/article/10.1057/s41302-022-00233-y,"An International Exercise to Increase Awareness of How Market, Political, and Cultural Institutions Affect Economic Activity",January 2023,Kathryn L. Combs,Monica E. Hartmann,Joseph L. Kreitzer,Female,Female,Male,Mix,,
49,1,Eastern Economic Journal,02 November 2022,https://link.springer.com/article/10.1057/s41302-022-00231-0,Virtually Nonexistent: Gender and Racial Representation in Online K-12 Economics Lessons,January 2023,Caroline Krafft,Kristine West,Karri Drain,Female,Female,Male,Mix,,
49,1,Eastern Economic Journal,21 October 2022,https://link.springer.com/article/10.1057/s41302-022-00225-y,Rent-Seeking Behavior and Economic Justice: A Classroom Exercise,January 2023,Aaron Pacitti,Michael Cauvel,,Male,Male,Unknown,Male,"Rent-seeking behavior is when individuals or firms acquire above-market returns through the exercise of economic and political power. With the rise of winner-take-all markets and politics (Hacker and Pierson 2011), superstar firms (Autor et al 2020), and a changing institutional structure that favors capital at the expense of labor (Cauvel and Pacitti 2022; Levy and Temin 2007), rent seeking is becoming increasingly central to explaining economic and political dynamics (CORE 2022; Bowles and Carlin 2020). More broadly, understanding the causes and effects of rent-seeking behavior helps fill the gap between economics and politics. The use of economic power—control over rents and affecting a preferred distribution of income—can be used to acquire political power through lobbying and campaign contributions, which leads to favorable policies that further increase economic power. In the run-up to the 2008 financial crisis, this investment in politics helps explain the rise and fall of large financial institutions, which is argued to be the result of a decades-long process of rent-seeking behavior, leading to deregulated markets, in addition to ideological and regulatory capture (Johnson and Kwak 2010). Furthermore, the degree to which financial firms lobbied positively affected the probability of receiving government bailouts (Igan et al. 2011). The Payroll Protection Program, part of the fiscal policy response to the COVID-19 pandemic, was designed to provide forgivable loans to incentivize firms in retaining workers. However, this program was regressive in nature, as the top quintile of households received approximately 75% of its benefits (Autor et al 2022). Given the central importance of rent seeking for understanding modern economies, it is a valuable topic for instructors to introduce to students. However, it can also be difficult to grasp. As such, active learning tools may be a useful way to help students better understand rent-seeking behavior. Active learning methods have a variety of benefits, including increasing student engagement and participation, requiring students to solve problems and think critically about issues, and presenting opportunities for students to collaborate. There is considerable evidence that active learning methods typically lead to better educational outcomes (Dorestani 2005; Freeman et al. 2014; Salemi 2002; Sheridan and Smith 2020; Tila 2021). There has already been interest in developing classroom exercises to help students understand rent seeking. Goeree and Holt (1999) initially developed an exercise in which students act as firms competing to gain a telephone license, or other prize awarded by the government, in a lottery-style system where teams have the opportunity to purchase up to 13 applications. Comparing the total application costs across teams to the value of the license provides a stark illustration of the social cost of rent seeking. Bischoff and Hofmann (2002) suggested an extension in which students fill out an in-game questionnaire to provide insights into their decision-making processes, along with minor tweaks to the parameters of the exercise. Strow and Strow (2006) further built upon this work, suggesting a modified version of the exercise in which a real monetary reward is distributed based on the results of a one-round all-pay auction in which students bid their own money. Hall et al. (2019) summarize the economics education literature on rent seeking, including these three exercises, and treatment of the topic in public economics textbooks. Roush and Johnson (2018) modify the Goeree and Holt (1999) exercise by translating it to the context of recruiting in college athletics and adding a new mechanic in which players can pay bribes, but risk punishment if they do so. Although the existing literature provides a number of exercises to introduce students to the concept of rent seeking, they focus on only one aspect—its inefficiency. We build on the existing literature by arguing that rent seeking also has important implications for equity, economic justice, and democracy. Our objective is to highlight how rent-seeking behavior affects the interactions between firms, markets, and the government, and their joint effects on economic outcomes; and to engage students in discussions about the efficiency of productive and nonproductive activities, as well as the broader issues mentioned above. We view our primary contribution as offering an exercise that can be easily used to introduce students to critical real-world implications of rent seeking across a variety of important outcomes of interest, including, but not limited to, efficiency. Although the exercise was initially developed in line with the approach presented in the textbook by Bowles et al. (2018), it does not require the use of any particular textbook or any specific prior knowledge for students. Although rent seeking can take many forms, some of which may be socially beneficial (such as a patent race), the exercise we present focuses on the use of political power. Students act as corporate decision makers, allocating firms’ resources among physical, financial, and political investment, competing with one another to earn the highest total return over a five-round game. Instructors then use the results to highlight the distinction between firm activities that are socially beneficial—leading to increased efficiency, output growth, and employment—and those that are not productive. The ways in which individual firm incentives may differ from socially optimal firm behavior can also be emphasized. Like the other classroom exercises on rent seeking summarized above, our exercise provides a striking illustration of the inefficiency of rent seeking by highlighting the social opportunity cost. However, our exercise is designed to encourage students to also consider outcomes through lenses of equity and democracy. This provides an opportunity to engage students in discussions of broader issues, such as the ways in which rent seeking can undermine democratic ideals and perpetuate inequality. Moreover, our exercise also shows that certain types of financial activities can have the same opportunity cost as rent seeking, providing a useful entry point for discussion of issues related to shareholder primacy and financialization. Below, we discuss the mechanics of the exercise, along with a menu of discussion questions and potential extensions of the game, which are designed to highlight the social and political ramifications of rent seeking, in addition to efficiency issues.",
49,1,Eastern Economic Journal,21 October 2022,https://link.springer.com/article/10.1057/s41302-022-00227-w,Two Experiments in Firm Choice,January 2023,James Staveley-O’Carroll,,,Male,Unknown,Unknown,Male,"While it is easy for undergraduates to imagine themselves as consumers and workers in an economic system—something they have often already experienced in person—it is more difficult for them to grasp the behavior of an entrepreneur trying to maximize the profits of their firm. This article describes two classroom experiments which, when conducted in back-to-back lectures, allow students to explore optimal firm behavior and connect market structure to pricing strategy first-hand. These experiments are best implemented in a microeconomic principles course when discussing firm choice and market structure; however, the imperfect competition experiment can also be deployed in advanced undergraduate and master’s level courses. The first experiment assumes a perfectly competitive market structure in which firms are price-takers and lays the groundwork for profit-maximizing firm behavior. The second experiment relaxes the assumptions of perfect competition and lets students experiment with pricing strategy. Both of these exercises are considered experiments and not demonstrations, because in a demonstration students are told how to behave and outcomes are consistent across trials, whereas in an experiment students determine their own behavior and outcomes may vary from class to class. Both experiments have been employed in eighteen sections of microeconomic principles taught at Lander University, the College of William and Mary, and Babson College. All classes consisted of 25 to 40 students and met in-person; however, these experiments could easily be extended to larger classes and would work well in online and hybrid courses—hybrid courses involve some students attending class in-person while others attend online. The imperfect competition experiment has also been implemented in four master’s courses which had closer to 50 students and were held in a hybrid class setting.",
49,1,Eastern Economic Journal,28 October 2022,https://link.springer.com/article/10.1057/s41302-022-00230-1,Online Interactive Pedagogical Tools for the Principles of Microeconomics Curriculum,January 2023,Amy Ehinomen Eremionkhale,Mya Eveland,J. Todd Swarthout,Female,Female,Unknown,Female,"A need exists for open education resources (OER) in economics education that assesses the student's understanding of the course material without relying on the professor's technological efficacy (Martin and Kimmons 2019; Petrides et al. 2011). Pursuant to this need, we have created a set of over one hundred OER formative (low-stakes) and summative (higher-stakes) assessments for the introductory microeconomics curriculum. The low-stakes formative assessments help increase the student’s participation levels by giving them the opportunity to practice and solidify their understanding of the course material (Ontong 2021; Stowell 2022; Brooks 2018, Curtis 2011). In addition, empirical evidence shows that low-stakes formative assessments increase performance on the students' future summative assessments (Ontong 2021; Swerdzewski et al. 2009; Dunn and Mulvenon 2009). This advantage is especially true in our case as the formative assessments are tied directly to interactive summative assessment tools. These tools have been used in eleven introductory microeconomics courses beginning in the fall of 2021 and were developed as part of a larger initiative to create a new open education resources (OER) principles of microeconomics course (Eremionkhale et al. 2022). The tools can be fully integrated into a learning management system (LMS) so that grades are automatically recorded into an LMS gradebook. The breadth of coverage and high quality of the tools allows for a substantive replacement of publisher-provided interactive platforms, and the associated cost savings can be passed onto students. Overall student feedback has been extremely positive. Two of the authors currently use these assessment tools within the context of a newly designed college-level microeconomics OER course; however, the tools can be selectively adopted independently from the structure of the course. Economics educators from the high school to collegiate level can explore, evaluate, and adopt the course or specific tools into their own courses at no monetary cost at http://econreimagined.gsu.edu.",
49,2,Eastern Economic Journal,13 March 2023,https://link.springer.com/article/10.1057/s41302-023-00241-6,Social Disparities and Social Distancing During the Covid Pandemic,April 2023,Lewis Davis,Justin Esposito,,Male,Male,Unknown,Male,"In the first year of the novel coronavirus pandemic, prior to the development and distribution of effective vaccines, many countries relied extensively on non-medical interventions to slow the spread of Covid. Social distancing played a central role in the recommendations of public health authorities on addressing the pandemic, and evidence has since emerged that social distancing was, as expected, an effective way to “flatten the curve,” leading to fewer Covid cases, hospitalizations, and deaths (Matrajt and Leung 2020). Despite widespread support for social distancing within the medical community, formal policies designed to encourage social distancing proved to be highly contentious and were, indeed, even subject to legal challenge. For example, the Wisconsin Supreme Court ruled a state stay-at-home order to be both unlawful and unenforceable, overturning the stay-at-home order in May 2020 (Jimenez and LeBlanc 2020). The Wisconsin court case cites two of the major difficulties surrounding social distancing protocols, those being personal values and beliefs for or against social distancing, and the difficulty of enforcement. In addition, as Malone and Bourassa (2020) have shown, social distancing predated stay-at-home orders in nearly every state, indicating that, to a significant degree, social distancing was independent of laws and policies designed to support it. Both the challenges of enforcing policies designed to support social distancing and evidence that social distancing occurred in the absence of these laws makes it important to understand the factors that contributed to voluntary social distancing. An important line of this research examines the role of social capital in supporting social distancing. Social capital can be defined as collective values, norms, bonds, and trust within a group of individuals that support communication and collective action (Bourdieu 1986; Coleman 1988; Putnam 1994). In keeping with this understanding of the functional role of social capital, emerging research finds that social capital supported social distancing during the current pandemic, for example, Bartscher et al. (2020), Durante et al. (2020), and Wu (2020).Footnote 1 These results echo similar findings regarding the role of social cohesion in responding to health emergencies involving Ebola (Blair et al. 2017; Vinck et al. 2019; Carrion Martin et al. 2016; Fallah et al. 2016) and influenza (Chuang et al. 2015; Rönnerstrand 2014). Social capital in any community is closely related to underlying measures of social heterogeneity. Alesina and Ferrara (2000) develop a formal model of group formation in heterogeneous societies,  which predicts greater participation in associational group activities in communities with more homogeneous populations. While they pay explicit attention to income, race and ethnicity, they note (p. 850) that their theory assumes that “individuals prefer to join groups composed of individuals with preferences similar to their own.” As such, their model’s prediction applies to any characteristic that is predictive of individual preferences, including age, gender, religion, racial or ethnic identity, income, and educational attainment. Looking beyond associational groups, however, the role of social disparities in social capital formation is hard to quantify. According to both Putnam (2000) and Bourdieu (1986), social disparities tend to correlate with forms of social capital that may serve to undermine social cohesion and the shared understandings and values that are the basis for broad-based collective action. In Putnam’s (2000) theory, social disparities may favor the formation of bonding, which reinforces social distinctions, rather than bridging social capital. Similarly, according to Bourdieu (1986), the individual dispositions that underly social capital are created and replicated among those with similar social backgrounds, and as a result, the bonds of social capital tend to reproduce and reinforce existing class structures. Moreover, existing social capital measures, such those developed by Putnam (1993), Guiso et al. (2004), and Rupasingha et al. (2006), do not directly address the role of social disparities and, thus, may not adequately capture the distinction between bridging and bonding social capital. Given the difficulty in fully accounting for the role of social disparities in certain dimensions of social capital, including the degree to which social groups bridge important social cleavages, we focus instead on the role of social disparities in social distancing. In doing so, we add to a large and diverse literature that finds that socioeconomic inequalities and racial diversity have important consequences for social cohesion (Rothstein and Uslaner 2005; Khambule and Siswana 2017; Taylor 1998), trust (Rothstein and Uslaner 2005; Alesina and Ferrara 2000; Costa and Kahn 2003), and the provision of public goods (La Porta et al. 2007). In particular, social disparities may be understood as undermining social cohesion and the voluntary provision of public goods. Applying this insight to social distancing under Covid, we expect regions characterize by greater social disparities to social distance less. We test this theory using a weekly panel of US county-level data. Social distancing is measured using cellphone mobility data, as in Ding et al. (2020). We consider three measures of social disparity, reflecting underlying differences in income, educational attainment, and racial identity and constructed from data in the 2019 American Community Survey. All specifications include a broad array of social, demographic and economic controls, and our key findings are robust to the inclusion of the county-level contemporaneous Covid infection rate and a commonly used measure of social capital. Our key finding is that all three measures of social disparity are significantly and negatively related to social distancing. This result is robust to the inclusion of a wide variety of economic, social and demographic and political controls, as well as to controls for other common measures of social capital. These variables are also economically significant. One-standard deviation increases in income, educational and racial disparities are associated with decreases in social distancing of 2.9%, 5.0% and 12.4% of a standard deviation, respectively. By comparison, a one-standard deviation increase in average income is associated with a 10.0% decrease in social distancing. The importance of social disparities is robust to the inclusion of a prominent measure of social capital. This suggests that social capital measures may not fully reflect the impact of social disparities on social cohesion. Our research contributes to existing research on social distancing in two distinct ways. First, we directly examine the relationship between social disparities and social distancing. In doing so, our work complements and extends existing work on the determinants of social distancing during the Covid pandemic and highlights the role of important dimensions of social structure that have not been previously addressed. To date, Egorov et al. (2021) is the only paper to address the role of social disparities on social distancing, and they focus exclusively on the role of racial diversity. Second, given the substantial difficulties in enforcing stay-at-home orders and related policies, our findings may also inform work on the efficacy of formal institutions designed to induce social distancing, such as Murray (2021). Second, our findings suggest the limits of existing approaches to measuring social capital (Putnam 1993; Guiso et al. 2004; Rupasingha et al. 2006), which are based primarily on prosocial behavior, such as voting, newspaper circulation, and blood donation, and the number of collective associations. In particular, these measures do not properly account for the role of social disparities in social capital formation and, thus, fail to adequately capture the distinction between bonding and bridging social capital. A measure that more fully reflected this distinction might better account for a community’s ability to respond collectively to a crisis.",
49,2,Eastern Economic Journal,22 March 2023,https://link.springer.com/article/10.1057/s41302-023-00243-4,The Local and Aggregated Impacts of Stay-at-Home Orders on State Level Unemployment Outcomes,April 2023,Allen Bellas,Lea-Rachel Kosnik,,Male,Unknown,Unknown,Male,"When COVID-19 hit the USA in early 2020, there was debate about the proper policy response to managing it (Greenstone and Nigam 2020; Thunstrom et al. 2020). By early April, however, the majority of states in the USA had decided to implement statewide stay-at-home orders (SAHO) in an effort to reduce infection rates and tame the pandemic. Such a policy response was not without projected costs, however, a primary one being the effect on business and employment. Any given state-level SAHO would be expected to affect that state’s economy, of course, but in an interconnected national economy like the USA, one would suspect that its impact would also be felt beyond a single state’s borders, as both bilateral trade and supply chains are impacted. For example, in April 2020 meatpacking plants in a few states were closed due to COVID concerns, and these plant closures affected downstream meat-related industries across the country (USDA Economic Research Service 2021). Similarly, many states are dependent on inter-state trade for basic energy needs. Missouri, for example, imports the majority of its coal from Wyoming, and California imports a third of its electricity from nearby Pacific Northwest states. This paper investigates the effect of SAHOs not just within a state, but beyond its border to other states as well. There is also the question as to whether SAHOs impacted different measures of unemployment differently, for example, lifting initial claims but not continuing claims or the broader unemployment rate in the same manner. By investigating the effect of SAHOs on different measures of unemployment – initial claims, continuing claims, and the overall unemployment rate – we seek an understanding of the nuanced effect of SAHOs on state-level unemployment measures. In particular, the CARES Act created an employee retention credit that incentivized employers to continue paying current employees who might otherwise have been laid off due to the COVID-19 outbreak, a policy likely to reduce initial claims for unemployment as current employees are retained, while perhaps reducing employment prospects for previously unemployed workers. The USA entered uncharted territory in 2020 when responding to the coronavirus pandemic with state-level SAHOs. While the initial SAHOs were all lifted by the end of May, 2020 (see Table 1), debate continues as to their ultimate effect on the economy and joblessness. This paper asks: What effect did early 2020 SAHOs have, both directly on an issuing state, and aggregated across the country at the national level, on unemployment rates and unemployment insurance claims throughout the USA?",
49,2,Eastern Economic Journal,18 July 2022,https://link.springer.com/article/10.1057/s41302-022-00224-z,Shadow Banks and the Collateral Multiplier,April 2023,Thomas Michl,Hyun Woong Park,,Male,,Unknown,Mix,,
49,2,Eastern Economic Journal,03 February 2023,https://link.springer.com/article/10.1057/s41302-023-00236-3,Unbundling For-Profit Higher Education: Relaxing the 90/10 Revenue Constraint,April 2023,Zachary G. Davis,,,Male,Unknown,Unknown,Male,"For-profit postsecondary institutions have become a large player in higher education in the past decade. Deming et al. (2012) and Gilpin et al. (2015) document and explain the large growth in the for-profit sector. Along with the growth in the for-profit sector, there has also been growth in scrutiny and regulation of the for-profit sector. One regulation, the 90/10 rule, has applied to for-profit postsecondary institutions in some form since 1992. Under the 90/10 rule, a for-profit school cannot receive more than 90 percent of their revenue from Title IV funds. Title IV funds are federal aid dollars disbursed by the Department of Education. For-profit institutions are allowed to bundle separate campuses together as one entity in order to comply with the 90/10 rule. In 2008, congress passed a reauthorization of the Higher Education Act that relaxes the 90/10 violation policy on for-profit institutions. The policy change allows schools to violate the rule 2 years in a row instead of 1 year before losing eligibility for Federal Title IV aid. Executives at for-profit corporations actively considered bundling campuses when deciding on 90/10 rule compliance strategies. According to a 2012 Senate report from the Health, Education, Labor and Pensions (HELP) Committee, for-profit institutions use many different strategies, including bundling campuses, to comply with the 90/10 rule. An executive at Herzing UniversityFootnote 1 wrote in an email in 2009: “My initial thought is to match Toledo with Omaha because they are smaller enterprises and that way we can reserve Minneapolis for Akron if necessary. Right now the Toledo/Omaha rate would be . . . 72.6% . . . Right now Akron/Minneapolis would be . . . 78.5%. This group could in theory go up to the $20,000,000.00 mark in combined revenue, with the current cash and still be under the 90% threshold.” The Senate HELP Committee also documents that “EDMC discussed internally a consolidation and reorganization of its campuses in late 2009 in part to address concerns with 90/10 issues at some campuses.”Footnote 2 Executives at for-profit institutions base organizational decisions on the 90/10 rate at their various campuses. I examine the impact of relaxing the 90/10 rule violation policy on the behavior of for-profit institutions and estimate the impact of the rule change on the amount of federal student aid received by for-profit institutions. I develop a theoretical model in which universities consider both the size of their campuses and the Title IV revenue percentages when making campus bundling decisions. I use the theoretical model to simulate the effect of moving from a 2-year violation rule to a 1-year violation rule. I also use the model to simulate the effect of moving to an 85/15 rule instead of a 90/10 rule. Using the Integrated Postsecondary Education Data System (IPEDS) and data from the Office of Postsecondary Education’s eZ-Audit system, I estimate the 90/10 revenue percentage for each campus of a for-profit institution. I show that relaxing the violation policy is associated with larger campus bundles and more revenue in for-profit institutions. I also estimate that relaxing the 90/10 rule violation policy is associated with an extra 900 million dollars in federal aid going to for-profit institutions. The for-profit education sector is increasingly important to understand as they become a larger player in postsecondary education. For-profit postsecondary growth is driven by a number of factors. In one of the earliest papers studying for-profit institution growth, Cellini (2009) finds that 2-year for-profit institutions are a substitute for non-profit community colleges. She found that local communities in California voting to fund a public community college decreased the number of for-profit institutions in the market, as well as private college enrollment, while increasing public college enrollment. In her 2010 paper, Cellini finds that increases in the Pell and Cal grant programs increaseFootnote 3 the number of public and for-profit institutions, though the increase in for-profit institutions is larger. Gilpin et al. (2015) find that occupation growth in the fields for-profit institutions offer explains some of the growth in for-profit sector. No matter the causes of the for-profit sector growth, a larger number of for-profit institutions mean more Title IV aid is directed to the for-profit sector, and any Title IV eligibility change will have a larger impact on both the students and the institutions. Understanding the regulations on the higher education industry is crucial to understanding how these regulations affect the for-profit sector. These regulations include the different types of Title IV aid and the eligibility requirements to receive this aid. Losing Title IV eligibility requirements can cause institutions to close, which happened to ITT Tech and Corinthian for-profit institutions in 2016. Eligibility requirements differ between for-profit institutions and non-profit institutions. The 90/10 rule is an eligibility requirement only applied to for-profit institutions. It is meant to ensure that at least some students value the education at the for-profit institution enough to be willing to pay for it out of pocket. Also, universities are allowed to bundle together campuses when submitting compliance reports to the Department of Education. Because of the 90/10 rule, for-profit institutions have a strong incentive to bundle campuses together when submitting compliance reports. The Higher Education Act of 1965, under Title IV, created a number of student aid programs administered by the Department of Education. Title IV aid is awarded to students as grants, loans, or work study programs either directly from the Department of Education or indirectly as campus-based aid from the student’s institution. Title IV aid distributed by the college is called campus-based aid. Students are not required to repay grants, but they are required to repay loans with interest. Title IV grants include Pell grants, Federal Supplemental Education Opportunity (FSEO) grants, Teacher Education Assistance for College and Higher Education (TEACH) grants, and the Iraq/Afghan Service grant.Footnote 4 Title IV loans included Direct Subsidized and Unsubsidized loans, Stafford Subsidized and Unsubsidized loans, Direct PLUS loans to parents or graduate students, and Federal Perkins loans. Perkins loans and FSEO grants are campus-based aid, while the rest of the grant and loan programs fund the student directly. Title IV work study programs are also campus-based aid. Schools are eligible to receive Title IV aid as long as they meet certain requirements. For a for-profit institution, those requirements include offering programs that prepare students for gainful employment, accreditation by a recognized accrediting agency, operating for at least 2 years, and satisfying the 90/10 constraint. Cellini and Goldin (2014) gather data on Title IV ineligible for-profit institutions. According to them, over half of all for-profit institutions are ineligible for Title IV aid, and Title IV eligible for-profit institutions charge 78 percent more for tuition than Title IV ineligible institutions. Cellini and Goldin are not able to identify whether the higher tuition accounts for higher costs associated with attaining and maintaining eligibility or is used to capture some of the Title IV aid by the for-profit institution. Attaining and maintaining Title IV eligibility possibly increases the costs of operating a for-profit institution, so any change in the eligibility requirements may incentivize for-profit institutions to change their tuition setting or bundling behavior. In the Higher Education Amendments of 1992, congress implemented the 85/15 rule to restrict the amount of federal funds for-profit postsecondary institutions could receive. The rule applied only to for-profit institutions and restricts them from earning more than 85 percent of their revenue from federal Title IV student aid. The rule is similar to a rule implemented by the Department of Veterans Affairs (VA), which states that not more than 85 percent of a program’s students may receive benefits from the VA.Footnote 5 While these 2 rules are similar, the 85/15 rule in the Higher Education Amendment of 1992 applies to revenue, while the VA’s rule applies to the number of students in a program. The 85/15 rule was implemented to ensure federal dollars were going to a reputable program. Legislators thought that if at least fifteen percent of students were willing to pay out of pocket,Footnote 6 then the program is valued enough to support with federal aid. The Higher Education Amendments of 1998 were more lenient to for-profit institutions, relaxing the 85/15 rule to the 90/10 rule. The 90/10 rule still applies only to for-profit institutions and restricts them from receiving more than 90 percent of their revenue from Title IV federal student aid. If the school violates the 90/10 rule for 1 year, it becomes provisionally certified. If the school is caught violating the 90/10 rule for 2 years in a row, the school loses Title IV eligibility. To regain eligibility, the school has to meet licensing, accreditation, and financial responsibility requirements for 2 years. Eight for-profit institutions have lost Title IV eligibility from 2008 to 2018 due to violating the 90/10 rule.Footnote 7 Calculating the 90/10 revenue percentage is rather complex and allows for-profit institutions to employ various strategies to satisfy the constraint. In general, aid disbursed by the Department of Education is considered Title IV aid, though there are exceptions. The Department of Education disburses both subsidized and unsubsidized loans, but only subsidized loans and a portion of unsubsidized loans count as Title IV aid. Also, federal aid to veterans and active military are not Title IV aid and does not count toward Title IV revenue, incentivizing for-profit institutions to recruit students eligible for veteran and military benefits.Footnote 8 While for-profit institutions have many strategiesFootnote 9 to comply with the 90/10 rule, I focus on how changes to the 90/10 rule in 2008 affect campus bundling behavior at for-profit institutions. Bundling describes how a for-profit institution with many campuses combines different subsets of those campuses. Each subset, or bundle, submits its own financial statements that determine its 90/10 revenue percentage. Each campus at every postsecondary institution that receives Title IV funds is issued a unique numeric identifier called the “unitid” in the Integrated Postsecondary Education Data System (IPEDS) by the National Center for Education Statistics (NCES). Data collected by the NCES are not used to determine regulatory compliance. The Office of Postsecondary Education (OPE)Footnote 10 issues a single numeric ID for each entity that receives Title IV funds, called an OPEID, in order to verify institutions are in compliance with Title IV regulations. While the unitid for a campus is tied to a specific geographic location, the OPEID does not necessarily have to be. At non-profit institutions in the IPEDS, each separate campus is associated with a specific OPEID, so the OPEID is tied to a unique geographic location. At for-profit institutions in the IPEDS, separate campuses from across the country can be associated with one OPEID. Institutions can also change which campuses are associated with an OPEID. In 2008, ITT Tech had 43 different campuses across the USA. These 43 different campuses were split into 22 different bundles. Each of these bundles is associated with just one 90/10 revenue percentage. These bundles are determined by the company that owns ITT Tech, and are unrelated to the geographical proximity of the campuses. For example, one bundle includes campuses located in Washington, Kansas, and North Carolina. Another includes campuses in California, Missouri, and Georgia. Herzing University was considering pairings that included Toledo with Omaha and Akron with Minneapolis, even though Toledo and Akron are geographically closer. Bundling campuses across states occurs relatively frequently in my data. The universities most frequently engaging in this type of bundling tend to be well known, publicly traded names such as ITT Tech, Everest College, and Brown Mackie College. Other for-profit institutions like the University of Phoenix, Bryant and Stratton College, and National American University bundle all their campuses together instead of dividing them into many smaller bundles. Since the 90/10 revenue percentage associated with a particular bundle of campuses cannot be tied to a specific geographic region, it is impossible to account for local economic and demographic conditions and changes without first unbundling the revenue percentages. In Section ""Potential Effect of the HEOA Rule Change"", I unbundle for-profit institution’s revenue percentages by calculating a revenue percentage for each individual campus. President Bush signed the Higher Education Opportunity Act of 2008 (HEOA) on August 14th. It reauthorized the Higher Education Act of 1965, which must be renewed every four to six years. The HEOA expired in 2013, though the changes it made remain in place until congress passes a reauthorization bill. As Section 1.1.1 mentioned, the government began imposing accountability measures on for-profit institutions in 1992. The HEOA changed some of those accountability measures, as well as adding new ones. The HEOA relaxed the 90/10 rule by moving the requirement into the program participation agreement instead of leaving it in the eligibility requirements. As an eligibility requirement, violating the 90/10 rule results in a loss of eligibility in the university’s next fiscal year. Moving the rule into the program participation agreement gives for-profit institution a second year to come back into compliance with the 90/10 rule. Moving the language was effective on the date of signing in 2008, but the Code of Federal Regulations (CFR) was not updated until July 1st, 2010. I consider the rule change effective in 2010 though for-profit institutions undoubtedly anticipated the change. The HEOA also contains changes to the calculation of the 90/10 revenue percentage. Before 2008, loan repayments counted as non-Title IV revenue but not the net present value of the loans. Between 2008 and 2012, the net present value of loans made by the for-profit institutions count as non-Title IV revenue. The calculation change increases the amount of non-Title IV revenue a for-profit institutions receives, effectively decreasing the revenue percentage if the institution does not change its behavior. The institution can accept more Title IV aid but, without a change in the 90/10 violation policy, the institution has no incentive to exceed their revenue percentage before the calculation change by increasing their Title IV revenue. Since the calculation change starts at the beginning of my data and expires near the end, it does not complicate my analysis of the violation rule change that occurs in the middle of my data. During the Obama administration, the Department of Education worked toward requiring for-profit institutions to prove their students are gainfully employed to maintain Title IV eligibility. While “gainful employment” regulations have existed in the Higher Education Act since 1965, schools have not been required to provide proof that their alumni are gainfully employed. The Department of Education worked toward defining gainful employment using metrics like debt-to-income ratios, loan repayment rates, and completion and job placement rates. The gainful employment rule was originally proposed in July 2010, but a federal judge struck the metrics down in July 2012. The final gainful employment rules were finalized in 2015 (Fountain 2019). Fountain (2019) found that enrollment at for-profit institutions grew more slowly than at non-profit institutions. She attributes this decline in growth to regulatory uncertainty surrounding gainful employment regulations. While regulatory uncertainty may affect for-profit institutions’ behavior, I do not consider this potential effect on for-profit institutions’ campus bundling behavior to keep my model tractable.",
49,2,Eastern Economic Journal,20 March 2023,https://link.springer.com/article/10.1057/s41302-023-00246-1,Crypto-Currencies and Crypto-Assets: An Introduction,April 2023,Alexandre Olbrecht,Gina Pieters,,Male,Female,Unknown,Mix,,
49,2,Eastern Economic Journal,20 January 2023,https://link.springer.com/article/10.1057/s41302-023-00235-4,Eliminating Environmental Costs to Proof-of-Work-Based Cryptocurrencies: A Proposal,April 2023,Gabriel Mathy,,,Male,Unknown,Unknown,Male,"Proof-of-Work (POW) mechanisms, such as those used by Bitcoin, require a huge amount of electricity (Vranken 2017). Proof-of-Stake (POS) mechanisms provide a promising alternative, with minimal computational requirements, which reduce electricity use (Saleh 2021). There are drawbacks to POS, however, including lessened security, the influence of large players, and illiquidity when staking. This paper proposes a market-based mechanism that will provide all the benefits of POW without any of the environmental costs, with electrical usage as low, or lower, than a POS mechanism. POW mechanisms are based on the solution to cryptographic problems, which are difficult to solve but trivially easy to verify. Miners that successfully solve these difficult cryptographic problems are allowed to add transactions to the blockchain through the formation of new blocks. I will consider the case of Bitcoin, as the most famous cryptocurrency (Velde 2013). POW mechanisms vary in their specifics, but the general principles remain the same across all POW-based cryptocurrencies. New block formation creates a permanent and indelible record of cryptocurrency transactions which can be viewed by anyone on the planet. The time required for new block formation, or the “block time,” will vary based on the amount of computing power in the system. If block time is too short, then fewer transactions will be added to each block, making it potentially less secure. If the block time is too long, then transactions are added infrequently and this delay is inconvenient for those trying to use Bitcoin for transactions. There is no single optimal block time. Bitcoin’s algorithm currently forms blocks every ten minutes. However, whatever block time is chosen, currently, all cryptocurrencies that use POW adjust the difficulty of mining a block to delay and accelerate block formation (Kraft 2016). This functions as a tax of sorts (Podhorsky 2021). While difficulty is part of the security aspect of the chain and cannot be eliminated entirely, increasing the difficulty does not improve any other aspect of Bitcoin’s functioning and simply makes mining use more electricity than it would if the difficulty was lower. Increasing difficulty involves a pure waste of electricity and is the primary reason why Bitcoin mining uses so much electricity. As interest in Bitcoin has grown and the price of Bitcoin has risen, difficulty has increased, reaching a difficulty in the trillions today. However, this is not the only way to ensure that block time hits a target. This paper proposes an alternative mechanism. Instead, the quantity of outstanding cryptocurrency would be scaled downward until the price of Bitcoin would be adjusted downward, or the amount of Bitcoin awarded to new miners would be adjusted downward, until miner interest declines. This would ensure a constant block time, and so difficulty could be set to its technological minimum at 1. This would reduce the electricity usage by Bitcoin on the order of trillions, resulting in an electricity usage far below that of the formal banking and financial system. This change would eliminate environmental concerns with Bitcoin. The differences between the two protocols, the current one, and the new one proposed in this paper, are derived formally using a microeconomic model of miner profit maximization. Some tradeoffs involved with having algorithmic quantity adjustment rather than algorithmic difficulty adjustment are also discussed, as well as a method to avoid 51% attacks, where a large enough miner could effectively take-over the Bitcoin network and post fraudulent transactions. Then, the paper concludes.",
49,2,Eastern Economic Journal,21 January 2023,https://link.springer.com/article/10.1057/s41302-023-00237-2,Valuation of Cryptocurrency Without Intrinsic Value: A Promise of Future Payment System and Implications to De-dollarization,April 2023,Garrison Hongyu Song,,,Male,Unknown,Unknown,Male,"The twenty-first century is a digital age in which our life will fundamentally be transformed by information technology and the Internet. People can easily perceive the impacts of the digital revolution on the real economy associated with goods and services. However, as one of the most important financial innovations, the creation of cryptocurrencies such as Bitcoin has not been fully appreciated. Known as the “founding father of international finance,” Mayer Amschel Rothschild once said: “give me control of a nation's money and I care not who makes the laws.” With the same logic, cryptocurrency is expected to play a central role in the digital economy, which may be the main reason why Facebook (Meta) has recently planned to issue its cryptocurrency (Libra) and the People’s Bank of China is doing so as well (e-CNY). There exist divergent opinions on cryptocurrency. For instance, one former chairperson of the Federal Reserve System (the Fed), Ben Bernanke, states that cryptocurrency “may have long-term promise,” while another former chairperson of the same institution, Alan Greenspan, indicates that “there is no point” for central banks to issue a digital currency. One important unresolved issue about cryptocurrency is how to value it. Here, the traditional valuation approach hinged on discounted cash flows (DCF) cannot be applied since cryptocurrency is generally considered without intrinsic value, which is profoundly different from stocks, bonds, or any tangible assets endowed with some future cash flows. In this paper, we establish a random search-based model to study the valuation of cryptocurrency. We provide a unique approach to valuing cryptocurrency according to people’s acceptance probability of cryptocurrency. We interpret why cryptocurrency without intrinsic value can have value in an exchange economy due to its role as a medium of exchange, which is contrary to the traditional view that an asset without intrinsic value should not have value. Our study fills in the missing chain in the field of cryptocurrency valuation. Not limited to the above contribution, our model can provide a tractable framework to further investigate various issues related to the digital economy. Our model indicates that the value of cryptocurrency is rather unstable owing to the existence of multiple market equilibria including barter equilibrium, dollar-dominated equilibrium, and cryptocurrency-dominated equilibrium. Ultimately, the high volatility of the value of cryptocurrency can severely block its application as a future payment system to replace the US dollar’s position in international trade. Lastly, we investigate the impacts of several key factors on the value of cryptocurrency via numerical examples and explore their empirical implications. Our numerical analysis shows that the value of cryptocurrency has a self-fulfilling property. In other words, Ceteris paribus, more people owning and using cryptocurrency could lead to a higher value of cryptocurrency. Meanwhile, the wide holding of the US dollar by people in the economy (originating from the oversupply of the dollar by the Fed) will strengthen people’s willingness to accept cryptocurrency as an alternative medium of exchange and thus raise the value of cryptocurrency. The theoretical foundation of this paper is search theory which is initially used in labor economics to study the matching behavior between workers and firms and then gradually applied in various areas of economics (Diamond 1984; Mortensen and Pissarides 1994). Duffie, Garleanu, and Pedersen (2002, 2005, 2007), Lagos and Rocheteau (2007), Vayanos and Weill (2008) are forerunners to advancing search theory to dynamic capital asset markets. Our model is directly inspired by the dual currency model of Kiyotaki and Wright (1993), a simplified version of Kehoe et al. (1993). Kiyotaki and Wright apply a search-theoretic approach to illustrate the essential function of money as a medium of exchange. Hendrickson et al. (2016) present a similar search-based model to study the circulation of Bitcoins in the presence of different types of government policies. Hendrickson and Luther (2017) further examine whether a government can prevent the circulation of Bitcoins. Rietz (2019) provides experimental evidence about the conditions for the acceptance of the secondary currency under the framework of the dual currency model. Different from their models, our model is a general equilibrium model. More importantly, we innovatively link the equilibrium solution to the main theme of our paper—the valuation of cryptocurrency. Caginalp and Caginalp (2018) study the same topic and provide a simple formula for the valuation of cryptocurrency in which an “ asset that has no value by traditional measures will tend to trade at a price that is determined largely by the fraction of the amount of dollars available for the asset divided by the total number of units of the asset.” This is derived from a mathematical model by Gaginalp and Balenovich (1999) and has been tested in several experiments (Caginalp et al. 1998, 2001). While their results are consistent with the simplest point estimation of our model, we have different opinions on the function played by cryptocurrency and on the source of its price volatility. Caginalp C. and Caginalp G. claim that “cryptocurrencies may be simply a mechanism for a transfer of wealth from the late-comers to the early entrants and nimble traders.” This can be traced back to the concept of “bubbles” as their valuation foundation. However, we consider that cryptocurrency, like any type of fiat currency, can serve as a medium of exchange and facilitate transactions of goods and services, which is valuable to our modern society characterized by its division of labor. Furthermore, they ascribe the price instability of cryptocurrency to the absence of valuation, while we resolve the valuation of cryptocurrency and consider the existence of multiple equilibria as the cause. Seligman (2015) discusses various features of Bitcoin as a currency and summarizes the legal and social requirements of sustained issuance and use of any cryptocurrency. Alstyne (2014) provides a conceptual description of why Bitcoin has value from four aspects: (1) Bitcoin solves the double-spending problem, i.e., only the rightful owner of bitcoins can spend them; (2) Bitcoin network enables near-frictionless commerce; (3) Bitcoin can better detect transaction frauds than credit cards; and more practically (4) Bitcoin has value since people accept it- “accepting money is what gives it value.” While Alstyne considers the concerted opposition by central banks as the only serious threat to Bitcoin, he has reservations on the other three objections against Bitcoin: (1) Bitcoin promotes illegal activities; (2) Bitcoin has a high-security risk; (3) Bitcoin price is highly volatile. Alstyne claims that the price volatility of Bitcoin alone is not enough to deprive Bitcoin of the role of currency. He further predicts that the value of each currency unit will increase with the growth of an economy and that the volatility itself should fall as Bitcoin gains wider acceptance. Kim (2015) also projects that “Bitcoin price may stabilize over time” by comparing the price patterns between mature and newly introduced virtual currencies used in online games. The rest of the paper proceeds as follows: Section “General Picture” depicts a random search-based model qualitatively; Section “Mathematical Model” establishes the mathematical model in a general format; Session 4 defines the concept of market equilibrium and targets the symmetric steady-state Nash equilibrium; Session 5 discusses three types of market equilibria and their economic implications; Section “Numerical Examples” investigates the impacts of key factors on the value of cryptocurrency via numerical examples; Section “Conclusions” concludes. Symbols and notations are summarized in Appendix A. Proofs of propositions are provided in Appendix B.",
49,2,Eastern Economic Journal,20 March 2023,https://link.springer.com/article/10.1057/s41302-023-00247-0,Relation Between Bitcoin and Its Forks: An Empirical Investigation,April 2023,Neveen Ahmed,Omar Farooq,Nidaa Hamed,Unknown,Male,Female,Mix,,
49,2,Eastern Economic Journal,04 March 2023,https://link.springer.com/article/10.1057/s41302-023-00240-7,Educational Game on Cryptocurrency Investment: Using Microeconomic Decision-Making to Understand Macroeconomics Principles,April 2023,J. Zhu,L. Zhang,,Unknown,Unknown,Unknown,Unknown,,
49,3,Eastern Economic Journal,20 May 2023,https://link.springer.com/article/10.1057/s41302-023-00253-2,Introduction to the Second Symposium on Teaching Economics: Innovations in Times of Uncertainty,June 2023,Cynthia Bansak,Julie K. Smith,,Female,Female,Unknown,Female,,
49,3,Eastern Economic Journal,07 March 2023,https://link.springer.com/article/10.1057/s41302-023-00238-1,Infusing Diversity in a History of Economic Thought Course: An Archival Study of Syllabi and Resources for Redesign,June 2023,Sarah F. Small,,,Female,Unknown,Unknown,Female,"Students of history of economic thought are bombarded with images of White men, which may make many feel disconnected from the course and from the field overall. However, a history of economic thought course presents an ideal opportunity to introduce students to pluralism in the field and to the importance of biography in an economist’s research. Offering a history of economic thought course with an explicit intention to diversify economists presented along dimensions of race, nationality, and gender could serve as a vehicle for introducing pluralism and could attract students who otherwise might find the course unrelatable.Footnote 1 Showing students economists who look like them and who have similar experiences improves their sense of belonging in the discipline. It also provides a richer understand of the field. After briefly detailing these motivations for teaching history of economic thought and for incorporating diversity into such courses, I conduct archival research using a collection of syllabi from the 1970s through 2020 to understand how history of economic thought courses have evolved. I find that they have shifted to include various new perspectives, but continue to focus on those from White men in Europe and the USA. I illustrate that very few have managed to truly diversify their syllabi in ways that incorporate economic thinkers beyond White men. To remedy this issue, I offer two paths for diversifying history of economic thought syllabi: (1) one which engages with the typical list of economists in a history of economic thought course, but adds perspectives from more diverse characters, and (2) another which delves into the history of feminist, stratification, and development economics to introduce more diverse characters and to more deeply engage with economic pluralism. Offering these resources is this paper’s main contributions: it proposes readings and syllabi for instructors looking to diversify their history of economic thought course. It also contributes to a limited literature on teaching history of economic thought and provides novel archival analysis of course syllabi. Ultimately, as the economics discipline tackles its problem with racial, gender, and ethnic diversity and inclusion, reforming the typical history of economic thought courses is a necessity both for students and for the subdiscipline’s survival.",
49,3,Eastern Economic Journal,20 April 2023,https://link.springer.com/article/10.1057/s41302-023-00248-z,Incorporating Racial Justice Topics into an Econometrics Course,June 2023,Marketa Halova Wolfe,,,Unknown,Unknown,Unknown,Unknown,,
49,3,Eastern Economic Journal,16 March 2023,https://link.springer.com/article/10.1057/s41302-023-00242-5,Telling My Story: Applying Storytelling to Complex Economic Data,June 2023,Karla Borja,Suzanne Dieringer,,Female,Female,Unknown,Female,"Economic data analysis and data-driven presentations are skills in high demand in almost any industry in the United States (US) and abroad. From health and sports to retail and manufacturing, every sector in the economy collects data and makes decisions supported by these figures. However, analyzing economic data can be a challenging task for students, particularly those who are exposed to evaluating the meaning and implications of data during class discussions (Ramadan and Hussein 2017; Wuthisatiana and Thanetsunthorn 2019). To improve data analysis and communication skills among college students, we designed an experiential learning activity consisting of collecting complex economic data, developing graphs, presenting the data in a visually appealing format, and at the same time connecting with their audience, by sharing a personal story. The activity is defined as the Storytelling Project (SP) and consisted of three assignments throughout the semester, including the development of a short story, and culminated in a five-minute video presentation. The evidence collected in this study suggests that the SP promotes a more confident, knowledgeable, and engaged student and improves soft skills, such as oral communication. Ample evidence shows the effectiveness of experiential learning activities over traditional teaching approaches in improving students’ engagement and understanding of the material (Gosen and Washbush 2004; Herz and Merz 1998; Voukelatou 2019). For instance, courses that include case studies, internships, in-site visits, games, and social media as a vehicle of learning display positive results in terms of hard and soft skills development, such as data analysis and oral communication (Flores and Savage 2007; Frontczak and Kelly 2000; Geerling 2012; Gulley and Jackson 2016; Jones and Baltzersen 2017; Kayes et al. 2005; Ray 2018). In addition, college students’ test scores and long-term assessment records are significantly higher among those students exposed to experiential activities (Christopher and Marek 2009; Steinhorst and Keeler 1995; Yoder and Hochevar 2005). However, the literature on effective experiential activities in economics courses is still limited, and the results warrant further investigation (Dolan and Stevens 2010; Spencer and Van Eynde 1986). The effectiveness of an experiential activity in the classroom is linked to the how and why. That is, students must complete activities that take them through a process of academic growth, so they can process and evaluate what they are doing. These class activities could include a set of tasks, self and peer evaluation at different stages, or a research journal so that students can recognize, adjust, and improve their work (Carlson and Winquist 2011; Chavan 2011; Kolb 2007). At the same time, students must recognize the purpose of the activities by having clear learning objectives and expectations linked to the tasks. Better yet, the experiential activity is more constructive when accompanied by a workbook or other written material that provides detailed information, examples, task instructions, and rubrics (Flanagan et al. 2005; Wallace and Jefferson 2015). The study aims to develop an experiential learning activity for principles of macroeconomics courses accompanied by a workbook, followed by an evaluation of its effectiveness. The authors compare self-efficacy, motivation, anxiety, effort, and understanding of macroeconomic data analysis and graphs among two groups of college students: those who completed the Storytelling Project (SP) and those who did not. The workbook was created to supplement the SP with learning objectives, tasks, multiple examples of data analysis and storytelling techniques, rubrics, and videos. The key component of the SP is the development of a personal story using the three stages of the Storytelling approach (Goaz 2020; Knaflic 2015). This study contributes to the literature in many ways. First, few experiential activities provide a supplementary workbook with learning objectives, tasks, multiple examples, rubrics, and videos. The workbook promotes a more independent learner and reduces classroom time for task instruction. Second, the SP and the workbook are learning tools that are centered on students rather than the lecturer: the story is developed and presented by the students. Third, the SP aims to instill highly desirable skills in the marketplace, such as data analysis and communication skills. Fourth, we evaluate the experiment using two complementary assessment tools: a test of cognition and a survey of self-efficacy and attitudes, providing different methods to determine the project’s effectiveness. Finally, the results from our experiment indicate that students retain more information and self-evaluate their own capacities, skills, and motivations better if they have more opportunities and time to participate in the experiential activity (Hawtrey 2007; Obi et al. 2022). This paper has six sections. Section ""Literature Review"" describes the literature in the areas of experiential learning and the storytelling approach. Section ""The Background of the Storytelling Project"" describes the storytelling project (SP), the classroom setup, and the assessment tools used when evaluating this type of experiential learning activities in the college classroom. Section ""Research Design"" provides the experiment design, which was completed during 2020–2021 (the first two years of the Coronavirus pandemic disease COVID-19), and Section ""Data Analysis"" presents the results. After comparing both groups, those completing the SP and those not completing the SP, our results indicate that the SP group shows a better understanding of macroeconomics data and graphs. These students also show higher self-efficacy, less anxiety, and more effort in completing coursework. Finally, Section ""Conclusions and Recommendations for Future Research"" provides concluding remarks and discusses different ways this project can be adapted to other introductory economic and business courses.",
49,3,Eastern Economic Journal,16 March 2023,https://link.springer.com/article/10.1057/s41302-023-00244-3,Bounty Hunters Can Teach Microeconomics: Illustrations from Netflix's Cowboy Bebop,June 2023,Amel Ben Abdesslem,Julien Picault,,Male,Male,Unknown,Male,"Economics educators have used pop culture to engage learners in the classroom for over twenty years. However, pop culture is beneficial for students' engagement and acts as a transversal to assimilating economics concepts (Picault 2019). Various forms of pop culture have been recommended and used by economics educators; the most popular being music (Krasnozhon 2013; Lawson et al. 2008; O'Roark et al. 2018; Rousu 2016), videos (Al‐Bahrani and Patel 2015; Calhoun and Mateer 2011; Exposito et al. 2020; Wooten 2018), video games (Ng 2019; Vidal 2020; Youngberg 2019), and poker (Ferguson and Ferguson 2003; Picault 2020; Reiley et al. 2008).Footnote 1 Educators can use traditional lectures as a primary form of instruction while incorporating more active learning activities to stimulate creative thinking and generate enthusiasm among students. Active learning strategies, such as using popular media, can make lectures more engaging (Calhoun and Mateer 2011). Students are, therefore, more likely to quickly grasp economic concepts (Hoyt 2003). We can identify two main concerns instructors may have about using alternative media in their classrooms. The first concern is the lack of assessment regarding increased engagement and performance. However, a few studies have attempted to assess the effectiveness of videos and popular media. Moreno and Ortegano-Layne (2008) found that students who watched any videos in the class reported being more favorable toward learning and could better apply the concepts than those from the control group. Another study found that movies helped to increase attention and learning skills in undergraduate classes (Ismaili 2013). The movie clips were integrated into an English course teaching material, and a control group was used to assess the effectiveness of integrated skills. Wooten et al. (2021) found that more than fifty percent of students considered using Popular Korean music (K-Pop) in the classroom enhanced their learning experience and interest in economics concepts. Empirical research on the effectiveness of using popular media in Economics classrooms remains limited. While we cannot conclude that it directly improves student learning, a consensus exists on the effectiveness of ""fun"" activities. These activities, such as watching popular TV shows, promote student engagement and create a positive learning environment (Whitton and Langan 2019; Robinson and Kakela 2006; Tews et al. 2015). The second concern is the cost/benefit of using a different material. Identifying suitable pop culture media while still staying relevant can be time-consuming. Our material can help instructors integrate video clips into their preferred active learning approach. For instance, this paper offers video clips that can be directly integrated into a Principles of Microeconomics course, which can support students learning about microeconomic concepts for the first time. Recent economics education literature also identified that popular culture could be used to create resources promoting diversity, inclusion, and gender awareness. This aligns with Al-Bahrani's (2022) recommendation to increase representation in the teaching material instructors present to students. Al-Bahrani recommends that educators deliberately increase their diversity efforts in three areas: economic content, pedagogy and assessment, and student interactions. One of his five principles of inclusive teaching is selecting course content that expresses diversity. For instance, Ben Abdesslem (2022), Geerling et al. (2021a), and Wooten et al. (2021) use music from various countries to bring some diversity and inclusion in the classroom. Similarly, Ben Abdesslem and Picault (2021), Geerling et al. (2022), and Geerling et al. (2021b) identified video clips from multiple origins that could be used in economics lectures. Resources highlighting diversity can foster a sense of belonging among first-year students (Bayer et al. 2020). This paper provides students with diverse illustrations of microeconomics concepts from the Netflix Originals series Cowboy Bebop. We selected eight illustrative clips that illustrate numerous microeconomic concepts. Specifically, we focused on developing teaching material based on concise videos, as shorter videos may be easier for instructors to use in the classroomFootnote 2. For instance, Goffe and Kauper (2014) showed that both in-class coverage and preparation time costs are significant deterrents that prevent economics instructors from using teaching methods other than traditional lectures. This paper provides solutions for these time constraints. In addition, we provide already prepared teaching material that is not time-consuming to implement in the classroom. Using the Netflix Originals series Cowboy Bebop provides diverse options when teaching microeconomics. This 2021 American science fiction series is based on the 1998 Japanese anime series of the same name. Set in 2171, the show focuses on the adventures of a group of bounty hunters. Earth is no longer habitable, and humanity has now migrated throughout the galaxy. This Netflix series offers excellent illustrations of microeconomics concepts and helps reinforce diversity and inclusion. First, Cowboy Bebop's cast is remarkably diverse, exemplified by the three main characters played by actors John Cho, Mustafa Shakir, and Daniella Pineda. This series was particularly praised for its castFootnote 3. The cast is indeed racially diverse and gender-inclusive. Second, the New Zealand-filmed series is based on the Japanese anime series and action movie both also named Cowboy Bebop. Third, the futuristic world exposed in the series immerses the spectator in a society where diverse people interact. The multicultural environments on different planets highlight racial diversity. This universe is filled with various cultures and languages that exist symbiotically.",1
49,3,Eastern Economic Journal,22 April 2023,https://link.springer.com/article/10.1057/s41302-023-00250-5,Approaches to Intermediate Microeconomics,June 2023,Amanda Page-Hoongrajok,Sai Madhurika Mamunuru,,Female,,Unknown,Mix,,
49,3,Eastern Economic Journal,04 March 2023,https://link.springer.com/article/10.1057/s41302-023-00239-0,Teaching Environmental Macroeconomics to Undergraduate Students,June 2023,Sahar Milani,,,Female,Unknown,Unknown,Female,"In 2018, William Nordhaus and Paul Romer were awarded the Nobel Prize in Economics for their work integrating climate change and innovation into long-run macroeconomic models (Nobel Prize Committee 2018). Despite this recognition, environmental economics has traditionally been considered a microeconomics field. As a consequence, there is less emphasis on macroeconomic issues in undergraduate environmental economics textbooks. One exception is the textbook by Harris et al. (2017) who provide a discussion of macroeconomic issues through the lens of both neoclassical economics and ecological economics. Ecological economists view the economic system as a subset of the broader ecological system. This approach implies that the economy should not grow larger than the biosphere or else the earth will face environmental destruction (Daly 2007). This conclusion differs from mainstream economic literature that indicates that technological change is simultaneously important for sustained growth and for limiting environmental impacts (Popp et al. 2010). The reduced coverage of macroeconomic topics presents a significant content gap for three reasons. First, it limits the discussion of global environmental policy and climate change within undergraduate environmental economics courses. Theoretical work indicates that optimal environmental policy includes both carbon taxes and research subsidies, leading to cleaner production methods without sacrificing long-run growth (Acemoglu et al. 2012). Recent empirical results confirm that carbon taxes do not negatively impact GDP growth rates (Metcalf and Stock 2020b). In other words, the latest and most significant findings regarding economic growth and the environment are not yet part of standard undergraduate curriculum. Second, less time is spent teaching growth topics to undergraduates in general. Acemoglu (2013) points out that growth is rarely taught in principles of economics courses, even though it is a topic many students have an interest in. Given the fact that curriculum for intermediate macroeconomic courses varies widely across textbooks and instructors, it is possible that students may not see the standard Solow model unless they take a course specific to economic development. Environmental economics courses are no exception to this deficiency. Third, when topics related to growth and the environment are included in environmental economics textbooks, they are often presented descriptively without any discussion of theoretical models. For example, the textbook by Callan et al. (2013) provides a brief discussion of the environmental Kuznets curve (EKC), the hypothesized inverted U-shaped relationship between economic growth and environmental degradation. The authors review the empirical findings for this relationship yet there is no accompanying quantitative exercise regarding what may generate this relationship. This is commonplace among environmental economics textbooks. Can we teach environmental macroeconomics solely using empirical examples? I argue that students deserve a complete picture of the topic that combines the elegance and intuition of economic theory with the real-world applications presented in most textbooks. Instructors often have multiple learning goals including content-driven objectives that help students gain the necessary foundation for future coursework and for understanding policy debates. Learning goals can also involve building analytical skills and challenging students with a new way of thinking (De Araujo et al. 2013). A simple theory model is well suited to expose students to topics not typically emphasized in environmental economics courses while also fostering critical thinking. In this paper, I propose a simplified version of Brock and Taylor (2010)’s “green” Solow model that is accessible for undergraduates. This model provides theoretical support for the EKC hypothesis by including technological progress in abatement in the standard Solow model. I present the model in five systematic steps, using only basic algebra to reinforce the economic intuition. This requires a simplifying assumption and omissions, namely that the growth rate of emissions is equal to the growth rate of capital per effective worker. These exclusions are justified as they result in a version of the model that requires no calculus or differential equations to understand. I outline the model as a stand-alone content reference and explain how the material can be taught in multiple learning modalities. There is often concern that the Solow model is too difficult, both intuitively and computationally, for undergraduates (Stein 2007). However, I argue that this restatement of the model is teachable to undergraduates since it builds upon micro-foundational insight and tools that are already consistently taught in principles and intermediate microeconomics. Since it extends what is widely regarded as a major workhorse model of modern macroeconomics, it is relatively simple to develop supplementary materials such as problem sets and exam questions. A further benefit is that the model provides theoretical support for the EKC, a topic that is already peripherally discussed in the environmental economics textbooks reviewed in the next section. Thus, it provides a complement to existing material. The rest of this paper is organized as follows. I review the macroeconomics content that is included in a set of standard environmental economics textbooks and provide context regarding the inclusion of the “green” Solow model in undergraduate coursework. Next, I present the simplified model and outline recommendations for including the model in different course formats as an accompaniment to empirical applications. Finally, I offer concluding remarks.",
49,3,Eastern Economic Journal,19 May 2023,https://link.springer.com/article/10.1057/s41302-023-00251-4,"Engaging Students, Faculty, and External Professionals with a Data-Centered Group Capstone Project",June 2023,Andre R. Neveu,Angela M. Smith,,Male,Female,Unknown,Mix,,
49,3,Eastern Economic Journal,18 May 2023,https://link.springer.com/article/10.1057/s41302-023-00252-3,A Flipped Classroom Experiment in Growth Theory,June 2023,Peter Mikek,,,Male,Unknown,Unknown,Male,"Flipped classroom pedagogy has attracted substantial attention for its ability to engage students and successfully bridge the gap between the teaching styles of professors and the learning styles of students (Lage, et al. 2000). We study the effectiveness of flipped classroom pedagogy on students’ performance in a growth theory module in intermediate macroeconomics. Specifically, the research question is: Can the flipped classroom approach to a growth module in small sections of intermediate macroeconomics effectively improve students’ performance on a high stakes test? Our contribution is two-fold: (a) we study the effectiveness of the teaching method within economic growth theory and (b) we investigate the method in an intermediate class with small sections at a liberal arts college. The reported benefits of flipped classroom pedagogy range from better short-term student learning outcomes and increased student-teacher interaction to more collaboration among students and individual pacing of learning. However, the challenges include stress for certain marginalized students working in groups, increased workloads, disparities in access to technology outside the classroom, and even the need for some adjustments in teaching and teachers themselves (Estes, et al. 2014; Akcayir and Akcayir 2018; Strelan, et al. 2020). There are relatively few studies that have endeavored to rigorously measure the effect of a flipped classroom approach on performance in economics. The studies in flipped classroom learning specifically in economics have been mostly done for large introductory classes (Wozny, et al. 2018; Balaban et al. 2016; Craft and Linask 2019). Our approach is different, as we worked with small sections that, in general and given the academic culture of a small liberal arts college, require strong direct engagement of both students and the professor. Additionally, we focused on students in intermediate economics. Finally, we consider a growth theory module within a core economics class and endeavor to measure performance immediately following exposure. Some authors acknowledge the challenges of teaching growth theory in undergraduate economics classes (Stein 2007). Additionally, our own experience from teaching the topic for many years indicates that students have substantial difficulties with the material. The attempts at overcoming these challenges suggest the use of active learning approaches (Swoboda and Feiler 2016), such as the flipped classroom. However, there is a lack of studies of its effectiveness (Purba et al. 2021), particularly for economic growth theory. Thus, we report on an attempt to rigorously measure the effectiveness of the flipped model in teaching intermediate growth theory. The effectiveness of flipped pedagogy reported for economics varies substantially, from studies that find some evidence of improved performance (Lage et al. 2000; Wozny, et al. 2018) to others that find no improvement at all (Craft and Linask 2019; Setren et al. 2020). Thus, our motivation stems from these contradictory results and from a lack of studies on the effectiveness of active learning approaches in teaching growth theory in particular. We report on a classroom experiment comparing different sections of Intermediate Macroeconomics conducted through the module on growth theory. The experimental design followed the standard format of contrasting the results of treatment groups (flipped/problems) to control sections (standard lecture). In addition to the two sections in the same semester, we report the results for randomly chosen sections in different semesters (Craft and Linask 2019; Ficano 2019). At the outset, we expected that the flipped classroom approach would improve students’ performance on a test. However, some of our doubts about the approach stemmed from our experience with students’ relatively short attention spans, a strong focus on grades vs. learning, and experience with previous “learn before the class” exercises. The results clearly reject the null hypothesis of better performance for treatment groups on a brief test immediately following the experiment. Additionally, the main determinant of test success is the quality of high school, reflecting the socio-economic environment of individual students.",1
